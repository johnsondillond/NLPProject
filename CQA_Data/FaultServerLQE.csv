Id,Title,Body,Tags,CreationDate
"810005","Nginx Server Block for SSL don't work","<p>My site is available without ssl via http but not via https. With <a href=""https://www.example.com"" rel=""nofollow noreferrer"">https://www.example.com</a> occurs a network timeout. Also the nginx error.log shows no errors. Why am I not able to redirect and reach the site via https?</p>

<p>This is my nginx server block file:</p>

<pre><code>server {
    listen 80;
    listen [::]:80;
    server_name www.example.com example.com example.org www.example.org
    return 301 https://$server_name$request_uri;
}
server {
    # SSL configuration
    listen 443 ssl;
    listen [::]:443 ssl;
    ssl on;
    ssl_certificate /etc/letsencrypt/live/www.example.com/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/www.example.com/privkey.pem;  

    root /var/www/example;

    # Add index.php to the list if you are using PHP
    index index.php index.html index.htm index.nginx-debian.html;

    server_name example.com www.example.com;

    # pass the PHP scripts to FastCGI server
    #
    location ~ \.php$ {
        try_files $uri  =404;
        fastcgi_split_path_info ^(.+\.php)(/.+)$;
        # With php5-fpm:
        fastcgi_pass unix:/var/run/php5-fpm.sock;
        fastcgi_index index.php;
        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
        include fastcgi_params;
    }

    # deny access to .htaccess files, if Apache's document root
    # concurs with nginx's one
    #
    location ~ /\.ht {
        deny all;
    }
}
</code></pre>

<p>Output of netstat -tulpen|grep 443</p>

<pre><code> tcp        0      0 0.0.0.0:443             0.0.0.0:*               LISTEN      0          417562386   339/nginx: worker p
    tcp6       0      0 :::443                  :::*                    LISTEN      0          417562387   339/nginx: worker p
</code></pre>

<p>Edit:
This is my nginx.conf</p>

<pre><code>user xy;
worker_processes 5;
pid /run/nginx.pid;

events {
        worker_connections 4096;
        # multi_accept on;
}

http {

        ##
        # Basic Settings
        ##

        sendfile on;
        tcp_nopush on;
        tcp_nodelay on;
        keepalive_timeout 65;
        types_hash_max_size 2048;
        # server_tokens off;

        # server_names_hash_bucket_size 64;
        # server_name_in_redirect off;

        include /etc/nginx/mime.types;
        default_type application/octet-stream;

        ##
        # SSL Settings
        ##

        ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # Dropping SSLv3, ref: POODLE
        ssl_prefer_server_ciphers on;

        ##
        # Logging Settings
        ##

        access_log /var/log/nginx/access.log;
        error_log /var/log/nginx/error.log;

        ##
        # Gzip Settings
        ##

        gzip on;
        gzip_disable ""msie6"";

        # gzip_vary on;
        # gzip_proxied any;
        # gzip_comp_level 6;
        # gzip_buffers 16 8k;
        # gzip_http_version 1.1;
        # gzip_types text/plain text/css application/json application/javascript text/xml application/$

        ##
        # Virtual Host Configs
        ##

        include /etc/nginx/conf.d/*.conf;
        include /etc/nginx/sites-enabled/*;
}
</code></pre>
","<nginx><ssl>","2016-10-19 15:03:18"
"810021","Use SSH to copy remote files to local NAS drive","<p>I'm looking to create a script that logs into a machine and pulls back files to a NAS server. The files will be put into a compressed file as it downloads.</p>

<p>Because the script will be looking to download files with varying owners and permissions, it will need to run as root to make sure it captures everything in the home directory. </p>

<p>I have edited my visudo file to allow the backup user to sudo tar without a password. But there is something stopping the command from running successfully:</p>

<pre><code>backup ALL=NOPASSWD: /usr/tar
</code></pre>

<p>Command ran from the NAS:</p>

<pre><code>ssh -t backup@192.168.1.4 ""sudo tar -zcf - /home/"" &gt; /var/backup/server1.tar.gz
</code></pre>

<p>Any help would be great, thanks</p>

<p>Update: It would of been good to know why I was down voted for this question.</p>
","<backup><sudo><tar>","2016-10-19 15:57:51"
"810036","bash script to detect when a file is added to folder and command exec","<p>I am trying to write a script to detect whenever a file is added to a specific folder and run a command using the last file added name. What I am specifically trying to do is to create a QR code for each file I add in a specific folder.</p>

<p>So what I would need to do is: detect when a file is added to folder, grab base filename and pass to <code>qrencode -o filename.png mysite/filename.ext</code>,
and ideally, have it a cronjob that starts at boot.</p>

<p>I was reading something about using <code>inotify</code>, but I'm not sure how to make this happen.</p>
","<bash><unix><scripting><shell-scripting><unix-shell>","2016-10-19 17:17:39"
"810374","How to protect this kind of fake email","<p>I received an email from a supplier that was hacked.</p>

<p>Customer domain is (header.from = dhavalgroup.com), but the email was sent from (smtp.mailfrom = deltaexports.us).</p>

<p>How to protect this kind of fake email?</p>

<pre><code>Authentication-Results: spf = none (IP sender is 173,201,192,164)
smtp.mailfrom = deltaexports.us; mydomain.com.br; dkim = none (message not
signed) header.d = none; mydomain.com.br; DMARC = none action = none
header.from = dhavalgroup.com; mydomain.com.br; dkim = none (message not
signed) header.d = none;
Received-SPF: None (protection.outlook.com: deltaexports.us does not designate
permitted sender hosts)
Received: from p3plwbeout13-02.prod.phx3.secureserver.net (173,201,192,164) by
BN1BFFO11FD048.mail.protection.outlook.com (10.58.145.3) with Microsoft SMTP
Server (version = TLS1_2, cipher = TLS_RSA_WITH_AES_256_CBC_SHA) id 15.1.669.7
Frontend via Transport; Mon, 17 Oct 2016 08:56:30 +0000
Received: from localhost ([173,201,192,136])
              by p3plwbeout13-02.prod.phx3.secureserver.net with bizsmtp
              id wYwV1t0012x1vXx01YwVbq; Mon, 17 Oct 2016 01:56:29 -0700
</code></pre>

<p>Thank you.</p>
","<spam>","2016-10-21 01:54:05"
"810442","SaltStack: create ssh keypair and add it to authorized_keys","<h1>Goal</h1>

<p>For testing, we want this to work: <code>ssh $USER@localhost</code>.</p>

<ol>
<li>create rsa keypair in ~/.ssh/, if not already there</li>
<li>add .ssh/id_rsa.pub to .ssh/authorized_keys, if not already in this file. </li>
</ol>

<h1>Question</h1>

<p>How to do this with salt-stack?</p>
","<ssh-keys><saltstack>","2016-10-21 11:07:48"
"879576","How Skype to Skype For Business Communication, is configured?","<p>Our office emails hosted on Office-365 / Skype For Business for instant messaging.</p>

<p>Our vendors do not have Skype for business accounts.
What configuration needs to be done on Exchange? We need to connect with them using </p>

<blockquote>
  <p>From Skype For Business ==> to Skype Personal.</p>
</blockquote>
","<lync><skype><skype-for-business-2016>","2017-10-21 05:55:19"
"879632","Add repository to SLES 12.3 google cloud instance","<p>I've created SLES 12.3 SP3 google cloud instance, but it has no repositories available. When I run <code>zypper install ...</code> I get </p>

<blockquote>
  <p>Warning: No repositories defined. Operating only with the installed resolvables. Nothing can be installed.</p>
</blockquote>

<p>Should I choose openSUSE 12.3 repositories instead?</p>
","<google-cloud-platform><sles>","2017-10-21 18:51:06"
"810720","What's the different of moving /var into a new partition between standard way and links?","<p>I want to move <code>/var</code> into <code>/mnt/var</code> since <code>/mnt</code> is mount as a new big partition <code>/dev/xvdb1</code>.</p>

<pre><code>[root@stepping-stone ~]$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/xvda1       20G  7.1G   12G  38% /
tmpfs           7.8G   16K  7.8G   1% /dev/shm
/dev/xvdb1       99G   23G   71G  25% /mnt
</code></pre>

<p>I did that by the following commands:</p>

<pre><code>mv /var/ /mnt/
ln -fs /mnt/var/ /var
</code></pre>

<p>Then I found it seems there is standard way to do that: <a href=""https://serverfault.com/a/703607"">https://serverfault.com/a/703607</a></p>

<p>I don't see the side effects of my 'easy way'. Anyone who can help point the difference between those two methods? or maybe I missed sth?</p>
","<centos><partition><var>","2016-10-23 03:54:34"
"810733","AWS EC2 command not found","<p>Created a new EC2 Instance with Centos 7, first thing i typed yum update, it says yum: command not found</p>

<pre><code># cat /etc/*-release
CentOS Linux release 7.0.1406 (Core)
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CentOS Linux release 7.0.1406 (Core)
CentOS Linux release 7.0.1406 (Core)


# whereis yum
yum: /etc/yum /usr/etc/yum
</code></pre>

<p>Tried almost everything I could find on internet. Any solutions?</p>
","<linux><amazon-ec2><amazon-web-services><centos7>","2016-10-23 08:46:01"
"810758","installing gtk-sharp2 on Centos7","<p>I'm trying to instal (on centOS 7) this package but I got this error below ( I admit that I'm a novice with yum as well):</p>

<pre><code>sudo yum install gtk-sharp2

Loaded plugins: fastestmirror, langpacks, priorities, protectbase

Loading mirror speeds from cached hostfile
 * base: mirror.jgotteswinter.com
 * elrepo: ftp.nluug.nl
 * epel: mirror.netcologne.de
 * extras: centos.copahost.com
 * nux-dextop: mirror.li.nux.ro
 * updates: mirror.rackspeed.de

0 packages excluded due to repository protections
Resolving Dependencies
--&gt; Running transaction check
---&gt; Package gtk-sharp2.x86_64 0:2.12.11-12.el7 will be updated
--&gt; Processing Dependency: gtk-sharp2 = 2.12.11-12.el7 for package: gtk-sharp2-devel-2.12.11-12.el7.x86_64
--&gt; Processing Dependency: mono(glib-sharp) = 2.12.0.0 for package: gtk-sharp2-2.12.26-0.x86_64
---&gt; Package gtk-sharp2.x86_64 0:2.12.26-0 will be an update
--&gt; Running transaction check
---&gt; Package glib-sharp2.x86_64 0:2.12.26-0 will be installed
---&gt; Package gtk-sharp2.x86_64 0:2.12.11-12.el7 will be updated
--&gt; Processing Dependency: gtk-sharp2 = 2.12.11-12.el7 for package: gtk-sharp2-devel-2.12.11-12.el7.x86_64
--&gt; Finished Dependency Resolution

Error: Package: gtk-sharp2-devel-2.12.11-12.el7.x86_64 (@epel)
           Requires: gtk-sharp2 = 2.12.11-12.el7
           Removing: gtk-sharp2-2.12.11-12.el7.x86_64 (@epel)
               gtk-sharp2 = 2.12.11-12.el7
           Updated By: gtk-sharp2-2.12.26-0.x86_64 (download.mono-project.com_repo_centos_)
               gtk-sharp2 = 2.12.26-0
           Available: gtk-sharp2-2.12.11-7.el7.nux.x86_64 (nux-dextop)
               gtk-sharp2 = 2.12.11-7.el7.nux  You could try using --skip-broken to work around the problem  You could try running: rpm -Va --nofiles --nodigest
</code></pre>

<p>Does anybody have an idea how can I fix yum to work it out?</p>

<p>Kind regards.</p>

<hr>

<p>still problems. Launching yum distro-sync:</p>

<pre><code>    Error: Package: mono-complete-4.6.1.5-0.xamarin.1.x86_64 (@download.mono-project.com_repo_centos_)
           Requires: mono-data = 4.6.1.5
           Removing: mono-data-4.6.1.5-0.xamarin.1.x86_64 (@download.mono-project.com_repo_centos_)
               mono-data = 4.6.1.5-0.xamarin.1
           Downgraded By: mono-data-2.10.8-9.el7.x86_64 (epel)
               mono-data = 2.10.8-9.el7
Error: Package: mono-complete-4.6.1.5-0.xamarin.1.x86_64 (@download.mono-project.com_repo_centos_)
           Requires: mono-data-sqlite = 4.6.1.5
           Removing: mono-data-sqlite-4.6.1.5-0.xamarin.1.x86_64 (@download.mono-project.com_repo_centos_)
               mono-data-sqlite = 4.6.1.5-0.xamarin.1
           Downgraded By: mono-data-sqlite-2.10.8-9.el7.x86_64 (epel)
               mono-data-sqlite = 2.10.8-9.el7
 You could try using --skip-broken to work around the problem
 You could try running: rpm -Va --nofiles --nodigest
</code></pre>

<p>Something is still going wrong....I got even more than these above errors</p>
","<yum><gtk>","2016-10-23 12:41:32"
"879846","SCP Doesn't work but SSH connection does. (Amazon EC2)","<p><strong>EDIT:</strong>
The problem was that I was using a bad username (ec2-user) instead of &quot;ubuntu&quot;. Sorry guys for wasting your time. :(</p>
<p><strong>Question:</strong></p>
<p>What happened:</p>
<p>I missmoved (-R) the /home of the EC2 to /var/www/html
so when I tried to connect to ssh to this machine the connection was refused. I made It work again by moving it back again to /home and changing the permissions from /home/user/.ssh.
But now the command scp refuses me:</p>
<blockquote>
<p>Permission denied (publickey).</p>
<p>lost connection</p>
</blockquote>
<p>So now I'm stucked here.</p>
<p>Things to know:</p>
<ul>
<li>I have a kind of backup of the &quot;/home&quot; on &quot;/var/www/html_what&quot; on the EC2 instance.</li>
<li>I tried to change all permissions of the /home directory to match to another working EC2 instance.</li>
<li>I also changed the owner:group of each directory inside /home and /home (itself).</li>
</ul>
<p>Thank you for reading!</p>
","<ubuntu><ssh><amazon-ec2><permissions><scp>","2017-10-23 14:56:01"
"879969","Postfix - sender dependent default transport maps","<p>Have this feature configured, in main.cf</p>

<pre><code>sender_dependent_default_transport_maps = hash:/etc/postfix/relay_by_sender
</code></pre>

<p>in <code>relay_by_sender</code> file -</p>

<pre><code>@a.b.com relay:[1.2.3.4]:26
</code></pre>

<p>Now I want to add another line in ""relay_by_sender"" file to forward all other emails to a different smart host.</p>

<p>I have tried below, but emails with sender domain other than @a.b.com won't get forwarded to 11.22.33.44.</p>

<pre><code>@a.b.com relay:[1.2.3.4]:26
* relay:[11.22.33.44]:25
</code></pre>

<p>Can anyone tell me why the above code is not working?</p>
","<postfix>","2017-10-24 08:47:18"
"811010","Not able to stop Windows service","<p>All this while, I was able to successfully start and stop a windows service that I created, but all of a sudden for the last 1 week, I get the following error when I stop it:</p>

<blockquote>
  <p>Windows could not stop the Apache Tomcat service A on Local computer.Error 1053: The service did not respond to the start or control request in a timely fashion</p>
</blockquote>

<p>This is happening on both Windows 2008 &amp; Windows 2012. This happens only for Tomcat service A and not for Tomcat service B which runs on the same servers.</p>

<p>Any help is appreciated!</p>
","<windows><windows-server-2008><windows-server-2012-r2><service>","2016-10-24 19:01:40"
"880016","400 bad request Nginx/1.4.6 (Ubuntu) in GCP","<p>getting an error in host: 443 &amp; while connecting on host showing 400bad request error. </p>

<pre><code>(venv)root@*****-dev:/usr/local/src/security_monkey/nginx# # sudo nginx
Enter PEM pass phrase:*****
nginx: [emerg] bind() to 0.0.0.0:443 failed (98: Address already in use)
nginx: [emerg] bind() to 0.0.0.0:443 failed (98: Address already in use)
nginx: [emerg] bind() to 0.0.0.0:443 failed (98: Address already in use)
nginx: [emerg] bind() to 0.0.0.0:443 failed (98: Address already in use)
nginx: [emerg] bind() to 0.0.0.0:443 failed (98: Address already in use)
nginx: [emerg] still could not bind()
</code></pre>
","<nginx><google-compute-engine>","2017-10-24 13:12:42"
"811180","AWS server security rules","<p>I created a AWS Linux server using EC2.</p>

<p>The following snapshot give me the security group configuration: </p>

<p><img src=""https://i.sstatic.net/sRhYN.png"" alt=""Configuration""></p>

<p>But unfortunately, the following cmd operation doesn't work: </p>

<p><img src=""https://i.sstatic.net/Sr4k7.png"" alt=""error""></p>

<p>What am I doing wrong? </p>

<p>I don't know if there is any other configuration to provide in order to use my server.</p>
","<linux><amazon-web-services><cloud>","2016-10-25 14:38:05"
"811195","Winscp - Trying to log in to a distant website and having a Can't open data connection","<p>I'm using WinScp to get to a distant ftp server. I cannot connect on my laptop, having the following message <code>Can't open data connection</code>. Yet when my colleague is connecting on her machine, she can connect to the distant ftp site</p>

<p>I listed the solutions, I tried </p>

<ul>
<li>Set to passive mode: Did not work</li>
<li>Set to active mode: Did not work</li>
<li>Use another ftp client (Filezilla here): Did not work</li>
<li>Opened up the range of port on the distant firewall for the server (included <code>39000</code> to <code>41000</code>): Did not work</li>
<li>Setup a bat file based on a shared drive with ftp access to the server I want to ftp: did not work</li>
</ul>

<p>Please also find a detailed excerpt of the log</p>

<pre><code>    2016-10-25 16:16:11.100 Session upkeep
    . 2016-10-25 16:16:11.103 Session upkeep
    . 2016-10-25 16:16:11.602 Session upkeep
    . 2016-10-25 16:16:12.124 Session upkeep
    . 2016-10-25 16:16:12.615 Session upkeep
    . 2016-10-25 16:16:13.129 Session upkeep
    . 2016-10-25 16:16:13.628 Session upkeep
    . 2016-10-25 16:16:14.143 Session upkeep
    . 2016-10-25 16:16:14.644 Session upkeep
    . 2016-10-25 16:16:15.157 Session upkeep
    . 2016-10-25 16:16:15.657 Session upkeep
    . 2016-10-25 16:16:16.171 Session upkeep
    . 2016-10-25 16:16:16.682 Session upkeep
    . 2016-10-25 16:16:17.185 Session upkeep
    . 2016-10-25 16:16:17.686 Session upkeep
    . 2016-10-25 16:16:18.199 Session upkeep
    . 2016-10-25 16:16:18.699 Session upkeep
    . 2016-10-25 16:16:19.214 Session upkeep
    . 2016-10-25 16:16:19.714 Session upkeep
    . 2016-10-25 16:16:20.228 Session upkeep
    . 2016-10-25 16:16:20.728 Session upkeep
    . 2016-10-25 16:16:21.242 Session upkeep
    . 2016-10-25 16:16:21.742 Session upkeep
    . 2016-10-25 16:16:22.256 Session upkeep
    . 2016-10-25 16:16:22.770 Session upkeep
    . 2016-10-25 16:16:23.271 Session upkeep
    . 2016-10-25 16:16:23.784 Session upkeep
    . 2016-10-25 16:16:24.284 Session upkeep
    . 2016-10-25 16:16:24.798 Session upkeep
    . 2016-10-25 16:16:25.297 Session upkeep
    . 2016-10-25 16:16:25.812 Session upkeep
    . 2016-10-25 16:16:26.312 Session upkeep
    . 2016-10-25 16:16:26.312 Sending dummy command to keep session alive.
    &gt; 2016-10-25 16:16:26.312 REST 0
    &lt; 2016-10-25 16:16:26.312 350 Rest supported. Restarting at 0
    . 2016-10-25 16:16:26.312 Dummy directory read to keep session alive.
    . 2016-10-25 16:16:26.312 Listage du répertoire en cours...
    &gt; 2016-10-25 16:16:26.312 TYPE A
    &lt; 2016-10-25 16:16:26.362 200 Type set to A
    &gt; 2016-10-25 16:16:26.363 PASV
    &lt; 2016-10-25 16:16:26.411 227 Entering Passive Mode (2x,92,x,143,159,76)
    &gt; 2016-10-25 16:16:26.411 MLSD
    . 2016-10-25 16:16:26.412 Connexion à 2x.92.x.143:40780...
    &lt; 2016-10-25 16:16:36.795 425 Can't open data connection.
    . 2016-10-25 16:16:36.795 Listage du répertoire impossible
    . 2016-10-25 16:16:36.795 Got reply 4 to the command 2
    . 2016-10-25 16:16:36.912 Session upkeep
    . 2016-10-25 16:16:36.967 Session upkeep
    . 2016-10-25 16:16:37.471 Session upkeep
    . 2016-10-25 16:16:37.981 Session upkeep
    . 2016-10-25 16:16:38.136 Session upkeep
    . 2016-10-25 16:16:38.481 Session upkeep
    . 2016-10-25 16:16:38.995 Session upkeep
    . 2016-10-25 16:16:39.497 Session upkeep
    . 2016-10-25 16:16:40.010 Session upkeep
    . 2016-10-25 16:16:40.514 Session upkeep
    . 2016-10-25 16:16:41.023 Session upkeep
    . 2016-10-25 16:16:41.523 Session upkeep
    . 2016-10-25 16:16:42.037 Session upkeep
    . 2016-10-25 16:16:42.537 Session upkeep
    . 2016-10-25 16:16:43.054 Session upkeep
    . 2016-10-25 16:16:43.551 Session upkeep
    . 2016-10-25 16:16:44.065 Session upkeep
    . 2016-10-25 16:16:44.565 Session upkeep
    . 2016-10-25 16:16:45.079 Session upkeep
    . 2016-10-25 16:16:45.578 Session upkeep
    . 2016-10-25 16:16:46.093 Session upkeep
    . 2016-10-25 16:16:46.598 Session upkeep
    . 2016-10-25 16:16:47.107 Session upkeep
    . 2016-10-25 16:16:47.607 Session upkeep
    . 2016-10-25 16:16:48.121 Session upkeep
    . 2016-10-25 16:16:48.674 Session upkeep
    . 2016-10-25 16:16:49.135 Session upkeep
    . 2016-10-25 16:16:49.638 Session upkeep
    . 2016-10-25 16:16:50.150 Session upkeep
    . 2016-10-25 16:16:50.649 Session upkeep
    . 2016-10-25 16:16:51.164 Session upkeep
    . 2016-10-25 16:16:51.312 --------------------------------------------------------------------------
    . 2016-10-25 16:16:51.313 WinSCP Version 5.9.2 (Compilation 6958) (OS 6.1.7601 Service Pack 1 - Windows 7 Professional)
    . 2016-10-25 16:16:51.313 Configuration: HKCU\Software\Martin Prikryl\WinSCP 2\
    . 2016-10-25 16:16:51.313 Log level: Debug 2
    . 2016-10-25 16:16:51.313 Local account: xxx
    . 2016-10-25 16:16:51.313 Working directory: C:\Program Files (x86)\WinSCP
    . 2016-10-25 16:16:51.313 Process ID: 7428
    . 2016-10-25 16:16:51.317 Command-line: ""C:\Program Files (x86)\WinSCP\WinSCP.exe"" 
    . 2016-10-25 16:16:51.317 Time zone: Current: GMT+2, Standard: GMT+1 (Paris, Madrid), DST: GMT+2 (Paris, Madrid (heure d’été)), DST Start: 27/03/2016, DST End: 30/10/2016
    . 2016-10-25 16:16:51.318 Login time: mardi 25 octobre 2016 16:16:51
    . 2016-10-25 16:16:51.318 --------------------------------------------------------------------------
    . 2016-10-25 16:16:51.319 Session name: zZz
    . 2016-10-25 16:16:51.319 Host name: 2x.92.46.143 (Port: 21)
    . 2016-10-25 16:16:51.319 User name: GERMANY (Password: Yes, Key file: No)
    . 2016-10-25 16:16:51.319 Transfer Protocol: FTP
    . 2016-10-25 16:16:51.319 Ping type: Dummy, Ping interval: 30 sec; Timeout: 15 sec
    . 2016-10-25 16:16:51.319 Disable Nagle: No
    . 2016-10-25 16:16:51.319 Proxy: None
    . 2016-10-25 16:16:51.319 Send buffer: 262144
    . 2016-10-25 16:16:51.319 UTF: Auto
    . 2016-10-25 16:16:51.319 FTPS: Explicit TLS/SSL [Client certificate: No]
    . 2016-10-25 16:16:51.319 FTP: Passive: Yes [Force IP: Auto]; MLSD: Auto [List all: Auto]; HOST: Auto
    . 2016-10-25 16:16:51.319 Session reuse: Yes
    . 2016-10-25 16:16:51.319 TLS/SSL versions: TLSv1.0-TLSv1.2
    . 2016-10-25 16:16:51.319 Local directory: C:\Users\xxx\Documents, Remote directory: /, Update: Yes, Cache: No
    . 2016-10-25 16:16:51.319 Cache directory changes: No, Permanent: Yes
    . 2016-10-25 16:16:51.319 Recycle bin: Delete to: No, Overwritten to: No, Bin path: 
    . 2016-10-25 16:16:51.321 Timezone offset: 0h 0m
    . 2016-10-25 16:16:51.321 --------------------------------------------------------------------------
    . 2016-10-25 16:16:51.324 Session upkeep
    . 2016-10-25 16:16:51.385 Connexion à 2x.92.x.143...
    . 2016-10-25 16:16:51.386 TLS layer changed state from unconnected to connecting
    . 2016-10-25 16:16:51.386 TLS layer changed state from connecting to connected
    . 2016-10-25 16:16:51.386 Connecté à 2x.92.x.143, négociation de la connexion SSL...
    &lt; 2016-10-25 16:16:51.422 220-FileZilla Server version 0.9.41 beta
    &lt; 2016-10-25 16:16:51.669 220-written by Tim Kosse (Tim.Kosse@gmx.de)
    &lt; 2016-10-25 16:16:51.669 220 Please visit http://sourceforge.net/projects/filezilla/
    &gt; 2016-10-25 16:16:51.669 AUTH TLS
    . 2016-10-25 16:16:51.695 Session upkeep
    &lt; 2016-10-25 16:16:51.717 234 Using authentication type TLS
    . 2016-10-25 16:16:51.768 TLS connect: SSLv3 read server hello A
    . 2016-10-25 16:16:51.768 TLS connect: SSLv3 read server certificate A
    . 2016-10-25 16:16:51.768 TLS connect: SSLv3 read server done A
    . 2016-10-25 16:16:51.771 TLS connect: SSLv3 write client key exchange A
    . 2016-10-25 16:16:51.771 TLS connect: SSLv3 write change cipher spec A
    . 2016-10-25 16:16:51.771 TLS connect: SSLv3 write finished A
    . 2016-10-25 16:16:51.771 TLS connect: SSLv3 flush data
    . 2016-10-25 16:16:51.824 TLS connect: SSLv3 read server session ticket A
    . 2016-10-25 16:16:51.824 TLS connect: SSLv3 read finished A
    . 2016-10-25 16:16:51.825 Verifying certificate for ""xxx GERMANY"" with fingerprint xcr and 18 failures
    . 2016-10-25 16:16:51.825 Certificate for ""xxx GERMANY"" matches cached fingerprint and failures
    . 2016-10-25 16:16:51.826 Using TLSv1, cipher TLSv1/SSLv3: AES256-SHA, 1024 bit RSA, AES256-SHA              SSLv3 Kx=RSA      Au=RSA  Enc=AES(256)  Mac=SHA1
    . 2016-10-25 16:16:51.826 Session upkeep
    . 2016-10-25 16:16:51.854 Connexion SSL établie. En attente du message de bienvenue...
    &gt; 2016-10-25 16:16:51.854 USER GERMANY
    &lt; 2016-10-25 16:16:51.874 331 Password required for germany
    &gt; 2016-10-25 16:16:51.874 PASS ********
    &lt; 2016-10-25 16:16:51.922 230 Logged on
    &gt; 2016-10-25 16:16:51.922 SYST
    &lt; 2016-10-25 16:16:51.970 215 UNIX emulated by FileZilla
    &gt; 2016-10-25 16:16:51.970 FEAT
    &lt; 2016-10-25 16:16:52.020 211-Features:
    &lt; 2016-10-25 16:16:52.020  MDTM
    &lt; 2016-10-25 16:16:52.020  REST STREAM
    &lt; 2016-10-25 16:16:52.020  SIZE
    &lt; 2016-10-25 16:16:52.020  MLST type*;size*;modify*;
    &lt; 2016-10-25 16:16:52.020  MLSD
    &lt; 2016-10-25 16:16:52.021  AUTH SSL
    &lt; 2016-10-25 16:16:52.021  AUTH TLS
    &lt; 2016-10-25 16:16:52.021  PROT
    &lt; 2016-10-25 16:16:52.021  PBSZ
    &lt; 2016-10-25 16:16:52.031  UTF8
    &lt; 2016-10-25 16:16:52.031  CLNT
    &lt; 2016-10-25 16:16:52.031  MFMT
    &lt; 2016-10-25 16:16:52.031 211 End
    &gt; 2016-10-25 16:16:52.031 CLNT WinSCP-release-5.9.2
    &lt; 2016-10-25 16:16:52.069 200 Don't care
    &gt; 2016-10-25 16:16:52.070 OPTS UTF8 ON
    &lt; 2016-10-25 16:16:52.118 200 UTF8 mode enabled
    &gt; 2016-10-25 16:16:52.118 PBSZ 0
    &lt; 2016-10-25 16:16:52.165 200 PBSZ=0
    &gt; 2016-10-25 16:16:52.165 PROT P
    . 2016-10-25 16:16:52.205 Session upkeep
    &lt; 2016-10-25 16:16:52.213 200 Protection level set to P
    . 2016-10-25 16:16:52.213 Session upkeep
    . 2016-10-25 16:16:52.263 Connecté
    . 2016-10-25 16:16:52.263 Got reply 1 to the command 1
    . 2016-10-25 16:16:52.263 --------------------------------------------------------------------------
    . 2016-10-25 16:16:52.263 Using FTP protocol.
    . 2016-10-25 16:16:52.263 Doing startup conversation with host.
    &gt; 2016-10-25 16:16:52.321 PWD
    &lt; 2016-10-25 16:16:52.369 257 ""/"" is current directory.
    . 2016-10-25 16:16:52.369 Got reply 1 to the command 16
    . 2016-10-25 16:16:52.369 Changing directory to ""/"".
    &gt; 2016-10-25 16:16:52.369 CWD /
    &lt; 2016-10-25 16:16:52.417 250 CWD successful. ""/"" is current directory.
    . 2016-10-25 16:16:52.417 Got reply 1 to the command 16
    . 2016-10-25 16:16:52.417 Getting current directory name.
    &gt; 2016-10-25 16:16:52.418 PWD
    &lt; 2016-10-25 16:16:52.466 257 ""/"" is current directory.
    . 2016-10-25 16:16:52.466 Got reply 1 to the command 16
    . 2016-10-25 16:16:52.580 Listage du répertoire en cours...
    &gt; 2016-10-25 16:16:52.580 TYPE A
    &lt; 2016-10-25 16:16:52.628 200 Type set to A
    &gt; 2016-10-25 16:16:52.628 PASV
    &lt; 2016-10-25 16:16:52.678 227 Entering Passive Mode (2x,92,x,143,159,77)
    &gt; 2016-10-25 16:16:52.678 MLSD
    . 2016-10-25 16:16:52.678 Connexion à 2x.92.x.143:40781...
    . 2016-10-25 16:16:52.680 Session upkeep
    . 2016-10-25 16:16:53.231 Session upkeep
    . 2016-10-25 16:16:53.731 Session upkeep
    . 2016-10-25 16:16:54.231 Session upkeep
    . 2016-10-25 16:16:54.732 Session upkeep
    . 2016-10-25 16:16:55.232 Session upkeep
    . 2016-10-25 16:16:55.735 Session upkeep
    . 2016-10-25 16:16:56.285 Session upkeep
    . 2016-10-25 16:16:56.785 Session upkeep
    . 2016-10-25 16:16:56.785 Sending dummy command to keep session alive.
    &gt; 2016-10-25 16:16:56.785 PWD
    &lt; 2016-10-25 16:16:56.785 257 ""/"" is current directory.
    . 2016-10-25 16:16:57.286 Session upkeep
    . 2016-10-25 16:16:57.786 Session upkeep
    . 2016-10-25 16:16:58.287 Session upkeep
    . 2016-10-25 16:16:58.813 Session upkeep
    . 2016-10-25 16:16:59.315 Session upkeep
    . 2016-10-25 16:16:59.815 Session upkeep
    . 2016-10-25 16:17:00.315 Session upkeep
</code></pre>

<p>Not sure what to think about it.</p>

<p>Any tips or hints are more than welcomed as I've been struggling with this for two days already.</p>
","<ftp><winscp>","2016-10-25 15:09:12"
"954275","ZFS disk data recovery","<p>I’m new to zfs, so .. I have an HDD, entire disk is zfs. It was created on a FreeBSD machine with <code>zpool create</code> command, and is the only disk in the pool. Data was copied into that pool. I’ve then removed the disk and plugged it into another computer. I didn’t do the <code>zpool export</code>, or anything else, I’ve simply powered off the computer and took the disk out. I suppose what I did next was a mistake: I've used <code>zpool create -f</code>, and gave the pool another name. That created new, empty pool. Nothing was written into that pool. Typing <code>zpool import</code> returns name of the new pool, and nothing else. Is there a way to recover my old pool and the data?</p>
","<zfs>","2019-02-16 14:11:03"
"880222","How to use the Ansible best practices repository layout with VMs?","<p>I am (relatively) new to ansible and should set up an ansible repository to manage our contiuous integration infrastructure with multiple servers (version control, build, build-agents, ...).</p>

<p>After reading about ansible best practices and layout, I believe that this is ideal: </p>

<pre><code>production
staging
group_vars/...
site.yml
roles/{common,vm,buildserver,buildagent,versioncontrol,...}
</code></pre>

<p>The whole infrastructure should run in our vmware cluster, and it seems the vmware_guest module is ideal for most of the vm role.</p>

<p>So the build server should have the vm and buildserver roles. </p>

<p>However, the hostname is only meaningfully after the vm has been created. Also, most likely details like the (unfortunately static) IP etc. should be set as vars, but where? The vm role should use {{ jinja2_vars }} for reusability. </p>

<p>I think I am missing or misunderstanding something. </p>

<p>How do I properly set up the Ansible best pratices repository layout together with VMs and vm roles?</p>
","<virtual-machines><ansible><vmware-vcenter>","2017-10-25 13:33:22"
"811270","How to set up Nic.io root domain","<p>I have a domain registered at nic.io.
I've set up a custom domain on strikingly.com and set up the dns of nic.io to point to the strikingly dns.</p>

<p>The domain works with www prefix but i'd like to have the www point to the root domain. without www prefix the browser displays dns error.</p>

<p>Do you have an idea how to achieve dns settings on nic.io where www redirects to the root and  the root works in itself?</p>

<p>Attached are the current settings and the available options:</p>

<p><img src=""https://i.sstatic.net/enKXP.png"" alt=""""></p>

<p><img src=""https://i.sstatic.net/BG7R1.png"" alt=""""></p>
","<domain-name-system>","2016-10-25 20:58:08"
"954344","Need to know where the location of PHP-extensions and others","<p>I need to know where I can find the location of the list below :</p>
<h1>include_path            = /path/PHP-pear/</h1>
<h1>extension_dir           = /path/PHP-extensions/</h1>
<h1>mime_magic.magicfile     = /path/PHP-magic.mime</h1>
<p>I installed php using remi repo and I need to know how to disable and enable the modules too.</p>
<p>Thanks</p>
","<php><php-fpm>","2019-02-17 09:36:41"
"954660","(CRON) info (No MTA installed, discarding output)j though there is no cron job is scheduled","<p>In my /var/log/syslog file I can see the above log is entered in every minute. Though I have checked my crontab and found nothing configured there to run.</p>

<p>My /var/log/syslog is as follows:
<img src=""https://i.sstatic.net/tshqD.png"" alt=""enter image description here""></p>

<p>And the crontab is as follows:</p>

<p><img src=""https://i.sstatic.net/KUlIp.png"" alt=""enter image description here""></p>

<p>Here I should mention that my system is an AWS-EC2 instance which is running on Ubuntu 16.04</p>
","<amazon-ec2><cron><ubuntu-16.04><syslog><amazon-cloudwatch>","2019-02-19 10:33:58"
"954723","Upgrading from Server 2008r2 to 2012r2 with IIS service","<p>I am trying to upgrade Windows Server 2008r2 to 2012r2. Currently I am running IIS and I have a few questions about the upgrade process.</p>

<ol>
<li>Can I do an in-place upgrade directly from server 2008r2 to 2012r2?</li>
<li>If I successfully perform an in-place upgrade, will my setting for IIS be preserved?</li>
</ol>

<p>Thanks in advance for your help!</p>
","<windows-server-2008-r2><iis><windows-server-2012-r2><upgrade>","2019-02-19 16:35:54"
"811740","Routing a subnet to OpenVPN Client (tcpdump shows no icmp)","<p>I have an OpenVPN server with the network 172.24.24.0/29 (IP: 172.24.24.1).
A Raspberry (172.24.24.2) is connected to this OpenVPN server.</p>

<p>The Raspberry has access to my local network 192.168.2.0/24 and should allow the OpenVPN server access to this network via NAT.</p>

<p>Kernel forwarding is enabled on both.</p>

<ul>
<li>Ping from OpenVPN server to Raspberry works vice versa</li>
<li>Ping from Raspberry to local LAN works vice versa</li>
<li>Ping from OpenVPN server to local LAN does not work</li>
</ul>

<p><strong>OpenVPN server:</strong></p>

<pre><code>[root@openvpn ~]# ip route
172.24.24.0/29 dev tun1  proto kernel  scope link  src 172.24.24.1
192.168.2.0/24 via 172.24.24.2 dev tun1

[root@openvpn ~]# ip route get 192.168.2.101
192.168.2.101 via 172.24.24.2 dev tun1  src 172.24.24.1
</code></pre>

<p>Ping 192.168.2.101 from OpenVPN server.
On the Raspberry, however, no packets are displayed at the interface with <code>tcpdump</code>. But <code>tcpdump</code> displays the outgoing packets:</p>

<pre><code>[root@openvpn ~]# tcpdump -ni tun1 icmp
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on tun1, link-type RAW (Raw IP), capture size 65535 bytes
20:10:00.610156 IP 172.24.24.1 &gt; 192.168.2.101: ICMP echo request, id 3413, seq 1, length 64
20:10:01.616216 IP 172.24.24.1 &gt; 192.168.2.101: ICMP echo request, id 3413, seq 2, length 64
</code></pre>

<p><strong>Raspberry</strong></p>

<ul>
<li>VPN-IP: 172.24.24.2 (tun0)</li>
<li>Local LAN: 192.168.2.101 (eth0)</li>
</ul>

<p>iptables: (policy ACCEPT for all chains)</p>

<pre><code>-A FORWARD -m state --state RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -s 172.24.24.0/29 -j ACCEPT
-A FORWARD -s 192.168.2.0/24 -j ACCEPT
-A POSTROUTING -o eth0 -j MASQUERADE
</code></pre>

<p>Routing table</p>

<pre><code>default via 192.168.2.1 dev eth0
172.24.24.0/29 dev tun0  proto kernel  scope link  src 172.24.24.2
192.168.2.0/24 dev eth0  proto kernel  scope link  src 192.168.2.101
</code></pre>

<p>I initially thought that it is due to an iptables rule or to the NAT. That no packages arrive at the tun0 interface of the Raspberry I find very unusual and guess there the mistake.</p>

<p>Where could the mistake be in this setup? I really appreciate any help you can provide.</p>
","<centos><debian><iptables><routing>","2016-10-27 18:44:09"
"954930","How can I provide software for userdata?","<p>I've got a userdata file that uses the <code>aws</code> and <code>jq</code> commands, but since these aren't ubuntu 18.04 AMI. How can I make software available for userdata?</p>

<ol>
<li><p>I tried making a new AMI with those installed. It lets me use <code>aws</code> and <code>jq</code> when I log in, but the logs from userdata still say ""aws: command not found"" and ""jq: command not found"".</p></li>
<li><p>I checked to see if the logs were left over from the AMI creation, but no. An instance started without userdata doesn't have this log file at all.</p></li>
<li><p>I don't want to explicitly install them from apt in userdata itself in case the repositories change in a way that breaks the servers.</p></li>
</ol>

<p>I'm still pretty sure that AMIs are the way to go, (older custom AMIs other people have made seem to work fine with this userdata), but if the commands are available to bash, why aren't they available to userdata?</p>
","<amazon-web-services><amazon-ec2><bash><amazon-ami>","2019-02-20 16:31:16"
"880980","SVNServer moved to other server, cant relocate project","<p>I just moved my repositories to another server by copying the pasting the files in the appropriate folder. In the new server I can see the repositories loaded correctly. But when I go to my visual studio projects folders and right click TortoiseSVN -> relocate 
From url: <code>http://xx.xx.x.101:81/svn/Proj</code> to Url: <code>http://xx.xx.x.1:81/svn/Proj</code> but it throws time out. Any suggestions how to fix it?</p>

<p><strong>EDIT</strong></p>

<p>I opened the 81 port in firewall with no luck...</p>

<pre><code>  Proto  Local Address          Foreign Address        State


TCP    0.0.0.0:135            0.0.0.0:0              LISTENING
  RpcSs
 [svchost.exe]
  TCP    0.0.0.0:443            0.0.0.0:0              LISTENING
 [VisualSVNServer.exe]
  TCP    0.0.0.0:445            0.0.0.0:0              LISTENING
</code></pre>

<p>I changed my url to <code>http://xx.xx.x.1:445/svn/Proj</code> but now I got this error: error running context an existing connection was forcibly closed by the remote host</p>
","<windows><visualsvn-server><tortoisesvn>","2017-10-30 11:49:34"
"812023","Two physical servers, one domain","<p>I currently have an IIS server hosting my website (www.example.com).
I've just set up my raspberry pi and I've got this hosted as a website that can be accessed through the local IP which is Apache.</p>

<p>However, How do I make this public through my domain, but separate from my IIS server? So for example <code>rasp.example.com</code> allows me to go through my pi, but example.com goes through my IIS server?</p>

<p>I've got DNS set up through goDaddy btw.</p>
","<iis><web>","2016-10-29 10:37:38"
"881043","Windows 10 Files and Folders restored","<p>I am using Windows 10. My file and folder contents are restored as they were 6 months ago. What should I do?</p>

<p>For example, I have an excel file in which I logged attendance of team members. I had updated the Excel file through October 26, 2017. When I open that excel file, it is showing data until May 1, 2017.</p>

<p>The same thing happened with every Word, Excel, and notepad documents. New folders and data which were created after May 1, 2017 have been deleted.</p>

<p>What should I do to recover file data. Is it a bug or something else?</p>
","<windows><windows-10>","2017-10-30 17:03:27"
"812067","SAS Raid Array A with Spare. If I remove bad drive will the Spare pick up?","<p>I re-posted below. My apologies, I accidentally uploaded a very old pic by accident.. </p>

<p><a href=""https://i.sstatic.net/s0jhG.png"" rel=""nofollow noreferrer"">My server Array</a></p>
","<raid><raid10>","2016-10-29 16:15:43"
"881152","while creating a file with normal user it saying permission denyed..why?","<p>I am new to RedHat 7. I am learning user management and file permission.
how to give permissions to normal user. So that he can create a file..</p>

<pre><code>[syed1@localhost /]$ touch f1
touch: cannot touch ‘f1’: Permission denied
</code></pre>
","<redhat>","2017-10-31 10:14:41"
"881169","Forward packets to a transparent proxy server","<p>I'm running Debian 8 and iptables.</p>

<p>Currently I'm able to forward packets to my own server by changing the destination IP on packets with iptables:</p>

<pre><code>... DNAT --to-destination MY-IP --persistent
</code></pre>

<p>But is it possible to preserve the destination IP on the packets and simply forward packages as a sort of proxy?</p>
","<linux><debian><iptables><proxy>","2017-10-31 11:43:35"
"812190","www.example.com Works but example.com does not work when I type in browser","<p>I am not sure if this is an iis setting or if this is a domain hosting setting. but when I do something like example.org it will go to my site but if I just type in chrome example.org(<a href=""http://example.org/"" rel=""nofollow noreferrer"">http://example.org/</a>) it does not work.</p>

<p>I tried to add in namecheap another cname with ""*"" pointing to my DNS name of my vm.</p>

<p>I have in namecheap</p>

<pre><code>A Record: @ | ip address
Cname: * | websites721.cloudapp.net
Cname: www | websites721.cloudapp.net
</code></pre>
","<domain-name-system><iis><iis-8><namecheap>","2016-10-30 20:58:49"
"955358","openvpn: no access to server lan formed by a switch","<p>Is there any way to access to openvpn server lan in the following scenario?</p>

<p><strong>Server1 (vpn server)</strong></p>

<p>IP (eth1): 10.0.0.199/24 </p>

<p>Vpn IP (tun0): 10.8.0.1/24</p>

<p><strong>Server2</strong></p>

<p>IP (eth1): 10.0.0.198/24</p>

<p><strong>Client</strong></p>

<p>IP (eth1): 192.168.64.2/24</p>

<p>Vpn IP (tun0): 10.8.0.1/24</p>

<p>Only Server1 is publicly open to internet, and servers are connected through a local switch with 10.0.0.0/24 ip range. All machines have ubuntu 16.04 installed, ip-forwarding enabled and ufw disabled.</p>

<p>Client is connected to vpn server, but <strong>can't access</strong> to Server2 by pinging <strong>10.0.0.198</strong>. However, Server2 <strong>can access</strong> to client machine by pinging <strong>10.8.0.4</strong>.</p>

<p>Routes are configured as below:</p>

<h3>openvpn server.conf</h3>

<pre><code>route 192.168.64.0 255.255.255.0
push ""route 10.0.0.0 255.255.255.0""
push ""route 192.168.64.0 255.255.255.0""
</code></pre>

<h3>ccd/client</h3>

<pre><code>ifconfig-push 10.8.0.4/24 10.8.0.1
iroute 192.168.64.0 255.255.255.0
</code></pre>

<p>EDIT:
Server2 has also this route: <code>10.8.0.0/24 via 10.0.0.199</code></p>

<p>Note: I don't have access to routers/gateways of neither client nor server1. So, I am trying to find a way to access server2 without touching routers.</p>
","<networking><vpn><openvpn><linux-networking>","2019-02-22 22:24:35"
"881303","Need help, Communication issues between vlans for my hp 2920","<p>(vlan noob here! plz help)</p>

<p>I had a Voip install dumped on me at the last min, and I am having issues getting my vlans to communicate to each other.  We purchased a Hp 2920 switch which says it has layer 3 capabilities..</p>

<p>I have 2 Vlans, My defaut vlan 1 where all my client workstations are, and vlan 2 which is used for voice. </p>

<p>I have configured each vlan with their own ip's and such as shown in the config below; however, the only time can get my workstation from vlan 1 to ping vlan 2 is when I change the my ipv4 adapter default gateway settings to 10.0.1.210 which is the Ip address from my vlan 1.  My ability to ping vlan 2 is very limited because I can only ping the vlan 2 Ip address(10.0.2.210).  I can't ping any systems on the vlan 2 from vlan 1.  </p>

<p>Now, I am able to ping both vlans and clients to the vlans via the console of the hp 2920, so that is working I guess.</p>

<p>The reason I need workstations to talk to my vlan 2 if needed is because our phone system and remote phone we need to get setup requires this ability to talk to the phone system on Vlan2 from vlan1.</p>

<p>My config is as follows: Show Run</p>

<pre><code>hostname ""HP-2920-48G-POE+""
module 1 type j9729a
ip default-gateway 10.0.1.1 ---&gt; Sonicwall Firewall
ip dns server-address priority 1 10.0.1.252 ---&gt; Dhcp server and other services
ip routing
snmp-server community ""public"" unrestricted
oobm
   ip address dhcp-bootp
   exit
vlan 1
   name ""DEFAULT_VLAN""
   no untagged 46,48
   untagged 1-45,47,A1-A2,B1-B2
   ip address 10.0.1.210 255.255.255.0
   exit
vlan 2
   name ""Voice Vlan""
   untagged 46,48
   tagged 1-45,47
   ip address 10.0.2.210 255.255.255.0
   qos priority 6
   voice
   exit
</code></pre>

<p>I would like to thank you in advance for the help because I really need it.  I am drawing up blanks, and no matter how hard I look on the internet I can't seem to get it setup correctly.</p>

<p>Also Here is my ip routes: Show Ip Routes</p>

<pre><code>Destination        Gateway         VLAN Type      Sub-Type   Metric     Dist.
------------------ --------------- ---- --------- ---------- ---------- -----
10.0.1.0/24        DEFAULT_VLAN    1    connected            1          0
10.0.2.0/24        Voice Vlan      2    connected            1          0
127.0.0.0/8        reject               static               0          0
127.0.0.1/32       lo0                  connected            1          0
</code></pre>

<p>I think there is need for a route here, so the traffic can bridge the gap, but I dunno.</p>

<p>Thanks again :)</p>
","<vlan>","2017-11-01 03:35:42"
"812317","How do I configure Dual Domain SAS for my D2600 enclosure and Linux","<p>I'm investigating how one would go about setting up dual domain SAS with a D2600 and two identical HP cards for the following: P411, P421, P431, P441, P822 or the P841</p>

<p>I've tried searching but I can't seem to find any documentation that explains this.</p>

<p>Is it done through the hpacucli utility?</p>
","<hp><hp-proliant><hp-smart-array><sas>","2016-10-31 15:48:04"
"812330","OpenVPN isn't found in FreeBSD 11 -- why?","<p>I'm trying to install OpenVPN on Freebsd 11, but it says it's not found:</p>

<pre><code># pkg install openvpn
Updating FreeBSD repository catalogue...
FreeBSD repository is up-to-date.
All repositories are up-to-date.
pkg: No packages available to install matching 'openvp' have been found in the repositories
</code></pre>

<p>And </p>

<pre><code>uname -a
FreeBSD 11.0-RELEASE-p1 FreeBSD 11.0-RELEASE-p1
</code></pre>

<p>How can I install it?</p>
","<vpn><openvpn><freebsd>","2016-10-31 16:37:58"
"955627","Megaraid storage manager. How to add drive in RAID 0 with OS","<p>I have a LSI megaraid sas 9271-8i controller and 8 physical drive.
I have error on Slot 0. Media count error: 1568 and Pred fail error: 12.
And I have error such as:</p>

<p>How can I check a partition of physical disk in MegaRaid Storage Managment? I think it's a partition with OS.</p>

<p><a href=""https://i.sstatic.net/x1Mo8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/x1Mo8.png"" alt=""Drive group""></a></p>

<p>Can i add a drive in group to create RAID 1 to mirrored OS without crash, and what about boot sector.</p>
","<raid><hardware-raid><megaraid-storage-manager>","2019-02-25 12:41:14"
"812607","Ansible and vault encrypted templates","<p>I have a problem with Ansible Vault.</p>

<p>Vault on file copy commands works perfect, but I can´t find any solution to get encrypted templates to work.</p>

<p>My goal is to deploy an SSH key to <code>authorized_keys</code> with some comments on top, with <code>{{ ansible_managed }}</code> on top, but Ansible just creates the file encrypted on target host.</p>

<p>My task:</p>

<pre><code>- name: Copy public RSA key
  template: src=id_rsa.pub.j2 dest=/root/.ssh/authorized_keys owner=root mode=600
</code></pre>

<p>Result on target server:</p>

<pre><code>$ANSIBLE_VAULT;1.1;AES256
66393735343333616637383238643132646134343235633662663262353530663133386439356334
6437633863333434393333336336396239636531306262640a623764303165333035633333643631
6631613234346133386261343162653931643865633139[...]
</code></pre>

<p>Has anyone tried the same and got it up and running?</p>
","<encryption><ansible>","2016-11-01 21:59:16"
"955726","Increasing the number of connections MySQL 8 can handle (Debian 9 server) and auto restart event scheduler","<p>I have a Debian 9 server (12 CPUs, 80GB of RAM), running Percona server for MySQL 8.0, about 1100 clients per second, the load is not very high around 0.3 to 2.30</p>

<p>Latested numbers from show engine innodb status: 183.82 inserts/s, 169.69 updates/s, 5.79 deletes/s, 2179444.29 reads/s</p>

<p>It runs really nice most of the time, but lately time to time it suddenly refuses new connections (when a very busy table gets locked for a long time or some query takes too long).</p>

<p>The logs shows:</p>

<p><strong>[ERROR] [MY-000000] [connection_h] Error log throttle:       1912 'Can't create thread to handle new connection' error(s) suppressed</strong></p>

<p><strong>[ERROR] [MY-010249] [Server] Can't create thread to handle new connection(errno= 11)</strong></p>

<p><strong>[ERROR] [MY-010053] [Server] Event_scheduler::execute_top: Can not create event worker thread (errno=11). Stopping event scheduler</strong></p>

<hr>

<p>So, basically I have two questions:</p>

<p><strong><em>1. How to tweak the server to accept more connections (I tried a lot of things which I will list below), it cant handle more than 4821
   to be exact</em></strong></p>

<p><strong><em>2. Whenever that happens, the event scheduler stops and I have to manually enable it again, its really bad because I have some MEMORY
   tables that gets full, they handle the heavy traffic and dumps to
   other tables every 5 seconds</em></strong></p>

<hr>

<p>Until now I've tried setting the soft and hard limits in <strong>/etc/security/limits.conf</strong> for *, mysql and root users:</p>

<pre><code>*         hard    nofile      400000
*         soft    nofile      400000
*         hard    nproc       400000
*         soft    nproc       400000
mysql     hard    nofile      400000
mysql     soft    nofile      400000
root      hard    nofile      400000
root      soft    nofile      400000
</code></pre>

<p>I also have made many adjusments to <strong>/etc/sysctl.conf</strong> listing some below:</p>

<pre><code>kernel.pid_max = 262144
vm.max_map_count = 262144
net.ipv4.tcp_keepalive_time = 1200
net.ipv4.ip_local_port_range = 2000 65500
net.ipv4.tcp_max_syn_backlog = 32768
fs.file-max = 450000
net.core.netdev_max_backlog = 450000
net.core.somaxconn = 32768
</code></pre>

<p>I also made changes to systemd conf files: <strong>/lib/systemd/system/mysql.service</strong> - setting:</p>

<pre><code>LimitNOFILE=220000
TasksMax=32768
</code></pre>

<p><strong>mysqld.conf</strong></p>

<pre><code>bind-address                    = 0.0.0.0

# GENERAL #
user                           = mysql
default-storage-engine         = InnoDB
socket                         = /var/run/mysqld/mysqld.sock
pid-file                       = /var/lib/mysql/mysql.pid

# SAFETY #
max-allowed-packet             = 16M
max-connect-errors             = 1000000
skip-name-resolve
sysdate-is-now                 = 1
innodb                         = FORCE

wait-timeout                   = 600

# DATA STORAGE #
datadir                        = /var/lib/mysql/

# BINARY LOGGING #
sync-binlog                    = 0

# CACHES AND LIMITS #
tmp-table-size                 = 32M
max-heap-table-size            = 32M
max-connections                = 20000
thread-cache-size              = 300
open-files-limit               = 65535
table-definition-cache         = 4096
table-open-cache               = 4096

# INNODB #
innodb-flush-method            = O_DIRECT
innodb-log-files-in-group      = 2
innodb-log-file-size           = 512M
innodb-flush-log-at-trx-commit = 0
innodb-file-per-table          = 1
innodb-buffer-pool-size        = 64G
innodb-fast-shutdown           = 0
innodb-buffer-pool-dump-pct    = 75
innodb-buffer-pool-dump-at-shutdown = 1
innodb-buffer-pool-load-at-startup  = 1
innodb-io-capacity             = 400
innodb-io-capacity-max         = 2000

# LOGGING #
log-error                      = /var/log/mysql/mysql-error.log
log-queries-not-using-indexes  = 0
slow-query-log                 = 1
slow-query-log-file            = /var/log/mysql/mysql-slow.log
long-query-time                = 5

event_scheduler = 1
general_log_file               = /var/log/mysql/general.log
general_log                    = 0
local-infile                   = 1
</code></pre>

<p>I don't know what else I can do or where else I can look, tried to search for the problems and couldn't find anything that works, for the event scheduler its even worse, there's almost nothing to be found about that (try to search in google <strong>MY-010053</strong> or <strong>Can not create event worker thread</strong> almost no results)</p>

<p>The server does not get unresponsive or slow, if I lock the tables on purpose, I start getting the error in about 10 seconds, and when I unlock it recover immediatly</p>

<p>As Wilson Hauck requested:</p>

<p><strong>ulimit -a</strong></p>

<pre><code>core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 326193
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 400000
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 326193
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
</code></pre>

<p><strong>iostat -xm 5 3</strong></p>

<pre><code>Linux 4.9.0-8-amd64 (zelda)     03/07/2019      _x86_64_        (12 CPU)

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          11.24    0.00    1.56    0.76    0.00   86.44

Device:         rrqm/s   wrqm/s     r/s     w/s    rMB/s    wMB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.41    44.17    2.41   67.61     0.05     1.61    48.55     0.37    5.23    4.74    5.25   1.37   9.59

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           9.77    0.00    1.91    0.82    0.00   87.51

Device:         rrqm/s   wrqm/s     r/s     w/s    rMB/s    wMB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00    52.20    0.00   81.60     0.00     1.85    46.51     0.45    5.51    0.00    5.51   1.14   9.28

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           5.83    0.00    1.41    0.77    0.00   91.99

Device:         rrqm/s   wrqm/s     r/s     w/s    rMB/s    wMB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00   132.00    0.00  161.40     0.00     8.12   103.00     0.88    5.47    0.00    5.47   0.66  10.72
</code></pre>

<p><strong>show full processlist</strong> there's nothing special:</p>

<pre><code>4087783 event_scheduler localhost       Daemon  1   Waiting for next activation     0   0
275050068   xgh 179.191.66.174:51143    xgh Sleep   3           0   0
275050130   xgh 179.191.66.174:51144    xgh Query   0   starting    show full processlist   0   0
300324788   xgh 179.191.66.174:40708    xgh Sleep   12595           5   61
304505269   xgh 179.191.66.174:51680    xgh Sleep   72          0   0
304505986   xgh 179.191.66.174:51706    xgh Sleep   72          0   0
305818676   xgh 172.30.5.2:57288    xgh Query   0   Sending data    SELECT *
 FROM (`noticias`.`noticia`)
 WHERE `texto` =  'Carlos Ghosn deixa prisão após mais de 100 dias detido em Tóquio'    0   0
305818680   xgh 172.30.5.2:57296    xgh Sleep   57          0   0
305818682   xgh 172.30.5.2:57302    xgh Sleep   57          0   0
305818689   xgh 172.30.5.2:57316    xgh Sleep   57          0   0
305818692   xgh 172.30.5.2:57324    xgh Sleep   57          0   0
305842475   xgh 172.30.5.2:49326    xgh Sleep   1           94  550698
305842479   xgh 172.30.5.2:49334    xgh Sleep   1           0   0
305842481   xgh 172.30.5.2:49340    xgh Sleep   1           0   0
305842486   xgh 172.30.5.2:49350    xgh Sleep   1           0   0
305842489   xgh 172.30.5.2:49358    xgh Sleep   1           0   0
305842492   xgh 172.30.5.2:49364    xgh Sleep   1           0   0
305842496   xgh 172.30.5.2:49372    xgh Sleep   1           0   0
305842498   xgh 172.30.5.2:49376    xgh Sleep   1           0   0
305842501   xgh 172.30.5.2:49382    xgh Sleep   1           0   0
305842505   xgh 172.30.5.2:49392    xgh Sleep   1           0   0
305842508   xgh 172.30.5.2:49398    xgh Sleep   1           0   0
</code></pre>

<p>SHOW GLOBAL STATUS: <a href=""https://pastebin.com/cg1v5bHD"" rel=""nofollow noreferrer"">https://pastebin.com/cg1v5bHD</a></p>

<p>SHOW GLOBAL VARIABLES: <a href=""https://pastebin.com/8mkTzpr0"" rel=""nofollow noreferrer"">https://pastebin.com/8mkTzpr0</a></p>
","<mysql><debian><percona><debian-stretch>","2019-02-26 01:31:17"
"812643","Transfer folder to AWS Ubuntu server. Permission denied (publickey)","<p>I installed shiny server on an AWS EC2 Ubuntu instance following instruction here: <a href=""https://www.rstudio.com/products/shiny/download-server/"" rel=""nofollow noreferrer"">https://www.rstudio.com/products/shiny/download-server/</a></p>

<p>I tried to transfer a folder to a remote directory: /srv/shiny-server 
by typing in terminal:</p>

<pre><code>scp -r path/Publickey path/folder ubuntu@ec2-52-15-128-161.us-east-2.compute.amazonaws.com:/srv/shiny-server
</code></pre>

<p>Then I got:</p>

<pre><code>Permission denied (publickey).
lost connection
</code></pre>

<p>Any help will be appreciated!</p>
","<linux><ubuntu><amazon-web-services><scp>","2016-11-02 02:11:46"
"812657","Shell Scripting in Linux","<p>How can I move few files that contain spaces to another directory using a bash script?</p>

<p>file name: ""Sublime Text 3.x""</p>

<p><strong>my code is :</strong> </p>

<pre><code>for file in $(ls -t | tail -n +1)
do 
mv $file /tmp/test
done
</code></pre>

<p><strong>Output shows as:</strong></p>

<pre><code>mv: cannot stat ‘Sublime’: No such file or directory
mv: cannot stat ‘Text’: No such file or directory
mv: cannot stat ‘3.x’: No such file or directory
</code></pre>
","<ubuntu><bash><shell-scripting><terminal>","2016-11-02 04:46:20"
"812699","Cloud server running slow","<p>Can anyone help with this:</p>

<p>One of our cloud servers is running slow and after running <code>iotop</code> I've noticed the below command running for nearly 2 hours, but I'm the only one who would run commands on this server. I haven't and there has been no breach.</p>

<pre><code>find / -ignore_readdir_race ( -fstype NFS -o -fstype nfs -o -fstype nfs4 -o -fstype afs ~var/spool$\)\|\(^/sfs$\)\|\(^/media$\)\|\(^/var/lib/schroot/mount$\) ) -prune -o -print0
</code></pre>

<p>Does anyone know what this is?</p>
","<linux><ubuntu><io><find>","2016-11-02 10:14:50"
"955808","Docker loses network connectivity after starting","<p>I have a weird issue in Linux, running under SLES12 over VMWare. After I start docker by issuing service docker start I lose network connectivity to the Linux machine from putty... In order to get back to it I have to ssh to it from another Linux machine. So it seems like its moving my interfaces out of order</p>

<p>Before starting docker</p>

<pre><code>$ ifconfig
eth0      Link encap:Ethernet  HWaddr 00:50:56:B6:33:30
          inet addr:10.100.0.101  Bcast:10.100.0.255  Mask:255.255.255.0
          inet6 addr: fe80::250:56ff:feb6:3330/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:163546 errors:0 dropped:10 overruns:0 frame:0
          TX packets:70909 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:18524059 (17.6 Mb)  TX bytes:15505464 (14.7 Mb)

lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:16094 errors:0 dropped:0 overruns:0 frame:0
          TX packets:16094 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:1425313 (1.3 Mb)  TX bytes:1425313 (1.3 Mb)
</code></pre>

<p>and <code>ifconfig</code> after</p>

<pre><code>$ ifconfig
docker0   Link encap:Ethernet  HWaddr 02:42:D6:7A:25:1C
          inet addr:172.17.0.1  Bcast:172.17.255.255  Mask:255.255.0.0
          inet6 addr: fe80::42:d6ff:fe7a:251c/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:318 errors:0 dropped:0 overruns:0 frame:0
          TX packets:23 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:26040 (25.4 Kb)  TX bytes:1394 (1.3 Kb)

docker_gw Link encap:Ethernet  HWaddr 02:42:01:39:74:46
          inet addr:172.18.0.1  Bcast:172.18.255.255  Mask:255.255.0.0
          inet6 addr: fe80::42:1ff:fe39:7446/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:115 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:0 (0.0 b)  TX bytes:5206 (5.0 Kb)

eth0      Link encap:Ethernet  HWaddr 00:50:56:B6:33:30
          inet addr:10.100.0.101  Bcast:10.100.0.255  Mask:255.255.255.0
          inet6 addr: fe80::250:56ff:feb6:3330/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:164222 errors:0 dropped:10 overruns:0 frame:0
          TX packets:71533 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:18582001 (17.7 Mb)  TX bytes:15580880 (14.8 Mb)
</code></pre>
","<linux><docker>","2019-02-26 14:12:24"
"955921","KVM guest cant fetch packages thus it's reachable from ssh","<p>I am a bit lost regarding my kvm setup.</p>

<p>I rented a root server at <a href=""https://hetzner.de"" rel=""nofollow noreferrer"">hetzner.de</a> and now wanted to setup virtualization using libvirt &amp; kvm.
I created a bridge <code>br0</code> which now holds the static server ip (instead of the default interface <code>enp2s0</code>). Linking a /29 subnet to the bridge and assigning a static ip to the guest went fine. Now I can reach it from the outside using the static ip, but can't fetch packages nor ping nameservers. 
So pinging <code>8.8.8.8</code> works fine, whilst pinging <code>google.com</code> answers with: </p>

<pre><code>Temporary failure in name resolution
</code></pre>

<hr>

<p>Here are the routes of the host(here and in the following text i replaced the first octave with 255)</p>

<pre><code>Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         255.251.246.1   0.0.0.0         UG    0      0        0 br0
255.9.179.64      0.0.0.0         255.255.255.255 UH    0      0        0 br0
255.9.179.65      0.0.0.0         255.255.255.255 UH    0      0        0 br0
255.9.179.66      0.0.0.0         255.255.255.255 UH    0      0        0 br0
255.9.179.67      0.0.0.0         255.255.255.255 UH    0      0        0 br0
255.9.179.68      0.0.0.0         255.255.255.255 UH    0      0        0 br0
255.9.179.69      0.0.0.0         255.255.255.255 UH    0      0        0 br0
255.9.179.70      0.0.0.0         255.255.255.255 UH    0      0        0 br0
255.251.246.0   0.0.0.0         255.255.255.224 U     0      0        0 br0
192.168.122.0   0.0.0.0         255.255.255.0   U     0      0        0 bro0
</code></pre>

<hr>

<p>Result of <code>brctl show</code>:</p>

<pre><code>bridge name     bridge id               STP enabled     interfaces
br0             8000.448a5b5dd0f1       no              enp2s0
                                                        vnet0
bro0            8000.52540006b883       yes             bro0-nic
</code></pre>

<hr>

<p><code>ifconfig</code> on host:</p>

<pre><code>br0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
        inet 255.251.246.11  netmask 255.255.255.224  broadcast 255.251.246.31
        inet6 255::468a:5bff:fe5d:d0f1  prefixlen 64  scopeid 0x20&lt;link&gt;
        ether 44:8a:5b:5d:d0:f1  txqueuelen 1000  (Ethernet)
        RX packets 51679  bytes 3634682 (3.4 MiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 90348  bytes 7395186 (7.0 MiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

bro0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt;  mtu 1500
        inet 192.168.122.1  netmask 255.255.255.0  broadcast 192.168.122.255
        ether 52:54:00:06:b8:83  txqueuelen 1000  (Ethernet)
        RX packets 0  bytes 0 (0.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 0  bytes 0 (0.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

enp2s0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
        ether 44:8a:5b:5d:d0:f1  txqueuelen 1000  (Ethernet)
        RX packets 104449  bytes 7981272 (7.6 MiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 89828  bytes 8144371 (7.7 MiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt;  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
        inet6 ::1  prefixlen 128  scopeid 0x10&lt;host&gt;
        loop  txqueuelen 1  (Local Loopback)
        RX packets 2  bytes 190 (190.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 2  bytes 190 (190.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

vnet0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
        inet6 255::fc54:ff:fe94:8198  prefixlen 64  scopeid 0x20&lt;link&gt;
        ether fe:54:00:94:81:98  txqueuelen 1000  (Ethernet)
        RX packets 1172  bytes 110476 (107.8 KiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 5399  bytes 321838 (314.2 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
</code></pre>

<p>and <code>ifconfig</code> of the guest</p>

<pre><code>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: ens2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:94:81:98 brd ff:ff:ff:ff:ff:ff
    inet 255.9.179.65 peer 255.251.246.11/32 brd 255.9.179.65 scope global ens2
       valid_lft forever preferred_lft forever
    inet6 255::5054:ff:fe94:8198/64 scope link
       valid_lft forever preferred_lft forever
</code></pre>

<hr>

<p>At the moment I haven't setup any type of firewall. I wanted to test out the connections etc. without getting interfered by it. Now I am thinking that this might be the problem. Could be a problem with masquerading..</p>

<p>Help is appreciated!</p>

<p>Cheers,
Tobi</p>
","<networking><kvm-virtualization><bridge><libvirt><hetzner>","2019-02-27 08:45:35"
"956015","My Nginx server is not working","<p>I have an Apache server on Ubunu 18.04 with a Drupal 8 site that I want to migrate to Nginx and PHP7.3-FPM.</p>

<p>This is the only site on the server. I do not know Nginx at all. The site works with Apache but it does not work with Nginx.</p>

<p>I deleted Apache with the following command :</p>

<pre><code>sudo apt autoremove --purge apache2*
</code></pre>

<p>Here is the configuration I had on Apache :</p>

<pre><code>&lt;VirtualHost *:80&gt;
   ServerAdmin contact@domaine.com
   ServerName domaine.com
   ServerAlias www.domaine.com
   Protocols h2 http/1.1
   DocumentRoot /var/www/www-domaine-com/web/

   &lt;Directory /var/www/www-domaine-com/web&gt;
      Options +Includes -Indexes +FollowSymLinks
      AllowOverride All
      Require all granted
   &lt;/Directory&gt;

   &lt;FilesMatch \.php$&gt;
      SetHandler ""proxy:unix:/var/run/php/php7.3-fpm.sock|fcgi://localhost/""
   &lt;/FilesMatch&gt;

   ErrorLog ${APACHE_LOG_DIR}/error.log
   CustomLog ${APACHE_LOG_DIR}/access.log combined
&lt;/VirtualHost&gt;
</code></pre>

<p>Here's how I installed Nginx :</p>

<pre><code>sudo apt install nginx
sudo ufw allow in ""Nginx HTTP""
</code></pre>

<p>I am testing the IP address of my server and the Nginx page is displayed.</p>

<pre><code>sudo unlink /etc/nginx/sites-enabled/default
</code></pre>

<p>Here is my Nginx configuration :</p>

<pre><code>sudo nano /etc/nginx/sites-available/www-domaine-com

server {
    listen 80;
    listen [::]:80;
    server_name domaine.com www.domaine.com;

    root   /var/www/www-domaine-com/web;
    index  index.html index.php;

    location / {
        try_files $uri $uri/ =404;
    }
    location ~ \.php$ {
            include snippets/fastcgi-php.conf;
            include fastcgi_params;
            fastcgi_pass unix:/run/php/php7.3-fpm.sock;
            fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
    }
}
</code></pre>

<p>I create a symbolic link of my new configuration :</p>

<pre><code>sudo ln -s /etc/nginx/sites-available/www.domaine.com /etc/nginx/sites-enabled/
</code></pre>

<p>I test my configuration :</p>

<pre><code>sudo nginx -t
</code></pre>

<p>This message is displayed :</p>

<pre><code>nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
nginx: configuration file /etc/nginx/nginx.conf test is successful
</code></pre>
","<ubuntu><nginx><virtualhost><drupal>","2019-02-27 17:54:08"
"881961","Is there any way to ban ALL proxy servers from accessing a website using some Apache configuration?","<p>I am using Apache2 with Ubuntu 16.04 and PHP7.0. I tried using .htaccess setup but that does not seem to work i.e. did not stop anonymous proxies from visiting the website. </p>

<p>I was trying this solution <a href=""http://www.phpgenious.com/2009/03/block-proxy-servers-using-htaccess/"" rel=""nofollow noreferrer"">http://www.phpgenious.com/2009/03/block-proxy-servers-using-htaccess/</a></p>

<pre><code>RewriteEngine on
RewriteCond %{HTTP:VIA} !^$ [OR]
RewriteCond %{HTTP:FORWARDED} !^$ [OR]
RewriteCond %{HTTP:USERAGENT_VIA} !^$ [OR]
RewriteCond %{HTTP:X_FORWARDED_FOR} !^$ [OR]
RewriteCond %{HTTP:PROXY_CONNECTION} !^$ [OR]
RewriteCond %{HTTP:XPROXY_CONNECTION} !^$ [OR]
RewriteCond %{HTTP:HTTP_PC_REMOTE_ADDR} !^$ [OR]
RewriteCond %{HTTP:HTTP_CLIENT_IP} !^$
RewriteRule ^(.*)$ – [F]
</code></pre>

<p>Another option is using  BlockScript <a href=""https://www.blocked.com/"" rel=""nofollow noreferrer"">https://www.blocked.com/</a> which seems to be considerably expensive. </p>
","<ubuntu><security><apache2>","2017-11-05 17:45:18"
"812946","How to configure two separate networks with one server two Nics?","<p>I have one server (win server 2012 R2) that have two NICs with two separate networks, my problem is I can't configure because network (B) has its internet connection from an modem  but network (A) not have internet connection the following image is example How can I configure the Network (B) using internet and access to share folders on server, also network (A) access to share folders on the same sever.</p>

<p><img src=""https://i.sstatic.net/iU5kM.gif"" alt=""Example of Network""></p>
","<windows-server-2012-r2><network-share>","2016-11-03 11:56:05"
"882023","How come I can still receive data when all my ports are closed?","<p>I launched an AWS instance with no inbound ports open, except 22.</p>

<p>How come I can still execute commands like these and receive responses:</p>

<p><code>curl google.com</code></p>

<p><code>apt install apache</code></p>

<p><code>ping google.com</code></p>

<p>How is the data reaching my server?</p>
","<networking><port>","2017-11-06 10:39:30"
"813094","How can I redirect HTTP traffic to an absolute link path with iptables?","<p>I'm trying to redirect a host to a specific link on my network when they try to browse through HTTP.</p>

<p>The IP address of the host I'm trying to redirect is <code>192.168.2.19</code> and my web-server is on <code>192.168.2.15:6969/test.js</code>. When I redirect the user I can only get him to <code>192.168.2.15:6969</code> and not the relative path. I tried specifying in the <code>iptables</code> command but it did not work.</p>

<p>Here's what I did:</p>

<pre><code>echo 1 &gt; /proc/sys/net/ipv4/ip_forward 
iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-port 3000
iptables -t nat -A POSTROUTING -j MASQUERADE 
iptables -t nat -A PREROUTING -s 192.168.2.19 -p tcp --dport 80 -j DNAT 
--to-destination 192.168.2.15:3000/test.js
</code></pre>
","<iptables>","2016-11-03 22:50:10"
"956264","How to configure TLS certificate for an IP address?","<p>We are hosting our backend server in ALIYUN, since we are a foreign company we don't have ICP license and domain name for that server, but we want to configure https for load balancer IP address.
I tried to buy SSL certificate from ALIYUN, since we don't have domain name, how should I select type of domain(wildcard domain, single domain, multiple domain)?
Any suggestion for this?</p>
","<ssl-certificate><https><ip-address><alibaba-cloud>","2019-03-01 03:42:02"
"882239","Exchange combined with another provider using imap","<p>Given a mail infrastructure on a shared hosting environment using ~10 main email inboxes and ~30 email inboxes for other purposes, all via IMAP.</p>

<p>We now want to migrate to Microsoft Exchange (using Exchange Online). We have to pay per inbox. This is possible to the 10 main email inboxes but too expensive for the 30 email inboxes for other purposes.</p>

<p>We cannot use aliases or shared inboxes because we have to write from those 30 email inboxes (set the correct sender).</p>

<p>I don't see any alternative here but here is my question:</p>

<p><strong>Is it possible to combine protocols / email infrastructures? That means: Can I use Exchange Online only for some email adresses and another provider for all other email adresses?</strong></p>

<p>This is not possible, because the MX record cannot be ""split up"", correct?</p>
","<exchange><imap>","2017-11-07 10:54:48"
"956282","Clients / Groups / Users not shown in group policy management console (GPMC)","<p>I have installed the <code>ActiveDirectory</code> app on my <code>Synology DS918+</code>. Already successfully added users and insert two PCs into the domain.</p>

<p>If I open the <code>GPMC</code> on my Windows 10 (v1809) machine, I'm missing the ""Clients"", ""Groups"", and ""Users"" tab <em>(I started the <code>GPMC</code> as domain administrator)</em>.</p>

<p>How can I make these items available?</p>

<p><strong>Synology Users and groups</strong></p>

<p><a href=""https://i.sstatic.net/q9j8d.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/q9j8d.png"" alt=""enter image description here""></a></p>

<p><strong>Active Directory Users and Computers</strong></p>

<p><a href=""https://i.sstatic.net/0mvU1.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0mvU1.png"" alt=""enter image description here""></a></p>

<p><strong>Group Policy Management Console</strong></p>

<p><a href=""https://i.sstatic.net/wpeSh.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wpeSh.png"" alt=""enter image description here""></a></p>

<p>This is what I expected (just a sample image; on this image users, groups and clients are visible):</p>

<p><a href=""https://i.sstatic.net/9YAU3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9YAU3.png"" alt=""enter image description here""></a></p>
","<active-directory><group-policy><ldap><synology>","2019-03-01 07:53:36"
"882421","504 Gateway Time-out nginx, on apache server","<p>I am using curl to scrape data from other website, i am running on apache server, then why i am getting this error</p>
<h2>504 Gateway Time-out</h2>
<h3>nginx</h3>
<p>I mean what does nginx mean here?</p>
","<apache-2.2><nginx>","2017-11-08 09:27:19"
"956496","For Loop in cmd and powershell - 1 is unexpected at this time","<p>I need to run a <code>for</code> loop, from powershell, in cmd... aka, at the PS Prompt:</p>

<pre><code>cmd /c For /L %i in (1,1,5) DO (Echo %i)
</code></pre>

<p>But, I get a ""1 is unexpected at this time""</p>

<p>Other responses in my research indicate ""<code>%i</code>"" is command line, and ""<code>%%i</code>"" is script/batch -- but I've tried both and neither worked.  Any ideas?  Is it not possible?  </p>
","<powershell><scripting><batch>","2019-03-03 05:15:33"
"813829","I can't provide internet using DHCP server","<p>I have the follwing Network, the DHCP + Internet are provided by the router ""TP-Link N600"" and I want to add DHCP rule on our server instead of the router, so how can I provide internet to the AP's and laptops.</p>

<p>Is it using gateway?</p>

<p><img src=""https://i.sstatic.net/WUZNe.png"" alt=""Network Scheme""></p>
","<windows-server-2008><internet><dhcp-server>","2016-11-08 16:27:05"
"956549","Replacing RAID 1 with larger drives - professional standards","<p>I work with one of the biggest hosting companies on the planet. I got a dedicated server with hardware RAID 1 and two SAS 15k disks. I asked my hosting company if they could replace SAS with much bigger SATA drives as I need space on this machine, I do not need speed. They said that they do not do SATA only SAS and SSD and that they do not do replacements with bigger drives as this is not safe.</p>

<p>Is replacing RAID 1 drives with bigger drives to create bigger volumes (using MegaCli -LdExpansion command) something that would be considered wrong on professional environments? Is that a common\standard practice to not do that? Is is really that risky to do so?</p>

<p>Why would they not do SATA drives. Is it considered wrong on professional environments? Are there any reasons to not use SATA at all? (controller allows it; the manual says 'Each port on the SAS RAID controller supports SAS devices, SATA II devices, or both')</p>

<p>EDIT:</p>

<p>The controller is LSI MegaRAID 1078</p>

<p>Just to clarify my question is: Is it considered unprofessional to use SATA or replace RAID 1 drives with bigger drives on dedicated servers?</p>
","<raid><hardware-raid><raid1><dedicated-server>","2019-03-03 18:21:04"
"814023","Access MAC addresses (IP to MAC - ARP) between two subnets. Seperated via router","<p>I've come across an issue, and after spending days investigating I've realised I need some advice. My networking skill level is still quite low. </p>

<p>I've got two routers, with two LANs. I've connected one of the routers to the other via a LAN &lt;-> WAN connection, creating a separate network to contain controller and devices. The main reason for two routers is so that I can have a separate DCHP server for <strong>Router B's</strong> connections. See below for the image:</p>

<p><img src=""https://i.sstatic.net/nofge.png"" alt=""Network""></p>

<p>I have access to the controller via <strong>Router A's</strong> network, I can get to the web based GUI to see all my devices on the network via setting the ports/virtual servers on <strong>Router B.</strong> However, I have code on the controller that needs to monitor devices on <strong>Router As</strong> LAN. The code needs access to the subnet so that it can resolve IPs to MAC addresses (ARP requests). Currently I can ping from <strong>Router B's</strong> devices to <strong>Router A's</strong> devices, but can't resolve IP to MAC addresses (I'm using NMAP). I believe this is because ARP requests only work in the same subnet. </p>

<p>Is it possible to add a <strong>second static IP</strong> in the same subnet as Router A's LAN (outside of DHCP range of A) <strong>to the controller</strong> on <strong>Router B's</strong> network, and somehow route this through <strong>Router B</strong> so that the requests are resolved and sent back? </p>

<p>If not, is there an alternative?</p>

<p><strong>Additional Information:</strong></p>

<p><strong>Controller :</strong> Linux - Raspbian</p>

<p><strong>Router A:</strong> Belkin Modem-Router</p>

<p><strong>Router B:</strong> Edimax Router</p>
","<linux><router><subnet><arp><raspbian>","2016-11-09 16:06:36"
"956794","isc-dhcp host using files","<p>I configured ise-dhcp-server, which works fine, but my dhcpd.conf is starting to become very big. I'm trying to find information (without results), on whether it's possible to use something like <code>import /path/to/file</code>. I want to use it to set static host in this separate file.</p>

<p>Everywhere people write only to use it to path to pxe/grub image.</p>
","<hosts><isc-dhcp><hosts-file>","2019-03-05 08:02:01"
"814331","merge space on linux - from /data to / (root)","<p>when i check space on centos server i see </p>

<pre><code># df -m

Filesystem     1M-blocks  Used Available Use% Mounted on
/dev/md1           58883 54328      1558  98% /
tmpfs              15946     0     15946   0% /dev/shm
/dev/md0             283    40       229  15% /boot
/dev/md2          659113    70    625556   1% /data
</code></pre>

<p>so ho can i delete /data (/dev/md2) and add all that space to /dev/md1 (/)</p>
","<linux><ubuntu><centos><storage><partition>","2016-11-11 00:42:50"
"814502","raw sockets, iptables and the fact that","<pre><code>iptables -P INPUT DROP
iptables -P FORWARD DROP
iptables -P OUTPUT DROP
</code></pre>
<p>So, I just <a href=""https://diablohorn.com/2013/11/28/qp-raw-sockets-iptables/"" rel=""nofollow noreferrer"">found out</a> that these cute rules don't do s**t when it comes to raw sockets.</p>
<p>I know that grabbing a raw socket requires root permission so I don't care about the OUTPUT chain not getting filtered (I'm sure I don't have any software with backdoor running as SU).</p>
<p>but what about the INPUT chain? isn't not being able to control incoming packets dangerous? if no, then why everyone bother setting the default policy to drop?</p>
","<iptables><linux-networking><network-protocols>","2016-11-12 00:45:14"
"814512","Azure-hosted VPN for Remote dev teams","<p>I need a VPN solution that is based on Azure.
We're a small tech development team, with 3 offices, and engineers often working remote.</p>

<p>I'm trying to setup a VPN to further secure my resources on Azure. We also have a development machine (Mac) sitting in one of our offices that need to be made available to remote dev teams over a LAN (i.e. want to be able to SSH/RDP in). Ideally when this Mac boots up, it should automatically VPN in, maybe using a certificate or something.</p>

<p>Here are my requirements:</p>

<ol>
<li>VPN Client should work on both Windows and Mac</li>
<li>Users can either login using a Azure Active Directory account OR a certificate</li>
<li>Would like to minimize the running of any servers (down to 0 if possible). Prefer to use PaaS over IaaS.</li>
</ol>

<p>I've done this kind of setup on OpenVPN before -- I can always do that, but I don't want to manage VMs. Trying to figure out if this is possible on Azure.</p>

<p>Seems like I can do Point-to-Site connectivity, but there's no clear information on whether a Mac client is supported (seems officially, no), but has anyone reverse engineered the client to make it work on the Mac?</p>

<p>UPDATE:
Ok, maybe the question got down voted because what I'm asking for is unclear. My question is this:</p>

<p>Can anyone share details if they have a working setup of Azure VPN, with Point-to-Site client connections from both Windows AND Mac? If not, some other equivalent solution that doesn't involve having to spin up a full-blown linux/windows machine?</p>
","<vpn><azure>","2016-11-12 03:38:09"
"814522","How to apply Domain GPO to local Work group Server","<p>I need to export all the settings in Domain GPO and need to import in local work group Server . 
I have seen many sites but there were only local settings import, but I need to import all setting including administrative template, windows setting, software setting. </p>

<p>Thanks in advance. </p>
","<windows><active-directory><group-policy><workgroup>","2016-11-12 07:30:57"
"957235","VM image vs Phyical server image?","<p>I often hear its easy  to create the image of one VM and setup it another VM(even instance types are of different types) than creating the image on physical server. In fact one of the devops guy in the past told me image from physical server can not be created . Is that true ?</p>

<p>My understanding is that image will be created from disk in both cases either in case of VM or physical server. Is n't it ? Then why image of physical server can't be created or difficult to create  than creating the image from VM?</p>
","<virtual-machines><disk-image><physical-environment>","2019-03-07 18:05:35"
"814591","Limit intranet to certain computers","<p>I've created a intranet website which I would like to limit to only certain computers spread throughout the city, my first thought was creating a vpn server on the web server and authorize the client computers with certificate and then limit the site only to lan. I used SoftEther VPN to setup the server, but a problem I immediately encountered was the slow bandwidth. Now to my actual question is it possible to enable the clients to browse the internet through their own connection and at the same time be able to access the website on the vpn?</p>

<p><strong>Edit:</strong> I'm also open for other solutions that may be less introsive.</p>
","<vpn><intranet>","2016-11-12 21:12:34"
"814610","Alternative ways of sending full contents of incoming email to external script in Exim?","<p>For some strange reason, using the <code>pipe</code> command does not work as expected for me in Exim, and neither Google <a href=""https://serverfault.com/questions/813509/how-to-execute-an-external-script-via-pipe-command-from-an-exim-system-filter"">nor ServerFault</a> has been able to provide a solution for this problem so far. :-(</p>

<p>I'm therefore switching over to searching for an alternative solution (a.k.a. workaround) instead, more specifically not explicitly using the <code>pipe</code> command at all, which makes me seek the answer to the following new question instead:</p>

<p><strong>How can I, using any available config method in Exim, make sure that incoming emails (including their complete contents such as headers, body etc) are sent to an external script on the same server (in my case, a Python script), in a similar fashion to what the <code>pipe</code> command is supposed to do, but not using the <code>pipe</code> command explicitly as I do in <a href=""https://serverfault.com/questions/813509/how-to-execute-an-external-script-via-pipe-command-from-an-exim-system-filter"">my other question</a>?</strong></p>

<p>My own primary suggestion so far is that I could configure a dedicated transport for this, similar to <a href=""https://serverfault.com/questions/574909/how-to-log-full-output-of-exim-pipe-command"">this one</a>, and then just make it so that all incoming emails are routed through this transport? My problem is just that I currently don't know how to accomplish the ""<em>make it so that all incoming emails are routed through this transport</em>"" part, so any solutions to this simple sub question might actually also be a valid reply to this entire question(!).</p>

<p>Also, <a href=""https://serverfault.com/questions/299349/exim-how-to-deliver-locally-and-send-a-copy-to-another-server"">this other question</a> is seemingly using something called a ""smarthost"" to redirect (copies of) incoming email to a certain transport, but I'm still lacking both a sufficient understanding and the complete config directives to make practical use of it, but a possible answer might use this too perhaps?</p>

<p>(and please include real Exim config file input in your reply, and assume I have no previous knowledge of neither any particular Exim nor SMTP terminology)</p>
","<exim>","2016-11-13 01:29:43"
"814643","System CPU use on freeBSD","<p>I have a large number of small jobs that I need to run.
If I run them on a 6core Xeon, Broadwell, it runs with at least 80-90% userland CPU</p>

<p>If I run the same assignment on a box with 2X16 core CPU Broadwell, if I scale the number of jobs, I end up with 80% system CPU, use, and the theoughput is only about a factor of 3 vs the single 6 core CPU, despite having 5x the cores and clocked faster.</p>

<p>Any suggestions to improve on this?</p>

<p><strong>EDIT</strong></p>

<p>The problem seems to become especially bad if the jobs are below a certain size, if they run on slightly larger data sets the system CPU use doesn't go as high - leading me to suspect that there is some limit as to the rate at which BSD can spawn processes.</p>

<p>as suggested below</p>

<pre><code>/usr/share/dtrace/toolkit/procsystime
</code></pre>

<p>gives us for its top entries on the 2x16 core machines</p>

<pre><code>    readlink        80898169570
      select       128032327883
      execve       215209078214
       wait4      2127022159693
        read      2545974471446
</code></pre>

<p>and on the 6 core machines</p>

<pre><code>    _umtx_op         5997915963
      select         8746697465
        read        59777849114
       wait4        61693132566
</code></pre>

<p>which doesn't seem to be enough of a difference to account for this non-linear scaling.</p>

<p><strong>EDIT</strong></p>

<p>When the system is under this load, running <code>uname</code> in a loop takes half a second per execution, vs milliseconds when the machine is idle. There seems to be some kind of kernel issue here</p>
","<freebsd>","2016-11-13 14:26:50"
"957386","multicast traffic across vlan","<p>Suppose I have a layer 2 managed switch which can be programmed to have multiple VLANs. Lets say there are 2 configured VLANs as of now vlan-1 and vlan-2. I also have a linux machine with 2 ethernet NIC with each ethernet port 1 and 2 connected to a switch port belonging to VLAN 1 and VLAN 2 respectively. My server is connected to vlan-1 and client connected to vlan-2 .The server is a multicast server. How should I configure the linux machine(to act as a router) to route multicast packets from vlan-1 to vlan-2 and vice-versa?  </p>

<p>The diagram of the network looks similar to this (with both the switches being same but with two vlan)
<a href=""https://i.sstatic.net/JXG6n.png"" rel=""nofollow noreferrer"">enter image description here</a></p>

<p>and since igmp snooping is not turned on the switch it will broadcast packets hence it will reach the linux machine. </p>
","<routing><vlan><multicast>","2019-03-08 15:30:50"
"957439","Reconfiguration of httpd.conf from one project to three projects on local server","<h3>Problem</h3>

<p>I need to properly configure my <code>httpd.conf</code> on apache2 server to change it from one project (<a href=""http://localhost/"" rel=""nofollow noreferrer"">http://localhost/</a>) to three projects. Would you be so kind and check the following codes and descriptions and help me to make a correct reconfiguration? </p>

<h3>Current one project directory model:</h3>

<pre><code>Library
+++ WebServer

+++ +++ Documents
+++ +++ database
</code></pre>

<h3>Desired three-project directory model:</h3>

<pre><code>Library
+++ WebServer

+++ +++ project_1
+++ +++ +++ public_html
+++ +++ +++ database

+++ +++ project_2
+++ +++ +++ public_html
+++ +++ +++ database

+++ +++ project_3
+++ +++ +++ public_html
+++ +++ +++ database
</code></pre>

<h3>Accessibility</h3>

<p>One project:</p>

<ul>
<li><a href=""http://localhost"" rel=""nofollow noreferrer"">http://localhost</a></li>
</ul>

<p>Three projects: </p>

<ul>
<li><p><a href=""http://localhost/project_1"" rel=""nofollow noreferrer"">http://localhost/project_1</a></p></li>
<li><p><a href=""http://localhost/project_2"" rel=""nofollow noreferrer"">http://localhost/project_2</a></p></li>
<li><p><a href=""http://localhost/project_3"" rel=""nofollow noreferrer"">http://localhost/project_3</a></p></li>
</ul>

<h3>Relevant portion of <code>httpd.conf</code> for one project (currently works for one project):</h3>

<pre><code>DocumentRoot ""/Library/WebServer/Documents""

&lt;Directory ""/Library/WebServer/Documents""&gt;
    Options Indexes FollowSymLinks MultiViews
    MultiviewsMatch Any
    AllowOverride All
    Require all granted
&lt;/Directory&gt;
</code></pre>

<h3>Suggested portion of <code>httpd.conf</code> for multiple projects:</h3>

<pre><code>&lt;VirtualHost&gt;
   ServerAdmin localhost
    DocumentRoot ""/Library/WebServer/project_1/public_html""
    ServerName localhost
&lt;/VirtualHost&gt;  

&lt;Directory ""/Library/WebServer/project_1/public_html""&gt;
    Options Indexes FollowSymLinks MultiViews
    MultiviewsMatch Any
    AllowOverride All
    Require all granted
&lt;/Directory&gt;


&lt;VirtualHost&gt;
   ServerAdmin localhost
    DocumentRoot ""/Library/WebServer/project_2/public_html""
    ServerName localhost
&lt;/VirtualHost&gt;  

&lt;Directory ""/Library/WebServer/project_2/public_html""&gt;
    Options Indexes FollowSymLinks MultiViews
    MultiviewsMatch Any
    AllowOverride All
    Require all granted
&lt;/Directory&gt;


&lt;VirtualHost&gt;
   ServerAdmin localhost
    DocumentRoot ""/Library/WebServer/project_3/public_html""
    ServerName localhost
&lt;/VirtualHost&gt;  

&lt;Directory ""/Library/WebServer/project_3/public_html""&gt;
    Options Indexes FollowSymLinks MultiViews
    MultiviewsMatch Any
    AllowOverride All
    Require all granted
&lt;/Directory&gt;
</code></pre>

<p>Server version: Apache/2.4.28 (Unix)</p>
","<httpd><httpd.conf>","2019-03-08 21:56:35"
"957470","DNS only works when domain prefixed with https://","<p>I bought the domain <code>example.com</code> (of course, that is not the real name of my domain) from GoDaddy (I doubt that the fact that it is through GoDaddy is relevant, but I do not know). </p>

<p>When I type <code>example.com</code> into the address bar of my browser, I see this:</p>

<p><a href=""https://i.sstatic.net/eKpX4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eKpX4.png"" alt=""website coming soon!""></a></p>

<p>When I type <code>https://example.com</code> into the address bar, I get my site's homepage (although it takes a very long time - maybe the issue of time is related?).</p>

<p>How do I make it so that I can type in <code>example.com</code> or <code>https://example.com</code>  into my address bar and
automatically get to my site?</p>

<p>This is what my DNS records look like right now:</p>

<p><a href=""https://i.sstatic.net/tJJ5A.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tJJ5A.png"" alt=""enter image description here""></a></p>
","<domain-name-system><https><godaddy>","2019-03-09 06:56:53"
"957488","How can I verify Haproxy backend HTTPS server as in browser I can?","<p>Haproxy's documentation says the <a href=""http://cbonte.github.io/haproxy-dconv/1.8/configuration.html#ssl%20(Server%20and%20default-server%20options)"" rel=""nofollow noreferrer"">ssl</a> and the <a href=""http://cbonte.github.io/haproxy-dconv/1.8/configuration.html#verify%20%28Server%20and%20default-server%20options%29"" rel=""nofollow noreferrer"">verify</a> server option enable verify on backend server's certificate via one <a href=""http://cbonte.github.io/haproxy-dconv/1.8/configuration.html#ca-file%20(Server%20and%20default-server%20options)"" rel=""nofollow noreferrer"">ca-file</a> but I try to use Firefox export the backend server's CA file then use the exported CA file to verify backend server and I get the <code>503 Service Unavailable</code> prompt.<br>
<strong>Why the CA file and SSL verify doesn't work?</strong></p>

<p><a href=""https://serverfault.com/a/957418/485358"">Note the simplest TCP mode reverse proxy not helpful in this case because I don't want to send the SNI information.</a>  </p>
","<reverse-proxy><haproxy>","2019-03-09 10:44:34"
"814887","How to get NAT to show original IP address?","<p>I am trying to get our NAT Ubuntu server to send all external internat traffic to our firewall Untrust interface. I see the traffic on the firewall, however the source for all the traffic is coming from 10.5.5.5 (the NAT's private IP). I know it has to do with the masquerade rule, but if I remove the rule I don't get any traffic.</p>

<p>Can a kind soul please help me get the traffic to show the original IP address? Any help would be much appreciated!</p>

<p>Firwall private IP: 10.5.1.4<br>
NAT private IP: 10.5.5.5</p>

<p>Here are my rules so far:</p>

<pre><code>sudo sysctl -w net.ipv4.ip_forward=1
sudo iptables -t nat -A PREROUTING -i eth0 -p tcp --dport 50022 -j DNAT --to 10.5.5.5:22
sudo iptables -t nat -A PREROUTING -i eth0 -j DNAT --to-destination 10.5.1.4
sudo iptables -A FORWARD -i eth0 -j ACCEPT
sudo iptables -t nat -A POSTROUTING -j MASQUERADE
</code></pre>

<p>EDIT-
What I'm trying to accomplish: We have a Palo Alto firewall deployed in Azure.  Azure only allows one public IP per VM. And in this situation we need the public IP assigned to the management interface of the firewall. So we need a NAT VM with a public IP to forward all traffic from its public interface to the Untrust interface of the Palo Alto firewall. I need the firewall to see the original outside internet traffic. The NAT VM and Firewall VM are in the same VNET in Azure.</p>
","<ubuntu><iptables><nat>","2016-11-14 22:52:08"
"814986","Unable to install some required packages in Red Hat Linux","<p>I Have RHEL6.5. I mam trying to install some required packages.Here is the response</p>

<pre><code>Error Downloading Packages:
  glibc-2.12-1.132.el6.i686: failure: Packages/glibc-2.12-1.132.el6.i686.rpm from Local: [Errno 256] No more mirrors to try.

  cracklib-2.8.16-4.el6.i686: failure: Packages/cracklib-2.8.16-4.el6.i686.rpm from Local: [Errno 256] No more mirrors to try.

  nss-softokn-freebl-3.14.3-9.el6.i686: failure: Packages/nss-softokn-freebl-3.14.3-9.el6.i686.rpm from Local: [Errno 256] No more mirrors to try
.
  db4-4.7.25-18.el6_4.i686: failure: Packages/db4-4.7.25-18.el6_4.i686.rpm from Local: [Errno 256] No more mirrors to try.

  libselinux-2.0.94-5.3.el6_4.1.i686: failure: Packages/libselinux-2.0.94-5.3.el6_4.1.i686.rpm from Local: [Errno 256] No more mirrors to try.
  pam-1.1.1-17.el6.i686: failure: Packages/pam-1.1.1-17.el6.i686.rpm from Local: [Errno 256] No more mirrors to try.

  audit-libs-2.2-2.el6.i686: failure: Packages/audit-libs-2.2-2.el6.i686.rpm from Local: [Errno 256] No more mirrors to try.


Error code: 1
Failed to install pam.i686

#################################################################################################
ERROR: Failed to install some required packages. 
Verify if yum repository is configured. (yum repolist)
You could configure a local repository using the local.repo template in installation directory.
#################################################################################################
</code></pre>

<p>Here the output of yum repolist command:</p>

<pre><code>Loaded plugins: product-id, refresh-packagekit, security, subscription-manager
This system is receiving updates from Red Hat Subscription Management.
repo id                                                             repo name                                                                          status
Local                                                               Local Media Repository                                                             3,690
repolist: 3,690


exiting setup...
</code></pre>
","<linux><redhat><yum><rpm><package-management>","2016-11-15 11:43:10"
"957676","How to get rid of WWW from domain, and make it always redirect to https://example.com or https://foo.example.com","<p><strong>I'm using Apache2, and I'm still pretty new to it.</strong></p>

<p>The issue I'm having is my site, is causing infinite loops, and I also want to get rid of the WWW portion from my domains and subdomains.</p>

<p>This is my conf file for port 80:</p>

<pre><code>&lt;VirtualHost *:80&gt;
    ServerName foo.example.com
    ServerAlias www.foo.example.com
    ServerAdmin webmaster@localhost
    DocumentRoot /var/www/html/foo/public
    ErrorLog ${APACHE_LOG_DIR}/error.log
    CustomLog ${APACHE_LOG_DIR}/access.log combined
RewriteEngine on
RewriteCond %{SERVER_NAME} =foo.example.ga
RewriteRule ^ https://%{SERVER_NAME}%{REQUEST_URI}
&lt;/VirtualHost&gt;
</code></pre>

<p>This is my conf file for port 443:</p>

<pre><code>&lt;IfModule mod_ssl.c&gt;
&lt;VirtualHost *:443&gt;
    ServerAdmin webmaster@localhost
    DocumentRoot /var/www/html/foo/public
    ErrorLog ${APACHE_LOG_DIR}/error.log
    CustomLog ${APACHE_LOG_DIR}/access.log combined
ServerName foo.example.com
ServerAlias www.foo.example.com
Include /etc/letsencrypt/options-ssl-apache.conf
SSLCertificateFile /etc/letsencrypt/live/foo.example.com/fullchain.pem
SSLCertificateKeyFile /etc/letsencrypt/live/foo.example.com/privkey.pem
RewriteEngine on
RewriteCond %{SERVER_NAME} =www.foo.example.com
RewriteRule ^ https://%{SERVER_NAME}%{REQUEST_URI} [END,NE,R=permanent]
&lt;/VirtualHost&gt;
&lt;/IfModule&gt;
</code></pre>

<p>Now I know that it isn't coded properly, but if someone could point me towards the right direction, that'd be wonderful. Thank you!</p>
","<apache-2.4><virtualhost><www>","2019-03-11 06:10:53"
"815006","VMware in one instance multiple server","<p>I have one questions, in my office we have 8 blade servers DELL PowerEdge M630 in one chassis. Now I want create one instance with 8 blade server. Is it possible to create one instance of 8 Blade server ? second I want to install NVIDIA graphic card with one BL server and use the same card for graphic design.
Here is confusion is should I start VMware Esxi with BL no.1 and configure BL-1 to BL-8. or should I install VMWARE ESXI on all BL servers.</p>

<p>Kindly suggest how I configure 1 instance with 8 BL servers.</p>

<p>Thanks and regards,</p>

<p>Anand M.</p>
","<virtualization>","2016-11-15 13:00:49"
"957728","Solved Lotus domino 550 5.1.1 Error: user unknown on a newly created database","<p>Good Morning</p>

<p>i'm having some trouble with a newly created Domino Database. When i try to send an email from an external address, i get ""550 5.1.1 Error: user unknown"" error</p>

<p>i've created the database using the registration tool inside Domino Administrator: </p>

<p>in the tab ""People &amp; Groups"", on the ""Tools"" column, i selected ""people > register"" and added all the information i usually add to create a new entry, with an email address. I did certify the email database with the correct id from my company.</p>

<p>Then i proceeded to register it and typed in the server console</p>

<p>load updall names.nsf 
tell adminp process all</p>

<p>after that, the database can send and receive emails from internal databases, and send emails otside too.</p>

<p>Any suggestion?</p>

<p>Thanks for all your time and help in advance</p>

<p>Enrico</p>

<p>[Edit] The problem wasn't caused by the server.</p>
","<email><email-server><ibm-domino>","2019-03-11 11:40:15"
"957732","Nginx SSL for all subdomains","<p>I want to access nginx subdomains on SSL also. I tried different want it didn't work. I am using let's encrypt.
SSL is working on 1.example.com but I want to access *.1.example.com on SSL.</p>

<pre><code>server {
    listen      80;
    server_name 1.example.de;

    # Strict Transport Security
    add_header Strict-Transport-Security max-age=2592000;
    return 301 https://$host$request_uri;
}
</code></pre>

<p>I also tried </p>

<pre><code>server {
    listen      80;
    server_name *.1.example.com;

    # Strict Transport Security
    add_header Strict-Transport-Security max-age=2592000;
    rewrite ^/.*$ https://$host$request_uri? permanent;
}
</code></pre>

<p>I want to access abc.1.example.com on SSL </p>

<p>I am getting DNS_PROBE_FINISHED_NXDOMAIN </p>
","<nginx>","2019-03-11 12:12:17"
"957880","HTTP request to unavailable resource gives different error message","<p>I have Ubuntu 18.04 client and I try to curl to the HOST that is not present on the network like:</p>

<pre><code>curl 192.168.11.1 --connect-timeout 10
curl 192.168.11.1 --connect-timeout 35
</code></pre>

<p>Why does it sometimes respond with </p>

<p><code>curl: (7) Failed to connect to 192.168.11.1 port 80: No route to host</code>, </p>

<p>and sometimes with </p>

<p><code>curl: (28) Connection timed out after 35001 milliseconds</code>, </p>

<p>even on the same timeout?</p>

<hr>

<p>I also tried making HTTP request with <code>NodeJS</code>'s http request library from the same Ubuntu client. And sometimes I get </p>

<p><code>Error: connect ETIMEDOUT 192.168.9.1:80</code> and other times I get </p>

<p><code>Error: connect EHOSTUNREACH 192.168.9.1:80</code>.</p>
","<linux><ubuntu><networking><linux-networking>","2019-03-12 10:01:16"
"815532","MS Exchange errors to solve for installing on win server 2012","<p>For a home project I wish to install MS Exchange to a Windows Server 2012. The little server needs to know the following:</p>

<ul>
<li>shared calender between clients</li>
<li>mailing</li>
<li>shared ToDo.
I try to achieve this using MS Exchange but I can't install due to some errors I enclose now. All my software are downloaded from my Microsoft Imagine account as I'm a student. The win server 2012 is just installed, nothing has been configured, guess that is the main reason why this isn't working. There are a lot of component missing. Do I have to install these one by one, or there is a solution that's quicker? How to make this work?</li>
</ul>

<blockquote>
  <p>Error: This computer isn't part of an Active Directory domain.</p>
  
  <p>Error: The user isn't logged on to an Active Directory domain.</p>
  
  <p>Error: This computer doesn't belong to a valid Active Directory site.
  Check the site and subnet definitions.</p>
  
  <p>Error: This computer requires .NET Framework 4.5.2</p>
  
  <p>Error: There is a pending reboot from a previous installation of a
  Windows Server role or feature. Please restart the computer and then
  run Setup again.</p>
  
  <p>Error: You must be a member of the 'Organization Management' role
  group or a member of the 'Enterprise Admins' group to continue.</p>
  
  <p>Error: You must use an account that's a member of the Organization
  Management role group to install or upgrade the first Mailbox server
  role in the topology.</p>
  
  <p>Error: You must use an account that's a member of the Organization
  Management role group to install the first Client Access server role
  in the topology.</p>
  
  <p>Error: You must use an account that's a member of the Organization
  Management role group to install the first Client Access server role
  in the topology.</p>
  
  <p>Error: You must use an account that's a member of the Organization
  Management role group to install or upgrade the first Mailbox server
  role in the topology.</p>
  
  <p>Error: You must use an account that's a member of the Organization
  Management role group to install or upgrade the first Client Access
  server role in the topology.</p>
  
  <p>Error: You must use an account that's a member of the Organization
  Management role group to install the first Mailbox server role in the
  topology.</p>
  
  <p>Error: Setup encountered a problem while validating the state of
  Active Directory: Active Directory server  is not available. Error
  message: Active directory response: The LDAP server is unavailable. 
  See the Exchange setup log for more information on this error.</p>
  
  <p>Error: The 'IIS 7 Dynamic Content Compression' component is required.
  Install the component via Server Manager.</p>
  
  <p>Error: The 'IIS 7 Digest Authentication' component is required.
  Install the component via Server Manager.</p>
  
  <p>Error: The 'IIS 6 WMI Compatibility' component is required. Install
  the component via Server Manager.</p>
  
  <p>Error: The 'Client Certificate Mapping Authentication' component is
  required. Install the component via Server Manager.</p>
  
  <p>Error: The 'HTTP Redirection' component is required. Install the
  component via Server Manager.</p>
  
  <p>Error: The 'Tracing' component is required. Install the component via
  Server Manager.</p>
  
  <p>Error: The 'Request Monitor' component is required. Install the
  component via Server Manager.</p>
  
  <p>Error: This computer requires the Microsoft Unified Communications
  Managed API 4.0, Core Runtime 64-bit.</p>
  
  <p>Error: Either Active Directory doesn't exist, or it can't be
  contacted.</p>
  
  <p>Warning: This computer requires the update described in Microsoft
  Knowledge Base article KB2884597</p>
  
  <p>Warning: This computer requires the update described in Microsoft
  Knowledge Base article KB2894875</p>
  
  <p>Warning: This computer requires the update described in Microsoft
  Knowledge Base article KB2822241</p>
</blockquote>
","<exchange><installation>","2016-11-17 13:27:07"
"882607","Asterisk compilation issue","<p>I've downloaded source code of Asterisk from <code>http://downloads.asterisk.org/pub/telephony/asterisk/</code><br>
I'm getting error while compiling this from source code in Ubuntu 16.04.1. 
    Please suggest prerequisites for asterisk which needed for compilation.</p>

<p>I'm getting this error while trying to make -</p>

<pre><code>./libasteriskssl.so: undefined reference to `CRYPTO_num_locks'  
collect2: error: ld returned 1 exit status  
Makefile:321: recipe for target 'asterisk' failed  
make[1]: *** [asterisk] Error 1  
Makefile:368: recipe for target 'main' failed  
make: *** [main] Error 2  
</code></pre>

<p>Please suggest......</p>
","<asterisk>","2017-11-09 08:10:32"
"815570","Windows server something takes up my drive space","<p>I've been trying to figure this out but I really cannot understand.
I have two 1 TB hard drives with raid 1 and I have only 500 GB free space.
But I do not have any files that take up that much space.
I ran WinDirStat but I'm using only 44 GB</p>

<p><a href=""https://i.sstatic.net/7lSBo.jpg"" rel=""nofollow noreferrer"">WinDirStat and current usage</a></p>

<p>I checked the shadow copies and I don't have any</p>

<p><a href=""https://i.sstatic.net/988xi.png"" rel=""nofollow noreferrer"">Shadow Copies</a></p>

<p>Can someone tell me how to free up this space?</p>
","<windows><raid><hard-drive>","2016-11-17 16:54:21"
"815620","MS Exchange noderunner very high RAM usage","<p>I've installed Windows Server 2012 and MS Exchange on a VMWare that has 4 GB of RAM and can use 2 cores of CPU (1.8 GHz each) for a school project. The server ran fine <em>before</em> the Exchange install, but after that the RAM usage is always around 92% or more, the most is used by <strong>noderunner</strong> task. I tried shutting the <strong>Microsoft Exchange Search Host Controller service</strong> down from the services but nothing changed. Is there a way to shut that memory consuming process? I can't wait for multiple days for it to do whatever it wants, time is of the essence. 
EDIT: another one is IIS Worker process that takes up a lot of memory. Any ideas?</p>
","<performance><exchange><windows-server-2012><memory-usage>","2016-11-17 21:11:29"
"882711","AS400 iseries with azure office365","<p>I switched recently to a company after using SAP for years and I'm completely lost after having found that they are using AS400 still!.
I never used it before and I have three questions:</p>

<ol>
<li><p>Is the ibm iaccess navigator good to use the console or there are other consoles specially for windows and macos?</p></li>
<li><p>As we use office365 and azure is there a way to get data exported and visualize it on some other cool saas service? Specially to have charts or any else?</p></li>
<li><p>Is there a settings panel or admin access or list of commands? Or even a good book to buy to understand commands?</p></li>
</ol>
","<ibm-midrange>","2017-11-09 18:03:47"
"958445","Unable to reach Apache website from NAT IP. Resolving to hostname","<p>CentOS 7.x</p>

<p>Apache (httpd)</p>

<p>External NAT:  10.140.x.x</p>

<p>Internal IP:  10.105.x.x</p>

<p>When trying to reach the apache server on <a href=""https://10.140.x.x"" rel=""nofollow noreferrer"">https://10.140.x.x</a>, it tries to resolve the hostname in the lower left corner of firefox.  This won't work because we can't control DNS for the NAT network.  How do I configure Apache to listen on an IP that obviously doesn't exist on any interface of the box?</p>

<p>I've tried editing /etc/httpd/conf/httpd.conf</p>

<pre><code>ServerName 10.140.x.x:80
</code></pre>

<p>and</p>

<p>/etc/httpd/conf.d/ssl.conf</p>

<pre><code>&lt;VirtualHost _default_:443&gt;

# General setup for the virtual host, inherited from global configuration
DocumentRoot ""/var/www/html""
ServerName 10.140.x.x:443
</code></pre>

<p>But these edits don't seem to do anything.  We are able to reach the site on the same internal network.</p>
","<apache-2.4><ip><nat>","2019-03-15 13:43:13"
"815778","IIS 10.0 error 2280 while executing 32 bit","<p>While trying to make a local deploy of a rest-api that requires to be executed on 32bit, the server stops the application pool under witch it runs. </p>

<p>The events viewer:</p>

<blockquote>
  <p>error while loading ""C:\WINDOWS\system32\inetsrv\aspnetcore.dll.""
  code 2280.</p>
</blockquote>

<p>Any solution?</p>

<p>Visual Studio does debug perfectly but local IIS does not...</p>
","<iis><windows-10><visual-studio><32-bit><dll>","2016-11-18 17:01:57"
"958485","What should the gateway address (""route"") be set to if there is no gateway on the network?","<p>I currently have a LAN-only network configuration with several devices including a Linux based controller. This network features a single managed switch but no router.</p>
<p>My Linux device can't seem to ping any of the other devices on the network, while all the other devices are able to ping each-other just fine.</p>
<p>Because my network does not have a gate-way, should my Linux device still feature a default gateway address? There is an old one address that had been saved to the device, however, because my Linux device isn't attempting to reach anything outside the network, it seems like this old gateway setting shouldn't even matter/affect things.</p>
<p>So my question: How should the gateway address on a Linux device be configured when there is no gateway on the network?</p>
<p>Update: Turns out the Linux device in question was not compatible with a Gigabit switches, hence being unable to ping the other devices on the network.</p>
","<networking><local-area-network><gateway>","2019-03-15 17:16:39"
"958541","AD child domain fails to authenticate when parent domain goes offline","<p>We have several child domains in our forest. All are geographically separated from the parent. One particular child is misbehaving.</p>

<p>When the parent (domain.local) or child (child.domain.local) loses Internet access the clients in the child domain cannot login using accounts within its domain.</p>

<p>Only the child domain administrator account seems to work -- and of course local client accounts. I've checked the child domain DNS settings, and they appear to be the same as other child domains in the forest.</p>

<p>For example, child domain DC IP settings...</p>

<pre><code>IP: 10.10.1.100
SN: 255.255.255.0
DNS1: 127.0.0.1
DNS2: 192.168.1.100
</code></pre>

<p>When the Internet is functioning AD replication and authentication works fine. </p>

<p>Any tips or suggestions on what might be causing this would be great.</p>
","<windows><active-directory>","2019-03-16 00:13:42"
"815847","How to prevent Users in Domain installing and removing software?","<p>I am using ADDS in my Company, using Windows server 2012 r2.</p>

<p>Like the tittle, how to prevent instaling, removing any software for my users in their computer? I want to protect company's computers. </p>

<p>I dont want my users can installing , and removing anything.</p>

<p>So what can I do with GPO, what GPO can do that ?</p>

<p>Tks all.</p>
","<windows-server-2012-r2><domain-controller>","2016-11-19 02:13:27"
"815860","Apache being hit by requests (cpu 100%)","<p>recently we moved to a new host (DO) for one of my client after being on a shared account .</p>

<p>I was monitoring the cpu and it was always at 100% , knowing that the site dosnt  get a lot of traffic , decided to check the access log and i saw the following (1-5 queries / second) NO STOP .</p>

<p>I removed my domain and added example.com</p>

<pre><code>10.17.0.2 - - [19/Nov/2016:09:41:15 +0000] ""GET /http:/example.com/wp-content/themes/jupiter/assets/stylesheet/min/critical-path.css HTTP/1.0"" 404 47448 ""-"" ""WordPress/4.3.6; http://example.com""
10.17.0.2 - - [19/Nov/2016:09:41:21 +0000] ""GET /http://example.com/wp-content/themes/jupiter/assets/stylesheet/min/critical-path.css HTTP/1.0"" 301 550 ""-"" ""WordPress/4.3.6; http://example.com""
10.17.0.2 - - [19/Nov/2016:09:41:22 +0000] ""GET /http://example.com/wp-content/themes/jupiter/assets/stylesheet/min/critical-path.css HTTP/1.0"" 301 550 ""-"" ""WordPress/4.3.6; http://example.com""
10.17.0.2 - - [19/Nov/2016:09:41:16 +0000] ""GET /http:/example.com/wp-content/themes/jupiter/assets/stylesheet/min/critical-path.css HTTP/1.0"" 404 47445 ""-"" ""WordPress/4.3.6; http://example.com""
10.17.0.2 - - [19/Nov/2016:09:41:16 +0000] ""GET /http:/example.com/wp-content/themes/jupiter/assets/stylesheet/min/critical-path.css HTTP/1.0"" 404 47472 ""-"" ""WordPress/4.3.6; http://example.com""
10.17.0.2 - - [19/Nov/2016:09:41:22 +0000] ""GET /http://example.com/wp-content/themes/jupiter/assets/stylesheet/min/critical-path.css HTTP/1.0"" 301 550 ""-"" ""WordPress/4.3.6; http://example.com""
10.17.0.2 - - [19/Nov/2016:09:41:23 +0000] ""GET /http://example.com/wp-content/themes/jupiter/assets/stylesheet/min/critical-path.css HTTP/1.0"" 301 550 ""-"" ""WordPress/4.3.6; http://example.com""
10.17.0.2 - - [19/Nov/2016:09:41:17 +0000] ""GET /http:/example.com/wp-content/themes/jupiter/assets/stylesheet/min/critical-path.css HTTP/1.0"" 404 47412 ""-"" ""WordPress/4.3.6; http://example.com""
10.17.0.2 - - [19/Nov/2016:09:41:17 +0000] ""GET /http:/example.com/wp-content/themes/jupiter/assets/stylesheet/min/critical-path.css HTTP/1.0"" 404 47438 ""-"" ""WordPress/4.3.6; http://example.com""
10.17.0.2 - - [19/Nov/2016:09:41:24 +0000] ""GET /http://example.com/wp-content/themes/jupiter/assets/stylesheet/min/critical-path.css HTTP/1.0"" 301 550 ""-"" ""WordPress/4.3.6; http://example.com""
10.17.0.2 - - [19/Nov/2016:09:41:24 +0000] ""GET /http://example.com/wp-content/themes/jupiter/assets/stylesheet/min/critical-path.css HTTP/1.0"" 301 550 ""-"" ""WordPress/4.3.6; http://example.com""
10.17.0.2 - - [19/Nov/2016:09:41:18 +0000] ""GET /http:/example.com/wp-content/themes/jupiter/assets/stylesheet/min/critical-path.css HTTP/1.0"" 404 47426 ""-"" ""WordPress/4.3.6; http://example.com""
10.17.0.2 - - [19/Nov/2016:09:41:18 +0000] ""GET /http:/example.com/wp-content/themes/jupiter/assets/stylesheet/min/critical-path.css HTTP/1.0"" 404 47446 ""-"" ""WordPress/4.3.6; http://example.com""
</code></pre>

<p>And whenever i restart apache it goes back to normal for few minutes then the flood starts again .</p>

<p>I remarked the followings :</p>

<p>IPS that request this are (10.17.0.2 , 127.0.0.1 , server ip , 37.1.213.192)</p>

<p>The files does exist and when  try to access it i only see GET /wp-content/...</p>

<p>Sometime the request is http:/example (1 slash) and sometime its <a href=""http://example"" rel=""nofollow noreferrer"">http://example</a></p>

<p>How can i stop this mess ?</p>

<p><a href=""https://i.sstatic.net/Zrh7g.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Zrh7g.png"" alt=""enter image description here""></a></p>
","<apache-2.4><wordpress><cpu-usage><hacking><flooding>","2016-11-19 09:56:25"
"883093","how to know if need to run e2fsck in order to fix corrupted blocks?","<p>We want to check the filesystem on the disks as <code>/deb/sdc</code>  ... <code>/dev/sdg</code> on each Red Hat Linux machine.</p>

<p>The target is to find what are the disks that require <code>e2fsck</code> ( as <code>e2fsck -y /dev/sdb</code> etc.)</p>

<p>According to man page </p>

<blockquote>
  <p><code>-n</code><br>
  Open  the filesystem read-only, and assume an answer of 'no' to all questions.  Allows <code>e2fsck</code> to be used non-interactively.  This option may not be specified at the same time as the <code>-p</code> or <code>-y</code> options.</p>
</blockquote>

<p>When we run the command (example)</p>

<pre><code> e2fsck -n /dev/sdXX
</code></pre>

<p>we get</p>

<blockquote>
<pre><code>e2fsck 1.42.9 (28-Dec-2013)
Warning!  /dev/sdc is mounted.
Warning: skipping journal recovery because doing a read-only filesystem check.
/dev/sdc: clean, 94/1310720 files, 156685/5242880 blocks
</code></pre>
</blockquote>

<p>So what do we need to capture from <code>e2fsck -n</code> output, that requires us to run <code>e2fsck</code> (without <code>-n</code>)?</p>

<p><strong>e2fsck process</strong></p>

<pre><code>init 1
umount /dev/sdXX
e2fsck -y /dev/sdXX  # (or e2fsck -C /dev/sdXX for full details) 
init 3
</code></pre>
","<linux><redhat><hard-drive><filesystems><e2fsck>","2017-11-12 20:08:24"
"883169","WordPress does not execute PHP file","<p>my main WordPress page shows:</p>

<pre><code>&lt;?php
/**
 * Front to the WordPress application. This file doesn't do anything, but loads
 * wp-blog-header.php which does and tells WordPress to load the theme.
 *
 * @package WordPress
 */

/**
 * Tells WordPress to load the WordPress theme and output it.
 *
 * @var bool
 */
define('WP_USE_THEMES', true);

/** Loads the WordPress Environment and Template */
require( dirname( __FILE__ ) . '/wp-blog-header.php' );
</code></pre>

<p>my platform: <strong>ubuntu 16.04</strong> and I am using <strong>Apache</strong>.
how can i solve this?</p>

<p><strong>apache2 -M | grep php</strong> output:</p>

<pre><code>[Mon Nov 13 06:58:35.785748 2017] [core:warn] [pid 20486] AH00111: Config variable ${APACHE_LOCK_DIR} is not defined
[Mon Nov 13 06:58:35.785929 2017] [core:warn] [pid 20486] AH00111: Config variable ${APACHE_PID_FILE} is not defined
[Mon Nov 13 06:58:35.786020 2017] [core:warn] [pid 20486] AH00111: Config variable ${APACHE_RUN_USER} is not defined
[Mon Nov 13 06:58:35.786087 2017] [core:warn] [pid 20486] AH00111: Config variable ${APACHE_RUN_GROUP} is not defined
[Mon Nov 13 06:58:35.786164 2017] [core:warn] [pid 20486] AH00111: Config variable ${APACHE_LOG_DIR} is not defined
[Mon Nov 13 06:58:35.790530 2017] [core:warn] [pid 20486:tid 140021226223488] AH00111: Config variable ${APACHE_LOG_DIR} is not defined
[Mon Nov 13 06:58:35.790931 2017] [core:warn] [pid 20486:tid 140021226223488] AH00111: Config variable ${APACHE_LOG_DIR} is not defined
[Mon Nov 13 06:58:35.791163 2017] [core:warn] [pid 20486:tid 140021226223488] AH00111: Config variable ${APACHE_LOG_DIR} is not defined
AH00526: Syntax error on line 74 of /etc/apache2/apache2.conf:
Invalid Mutex directory in argument file:${APACHE_LOCK_DIR}
</code></pre>

<p><strong>sudo dpkg --get-selections | grep php</strong> output:</p>

<pre><code>libapache2-mod-php              install
libapache2-mod-php7.0               install
php                     install
php-common                  install
php-curl                    install
php-gd                      install
php-gettext                 install
php-mbstring                    install
php-mcrypt                  install
php-mysql                   install
php-pear                    install
php-phpseclib                   install
php-tcpdf                   install
php-xml                     install
php-xmlrpc                  install
php7.0                      install
php7.0-cli                  install
php7.0-common                   install
php7.0-curl                 install
php7.0-gd                   install
php7.0-json                 install
php7.0-mbstring                 install
php7.0-mcrypt                   install
php7.0-mysql                    install
php7.0-opcache                  install
php7.0-readline                 install
php7.0-xml                  install
php7.0-xmlrpc                   install
phpmyadmin                  install
</code></pre>

<p><strong>systemctl status apache2.service</strong> output: </p>

<pre><code>* apache2.service - LSB: Apache2 web server
   Loaded: loaded (/etc/init.d/apache2; bad; vendor preset: enabled)
  Drop-In: /lib/systemd/system/apache2.service.d
           `-apache2-systemd.conf
   Active: failed (Result: exit-code) since Mon 2017-11-13 07:12:52 EST; 5min ago
     Docs: man:systemd-sysv-generator(8)
  Process: 20848 ExecStop=/etc/init.d/apache2 stop (code=exited, status=0/SUCCESS)
  Process: 19096 ExecReload=/etc/init.d/apache2 reload (code=exited, status=0/SUCCESS)
  Process: 20898 ExecStart=/etc/init.d/apache2 start (code=exited, status=1/FAILURE)

Nov 13 07:12:52 student apache2[20898]:  * The apache2 configtest failed.
Nov 13 07:12:52 student apache2[20898]: Output of config test was:
Nov 13 07:12:52 student apache2[20898]: [Mon Nov 13 07:12:51.996158 2017] [:crit] [pid 20909:tid 139772608190336] Apache is running a threaded MPM, but your PHP Module is not compiled to be threadsafe.  You need
Nov 13 07:12:52 student apache2[20898]: AH00013: Pre-configuration failed
Nov 13 07:12:52 student apache2[20898]: Action 'configtest' failed.
Nov 13 07:12:52 student apache2[20898]: The Apache error log may have more information.
Nov 13 07:12:52 student systemd[1]: apache2.service: Control process exited, code=exited status=1
Nov 13 07:12:52 student systemd[1]: Failed to start LSB: Apache2 web server.
Nov 13 07:12:52 student systemd[1]: apache2.service: Unit entered failed state.
Nov 13 07:12:52 student systemd[1]: apache2.service: Failed with result 'exit-code'.
</code></pre>
","<wordpress><ubuntu-16.04><apache2>","2017-11-13 11:33:52"
"958801","VMWare device mapper size smaller than parition size","<p>I am trying to understand what is going on with my VMWare instance. I have had 50GB partition that I extended to 150GB and resized the partition. However, the partition still shows that it only has 50GB size. Here is a screenshot from GParted:</p>

<p><a href=""https://i.sstatic.net/DdfnZ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/DdfnZ.jpg"" alt=""GParted""></a></p>

<p>Here is the output of <code>parted -l</code></p>

<pre><code>Model: VMware Virtual disk (scsi)
Disk /dev/sda: 161GB
Sector size (logical/physical): 512B/512B
Partition Table: msdos
Disk Flags:

Number  Start   End    Size   Type      File system  Flags
 1      1049kB  512MB  511MB  primary   ext2         boot
 2      513MB   161GB  161GB  extended
 5      513MB   161GB  161GB  logical                lvm


Model: Linux device-mapper (linear) (dm)
Disk /dev/mapper/mx1--vg-swap_1: 1074MB
Sector size (logical/physical): 512B/512B
Partition Table: loop
Disk Flags:

Number  Start  End     Size    File system     Flags
 1      0.00B  1074MB  1074MB  linux-swap(v1)


Model: Linux device-mapper (linear) (dm)
Disk /dev/mapper/mx1--vg-root: 52.1GB
Sector size (logical/physical): 512B/512B
Partition Table: loop
Disk Flags:

Number  Start  End     Size    File system  Flags
 1      0.00B  52.1GB  52.1GB  ext4

</code></pre>

<p>and here is the output of <code>fdisk -l</code></p>

<pre><code>Disk /dev/sda: 150 GiB, 161061273600 bytes, 314572800 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0xe541908b

Device     Boot   Start       End   Sectors   Size Id Type
/dev/sda1  *       2048    999423    997376   487M 83 Linux
/dev/sda2       1001470 314572799 313571330 149.5G  5 Extended
/dev/sda5       1001472 314572799 313571328 149.5G 8e Linux LVM


Disk /dev/mapper/mx1--vg-root: 48.5 GiB, 52097449984 bytes, 101752832 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes


Disk /dev/mapper/mx1--vg-swap_1: 1 GiB, 1073741824 bytes, 2097152 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
</code></pre>

<p>Output of <code>df -h</code>:</p>

<pre><code>Filesystem                Size  Used Avail Use% Mounted on
udev                      7.9G     0  7.9G   0% /dev
tmpfs                     1.6G  9.0M  1.6G   1% /run
/dev/mapper/mx1--vg-root   48G   46G  452K 100% /
tmpfs                     7.9G     0  7.9G   0% /dev/shm
tmpfs                     5.0M     0  5.0M   0% /run/lock
tmpfs                     7.9G     0  7.9G   0% /sys/fs/cgroup
/dev/sda1                 472M   57M  391M  13% /boot
tmpfs                     1.6G     0  1.6G   0% /run/user/1000
</code></pre>

<p>/dev/sda has a size of 160GB; however, the device mapper <code>/dev/mapper/mx1--vg-root</code> has only size of 50GB. The mapper maps the VDisk into the OS as far as I understand. When I do anything, I keep getting an error that there is no physical space in disk.</p>

<p>EDIT: I think I did not clarify my question properly. My issue is not in ESXI host but a Linux guest running in the hypervisor.</p>
","<ubuntu><vmware-esxi><vmware-vsphere><device-mapper>","2019-03-18 14:19:08"
"883254","Redirect to localhost:8080?","<p>I have a Jenkins installation which is accessible at <code>localhost:8080</code> or if accessing externally as website via <code>IPaddress:8080</code>.</p>
<p>I'd like to access the website with something like <code>IPaddress/jenkins</code> instead and have IIS know how to redirect to <code>localhost:8080</code> internally when accessed like that.</p>
<p>This is my web.config file:</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;configuration&gt;
    &lt;system.webServer&gt;
        &lt;rewrite&gt;
            &lt;rules&gt;
                &lt;rule name=&quot;ReverseProxyInboundRule1&quot; stopProcessing=&quot;true&quot;&gt;
                    &lt;match url=&quot;(jenkins.*)&quot; /&gt;
                    &lt;action type=&quot;Rewrite&quot; url=&quot;http://localhost:8080/{R:1}&quot; /&gt;
                &lt;/rule&gt;
            &lt;/rules&gt;
        &lt;/rewrite&gt;
    &lt;/system.webServer&gt;
&lt;/configuration&gt;
</code></pre>
<p>Unfortunately, after visiting <code>IPaddress/jenkins</code> I get an error 404.</p>
","<iis><rewrite><jenkins>","2017-11-13 19:16:10"
"883281","ElasticSearch doesn't start after upgrade from 2.3.1 to 5.6.4","<p>I was running on elasticsearch 2.3.1, but I had to upgrade to 5.6.4 (because I wanted to install Kibana/Sense), but ElasticSearch won't start anymore.</p>

<p>This is <strong>log</strong> from ElasticSearch:</p>

<pre><code>java.lang.IllegalStateException: The index [[cyclone3.mysite_sk/Qf5ccdWNRYKFGs4lBffSAQ]] was created with version [1.2.1] but the minimum compatible version is [2.0.0-beta1]. It should be re-indexed in Elasticsearch 2.x before upgrading to 5.6.4.
</code></pre>

<p>And lot of the same errors.</p>

<p>Yes I understand that I need to reindex, but how? And does I lose all data?</p>

<p><strong>THIS IS NOT A DUPLICATE OF <a href=""https://serverfault.com/questions/883269/elasticsearch-doesnt-start-after-installing-5-6-4"">ElasticSearch doesn&#39;t start after installing 5.6.4</a> BECAUSE THIS IS ON THE DIFFERENT SERVER</strong></p>
","<elasticsearch><start>","2017-11-13 22:29:00"
"959014","Configure bind9 zone records to ping/dig/nslookup/web browse using hostname only or fqdn","<p>What is the proper way to configure a BIND9 zone file to allow ping/dig/nslookup/web browsers to query a host using its hostname or a shortened CNAME record that points to this hostname instead of the entire fqdn of the host.</p>

<p>Example:  This works.</p>

<pre><code>dnsadmin@tre-lfs4:~$ dig fs.domain.org
; &lt;&lt;&gt;&gt; DiG 9.10.3-P4-Ubuntu &lt;&lt;&gt;&gt; fs.domain.org
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 3925
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 1, ADDITIONAL: 1
;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;fs.domain.org.           IN      A
;; ANSWER SECTION:
fs.domain.org.    38400   IN      CNAME   tre-lfs1.domain.org.
tre-lfs1.domain.org. 38400 IN     A       10.30.0.31
</code></pre>

<p>From this zone file entry:</p>

<pre><code>tre-lfs1.domain.org.    IN  A   10.30.0.31
fs.domain.org.  IN  CNAME   tre-lfs1.domain.org.
</code></pre>

<p>I want to be able to do this:</p>

<pre><code>nslookup/ping/dig fs
</code></pre>

<p>With a zone file entry that looks more like this:</p>

<pre><code>tre-lfs1.   IN  A   10.30.0.31
fs. IN  CNAME   tre-lfs1.
</code></pre>

<p>So that I can both use the entire FQDN OR just the shortened hostname/cname. </p>

<p>The reason is that I may have the same machine hosting web and other services for different domains, but when locally administering I want to just use the short names.</p>

<p>Anyone know how to achieve this or a resource to look to in order to do so?</p>
","<domain-name-system><bind><hostname><fqdn>","2019-03-19 20:30:57"
"959101","htaccess config to block on browser agent","<p><a href=""https://serverfault.com/questions/253419/htaccess-config-to-block-depending-on-browser-agent"">htaccess config to block depending on browser agent</a></p>

<p>how is the opposite done of this code ?</p>

<p>for example inhibition of <code>^.Chrome/10.$</code> entry</p>
","<.htaccess><mod-rewrite>","2019-03-20 11:17:04"
"816448","HP ProLiant DL160 G6 fan conversion","<p>I came across this forum a back while looking for an answer to this question and found some excellent advise that I would like some additional info for.</p>

<p>I have purchased the above server in a 1U case however it's far to noisy for its location.  I have moved the motherboard into a 4u case and hoping to use active cooling, using 2 pwm fans using an official HP splitter with a 4 pin adaptation.</p>

<p>Now this server needs all 6 fan headers connected to the board to boot and making those connectors is fine, however 12 fans in a pc that won't really require them for airflow due to the new 4u case and the active cooling on the heatsinks.</p>

<p>My question is can I connect the wires together to make 1 single 4 pin pwm connector as 1 sensor wire is for input and the other is output as the fans from the server are back to back running 8k rpm each or will I have to have all 12 fans.</p>

<p>Happy to give more information if required I tried to add photos but I needed more rep to post.</p>
","<hardware>","2016-11-22 21:58:43"
"883670","Running an Ansible ping on a remote host fails","<p>I've just getting started to study Ansible and how it can improves my job. With that said, I'm facing issues when I try to ping a host with Ansible (<code>ansible all -m ping -vv</code>). Ansible is installed on host A and host B is configured on /etc/ansible/hosts. Both hosts run Ubuntu 16.04.03 LTS and I can SSH from A to B without password.</p>

<p>Ansible ping returns</p>

<pre><code>172.28.25.81 | FAILED! =&gt; {
    ""changed"": false, 
    ""failed"": true, 
    ""module_stderr"": ""Shared connection to 172.28.25.81 closed.\r\n"", 
    ""module_stdout"": ""/bin/sh: 1: /usr/bin/python: not found\r\n"", 
    ""msg"": ""MODULE FAILURE"", 
    ""rc"": 0
}
</code></pre>

<p>My software versions are: </p>

<ul>
<li>Ansible 2.4.1.0</li>
<li>Python 2.7.12</li>
</ul>
","<ubuntu><ansible><automation>","2017-11-16 02:03:02"
"816659","very low performance in apache server (windows 2012 R2)","<p>I have a brand new HP server with windows 2012 r2 64 bits. It has 2 Xeon Processors E-5 2603 v3. (1.6ghz) and 32 GB ram and runs XAMPP (apache 2.4, mysql 5.7, php 5.6). The server is not yet in production..</p>

<p>The program has some big scripts (like generating large pdf files of patients clinical histories, over 450 pages).</p>

<p>My personal computer is a standard pc, with an Intel core i3 (3.2 ghz) processor, 4GB ram, Windows 10 64 bit.</p>

<p>How is it possible that my personal computer has much better performance than the new server? The large scripts that take about 40 seconds on my computer, take up to 3 minutes on the new server, running the same versions of apache, mysql, php and exactly the same settings.</p>

<p>While the script is running, I open the task manager, and I see that the new server only uses 9% of its cpu. It uses only one of its 12 cores. Also the usage of memory is very low.</p>

<p>I suspect the difference is made by the hyper threading of the i3 processor over the xeon. it's possible?</p>

<p>Is there a way to make better use of resources? I know that the new server hardware is working properly because it has been reviewed by HP.</p>
","<performance><apache-2.4><memory-usage>","2016-11-23 15:46:38"
"959580","Domain still pointed to old server despite exchange being fully installed on new server","<p>I had exchange 2010 on an old server. I forcefully uninstalled Exchange and turned the Server Off.</p>

<p>I installed exchange 2019 on a new server. I was able to install and successfully rekeyed and added the SSL License .</p>

<p>Problem using SSLlabs SSLtest I found that ""mail.advsystems.com"" is still pointing to the old server. </p>

<p><a href=""https://www.ssllabs.com/ssltest/analyze.html?d=mail.advsystems.com"" rel=""nofollow noreferrer"">https://www.ssllabs.com/ssltest/analyze.html?d=mail.advsystems.com</a>.</p>

<p>How do I get mail.advsystems.com to point to the new server?</p>

<h2>Edit</h2>

<p>when I run NSLookup on the Exchange Server it gives me the Domain IP</p>

<p>I edited GoDaddy Public DNS record in DNS Management for the Exchange Server IP 192.168.103.150 from (98.191.213.57) and now it is spitting the error </p>

<p>Assessment failed: IP address is from private address space (RFC 1918) </p>

<p><a href=""https://www.ssllabs.com/ssltest/analyze.html?d=mail.advsystems.com"" rel=""nofollow noreferrer"">https://www.ssllabs.com/ssltest/analyze.html?d=mail.advsystems.com</a></p>
","<exchange><ssl-certificate><domain><domain-name><exchange-2016>","2019-03-22 23:13:14"
"959596","Disabling SSH access into prod VM on GCP","<p>As per title, for example if I disable SSH access into VM on GCP, but someone wants to remotely manage VMS , build docker containers or manage Cloud storage objects what they have to do?</p>

<ol>
<li>Grant people access to use Google Cloudshell</li>
<li>Config VPN connection to GCP to allow SSH access to Cloud vms.</li>
</ol>

<p>My though:</p>

<ol>
<li>If people asks for remote access to vm just like SSH, so if machine still has external IP, they can access SSH using Cloud shell</li>
<li>Option 2 is possible if they mention external IP is removed but it seems like that is not the case. </li>
</ol>
","<ssh><google-cloud-platform><google-compute-engine><google-cloud-shell>","2019-03-23 03:51:11"
"816902","Insufficient quota exists to complete this operation, net ads join","<p>I'm trying to join Active Directory in Xubuntu 16.04 in a enterprise business enviroment so I'll change the name of my REALM by MY.EXAMPLE.CORP. My issue is: when I run</p>

<pre><code>net ads join -U Administrator
</code></pre>

<p>it appears:</p>

<pre><code>Failed to join domain: failed to join domain 'MY.EXAMPLE.CORP' over rpc: Insufficient quota exists to complete the operation.
</code></pre>

<p>I tried the kinit and klist commands and the result is:</p>

<pre><code>Tickect cache: FILE:/tmp/krb5cc_0
Default principal: Administrator@MY.EXAMPLE.CORP

Valid starting      Expires         Service principal
24/11/16 10:18:49   24/11/16 20:18:49   krbgt/MY.EXAMPLE.CORP@MY.EXAMPLE.CORP
    renew until   25/11/16 10:18:25
</code></pre>

<p>It means that kerberos is working well, it seems to be a problem of samba but I what is wrong with my smb.conf file or if I'm missing something else. I've tried every thing that I've read but it still no working</p>

<p>These are the changes that I've done in every file. krb5.conf, smb.conf, nsswitch.conf</p>

<p>krb5.conf</p>

<pre><code>[libdefaults]
    default_realm = MY.DOMAIN.CORP

....

[realms]
DOMAIN = {
        kdc = SERVER01.MY.DOMAIN.CORP
        kdc = SERVER02.MY.DOMAIN.CORP
        admin_server = SERVER01.MY.DOMAIN.CORP SERVER.MY.DOMAIN.CORP
        default_domain = MY.DOMAIN.CORP
    }

....


[domain_realm]
    SERVER01.MY.DOMAIN.CORP = MY.DOMAIN.CORP
    SERVER02.MY.DOMAIN.CORP = MY.DOMAIN.CORP
    .MY.DOMAIN.CORP = MY.DOMAIN.CORP
    MY.DOMAIN.CORP = MY.DOMAIN.CORP
</code></pre>

<p>smb.conf</p>

<pre><code>[global]
    workgroup = MYWORKGROUP
    realm = MY.DOMAIN.CORP
    security = ADS
    encrypt passwords = yes
    password server = SERVER01.MY.DOMAIN.CORP SERVER02.MYDOMAIN.CORP
    idmap uid = 10000-20000
    idmap gid = 10000-20000
    winbind enum users = yes
    winbind enum groups = yes
    winbind refresh tickets = true
    template homedir = /home/%D/%U
    template shell = /bin/bash
    winbind use default domain = yes
    restrict anonymous = 2
    winbind offline logon = yes
</code></pre>

<p>nsswitch.conf</p>

<pre><code>passwd:         compat winbind
group:          compat winbind
shadow:         compat
</code></pre>

<p>And this is what I have in my .log file:</p>

<pre><code>[2016/11/29 08:13:22.207182,  0] ../source3/auth/auth_domain.c:121(connect_to_domain_password_server)
  connect_to_domain_password_server: unable to open the domain client session to machine SERVER01.MY.DOMAIN.CORP. Error was : NT_STATUS_CANT_ACCESS_DOMAIN_INFO.
[2016/11/29 08:13:22.211148,  0] ../source3/auth/auth_domain.c:121(connect_to_domain_password_server)
  connect_to_domain_password_server: unable to open the domain client session to machine SERVER01.MY.DOMAIN.CORP. Error was : NT_STATUS_CANT_ACCESS_DOMAIN_INFO.
[2016/11/29 08:13:22.215292,  0] ../source3/auth/auth_domain.c:121(connect_to_domain_password_server)
  connect_to_domain_password_server: unable to open the domain client session to machine SERVER01.MY.DOMAIN.CORP. Error was : NT_STATUS_CANT_ACCESS_DOMAIN_INFO.
[2016/11/29 08:13:22.215350,  0] ../source3/auth/auth_domain.c:184(domain_client_validate)
  domain_client_validate: Domain password server not available.
</code></pre>
","<active-directory><samba><kerberos>","2016-11-24 17:11:26"
"816907","Amazon EC2 Centos SSL installation error","<p><strong>Apache version : 2.4.6,
OS : Centos</strong></p>

<p>I am trying to install SSL on my Amazon EC2 CentOS but I am having error while restarting httpd after changing httpd config file.</p>

<p>httpd config:</p>

<pre><code>#craveinn
&lt;VirtualHost *:443&gt;
    SSLEngine on
    SSLCertificateFile /etc/ssl/certs/crave.demo.crt
    SSLCertificateKeyFile /etc/ssl/certs/crave.demo.key
    DocumentRoot /var/www/craveinn.com/public_html
    ServerName craveinn.com
    ServerAlias www.craveinn.com
&lt;/VirtualHost&gt;
</code></pre>

<p>Trying to restart httpd</p>

<pre><code>[centos@ip-xxx-xx-xx-xxx conf]$ sudo service httpd restart
Redirecting to /bin/systemctl restart  httpd.service
Job for httpd.service failed because the control process exited with error code. See ""systemctl status httpd.service"" and ""journalctl -xe"" for details.
</code></pre>

<p>Checking the error</p>

<pre><code>[centos@ip-xxx-xx-xx-xxx conf]$ systemctl status -l httpd.service
● httpd.service - The Apache HTTP Server
   Loaded: loaded (/usr/lib/systemd/system/httpd.service; enabled; vendor preset: disabled)
   Active: failed (Result: exit-code) since Thu 2016-11-24 18:19:00 UTC; 4s ago
     Docs: man:httpd(8)
           man:apachectl(8)
  Process: 30035 ExecStop=/bin/kill -WINCH ${MAINPID} (code=exited, status=1/FAILURE)
  Process: 15440 ExecReload=/usr/sbin/httpd $OPTIONS -k graceful (code=exited, status=0/SUCCESS)
  Process: 30034 ExecStart=/usr/sbin/httpd $OPTIONS -DFOREGROUND (code=exited, status=1/FAILURE)
 Main PID: 30034 (code=exited, status=1/FAILURE)

Nov 24 18:19:00 ip-xxx-xx-xx-xxx.eu-central-1.compute.internal systemd[1]: Starting The Apache HTTP Server...
Nov 24 18:19:00 ip-xxx-xx-xx-xxx.eu-central-1.compute.internal systemd[1]: httpd.service: main process exited, code=exited, status=1/FAILURE
Nov 24 18:19:00 ip-xxx-xx-xx-xxx.eu-central-1.compute.internal kill[30035]: kill: cannot find process """"
Nov 24 18:19:00 ip-xxx-xx-xx-xxx.eu-central-1.compute.internal systemd[1]: httpd.service: control process exited, code=exited status=1
Nov 24 18:19:00 ip-xxx-xx-xx-xxx.eu-central-1.compute.internal systemd[1]: Failed to start The Apache HTTP Server.
Nov 24 18:19:00 ip-xxx-xx-xx-xxx.eu-central-1.compute.internal systemd[1]: Unit httpd.service entered failed state.
Nov 24 18:19:00 ip-xxx-xx-xx-xxx.eu-central-1.compute.internal systemd[1]: httpd.service failed.
</code></pre>
","<apache-2.2><centos><amazon-ec2><ssl><apache-2.4>","2016-11-24 17:53:27"
"884009","httpd virtual hosts redirecting to default installation page","<p>i defined</p>

<pre><code>NameVirtualHost *:80

&lt;VirtualHost *:80&gt;
    DocumentRoot ""/home/andrei06041990/sites/cosmeticremix""
    ServerName cosmeticremix.dev
    ServerAlias www.cosmeticremix.dev
    &lt;Directory ""/home/andrei06041990/sites/cosmeticremix""&gt;
        Options Indexes FollowSymLinks
        #AllowOverride All
        AllowOverride None
        Require all Granted
    &lt;/Directory&gt;
&lt;/VirtualHost&gt;
</code></pre>

<p>at the end of the /etc/httpd/conf/httpd.conf file
but when i access <code>http://www.cosmeticremix.dev/</code> in browser i end up with the default page of the apache server
how are virtual hosts defined?</p>
","<php><virtualhost><httpd>","2017-11-17 22:25:17"
"816965","Ubuntu Sendmail Gmail Relay verify=FAIL","<p>I realize there are a lot of posts on this topic, though after hours I am still unable to connect to gmail.</p>

<p>I noticed that the output does not include <code>DIGEST-MD5</code> listed, could this be an issue?</p>

<p>Any thoughts would be great appreciated, thank you.</p>

<p><strong>Testing output</strong> - <code>/usr/sbin/sendmail -Am -d60.5 -v -i -f EMAIL_FROM -- EMAIL_TO</code></p>

<p>Replacements: HOST, EMAIL_TO, EMAIL_FROM - to hide my info</p>

<pre><code>map_lookup(host, gmail.com, %0=gmail.com) =&gt; gmail.com. (0)
map_lookup(host, gmail.com, %0=gmail.com) =&gt; gmail.com. (0)
map_lookup(host, hotmail.com, %0=hotmail.com) =&gt; hotmail.com. (0)
map_lookup(host, gmail.com, %0=gmail.com) =&gt; gmail.com. (0)
EMAIL_TO@hotmail.com... Connecting to smtp.gmail.com port 587 via relay...
220 smtp.gmail.com ESMTP c142sm62872634pfb.23 - gsmtp
&gt;&gt;&gt; EHLO host.HOST.us
250-smtp.gmail.com at your service, [2604:880:a:6::49]
250-SIZE 35882577
250-8BITMIME
250-STARTTLS
250-ENHANCEDSTATUSCODES
250-PIPELINING
250-CHUNKING
250 SMTPUTF8
map_lookup(access, Try_TLS:smtp.gmail.com, %0=Try_TLS:smtp.gmail.com) =&gt; NOT FOUND (0)
map_lookup(access, Try_TLS:gmail.com, %0=Try_TLS:gmail.com) =&gt; NOT FOUND (0)
map_lookup(access, Try_TLS:com, %0=Try_TLS:com) =&gt; NOT FOUND (0)
map_lookup(access, Try_TLS:IPv6:2607:f8b0:400e:c04::6d, %0=Try_TLS:IPv6:2607:f8b0:400e:c04::6d) =&gt; NOT FOUND (0)
map_lookup(access, Try_TLS:IPv6:2607:f8b0:400e:c04, %0=Try_TLS:IPv6:2607:f8b0:400e:c04) =&gt; NOT FOUND (0)
map_lookup(access, Try_TLS:IPv6:2607:f8b0:400e, %0=Try_TLS:IPv6:2607:f8b0:400e) =&gt; NOT FOUND (0)
map_lookup(access, Try_TLS:IPv6:2607:f8b0, %0=Try_TLS:IPv6:2607:f8b0) =&gt; NOT FOUND (0)
map_lookup(access, Try_TLS:IPv6:2607, %0=Try_TLS:IPv6:2607) =&gt; NOT FOUND (0)
map_lookup(access, Try_TLS:IPv6, %0=Try_TLS:IPv6) =&gt; NOT FOUND (0)
map_lookup(access, Try_TLS:, %0=Try_TLS:) =&gt; NOT FOUND (0)
&gt;&gt;&gt; STARTTLS
220 2.0.0 Ready to start TLS
map_lookup(macro, {TLS_Name}, %0={TLS_Name}, %1=smtp.gmail.com) =&gt;  (0)
map_lookup(access, TLS_Srv:smtp.gmail.com, %0=TLS_Srv:smtp.gmail.com) =&gt; NOT FOUND (0)
map_lookup(access, TLS_Srv:gmail.com, %0=TLS_Srv:gmail.com) =&gt; NOT FOUND (0)
map_lookup(access, TLS_Srv:com, %0=TLS_Srv:com) =&gt; NOT FOUND (0)
map_lookup(access, TLS_Srv:IPv6:2607:f8b0:400e:c04::6d, %0=TLS_Srv:IPv6:2607:f8b0:400e:c04::6d) =&gt; NOT FOUND (0)
map_lookup(access, TLS_Srv:IPv6:2607:f8b0:400e:c04, %0=TLS_Srv:IPv6:2607:f8b0:400e:c04) =&gt; NOT FOUND (0)
map_lookup(access, TLS_Srv:IPv6:2607:f8b0:400e, %0=TLS_Srv:IPv6:2607:f8b0:400e) =&gt; NOT FOUND (0)
map_lookup(access, TLS_Srv:IPv6:2607:f8b0, %0=TLS_Srv:IPv6:2607:f8b0) =&gt; NOT FOUND (0)
map_lookup(access, TLS_Srv:IPv6:2607, %0=TLS_Srv:IPv6:2607) =&gt; NOT FOUND (0)
map_lookup(access, TLS_Srv:IPv6, %0=TLS_Srv:IPv6) =&gt; NOT FOUND (0)
map_lookup(access, TLS_Srv:, %0=TLS_Srv:) =&gt; NOT FOUND (0)
&gt;&gt;&gt; EHLO host.HOST.us
250-smtp.gmail.com at your service, [2604:880:a:6::49]
250-SIZE 35882577
250-8BITMIME
250-AUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH
250-ENHANCEDSTATUSCODES
250-PIPELINING
250-CHUNKING
250 SMTPUTF8
map_lookup(authinfo, AuthInfo:smtp.gmail.com, %0=AuthInfo:smtp.gmail.com) =&gt; ""U:root"" ""I:EMAIL_FROM@gmail.com"" ""P:ThePwd"" ""M:LOGIN PLAIN"" (0)
&gt;&gt;&gt; AUTH LOGIN
334 VXNlcm5hbWU6
&gt;&gt;&gt; amNoYW1iZXJzLmRldkBnbWFpbC5jb20=
334 UGFzc3dvcmQ6
&gt;&gt;&gt; R2F0b3JhZGUxMw==
534-5.7.14 &lt;https://accounts.google.com/signin/continue?sarp=1&amp;scc=1&amp;plt=AKgnsbuS
534-5.7.14 Kjh2f9Ji2tN_QIHz37GgrtjUaZplTN6wTQCrTqS81MBGxZ_06YW_UStfTEeNrr22ojuR1m
534-5.7.14 hL5QeQZthDLzX_YFsm_CCKakd5IgiVSJR_q9P3oHWVG3vku1bZfyfjL-1H9W2asQNeSErX
534-5.7.14 TKklx2lrQouxJRfUDwjDKgEoRuNT2Sepk9ivzWTyAz8ts_Y6X7ZrZmKaTChYop6nA7UI8O
534-5.7.14 egJYhLam_mut3Dy7fdNcadDlFs6hM&gt; Please log in via your web browser and
534-5.7.14 then try again.
534-5.7.14  Learn more at
534 5.7.14  https://support.google.com/mail/answer/78754 c142sm62872634pfb.23 - gsmtp
&gt;&gt;&gt; AUTH PLAIN cm9vdABqY2hhbWJlcnMuZGV2QGdtYWlsLmNvbQBHYXRvcmFkZTEz
534-5.7.14 &lt;https://accounts.google.com/signin/continue?sarp=1&amp;scc=1&amp;plt=AKgnsbtA
534-5.7.14 ZCJ1UdQ2ZBQLxWkMA8M5RLayX61Hbjuoqa-OwEBAynkDlLtuBK3e-UMiOCVnmhtoTLq0_O
534-5.7.14 KayzsmxccKQ8ak2jO5qzEdL6DuZh5KpkbUpIusonC-FpfGjq162R0gzQi1jKB-SmAkh3lG
534-5.7.14 ezDlPu5uxJXUQY3gGqmLD4DWBuTAscp5NheklEtCzg1dqbkkjBnXZlWdEyMLJrS20RKSU8
534-5.7.14 hcAKMtbvybC4BBchJlj7FH0Wpw4HA&gt; Please log in via your web browser and
534-5.7.14 then try again.
534-5.7.14  Learn more at
534 5.7.14  https://support.google.com/mail/answer/78754 c142sm62872634pfb.23 - gsmtp
&gt;&gt;&gt; MAIL From:&lt;EMAIL_FROM@gmail.com&gt; SIZE=77
530-5.5.1 Authentication Required. Learn more at
530 5.5.1  https://support.google.com/mail/?p=WantAuthError c142sm62872634pfb.23 - gsmtp
map_lookup(host, gmail.com, %0=gmail.com) =&gt; gmail.com. (0)
map_lookup(host, gmail.com, %0=gmail.com) =&gt; gmail.com. (0)
map_lookup(dequote, MAILER-DAEMON, %0=MAILER-DAEMON) =&gt; NOT FOUND (0)
map_lookup(host, gmail.com, %0=gmail.com) =&gt; gmail.com. (0)
map_lookup(host, hotmail.com, %0=hotmail.com) =&gt; hotmail.com. (0)
EMAIL_FROM@gmail.com... Using cached ESMTP connection to smtp.gmail.com via relay...
&gt;&gt;&gt; RSET
250 2.1.5 Flushed c142sm62872634pfb.23 - gsmtp
&gt;&gt;&gt; MAIL From:&lt;&gt;
530-5.5.1 Authentication Required. Learn more at
530 5.5.1  https://support.google.com/mail/?p=WantAuthError c142sm62872634pfb.23 - gsmtp
map_lookup(dequote, MAILER-DAEMON, %0=MAILER-DAEMON) =&gt; NOT FOUND (0)
map_lookup(dequote, postmaster, %0=postmaster) =&gt; NOT FOUND (0)
map_lookup(dequote, MAILER-DAEMON, %0=MAILER-DAEMON) =&gt; NOT FOUND (0)
map_lookup(host, gmail.com, %0=gmail.com) =&gt; gmail.com. (0)
map_lookup(dequote, MAILER-DAEMON, %0=MAILER-DAEMON) =&gt; NOT FOUND (0)
map_lookup(host, gmail.com, %0=gmail.com) =&gt; gmail.com. (0)
MAILER-DAEMON... Saved message in /var/lib/sendmail/dead.letter
Closing connection to smtp.gmail.com
&gt;&gt;&gt; QUIT
221 2.0.0 closing connection c142sm62872634pfb.23 - gsmtp
</code></pre>

<p><strong>sendmail.mc</strong> - showing only what I changed</p>

<pre><code>MAILER_DEFINITIONS
dnl#
define(`SMART_HOST',`smtp.gmail.com')dnl
define(`RELAY_MAILER_ARGS', `TCP $h 587')dnl
define(`ESMTP_MAILER_ARGS', `TCP $h 587')dnl
define(`confAUTH_MECHANISMS', `EXTERNAL DIGEST-MD5 CRAM-MD5 LOGIN PLAIN')dnl
TRUST_AUTH_MECH(`EXTERNAL DIGEST-MD5 CRAM-MD5 LOGIN PLAIN')dnl
dnl#
FEATURE(`authinfo',`hash /etc/mail/auth/auth-info.db')dnl
dnl#
define(`CERT_DIR', `MAIL_SETTINGS_DIR`'certs')dnl
define(`confCACERT_PATH', `CERT_DIR')dnl
define(`confCACERT', `CERT_DIR/ca-cert.crt')dnl
define(`confSERVER_KEY', `CERT_DIR/sendmail.pem')dnl
define(`confSERVER_CERT', `CERT_DIR/sendmail.pem')dnl
define(`confCLIENT_CERT', `CERT_DIR/sendmail.pem')dnl
define(`confCLIENT_KEY', `CERT_DIR/sendmail.pem')dnl
dnl #
define(`confAUTH_OPTIONS', `A p y')dnl
MAILER(`local')dnl
MAILER(`smtp')dnl
</code></pre>

<p><strong>Setup script</strong></p>

<pre><code>#!/bin/bash

cd /etc/mail

echo
echo

if [ ! -d ""/etc/mail/auth"" ]; then
  /bin/mkdir /etc/mail/auth
fi

rm -rf /var/spool/mqueue/*
rm -rf /var/spool/mqueue-client/*
rm -rf /etc/mail/certs


if [ ! -d ""/etc/mail/certs"" ]; then
    echo ""CERT""
    echo

    /bin/mkdir /etc/mail/certs
    cd /etc/mail/certs

    /usr/bin/openssl req \
        -x509 -nodes -days 365 -sha384 \
        -subj '/C=US/ST=California/L=Westminster/CN=host.DOMAIN.us' \
        -newkey rsa:2048 -keyout ca-cert.pem -out ca-cert.crt

    /usr/bin/openssl req \
        -x509 -nodes -days 365 -sha384 \
        -subj '/C=US/ST=California/L=Westminster/CN=host.DOMAIN.us' \
        -newkey rsa:2048 -keyout sendmail.pem -out sendmail.pem

    /bin/cat /srv/scripts/ca-google.txt &gt;&gt; ./ca-cert.crt

    /bin/chmod 0600 ./*

    cd /etc/mail

    echo
fi



echo ""AuthInfo:smtp.gmail.com  \""U:root\"" \""I:EMAIL_FROM@gmail.com\"" \""P:ThePwd\"" "" &gt; ./auth/auth-info
echo ""AuthInfo: \""U:root\"" \""I:EMAIL_FROM@gmail.com\"" \""P:ThePwd\"" "" &gt;&gt; ./auth/auth-info
/usr/sbin/makemap hash ./auth/auth-info.db &lt; ./auth/auth-info
/bin/chmod 0600 ./auth/*

/bin/cp /srv/scripts/sendmail.mc /etc/mail/sendmail.mc
/bin/cp /srv/scripts/sendmail.mc /usr/share/sendmail/cf/debian/sendmail.mc
/usr/bin/m4 sendmail.mc &gt; sendmail.cf
#/usr/bin/make -C /etc/mail
/bin/sh /etc/init.d/sendmail restart

echo
echo
</code></pre>

<p><strong>CA-Google.txt</strong></p>

<pre><code>-----BEGIN CERTIFICATE-----
MIIEgDCCA2igAwIBAgIIeD5JWPwgbC4wDQYJKoZIhvcNAQELBQAwSTELMAkGA1UE
BhMCVVMxEzARBgNVBAoTCkdvb2dsZSBJbmMxJTAjBgNVBAMTHEdvb2dsZSBJbnRl
cm5ldCBBdXRob3JpdHkgRzIwHhcNMTYxMTEwMTU1MjM4WhcNMTcwMjAyMTUzMTAw
WjBoMQswCQYDVQQGEwJVUzETMBEGA1UECAwKQ2FsaWZvcm5pYTEWMBQGA1UEBwwN
TW91bnRhaW4gVmlldzETMBEGA1UECgwKR29vZ2xlIEluYzEXMBUGA1UEAwwOc210
cC5nbWFpbC5jb20wggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCu8hp6
49Q6yFc1oXjE/BF0rmAgNwa3LjlzyFos46FPOhTG+4r+jyhOdjvBpdp7TA+3vI5y
3nhqVXl00EvhIvOQ7w0cMpOCRYtFSUIP2//eSuCFENL4mr+DeY8QPIL48Pg33tCT
laUGVGrSEkNId3Sh/TfWwvW4LzPXzYkWZ/oBOp6yXHWN2pqdaY1xQMWleBBGT0g0
pRzoN5iehiThFddu4XpLyT6Tz6hoj2ri1r9LlrOZF7ZR1aNhXTcGcw5LLI/Ap9Tm
R7FhSR5XYiUCmkij81Ra7lGHxyCbbFgs+Dug2o9jKp9CoNTKjUvSkVV1qZ/Too80
lEFuAA6pXctH5s7zAgMBAAGjggFLMIIBRzAdBgNVHSUEFjAUBggrBgEFBQcDAQYI
KwYBBQUHAwIwGQYDVR0RBBIwEIIOc210cC5nbWFpbC5jb20waAYIKwYBBQUHAQEE
XDBaMCsGCCsGAQUFBzAChh9odHRwOi8vcGtpLmdvb2dsZS5jb20vR0lBRzIuY3J0
MCsGCCsGAQUFBzABhh9odHRwOi8vY2xpZW50czEuZ29vZ2xlLmNvbS9vY3NwMB0G
A1UdDgQWBBRe+GRFPyCZGExoLYvnGMSjpmjBcjAMBgNVHRMBAf8EAjAAMB8GA1Ud
IwQYMBaAFErdBhYbvPZotXb1gba7Yhq6WoEvMCEGA1UdIAQaMBgwDAYKKwYBBAHW
eQIFATAIBgZngQwBAgIwMAYDVR0fBCkwJzAloCOgIYYfaHR0cDovL3BraS5nb29n
bGUuY29tL0dJQUcyLmNybDANBgkqhkiG9w0BAQsFAAOCAQEAgCIi5DL0iR9gJYyU
1uiXGwEr2RkSS8I8t1Ep7CSKcnZXCvLkqzUBhie3mzBb8IvQoiihiTMtCzT80pGZ
bWJvfRcRcrV3BT6hh9y2gW4kmVwkNyJKSQsAs5bMfgnQ1K4YgPhDx9ZVhtX64cvE
aKlMaoW7boX/Y+WEJDLI846+qXIja5Yj29GQbP3v1wZsVykkx+RpIMlVgnpqKIgb
erKxn6cpIvc99hkM3s5ssCrCul9H3a+/2uKp3gkliyRcGSq+3Ksoch/H/7DEdVPg
mbHjTW/y8b3+shfV1F3aReJDaL9rpw6dvGGgjR7hFHTYwDZ8wn5XOwSkBoq1cnm3
aNQWGA==
-----END CERTIFICATE-----
</code></pre>

<p><strong>Log output</strong></p>

<pre><code>Nov 24 19:44:45 host sendmail[14916]: uAP0ijkb014916: from=EMAIL_FROM@gmail.com, size=77, class=0, nrcpts=1, msgid=&lt;201611250044.uAP0ijkb014916@host.DOMAIN.us&gt;, relay=root@localhost
Nov 24 19:44:45 host sendmail[14916]: STARTTLS=client, relay=smtp.gmail.com, version=TLSv1/SSLv3, verify=FAIL, cipher=ECDHE-RSA-AES128-GCM-SHA256, bits=128/128
Nov 24 19:44:46 host sendmail[14916]: uAP0ijkb014916: to=EMAIL_TO@hotmail.com, ctladdr=EMAIL_FROM@gmail.com (0/0), delay=00:00:01, xdelay=00:00:01, mailer=relay, pri=30077, relay=smtp.gmail.com [IPv6:2607:f8b0:400e:c04::6d], dsn=5.0.0, stat=Service unavailable
Nov 24 19:44:46 host sendmail[14916]: uAP0ijkb014916: uAP0ijkc014916: DSN: Service unavailable
Nov 24 19:44:46 host sendmail[14916]: uAP0ijkc014916: to=EMAIL_FROM@gmail.com, delay=00:00:00, xdelay=00:00:00, mailer=relay, pri=30000, relay=smtp.gmail.com, dsn=5.0.0, stat=Service unavailable
Nov 24 19:44:46 host sendmail[14916]: uAP0ijkc014916: to=MAILER-DAEMON, delay=00:00:00, mailer=local, pri=30000, dsn=5.1.1, stat=User unknown
Nov 24 19:44:46 host sendmail[14916]: uAP0ijkc014916: to=postmaster, delay=00:00:00, mailer=local, pri=30000, dsn=5.1.1, stat=User unknown
Nov 24 19:44:46 host sendmail[14916]: uAP0ijkc014916: uAP0ijkd014916: return to sender: User unknown
Nov 24 19:44:46 host sendmail[14916]: uAP0ijkd014916: to=MAILER-DAEMON, delay=00:00:00, mailer=local, pri=0, dsn=5.1.1, stat=User unknown
Nov 24 19:44:46 host sendmail[14916]: uAP0ijkc014916: Saved message in /var/lib/sendmail/dead.letter
Nov 24 19:45:01 host sendmail[14940]: uAP0j1VI014940: from=root, size=261, class=0, nrcpts=1, msgid=&lt;201611250045.uAP0j1VI014940@host.DOMAIN.us&gt;, relay=root@localhost
Nov 24 19:45:01 host sendmail[14946]: uAP0j12W014946: from=root, size=264, class=0, nrcpts=1, msgid=&lt;201611250045.uAP0j12W014946@host.DOMAIN.us&gt;, relay=root@localhost
</code></pre>

<p>Cretits:</p>

<p><a href=""https://www.madboa.com/geek/openssl/#key-removepass"" rel=""nofollow noreferrer"">https://www.madboa.com/geek/openssl/#key-removepass</a></p>

<p><a href=""https://linuxconfig.org/configuring-gmail-as-sendmail-email-relay"" rel=""nofollow noreferrer"">https://linuxconfig.org/configuring-gmail-as-sendmail-email-relay</a></p>

<hr>

<p><strong>Edit:</strong></p>

<p>I updated the script to the correct one.</p>

<p>After Andrzej added the suggestion about checking the ca-certs I found the gmail ca-cert and added that in.</p>

<p>Next Andrzej suggested removing <code>"" M: LOGIN PLAIN ""</code> from <code>echo ""AuthInfo:smtp.gmail.com  \""U:root\"" \""I:EMAIL_FROM@gmail.com\"" \""P:ThePwd\"" "" &gt; ./auth/auth-info</code> and that did it.</p>

<p>Thank again, this was a frustrating one.</p>
","<linux><sendmail><gmail>","2016-11-25 01:19:11"
"884063","openvpn access LAN behind client behind nat and dynamic IP from other VPN clients","<p>I've read and tried many guides, including the <a href=""https://openvpn.net/index.php/open-source/documentation/howto.html#scope"" rel=""nofollow noreferrer"">official guide about this</a>, but I'm still unable to get this to work. Here's the setup:</p>

<pre><code>(client1 LAN: 192.168.10.0/24
(OpenVPN client: client1, 192.168.10.101)
             |
             |
             v
       (NAT router,
        *dynamic* internet IP: 178.1.2.3)
             |
             |
     (...internet...)
             |
             |
             v
    (OpenVPN server,
     *static* IP:
     1.2.3.4)      &lt;--- (...internet...)  &lt;--- (OpenVPN client,
                                                *dynamic* internet IP:
                                                client2, 88.1.2.3)
</code></pre>

<p>What I want is for client2 to be able to access ALL of the client1's network. As I said in EDIT #2 below, I got it to work properly only by setting static routes on all machines in the clien1 LAN (for some reason, a static route on the NAT router doesn't quite work, see EDIT #2 below).</p>

<p>All machines run Ubuntu 16.04 with OpenVPN 2.3.10. The NAT router is a TP-Link MR200 In case it matters, client1 runs a KVM virtual machine server, and the OpenVPN server runs a docker container.</p>

<p>The tunnel works fine between the server, client1 and client2. client2 can also ping 192.168.10.101 but cannot ping 192.168.10.211, for example. I have dedicated certificates for each client which work fine and I enabled IP forwarding on the openvpn server and on client1.</p>

<p>This is the server's config:</p>

<pre><code># cat /etc/openvpn/graphyc.conf
server 10.8.0.0 255.255.255.0
verb 3
key server-key.pem
ca ca.pem
cert server-cert.pem
dh dh.pem
keepalive 10 120
persist-key
persist-tun
comp-lzo

user nobody
group nogroup

# dir ""clients"" is chown'ed to nobody:nogroup
client-config-dir clients

client-to-client
push ""route 192.168.10.0 255.255.255.0""
route 192.168.10.0 255.255.255.0

proto udp
port 1194
dev tun
</code></pre>

<p>content of <code>/etc/openvpn/client1</code> on the server (static vpn lease)</p>

<pre><code># cat /etc/openvpn/clients/client1
ifconfig-push 10.8.0.101 10.8.0.5
iroute 192.168.10.0 255.255.255.0
</code></pre>

<p>content of client1's config</p>

<pre><code># cat /etc/openvpn/client1.conf
client
remote 1.2.3.4 1194 udp
nobind
dev tun
comp-lzo yes
verb 3
explicit-exit-notify 5    
key /etc/openvpn/client1-key.pem
cert /etc/openvpn/client1-cert.pem
ca /etc/openvpn/ca.pem
</code></pre>

<p>content of client2's config</p>

<pre><code># cat /etc/openvpn/client2.conf
client
remote 1.2.3.4 1194 udp
nobind
dev tun
comp-lzo yes
verb 3
explicit-exit-notify 5    
key /etc/openvpn/client2-key.pem
cert /etc/openvpn/client2-cert.pem
ca /etc/openvpn/ca.pem
</code></pre>

<p>I can ping 192.168.10.101 from client2, but cannot ping 192.168.10.211 for instance. When trying the latter, the server sees the ping and routes it to the internet IP of the client1's router 178.1.2.3:</p>

<pre><code>Sat Nov 18 11:48:29 2017 client2/88.1.2.3:48069 MULTI: Learn: 192.168.10.211 -&gt; client1/178.1.2.3:52928
</code></pre>

<p>The official guide says that I should also add a route on the NAT router. Specifically it says: ""<em>if the client machine running OpenVPN is not also the gateway for the client LAN, then the gateway for the client LAN must have a route which directs all subnets which should be reachable through the VPN to the OpenVPN client machine</em>"".</p>

<p>However, I am unable to do so right now as I don't have access to the router's web interface, but I also am unsure what route to add exactly on the NAT router (i'm not sure this is needed since I don't need to access the LAN behind the server).</p>

<p>I'd be grateful for some concrete steps that I still need to do (or correct).</p>

<h3>EDIT:**</h3>

<p>Here are the routing tables for the server, client1 and client2 after they connect to the OpenVPN server.</p>

<p>server:</p>

<pre><code># route
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
default         1.1.1.1         0.0.0.0         UG    0      0        0 eth0
10.8.0.0        10.8.0.2        255.255.255.0   UG    0      0        0 tun
10.8.0.2        *               255.255.255.255 UH    0      0        0 tun
localnet        *               255.255.255.0   U     0      0        0 eth0
link-local      *               255.255.0.0     U     1000   0        0 eth0
172.17.0.0      *               255.255.0.0     U     0      0        0 docker0
192.168.10.0    10.8.0.2        255.255.255.0   UG    0      0        0 tun
</code></pre>

<p>client1:</p>

<pre><code># route
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
default         192.168.10.1    0.0.0.0         UG    0      0        0 eno1
10.8.0.0        10.8.0.5        255.255.255.0   UG    0      0        0 tun
10.8.0.5        *               255.255.255.255 UH    0      0        0 tun
link-local      *               255.255.0.0     U     1000   0        0 eno1
192.168.10.0    *               255.255.255.0   U     0      0        0 eno1
192.168.122.0   *               255.255.255.0   U     0      0        0 virbr0
</code></pre>

<p>client2:</p>

<pre><code># route
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
default         178.1.1.1       0.0.0.0         UG    0      0        0 eth0
10.8.0.0        10.8.0.5        255.255.255.0   UG    0      0        0 tun
10.8.0.5        *               255.255.255.255 UH    0      0        0 tun
88.1.2.0        *               255.255.255.0   U     0      0        0 eth0
192.168.10.0    10.8.0.5        255.255.255.0   UG    0      0        0 tun
</code></pre>

<h3>EDIT #2:</h3>

<p>Ok, this is weird (to me): I added a static route on the NAT router to route all packets destined for 10.8.0.0/24 via 192.168.10.101. I then tried to ping 192.168.10.211 from client2 and it didn't work. I then logged into 192.168.10.211 and ping'ed client2 on 10.8.0.6, and it said:</p>

<pre><code># ping 10.8.0.6
PING 10.8.0.6 (10.8.0.6) 56(84) bytes of data.
64 bytes from 10.8.0.6: icmp_seq=1 ttl=63 time=83.5 ms
From 192.168.10.1: icmp_seq=2 Redirect Host(New nexthop: 192.168.10.101)
64 bytes from 10.8.0.6: icmp_seq=2 ttl=63 time=75.4 ms
64 bytes from 10.8.0.6: icmp_seq=3 ttl=63 time=79.9 ms
</code></pre>

<p>Whoa! Hold on ... I then when back to client2 and ping'ed 192.168.10.211 and it now works:</p>

<pre><code># ping 192.168.10.201
PING 192.168.10.201 (192.168.10.201) 56(84) bytes of data.
64 bytes from 192.168.10.201: icmp_seq=1 ttl=63 time=78.6 ms
64 bytes from 192.168.10.201: icmp_seq=2 ttl=63 time=77.5 ms
64 bytes from 192.168.10.201: icmp_seq=3 ttl=63 time=75.1 ms
</code></pre>

<p>It looks like the NAT router doesn't route just the ping reply, possibly because it never saw the forward ping packet -- and I tried to see if it has a stateful firewall that can be disabled, but it does not.</p>

<p>I then went on another machine 192.168.10.207 and created a static route on itself specifically for 10.8.0.0/24 via 192.168.10.101 (overrides the NAT router):</p>

<pre><code>route add -net 10.8.0.0 netmask 255.255.255.0 gw 192.168.10.101
</code></pre>

<p>And then ping'ing 192.168.10.207 from client2 worked fine.</p>

<p>Isn't there a way to avoid creating static rules on every local machine in the client1 LAN? Should the rule on the NAT router just work, but for some reason it does not?</p>
","<openvpn>","2017-11-18 12:06:33"
"959936","Email server multiple domain with different IPs","<p>I have 2 domains:</p>

<ol>
<li><code>ex1.example</code> with IP <code>192.0.2.1</code></li>
<li><code>ex2.example</code> with IP <code>203.0.113.2</code></li>
</ol>

<p>On <code>ex1.example</code> I installed <code>postfix</code> and <code>dovecot</code> with support working on multiple domains (MariaDB with tables and other stuff). Now I want to receive email for the following email addresses:</p>

<ul>
<li><code>support@ex1.example</code></li>
<li><code>postmaster@ex1.example</code></li>
<li><code>support@ex2.example</code></li>
<li><code>postmaster@ex2.example</code></li>
</ul>

<p>With <code>postmaster@ex1.example</code> and <code>support@ex1.example</code> everything okay, I can receive emails, but when I try to send email to <code>support@ex2.example</code> or <code>postmaster@ex2.example</code> - nothing happens.</p>

<p>DNS zone settings for domain ex1.com:</p>

<ul>
<li>A Record @ <code>192.0.2.1</code></li>
<li>A Record mail <code>192.0.2.1</code></li>
<li>A Record www <code>192.0.2.1</code></li>
<li>MX Record <code>ex1.example</code> <code>mail.ex1.example.</code></li>
<li>MX Record <code>mail.ex1.example</code> <code>ex1.example.</code></li>
</ul>

<p>DNS zone settings for domain ex2.com:</p>

<ul>
<li>A Record @ <code>203.0.113.2</code></li>
<li>A Record mail <code>192.0.2.1</code></li>
<li>MX Record <code>ex2.example</code> <code>mail.ex1.example.</code></li>
</ul>

<p>When I try to send an email to <code>support@mail.ex2.example</code> I got an error in maillot:</p>

<pre><code>Mar 25 22:47:02 ex1 postfix/smtpd[6000]: NOQUEUE: reject: RCPT from: 454 4.7.1 &lt;support@mail.ex2.example&gt;: Relay access denied; from=&lt;user@domain.localdomain&gt; to=&lt;support@mail.ex2.example&gt; proto=ESMTP helo=&lt;user.localdomain&gt;
</code></pre>
","<domain-name-system><email><postfix>","2019-03-25 22:50:57"
"817306","Circular dependency when trying to install openssl and openssl-libs on CentOS","<p>I want to install my c++ library on my CentOS 7.2 server. When I'm trying to compile it with g++ The compiler says this error <code>invalid use of incomplete type ‘HMAC_CTX {aka struct hmac_ctx_st}</code>.
I googled the error and itis reletad to an old version of OpenSSL. </p>

<p>When I'm trying to install the newest OpenSSL version (1.1.0c) the rmp requires to install <code>libssl.so.1.1()(64bit)</code>. </p>

<p>I tried to install the newest version with <code>yum install openssl-libs-1.1.0c-2.fc26.x86_64.rpm</code>. When I'm installing <code>openssl-libs</code> he rpm requires to install <code>libssl.so.10()(64bit)</code>.</p>

<p>How can I resolve the circular dependency between them?</p>
","<centos><openssl>","2016-11-27 12:51:30"
"884438","Is there any way to find repeating url using rewriterule?","<p>My url is <a href=""http://www.abctest.com/mysamples/"" rel=""nofollow noreferrer"">http://www.abctest.com/mysamples/</a> . I need to replace this with <a href=""http://www.abctest.net/mysamples/"" rel=""nofollow noreferrer"">http://www.abctest.net/mysamples/</a> .
Please let me know how to write a regular expression to match the repeated pattern (www.abctest.com/mysamples) and replace it with <a href=""http://www.abctest.net/mysamples"" rel=""nofollow noreferrer"">http://www.abctest.net/mysamples</a> using RewriteRule.</p>
","<iis><apache-2.4>","2017-11-21 11:25:59"
"960097","nginx server_name with directories ""domain.com/directory1/directory2""","<p>I'm trying to use Nginx as a reverse proxy and I have the following site config:</p>

<pre><code>server {
    listen 80;

    listen [::]:80;

    server_name domain.com/directory1/directory2;

    location / {

        proxy_pass http://google.com;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;

    }

 }
</code></pre>

<p>when I run nginx -t, I'm getting the following information:</p>

<pre><code>nginx: [warn] server name ""domain.com/directory1/directory2"" has suspicious symbols in /etc/nginx/sites-enabled/default:4
</code></pre>

<p>I was wondering what is the right syntax for the server name so I can actually use this kind of URL in my server_name?</p>

<p>Thank you</p>
","<nginx>","2019-03-26 18:01:00"
"884514","OpenSSH ignores ""AllowTcpForwarding no"" in sshd_config","<p>On ubuntu 16.04.3 / OpenSSH_7.2p2 disabling Tunneling globally
by setting these values in /etc/ssh/sshd_config has no effect and Dynamic / Local tunnels still work</p>

<pre><code>AllowAgentForwarding no
AllowTcpForwarding no
AllowStreamLocalForwarding no
PermitOpen none
PermitTunnel no
X11Forwarding no
</code></pre>

<p>Update:
I can only make it work by using ""Match"" on selected User(s)</p>

<pre><code>Match User zuser
  AllowTcpForwarding no
  X11Forwarding no
  AllowAgentForwarding no
  AllowStreamLocalForwarding no
  PermitOpen none
  PermitTunnel no
</code></pre>
","<ssh><ssh-tunnel><ubuntu-16.04>","2017-11-21 18:14:43"
"817581","My site is HTTPS and want only a URL with HTTP with Nginx","<p>I have a site in a Nginx server forces HTTPS for all site, but i need only an URL in HTTP.</p>

<p>This is the URL:</p>

<pre><code>https://site.meudominio.com.br/index.php?route=primeiro/segundo/terceiro/funcao
</code></pre>

<p>I want it to be forced to stay in HTTP just like this and hold:</p>

<pre><code>http://site.meudominio.com.br/index.php?route=primeiro/segundo/terceiro/funcao
</code></pre>

<p>My config file is:</p>

<pre><code>server {

    listen      80;
    listen [::]:80;
    server_name site.meudominio.com.br;
    return 301 https://site.meudominio.com.br$request_uri;
}

server {
    listen      443 ssl http2;
    listen [::]:443 ssl http2;
    server_name site.meudominio.com.br;

    ssl on;
    ssl_certificate     /etc/ssl/fullchain.pem;
    ssl_certificate_key /etc/ssl/privkey.pem;

    include snippets/ssl-params.conf;

    root /var/www/meudominio.com.br/site;

    index index.html index.htm index.php;

    charset utf-8;

    location / {
         try_files $uri @meudominio;
    }

    location @meudominio {
      rewrite ^/(.+)$ /index.php?_route_=$1 last;
    }

location ~ \.php$ {
        fastcgi_split_path_info ^(.+\.php)(/.+)$;
        fastcgi_pass unix:/run/php/php-fpm.sock;
        fastcgi_index index.php;
        include fastcgi_params;
        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
        fastcgi_intercept_errors off;
        fastcgi_buffer_size 16k;
        fastcgi_buffers 4 16k;
    }

    location ~ /\.ht {
        deny all;
    }
}
</code></pre>
","<nginx><ssl><http><https><port-80>","2016-11-28 23:28:12"
"960385","My mongodb database was dropped by attacker","<p>I already setup mongod with authentication but forget to expose port 27017 to public but I don't understand why attacker can drop my database?</p>

<p>*** Updated, issued has been resolved, I forgot to re-enable auth after change something on config files and attacker random scripting execute drop database command on server which not enable auth.</p>

<p>Server Information:
MongoDB Server 4.0.3 on Ubuntu 16.04.5</p>
","<mongodb>","2019-03-28 07:49:28"
"817708","Workstation to server ping/connection not working(*more in description)","<p>While am pinging from workstation to the server it doesn't ping and showing <code>destination host unreachable</code>. However when am pinging from server it works, and now if I am pinging from the workstation it also works. And this is only for few minutes; the workstation goes to the old state after a few sec.</p>

<p><strong><em>In short if I need connection to server I am required to first ping from the server to workstation.</em></strong> Connection between other devices in the network works fine.</p>

<p>Firewalls are off, machines are in same network, workstation is not added to the domain.</p>

<p><strong>EDIT3:</strong> The server PC got another NIC and I enabled that. It's IP is <code>192.168.2.106</code>. Now the connection is seamless to <code>.106</code> and its still the same with <code>.68</code>(the other NIC which is every other person in the network connected to).</p>

<p>EDIT1: more details</p>

<p>When I ping from the server to the workstation it is creating an arp cache entry in the workstation for the server. And can access the server from workstation. After couple of minutes the entry for the server gets deleted from the arp cache and now it is unreachable. The connection can only be restored when I ping from server to workstation.</p>

<p>EDIT2:</p>

<p><strong><em>Workstation</em></strong>: windows 7 Ultimate</p>

<p>ipconfig info:</p>

<p><a href=""https://i.sstatic.net/OdR4P.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/OdR4P.png"" alt=""enter image description here""></a></p>

<p><strong><em>Server</em></strong>: Windows Server 2012 R2</p>

<p>ipconfig info:</p>

<p><a href=""https://i.sstatic.net/3EoKt.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3EoKt.png"" alt=""enter image description here""></a></p>
","<windows><networking><connection>","2016-11-29 16:38:05"
"817792","Cloudstack VM use more CPU than restricted","<p>I have a private cloudstack cloud with the following configuration:</p>

<p>host server:</p>

<ul>
<li>ubuntu 14.04</li>
<li>KVM hypervisor</li>
<li>2x 2400 MHz CPU (amd athlon 64 X2 Dual Core Processor 4600+)</li>
</ul>

<p>virtual machines:</p>

<ul>
<li>1x 500 MHz CPU (RESTRICTED use of 500mhz only by cloudstack)</li>
<li>ubuntu 14.04</li>
</ul>

<p>When i utilize the vm with the <code>stress</code> utility to 100% the cpu of the host shows a utilization of 50%. So the vm is using one full cpu core of the host.</p>

<p>Is this a bug of cloudstack? Maybe of KVM?</p>

<hr>

<p>UPDATE</p>

<p>this is vm configuration generated by cloudstack:</p>

<pre><code>&lt;domain type='kvm'&gt;
  &lt;name&gt;i-4-118-VM&lt;/name&gt;
  &lt;uuid&gt;0c795c99-5bab-46f8-a321-71e3e398036b&lt;/uuid&gt;
  &lt;description&gt;Ubuntu 14.04 (64-bit)&lt;/description&gt;
  &lt;memory unit='KiB'&gt;512000&lt;/memory&gt;
  &lt;currentMemory unit='KiB'&gt;512000&lt;/currentMemory&gt;
  &lt;vcpu placement='static'&gt;1&lt;/vcpu&gt;
  &lt;cputune&gt;
    &lt;shares&gt;500&lt;/shares&gt;
  &lt;/cputune&gt;
  &lt;resource&gt;
    &lt;partition&gt;/machine&lt;/partition&gt;
  &lt;/resource&gt;
  &lt;sysinfo type='smbios'&gt;
    &lt;system&gt;
      &lt;entry name='manufacturer'&gt;Apache Software Foundation&lt;/entry&gt;
      &lt;entry name='product'&gt;CloudStack KVM Hypervisor&lt;/entry&gt;
      &lt;entry name='uuid'&gt;0c795c99-5bab-46f8-a321-71e3e398036b&lt;/entry&gt;
    &lt;/system&gt;
  &lt;/sysinfo&gt;
  &lt;os&gt;
    &lt;type arch='x86_64' machine='pc-i440fx-trusty'&gt;hvm&lt;/type&gt;
    &lt;boot dev='cdrom'/&gt;
    &lt;boot dev='hd'/&gt;
    &lt;smbios mode='sysinfo'/&gt;
  &lt;/os&gt;
  &lt;features&gt;
    &lt;acpi/&gt;
    &lt;apic/&gt;
    &lt;pae/&gt;
  &lt;/features&gt;
  &lt;cpu&gt;
  &lt;/cpu&gt;
  &lt;clock offset='utc'&gt;
    &lt;timer name='kvmclock'/&gt;
  &lt;/clock&gt;
  &lt;on_poweroff&gt;destroy&lt;/on_poweroff&gt;
  &lt;on_reboot&gt;restart&lt;/on_reboot&gt;
  &lt;on_crash&gt;destroy&lt;/on_crash&gt;
  &lt;devices&gt;
    &lt;emulator&gt;/usr/bin/kvm-spice&lt;/emulator&gt;
    &lt;disk type='file' device='disk'&gt;
      &lt;driver name='qemu' type='qcow2' cache='none'/&gt;
      &lt;source file='/mnt/a5bb6304-61f7-3d9e-9706-1f447a6a5fdb/af0c4ab6-aaad-4990-9046-da6ac83a575f'/&gt;
      &lt;target dev='vda' bus='virtio'/&gt;
      &lt;serial&gt;af0c4ab6aaad49909046&lt;/serial&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/&gt;
    &lt;/disk&gt;
    &lt;disk type='file' device='cdrom'&gt;
      &lt;driver name='qemu' type='raw' cache='none'/&gt;
      &lt;target dev='hdc' bus='ide'/&gt;
      &lt;readonly/&gt;
      &lt;address type='drive' controller='0' bus='1' target='0' unit='0'/&gt;
    &lt;/disk&gt;
    &lt;controller type='usb' index='0'&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x2'/&gt;
    &lt;/controller&gt;
    &lt;controller type='pci' index='0' model='pci-root'/&gt;
    &lt;controller type='ide' index='0'&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/&gt;
    &lt;/controller&gt;
    &lt;interface type='bridge'&gt;
      &lt;mac address='06:0b:00:00:00:15'/&gt;
      &lt;source bridge='cloudbr0'/&gt;
      &lt;bandwidth&gt;
        &lt;inbound average='25600' peak='25600'/&gt;
        &lt;outbound average='25600' peak='25600'/&gt;
      &lt;/bandwidth&gt;
      &lt;model type='virtio'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/&gt;
    &lt;/interface&gt;
    &lt;serial type='pty'&gt;
      &lt;target port='0'/&gt;
    &lt;/serial&gt;
    &lt;console type='pty'&gt;
      &lt;target type='serial' port='0'/&gt;
    &lt;/console&gt;
    &lt;input type='tablet' bus='usb'/&gt;
    &lt;input type='mouse' bus='ps2'/&gt;
    &lt;input type='keyboard' bus='ps2'/&gt;
    &lt;graphics type='vnc' port='-1' autoport='yes' listen='10.0.0.11' passwd='-'&gt;
      &lt;listen type='address' address='10.0.0.11'/&gt;
    &lt;/graphics&gt;
    &lt;video&gt;
      &lt;model type='cirrus' vram='9216' heads='1'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/&gt;
    &lt;/video&gt;
    &lt;memballoon model='none'/&gt;
  &lt;/devices&gt;
  &lt;seclabel type='none'/&gt;
&lt;/domain&gt;
</code></pre>

<p>I HOPE SOMEONE CAN HELP OR GOT AN IDEA!</p>
","<virtualization><virtual-machines><kvm-virtualization><virsh><cloudstack>","2016-11-30 00:01:48"
"746396","error iptables-restore: unable to initialize table filter","<p>i have Debian 6.0.5 x86_64 on VPS  and create file named 'iptables.rules' with following command:</p>

<pre><code>*filter
-A INPUT -i lo -j ACCEPT
-A INPUT -d 127.0.0.0/8 ! -i lo -j REJECT --reject-with icmp-port-unreachable
-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT
-A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT
-A INPUT -p tcp -m state --state NEW -m tcp --dport 80 -j ACCEPT
-A INPUT -p tcp -m state --state NEW -m tcp --dport 465 -j ACCEPT
-A INPUT -p tcp -m state --state NEW -m tcp --dport 587 -j ACCEPT
-A INPUT -p tcp -m state --state NEW -m tcp --dport 993 -j ACCEPT
-A INPUT -p tcp -m state --state NEW -m tcp --dport 995 -j ACCEPT
-A INPUT -p tcp -m state --state NEW -m tcp --dport 2222 -j ACCEPT
-A INPUT -p tcp -m state --state NEW -m tcp --dport 8080 -j ACCEPT
-A INPUT -p esp -j ACCEPT
-A INPUT -p ah -j ACCEPT
-A INPUT -p udp --dport 500 -j ACCEPT
-A INPUT -p udp --dport 4500 -j ACCEPT
-A INPUT -i ipsec+ -p udp -m udp --dport l2tp -j ACCEPT
-A INPUT -j REJECT --reject-with icmp-port-unreachable
-A FORWARD -s 10.0.0.0/8 -p tcp -m multiport --dports 80,443,554,1935,7070,8000,8001,6971:6999 -j ACCEPT
-A FORWARD -s 10.0.0.0/8 -p udp -m multiport --dports 80,443,554,1935,7070,8000,8001,6971:6999 -j ACCEPT
-A FORWARD -s 10.0.0.0/8 -d 8.8.8.8 -p tcp --dport 53 -j ACCEPT
-A FORWARD -s 10.0.0.0/8 -d 8.8.8.8 -p udp --dport 53 -j ACCEPT
-A FORWARD -s 10.0.0.0/8 -d 8.8.4.4 -p tcp --dport 53 -j ACCEPT
-A FORWARD -s 10.0.0.0/8 -d 8.8.4.4 -p udp --dport 53 -j ACCEPT
-A FORWARD -s 10.0.0.0/8 -d 10.0.0.0/8 -j DROP
-A FORWARD -s 10.0.0.0/8 -j DROP 
-A OUTPUT -p tcp -m multiport --dports 53,80,443,554,1935,7070,8000,8001,6971:6999 -j ACCEPT
-A OUTPUT -p udp -m multiport --dports 53,80,443,554,1935,7070,8000,8001,6971:6999 -j ACCEPT
-A OUTPUT -p udp -m udp --dport 123 -j ACCEPT
-A OUTPUT -p tcp -m tcp --sport 22 -j ACCEPT
-A OUTPUT -p tcp -m tcp --sport 80 -j ACCEPT
-A OUTPUT -p tcp -m tcp --sport 465 -j ACCEPT
-A OUTPUT -p tcp -m tcp --sport 587 -j ACCEPT
-A OUTPUT -p tcp -m tcp --sport 993 -j ACCEPT
-A OUTPUT -p tcp -m tcp --sport 995 -j ACCEPT
-A OUTPUT -p tcp -m tcp --sport 2222 -j ACCEPT
-A OUTPUT -p tcp -m tcp --sport 8080 -j ACCEPT
-A OUTPUT -p esp -j ACCEPT
-A OUTPUT -p ah -j ACCEPT
-A OUTPUT -p udp --sport 500 -j ACCEPT
-A OUTPUT -p udp --sport 4500 -j ACCEPT
-A OUTPUT -o ipsec+ -p udp -m udp --dport l2tp -j ACCEPT
-A OUTPUT -j DROP
COMMIT

*nat
-A PREROUTING -i eth+ -p tcp --dport 443 -j DNAT --to-destination :8080
-A POSTROUTING -s 10.0.0.0/8 -o eth+ -j MASQUERADE
COMMIT  
</code></pre>

<p>but when i run <code>iptables-restore &lt; /etc/iptables.rules</code> command  ,putty show me error :  </p>

<pre><code> iptables-restore: unable to initialize table 'filter  
</code></pre>

<p>how can i solve this error? </p>
","<debian><iptables><putty>","2016-01-02 21:00:19"
"960867","Set up ns1, ns2 name servers at google cloud from a registered name server","<p>I have regestered ns1.grant-fullen.com and ns2.grant-fullen.com and (Proper IPS) at namecheap.com</p>

<p>I am using google cloud dns.
I have set up A records at google cloud dns.</p>

<p>ns1.grant-fullen.com.   A   300 35.231.125.190</p>

<p>ns2.grant-fullen.com.   A   300 35.231.125.190</p>

<p>Problem I am haveing is with CWP7 web panel.
Can not start the DNS BIND service </p>

<p>.Warning for this is</p>

<pre><code>Mar 31 22:26:38 centos7 bash[3667]: zone nf1j.com/IN: loaded serial 2019033167
Mar 31 22:26:38 centos7 bash[3667]: zone grantfullen.com/IN: loaded serial 2019033168
Mar 31 22:26:38 centos7 bash[3667]: zone grant-fullen.com/IN: NS 'ns1.grant-fullen.com' has no address records (A or AAAA)
Mar 31 22:26:38 centos7 bash[3667]: zone grant-fullen.com/IN: NS 'ns2.grant-fullen.com' has no address records (A or AAAA)
Mar 31 22:26:38 centos7 bash[3667]: zone grant-fullen.com/IN: not loaded due to errors.
Mar 31 22:26:38 centos7 bash[3667]: _default/grant-fullen.com/IN: bad zone
Mar 31 22:26:38 centos7 systemd[1]: named.service: control process exited, code=exited status=1
Mar 31 22:26:38 centos7 systemd[1]: Failed to start Berkeley Internet Name Domain (DNS).
Mar 31 22:26:38 centos7 systemd[1]: Unit named.service entered failed state.
Mar 31 22:26:38 centos7 systemd[1]: named.service failed.
</code></pre>

<p>Question:= DO I have something configured wrong in the a records at google cloud dns ?</p>
","<google-cloud-platform><dns-zone>","2019-03-31 22:49:22"
"746469","MariaDB on Centos7, service still called MySql","<p>I have a Centos 7 server. </p>

<p>Removed previous MariaDb 5.5 and installed MariaDb 10 following <a href=""https://mariadb.com/kb/en/mariadb/yum/"" rel=""nofollow noreferrer"">https://mariadb.com/kb/en/mariadb/yum/</a></p>

<p>Added the MariaDB YUM Repository,and then executed:</p>

<pre><code>sudo yum install MariaDB-server MariaDB-client
</code></pre>

<p>All good, then when I try to run:</p>

<pre><code>sudo systemctl start mariadb
</code></pre>

<p>I get:</p>

<pre><code>Failed to start mariadb.service: Unit mariadb.service failed to load: No such file or directory.
</code></pre>

<p>Yet if I run:</p>

<pre><code> sudo systemctl start mysql
</code></pre>

<p>Works and starts MariaDb. On a different server I can start MariaDb by calling it MariaDb not MySql as on this instance. I'm finding this a bit annoying. Have repeatedly removed MariaDb, rm -r /var/lib/mysql* but to no avail.</p>

<p>Server one works:</p>

<pre><code>[root@phos bod]# rpm --query centos-release
centos-release-7-2.1511.el7.centos.2.10.x86_64
[root@phos bod]# systemctl start mariadb.service
[root@phos bod]# 
</code></pre>

<p>Server Two (same OS &amp; same version of MariaDB) doesn't work:</p>

<pre><code>[root@carpo bod]# rpm --query centos-release
centos-release-7-2.1511.el7.centos.2.10.x86_64
[root@carpo bod]# systemctl start mariadb.service
Failed to start mariadb.service: Unit mariadb.service failed to load: No such file or directory.
[root@carpo bod]# systemctl start mysql
[root@carpo bod]# 
</code></pre>

<p>My question is: How can I manage the MariaDb service by it's correct name on Server Two?</p>
","<centos7><mariadb>","2016-01-03 15:18:34"
"885397","Login failed for root from 190.20.86.0. AUTHENTICATE PLAIN: * BYE. Internal error ocurred, refer to server log for more information","<h1>Introduction</h1>

<p>I have been dealing for over 12 hours looking with an issue with my <a href=""http://www.roundcube.net"" rel=""nofollow noreferrer"">Roundcube</a> installation in my VPS, Ubuntu 16.04. The IMAP server (dovecot) and SMTP server are working and can <code>telnet</code> without connection refused errors.</p>

<h1>Issue</h1>

<p>However, when I go to my webmail and try to login with username <code>root</code> and the relevant SSH/SFTP password (in order to test if it is working, then I will create additional email addresses), it returns the following error:</p>

<pre><code>IMAP Error in /usr/share/nginx/roundcubemail/program/lib/Roundcube/rcube_imap.php (193): Login failed for root from 190.20.86.0. AUTHENTICATE PLAIN: * BYE. Internal error ocurred. Refer to server log for more information
</code></pre>

<h1>Logs</h1>

<p>The <code>/var/log/nginx/mail.log</code> and the <code>/var/log/nginx/roundcube.error</code> are empty</p>

<h1>Configs</h1>

<p>Roundcube <code>RC_PATH/config/config.inc.php</code>:</p>

<pre><code>&lt;?php

/* Local configuration for Roundcube Webmail */

// ----------------------------------
// SQL DATABASE
// ----------------------------------
// Database connection string (DSN) for read+write operations
// Format (compatible with PEAR MDB2): db_provider://user:password@host/database
// Currently supported db_providers: mysql, pgsql, sqlite, mssql, sqlsrv, oracle
// For examples see http://pear.php.net/manual/en/package.database.mdb2.intro-dsn.php
// NOTE: for SQLite use absolute path (Linux): 'sqlite:////full/path/to/sqlite.db?mode=0646'
//       or (Windows): 'sqlite:///C:/full/path/to/sqlite.db'
$config['db_dsnw'] = 'mysql://roundcubeuser:XXXXXXXXXXXX@localhost/roundcubemail';

// ----------------------------------
// IMAP
// ----------------------------------
// The mail host chosen to perform the log-in.
// Leave blank to show a textbox at login, give a list of hosts
// to display a pulldown menu or set one host as string.
// To use SSL/TLS connection, enter hostname with prefix ssl:// or tls://
// Supported replacement variables:
// %n - hostname ($_SERVER['SERVER_NAME'])
// %t - hostname without the first part
// %d - domain (http hostname $_SERVER['HTTP_HOST'] without the first part)
// %s - domain name after the '@' from e-mail address provided at login screen
// For example %n = mail.domain.tld, %t = domain.tld
// WARNING: After hostname change update of mail_host column in users table is
//          required to match old user data records with the new host.
$config['default_host'] = 'imaps://mail.domain.com';

// TCP port used for IMAP connections
$config['default_port'] = 993;

// ----------------------------------
// SMTP
// ----------------------------------
// SMTP server host (for sending mails).
// To use SSL/TLS connection, enter hostname with prefix ssl:// or tls://
// If left blank, the PHP mail() function is used
// Supported replacement variables:
// %h - user's IMAP hostname
// %n - hostname ($_SERVER['SERVER_NAME'])
// %t - hostname without the first part
// %d - domain (http hostname $_SERVER['HTTP_HOST'] without the first part)
// %z - IMAP domain (IMAP hostname without the first part)
// For example %n = mail.domain.tld, %t = domain.tld
$config['smtp_server'] = 'tls://mail.domain.com';
$config['imap_auth_type'] = 'PLAIN';

// SMTP port (default is 25; use 587 for STARTTLS or 465 for the
// deprecated SSL over SMTP (aka SMTPS))
$config['smtp_port'] = 587;

// SMTP username (if required) if you use %u as the username Roundcube
// will use the current username for login
$config['smtp_user'] = '%u';

// SMTP password (if required) if you use %p as the password Roundcube
// will use the current user's password for login
$config['smtp_pass'] = '%p';

// provide an URL where a user can get support for this Roundcube installation
// PLEASE DO NOT LINK TO THE ROUNDCUBE.NET WEBSITE HERE!
$config['support_url'] = '';

// This key is used for encrypting purposes, like storing of imap password
// in the session. For historical reasons it's called DES_key, but it's used
// with any configured cipher_method (see below).
$config['des_key'] = 'XXXXXXXXXXXXXXXX';

// Name your service. This is displayed on the login screen and in the window title
$config['product_name'] = 'Webmail';

// ----------------------------------
// PLUGINS
// ----------------------------------
// List of active plugins (in plugins/ directory)
$config['plugins'] = array();

$config['debug_level'] = 4; //to debug the php authentication failed error
</code></pre>

<p>How do I solve this issue while trying to login? I am very worried about the amount of emails I have losed with this 24-hour downtime (20+).</p>

<p>Thanks for your answers.
Kevin Nathan Andrews</p>
","<ubuntu><nginx><php><dovecot><roundcube>","2017-11-27 19:59:58"
"961131","ftp connexion trough haproxy","<p>At the moment I have two servers under centos 7.</p>

<ul>
<li>10.10.104.200 = Loadbalencer with Haproxy</li>
<li>10.10.105.100 = web server</li>
</ul>

<p>I want the clients to enter their ftp (ex Filezilla) 10.10.104.200 and be redirected automatically to the server 10.10.105.100. I still have not found a way that works well.</p>

<p>Here is my current configuration of haproxy.cfg:</p>

<pre><code>global
  log /dev/log local0
  log /dev/log local1 notice
  chroot /var/lib/haproxy
  stats timeout 30s
  user haproxy
  group haproxy
  daemon

defaults
  log global
  mode http
  option httplog
  option tcplog
  retries 3
  maxconn 10000
  option redispatch
  option dontlognull
  timeout connect 5000
  timeout client 50000
  timeout server 50000

frontend http_front
  bind *:80
  default_backend http_back

backend http_back
  balance roundrobin
  server serveurBack 10.10.105.100:80 check

listen stats
  bind *:8181
  stats enable
  stats uri /
  stats realm Haproxy\ Statistics
  stats auth ****:*****
</code></pre>
","<ftp><haproxy>","2019-04-02 15:35:14"
"746724","vBulletin upstreaming to Nginx server","<p>I have two acount on RHC Openshift so i want install nginx one one server and vbulletin on the other server , so have combine server 1 to server two (nginx to Vbulletine) I try this configuration for nginx:</p>

<pre><code>#user  nobody;
worker_processes  1;

#error_log  logs/error.log;
#error_log  logs/error.log  notice;
#error_log  logs/error.log  info;
error_log /var/lib/openshift/568adf5a0c1e66dac3000283/app-root/logs/nginx_error.log debug;

pid        /var/lib/openshift/568adf5a0c1e66dac3000283/app-root/runtime/srv/nginx/logs/nginx.pid;


events {
    worker_connections  1024;
}


http {
    include       mime.types;
    default_type  application/octet-stream;

    #log_format  main  '$remote_addr - $remote_user [$time_local] ""$request"" '
    #                  '$status $body_bytes_sent ""$http_referer"" '
    #                  '""$http_user_agent"" ""$http_x_forwarded_for""';

    #access_log  logs/access.log  main;
    #access_log $OPENSHIFT_DIY_LOG_DIR/access.log main;
    port_in_redirect off;

    sendfile        on;
    #tcp_nopush     on;

    #keepalive_timeout  0;
    keepalive_timeout  165;

    gzip  on;

    upstream frontends {
        #server pr4ss.tk;
        #server 222.66.115.233:80 weight=1;
        #server 127.6.145.1:8081 ;
        server vb2-fishsmarkets.rhcloud.com;

    }
    upstream frontends2 {
        server vb2-fishsmarkets.rhcloud.com;
        #server 222.66.115.233:80 weight=1;
        #server 127.6.145.1:8081 ;

    }
    upstream index {
        #server  community.elasa.ir;
        server vb2-fishsmarkets.rhcloud.com;
        #server 127.6.145.1:15001 weight=1;
        #server 127.6.145.1:15002 weight=2;
        #server 127.6.145.1:15002 weight=3;

    }

    server {
        listen      127.6.145.1:8080;
        server_name  diy-elasa2.rhcloud.com www.diy-elasa2.rhcloud.com;
        root /var/lib/openshift/568adf5a0c1e66dac3000283/app-root/runtime/repo/www;


        set_real_ip_from 127.6.145.1;
        real_ip_header X-Forwarded-For;

        #charset koi8-r;

        #access_log  logs/host.access.log  main;

        location / {
            root   /var/lib/openshift/568adf5a0c1e66dac3000283/app-root/runtime/repo/www;
            index  index.html index.htm;
            try_files $uri $uri/ =404;

            autoindex on;
            autoindex_exact_size off;
            autoindex_localtime on;

            #proxy_set_header Authorization base64_encoding_of_""user:password"";
            #proxy_pass_header Server;
            proxy_set_header Host $http_host;
            proxy_redirect off;
            proxy_set_header  X-Real-IP  $remote_addr;
            proxy_set_header X-Scheme $scheme;
            proxy_pass http://frontends;

        }
        location /forum\.php {
                if (!-f $request_filename) {
                        rewrite ^/(.*)$ /index.php?routestring=$1 last;
                }
                autoindex on;
            autoindex_exact_size off;
            autoindex_localtime on;

            #proxy_set_header Authorization base64_encoding_of_""user:password"";
            #proxy_pass_header Server;
            proxy_set_header Host $http_host;
            proxy_redirect off;
            proxy_set_header  X-Real-IP  $remote_addr;
            proxy_set_header X-Scheme $scheme;
            proxy_pass http://frontends;
        }
        location /forum2 {
            #root   /var/lib/openshift/568adf5a0c1e66dac3000283/app-root/runtime/repo/www;
            index  index.html index.htm;

            autoindex on;
            autoindex_exact_size off;
            autoindex_localtime on;

            #proxy_set_header Authorization base64_encoding_of_""user:password"";
            #proxy_pass_header Server;
            proxy_set_header Host $http_host;
            proxy_redirect off;
            proxy_set_header  X-Real-IP  $remote_addr;
            proxy_set_header X-Scheme $scheme;
            proxy_pass http://frontends;
        }
        location /categories {
            #root   /var/lib/openshift/568adf5a0c1e66dac3000283/app-root/runtime/repo/www;
            index  index.html index.htm;

            autoindex on;
            autoindex_exact_size off;
            autoindex_localtime on;

            #proxy_set_header Authorization base64_encoding_of_""user:password"";
            #proxy_pass_header Server;
            proxy_set_header Host $http_host;
            proxy_redirect off;
            proxy_set_header  X-Real-IP  $remote_addr;
            proxy_set_header X-Scheme $scheme;
            proxy_pass http://frontends2;
        }
        location /index {
            #root   /var/lib/openshift/568adf5a0c1e66dac3000283/app-root/runtime/repo/www;
            index  index.html index.htm;

            autoindex on;
            autoindex_exact_size off;
            autoindex_localtime on;
            # an HTTP header important enough to have its own Wikipedia entry:
            #   http://en.wikipedia.org/wiki/X-Forwarded-For
            proxy_set_header   X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header   Host             $host;
            proxy_set_header   X-Real-IP        $remote_addr;


            # enable this if you forward HTTPS traffic to unicorn,
            # this helps Rack set the proper URL scheme for doing redirects:
            # proxy_set_header X-Forwarded-Proto $scheme;

            # pass the Host: header from the client right along so redirects
            # can be set properly within the Rack application
            proxy_set_header Host $http_host;

            # we don't want nginx trying to do something clever with
            # redirects, we set the Host: header above already.
            proxy_redirect off;

            # set ""proxy_buffering off"" *only* for Rainbows! when doing
            # Comet/long-poll/streaming.  It's also safe to set if you're using
            # only serving fast clients with Unicorn + nginx, but not slow
            # clients.  You normally want nginx to buffer responses to slow
            # clients, even with Rails 3.1 streaming because otherwise a slow
            # client can become a bottleneck of Unicorn.
            #
            # The Rack application may also set ""X-Accel-Buffering (yes|no)""
            # in the response headers do disable/enable buffering on a
            # per-response basis.
            # proxy_buffering off;






            client_max_body_size       10m;
            client_body_buffer_size    128k;

            proxy_connect_timeout      10;
            proxy_send_timeout         5;
            proxy_read_timeout         3600;

            proxy_buffer_size          4k;
            proxy_buffers              4 132k;
            proxy_busy_buffers_size    264k;
            proxy_temp_file_write_size 164k;
            proxy_pass http://index;            


            #proxy_set_header Authorization base64_encoding_of_""user:password"";
            #proxy_pass_header Server;
            proxy_set_header Host $http_host;
        }



        #error_page  404              /404.html;

        # redirect server error pages to the static page /50x.html
        #
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }

        # proxy the PHP scripts to Apache listening on 127.0.0.1:80
        #
        #location ~ \.php$ {
        #    proxy_pass   http://127.0.0.1;
        #}

        # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000
        #
        location ~ \.php$ {
                # handles legacy scripts
                if (!-f $request_filename) {
                        rewrite ^/(.*)$ /index.php?routestring=$1 break;
                }

                fastcgi_split_path_info ^(.+\.php)(.*)$;
                fastcgi_pass   127.6.145.1:9000;
                fastcgi_index index.php;
                fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
                include fastcgi_params;
                fastcgi_param QUERY_STRING $query_string;
                fastcgi_param REQUEST_METHOD $request_method;
                fastcgi_param CONTENT_TYPE $content_type;
                fastcgi_param CONTENT_LENGTH $content_length;
                fastcgi_intercept_errors on;
                fastcgi_ignore_client_abort off;
                fastcgi_connect_timeout 60;
                fastcgi_send_timeout 180;
                fastcgi_read_timeout 180;
                fastcgi_buffers 256 16k;
                fastcgi_buffer_size 32k;
                fastcgi_temp_file_write_size 256k;
        }

        # deny access to .htaccess files, if Apache's document root
        # concurs with nginx's one
        #
        #location ~ /\.ht {
        #    deny  all;
        #}
    }


    # another virtual host using mix of IP-, name-, and port-based configuration
    #
    #server {
    #    listen       8000;
    #    listen       somename:8080;
    #    server_name  somename  alias  another.alias;

    #    location / {
    #        root   html;
    #        index  index.html index.htm;
    #    }
    #}


    # HTTPS server
    #
    #server {
    #    listen       443;
    #    server_name  localhost;

    #    ssl                  on;
    #    ssl_certificate      cert.pem;
    #    ssl_certificate_key  cert.key;

    #    ssl_session_timeout  5m;

    #    ssl_protocols  SSLv2 SSLv3 TLSv1;
    #    ssl_ciphers  HIGH:!aNULL:!MD5;
    #    ssl_prefer_server_ciphers   on;

    #    location / {
    #        root   html;
    #        index  index.html index.htm;
    #    }
    #}

}
</code></pre>

<p>But it not working properly.
I got this error:</p>

<pre><code>2016/01/04 17:49:05 [notice] 375279#0: OS: Linux 2.6.32-573.12.1.el6.x86_64
2016/01/04 17:49:05 [notice] 375279#0: getrlimit(RLIMIT_NOFILE): 1024:16384
2016/01/04 17:49:05 [notice] 375304#0: start worker processes
2016/01/04 17:49:05 [notice] 375304#0: start worker process 375306
2016/01/04 17:49:16 [notice] 375306#0: *7 ""^/(.*)$"" matches ""/forum.php"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /forum.php HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:49:16 [notice] 375306#0: *7 rewritten data: ""/index.php"", args: ""routestring=forum.php"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /forum.php HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:49:16 [error] 375306#0: *7 connect() failed (111: Connection refused) while connecting to upstream, client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /forum.php HTTP/1.1"", upstream: ""fastcgi://127.6.145.1:9000"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:51:02 [notice] 379852#0: using the ""epoll"" event method
2016/01/04 17:51:02 [notice] 379852#0: nginx/1.6.0
2016/01/04 17:51:02 [notice] 379852#0: built by gcc 4.4.7 20120313 (Red Hat 4.4.7-16) (GCC) 
2016/01/04 17:51:02 [notice] 379852#0: OS: Linux 2.6.32-573.12.1.el6.x86_64
2016/01/04 17:51:02 [notice] 379852#0: getrlimit(RLIMIT_NOFILE): 1024:16384
2016/01/04 17:51:02 [notice] 379855#0: start worker processes
2016/01/04 17:51:02 [notice] 379855#0: start worker process 379856
2016/01/04 17:51:08 [notice] 379856#0: *1 ""^/(.*)$"" matches ""/www"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /www HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:51:08 [notice] 379856#0: *1 rewritten data: ""/index.php"", args: ""routestring=www"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /www HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:51:08 [notice] 379856#0: *1 ""^/(.*)$"" matches ""/index.php"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /www HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:51:08 [notice] 379856#0: *1 rewritten data: ""/index.php"", args: ""routestring=index.php&amp;routestring=www"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /www HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:51:08 [error] 379856#0: *1 connect() failed (111: Connection refused) while connecting to upstream, client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /www HTTP/1.1"", upstream: ""fastcgi://127.6.145.1:9000"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:51:13 [notice] 379856#0: *3 ""^/(.*)$"" matches ""/forum"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /forum HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:51:13 [notice] 379856#0: *3 rewritten data: ""/index.php"", args: ""routestring=forum"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /forum HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:51:13 [notice] 379856#0: *3 ""^/(.*)$"" matches ""/index.php"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /forum HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:51:13 [notice] 379856#0: *3 rewritten data: ""/index.php"", args: ""routestring=index.php&amp;routestring=forum"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /forum HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:51:13 [error] 379856#0: *3 connect() failed (111: Connection refused) while connecting to upstream, client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /forum HTTP/1.1"", upstream: ""fastcgi://127.6.145.1:9000"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:51:17 [notice] 379856#0: *3 ""^/(.*)$"" matches ""/"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET / HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:51:17 [notice] 379856#0: *3 rewritten data: ""/index.php"", args: ""routestring="", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET / HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:51:17 [notice] 379856#0: *3 ""^/(.*)$"" matches ""/index.php"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET / HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:51:17 [notice] 379856#0: *3 rewritten data: ""/index.php"", args: ""routestring=index.php&amp;routestring="", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET / HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:51:17 [error] 379856#0: *3 connect() failed (111: Connection refused) while connecting to upstream, client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET / HTTP/1.1"", upstream: ""fastcgi://127.6.145.1:9000"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:51:17 [notice] 379856#0: *6 ""^/(.*)$"" matches ""/www"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /www HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:51:17 [notice] 379856#0: *6 rewritten data: ""/index.php"", args: ""routestring=www"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /www HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:51:17 [notice] 379856#0: *6 ""^/(.*)$"" matches ""/index.php"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /www HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:51:17 [notice] 379856#0: *6 rewritten data: ""/index.php"", args: ""routestring=index.php&amp;routestring=www"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /www HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:51:17 [error] 379856#0: *6 connect() failed (111: Connection refused) while connecting to upstream, client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /www HTTP/1.1"", upstream: ""fastcgi://127.6.145.1:9000"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:51:19 [notice] 379856#0: *3 ""^/(.*)$"" matches ""/"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET / HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:51:19 [notice] 379856#0: *3 rewritten data: ""/index.php"", args: ""routestring="", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET / HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:51:19 [notice] 379856#0: *3 ""^/(.*)$"" matches ""/index.php"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET / HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:51:19 [notice] 379856#0: *3 rewritten data: ""/index.php"", args: ""routestring=index.php&amp;routestring="", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET / HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:51:19 [error] 379856#0: *3 connect() failed (111: Connection refused) while connecting to upstream, client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET / HTTP/1.1"", upstream: ""fastcgi://127.6.145.1:9000"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:52:56 [notice] 384714#0: using the ""epoll"" event method
2016/01/04 17:52:56 [notice] 384714#0: nginx/1.6.0
2016/01/04 17:52:56 [notice] 384714#0: built by gcc 4.4.7 20120313 (Red Hat 4.4.7-16) (GCC) 
2016/01/04 17:52:56 [notice] 384714#0: OS: Linux 2.6.32-573.12.1.el6.x86_64
2016/01/04 17:52:56 [notice] 384714#0: getrlimit(RLIMIT_NOFILE): 1024:16384
2016/01/04 17:52:56 [notice] 384716#0: start worker processes
2016/01/04 17:52:56 [notice] 384716#0: start worker process 384717
2016/01/04 17:53:18 [notice] 384717#0: *15 ""^/(.*)$"" matches ""/forum.php"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /forum.php HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:53:18 [notice] 384717#0: *15 rewritten data: ""/index.php"", args: ""routestring=forum.php"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /forum.php HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:53:18 [error] 384717#0: *15 connect() failed (111: Connection refused) while connecting to upstream, client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /forum.php HTTP/1.1"", upstream: ""fastcgi://127.6.145.1:9000"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:54:35 [notice] 389310#0: using the ""epoll"" event method
2016/01/04 17:54:35 [notice] 389310#0: nginx/1.6.0
2016/01/04 17:54:35 [notice] 389310#0: built by gcc 4.4.7 20120313 (Red Hat 4.4.7-16) (GCC) 
2016/01/04 17:54:35 [notice] 389310#0: OS: Linux 2.6.32-573.12.1.el6.x86_64
2016/01/04 17:54:35 [notice] 389310#0: getrlimit(RLIMIT_NOFILE): 1024:16384
2016/01/04 17:54:35 [notice] 389312#0: start worker processes
2016/01/04 17:54:35 [notice] 389312#0: start worker process 389313
2016/01/04 17:54:38 [notice] 389313#0: *1 ""^/(.*)$"" matches ""/forum"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /forum HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:54:38 [notice] 389313#0: *1 rewritten data: ""/index.php"", args: ""routestring=forum"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /forum HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:54:38 [notice] 389313#0: *1 ""^/(.*)$"" matches ""/index.php"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /forum HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:54:38 [notice] 389313#0: *1 rewritten data: ""/index.php"", args: ""routestring=index.php&amp;routestring=forum"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /forum HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:54:38 [error] 389313#0: *1 connect() failed (111: Connection refused) while connecting to upstream, client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /forum HTTP/1.1"", upstream: ""fastcgi://127.6.145.1:9000"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 17:56:30 [notice] 394970#0: using the ""epoll"" event method
2016/01/04 17:56:30 [notice] 394970#0: nginx/1.6.0
2016/01/04 17:56:30 [notice] 394970#0: built by gcc 4.4.7 20120313 (Red Hat 4.4.7-16) (GCC) 
2016/01/04 17:56:30 [notice] 394970#0: OS: Linux 2.6.32-573.12.1.el6.x86_64
2016/01/04 17:56:30 [notice] 394970#0: getrlimit(RLIMIT_NOFILE): 1024:16384
2016/01/04 17:56:30 [notice] 394971#0: start worker processes
2016/01/04 17:56:30 [notice] 394971#0: start worker process 394972
2016/01/04 18:02:37 [notice] 417803#0: using the ""epoll"" event method
2016/01/04 18:02:37 [notice] 417803#0: nginx/1.6.0
2016/01/04 18:02:37 [notice] 417803#0: built by gcc 4.4.7 20120313 (Red Hat 4.4.7-16) (GCC) 
2016/01/04 18:02:37 [notice] 417803#0: OS: Linux 2.6.32-573.12.1.el6.x86_64
2016/01/04 18:02:37 [notice] 417803#0: getrlimit(RLIMIT_NOFILE): 1024:16384
2016/01/04 18:02:37 [notice] 417938#0: start worker processes
2016/01/04 18:02:37 [notice] 417938#0: start worker process 417982
2016/01/04 18:02:52 [info] 417982#0: *5 client 127.6.145.1 closed keepalive connection
2016/01/04 18:03:06 [info] 417982#0: *1 client 127.6.145.1 closed keepalive connection
2016/01/04 18:03:06 [info] 417982#0: *2 client 127.6.145.1 closed keepalive connection
2016/01/04 18:03:28 [notice] 417982#0: *47 ""^/(.*)$"" matches ""/forum.php"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /forum.php HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 18:03:28 [notice] 417982#0: *47 rewritten data: ""/index.php"", args: ""routestring=forum.php"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /forum.php HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 18:03:28 [error] 417982#0: *47 connect() failed (111: Connection refused) while connecting to upstream, client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /forum.php HTTP/1.1"", upstream: ""fastcgi://127.6.145.1:9000"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 18:03:39 [info] 417982#0: *23 client 127.6.145.1 closed keepalive connection
2016/01/04 18:04:30 [info] 417982#0: *47 client 127.6.145.1 closed keepalive connection
2016/01/04 18:05:30 [info] 417982#0: *49 client 127.6.145.1 closed keepalive connection
2016/01/04 18:05:57 [notice] 444106#0: using the ""epoll"" event method
2016/01/04 18:05:57 [notice] 444106#0: nginx/1.6.0
2016/01/04 18:05:57 [notice] 444106#0: built by gcc 4.4.7 20120313 (Red Hat 4.4.7-16) (GCC) 
2016/01/04 18:05:57 [notice] 444106#0: OS: Linux 2.6.32-573.12.1.el6.x86_64
2016/01/04 18:05:57 [notice] 444106#0: getrlimit(RLIMIT_NOFILE): 1024:16384
2016/01/04 18:05:57 [notice] 444107#0: start worker processes
2016/01/04 18:05:57 [notice] 444107#0: start worker process 444108
2016/01/04 18:06:06 [notice] 444108#0: *1 ""^/(.*)$"" matches ""/forum.php"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /forum.php HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 18:06:06 [notice] 444108#0: *1 rewritten data: ""/index.php"", args: ""routestring=forum.php"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /forum.php HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 18:06:06 [error] 444108#0: *1 connect() failed (111: Connection refused) while connecting to upstream, client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /forum.php HTTP/1.1"", upstream: ""fastcgi://127.6.145.1:9000"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 18:06:16 [notice] 444108#0: *1 ""^/(.*)$"" matches ""/forum.php"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /forum.php HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 18:06:16 [notice] 444108#0: *1 rewritten data: ""/index.php"", args: ""routestring=forum.php"", client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /forum.php HTTP/1.1"", host: ""diy-elasa2.rhcloud.com""
2016/01/04 18:06:16 [error] 444108#0: *1 connect() failed (111: Connection refused) while connecting to upstream, client: 188.158.90.71, server: diy-elasa2.rhcloud.com, request: ""GET /forum.php HTTP/1.1"", upstream: ""fastcgi://127.6.145.1:9000"", host: ""diy-elasa2.rhcloud.com""
</code></pre>

<p>So what do you think!!!
the php-fpm is working correctly and could be checked by this link:</p>

<p><a href=""http://diy-elasa2.rhcloud.com/phpinfo.php"" rel=""nofollow noreferrer"">http://diy-elasa2.rhcloud.com/phpinfo.php</a></p>
","<nginx><php><vbulletin>","2016-01-04 23:13:10"
"961203","How do I reset a password in google cloud?","<p>I am trying to connect to Google cloud using Putty. I got the public/private keys to work, but when I connect, it asks for a password. It will not accept the password for the private key or my Google log in or my email log in. I am try to either reset the password or ideally disable prompt for password using the console. None of the entries below were successful and resulted in  a series of error messages including:</p>

<pre><code>Positional argument deprecated_host has been removed. Use --host instead.
</code></pre>

<p>How do I reset or cancel the password?</p>

<pre><code>gcloud sql users set-password james_celentano –instance=instance-1 --prompt-for-password=off

Big problems “Positional argument deprecated_host has been removed”

gcloud sql users set-password --host –instance=instance-1 --prompt-for-password=off
</code></pre>

<p>Now does not like <code>--prompt-for-password=off</code></p>

<pre><code>gcloud sql users set-password --host –instance instance-1 -- password=

gcloud sql users set-password --host –instance= instance-1 –i instance-1 -- password=

gcloud sql users set-password james_celentano --host –instance=instance-1 password=

gcloud sql users set-password --host –instance=instance-1 password=

gcloud sql users set-password --host –instance=instance-1 –i=instance-1  password=
</code></pre>
","<google-cloud-platform>","2019-04-02 22:46:44"
"961231","How to change the limit quotas per day for Place API","<p>There are a quotas for Place API per day is 2400, anybody how to increase this limit ?</p>
","<google-cloud-platform>","2019-04-03 05:38:34"
"818230","Resolve keywords and https on dnsmasq","<p>Using <code>address=/#/192.168.2.1</code> on <code>/etc/dnsmasq.conf</code> to resolve all domain names for example google.com redirects you to <code>192.168.2.2</code> and got that working. </p>

<p>Now how do you resolve any keyword on the browser? For example I type in foo and press enter and get redirected to <code>192.168.2.2</code>. I get <strong>Not Found</strong> on my current configuration. </p>

<p>Also is it possible to resolve https urls to <code>192.168.2.2</code>? I'm currently not running 443 on my web server on 192.168.2.2.</p>
","<domain-name-system><dnsmasq>","2016-12-01 20:30:30"
"746785","SMTP Server with dynamic / Static IP","<p>I have 3 servers and 3 IP's.</p>

<pre><code>Server 1 - Static IP - 111.111.111.111
Server 2 - Static IP - 222.222.222.222
Server 3 - Dynamic IP - 333.333.333.333
</code></pre>

<p>Now <code>Server 3</code> is always assigned the IP <code>333.333.333.333</code> because it's the only one left, the two others are, as shown above, static to the two other servers.</p>

<p>Is there any reason to assign <code>Server 3</code> a static IP in regards of ""credibility"" to the ""outside world"" (Such as in terms of SMTP / Server credibility) despite the server always have and will be identified by the same IP?</p>

<p>Are there other reasons to give <code>Server 3</code> a ""static IP""?</p>

<p>This is not about SPF / DKIM / Reverse DNS, I'm very well aware of these things and they have been configured. - The question is not about getting unblocked.</p>
","<smtp><ip><static-ip>","2016-01-05 09:02:12"
"746814","how to solve SVN Commit Issue: forbidden by server","<p>When I tried to commit my code using Eclipse. I got the following error: </p>

<pre><code>org.apache.subversion.javahl.ClientException: The operation is forbidden by the server
svn: Commit failed (details follow):
svn: Changing file 'D:\workspace\DataItem.java' is forbidden by the server
URL access forbidden for unknown reason
svn: Access to '/abc/!svn/ver/8211/trunk/01_Application/DataItem.java' forbidden
RA layer request failed
svn: Additional errors:
svn: CHECKOUT of '/abc/!svn/ver/8211/trunk/01_Application/DataItem.java': 403 Forbidden
</code></pre>

<p>How to solve the issue?</p>
","<java><svn><eclipse>","2016-01-05 10:34:27"
"961415","Dovecot authentication failure","<p>I've installed a postfix/dovecot mail services on DigitalOcean. I'm using certificates provided by letsencrypt. There are no errors in syslog that relate to problems with the certificates. </p>

<p>I'm using mysql to store the virtual users.</p>

<p>The connection string is:</p>

<blockquote>
  <p>connect = host=127.0.0.1 dbname=servermail user=usermail
  password=<em>REDACTED</em> (with the correct password, this tests okay and
  there are no connection errors reported on startup)</p>
</blockquote>

<p>When I attempt to connect from a client, I see this in the log:</p>

<blockquote>
  <p>Apr 04 10:15:43 imap-login: Info: Disconnected (auth failed, 1
  attempts in 2 secs): user=, method=PLAIN,
  rip=42.115.84.125, lip=206.189.150.255, TLS: Disconnected,
  session=</p>
</blockquote>

<p>Incoming mail is being delivered successfully by Postfix.</p>

<p>Output of dovecot -n </p>

<pre><code># 2.2.33.2 (d6601f4ec): /etc/dovecot/dovecot.conf
# Pigeonhole version 0.4.21 (92477967)
# OS: Linux 4.15.0-47-generic x86_64 Ubuntu 18.04.2 LTS ext4
auth_mechanisms = plain login
log_path = /var/log/dovecot.log
mail_location = maildir:/var/mail/vhosts/%d/%n/
mail_privileged_group = mail
namespace inbox {
  inbox = yes
  location = 
  mailbox Drafts {
    special_use = \Drafts
  }
  mailbox Junk {
    special_use = \Junk
  }
  mailbox Sent {
    special_use = \Sent
  }
  mailbox ""Sent Messages"" {
    special_use = \Sent
  }
  mailbox Trash {
    special_use = \Trash
  }
  prefix = 
}
passdb {
  args = /etc/dovecot/deny-users
  deny = yes
  driver = passwd-file
}
passdb {
  args = /etc/dovecot/dovecot-sql.conf.ext
  driver = sql
}
postmaster_address = dev@vietfeir.com
protocols = imap lmtp
service auth-worker {
  user = vmail
}
service auth {
  unix_listener /var/spool/postfix/private/auth {
    group = postfix
    mode = 0666
    user = postfix
  }
  unix_listener auth-userdb {
    mode = 0600
    user = vmail
  }
  user = dovecot
}
service imap-login {
  inet_listener imap {
    port = 0
  }
  inet_listener imaps {
    port = 993
    ssl = yes
  }
}
service lmtp {
  unix_listener /var/spool/postfix/private/dovecot-lmtp {
    group = postfix
    mode = 0600
    user = postfix
  }
}
service pop3-login {
  inet_listener pop3 {
    port = 0
  }
  inet_listener pop3s {
    port = 995
    ssl = yes
  }
}
ssl = required
ssl_cert = &lt;/etc/letsencrypt/live/civicrm.vietfeir.com/fullchain.pem
ssl_client_ca_dir = /etc/ssl/certs
ssl_key =  # hidden, use -P to show it
userdb {
  driver = passwd
}
userdb {
  args = uid=vmail gid=vmail home=/var/mail/vhosts/%d/%n
  driver = static
}
</code></pre>

<p>Please give me some troubleshooting ideas.</p>

<p>Here is the SQL query used to create a user in the database:</p>

<pre><code>    INSERT INTO `servermail`.`virtual_users`
  (`domain_id`, `password` , `email`)
VALUES
  ('1', ENCRYPT('*REDACTED*!', CONCAT('$6$', SUBSTRING(SHA(RAND()), -16))), 'dennis@vietfeir.com')
</code></pre>

<p>My client (Bluemail) has a choice of plain or CRAM-MD5 passwords. Outlook seems to only allow plain, so I think this might be the problem as I have chose an SHA based scheme. (SHA512-CRYPT) </p>
","<dovecot>","2019-04-04 03:31:54"
"885699","Encrypted password","<p>I need to transfer 3-4 files located on multiple servers to a central server.I currently do manual ""scp"" from these servers to copy files on central server. </p>

<p>Please suggest how can i do that automatically without it asking for password.I know i can do it by configuring ""passwordless"" authentication using ""ssh-keygen"" but this is not too secure and our organization is not approving the same.I have also use ""sshpass"" but ""sshpass"" is showing my password in a script which again is a security concern.</p>

<p>Please suggest how to provide encrypted password to ""sshpass"" whhich will solve my issue.</p>
","<ssh>","2017-11-29 11:38:13"
"818411","How to run or integrate Shell scripts in Chef","<p>I have developed many automation solutions as shell scripts and it has been successfully being used in Prod/Dev environments.
With Chef being introduced, I need to start using Chef as a future automation solution.</p>

<p>As a First Step, I am trying to integrate existing shell script solution to run within Chef and then convert the shellcodes to the chef at the later point.</p>

<p>I am new to Chef and have taken the basic fundamental course.
So, Please help me how to run the existing shell scripts in Chef as a starting point.</p>

<p>I just need some basic examples, so that I can start to build from there.</p>
","<shell><chef>","2016-12-02 17:42:57"
"818534","Is it possible to have two different servers for my MX records?","<p>I have a domain name registered, and I want to use two services to receive email on it.</p>

<p>I want to be able to use a service such as Zoho or gmail, but I also want to receive mail on my own server running Postfix.</p>

<p>Is there a way for me to receive email from both services? For instance, can I use MX records that point to two different servers?</p>
","<domain-name-system><mx-record>","2016-12-03 10:31:10"
"818643","Point a url in magento app to a different app","<p>I have a magneto application running on Apache. I want the home page of this application to point to a completely different app:</p>

<p>So for example, if</p>

<pre><code>localhost:80/
</code></pre>

<p>was accessed, redirect it to</p>

<pre><code>localhost:3000/
</code></pre>

<p>I want this since we are changing magento app to react app page by page and we have our react app running on <code>localhost:3000</code>, what would be the way to go?</p>
","<apache-2.2>","2016-12-04 10:58:51"
"818677","How does raspbian choose amongst IP addresses?","<p>I'm running a Raspbian client connecting via HTTP to a Raspbian server. Apache on server is setup to not require authentication for local IPs- both ip4 and ip6. Both client and server are attached to single FritzBox router.</p>

<p>Client has multiple IPs available:</p>

<pre><code>wlan0     Link encap:Ethernet  HWaddr 80:1f:02:7c:de:8c
      inet addr:192.168.0.46  Bcast:192.168.0.255  Mask:255.255.255.0
      inet6 addr: fe80::821f:2ff:fe7c:de8c/64 Scope:Link
      inet6 addr: 2a02:8108:xxxx:xxxx:xxxx:xxxx:xxxx:8a0a/64 Scope:Global
      UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
      RX packets:389873 errors:0 dropped:1009 overruns:0 frame:0
      TX packets:265130 errors:0 dropped:1 overruns:0 carrier:0
      collisions:0 txqueuelen:1000
      RX bytes:186117464 (177.4 MiB)  TX bytes:35864729 (34.2 MiB)
</code></pre>

<p>Server's Apache access log shows that client uses <code>2a02:8108...</code> which is not in the allowed local IP range.</p>

<p>Why does the client chose a global IP for connecting inside the local subnet and how can this be influenced?</p>

<p><strong>Update</strong></p>

<p>Both server and client are in the same local network and both have link-local addresses:</p>

<p>client</p>

<pre><code>eth0      Link encap:Ethernet  HWaddr b8:27:eb:a9:e8:80
      inet addr:192.168.0.48  Bcast:192.168.0.255  Mask:255.255.255.0
      inet6 addr: fe80::3e43:e197:e064:1be8/64 Scope:Link
      inet6 addr: 2a02:8108:9c40:...:1192/64 Scope:Global
</code></pre>

<p>server</p>

<pre><code>eth0      Link encap:Ethernet  HWaddr b8:27:eb:50:df:c6
      inet addr:192.168.0.21  Bcast:192.168.0.255  Mask:255.255.255.0
      inet6 addr: fe80::ba27:ebff:fe50:dfc6/64 Scope:Link
      inet6 addr: 2a02:8108:9c40:...:dfc6/64 Scope:Global
</code></pre>

<p>server apache log </p>

<pre><code>2a02:8108:9c40:...:1dcd:8339 - - [11/Dec/2016:13:30:30 +0000] ""GET /middleware.php/capabilities/definitions.json HTTP/1.1"" 401 387 ""http://keller.fritz.box/frontend/"" ""Mozilla/5.0 (Windows NT 6.1; WOW64; rv:50.0) Gecko/20100101 Firefox/50.0""
</code></pre>
","<debian><ipv6><raspbian>","2016-12-04 16:03:17"
"747017","RHEL 6.5 iso issue","<p>the company Im currently working was able to installed redhat 6.5 in their VM and they uploaded the Rhel 6.5 iso in the redhat server. I was able to install the apache and the php but when I tried to install the mysql, it gives me the dependency problem. Also, the VM is not connected to the internet as of the moment that is why Im using the iso to install mysql. Please click the link below: <br>
<img src=""https://i.sstatic.net/3pAHS.png"" alt=""Dependency problem for the mysql"">
<br>
I mounted the iso and was able to install the other things but not the mysql. I've downloaded also those broken dependency file from the internet and try to install them but it seems that there are some .rpm file requires to connect to the internet. <br>Do you have any idea on how can I solve this? </p>
","<mysql><redhat><installation><rhel6><iso>","2016-01-06 04:53:13"
"818795","Print driver host 32bit applications has stopped working","<p>I get an error message : Print driver host 32bit applications has stopped working.
I work with Server 2008 R2
I stopped the Print Spooler Service and deleted all files in this folder : %WINDIR%\system32\spool\printers.
After that I started the Print Spooler Service.
It does not help, still shows an error message.
Please help me.</p>
","<windows><windows-server-2008-r2><network-printer>","2016-12-05 13:07:53"
"961432","Integrated Raid Controller Dieds","<p>I have an HP ProLiant ml110 G3 with an sata raid controller intrgrated in the motherboard. Few days ago the controller died. Can I use sata ports without the raid?</p>
","<hp-proliant><sata>","2019-04-04 06:19:41"
"886325","GPO still applied from wrong server","<p>There are two servers in the network. The old SBS 2011 Server and a new Windows Server 2012 R2 Foundation Server. The group policies have been migrated to the new server. However, if I change gpo on the new Server, these changes are not applied to the users. It seems, as if the old server is still the one, from which the gpo are loaded. </p>

<p>How do I tackle this?  </p>

<p>Update: </p>

<p>Both servers are in the same domain. The policies are on the top level. </p>

<p><a href=""https://i.sstatic.net/gDVbh.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gDVbh.png"" alt=""enter image description here""></a></p>

<p>Update 2: </p>

<p>The new server should replace the old one, but both servers are still running until everythings is migrated correctly. Unfortunately, I cannot recall how exactly I migrated the gpo. </p>
","<windows-server-2012-r2><group-policy><windows-sbs-2011>","2017-12-03 09:24:51"
"961480","VirtualHost not Resolved","<p>I have two identical VH in httpd/conf.d/.</p>

<pre><code>//test.conf
&lt;VirtualHost *:80&gt;
   ServerName testadmin.mysite.it
   DocumentRoot /var/www/html/testadmin
&lt;/VirtualHost&gt;


//release.conf
&lt;VirtualHost *:80&gt;
   ServerName deploy.mysite.it
   DocumentRoot /var/www/html/release
&lt;/VirtualHost&gt;



httpd -S|grep release
         port 80 namevhost release.mysite.it (/var/www/mysite/deploy.conf:1)
</code></pre>

<p>But a ping on <code>test.mysite.it</code> works, a ping on <code>deploy.mysite.it</code> does not.
I know ping is not the best tool to check. Both subdomains are handled by the same webserver, as you can see from the VH. But testadmin.mysite.it is reachable in the browser (it is resolved), the other is not.</p>

<p>Sure, if I put in my /etc/hosts the mapping </p>

<pre><code>&lt;ip of webserver&gt; release.mysite.it 
</code></pre>

<p>it works.</p>

<p>My question is: if I had not to set up the host for the first site, why I have to do it for the second?</p>

<p>There is something I can use to check for this problem?</p>

<p>Both files are included in <code>http.conf</code>.</p>
","<apache-2.2><domain-name-system><virtualhost><httpd>","2019-04-04 11:01:20"
"886365","One vmware vsan and two data stores","<p>I have five servers in one vsan cluster and one distributed switch. I have two disk groups per server, for a total of ten disk groups. I would like to have the first disk group in each server to contribute to one data store and the other disk group to another data store. </p>

<p>How can I accomplish this?</p>
","<vmware-vsan>","2017-12-03 19:49:05"
"961506","Do I need to setup a Docker network to expose container to the Internet, and if so, how to do it?","<blockquote>
  <p><strong>Summary</strong></p>
</blockquote>

<p>I mounted a local Jenkins Build Server in a local Docker Container and exposed ports 8080 both from the host to the Jenkins container.</p>

<p>I want to test the integration of Jenkins with Azure DevOps so Jenkins is the build server for the project.</p>

<blockquote>
  <p><strong>Docker</strong></p>
</blockquote>

<p>I simply used the official jenkins/jenkins Docker image from Docker Hub and ran it like so:</p>

<pre><code>docker run -d -p 8080:8080 --name jenkins jenkins/jenkins
</code></pre>

<p>Container is reachable from <code>localhost:8080</code> and responds well.</p>

<blockquote>
  <p><strong>Windows 10 Pro</strong></p>
</blockquote>

<p>In Windows 10 Firewall I created two port rules:</p>

<ol>
<li>Inbound, TCP, Local Port: All, Remote Port: All</li>
<li>Inbound, UDP, Local Port: All, Remote Port: All</li>
</ol>

<blockquote>
  <p><strong>Bell Fibe HomeHub 2000</strong></p>
</blockquote>

<p>I created a port forwarding for my IP address which the modem did assosciate to the correct computer where the Docker container is running.</p>

<p><a href=""https://i.sstatic.net/9X0p9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9X0p9.png"" alt=""Home Hub 2000 Port Forwarding""></a></p>

<blockquote>
  <p><strong>Internet</strong></p>
</blockquote>

<p>When I try to access my <code>[public ip address]:80</code> that I obtain by googling ""what's my ip address"", I hit this page. </p>

<p><a href=""https://i.sstatic.net/4e3Z5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4e3Z5.png"" alt=""Site can&#39;t be reached""></a></p>

<blockquote>
  <p><strong>Related SF Question</strong></p>
</blockquote>

<p><a href=""https://serverfault.com/questions/868837/webserver-in-docker-container-is-not-reachable-via-the-internet"">Webserver in Docker container is not reachable via the Internet</a></p>

<p>And I'm unsure as to how to proceed or even if it's necessary. I'm rather a software architect than a system admin, and I don't really understand the need of a Docker network if my ports forwarding is right.</p>
","<docker><port-forwarding><windows-10>","2019-04-04 13:43:02"
"818892","CloudFlare SSL Full Nginx Puma Rails","<p>I have a rails app with force_ssl turned on.
I generated DH params
I'm using Puma as web server.
For this configuration, I'm fine with a self-assigned ssl cert, which I generated together with a key and placed in etc/ssl</p>

<p>My domain is set up on cloudflare and I turned ""Full"" SSL</p>

<p>My nginx config is as follows. I think the solution to my problem is probably in this setting:</p>

<pre><code>upstream puma {
  server unix:///home/user/app/production/shared/tmp/sockets/puma.sock;
}

server {
  server_name 123.123.12.123;
  add_header X-Frame-Options ""SAMEORIGIN"";
  return 301 https://awebsite.com$request_uri;
}
server {
  server_name awebsite.com www.awebsite.com;
  add_header X-Frame-Options ""SAMEORIGIN"";
  return 301 https://awebsite.com$request_uri;
}
server {
  listen 443;
  server_name www.awebsite.com;
  add_header X-Frame-Options ""SAMEORIGIN"";
  return 301 https://awebsite.com$request_uri;
}

# Server configuration
server {
  listen 80;
  listen 443 default ssl;
  server_name awebsite.com;

  ssl_protocols TLSv1 TLSv1.1 TLSv1.2;
  ssl_dhparam /etc/ssl/dhparams.pem;
  ssl_ciphers 'ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA256:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA:DHE-RSA-AES256-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:AES:CAMELLIA:DES-CBC3-SHA:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!PSK:!aECDH:!EDH-DSS-DES-CBC3-SHA:!EDH-RSA-DES-CBC3-SHA:!KRB5-DES-CBC3-SHA';
  ssl_prefer_server_ciphers on;
  ssl_session_cache shared:ssl_session_cache:10m;
  ssl_certificate /etc/ssl/server.crt;
  ssl_certificate_key /etc/ssl/server.key;

  root /home/user/app/production/current/public;
  access_log /home/user/app/production/current/log/nginx.access.log;
  error_log /home/user/app/production/current/log/nginx.error.log info;

  location ^~ /assets/ {
    gzip_static on;
    expires max;
    add_header Cache-Control public;
  }

  try_files $uri/index.html $uri @puma;
  location @puma {
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $scheme;
    proxy_set_header Host $http_host;
    proxy_redirect off;

    proxy_pass http://puma;
  }

  error_page 500 502 503 504 /500.html;
  error_page 401 403 404 /404.html;
  client_max_body_size 10M;
  keepalive_timeout 10;
}
</code></pre>

<p>I'm getting this nginx error:</p>

<pre><code>2016/12/05 17:33:08 [info] 16279#16279: *1 epoll_wait() reported that client prematurely closed connection, so upstream connection is closed too while sending request to upstream, client: 123.123.12.123, server: awebsite.com, request: ""GET / HTTP/1.1"", upstream: ""http://unix:///home/user/app/production/shared/tmp/sockets/puma.sock:/"", host: ""awebsite.com"", referrer: ""https://awebsite.com/""
</code></pre>
","<nginx><ssl><ruby-on-rails><cloudflare>","2016-12-05 21:34:44"
"747220","How to check which domains and groups I belong to?","<p>This is pertaining to Windows 7 and Windows Server 2008 R2, but probably may apply to other versions.</p>

<p><strong>Question 1</strong>: How to find out preferably from CLI all the domains a user belongs to (it is probably not trivial, but somehow it can be done)?</p>

<p><strong>Question 2</strong>: How to find out all the groups a user is a member of form a domain that this user is not currently logged to (provided a user knows the required credentials)?</p>
","<windows-server-2008><active-directory><windows-7><command-line-interface>","2016-01-06 21:40:11"
"961857","BIND zone file error","<p>Im having an issue with a zone file in BIND. </p>

<p>Here is the zone file, private information is masked.</p>

<pre><code>$ORIGIN xx.com.
$TTL 60

@                       IN SOA  ns01.xx.com. hostmaster.xx.com. (
                            2019040702   ; serial number DATE PLUS SEQUENCENR
                            3600       ; refresh (15 minutes)
                            3600       ; retry (10 minutes)
                            86400      ; expire (1 day)
                            3600       ; minimum (1 hour)
                            )

                    NS      ns01.xx.com.
                    NS      ns02.xx.com.

; Mail-servers
@                       MX      10      ASPMX.L.GOOGLE.COM.
@                       MX      20      ALT1.ASPMX.L.GOOGLE.COM.
@                       MX      20      ALT2.ASPMX.L.GOOGLE.COM.
@                       MX      30      ASPMX2.GOOGLEMAIL.COM.
@                       MX      30      ASPMX3.GOOGLEMAIL.COM.

@                       A       1.2.3.4
www                     A       1.2.3.4
</code></pre>

<p>Whenever i run named-checkconf xx.com i get this error:</p>

<pre><code>xx.com:1: unknown option '$ORIGIN'
xx.com:5: unknown option 'serial'
xx.com:6: unknown option 'refresh'
xx.com:7: unknown option 'retry'
xx.com:8: unknown option 'expire'
xx.com:9: unknown option 'minimum'
xx.com:15: unknown option 'Mail-servers'
xx.com:24: unexpected token near end of file
</code></pre>

<p>And then BIND doesnt start up. </p>

<p>Anyone see any issues with this file?</p>
","<domain-name-system><centos><bind><dns-zone>","2019-04-07 01:27:53"
"819233","How to change IP Address of remote VM with Powershell?","<p>I am creating an auto-build process in Powershell and need to build a VM with a static IP address. The VM gets built by the script and is assigned a DHCP IP Address. I can query the Hyper-V host to get the IP Address of the newly created VM with <code>Get-NetAdapter</code> so that I can remote to it with <code>Invoke-Command -ComputerName &lt;ip_address&gt;</code>. All good so far. The next line in my powershell script then changes the IP address of the remote VM with <code>New-NetIPAddress</code> but the <code>Invoke-Command</code> cmdlet then times out after 4 minutes.</p>

<p>My question is, is there a better way to change the IP remotely? Can I set a timeout and handle it gracefully somehow? At the moment it sits there for 4 mins and then gives a connection error.</p>

<p><strong>EDIT</strong>: If I change the IpAddress in a job as suggested by Gerald, how would I pass parameters into the job? My current code for changing the IP looks like:</p>

<p><code>Invoke-Command -ComputerName $TempIpAddress -Credential $cred -scriptblock {param ($IpAddress, $DefaultGateway) Get-NetIpAddress | Where-Object {$_.InterfaceAlias -match ""Ethernet"" -and $_.AddressFamily -eq ""IPv4""} | New-NetIPAddress –IPAddress $IpAddress –PrefixLength 24 -DefaultGateway $DefaultGateway} -ArgumentList $NewIpAddress, $DefaultGateway</code></p>

<p><strong>EDIT2</strong>: I tried this, the job gets created but doesn't do anything. And when I run <code>Get-Job</code> nothing gets returned.</p>

<p><code>Invoke-Command -ComputerName $TempIpAddress -Credential $cred -scriptblock {Start-Job -ScriptBlock {param ($IpAddress, $DefaultGateway) Get-NetIpAddress | Where-Object {$_.InterfaceAlias -match ""Ethernet"" -and $_.AddressFamily -eq ""IPv4""} | New-NetIPAddress –IPAddress $IpAddress –PrefixLength 24 -DefaultGateway $DefaultGateway} -ArgumentList $NewIpAddress, $DefaultGateway}</code></p>
","<powershell><powershell-v5.0>","2016-12-07 09:53:21"
"747320","How could I share a dedicated IP amongst multiple servers?","<p>I'm trying to split up a big server for a few people. They aren't going to run a website on it or anything like that but they want to at least connect to the server remotely.</p>

<p>My hosting provider is charging me about $3 a month for each dedicated IP and I think it's a bit ridiculous, especially for my use case. I was wondering if there was a way to connect to the same IP but give each virtual machine a different port?</p>

<pre><code>Server 1: 50.50.50.50:5261 or bobserver.mydomain.com
Server 2: 50.50.50.50:5263 or mikeserver.mydomain.com
Server 3: 50.50.50.50:5266 or johnserver.mydomain.com
</code></pre>

<p>I know this is how Microsoft Azure kind of does their servers and I was wondering if there was anyway to replicate something like that. I've tried researching everywhere and I can't find anything.</p>

<p>Again, it doesn't matter how efficient or stable it would be.</p>
","<linux><windows><networking><virtualization><ip>","2016-01-07 09:52:56"
"819364","Correct directory for my own jar applications, and their configuration","<p>I coded a Java application (in this case an Apache Camel application) that I package as a jar file. It's an executable program (not a library). I am wondering where is the appropriate place for such jar application on my production Unix system, <code>/usr/local/name-of-the-app/</code>, <code>/usr/local/bin/name-of-the-app/</code>, <code>/opt/name-of-the-app/</code>?<br>
Also this application relies on some property/configuration file (containing for instance database credentials, API keys, etc.), should this file be in <code>/etc/name-of-the-app/config.properties</code>?</p>
","<linux><filesystems><java>","2016-12-07 19:35:41"
"747483","Process wont run","<p>I have a piece of data management software that I cannot seem to get to run. Whenever I start the process it gives me this</p>

<pre><code>[root@001 install]# bash niagarad_generic start
Starting Niagara Daemon: [  OK  ]
</code></pre>

<p>but when I check the status I get </p>

<pre><code>[root@001 install]# bash niagarad_generic status
niagarad is stopped
</code></pre>

<p>How do I tell why the service is not starting?</p>

<p>I ran it in debug mode and this is what I got</p>

<pre><code>[root@001 install]# Niagara-3.8.41bash -x niagarad_generic start
-bash: Niagara-3.8.41bash: command not found
[root@001 install]# bash -x niagarad_generic start
+ DAEMON=/bin/niagarad
+ PROG='Niagara Daemon'
+ SERVICE_NAME=naxd
+ PIDDIR=/var/run/niagarad
+ PIDFILE=/var/run/niagarad/niagarad.pid
+ REDHAT=false
+ DEBIAN=false
+ '[' -f /etc/init.d/functions ']'
+ . /etc/init.d/functions
++ TEXTDOMAIN=initscripts
++ umask 022
++ PATH=/sbin:/usr/sbin:/bin:/usr/bin
++ export PATH
++ '[' 676 -ne 1 -a -z '' ']'
++ /bin/mountpoint -q /cgroup/systemd
++ /bin/mountpoint -q /sys/fs/cgroup/systemd
++ case ""$0"" in
++ '[' -z '' ']'
++ COLUMNS=80
++ '[' -z '' ']'
++ '[' -c /dev/stderr -a -r /dev/stderr ']'
+++ /sbin/consoletype
++ CONSOLETYPE=pty
++ '[' -z '' ']'
++ '[' -z '' ']'
++ '[' -f /etc/sysconfig/i18n -o -f /etc/locale.conf ']'
++ . /etc/profile.d/lang.sh
++ unset LANGSH_SOURCED
++ '[' -z '' ']'
++ '[' -f /etc/sysconfig/init ']'
++ . /etc/sysconfig/init
+++ BOOTUP=color
+++ RES_COL=60
+++ MOVE_TO_COL='echo -en \033[60G'
+++ SETCOLOR_SUCCESS='echo -en \033[0;32m'
+++ SETCOLOR_FAILURE='echo -en \033[0;31m'
+++ SETCOLOR_WARNING='echo -en \033[0;33m'
+++ SETCOLOR_NORMAL='echo -en \033[0;39m'
++ '[' pty = serial ']'
++ __sed_discard_ignored_files='/\(~\|\.bak\|\.orig\|\.rpmnew\|\.rpmorig\|\.rpmsave\)$/d'
++ '[' '' = 1 ']'
+ REDHAT=true
+ DEBIAN=false
+ LOCKFILE=/var/lock/subsys/niagarad
+ '[' '!' -d /var/run/niagarad ']'
+ true
+ . /etc/sysconfig/network
++ NETWORKING=yes
++ GATEWAYDEV=venet0
++ NETWORKING_IPV6=yes
++ IPV6_DEFAULTDEV=venet0
+ '[' yes = no ']'
+ '[' root '!=' niagarad ']'
+ SUDO_REQUIRES_TTY=true
+ true
+ '[' root = root ']'
+ cat /etc/sudoers
+ grep '^Defaults.*\!requiretty'
+ cat /etc/sudoers
+ grep '^Defaults.*requiretty'
+ SUDO_REQUIRES_TTY=true
+ HAS_TTY=false
+ tty -s
+ '[' -t 0 ']'
++ who
+ WHO_OUTPUT='root     pts/0        2016-01-07 14:54 (dhcp-d0-3-4b-d1-33-5a.cpe.wightman.ca)'
+ '[' 78 '!=' 0 ']'
+ HAS_TTY=true
+ true
+ true
+ sudo -u niagarad /usr/bin/niagaradctl start
Starting Niagara Daemon: [  OK  ]
+ exit 0
</code></pre>
","<centos7>","2016-01-07 20:15:35"
"886959","How can I adjust the name of a process I run?","<p>I have a rack/ruby application on a server. I run it as follows:</p>

<pre><code>  nohup rackup -o 0.0.0.0 -p 1234 &amp;
</code></pre>

<p>In the output of ""top"" command it appears as simply ""ruby""</p>

<pre><code>PID   USERNAME    THR PRI NICE   SIZE    RES STATE    TIME    WCPU COMMAND

98421  my_user123       3  20    0   140M 29576K select   0:29   0.01% ruby
</code></pre>

<p>How can I adjust its name in the ""top""? What if I run 5 such web apps, how would I distinguish which is which if they're all ""ruby""?</p>
","<linux><freebsd><process><top><rack>","2017-12-07 03:31:49"
"819471","How to setup DNS to connect 2 servers with 1 Domain?","<p>I have 2 Servers:</p>

<pre><code>Server 1 IPs: 
45.35.56.194
45.35.56.195

Server 2 IPs:
173.254.214.217
173.254.214.218
</code></pre>

<p>Each server has different clients, therefore, Server 1 client has nothing to do with Server 2 clients.</p>

<p>To my Server 1 clients I gave these Name Servers:</p>

<pre><code>ns1.resourcesofnet.net - 45.35.56.194
ns1.resourcesofnet.net - 45.35.56.195
</code></pre>

<p>To my Server 2 clients I gave these Name Servers:</p>

<pre><code>ns3.resourcesofnet.net - 173.254.214.217
ns4.resourcesofnet.net - 173.254.214.218
</code></pre>

<p>In my domain registrar, I have created 4 child Name Servers.</p>

<pre><code>ns1.resourcesofnet.net - 45.35.56.194
ns2.resourcesofnet.net - 45.35.56.195
ns3.resourcesofnet.net - 173.254.214.217
ns4.resourcesofnet.net - 173.254.214.218
</code></pre>

<p>On my both servers I created a vhost for resourcesofnet.net and added A records accordingly.</p>

<p>Now the problem is, it's been many days, my DNS still randomly stops resolving for some domains.</p>

<p>Server 1 Example site:</p>

<pre><code>www.unique-links.com.pk
</code></pre>

<p>Server 2 Example Site:</p>

<pre><code>al-quba.com
</code></pre>

<p>Can anyone please check why is it happening? Has it got anything to do with configuration of DNS on both Servers?</p>

<p>Here is <a href=""http://dnscheck.pingdom.com/?domain=resourcesofnet.net"" rel=""nofollow noreferrer"">DNS report</a>.</p>

<p>Any help would be much appreciated.</p>

<p>Thanks</p>
","<domain-name-system><nameserver><dns-zone><a-record>","2016-12-08 09:44:58"
"747598","Can't connect to any remote mongoDb","<p>I'm running ubuntu 14.04 and am unable to connect to any remote mongodb server on my home network. I'm pretty sure it worked only a month ago. Furthermore, I have no problems connecting from my work network. </p>

<p>For example, I have an aws server running mongodb. I can ssh to it, but trying to connect with mongo results in the uniform connection error (""couldn't connect to server... connection attempt failed"").</p>

<p>Any suggestions on how I can troubleshoot this?</p>

<p>EDIT: I just confirmed the same behavior on a windows machine. Could it have to do with my ISP?</p>

<p>EDIT 2: turns out it was a problem with the router. I'd still be interested to know of there's any way to identify where the issue is from the computer, without going to the router. Though even if I had looked at the router I wouldn't have known that it was blocking all non-standard ports. </p>
","<remote-access><ubuntu-14.04><mongodb>","2016-01-08 10:49:39"
"962270","How to forward all incoming packets to certain ip?","<p>I want to forward all tcp packets incoming to my PC`s <code>7778</code> port to the connected device. It's ip - <code>192.168.137.111</code>. Nmap shows that it listens for <code>7778</code> port and I can ping it from my PC. According to <a href=""https://serverfault.com/a/765539/516465"">this</a> answer, I've added the following rules:<br>
<code>iptables -t nat -A PREROUTING -p tcp --dport 7778 -j DNAT --to-destination 192.168.137.111:7778</code><br>
<code>iptables -t nat -A POSTROUTING -p tcp -d 192.168.137.111 --dport 7778 -j SNAT --to-source 10.99.220.62</code><br><br>
Where <code>10.99.220.62</code> is my PC`s ip.
<br>Also, I've added <code>iptables -I INPUT -p tcp -m tcp --dport 7778 -j ACCEPT</code><br><br>
Then, when I'm trying to connect to <code>10.99.220.62:7778</code> from another computer, <code>tcpdump -i enp3s0 'port 7778'</code> shows one packet: <code>19:56:08.035819 IP 10.99.221.104.60827 &gt; alexandr-All-Series.7778: Flags [S], seq 568409282, win 8192, options [mss 1460,nop,nop,sackOK], length 0</code> <br> But when I'm trying to tcpdump <code>enx006037a1f5ef</code> that ""stands"" for the connected device, it shows nothhing. And I don't get any response on the computer that has sent the packet.<br>
I'm using Mint 19</p>
","<networking><iptables>","2019-04-09 17:33:00"
"962291","about networking and VPN connections between different sites","<p><a href=""https://i.sstatic.net/2HSQX.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2HSQX.jpg"" alt=""topology""></a></p>

<p>Hello everyone, i have a general networking question about an infrastructure described at the scheme above.</p>

<p>We have multiple sites connected at the HQs via Site2Site VPN using zyxel zywall routers/fw .</p>

<p>Each site has 1 lan like this: 
site 1: 192.168.1.X/24
site 2: 192.168.2.X/24
site 3: 192.168.3.X/24
HQ:     192.168.10.X/24</p>

<p>I want to connect a server machine from site 1, site 2 and site 3 to a cloud infrastructure using Point2Site VPN.</p>

<p>Is it possible to have point2site VPN(server machine to azure) from a site that already has site2site vpn connection at the same time?</p>

<p>Do i have to create a separate network for the server that will be connected with p2s at azure?</p>

<p>I hope that my post is clear and understandable. feel free to ask me anything.</p>

<p>Thanks in advance!</p>
","<vpn><azure><site-to-site-vpn><point-to-site-vpn>","2019-04-09 19:00:32"
"887286","Remove wildcard entries between IP's in file","<p>I have a very lengthy file(more than 4000 lines generated per day)in the following format.</p>

<pre><code>yqiemmtcveihai

test-trans

10.227.30.66

&lt;-----------&gt;

14.192.17.143

&lt;-----------&gt;

peuddnbtmzdptw

Ttest-trans1

10.227.30.67

&lt;-----------&gt;

14.192.17.142

&lt;-----------&gt;

cqykfavuxpuqiq

Med

202.21.32.218

&lt;-----------&gt;

hziuqbvuncwkie

Myubun

202.21.32.230

&lt;-----------&gt;
</code></pre>

<p>I want to remove the <code>&lt;-----------&gt;</code> entry only between the IP's. So the result should be in this format.</p>

<pre><code>yqiemmtcveihai

test-trans

10.227.30.66

14.192.17.143

&lt;-----------&gt;

peuddnbtmzdptw

Ttest-trans1

10.227.30.67

14.192.17.142

&lt;-----------&gt;

cqykfavuxpuqiq

Med

202.21.32.218

&lt;-----------&gt;

hziuqbvuncwkie

Myubun

202.21.32.230

&lt;-----------&gt;
</code></pre>

<p>Can anyone suggest to me a script, preferably in bash, to achieve this? I was breaking my head, but could not figure it out.</p>
","<linux><bash><scripting>","2017-12-09 00:10:26"
"819741","Cannot Reach Website - GoDaddy Domain & AWS Server","<p>I'm in urgent need of help. My website is down and I don't know where to begin troubleshooting.</p>

<p>I have a GoDaddy domain and am running an AWS EC2 instance. </p>

<p><strong>The Issue:</strong>  </p>

<ul>
<li><p>When I try to access the website from my chrome browser, it keeps loading, and keeps saying ""Waiting for www.xyz.com..."". In my Developer Tools window, I do not see any request being sent to the URL.</p></li>
<li><p>I am able to connect to my EC2 instance via terminal and see the server running live. No request is being received. </p></li>
</ul>

<p>Where could the problem be? How do I check where the request is failing? </p>

<p>Any help would be appreciated.</p>

<p><strong>++++++++++ BELOW IS THE TRACEROUTE Results ++++++++++++++</strong></p>

<p><code>traceroute to www.xyz.co (xx.xx.xx.xx), 30 hops max, 60 byte packets</code></p>

<pre><code>1               *   *   *


2   core22.hetzner.de   213.239.229.133 de  0.188 ms         
core21.hetzner.de   213.239.229.129 de  0.246 ms    0.251 ms


3   core1.hetzner.de    213.239.245.177 de  4.909 ms         
core4.hetzner.de    213.239.245.18  de  4.871 ms     
core1.hetzner.de    213.239.245.177 de  4.909 ms


4   juniper4.ffm.hetzner.de 213.239.245.10  de  4.948 ms    4.956 ms     
        in  


5   ae1-710.fra20.core-backbone.com 80.255.15.121   de  4.960 ms    4.974 ms    4.982 ms


6   ffm-b4-link.telia.net   213.248.81.209      4.985 ms    5.020 ms    5.026 ms


7   ffm-bb3-link.telia.net  62.115.120.3        5.455 ms         
ffm-bb3-link.telia.net  62.115.120.1        5.545 ms     
ffm-bb4-link.telia.net  62.115.120.9        5.459 ms


8   nyk-bb4-link.telia.net  62.115.139.15       99.212 ms        
ffm-b1-link.telia.net   62.115.116.162      5.777 ms     
nyk-bb1-link.telia.net  213.155.135.61      93.229 ms


9   chi-b21-link.telia.net  213.155.131.241     116.821 ms       
ffm-bb4-link.telia.net  62.115.116.159      104.857 ms   
chi-b21-link.telia.net  80.91.246.18        123.398 ms


10  hbg-bb1-link.telia.net  62.115.123.81       12.153 ms        
hbg-bb1-link.telia.net  62.115.123.77       12.160 ms    
sea-b1-link.telia.net   62.115.117.48       162.375 ms


11  amazon-ic-307566-sea-b1.c.telia.net 62.115.47.198       164.111 ms       
kbn-bb4-link.telia.net  62.115.119.250      24.006 ms    
amazon-ic-307566-sea-b1.c.telia.net 62.115.47.198       164.111 ms


12  nyk-bb1-link.telia.net  80.91.249.24        96.662 ms   *   *


13              *   *   *


14  sea-b1-link.telia.net   62.115.116.140      163.699 ms  164.885 ms  167.923 ms


15  amazon-ic-307562-sea-b1.c.telia.net 213.248.92.242      166.730 ms  *    
amazon-ic-302506-sea-b1.c.telia.net 213.248.84.190      164.407 ms


16              *   *   *


17              *   *   *


18              *   *   *
</code></pre>

<p>No reply for 3 hops. Assuming we reached firewall.</p>
","<amazon-ec2><amazon-web-services><http><connection><godaddy>","2016-12-09 08:46:42"
"887365","How To Mount USB External Storage Drive on to ESXi 5.5 Host for VM backup","<p>How To Mount USD External Storage Drive on to ESXi 5.5 Host for VM backup?</p>

<p>After USB Drive plugin, ""esxcli storage core device list"" shows there is a usb drive attached. But unable to access it. </p>

<pre><code>esxcli storage core device list
mpx.vmhba38:C0:T0:L0
   Display Name: Local USB Direct-Access (mpx.vmhba38:C0:T0:L0)
   Has Settable Display Name: false
   Size: 1907729
   Device Type: Direct-Access 
   Multipath Plugin: NMP
   Devfs Path: /vmfs/devices/disks/mpx.vmhba38:C0:T0:L0
   Vendor: Seagate 
   Model: BUP Slim BL     
   Revision: 0108
   SCSI Level: 2
   Is Pseudo: false
   Status: on
   Is RDM Capable: false
   Is Local: true
   Is Removable: true
   Is SSD: false
   Is Offline: false
   Is Perennially Reserved: false
   Queue Full Sample Size: 0
   Queue Full Threshold: 0
   Thin Provisioning Status: unknown
   Attached Filters: 
   VAAI Status: unsupported
   Other UIDs: vml.0000000000766d68626133383a303a30
   Is Local SAS Device: false
   Is USB: true
   Is Boot USB Device: false
   No of outstanding IOs with competing worlds: 32
</code></pre>

<p>Note: I stopped usbarbitrator.</p>

<pre><code>/etc/init.d/usbarbitrator status
usbarbitrator is not running
</code></pre>
","<vmware-esxi><usb>","2017-12-10 01:35:33"
"887410","What will happen if a single AWS region center becomes unavailable?","<p>What will happen if an individual AWS region becomes unavailable? I am quoting from their website that says that,</p>

<blockquote>
  <p>It is important to remember that each AWS Region is completely
  independent. Any Amazon RDS activity you initiate (for example,
  creating database instances or listing available database instances)
  runs only in your current default AWS Region. The default AWS Region
  can be changed in the console, by setting the EC2_REGION environment
  variable, or it can be overridden by using the --region parameter with
  the AWS Command Line Interface.</p>
</blockquote>

<p>It is my wish to have at least several synchronized data centers in the event of downtime.</p>
","<amazon-web-services>","2017-12-10 13:11:53"
"887454","/etc/fstab doesn't show temp but temp is mounted. CentOS 7 on linode","<p>In CentOS 7, I have been told to secure the /tmp folder with with noexec,nosuid in /etc/fstab</p>

<p>When I view the /etc/fstab, I can only see few lines without /tmp mounting option.</p>

<p>I have created a tmp disk (1gb) and assign this disk via linode manager to /dev/sdc</p>

<p>and in order to mount this in /etc/fstab</p>

<p>I write the following:</p>

<p>/dev/sdc /tmp  ext4    defaults,nodev,nosuid,noexec 0 0
then I save this and reboot the server.</p>

<p>I test my /tmp folder by downloading something using wget but nothing got saved in the folder.</p>

<p>How do I secure the /tmp folder in /tmp and /var/tmp?</p>

<p>I read that I must create a new drive for /tmp in order for it to be able to mount as nosuid and noexec</p>

<p>Can anyone guide me? I'm new to CentOS 7.</p>
","<centos7><linode><fstab>","2017-12-10 23:49:16"
"962625","trigger a autologon from computer startup script","<p>I am looking for a way, to automatically logon a User from computer startup script, without rebooting the machine. I know, I can write in <a href=""https://support.microsoft.com/en-us/help/324737/how-to-turn-on-automatic-logon-in-windows"" rel=""nofollow noreferrer"">HKLM\SOFTWARE\Microsoft\Windows NT\CurrentVersion\Winlogon... Keys</a> or use <a href=""https://docs.microsoft.com/en-us/sysinternals/downloads/autologon"" rel=""nofollow noreferrer"">Sysinternals autologon.exe</a>, but this all don't work without a reboot.</p>

<p>OS: Windows >= 7</p>

<p>A while ago, I have seen a small until, who could do it. But I forgot about it and can't find it again. Or maybe there is any other solution?</p>

<p>I think, it is only possible with a <a href=""https://docs.microsoft.com/en-us/windows/desktop/secauthn/gina"" rel=""nofollow noreferrer"">GINA plugin</a>?</p>
","<windows><group-policy><startup-scripts>","2019-04-11 15:01:00"
"748059","G-WAN: ""no listener"" error after installation on Ubuntu Server 14.04 with XFS filesystem","<p>I'm desperately trying to start G-WAN since two days without any progress, so please let me now ask for your gentle help because I'm becoming (more) crazy !</p>

<p>Here is the log after a start with ./gwan -d -w 1</p>

<pre><code>[Sun Jan 10 13:32:58 2016 GMT] ------------------------------------------------
[Sun Jan 10 13:32:58 2016 GMT] G-WAN 4.3.14 64-bit (Mar 14 2013 07:33:12)
[Sun Jan 10 13:32:58 2016 GMT] ------------------------------------------------
[Sun Jan 10 13:32:58 2016 GMT] Local Time: Sun, 10 Jan 2016 14:32:58 GMT+1
[Sun Jan 10 13:32:58 2016 GMT] RAM: (7.27 GiB free + 0 shared + 704.00 KiB buffers) / 7.84 GiB total
[Sun Jan 10 13:32:58 2016 GMT] Physical Pages: 7.27 GiB / 7.84 GiB
[Sun Jan 10 13:32:58 2016 GMT] DISK: 115.94 GiB free / 118.47 GiB total
[Sun Jan 10 13:32:58 2016 GMT]  Filesystem     Type      Size  Used Avail Use% Mounted on
[Sun Jan 10 13:32:58 2016 GMT]  udev           devtmpfs  3.9G  4.0K  3.9G   1% /dev
[Sun Jan 10 13:32:58 2016 GMT]  tmpfs          tmpfs     798M  468K  797M   1% /run
[Sun Jan 10 13:32:58 2016 GMT]  /dev/sda2      xfs       119G  2.2G  116G   2% /
[Sun Jan 10 13:32:58 2016 GMT]  none           tmpfs     4.0K     0  4.0K   0% /sys/fs/cgroup
[Sun Jan 10 13:32:58 2016 GMT]  none           tmpfs     5.0M     0  5.0M   0% /run/lock
[Sun Jan 10 13:32:58 2016 GMT]  none           tmpfs     3.9G     0  3.9G   0% /run/shm
[Sun Jan 10 13:32:58 2016 GMT]  none           tmpfs     100M     0  100M   0% /run/user
[Sun Jan 10 13:32:58 2016 GMT]  /dev/sda1      xfs       186M   46M  141M  25% /boot
[Sun Jan 10 13:32:58 2016 GMT] 162 processes, including pid:5280 './gwan -d -w 1'
[Sun Jan 10 13:32:58 2016 GMT] Page-size:4,096 Child-max:63,605 Stream-max:16
[Sun Jan 10 13:32:58 2016 GMT] CPU: 1x Intel(R) Atom(TM) CPU C2750 @ 2.40GHz
[Sun Jan 10 13:32:58 2016 GMT]  0 id: 0     0
[Sun Jan 10 13:32:58 2016 GMT]  1 id: 1     1
[Sun Jan 10 13:32:58 2016 GMT]  2 id: 2     2
[Sun Jan 10 13:32:58 2016 GMT]  3 id: 3     3
[Sun Jan 10 13:32:58 2016 GMT]  4 id: 4     4
[Sun Jan 10 13:32:58 2016 GMT]  5 id: 5     5
[Sun Jan 10 13:32:58 2016 GMT]  6 id: 6     6
[Sun Jan 10 13:32:58 2016 GMT]  7 id: 7     7
[Sun Jan 10 13:32:58 2016 GMT] Cores: possible:0-7 present:0-7 online:0-7 
[Sun Jan 10 13:32:58 2016 GMT] L1d cache:   24K line:64     0
[Sun Jan 10 13:32:58 2016 GMT] L1i cache:   32K line:64     0
[Sun Jan 10 13:32:58 2016 GMT] L2  cache: 1024K line:64   0-1
[Sun Jan 10 13:32:58 2016 GMT] NUMA node #1 0-7
[Sun Jan 10 13:32:58 2016 GMT] CPU(s):1, Core(s)/CPU:4, Thread(s)/Core:2
[Sun Jan 10 13:32:58 2016 GMT] Bogomips: 4,800.19
[Sun Jan 10 13:32:58 2016 GMT] Virtualization: VT-x
[Sun Jan 10 13:32:58 2016 GMT] './gwan -w 1' used to override 1 x 4-Core CPU(s)
[Sun Jan 10 13:32:58 2016 GMT]   using   1 workers 0[00000001]0
[Sun Jan 10 13:32:58 2016 GMT]   among   8 threads 0[11111111]7
[Sun Jan 10 13:32:58 2016 GMT] 64-bit little-endian (least significant byte first)
[Sun Jan 10 13:32:58 2016 GMT] Ubuntu 14.04.3 LTS \n \l (3.13.0-74) 64-bit
[Sun Jan 10 13:32:58 2016 GMT] user: root (uid:0), group: root (uid:0), members: lm4ppdcd
[Sun Jan 10 13:32:58 2016 GMT] system  fd_max: 1,024
[Sun Jan 10 13:32:58 2016 GMT] program fd_max: 1,024
[Sun Jan 10 13:32:58 2016 GMT] updated fd_max: 500,000
[Sun Jan 10 13:32:58 2016 GMT] Available network interfaces (2):
[Sun Jan 10 13:32:58 2016 GMT] 127.0.0.1
[Sun Jan 10 13:32:58 2016 GMT] 62.210.169.132
[Sun Jan 10 13:32:58 2016 GMT] * no listener in /opt/gwan64

[Sun, 10 Jan 2016 13:32:58 GMT] * child normal exit(1)

[Sun Jan 10 13:32:58 2016 GMT] * no listener in /opt/gwan64

[Sun, 10 Jan 2016 13:32:58 GMT] * child normal exit(1)

[Sun Jan 10 13:32:58 2016 GMT] * no listener in /opt/gwan64

[Sun, 10 Jan 2016 13:32:58 GMT] * child normal exit(1)

[Sun Jan 10 13:32:58 2016 GMT] * no listener in /opt/gwan64

[Sun, 10 Jan 2016 13:32:58 GMT] * child died 3 times within 3 seconds
</code></pre>

<p>I've turned all files and directories under gwan64 to 777 and all of them are now owned by root in the root group.</p>

<p>I've changed the active interface from em1 to eth0 too !</p>

<p>Thanks in advance for your guidance to solve this problem encountered on my new server and kind regards, Germain@ADtlas.com</p>

<p>Hi Law29, here is the result of ls -ltrR /opt/gwan64</p>

<pre><code>  /opt/gwan64:
  total 220
  drwxrwxrwx 2 root root     21 Aug 13  2011 fonts
  -rwxrwxrwx 1 root root   5619 Aug 13  2011 license.txt
  -rwxrwxrwx 1 root root    877 Dec 21  2011 hello.c
  -rwxrwxrwx 1 root root   3138 Jan  6  2012 readme.txt
  -rwxrwxrwx 1 root root   1056 Jan 18  2012 argv.c
  -rwxrwxrwx 1 root root   1322 Jan 27  2012 main.c__
  drwxrwxrwx 6 root root     56 Dec 25  2012 libraries
  drwxrwxrwx 2 root root   4096 Mar 10  2013 include
  -rwxrwxrwx 1 root root 188152 Mar 14  2013 gwan
  drwxrwxrwx 3 root root     28 Jan  9 18:09 62.210.169.132_80
  drwxrwxrwx 2 root root     47 Jan 10 14:32 logs
  -rw-r--r-- 1 root root      0 Jan 10 14:32 gwan_05280.pid
  -rw-r--r-- 1 root root      0 Jan 10 14:32 gwan_05286.pid
  -rw-r--r-- 1 root root      0 Jan 10 14:32 gwan_05290.pid
  -rw-r--r-- 1 root root      0 Jan 10 14:32 gwan_05294.pid
  -rw-rw-rw- 1 root root      0 Jan 10 14:32 Gwan_05279.pid
  -rw-rw-rw- 1 root root   6424 Jan 10 14:32 trace

  /opt/gwan64/fonts:
  total 4
  -rwxrwxrwx 1 root root 576 Jul 29  2010 9pts.gif

  /opt/gwan64/libraries:
  total 0
  drwxrwxrwx 3 root root 16 Feb 23  2012 java
  drwxrwxrwx 2 root root 24 Sep  6  2012 cs
  drwxrwxrwx 2 root root 49 Oct  8  2012 tkcabinet
  drwxrwxrwx 2 root root 41 Oct  8  2012 sqlite3

  /opt/gwan64/libraries/java:
  total 0
  drwxrwxrwx 2 root root 39 Oct  8  2012 api

  /opt/gwan64/libraries/java/api:
  total 8
  -rwxrwxrwx 1 root root 370 Feb 23  2012 Gwan.class
  -rwxrwxrwx 1 root root 874 Aug 28  2012 Gwan.java

  /opt/gwan64/libraries/cs:
  total 4
  -rwxrwxrwx 1 root root 1501 Sep  6  2012 gwan_api.cs

  /opt/gwan64/libraries/tkcabinet:
  total 320
  -rwxrwxrwx 1 root root 190377 Aug  5  2010 tcutil.h
  -rwxrwxrwx 1 root root  45754 Aug  5  2010 tchdb.h
  -rwxrwxrwx 1 root root  82844 Aug  5  2010 tcfdb.c

  /opt/gwan64/libraries/sqlite3:
  total 288
  -rwxrwxrwx 1 root root  20686 Dec  8  2009 sqlite3ext.h
  -rwxrwxrwx 1 root root 268351 Feb  3  2010 sqlite3.h

  /opt/gwan64/include:
  total 112
  -rwxrwxrwx 1 root root  1149 Jan  6  2012 short_types.h
  -rwxrwxrwx 1 root root  4545 Jan 31  2012 xbuffer.h
  -rwxrwxrwx 1 root root  2968 Jun 19  2012 fastcgi.h
  -rwxrwxrwx 1 root root  1530 Jan 10  2013 float.h
  -rwxrwxrwx 1 root root   356 Jan 10  2013 varargs.h
  -rwxrwxrwx 1 root root   545 Jan 10  2013 stddef.h
  -rwxrwxrwx 1 root root   180 Jan 10  2013 stdbool.h
  -rwxrwxrwx 1 root root   401 Jan 10  2013 stdarg.h
  -rwxrwxrwx 1 root root 27140 Feb 19  2013 gwdbg.h
  -rwxrwxrwx 1 root root 46307 Mar 10  2013 gwan.h

  /opt/gwan64/62.210.169.132_80:
  total 0
  drwxrwxrwx 7 root root 64 Jan  3  2013 #62.210.169.132

  /opt/gwan64/62.210.169.132_80/#62.210.169.132:
  total 4
  drwxrwxrwx 2 root root    6 Jan 27  2012 _logs
  drwxrwxrwx 2 root root    6 Apr 20  2012 gzip
  drwxrwxrwx 2 root root  124 Jan  3  2013 handlers
  drwxrwxrwx 3 root root 4096 Mar  8  2013 csp
  drwxrwxrwx 3 root root  135 Mar 14  2013 www

  /opt/gwan64/62.210.169.132_80/#62.210.169.132/_logs:
  total 0

  /opt/gwan64/62.210.169.132_80/#62.210.169.132/gzip:
  total 0

  /opt/gwan64/62.210.169.132_80/#62.210.169.132/handlers:
  total 36
  -rwxrwxrwx 1 root root  783 Mar  3  2012 html.c_
  -rwxrwxrwx 1 root root 2057 May  4  2012 flv.c_
  -rwxrwxrwx 1 root root 9789 May  4  2012 main_generic.c_
  -rwxrwxrwx 1 root root 3744 May  4  2012 main_hello.c_
  -rwxrwxrwx 1 root root 7226 May  4  2012 main_404_redirect.c_
  -rwxrwxrwx 1 root root 2136 May  4  2012 main_restful.c_

  /opt/gwan64/62.210.169.132_80/#62.210.169.132/csp:
  total 592
  -rwxrwxrwx 1 root root  1292 Dec  6  2010 argv.c
  -rwxrwxrwx 1 root root  2791 Jul  4  2011 report.c
  -rwxrwxrwx 1 root root  1727 Nov 28  2011 trace.c
  -rwxrwxrwx 1 root root  8329 Nov 28  2011 captcha.c
  -rwxrwxrwx 1 root root  6674 Nov 28  2011 data_uri.c
  -rwxrwxrwx 1 root root   963 Nov 29  2011 exit.c
  -rwxrwxrwx 1 root root  3125 Dec 17  2011 crash_libc.c
  -rwxrwxrwx 1 root root  2384 Dec 17  2011 crash.c
  -rwxrwxrwx 1 root root  1470 Dec 17  2011 asm.c
  -rwxrwxrwx 1 root root  3153 Dec 21  2011 crash_gwcall.c
  -rwxrwxrwx 1 root root  2180 Dec 27  2011 email.c
  -rwxrwxrwx 1 root root  1672 Dec 27  2011 div_by_zero.c
  -rwxrwxrwx 1 root root   898 Dec 31  2011 servlet_name.c
  -rwxrwxrwx 1 root root  1605 Dec 31  2011 sql.c_
  -rwxrwxrwx 1 root root  1418 Jan  6  2012 redirect.c
  -rwxrwxrwx 1 root root  1706 Jan  8  2012 hello.m_
  -rwxrwxrwx 1 root root  1755 Jan  8  2012 hello.mm_
  -rwxrwxrwx 1 root root  2848 Jan  8  2012 gnustep.m__
  -rwxrwxrwx 1 root root  1038 Jan 15  2012 100.c
  -rwxrwxrwx 1 root root  2187 Jan 15  2012 base64.c
  drwxrwxrwx 2 root root    48 Jan 15  2012 folder
  -rwxrwxrwx 1 root root 23428 Jan 16  2012 sqlite.c_
  -rwxrwxrwx 1 root root 53260 Jan 27  2012 kv_bench.c_
  -rwxrwxrwx 1 root root   882 Jan 27  2012 www_csp.c
  -rwxrwxrwx 1 root root  2070 Jan 30  2012 rates.xml
  -rwxrwxrwx 1 root root   802 Feb 23  2012 report.java
  -rwxrwxrwx 1 root root 10387 Feb 25  2012 charts.c
  -rwxrwxrwx 1 root root  1293 Mar  4  2012 all.java
  -rwxrwxrwx 1 root root  6759 Mar  5  2012 getheaders.c
  -rwxrwxrwx 1 root root  8096 Mar  5  2012 forum.c
  -rwxrwxrwx 1 root root  1414 Mar  5  2012 argv.java
  -rwxrwxrwx 1 root root  2516 Mar 10  2012 bigtable.c
  -rwxrwxrwx 1 root root  1126 Mar 18  2012 power.c
  -rwxrwxrwx 1 root root  1457 Mar 21  2012 cache.c
  -rwxrwxrwx 1 root root 20943 Mar 29  2012 json.c
  -rwxrwxrwx 1 root root 19191 Apr 11  2012 kv.c
  -rwxrwxrwx 1 root root 11911 Apr 22  2012 loan.java
  -rwxrwxrwx 1 root root  2960 Aug 29  2012 cairo.c_
  -rwxrwxrwx 1 root root 11074 Sep 12  2012 comet.c
  -rwxrwxrwx 1 root root  7090 Sep 14  2012 contact.c
  -rwxrwxrwx 1 root root  1423 Sep 15  2012 hellox.c
  -rwxrwxrwx 1 root root 14838 Sep 16  2012 chart.c
  -rwxrwxrwx 1 root root 12005 Sep 16  2012 request.c
  -rwxrwxrwx 1 root root  2915 Sep 16  2012 curl.c_
  -rwxrwxrwx 1 root root  5933 Sep 16  2012 httpdate.c
  -rwxrwxrwx 1 root root 12693 Oct  6  2012 fractal.c
  -rwxrwxrwx 1 root root  7862 Oct 18  2012 served_from.c
  -rwxrwxrwx 1 root root  2487 Nov 22  2012 persistence.c
  -rwxrwxrwx 1 root root  6186 Dec  8  2012 tidy.c_
  -rwxrwxrwx 1 root root  2565 Dec 10  2012 hello.go
  -rwxrwxrwx 1 root root   738 Dec 11  2012 hello.cs
  -rwxrwxrwx 1 root root  2179 Dec 11  2012 hello.pl
  -rwxrwxrwx 1 root root  3395 Dec 11  2012 hello.php
  -rwxrwxrwx 1 root root  2260 Dec 11  2012 hello.js
  -rwxrwxrwx 1 root root  2347 Dec 11  2012 hello.lua
  -rwxrwxrwx 1 root root  2084 Dec 11  2012 hello.py
  -rwxrwxrwx 1 root root  2343 Dec 11  2012 hello.rb
  -rwxrwxrwx 1 root root   810 Dec 11  2012 hello.java
  -rwxrwxrwx 1 root root  1253 Dec 14  2012 mysql.c_
  -rwxrwxrwx 1 root root  3288 Dec 20  2012 memcached.c_
  -rwxrwxrwx 1 root root  5460 Dec 21  2012 json_bench.c
  -rwxrwxrwx 1 root root  1381 Dec 21  2012 hello.cpp
  -rwxrwxrwx 1 root root  1778 Dec 23  2012 throttle.c
  -rwxrwxrwx 1 root root  1747 Dec 25  2012 hello.d_
  -rwxrwxrwx 1 root root   873 Dec 25  2012 hello.scala_
  -rwxrwxrwx 1 root root  3414 Dec 25  2012 rnd.c
  -rwxrwxrwx 1 root root  2457 Dec 25  2012 auth_oauth.c_
  -rwxrwxrwx 1 root root   830 Dec 25  2012 hello.c
  -rwxrwxrwx 1 root root  1043 Dec 25  2012 argv.cs
  -rwxrwxrwx 1 root root  1920 Jan  5  2013 cookies.c
  -rwxrwxrwx 1 root root  4062 Jan 25  2013 setheaders.c
  -rwxrwxrwx 1 root root  2389 Jan 25  2013 noheaders.c
  -rwxrwxrwx 1 root root  1231 Jan 28  2013 redirect2.c
  -rwxrwxrwx 1 root root  2832 Feb 13  2013 stream1.c
  -rwxrwxrwx 1 root root  3130 Feb 13  2013 extern.c
  -rwxrwxrwx 1 root root  3233 Feb 13  2013 stream2.c
  -rwxrwxrwx 1 root root  7909 Feb 18  2013 imgsz.c_
  -rwxrwxrwx 1 root root  1393 Feb 19  2013 cookie.c
  -rwxrwxrwx 1 root root 12674 Feb 19  2013 entity.c
  -rwxrwxrwx 1 root root  2062 Feb 19  2013 entity_size.c
  -rwxrwxrwx 1 root root  9523 Feb 26  2013 loan.cs
  -rwxrwxrwx 1 root root  7164 Feb 26  2013 loan.php
  -rwxrwxrwx 1 root root 12982 Feb 26  2013 loan.c
  -rwxrwxrwx 1 root root  7832 Feb 26  2013 auth_basic.c
  -rwxrwxrwx 1 root root  8788 Mar  2  2013 stream3.c

  /opt/gwan64/62.210.169.132_80/#62.210.169.132/csp/folder:
  total 12
  -rwxrwxrwx 1 root root 1453 Dec 21  2012 hellox.c
  -rwxrwxrwx 1 root root 1033 Dec 21  2012 hello.c
  -rwxrwxrwx 1 root root 1295 Dec 21  2012 argv.c

  /opt/gwan64/62.210.169.132_80/#62.210.169.132/www:
  total 36
  -rwxrwxrwx 1 root root  100 Nov 24  2009 100.html
  -rwxrwxrwx 1 root root 1136 Mar  8  2012 csp_contact.html
  -rwxrwxrwx 1 root root 5036 Mar 24  2012 csp_comet.html
  -rwxrwxrwx 1 root root 1663 Oct 17  2012 csp_entity.html
  -rwxrwxrwx 1 root root 3834 Feb 11  2013 csp_loan.html
  drwxrwxrwx 2 root root 4096 Mar  8  2013 imgs
  -rwxrwxrwx 1 root root 7497 Mar 14  2013 index.html

  /opt/gwan64/62.210.169.132_80/#62.210.169.132/www/imgs:
  total 128
  -rwxrwxrwx 1 root root   102 Apr  7  2011 gwan16.gif
  -rwxrwxrwx 1 root root   202 Jun 29  2011 gwan_pw.gif
  -rwxrwxrwx 1 root root   284 Jul  4  2011 rss.png
  -rwxrwxrwx 1 root root   206 Jul  4  2011 gwan_rulez.gif
  -rwxrwxrwx 1 root root   470 Aug 13  2011 errors.css
  -rwxrwxrwx 1 root root   748 Mar 21  2012 ajax.js
  -rwxrwxrwx 1 root root  2807 Mar 21  2012 comet.js
  -rwxrwxrwx 1 root root 81114 Oct  1  2012 OOW_new_demo.jpg
  -rwxrwxrwx 1 root root  1308 Jan 13  2013 dl.css
  -rwxrwxrwx 1 root root 10561 Jan 17  2013 style.css
  -rwxrwxrwx 1 root root  1913 Feb 18  2013 watermark.png

  /opt/gwan64/logs:
  total 20
  -rwxrwxrwx 1 root root 14344 Jan  9 18:10 gwan_2016-01-09.log
  -rw-rw-rw- 1 root root  3674 Jan 10 14:32 gwan.log
</code></pre>
","<configuration><listener><g-wan>","2016-01-10 18:58:06"
"887535","new email send to Outlook.com never deliver but reply on Outlook.com email does","<p>I dont know where i could start. There is the probleme.</p>

<p>If someone send me an email from Outlook.com, i received the email. If i reply, he will received my email.</p>

<p>If i write a new email and send it to the same addresse, the email will never arrived.</p>

<p>I check my server reputation, and everything is fine ..</p>

<p>My spf is good, my ptr is good ...</p>

<p>Any idea what i should check ?</p>
","<email><smtp><ptr-record>","2017-12-11 14:37:21"
"748135","How can I remove port number on ssl domains","<p>I serve two ssl domains on different ports using apache2.2.3. My server version is CentOS 5.11. <strong>I can't update the server</strong>.</p>

<p>For example:</p>

<ul>
<li><code>https://one.domain.com/</code></li>
<li><code>https://two.domain.com:444/</code></li>
</ul>

<p>These works properly.</p>

<p>I want to remove port number from address in second domain like <code>https://two.domain.com/</code>.</p>

<p>How can I do it with current environment?</p>
","<ssl><proxy>","2016-01-11 07:17:20"
"887641","Want to know the flow of a network from my provided hostgator IP to my local Server","<p>Well, let me give you a little detail about my problem where I can't figure out any solution to this.I basically have a local server within my company which is all set. We use xencenter to manage all the VMs. To access them, we have given them the static unique IPs to every VM which is connected to one router. Since it's within LAN. Now the issue is, I want to access from outside but not inside that is from another network my house maybe. The server is assigned with the IP of 10.0.0.4. Can you please tell me how come I do this. I just want to connect to the server's Ip from another network. Once Ill access the server I can further connect to other VMs. I don't know if I have to play with the router.</p>

<p>Also, Ive heard that I can access from outside if I have the hosting IP (Outside IP). So what have I done is, I have a shared hosting from Hostgator. They gave me an IP, to which I mapped into a domain that is dev.xxxxxx.com from hostgator cpanel ""Advanced DNS"". Now, what do I have to do to connect my outside IP (hosting IP) to my LAN so that I can Access it from outside? Again, Do I have to port forward something or define my Hostgator IP with my LAN router where the server is connected?</p>

<p>My desire:</p>

<p>I want to access any of my VM like. user@dev.xxxxxxx.com -p 2211 where -p is the port. This is the way you connect to a VM using linux terminal. I that port 2211 have be a forward port within my router and I know this how. Please help me both the question I am quite confused with this and can't figure out the way. Just simple want to connect WAN ip to my LAN so that I can access I from outside.</p>

<p>Ill highly appreciate it if anybody will define with a good network knowledge so that I can grab the idea of this as much as I can.</p>

<p>Here is the basic diagram:</p>

<p><img src=""https://i.sstatic.net/SuEHa.jpg"" alt=""""></p>
","<linux><ip><linux-networking><router>","2017-12-12 06:46:19"
"820110","How can I setup multiple e-mail server to serve on the same wan ip?","<p>I'm trying to figure out how to setup the network to serve multiple organization e-mail server through the same WAN IP.</p>

<p>It can be done with a multi organization e-mail server, but is there a way to set it up using multiple, single organization, e-mail servers?</p>

<p>The only way I can conceive, is through port translation in the port forwarding on the router, e.g.:</p>

<p>IMAP service of the e-mail server1 (mail.domain1.com) is offered on port 25000</p>

<p>SMTP service of the e-mail server1 (mail.domain1.com) is offered on port 35000</p>

<p>IMAP service of the e-mail server2 (mail.domain2.com) is offered on port 25001</p>

<p>SMTP service of the e-mail server2 (mail.domain2.com) is offered on port 35001</p>

<p>IMAP service of the e-mail server3 (mail.domain3.com) is offered on port 25002</p>

<p>SMTP service of the e-mail server3 (mail.domain3.com) is offered on port 35002</p>

<p>etc...</p>

<p>Is there a better solution?</p>

<p>Thank you</p>

<hr>

<p>What I would like to accomplish is to find an easy solution, to serve multiple e-mail domains inside a single company, with an unique WAN IP.</p>

<p>I could opt for a multi organization e-mail server, like Zimbra, Zarafa, but I would like to continue to use Zentyal, Nethserver solutions, without delving too much into new e-mail server solutions.</p>

<p>Now I'm doing a lot of experimentation, before to accomplish this in production environments, trying to find the best, and easier, solutions based on my skills and needs.</p>

<p>Both Zentyal and Nethserver, can be adapted to serve this purpose, but they are conceived for a single organization: each e-mail address is associated to an user of the same local domain, so I need to use alias and similar. Making the whole thing a mess.</p>

<p>Usually, my customers use max 2 or 3 e-mail domains, so would be very easy to use 2 or 3 VMs, each one serving e-mail for a domain.
And, moreover, very often it happens that my customers ask me to use an additional e-mail domain, after many years using just one.
It would be easier for me, just to add a VM with another e-mail server instance, instead to change the e-mail server from ground up, so needing to migrate all the existent e-mails...</p>

<p>A solution would be just start to use a multi organization capable e-mail server since the beginning, so to be abble to add a domain when needed.</p>

<p>Just trying to understand if there is a way to continue to use the existent e-mail server, just adding a new one.
99% of times, everything resides inside an virtualization server, so would be very easy, just to add a new e-mail server.</p>

<p>Thank you for the suggestions, of course the lack of knowledge about the subject, is the culprit</p>
","<email-server><nat>","2016-12-11 18:45:55"
"962871","Postfix receives email but doesn't send [new to server management]","<p>As the title suggest, I'm struggling to get the mail server to send emails.</p>

<p>I recently purchased a VPS for hosting and I'm currently trying to set up the mail server which can now receive emails but still not send them.</p>

<p>This is my postfix main.cf:</p>

<pre><code># See /usr/share/postfix/main.cf.dist for a commented, more complete version


# Debian specific:  Specifying a file name will cause the first
# line of that file to be used as the name.  The Debian default
# is /etc/mailname.
#myorigin = /etc/mailname

smtpd_banner = $myhostname ESMTP $mail_name (Ubuntu)
biff = no

# appending .domain is the MUA's job.
append_dot_mydomain = no

# Uncomment the next line to generate ""delayed mail"" warnings
#delay_warning_time = 4h

readme_directory = no

# See http://www.postfix.org/COMPATIBILITY_README.html -- default to 2 on
# fresh installs.
compatibility_level = 2

# TLS parameters
smtpd_tls_cert_file=/etc/ssl/certs/ssl-cert-snakeoil.pem
smtpd_tls_key_file=/etc/ssl/private/ssl-cert-snakeoil.key
smtpd_use_tls = yes
smtpd_tls_session_cache_database = btree:${data_directory}/smtpd_scache
smtp_tls_session_cache_database = btree:${data_directory}/smtp_scache

# See /usr/share/doc/postfix/TLS_README.gz in the postfix-doc package for
# information on enabling SSL in the smtp client.

smtpd_relay_restrictions = permit_mynetworks permit_sasl_authenticated defer_unauth_destination
myhostname = mail.aperturedesigns.uk
alias_maps = hash:/etc/aliases, hash:/var/spool/postfix/plesk/aliases
alias_database = hash:/etc/aliases
myorigin = /etc/mailname
mydestination = $myhostname, localhost.localdomain, localhost.localdomain, , localhost
mynetworks = 127.0.0.0/8 [::ffff:127.0.0.0]/104 [::1]/128 192.168.1.0/24
mailbox_size_limit = 0
recipient_delimiter = +
inet_interfaces = all
inet_protocols = all
mydomain = aperturedesigns.uk
virtual_mailbox_domains = $virtual_mailbox_maps, hash:/var/spool/postfix/plesk/virtual_domains
virtual_alias_maps = $virtual_maps, hash:/var/spool/postfix/plesk/virtual
virtual_mailbox_maps = , hash:/var/spool/postfix/plesk/vmailbox
transport_maps = , hash:/var/spool/postfix/plesk/transport
smtpd_tls_security_level = may
smtp_tls_security_level = may
smtp_use_tls = no
smtpd_timeout = 3600s
smtpd_proxy_timeout = 3600s
disable_vrfy_command = yes
smtpd_sender_restrictions = check_sender_access hash:/var/spool/postfix/plesk/blacklists, permit_sasl_authenticated
smtpd_client_restrictions = permit_mynetworks, permit_sasl_authenticated
authorized_flush_users =
authorized_mailq_users =
smtp_send_xforward_command = yes
smtpd_authorized_xforward_hosts = 127.0.0.0/8 [::1]/128
smtpd_sasl_auth_enable = yes
smtpd_recipient_restrictions = permit_mynetworks, permit_sasl_authenticated, reject_unauth_destination
virtual_mailbox_base = /var/qmail/mailnames
virtual_uid_maps = static:30
virtual_gid_maps = static:31
smtpd_milters = , inet:127.0.0.1:12768
sender_dependent_default_transport_maps = hash:/var/spool/postfix/plesk/sdd_transport_maps
virtual_transport = plesk_virtual
plesk_virtual_destination_recipient_limit = 1
mailman_destination_recipient_limit = 1
message_size_limit = 10240000
virtual_mailbox_limit = 0
recipient_canonical_maps = tcp:127.0.0.1:12346
recipient_canonical_classes = envelope_recipient,header_recipient
smtpd_tls_ciphers = medium
smtpd_tls_mandatory_ciphers = medium
tls_medium_cipherlist = HIGH:!aNULL:!MD5
smtpd_tls_mandatory_protocols = TLSv1 TLSv1.1 TLSv1.2
smtpd_tls_protocols = TLSv1 TLSv1.1 TLSv1.2
</code></pre>

<p>Bearing in mind I am extremely new to server management and probably should learn a bit more before attempting a task like this, does anyone have an idea what I might have done wrong?</p>

<p>TIA</p>

<p>UPDATE:</p>

<p>Here is my mail log as suggested:</p>

<pre><code>Apr 13 06:28:45 localhost check-quota[11773]: Starting the check-quota filter...
Apr 13 06:28:45 localhost plesk sendmail[11772]: handlers_stderr: SKIP
Apr 13 06:28:45 localhost plesk sendmail[11772]: SKIP during call 'check-quota' handler
Apr 13 06:28:45 localhost postfix/pickup[6003]: 83981800DC: uid=0 from=&lt;root&gt;
Apr 13 06:28:45 localhost postfix/cleanup[11777]: 83981800DC: message-id=&lt;20190413062845.83981800DC@mail.aperturedesigns.uk&gt;
Apr 13 06:28:45 localhost postfix/qmgr[2097]: 83981800DC: from=&lt;root@localhost.localdomain&gt;, size=1061, nrcpt=1 (queue active)
Apr 13 06:28:45 localhost postfix/trivial-rewrite[11778]: warning: do not list domain localhost.localdomain in BOTH mydestination and virtual_alias_domains
Apr 13 06:28:45 localhost postfix/trivial-rewrite[11778]: warning: do not list domain localhost.localdomain in BOTH mydestination and virtual_alias_domains
Apr 13 06:28:45 localhost postfix/local[11781]: 83981800DC: to=&lt;root@localhost.localdomain&gt;, orig_to=&lt;root&gt;, relay=local, delay=0.03, delays=0.02/0.01/0/0, dsn=2.0.0, status=sent (delivered to mailbox)
Apr 13 06:28:45 localhost postfix/qmgr[2097]: 83981800DC: removed
Apr 13 06:31:27 localhost postfix/qmgr[2097]: A5DAC7F861: from=&lt;admin@aperturedesigns.uk&gt;, size=885, nrcpt=1 (queue active)
Apr 13 06:31:57 localhost postfix/smtp[11786]: connect to gmail-smtp-in.l.google.com[64.233.167.26]:25: Connection timed out
Apr 13 06:31:57 localhost postfix/smtp[11786]: connect to gmail-smtp-in.l.google.com[2a00:1450:400c:c00::1b]:25: No route to host
Apr 13 06:32:27 localhost postfix/smtp[11786]: connect to alt1.gmail-smtp-
in.l.google.com[74.125.205.26]:25: Connection timed out
Apr 13 06:32:57 localhost postfix/smtp[11786]: connect to alt2.gmail-smtp-in.l.google.com[74.125.68.26]:25: Connection timed out
Apr 13 06:32:57 localhost postfix/smtp[11786]: A5DAC7F861: to=&lt;mconie95@gmail.com&gt;, relay=none, delay=28083, delays=27993/0.01/90/0, dsn=4.4.1, status=deferred (connect to alt2.gmail-smtp-
in.l.google.com[74.125.68.26]:25: Connection timed out)
Apr 13 07:41:27 localhost postfix/qmgr[2097]: A5DAC7F861: from=&lt;admin@aperturedesigns.uk&gt;, size=885, nrcpt=1 (queue active)
Apr 13 07:41:57 localhost postfix/smtp[12548]: connect to gmail-smtp-in.l.google.com[173.194.76.27]:25: Connection timed out
Apr 13 07:41:57 localhost postfix/smtp[12548]: connect to gmail-smtp-in.l.google.com[2a00:1450:400c:c0a::1b]:25: No route to host
Apr 13 07:42:27 localhost postfix/smtp[12548]: connect to alt1.gmail-smtp-in.l.google.com[74.125.205.27]:25: Connection timed out
Apr 13 07:42:27 localhost postfix/smtp[12548]: connect to alt1.gmail-smtp-in.l.google.com[2a00:1450:4010:c02::1a]:25: No route to host
Apr 13 07:42:57 localhost postfix/smtp[12548]: connect to alt2.gmail-smtp-in.l.google.com[74.125.68.27]:25: Conn
ection timed out
Apr 13 07:42:57 localhost postfix/smtp[12548]: A5DAC7F861: to=&lt;mconie95@gmail.com&gt;, relay=none, delay=32283, delays=32192/0.01/90/0, dsn=4.4.1, status=deferred (connect to alt2.gmail-smtp-in.l.google.com[74.125.68.27]:25: Connection timed out)
Apr 13 08:51:27 localhost postfix/qmgr[2097]: A5DAC7F861: from=&lt;admin@aperturedesigns.uk&gt;, size=885, nrcpt=1 (queue active)
Apr 13 08:51:57 localhost postfix/smtp[12987]: connect to gmail-smtp-in.l.google.com[173.194.76.27]:25: Connection timed out
Apr 13 08:51:57 localhost postfix/smtp[12987]: connect to gmail-smtp-in.l.google.com[2a00:1450:400c:c0a::1b]:25: No route to host
Apr 13 08:52:27 localhost postfix/smtp[12987]: connect to alt1.gmail-smtp-in.l.google.com[74.125.205.26]:25: Connection timed out
Apr 13 08:52:27 localhost postfix/smtp[12987]: connect to alt1.gmail-smtp-in.l.google.com[2a00:1450:4010:c02::1b]:25: No route to host
Apr 13 08:52:57 localhost postfix/smtp[12987]: connect to alt2.gmail-smtp-in.l.google.com[74.125.68.26]:25: Connection timed out
Apr 13 08:52:57 localhost postfix/smtp[12987]: A5DAC7F861: to=&lt;mconie95@gmail.com&gt;, relay=none, delay=36483, delays=36393/0.01/90/0, dsn=4.4.1, status=deferred (connect to alt2.gmail-smtp-in.l.google.com[74.125.68.26]:25: Connection timed out)
Apr 13 10:01:27 localhost postfix/qmgr[2097]: A5DAC7F861: from=&lt;admin@aperturedesigns.uk&gt;, size=885, nrcpt=1 (queue active)
Apr 13 10:01:57 localhost postfix/smtp[13454]: connect to gmail-smtp-in.l.google.com[64.233.167.27]:25: Connection timed out
Apr 13 10:01:57 localhost postfix/smtp[13454]: connect to gmail-smtp-in.l.google.com[2a00:1450:400c:c0a::1b]:25: No route to host
Apr 13 10:02:27 localhost postfix/smtp[13454]: connect to alt1.gmail-smtp-in.l.google.com[74.125.205.27]:25: Connection timed out
Apr 13 10:02:27 localhost postfix/smtp[13454]: connect to alt1.gmail-smtp-in.l.google.com[2a00:1450:4010:c02::1b]:25: No route to host
Apr 13 10:02:57 localhost postfix/smtp[13454]: connect to alt2.gmail-smtp-in.l.google.com[74.125.68.27]:25: Connection timed out
Apr 13 10:02:57 localhost postfix/smtp[13454]: A5DAC7F861: to=&lt;mconie95@gmail.com&gt;, relay=none, delay=40683, delays=40593/0.01/90/0, dsn=4.4.1, status=deferred (connect to alt2.gmail-smtp-in.l.google.com[74.125.68.27]:25: Connection timed out)
Apr 13 11:11:27 localhost postfix/qmgr[2097]: A5DAC7F861: from=&lt;admin@aperturedesigns.uk&gt;, size=885, nrcpt=1 (queue active)
Apr 13 11:11:57 localhost postfix/smtp[13957]: connect to gmail-smtp-in.l.google.com[64.233.167.26]:25: Connection timed out
Apr 13 11:11:57 localhost postfix/smtp[13957]: connect to gmail-smtp-in.l.google.com[2a00:1450:400c:c00::1a]:25: No route to host
Apr 13 11:12:27 localhost postfix/smtp[13957]: connect to alt1.gmail-smtp-in.l.google.com[74.125.205.26]:25: Connection timed out
Apr 13 11:12:27 localhost postfix/smtp[13957]: connect to alt1.gmail-smtp-in.l.google.com[2a00:1450:4010:c02::1a]:25: No route to host
Apr 13 11:12:57 localhost postfix/smtp[13957]: connect to alt2.gmail-smtp-in.l.google.com[74.125.68.26]:25: Connection timed out
Apr 13 11:12:57 localhost postfix/smtp[13957]: A5DAC7F861: to=&lt;mconie95@gmail.com&gt;, relay=none, delay=44883, delays=44793/0.01/90/0, dsn=4.4.1, status=deferred (connect to alt2.gmail-smtp-in.l.google.com[74.125.68.26]:25: Connection timed out)
Apr 13 11:19:42 localhost dovecot: imap-login: Disconnected (no auth attempts in 0 secs): user=&lt;&gt;, rip=139.162.109.245, lip=77.68.29.49, session=&lt;hQKHmGeGGqyLom31&gt;
Apr 13 12:04:00 localhost dovecot: imap-login: Disconnected (no auth attempts in 1 secs): user=&lt;&gt;, rip=196.52.43.62, lip=77.68.29.49, TLS handshaking: Connection closed, session=&lt;kAn1NmiGs/TENCs+&gt;
Apr 13 12:21:27 localhost postfix/qmgr[2097]: A5DAC7F861: from=&lt;admin@aperturedesigns.uk&gt;, size=885, nrcpt=1 (queue active)
Apr 13 12:21:57 localhost postfix/smtp[14445]: connect to gmail-smtp-in.l.google.com[64.233.167.27]:25: Connection timed out
Apr 13 12:21:57 localhost postfix/smtp[14445]: connect to gmail-smtp-in.l.google.com[2a00:1450:400c:c00::1a]:25: No route to host
Apr 13 12:22:27 localhost postfix/smtp[14445]: connect to alt1.gmail-smtp-in.l.google.com[74.125.205.27]:25: Connection timed out
Apr 13 12:22:27 localhost postfix/smtp[14445]: connect to alt1.gmail-smtp-in.l.google.com[2a00:1450:4010:c02::1a]:25: No route to host
Apr 13 12:22:57 localhost postfix/smtp[14445]: connect to alt2.gmail-smtp-in.l.google.com[74.125.68.27]:25: Connection timed out
Apr 13 12:22:57 localhost postfix/smtp[14445]: A5DAC7F861: to=&lt;mconie95@gmail.com&gt;, relay=none, delay=49083, delays=48993/0.01/90/0, dsn=4.4.1, status=deferred (connect to alt2.gmail-smtp-in.l.google.com[74.125.68.27]:25: Connection timed out)
Apr 13 12:25:52 localhost dovecot: pop3-login: Aborted login (no auth attempts in 1 secs): user=&lt;&gt;, rip=198.108.66.208, lip=77.68.29.49, TLS, session=&lt;MiAnhWiGxGvGbELQ&gt;
Apr 13 12:56:22 localhost dovecot: imap-login: Login: user=&lt;admin@aperturedesigns.uk&gt;, method=CRAM-MD5, rip=86.22.6.100, lip=77.68.29.49, mpid=14684, TLS, session=&lt;gGI38miG/N5WFgZk&gt;
Apr 13 12:56:23 localhost postfix/smtpd[14685]: connect from keeper-us-east-1c.mxtoolbox.com[18.205.72.90]
Apr 13 12:56:25 localhost postfix/smtpd[14685]: NOQUEUE: reject: RCPT from keeper-us-east-1c.mxtoolbox.com[18.205.72.90]: 554 5.7.1 &lt;test@mxtoolboxsmtpdiag.com&gt;: Relay access denied; from=&lt;supertool@mxtoolbox.com&gt; to=&lt;test@mxtoolboxsmtpdiag.com&gt; proto=ESMTP helo=&lt;keeper-us-east-1c.mxtoolbox.com&gt;
Apr 13 12:56:26 localhost /usr/lib/plesk-9.0/psa-pc-remote[731]: Message aborted.
Apr 13 12:56:26 localhost /usr/lib/plesk-9.0/psa-pc-remote[731]: Message aborted.
Apr 13 12:56:26 localhost postfix/smtpd[14685]: disconnect from keeper-us-east-1c.mxtoolbox.com[18.205.72.90] ehlo=1 mail=1 rcpt=0/1 quit=1 commands=3/4
Apr 13 12:57:09 localhost dovecot: imap-login: Login: user=&lt;admin@aperturedesigns.uk&gt;, method=CRAM-MD5, rip=86.22.6.100, lip=77.68.29.49, mpid=14744, TLS, session=&lt;nikF9WiGH99WFgZk&gt;
Apr 13 12:57:09 localhost dovecot: imap-login: Login: user=&lt;admin@aperturedesigns.uk&gt;, method=CRAM-MD5, rip=86.22.6.100, lip=77.68.29.49, mpid=14746, TLS, session=&lt;AgwK9WiGIN9WFgZk&gt;
Apr 13 12:57:10 localhost dovecot: imap-login: Login: user=&lt;admin@aperturedesigns.uk&gt;, method=CRAM-MD5, rip=86.22.6.100, lip=77.68.29.49, mpid=14748, TLS, session=&lt;3xIY9WiGId9WFgZk&gt;
Apr 13 12:59:46 localhost postfix/anvil[14687]: statistics: max connection rate 1/60s for (smtp:18.205.72.90) at Apr 13 12:56:23
Apr 13 12:59:46 localhost postfix/anvil[14687]: statistics: max connection count 1 for (smtp:18.205.72.90) at Apr 13 12:56:23
Apr 13 12:59:46 localhost postfix/anvil[14687]: statistics: max cache size 1 at Apr 13 12:56:23
Apr 13 13:07:19 localhost postfix/smtpd[14890]: connect from cpc92102-nrte30-2-0-cust99.8-4.cable.virginm.net[86.22.6.100]
</code></pre>
","<ubuntu><email><postfix><email-server><plesk>","2019-04-12 22:58:54"
"748281","Access Domain through Browser Works but Ping to the same Domain Fails","<p>I want to know something <em>I can access a domain through browser and is working</em>. But when i tried a ping on that domain <strong>Request Time Out</strong> happens always</p>

<p>I thought i might have lost the internet connection or something wrong with ping but pinging other sites works!</p>

<p>Can something like that can be configured in server or domain control panel? so the ping to that domain doesn't works? </p>

<p>Please share your thoughts</p>
","<linux><windows-server-2008><amazon-ec2><amazon-web-services><mac-osx-server>","2016-01-11 18:12:55"
"748329","NAPTR records in DNS BIND","<p>I am trying to configure NAPTR records in DNS Bind in Windows 7.</p>

<p>I have the following zone file:</p>

<pre><code>$TTL 6h
@   IN  SOA     sip-udp01.example.org.  root.example.org.(

                2016010101
                10800
                3600
                604800
                86400 )
; the server where everything will run

@               NS              sip-udp01.example.org.
@               NS              sip-udp02.example.org.

sip-udp01           IN     A      127.0.0.1
sip-udp02           IN     A      127.0.0.1


_sip._udp.example.org SRV 5 100 5060 sip-udp01.example.org.
_sip._udp.example.org SRV 10 100 5060 sip-udp02.example.org.


example.org NAPTR 10 100 ""S"" ""SIP+D2U"" """" _sip._udp.example.org.
example.org NAPTR 20 100 ""S"" ""SIP+D2T"" """" _sip._tcp.example.org.
</code></pre>

<p>Bind runs successfully but I am not getting the answer.
host sip_udp01.example.org it says host sip_udp01.example.org not found.</p>
","<domain-name-system><bind>","2016-01-11 23:27:44"
"887911","ASP.NET Angular app running on an Nginx proxy not locating static files","<p>I have just created a basic application on Visual Studios and tried to get it running on my Ubuntu server on an Nginx proxy. Once I start it the application runs but the front-end cannot locate the static files and returns a net::ERR_ABORTED on 6 static files listed in <a href=""https://i.sstatic.net/QMOdT.png"" rel=""nofollow noreferrer"">this screen grab.</a></p>

<p>My Nginx proxy looks like this:</p>

<pre><code>location / {
            # Proxy for dotnet app
            proxy_pass http://localhost:5000;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection keep-alive;
            proxy_set_header Host $host;
            proxy_cache_bypass $http_upgrade;
    }
</code></pre>

<p>I have Certbot setup to provide SSL. I'm also not getting any errors in my Nginx error file.</p>

<p>How can I resolve the net::ERR_ABORTED on the static files?</p>
","<nginx><reverse-proxy>","2017-12-13 12:52:38"
"963246","Limit Apache's child process count in Dockerized environment","<p>We have a Dockerized environment for our clients so that each customer gets his own container to host his website. Each container usually runs on the <code>php:7</code> image so we're speaking about <code>Apache 2.4</code> using <code>mpm_prefork</code>. Each container has a memory limit of 256Mb.</p>

<p>Now should any container be compromised for some reason, using the default Apache settings, it spawns lots of child processes eating essentially entire machine's RAM. That happens, because, apparently, total child process memory usage doesn't count towards the limit.</p>

<p>I'd like to prevent Apache from spawning child processes at all. So if the container is hacked, it will reach its memory limit and get killed by Docker.</p>

<p>I tried with:</p>

<pre><code>StartServers         1
MinSpareServers      1
MaxSpareServers      1
ServerLimit          1
MaxClients           1
MaxRequestsPerChild  4000
</code></pre>

<p>But the performance was terrible, basic Drupal site with multiple CSS/script assets was loading for almost 60 seconds.</p>

<p>Is it possible to force Apache to use only one process and keep at least some performance? Sites are small and don't get a lot traffic so we don't care much, but the site should load fast for a single user at least.</p>
","<apache-2.4>","2019-04-16 08:37:29"
"748669",".htaccess Permission Denied (Apache, CentOS 7)","<p>I'm encountering an issue with <code>.htaccess</code>. I have searched through the Internet and all the solutions didn't work. Please help me! Thx!</p>

<p>In the Apache error log, it has the following line:</p>

<pre><code>(13)Permission denied: /[website root]/.htaccess pcfg_openfile: unable to check htaccess file, ensure it is readable and that '/[website root]/' is executable
</code></pre>

<p>It seems that it is just a misconfiguration of file permissions. However, changing the <code>.htaccess</code> file and the website root directory to <code>755</code> didn't work. To do an experiment, I changed both the .htaccess and website root to <code>777</code>, and it didn't work either.</p>

<p>Some people says adding <code>AllowOverride all</code> into the directory settings, I have already set this and neither adding nor deleting works.</p>

<p>I have disabled SElinux, the system environment is CentOS 7.2.1511 + Apache 2.4.6.</p>

<p>Can someone give me some advices please?</p>

<p>I'm thinking that it seems to be an error with file permissions, but it may be an error with something else. Although, I have totally no idea.</p>

<p>Thank you very much!</p>
","<linux><centos><permissions><.htaccess><centos7>","2016-01-13 01:31:06"
"748708","ntpdate -u ipaddress working but ntpd not updating the time","<p>ntpdate -u ipaddress working fine but ntpd not updating the time. Please suggest what is the reason.</p>

<pre><code>[root@commander_a ~]# ntpdate -d 10.120.8.32
13 Jan 01:14:46 ntpdate[6172]: ntpdate 4.2.6p3@1.2290 Fri Aug 28 07:43:22 UTC 2015 (1)
Looking for host 10.120.8.32 and service ntp
host found : 10.120.8.32
transmit(10.120.8.32)
receive(10.120.8.32)
transmit(10.120.8.32)
receive(10.120.8.32)
transmit(10.120.8.32)
receive(10.120.8.32)
transmit(10.120.8.32)
receive(10.120.8.32)
transmit(10.120.8.32)
server 10.120.8.32, port 123
stratum 4, precision -6, leap 00, trust 000
refid [10.120.8.32], delay 0.04166, dispersion 0.00772
transmitted 4, in filter 4
reference time:    da406681.2209f623  Wed, Jan 13 2016  1:00:33.132
originate timestamp: da4069fc.e9768166  Wed, Jan 13 2016  1:15:24.911
transmit timestamp:  da4069dc.bc3ab362  Wed, Jan 13 2016  1:14:52.735
filter delay:  0.07344  0.04166  0.04185  0.04182
         0.00000  0.00000  0.00000  0.00000
filter offset: 32.18569 32.16714 32.16395 32.17640
         0.000000 0.000000 0.000000 0.000000
delay 0.04166, dispersion 0.00772
offset 32.167146

13 Jan 01:14:54 ntpdate[6172]: step time server 10.120.8.32 offset 32.167146 sec
</code></pre>

<p>My ntp.conf :</p>

<pre><code>driftfile /etc/ntp/drift

statistics loopstats peerstats clockstats
filegen loopstats file loopstats type day enable
filegen peerstats file peerstats type day enable
filegen clockstats file clockstats type day enable

server 10.120.8.32

restrict -4 default kod notrap nomodify nopeer noquery
restrict -6 default kod notrap nomodify nopeer noquery

restrict 127.0.0.1
restrict ::1
</code></pre>

<p>The output of the ntpd is as follows</p>

<pre><code>daemon.notice: Jan 13 07:01:56 ntpd  ntpd 4.2.6p3@1.2290 Fri Aug 28 07:43:22 UTC 2015 (1)
daemon.notice: Jan 13 07:01:56 ntpd  proto: precision = 0.120 usec
daemon.debug: Jan 13 07:01:56 ntpd  ntp_io: estimated max descriptors: 1024, initial socket boundary: 16
daemon.info: Jan 13 07:01:56 ntpd  Listen and drop on 0 v4wildcard 0.0.0.0 UDP 123
daemon.info: Jan 13 07:01:56 ntpd  Listen and drop on 1 v6wildcard :: UDP 123
daemon.info: Jan 13 07:01:56 ntpd  Listen normally on 2 lo 127.0.0.1 UDP 123
daemon.info: Jan 13 07:01:56 ntpd  Listen normally on 3 eth0 10.120.9.199 UDP 123
daemon.info: Jan 13 07:01:56 ntpd  Listen normally on 4 eth1 192.168.31.11 UDP 123
daemon.info: Jan 13 07:01:56 ntpd  Listen normally on 5 eth1:0 192.168.31.1 UDP 123
daemon.info: Jan 13 07:01:56 ntpd  Listen normally on 6 eth0 fe80::202:6bff:fe10:742d UDP 123
daemon.info: Jan 13 07:01:56 ntpd  Listen normally on 7 eth1 fe80::202:6bff:fe10:742e UDP 123
daemon.info: Jan 13 07:01:56 ntpd  Listen normally on 8 lo ::1 UDP 123
daemon.info: Jan 13 07:01:56 ntpd  peers refreshed
daemon.info: Jan 13 07:01:56 ntpd  Listening on routing socket on fd #25 for interface updates
</code></pre>

<p>Here we are running a custom Ubuntu OS where i cannot run ntpq -c pe and ntpq -c rv. </p>

<p>Thanks for quick reply.
When I execute ntpdate -d with the IPAdd of the NTP server which is located in my company network I got the following output.</p>

<pre><code>[root@commander_a ~]# ntpdate -d 10.120.8.32
14 Jan 06:09:29 ntpdate[5566]: ntpdate 4.2.6p3@1.2290 Fri Aug 28 07:43:22 UTC 2015 (1)
Looking for host 10.120.8.32 and service ntp
host found : 10.120.8.32
transmit(10.120.8.32)
receive(10.120.8.32)
transmit(10.120.8.32)
receive(10.120.8.32)
transmit(10.120.8.32)
receive(10.120.8.32)
transmit(10.120.8.32)
receive(10.120.8.32)
transmit(10.120.8.32)
server 10.120.8.32, port 123
stratum 4, precision -6, leap 00, trust 000
refid [10.120.8.32], delay 0.04179, dispersion 0.00783
transmitted 4, in filter 4
reference time:    da41fe81.d7c98f2a  Thu, Jan 14 2016  6:01:21.842
originate timestamp: da420077.52aad6d8  Thu, Jan 14 2016  6:09:43.322
transmit timestamp:  da42006f.a18e4ab9  Thu, Jan 14 2016  6:09:35.631
filter delay:  0.04184  0.04179  0.04193  0.04195
         0.00000  0.00000  0.00000  0.00000
filter offset: 7.685453 7.697911 7.694673 7.691484
         0.000000 0.000000 0.000000 0.000000
delay 0.04179, dispersion 0.00783
offset 7.697911

14 Jan 06:09:37 ntpdate[5566]: step time server 10.120.8.32 offset 7.697911 sec
</code></pre>

<p>And when I execute ntpdate -d with the IPAdd of the NTP server which is located in outside company network I got the following out.</p>

<pre><code>[root@commander_a ~]# ntpdate -d 120.88.46.10
14 Jan 06:09:51 ntpdate[5578]: ntpdate 4.2.6p3@1.2290 Fri Aug 28 07:43:22 UTC 2015 (1)
Looking for host 120.88.46.10 and service ntp
host found : 120.88.46.10
transmit(120.88.46.10)
transmit(120.88.46.10)
transmit(120.88.46.10)
transmit(120.88.46.10)
transmit(120.88.46.10)
120.88.46.10: Server dropped: no data
server 120.88.46.10, port 123
stratum 0, precision 0, leap 00, trust 000
refid [120.88.46.10], delay 0.00000, dispersion 64.00000
transmitted 4, in filter 4
reference time:    00000000.00000000  Sun, Dec 31 1899 19:00:00.000
originate timestamp: 00000000.00000000  Sun, Dec 31 1899 19:00:00.000
transmit timestamp:  da420085.ea58e3ed  Thu, Jan 14 2016  6:09:57.915
filter delay:  0.00000  0.00000  0.00000  0.00000
         0.00000  0.00000  0.00000  0.00000
filter offset: 0.000000 0.000000 0.000000 0.000000
         0.000000 0.000000 0.000000 0.000000
delay 0.00000, dispersion 64.00000
offset 0.000000

14 Jan 06:09:59 ntpdate[5578]: no server suitable for synchronization found
</code></pre>

<p>Do you know what is the reason for this. Because you told that ntpdate -d option will bypass the firewall. In that case ntpdate should work in both the cases at least.</p>
","<ntpd><ntpdate>","2016-01-13 06:59:41"
"820621","Is it possible to set same port work with http and https?","<p>I've nodejs application which port number is 3001 and this app working with http but not work with https. On apache server, set the revers proxy for 80 or 443 to 5001 and set revers proxy for 3001 also.</p>

<pre><code>http://&lt;domain&gt;:3001/socket.io/socket.js   ---&gt; Work

https://&lt;domain&gt;:3001/socket.io/socket.js   ---&gt; Not Work(Secure Connection Failed or This site can’t be reached)
</code></pre>

<p>Here I need to know same port(3001) is work with http and https ?</p>

<p>Any solution for that?</p>
","<linux><networking><apache-2.4><reverse-proxy><node.js>","2016-12-14 09:34:36"
"748775","auto delete disabled AD accounts after xx day","<p>I'm looking for an app that can run to auto delete disabled AD user accounts in a specific OU after say 30 days </p>

<p>Any advice will be highly appreciated</p>
","<active-directory><domain><scheduled-task>","2016-01-13 11:17:36"
"963456","Removing VLAN 1 traffic from a port","<p>If I look at <a href=""https://support.citrix.com/article/CTX123489"" rel=""nofollow noreferrer"">this</a> page for Xen Server VLANs, it says</p>
<blockquote>
<p>&quot;Configuring the XenServer management interface on a VLAN network is
not supported&quot;</p>
</blockquote>
<p>So then the Server must be on 'VLAN1' yes, my problem is that VLAN 1 traffic is enabled untagged on all switch ports (on my tp-link switches at least), so it seems using VLAN 1 as a management port means that all this traffic is on the wire for all ports. This seems kinda derp regarding the point of VLANs</p>
<p>edit: who are the trolls who downvote questions without being able to answer them?</p>
","<vlan>","2019-04-17 11:53:57"
"748788","How to send lan traffic to bypass the vpn for android","<p>How to send lan traffic to bypass the vpn, for that network 192.168.3.0/24 directly appealed to the wifi router, and do not through the VPN</p>
","<vpn><router><wifi><local-area-network>","2016-01-13 12:18:34"
"888457","Restore /etc/shadow with the contents of /etc/shadow.bak","<p>I need to restore /etc/shadow with the contents of /etc/shadow.bak. I don't have a sudo password but was told that maybe ""Non-default shadow group permissions"" would help?</p>

<p>I run</p>

<pre><code>sudo -ll
</code></pre>

<p>I get the following</p>

<pre><code>james@machine:~$ sudo -ll
Matching Defaults entries for elf on 8ee2351512ca:
    env_reset, mail_badpass, secure_path=/usr/local/sbin\:/usr/local/bin\:/usr/sbin\:/usr/bin\:/sbin\:/bin\:/snap/bin
User james may run the following commands on machine:
Sudoers entry:
    RunAsUsers: james
    RunAsGroups: shadow
    Options: !authenticate
    Commands:
        /usr/bin/find
</code></pre>

<p>Permissions on shadow are as follows</p>

<pre><code>james@machine:~$ ls -laah /etc/shadow
-rw-rw---- 1 root shadow 0 Dec 15 20:00 /etc/shadow
</code></pre>
","<linux>","2017-12-16 17:19:54"
"748895","https only works for https://localhost, not from outside","<p>It worked yesterday.</p>

<p>This morning I added the <code>asp.net</code> users/roles database to an aws rds instance. Works fine on my computer, but when I deploy it to an aws ec2 server(windows server 2012, iis 8.5) the <code>https</code> doesn't work. I don;t think i did anything else.</p>

<p>Bindings are <code>*:443</code> and <code>*:80</code>.</p>

<pre><code>HTTP/1.1 502 Fiddler - Connection Failed

Cache-Control: no-cache, must-revalidate
Timestamp: 19.05.31.337

[Fiddler] The connection to '--' failed. &lt;br /&gt;Error: TimedOut (0x274c).
</code></pre>
","<windows><iis><iis-8.5>","2016-01-13 19:35:01"
"963897","Linux - stuck on boot","<p>restarted machine but I'm stuck now on boot with errors in attach.</p>

<p>The os is an OL 7.5.</p>

<p>I tried to reinstall/reconfigure Grub with no success. Does LVM can be the cause ?</p>

<p>Please help !</p>

<p>Thanks.</p>

<p><img src=""https://i.sstatic.net/kYiyM.jpg"" alt=""error image""></p>

<p><img src=""https://i.sstatic.net/c2BEu.jpg"" alt=""error image 2""></p>
","<linux>","2019-04-20 12:28:55"
"749291","Cannot mount XFS partition","<p>I have a XFS partition in one of my HDD partitions.  </p>

<p>Here's my disks:</p>

<pre><code>aksesfarma@elementary-PC:~$ sudo blkid -o list

device fs_type  label  mount point  UUID
-------------------------------------------

/dev/sda1  ext4  /                               xxxx
/dev/sda3  ntfs  System Reserved (not mounted)   xxxx
/dev/sda4  ntfs  (not mounted)                   xxxx
/dev/sda5  ext4  /home                           xxxx
/dev/sda6  swap  &lt;swap&gt;                          xxxx

/dev/sdb1  xfs   (not mounted)                   xxxx
/dev/sdb5  xfs   (not mounted)                   xxxx
/dev/sdb6  xfs   (not mounted)                   xxxx
</code></pre>

<p>Note here that <code>/dev/sda</code> is my main HDD (Linux/Windows dual boot) and works fine. The problem is in <code>/dev/sdb</code> or my second drive.</p>

<p>Here's after I mount <code>/dev/sdb1</code> to <code>/dev/sdb6</code> using command:</p>

<pre><code>aksesfarma@elementary-PC:~$ sudo mount -t xfs /dev/sdb1 /home/aksesfarma/drive1
aksesfarma@elementary-PC:~$ sudo mount -t xfs /dev/sdb5 /home/aksesfarma/drive2
aksesfarma@elementary-PC:~$ sudo mount -t xfs /dev/sdb6 /home/aksesfarma/drive3

aksesfarma@elementary-PC:~$ sudo blkid -o list
/dev/sdb1  xfs   /home/aksesfarma/drive1        xxxx
/dev/sdb5  xfs   /home/aksesfarma/drive2        xxxx
/dev/sdb6  xfs   (in use)                       xxxx
</code></pre>

<p>When I try the last command to mount <code>/dev/sdb6</code>, the cursor cannot receive input anymore. It just blinked. Note: the repeated <code>^C</code> is me, trying to cancel the command using Ctrl-C. No response and no output whatsoever. </p>

<p><a href=""https://i.sstatic.net/bM0YJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bM0YJ.png"" alt=""enter image description here""></a></p>

<p>Then I read an article about XFS-partition: <a href=""http://docs.cray.com/books/S-2377-22/html-S-2377-22/z1029470303.html"" rel=""nofollow noreferrer"">Here's the article</a>
Tried to enter this command <code>xfs_repair -n /dev/sdb6</code> or <code>xfs_check /dev/sdb6</code></p>

<p>Still, Nothing happen, the partition still unmounted and my cursor still blinking</p>
","<ubuntu><xfs>","2016-01-15 06:19:28"
"749537","How to revert SSH changes back to default after changing port from default?","<p>Got myself into a sticky situation. I've got a bare metal server in a DC which is reasonably far in distance from me.</p>

<p>In any case, a few day ago I changed the SSH port from default to 6298 and now I need to change it back. Upon attempting to do so, it appears that I am no longer able to connect via SSH on any port number. Is there a way to reset SSH back to default? I don't really want to risk performing a system reboot as it could result in me not being able to access the server.</p>
","<ssh><port>","2016-01-16 00:04:48"
"749675","Error when access folder cgi-bin","<p>I created an folder with name ""cgi-bin"" on /var/www"" When I tried access this folder I receive error 403 Forbidden - You don't have permission to access /cgi-bin/ on this server.</p>

<p>I try this on Virtual Machine on environment Vagrant in machine Ubuntu with apache install.</p>

<p>How can I fix this?</p>
","<apache-2.2><apache-2.4><cgi-bin>","2016-01-17 01:08:25"
"889402","Windows Update Error Code 80072EFE","<p>I have installed Windows Server 2012 R2 RTM. Now, in order to install SharePoint, seems like I will have to update the server upto April 2014 update. But no matter, whatever I do, I see the following error: 
<a href=""https://i.sstatic.net/xTuxU.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xTuxU.jpg"" alt=""enter image description here""></a>
Anyone who can help me on this? Need an urgent fix.</p>
","<windows-server-2012-r2><windows-update>","2017-12-22 20:56:53"
"964358","Email not getting redirected to as per MX record to business email server","<p>I've purchased business email so all emails to <code>contact@anikadigital.in</code> should go to <code>privateemail.com</code> as per the MX record of <code>anikadigital.in</code>.</p>

<p>Here is the MX lookup of anikadigital.in:</p>

<pre><code>DNS server handling your query: localhost
 DNS server's address:  127.0.0.1#53

 Non-authoritative answer:
 anikadigital.in    mail exchanger = 10 mx1.privateemail.com.
 anikadigital.in    mail exchanger = 10 mx2.privateemail.com.
</code></pre>

<p>but still I'm getting this error in the <code>syslog</code> file:</p>

<pre><code> A091021432: to=&lt;contact@anikadigital.in&gt;, relay=none, delay=0.02, delays=0.01/0/0/0.01, dsn=5.1.1, status=bounced (User unknown in virtual alias table)
</code></pre>

<p>How can that be?</p>

<p>Here is the postfix configuration. I've mangled other domains however I've retained the anikadigital.in domain  as it is:</p>

<pre><code>alias_database = hash:/etc/aliases
alias_maps = hash:/etc/aliases
always_bcc = contact@anikadigital.in
append_dot_mydomain = no
biff = no
inet_interfaces = all
inet_protocols = ipv4
mailbox_command = procmail -a ""$EXTENSION""
mailbox_size_limit = 0
mydestination = $myhostname, localhost, localhost.localdomain, localhost
myhostname = host.digittions.in
mynetworks = 127.0.0.0/8 [::ffff:127.0.0.0]/104 [::1]/128
myorigin = /etc/mailname
readme_directory = no
recipient_delimiter = +
relayhost =
smtp_tls_session_cache_database = btree:${data_directory}/smtp_scache
smtpd_banner = $myhostname ESMTP $mail_name (Ubuntu)
smtpd_relay_restrictions = permit_mynetworks permit_sasl_authenticated defer_unauth_destination
smtpd_tls_cert_file = /etc/ssl/certs/ssl-cert-snakeoil.pem
smtpd_tls_key_file = /etc/ssl/private/ssl-cert-snakeoil.key
smtpd_tls_session_cache_database = btree:${data_directory}/smtpd_scache
smtpd_use_tls = yes
virtual_alias_domains = digittions.in indiacc.com cffd.com def.com duplicateticket.com karera.digitaltransactions.in mail.digittions.in idoff.com accrec.in anikadigital.in passport.anikadigital.in pan.anikadigital.in tpst.in
virtual_alias_maps = hash:/etc/postfix/virtual
</code></pre>
","<postfix>","2019-04-24 07:48:16"
"964396","Cluster a custom application","<p>I have a custom application which is written by java. I want to know is it possible to cluster this application by pacemaker. For example copy this application in two different nodes and setup a cluster by pacemaker. In one time only one application work and when primary node goes down my application start to work on second node in simple word i need a way to cluster my simple application like clustering nginx server with packemaker,</p>

<p>And also is there any solution to cluster two OS. If first OS goes down second OS start to serve request ?</p>

<p>Thanks in advance</p>

<hr>

<p>To clarify my scenario. 
I have a java application which process some messages. It is not like a service and acts like a local application. I want to be sure one instance of my application is always running in two or more nodes of my cluster and when one instance of my application is running in one node of my cluster other instances not allowed to work. I Know how pacemaker work and use float ip as a resource. I want to know is there any solution to add my application as resource or pacemaker monitor my application and when failed switch to other node.</p>

<hr>

<p>Updated:
for more information
I want to deploy application on physical servers and I cannot use virtualization and VM. Furthermore methods like Vmware FT has some limitations like max no of VCPU and also RAM.
I think it should be much easier if I can use pacemaker as a cluster resource manager but I dont know how to introduce my application as a resource and how to route all traffic to my primary server and when server or application goes down traffic and requests route to other node.</p>

<p>Unfortunately my application is not like a service. normally my application gathers information  from different servers and sends them to another servers thus methods like loadbalancer is not suitable for my scenario.</p>

<p>I need a floating IP for my cluster and when serverA goes down All traffic switch to ServerB( Input and Output) also cluster start Stopped application on serverB. And when ServerA backs online all requests (Input or Output ) route to ServerA and Stop App on ServerB.</p>
","<cluster><pacemaker><application>","2019-04-24 12:03:24"
"749806","OpenVPN cannot route traffic","<p>Scenario</p>

<p>OpenVPN server running on Gentoo</p>

<p>eno1: (server public ip) has static ip... NO dhcp</p>

<p>eno2: vlan (down for now)</p>

<p>tun0: openvpn tunnel interface</p>

<p>server.conf</p>

<pre><code>local &lt;external ip&gt;
port 443
proto tcp
dev tun

ca              ca.crt
cert            cert.crt
key             key.key
dh              dh.pem
tls-auth        hmac.key 0

server          10.77.198.0 255.255.255.0
ifconfig-pool-persist ipp.txt

push            ""redirect-gateway def1""
push            ""dhcp-option DNS 8.8.8.8""

keepalive 10 120
comp-lzo
max-clients 10
user nobody
group nobody

persist-key
persist-tun

status          /var/log/openvpn-status.log
log             /var/log/openvpn.log
log-append      /var/log/openvpn.log
verb 4
</code></pre>

<p>client.conf</p>

<pre><code>client
remote &lt;server ext ip&gt;
port 443
comp-lzo
dev tun
proto tcp

persist-key
persist-tun
verb 4

&lt;ca&gt;
removed
&lt;/ca&gt;

&lt;cert&gt;
removed
&lt;/cert&gt;

&lt;key&gt;
removed
&lt;/key&gt;

key-direction 1
&lt;tls-auth&gt;
removed
&lt;/tls-auth&gt;
</code></pre>

<p>Connecting from Win7 x64 machine</p>

<p>...works fine</p>

<p>i am being assigned an ip from 10.77.198.0 pool</p>

<p>i can ping the server's external ip from the client</p>

<p>ISSUE:</p>

<p>traffic is not routed through the vpn</p>

<pre><code>route -n

Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         &lt;ext ip&gt;.254    0.0.0.0         UG    2      0        0 eno1
10.77.198.0     10.77.198.2     255.255.255.0   UG    0      0        0 tun0
10.77.198.2     0.0.0.0         255.255.255.255 UH    0      0        0 tun0
&lt;ext ip&gt;.0      0.0.0.0         255.255.255.0   U     0      0        0 eno1
127.0.0.0       0.0.0.0         255.0.0.0       U     0      0        0 lo
169.254.0.0     0.0.0.0         255.255.0.0     U     0      0        0 eno2
</code></pre>

<p>Am i missing anything, apart from the iptables rules?</p>

<p>If not, please provide a COMPLETE set of iptables rules to properly route traffic between tun0 and eno1. Keep in mind eno1 has a static ip.</p>

<p>P.S.</p>

<p>Gateway being pushed to the client is from 10.77.198.0 pool also</p>
","<iptables><openvpn>","2016-01-17 22:23:07"
"820800","Unable to route to other hosts in GCE network via OpenVPN","<p>OpenVPN has been installed on a Google Cloud Instance using <a href=""https://github.com/Angristan/OpenVPN-install"" rel=""nofollow noreferrer"">this script</a>. The Google Cloud Instance is Ubuntu 14.04 LTS, has been tagged with ""vpn"", with IP Forwarding enabled. Additionally, 3 firewalls have been setup:</p>

<ul>
<li>Allow from any source ICMP, apply to all targets</li>
<li>Allow from any source TCP port 22, apply to all targets</li>
<li>Allow from any source UDP port 1194, apply to ""vpn"" tagged targets</li>
</ul>

<p>Using the client.ovpn file generated by the script it is possible to connect to the Instance with OpenVPN. It is possible to ping the internal IP address of the Instance (i.e 10.20.0.2), however it is not possible to ping other hosts in the same virtual subnet (i.e 10.20.0.3).</p>

<p>With logging enabled, the following is an output of the syslog when a connection is made to the Instance and other hosts on the subnet are pinged remotely.</p>

<pre><code>root@vpn:/home/user# tail /var/log/syslog 
Dec 15 03:54:26 vpn ovpn-server[2633]: client/118.209.255.26:64219 MULTI_sva: pool returned IPv4=10.8.0.2, IPv6=(Not enabled)
Dec 15 03:54:26 vpn ovpn-server[2633]: client/118.209.255.26:64219 MULTI: Learn: 10.8.0.2 -&gt; client/118.209.255.26:64219
Dec 15 03:54:26 vpn ovpn-server[2633]: client/118.209.255.26:64219 MULTI: primary virtual IP for client/118.209.255.26:64219: 10.8.0.2
Dec 15 03:54:29 vpn kernel: [  630.713536] iptables denied: IN=eth0 OUT= MAC=42:01:0a:14:00:02:42:01:00:00:00:00:08:00 SRC=118.209.255.26 DST=10.20.0.2 LEN=140 TOS=0x00 PREC=0x00 TTL=50 ID=19209 PROTO=UDP SPT=64219 DPT=1194 LEN=120 
Dec 15 03:54:29 vpn ovpn-server[2633]: client/118.209.255.26:64219 PUSH: Received control message: 'PUSH_REQUEST'
Dec 15 03:54:29 vpn ovpn-server[2633]: client/118.209.255.26:64219 send_push_reply(): safe_cap=940
Dec 15 03:54:29 vpn ovpn-server[2633]: client/118.209.255.26:64219 SENT CONTROL [client]: 'PUSH_REPLY,dhcp-option DNS 8.8.8.8,dhcp-option DNS 8.8.4.4,redirect-gateway def1 bypass-dhcp,route-gateway 10.8.0.1,topology subnet,ping 10,ping-restart 120,ifconfig 10.8.0.2 255.255.255.0' (status=1)
Dec 15 03:54:40 vpn kernel: [  641.643199] iptables denied: IN=eth0 OUT= MAC=42:01:0a:14:00:02:42:01:00:00:00:00:08:00 SRC=118.209.255.26 DST=10.20.0.2 LEN=173 TOS=0x00 PREC=0x00 TTL=50 ID=25886 PROTO=UDP SPT=64219 DPT=1194 LEN=153 
Dec 15 03:54:52 vpn kernel: [  653.789393] iptables denied: IN=eth0 OUT= MAC=42:01:0a:14:00:02:42:01:00:00:00:00:08:00 SRC=74.125.41.32 DST=10.20.0.2 LEN=104 TOS=0x00 PREC=0x00 TTL=53 ID=23017 PROTO=TCP SPT=35702 DPT=22 WINDOW=29016 RES=0x00 ACK PSH URGP=0 
Dec 15 03:55:05 vpn kernel: [  667.589193] iptables denied: IN=eth0 OUT= MAC=42:01:0a:14:00:02:42:01:00:00:00:00:08:00 SRC=74.125.41.32 DST=10.20.0.2 LEN=104 TOS=0x00 PREC=0x00 TTL=53 ID=29539 PROTO=TCP SPT=35702 DPT=22 WINDOW=29358 RES=0x00 ACK PSH URGP=0 
</code></pre>

<p>Additionally:</p>

<ul>
<li><code>net.ipv4.ip_forward = 1</code> has been set in <code>/etc/sysctl.conf</code></li>
<li>iptables has been configured using <a href=""https://arashmilani.com/post?id=53"" rel=""nofollow noreferrer"">these rules</a></li>
<li>Both TCP and UDP OpenVPN protocol configurations do not work</li>
<li>It is possible to ping other Instances from the VPN server when connected via SSH, just not from a remote host connected via VPN</li>
</ul>
","<openvpn><linux-networking><google-cloud-platform><google-compute-engine>","2016-12-15 04:00:08"
"964840","CDN solution for VOD","<p>We're trying to improve video delivery for a VOD solution. Right now we use over 100Tbytes per month running on several dedicated servers. We have a total of 4 dedicated servers and over the last six months we've needed to add a new server every 2-3 months so I want to try a CDN solution in hopes of not adding new dedicated servers. </p>

<p>Is there anyway to know how much bandwidth will be shifted off the origin server to the CDN? I'd like to see a situation where we have one origin server with 30TBytes bandwidth that is sending files to the CDN which is taking care of 80TByes/month due to caching</p>
","<cdn><video-streaming>","2019-04-27 10:06:54"
"964887","ip.route iptable does not nat","<pre><code>|--------------|          |--------------|          |--------------|
|  computer A  |      eth0|  computer B  |          |  computer C  |internet
|  (10.5.0.2)  |----------|  (10.5.0.1)  |          |  (x.x.x.x)   |--------
|              |          |              |          |         ^    |
|              |          |     NAT |    |          |    NAT  |    |
|              |          |         ˇ    | tun0     |              |
|              |          |  (10.8.0.14) |----------|  (10.8.0.1)  |
|--------------|          |--------------|          |--------------|
</code></pre>

<p>Here the schematic of the network I've build.<br>
My problem is when I try to configure the NAT in the B computer<br>
    ''bash
    root@computerB#cat /proc/sys/net/ipv4/ip_forward
    1</p>

<pre><code>root@computerB# iptables-save 
*nat
:PREROUTING ACCEPT [13:1108]
:INPUT ACCEPT [10:600]
:OUTPUT ACCEPT [6708:457650]
:POSTROUTING ACCEPT [5782:389727]
-A POSTROUTING -o wlan0 -j MASQUERADE
COMMIT
# Completed on Sat Apr 27 23:56:29 2019
# Generated by iptables-save v1.6.0 on Sat Apr 27 23:56:29 2019
*filter
:INPUT ACCEPT [1235765:1640284761]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [759264:248481682]
-A FORWARD -i eth0 -j ACCEPT
COMMIT
# Completed on Sat Apr 27 23:56:29 2019
</code></pre>

<ul>
<li>when I whant to ping 10.5.0.1 from computer A it's work  </li>
<li>when I whant to ping 10.8.0.14 from computer A it's work  </li>
<li><p>when I whant to ping 10.8.0.1 from computer A it's not working<br>
so i take my friend wireshark and start to listen on tun0 I can see packets like that</p>

<p>No. time        mac  source   destination  protocol length  info
285 106.310258  N/A 10.5.0.2    10.8.0.1    ICMP      84    Echo (ping) request  id=0x8114, seq=0/0, ttl=63 (no response found!)</p></li>
</ul>

<p>I don't understand why I get ip 10.5.0.2 on interface tun0</p>
","<nat><debian-stretch>","2019-04-27 22:33:58"
"750450","port 80 is open by iptables but Connection timed out","<p>I have nginx on my server, I run the script below to add rules to iptables:</p>

<pre><code>#!/bin/bash
ip=/sbin/iptables
services='22 25 53 80 443 5432 8000 8080'
$ip -F;
$ip -t nat -F;

echo -e ""Opening port 22 for ssh"";
$ip -A INPUT -p tcp --dport 22 -j ACCEPT;
$ip -A OUTPUT -p tcp --sport 22 -m state --state NEW,RELATED,ESTABLISHED -j ACCEPT;

$ip -P INPUT DROP;
$ip -P OUTPUT ACCEPT;
$ip -P FORWARD DROP;

echo -e ""Accept lo"";
$ip -A INPUT -i lo -j ACCEPT;
$ip -A OUTPUT -o lo -j ACCEPT;

$ip -A INPUT -i docker0 -j ACCEPT;
$ip -A OUTPUT -o docker0 -j ACCEPT;

# ethernet
#echo -e ""Accept eth0"";
#$ip -A INPUT -i eth0 -j ACCEPT;
#$ip -A OUTPUT -o eth0 -j ACCEPT;

for service in $services; do
echo -e ""Open port $service"";
$ip -A INPUT -p tcp -m tcp --sport $service -j ACCEPT;
done

$ip -A INPUT -p udp -m udp --sport 53 -j ACCEPT;
$ip -A OUTPUT -p udp -m udp --dport 53 -j ACCEPT;
$ip -A INPUT -p udp -m udp --dport 51914 -j ACCEPT;
</code></pre>

<p>I open necessary ports, but when i try to access my website using wget i get the following error:</p>

<pre><code>--2016-01-20 14:28:33--  &lt;ip&gt;
Connecting to &lt;ip&gt;|:80... failed: Connection     timed out.
Retrying.
</code></pre>

<p>well, what is the problem of my script for firewall?</p>

<p>edit:
the output of <code>iptables -L INPUT -vn</code> is</p>

<pre><code>Chain INPUT (policy DROP 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source                   destination         
  218 18351 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0           tcp dpt:22 
   29  4456 ACCEPT     all  --  lo     *       0.0.0.0/0            0.0.0.0/0           
    0     0 ACCEPT     all  --  docker0 *       0.0.0.0/0            0.0.0.0/0                    
    0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0           tcp spt:22 
    0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0           tcp spt:25 
    0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0           tcp spt:53 
    0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0           tcp spt:80 
    0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0           tcp spt:443 
    0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0           tcp spt:5432 
    0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0           tcp spt:8000 
    0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0           tcp spt:8080 
    0     0 ACCEPT     udp  --  *      *       0.0.0.0/0            0.0.0.0/0           udp spt:53 
    0     0 ACCEPT     udp  --  *      *       0.0.0.0/0            0.0.0.0/0           udp dpt:51914 
</code></pre>
","<nginx><iptables><firewall>","2016-01-20 11:18:24"
"964998","Why don't the NS records on my registrar's name server reflect the values of the alternate name servers I've specified?","<p>Example: example.com is registered at Namecheap but I want to use Cloudflare DNS.  So on my Namecheap DNS I set the domain's nameservers (I thought this changed the NS records) to the Cloudflare name servers - deb.ns.cloudflare.com and sri.ns.cloudflare.com).  Later, then I do an NSLOOKUP directly again namecheap's name servers (dns1.registrar-servers.com) and query the NS records, I expected to see deb.ns.cloudflare.com and sri.ns.cloudflare.com.  But I do not.  I see namecheap's nameservers showing.</p>

<pre><code>nslookup - dns1.registrar-servers.com
&gt; set type=NS
&gt; example.com
Server: dns1.namecheaphosting.com
Address: 216.87.155.33

example.com      nameserver = dns1.registrar-servers.com
example.com      nameserver = dns2.registrar-servers.com
</code></pre>
","<domain-name-system><dns-zone><ns-record>","2019-04-28 21:34:20"
"750622","CentOS 6 ClamAV scan unexpected results","<p>I am trying to scan whole server with below command using clamav. However I already installed Malware detector. The server scans for upto a minute or two and found some unexpected scanning results where I am unable to find any solution while googling.</p>

<blockquote>
  <p>clamscan -r /</p>
</blockquote>

<p>Below is the result</p>

<pre><code>/proc/43/task/43/mem: Excluded (/proc)
/proc/43/task/43/cwd: Symbolic link
/proc/43/task/43/root: Symbolic link
/proc/43/task/43/exe: Symbolic link
/proc/43/task/43/mounts: Excluded (/proc)
/proc/43/task/43/mountinfo: Excluded (/proc)
/proc/43/task/43/clear_refs: Excluded (/proc)
/proc/43/task/43/smaps: Excluded (/proc)
/proc/43/task/43/pagemap: Excluded (/proc)
/proc/43/task/43/attr/current: Excluded (/proc)
/proc/43/task/43/attr/prev: Excluded (/proc)
/proc/43/task/43/attr/exec: Excluded (/proc)
/proc/43/task/43/attr/fscreate: Excluded (/proc)
/proc/43/task/43/attr/keycreate: Excluded (/proc)
/proc/43/task/43/attr/sockcreate: Excluded (/proc)
/proc/43/task/43/wchan: Excluded (/proc)
/proc/43/task/43/stack: Excluded (/proc)
/proc/43/task/43/schedstat: Excluded (/proc)
/proc/43/task/43/cpuset: Excluded (/proc)
/proc/43/task/43/cgroup: Excluded (/proc)
/proc/43/task/43/oom_score: Excluded (/proc)
/proc/43/task/43/oom_adj: Excluded (/proc)
/proc/43/task/43/oom_score_adj: Excluded (/proc)
/proc/43/task/43/loginuid: Excluded (/proc)
/proc/43/task/43/sessionid: Excluded (/proc)
/proc/43/task/43/io: Excluded (/proc)
/proc/43/ns/net: Symbolic link
/proc/43/ns/uts: Symbolic link
/proc/43/ns/ipc: Symbolic link
/proc/43/ns/mnt: Symbolic link
/proc/43/ns/pid: Symbolic link
/proc/43/net/ip6_tables_targets: Excluded (/proc)
/proc/43/net/ip6_tables_matches: Excluded (/proc)
/proc/43/net/ip6_tables_names: Excluded (/proc)
/proc/43/net/nf_conntrack: Excluded (/proc)
/proc/43/net/nf_conntrack_expect: Excluded (/proc)
/proc/43/net/ip_tables_targets: Excluded (/proc)
/proc/43/net/ip_tables_matches: Excluded (/proc)
/proc/43/net/ip_tables_names: Excluded (/proc)
/proc/43/net/packet: Excluded (/proc)
/proc/43/net/unix: Excluded (/proc)
/proc/43/net/snmp: Excluded (/proc)
/proc/43/net/netstat: Excluded (/proc)
/proc/43/net/sockstat: Excluded (/proc)
/proc/43/net/icmp: Excluded (/proc)
/proc/43/net/udp: Excluded (/proc)
/proc/43/net/tcp: Excluded (/proc)
/proc/43/net/raw: Excluded (/proc)
/proc/43/net/ip_mr_cache: Excluded (/proc)
/proc/43/net/ip_mr_vif: Excluded (/proc)
/proc/43/net/udplite: Excluded (/proc)
/proc/43/net/mcfilter: Excluded (/proc)
/proc/43/net/igmp: Excluded (/proc)
/proc/43/net/xfrm_stat: Excluded (/proc)
/proc/43/net/rt_acct: Excluded (/proc)
/proc/43/net/rt_cache: Excluded (/proc)
/proc/43/net/route: Excluded (/proc)
/proc/43/net/arp: Excluded (/proc)
/proc/43/net/psched: Excluded (/proc)
/proc/43/net/dev_mcast: Excluded (/proc)
/proc/43/net/wireless: Excluded (/proc)
/proc/43/net/ptype: Excluded (/proc)
/proc/43/net/softnet_stat: Excluded (/proc)
/proc/43/net/dev: Excluded (/proc)
/proc/43/net/protocols: Excluded (/proc)
/proc/43/net/connector: Excluded (/proc)
/proc/43/net/netlink: Excluded (/proc)
/proc/43/net/netfilter/nf_log: Excluded (/proc)
/proc/43/net/netfilter/nf_queue: Excluded (/proc)
/proc/43/net/stat/nf_conntrack: Excluded (/proc)
/proc/43/net/stat/rt_cache: Excluded (/proc)
/proc/43/net/stat/arp_cache: Excluded (/proc)
/proc/43/environ: Excluded (/proc)
/proc/43/auxv: Excluded (/proc)
/proc/43/status: Excluded (/proc)
/proc/43/personality: Excluded (/proc)
/proc/43/limits: Excluded (/proc)
/proc/43/sched: Excluded (/proc)
/proc/43/autogroup: Excluded (/proc)
/proc/43/comm: Excluded (/proc)
/proc/43/syscall: Excluded (/proc)
/proc/43/cmdline: Excluded (/proc)
/proc/43/stat: Excluded (/proc)
/proc/43/statm: Excluded (/proc)
/proc/43/maps: Excluded (/proc)
/proc/43/numa_maps: Excluded (/proc)
/proc/43/mem: Excluded (/proc)
/proc/43/cwd: Symbolic link
/proc/43/root: Symbolic link
/proc/43/exe: Symbolic link
/proc/43/mounts: Excluded (/proc)
/proc/43/mountinfo: Excluded (/proc)
/proc/43/mountstats: Excluded (/proc)
/proc/43/clear_refs: Excluded (/proc)
/proc/43/smaps: Excluded (/proc)
/proc/43/pagemap: Excluded (/proc)
/proc/43/attr/current: Excluded (/proc)
/proc/43/attr/prev: Excluded (/proc)
/proc/43/attr/exec: Excluded (/proc)
/proc/43/attr/fscreate: Excluded (/proc)
/proc/43/attr/keycreate: Excluded (/proc)
/proc/43/attr/sockcreate: Excluded (/proc)
/proc/43/wchan: Excluded (/proc)
/proc/43/stack: Excluded (/proc)
/proc/43/schedstat: Excluded (/proc)
/proc/43/cpuset: Excluded (/proc)
/proc/43/cgroup: Excluded (/proc)
/proc/43/oom_score: Excluded (/proc)
/proc/43/oom_adj: Excluded (/proc)
/proc/43/oom_score_adj: Excluded (/proc)
/proc/43/loginuid: Excluded (/proc)
/proc/43/sessionid: Excluded (/proc)
/proc/43/coredump_filter: Excluded (/proc)
/proc/43/io: Excluded (/proc)
/proc/44/task/44/ns/net: Symbolic link
/proc/44/task/44/ns/uts: Symbolic link
/proc/44/task/44/ns/ipc: Symbolic link
/proc/44/task/44/ns/mnt: Symbolic link
/proc/44/task/44/ns/pid: Symbolic link
/proc/44/task/44/environ: Excluded (/proc)
/proc/44/task/44/auxv: Excluded (/proc)
/proc/44/task/44/status: Excluded (/proc)
/proc/44/task/44/personality: Excluded (/proc)
/proc/44/task/44/limits: Excluded (/proc)
/proc/44/task/44/sched: Excluded (/proc)
/proc/44/task/44/comm: Excluded (/proc)
/proc/44/task/44/syscall: Excluded (/proc)
/proc/44/task/44/cmdline: Excluded (/proc)
/proc/44/task/44/stat: Excluded (/proc)
/proc/44/task/44/statm: Excluded (/proc)
/proc/44/task/44/maps: Excluded (/proc)
/proc/44/task/44/numa_maps: Excluded (/proc)
/proc/44/task/44/mem: Excluded (/proc)
/proc/44/task/44/cwd: Symbolic link
/proc/44/task/44/root: Symbolic link
/proc/44/task/44/exe: Symbolic link
/proc/44/task/44/mounts: Excluded (/proc)
/proc/44/task/44/mountinfo: Excluded (/proc)
/proc/44/task/44/clear_refs: Excluded (/proc)
/proc/44/task/44/smaps: Excluded (/proc)
/proc/44/task/44/pagemap: Excluded (/proc)
/proc/44/task/44/attr/current: Excluded (/proc)
/proc/44/task/44/attr/prev: Excluded (/proc)
/proc/44/task/44/attr/exec: Excluded (/proc)
/proc/44/task/44/attr/fscreate: Excluded (/proc)
/proc/44/task/44/attr/keycreate: Excluded (/proc)
/proc/44/task/44/attr/sockcreate: Excluded (/proc)
/proc/44/task/44/wchan: Excluded (/proc)
/proc/44/task/44/stack: Excluded (/proc)
/proc/44/task/44/schedstat: Excluded (/proc)
/proc/44/task/44/cpuset: Excluded (/proc)
/proc/44/task/44/cgroup: Excluded (/proc)
/proc/44/task/44/oom_score: Excluded (/proc)
/proc/44/task/44/oom_adj: Excluded (/proc)
/proc/44/task/44/oom_score_adj: Excluded (/proc)
/proc/44/task/44/loginuid: Excluded (/proc)
/proc/44/task/44/sessionid: Excluded (/proc)
/proc/44/task/44/io: Excluded (/proc)
/proc/44/ns/net: Symbolic link
/proc/44/ns/uts: Symbolic link
/proc/44/ns/ipc: Symbolic link
/proc/44/ns/mnt: Symbolic link
/proc/44/ns/pid: Symbolic link
/proc/44/net/ip6_tables_targets: Excluded (/proc)
/proc/44/net/ip6_tables_matches: Excluded (/proc)
/proc/44/net/ip6_tables_names: Excluded (/proc)
/proc/44/net/nf_conntrack: Excluded (/proc)
/proc/44/net/nf_conntrack_expect: Excluded (/proc)
/proc/44/net/ip_tables_targets: Excluded (/proc)
/proc/44/net/ip_tables_matches: Excluded (/proc)
/proc/44/net/ip_tables_names: Excluded (/proc)
/proc/44/net/packet: Excluded (/proc)
/proc/44/net/unix: Excluded (/proc)
/proc/44/net/snmp: Excluded (/proc)
/proc/44/net/netstat: Excluded (/proc)
/proc/44/net/sockstat: Excluded (/proc)
/proc/44/net/icmp: Excluded (/proc)
/proc/44/net/udp: Excluded (/proc)
/proc/44/net/tcp: Excluded (/proc)
/proc/44/net/raw: Excluded (/proc)
/proc/44/net/ip_mr_cache: Excluded (/proc)
/proc/44/net/ip_mr_vif: Excluded (/proc)
/proc/44/net/udplite: Excluded (/proc)
/proc/44/net/mcfilter: Excluded (/proc)
/proc/44/net/igmp: Excluded (/proc)
/proc/44/net/xfrm_stat: Excluded (/proc)
/proc/44/net/rt_acct: Excluded (/proc)
/proc/44/net/rt_cache: Excluded (/proc)
/proc/44/net/route: Excluded (/proc)
/proc/44/net/arp: Excluded (/proc)
/proc/44/net/psched: Excluded (/proc)
/proc/44/net/dev_mcast: Excluded (/proc)
/proc/44/net/wireless: Excluded (/proc)
/proc/44/net/ptype: Excluded (/proc)
/proc/44/net/softnet_stat: Excluded (/proc)
/proc/44/net/dev: Excluded (/proc)
/proc/44/net/protocols: Excluded (/proc)
/proc/44/net/connector: Excluded (/proc)
/proc/44/net/netlink: Excluded (/proc)
/proc/44/net/netfilter/nf_log: Excluded (/proc)
/proc/44/net/netfilter/nf_queue: Excluded (/proc)
/proc/44/net/stat/nf_conntrack: Excluded (/proc)
/proc/44/net/stat/rt_cache: Excluded (/proc)
/proc/44/net/stat/arp_cache: Excluded (/proc)
/proc/44/environ: Excluded (/proc)
/proc/44/auxv: Excluded (/proc)
/proc/44/status: Excluded (/proc)
/proc/44/personality: Excluded (/proc)
/proc/44/limits: Excluded (/proc)
/proc/44/sched: Excluded (/proc)
/proc/44/autogroup: Excluded (/proc)
/proc/44/comm: Excluded (/proc)
/proc/44/syscall: Excluded (/proc)
/proc/44/cmdline: Excluded (/proc)
/proc/44/stat: Excluded (/proc)
/proc/44/statm: Excluded (/proc)
/proc/44/maps: Excluded (/proc)
/proc/44/numa_maps: Excluded (/proc)
/proc/44/mem: Excluded (/proc)
/proc/44/cwd: Symbolic link
/proc/44/root: Symbolic link
/proc/44/exe: Symbolic link
/proc/44/mounts: Excluded (/proc)
/proc/44/mountinfo: Excluded (/proc)
/proc/44/mountstats: Excluded (/proc)
/proc/44/clear_refs: Excluded (/proc)
/proc/44/smaps: Excluded (/proc)
/proc/44/pagemap: Excluded (/proc)
/proc/44/attr/current: Excluded (/proc)
/proc/44/attr/prev: Excluded (/proc)
/proc/44/attr/exec: Excluded (/proc)
/proc/44/attr/fscreate: Excluded (/proc)
/proc/44/attr/keycreate: Excluded (/proc)
/proc/44/attr/sockcreate: Excluded (/proc)
/proc/44/wchan: Excluded (/proc)
/proc/44/stack: Excluded (/proc)
/proc/44/schedstat: Excluded (/proc)
/proc/44/cpuset: Excluded (/proc)
/proc/44/cgroup: Excluded (/proc)
/proc/44/oom_score: Excluded (/proc)
/proc/44/oom_adj: Excluded (/proc)
/proc/44/oom_score_adj: Excluded (/proc)
/proc/44/loginuid: Excluded (/proc)
/proc/44/sessionid: Excluded (/proc)
/proc/44/coredump_filter: Excluded (/proc)
/proc/44/io: Excluded (/proc)
/proc/45/task/45/ns/net: Symbolic link
/proc/45/task/45/ns/uts: Symbolic link
/proc/45/task/45/ns/ipc: Symbolic link
/proc/45/task/45/ns/mnt: Symbolic link
/proc/45/task/45/ns/pid: Symbolic link
/proc/45/task/45/environ: Excluded (/proc)
/proc/45/task/45/auxv: Excluded (/proc)
/proc/45/task/45/status: Excluded (/proc)
/proc/45/task/45/personality: Excluded (/proc)
/proc/45/task/45/limits: Excluded (/proc)
/proc/45/task/45/sched: Excluded (/proc)
/proc/45/task/45/comm: Excluded (/proc)
/proc/45/task/45/syscall: Excluded (/proc)
/proc/45/task/45/cmdline: Excluded (/proc)
/proc/45/task/45/stat: Excluded (/proc)
/proc/45/task/45/statm: Excluded (/proc)
/proc/45/task/45/maps: Excluded (/proc)
/proc/45/task/45/numa_maps: Excluded (/proc)
/proc/45/task/45/mem: Excluded (/proc)
/proc/45/task/45/cwd: Symbolic link
/proc/45/task/45/root: Symbolic link
/proc/45/task/45/exe: Symbolic link
/proc/45/task/45/mounts: Excluded (/proc)
/proc/45/task/45/mountinfo: Excluded (/proc)
/proc/45/task/45/clear_refs: Excluded (/proc)
/proc/45/task/45/smaps: Excluded (/proc)
/proc/45/task/45/pagemap: Excluded (/proc)
/proc/45/task/45/attr/current: Excluded (/proc)
/proc/45/task/45/attr/prev: Excluded (/proc)
/proc/45/task/45/attr/exec: Excluded (/proc)
/proc/45/task/45/attr/fscreate: Excluded (/proc)
/proc/45/task/45/attr/keycreate: Excluded (/proc)
/proc/45/task/45/attr/sockcreate: Excluded (/proc)
/proc/45/task/45/wchan: Excluded (/proc)
/proc/45/task/45/stack: Excluded (/proc)
/proc/45/task/45/schedstat: Excluded (/proc)
/proc/45/task/45/cpuset: Excluded (/proc)
/proc/45/task/45/cgroup: Excluded (/proc)
/proc/45/task/45/oom_score: Excluded (/proc)
/proc/45/task/45/oom_adj: Excluded (/proc)
/proc/45/task/45/oom_score_adj: Excluded (/proc)
/proc/45/task/45/loginuid: Excluded (/proc)
/proc/45/task/45/sessionid: Excluded (/proc)
/proc/45/task/45/io: Excluded (/proc)
/proc/45/ns/net: Symbolic link
/proc/45/ns/uts: Symbolic link
/proc/45/ns/ipc: Symbolic link
/proc/45/ns/mnt: Symbolic link
/proc/45/ns/pid: Symbolic link
/proc/45/net/ip6_tables_targets: Excluded (/proc)
/proc/45/net/ip6_tables_matches: Excluded (/proc)
/proc/45/net/ip6_tables_names: Excluded (/proc)
/proc/45/net/nf_conntrack: Excluded (/proc)
/proc/45/net/nf_conntrack_expect: Excluded (/proc)
/proc/45/net/ip_tables_targets: Excluded (/proc)
/proc/45/net/ip_tables_matches: Excluded (/proc)
/proc/45/net/ip_tables_names: Excluded (/proc)
/proc/45/net/packet: Excluded (/proc)
/proc/45/net/unix: Excluded (/proc)
/proc/45/net/snmp: Excluded (/proc)
/proc/45/net/netstat: Excluded (/proc)
/proc/45/net/sockstat: Excluded (/proc)
/proc/45/net/icmp: Excluded (/proc)
/proc/45/net/udp: Excluded (/proc)
/proc/45/net/tcp: Excluded (/proc)
/proc/45/net/raw: Excluded (/proc)
/proc/45/net/ip_mr_cache: Excluded (/proc)
/proc/45/net/ip_mr_vif: Excluded (/proc)
/proc/45/net/udplite: Excluded (/proc)
/proc/45/net/mcfilter: Excluded (/proc)
/proc/45/net/igmp: Excluded (/proc)
/proc/45/net/xfrm_stat: Excluded (/proc)
/proc/45/net/rt_acct: Excluded (/proc)
/proc/45/net/rt_cache: Excluded (/proc)
/proc/45/net/route: Excluded (/proc)
/proc/45/net/arp: Excluded (/proc)
/proc/45/net/psched: Excluded (/proc)
/proc/45/net/dev_mcast: Excluded (/proc)
/proc/45/net/wireless: Excluded (/proc)
/proc/45/net/ptype: Excluded (/proc)
/proc/45/net/softnet_stat: Excluded (/proc)
/proc/45/net/dev: Excluded (/proc)
/proc/45/net/protocols: Excluded (/proc)
/proc/45/net/connector: Excluded (/proc)
/proc/45/net/netlink: Excluded (/proc)
/proc/45/net/netfilter/nf_log: Excluded (/proc)
/proc/45/net/netfilter/nf_queue: Excluded (/proc)
/proc/45/net/stat/nf_conntrack: Excluded (/proc)
/proc/45/net/stat/rt_cache: Excluded (/proc)
/proc/45/net/stat/arp_cache: Excluded (/proc)
/proc/45/environ: Excluded (/proc)
/proc/45/auxv: Excluded (/proc)
/proc/45/status: Excluded (/proc)
/proc/45/personality: Excluded (/proc)
/proc/45/limits: Excluded (/proc)
/proc/45/sched: Excluded (/proc)
/proc/45/autogroup: Excluded (/proc)
/proc/45/comm: Excluded (/proc)
/proc/45/syscall: Excluded (/proc)
/proc/45/cmdline: Excluded (/proc)
/proc/45/stat: Excluded (/proc)
/proc/45/statm: Excluded (/proc)
/proc/45/maps: Excluded (/proc)
/proc/45/numa_maps: Excluded (/proc)
/proc/45/mem: Excluded (/proc)
/proc/45/cwd: Symbolic link
/proc/45/root: Symbolic link
/proc/45/exe: Symbolic link
/proc/45/mounts: Excluded (/proc)
/proc/45/mountinfo: Excluded (/proc)
/proc/45/mountstats: Excluded (/proc)
/proc/45/clear_refs: Excluded (/proc)
/proc/45/smaps: Excluded (/proc)
/proc/45/pagemap: Excluded (/proc)
/proc/45/attr/current: Excluded (/proc)
/proc/45/attr/prev: Excluded (/proc)
/proc/45/attr/exec: Excluded (/proc)
/proc/45/attr/fscreate: Excluded (/proc)
/proc/45/attr/keycreate: Excluded (/proc)
/proc/45/attr/sockcreate: Excluded (/proc)
/proc/45/wchan: Excluded (/proc)
/proc/45/stack: Excluded (/proc)
/proc/45/schedstat: Excluded (/proc)
/proc/45/cpuset: Excluded (/proc)
/proc/45/cgroup: Excluded (/proc)
/proc/45/oom_score: Excluded (/proc)
/proc/45/oom_adj: Excluded (/proc)
/proc/45/oom_score_adj: Excluded (/proc)
/proc/45/loginuid: Excluded (/proc)
/proc/45/sessionid: Excluded (/proc)
/proc/45/coredump_filter: Excluded (/proc)
/proc/45/io: Excluded (/proc)
/proc/46/task/46/ns/net: Symbolic link
/proc/46/task/46/ns/uts: Symbolic link
/proc/46/task/46/ns/ipc: Symbolic link
/proc/46/task/46/ns/mnt: Symbolic link
/proc/46/task/46/ns/pid: Symbolic link
/proc/46/task/46/environ: Excluded (/proc)
/proc/46/task/46/auxv: Excluded (/proc)
/proc/46/task/46/status: Excluded (/proc)
/proc/46/task/46/personality: Excluded (/proc)
/proc/46/task/46/limits: Excluded (/proc)
/proc/46/task/46/sched: Excluded (/proc)
/proc/46/task/46/comm: Excluded (/proc)
/proc/46/task/46/syscall: Excluded (/proc)
/proc/46/task/46/cmdline: Excluded (/proc)
/proc/46/task/46/stat: Excluded (/proc)
/proc/46/task/46/statm: Excluded (/proc)
/proc/46/task/46/maps: Excluded (/proc)
/proc/46/task/46/numa_maps: Excluded (/proc)
/proc/46/task/46/mem: Excluded (/proc)
/proc/46/task/46/cwd: Symbolic link
/proc/46/task/46/root: Symbolic link
/proc/46/task/46/exe: Symbolic link
/proc/46/task/46/mounts: Excluded (/proc)
/proc/46/task/46/mountinfo: Excluded (/proc)
/proc/46/task/46/clear_refs: Excluded (/proc)
/proc/46/task/46/smaps: Excluded (/proc)
/proc/46/task/46/pagemap: Excluded (/proc)
/proc/46/task/46/attr/current: Excluded (/proc)
/proc/46/task/46/attr/prev: Excluded (/proc)
/proc/46/task/46/attr/exec: Excluded (/proc)
/proc/46/task/46/attr/fscreate: Excluded (/proc)
/proc/46/task/46/attr/keycreate: Excluded (/proc)
/proc/46/task/46/attr/sockcreate: Excluded (/proc)
/proc/46/task/46/wchan: Excluded (/proc)
/proc/46/task/46/stack: Excluded (/proc)
/proc/46/task/46/schedstat: Excluded (/proc)
/proc/46/task/46/cpuset: Excluded (/proc)
/proc/46/task/46/cgroup: Excluded (/proc)
/proc/46/task/46/oom_score: Excluded (/proc)
/proc/46/task/46/oom_adj: Excluded (/proc)
/proc/46/task/46/oom_score_adj: Excluded (/proc)
/proc/46/task/46/loginuid: Excluded (/proc)
/proc/46/task/46/sessionid: Excluded (/proc)
/proc/46/task/46/io: Excluded (/proc)
/proc/46/ns/net: Symbolic link
/proc/46/ns/uts: Symbolic link
/proc/46/ns/ipc: Symbolic link
/proc/46/ns/mnt: Symbolic link
/proc/46/ns/pid: Symbolic link
/proc/46/net/ip6_tables_targets: Excluded (/proc)
/proc/46/net/ip6_tables_matches: Excluded (/proc)
/proc/46/net/ip6_tables_names: Excluded (/proc)
/proc/46/net/nf_conntrack: Excluded (/proc)
/proc/46/net/nf_conntrack_expect: Excluded (/proc)
/proc/46/net/ip_tables_targets: Excluded (/proc)
/proc/46/net/ip_tables_matches: Excluded (/proc)
/proc/46/net/ip_tables_names: Excluded (/proc)
/proc/46/net/packet: Excluded (/proc)
/proc/46/net/unix: Excluded (/proc)
/proc/46/net/snmp: Excluded (/proc)
/proc/46/net/netstat: Excluded (/proc)
/proc/46/net/sockstat: Excluded (/proc)
/proc/46/net/icmp: Excluded (/proc)
/proc/46/net/udp: Excluded (/proc)
/proc/46/net/tcp: Excluded (/proc)
/proc/46/net/raw: Excluded (/proc)
/proc/46/net/ip_mr_cache: Excluded (/proc)
/proc/46/net/ip_mr_vif: Excluded (/proc)
/proc/46/net/udplite: Excluded (/proc)
/proc/46/net/mcfilter: Excluded (/proc)
/proc/46/net/igmp: Excluded (/proc)
/proc/46/net/xfrm_stat: Excluded (/proc)
/proc/46/net/rt_acct: Excluded (/proc)
/proc/46/net/rt_cache: Excluded (/proc)
/proc/46/net/route: Excluded (/proc)
/proc/46/net/arp: Excluded (/proc)
/proc/46/net/psched: Excluded (/proc)
/proc/46/net/dev_mcast: Excluded (/proc)
/proc/46/net/wireless: Excluded (/proc)
/proc/46/net/ptype: Excluded (/proc)
/proc/46/net/softnet_stat: Excluded (/proc)
/proc/46/net/dev: Excluded (/proc)
/proc/46/net/protocols: Excluded (/proc)
/proc/46/net/connector: Excluded (/proc)
/proc/46/net/netlink: Excluded (/proc)
/proc/46/net/netfilter/nf_log: Excluded (/proc)
/proc/46/net/netfilter/nf_queue: Excluded (/proc)
/proc/46/net/stat/nf_conntrack: Excluded (/proc)
/proc/46/net/stat/rt_cache: Excluded (/proc)
/proc/46/net/stat/arp_cache: Excluded (/proc)
/proc/46/environ: Excluded (/proc)
/proc/46/auxv: Excluded (/proc)
/proc/46/status: Excluded (/proc)
/proc/46/personality: Excluded (/proc)
/proc/46/limits: Excluded (/proc)
/proc/46/sched: Excluded (/proc)
/proc/46/autogroup: Excluded (/proc)
/proc/46/comm: Excluded (/proc)
/proc/46/syscall: Excluded (/proc)
/proc/46/cmdline: Excluded (/proc)
/proc/46/stat: Excluded (/proc)
/proc/46/statm: Excluded (/proc)
/proc/46/maps: Excluded (/proc)
/proc/46/numa_maps: Excluded (/proc)
/proc/46/mem: Excluded (/proc)
/proc/46/cwd: Symbolic link
/proc/46/root: Symbolic link
/proc/46/exe: Symbolic link
/proc/46/mounts: Excluded (/proc)
/proc/46/mountinfo: Excluded (/proc)
/proc/46/mountstats: Excluded (/proc)
/proc/46/clear_refs: Excluded (/proc)
/proc/46/smaps: Excluded (/proc)
/proc/46/pagemap: Excluded (/proc)
/proc/46/attr/current: Excluded (/proc)
/proc/46/attr/prev: Excluded (/proc)
/proc/46/attr/exec: Excluded (/proc)
/proc/46/attr/fscreate: Excluded (/proc)
/proc/46/attr/keycreate: Excluded (/proc)
/proc/46/attr/sockcreate: Excluded (/proc)
/proc/46/wchan: Excluded (/proc)
/proc/46/stack: Excluded (/proc)
/proc/46/schedstat: Excluded (/proc)
/proc/46/cpuset: Excluded (/proc)
/proc/46/cgroup: Excluded (/proc)
/proc/46/oom_score: Excluded (/proc)
/proc/46/oom_adj: Excluded (/proc)
/proc/46/oom_score_adj: Excluded (/proc)
/proc/46/loginuid: Excluded (/proc)
/proc/46/sessionid: Excluded (/proc)
/proc/46/coredump_filter: Excluded (/proc)
/proc/46/io: Excluded (/proc)
/proc/47/task/47/ns/net: Symbolic link
/proc/47/task/47/ns/uts: Symbolic link
/proc/47/task/47/ns/ipc: Symbolic link
/proc/47/task/47/ns/mnt: Symbolic link
/proc/47/task/47/ns/pid: Symbolic link
/proc/47/task/47/environ: Excluded (/proc)
/proc/47/task/47/auxv: Excluded (/proc)
/proc/47/task/47/status: Excluded (/proc)
/proc/47/task/47/personality: Excluded (/proc)
/proc/47/task/47/limits: Excluded (/proc)
/proc/47/task/47/sched: Excluded (/proc)
/proc/47/task/47/comm: Excluded (/proc)
/proc/47/task/47/syscall: Excluded (/proc)
/proc/47/task/47/cmdline: Excluded (/proc)
/proc/47/task/47/stat: Excluded (/proc)
/proc/47/task/47/statm: Excluded (/proc)
/proc/47/task/47/maps: Excluded (/proc)
/proc/47/task/47/numa_maps: Excluded (/proc)
/proc/47/task/47/mem: Excluded (/proc)
/proc/47/task/47/cwd: Symbolic link
/proc/47/task/47/root: Symbolic link
/proc/47/task/47/exe: Symbolic link
/proc/47/task/47/mounts: Excluded (/proc)
/proc/47/task/47/mountinfo: Excluded (/proc)
/proc/47/task/47/clear_refs: Excluded (/proc)
/proc/47/task/47/smaps: Excluded (/proc)
/proc/47/task/47/pagemap: Excluded (/proc)
/proc/47/task/47/attr/current: Excluded (/proc)
/proc/47/task/47/attr/prev: Excluded (/proc)
/proc/47/task/47/attr/exec: Excluded (/proc)
/proc/47/task/47/attr/fscreate: Excluded (/proc)
/proc/47/task/47/attr/keycreate: Excluded (/proc)
/proc/47/task/47/attr/sockcreate: Excluded (/proc)
/proc/47/task/47/wchan: Excluded (/proc)
/proc/47/task/47/stack: Excluded (/proc)
/proc/47/task/47/schedstat: Excluded (/proc)
/proc/47/task/47/cpuset: Excluded (/proc)
/proc/47/task/47/cgroup: Excluded (/proc)
/proc/47/task/47/oom_score: Excluded (/proc)
/proc/47/task/47/oom_adj: Excluded (/proc)
/proc/47/task/47/oom_score_adj: Excluded (/proc)
/proc/47/task/47/loginuid: Excluded (/proc)
/proc/47/task/47/sessionid: Excluded (/proc)
/proc/47/task/47/io: Excluded (/proc)
/proc/47/ns/net: Symbolic link
/proc/47/ns/uts: Symbolic link
/proc/47/ns/ipc: Symbolic link
/proc/47/ns/mnt: Symbolic link
/proc/47/ns/pid: Symbolic link
/proc/47/net/ip6_tables_targets: Excluded (/proc)
/proc/47/net/ip6_tables_matches: Excluded (/proc)
/proc/47/net/ip6_tables_names: Excluded (/proc)
/proc/47/net/nf_conntrack: Excluded (/proc)
/proc/47/net/nf_conntrack_expect: Excluded (/proc)
/proc/47/net/ip_tables_targets: Excluded (/proc)
/proc/47/net/ip_tables_matches: Excluded (/proc)
/proc/47/net/ip_tables_names: Excluded (/proc)
/proc/47/net/packet: Excluded (/proc)
/proc/47/net/unix: Excluded (/proc)
/proc/47/net/snmp: Excluded (/proc)
/proc/47/net/netstat: Excluded (/proc)
/proc/47/net/sockstat: Excluded (/proc)
/proc/47/net/icmp: Excluded (/proc)
/proc/47/net/udp: Excluded (/proc)
/proc/47/net/tcp: Excluded (/proc)
/proc/47/net/raw: Excluded (/proc)
/proc/47/net/ip_mr_cache: Excluded (/proc)
/proc/47/net/ip_mr_vif: Excluded (/proc)
/proc/47/net/udplite: Excluded (/proc)
/proc/47/net/mcfilter: Excluded (/proc)
/proc/47/net/igmp: Excluded (/proc)
/proc/47/net/xfrm_stat: Excluded (/proc)
/proc/47/net/rt_acct: Excluded (/proc)
/proc/47/net/rt_cache: Excluded (/proc)
/proc/47/net/route: Excluded (/proc)
/proc/47/net/arp: Excluded (/proc)
/proc/47/net/psched: Excluded (/proc)
/proc/47/net/dev_mcast: Excluded (/proc)
/proc/47/net/wireless: Excluded (/proc)
/proc/47/net/ptype: Excluded (/proc)
/proc/47/net/softnet_stat: Excluded (/proc)
/proc/47/net/dev: Excluded (/proc)
/proc/47/net/protocols: Excluded (/proc)
/proc/47/net/connector: Excluded (/proc)
/proc/47/net/netlink: Excluded (/proc)
/proc/47/net/netfilter/nf_log: Excluded (/proc)
/proc/47/net/netfilter/nf_queue: Excluded (/proc)
/proc/47/net/stat/nf_conntrack: Excluded (/proc)
/proc/47/net/stat/rt_cache: Excluded (/proc)
/proc/47/net/stat/arp_cache: Excluded (/proc)
/proc/47/environ: Excluded (/proc)
/proc/47/auxv: Excluded (/proc)
</code></pre>
","<centos6><clamav>","2016-01-20 23:00:34"
"750643","NGINX wont start: localtion directive is not allowed","<p>Nginx is not starting or restarting, and gives the error: ""nginx: [emerg] ""location"" directive is not allowed here in /home/admin/conf/web/nginx.conf:100"".</p>

<p>I understand that this is because a location section is not inside of the { } tags, but this account's config file is pretty large and I do not know how to fix this.</p>

<p>Old code took up too much space to post.</p>

<p>Thanks for any help!</p>

<p>Updated Config:</p>

<pre><code>server {
listen      192.168.0.39:80;
server_name test-page.ga www.test-page.ga;
error_log  /var/log/apache2/domains/test-page.ga.error.log error;

location / {
    proxy_pass      http://192.168.0.39:8080;
    location ~* ^.+\.(jpeg|jpg|png|gif|bmp|ico|svg|tif|tiff|css|js|htm|html|ttf|otf|webp|woff|txt|csv|rtf|doc|docx|xls|xlsx|ppt|pptx|odf|odp|ods|odt|pdf|psd|ai|eot|eps|ps|zip|tar|tgz|gz|rar|bz2|7z|aac|m4a|mp3|mp4|ogg|wav|wma|3gp|avi|flv|m4v|mkv|mov|mp4|mpeg|mpg|wmv|exe|iso|dmg|swf)$ {
        root           /home/admin/web/test-page.ga/public_html;
        access_log     /var/log/apache2/domains/test-page.ga.log combined;
        access_log     /var/log/apache2/domains/test-page.ga.bytes bytes;
        expires        max;
        try_files      $uri @fallback;
    }
}

location /error/ {
    alias   /home/admin/web/test-page.ga/document_errors/;
}

location @fallback {
    proxy_pass      http://192.168.0.39:8080;
}

location ~ /\.ht    {return 404;}
location ~ /\.svn/  {return 404;}
location ~ /\.git/  {return 404;}
location ~ /\.hg/   {return 404;}
location ~ /\.bzr/  {return 404;}

include /home/admin/conf/web/nginx.test-page.ga.conf*;
}

server {
listen      192.168.0.39:80;
server_name mailmyserver.ga www.mailmyserver.ga;
error_log  /var/log/apache2/domains/mailmyserver.ga.error.log error;

location / {
    proxy_pass      http://192.168.0.39:8080;
    location ~* ^.+\.(jpeg|jpg|png|gif|bmp|ico|svg|tif|tiff|css|js|htm|html|ttf|otf|webp|woff|txt|csv|rtf|doc|docx|xls|xlsx|ppt|pptx|odf|odp|ods|odt|pdf|psd|ai|eot|eps|ps|zip|tar|tgz|gz|rar|bz2|7z|aac|m4a|mp3|mp4|ogg|wav|wma|3gp|avi|flv|m4v|mkv|mov|mp4|mpeg|mpg|wmv|exe|iso|dmg|swf)$ {
        root           /home/admin/web/mailmyserver.ga/public_html;
        access_log     /var/log/apache2/domains/mailmyserver.ga.log combined;
        access_log     /var/log/apache2/domains/mailmyserver.ga.bytes bytes;
        expires        max;
        try_files      $uri @fallback;
    }
}

location /error/ {
    alias   /home/admin/web/mailmyserver.ga/document_errors/;
}

location @fallback {
    proxy_pass      http://192.168.0.39:8080;
}

location ~ /\.ht    {return 404;}
location ~ /\.svn/  {return 404;}
location ~ /\.git/  {return 404;}
location ~ /\.hg/   {return 404;}
location ~ /\.bzr/  {return 404;}

include /home/admin/conf/web/nginx.mailmyserver.ga.conf*;
}

server {
listen      192.168.0.39:80;
server_name globalcdg.ga www.globalcdg.ga;
error_log  /var/log/apache2/domains/globalcdg.ga.error.log error;

location / {
    proxy_pass      http://192.168.0.39:8080;
    location ~* ^.+\.(jpeg|jpg|png|gif|bmp|ico|svg|tif|tiff|css|js|htm|html|ttf|otf|webp|woff|txt|csv|rtf|doc|docx|xls|xlsx|ppt|pptx|odf|odp|ods|odt|pdf|psd|ai|eot|eps|ps|zip|tar|tgz|gz|rar|bz2|7z|aac|m4a|mp3|mp4|ogg|wav|wma|3gp|avi|flv|m4v|mkv|mov|mp4|mpeg|mpg|wmv|exe|iso|dmg|swf)$ {
        root           /home/admin/web/globalcdg.ga/public_html;
        access_log     /var/log/apache2/domains/globalcdg.ga.log combined;
        access_log     /var/log/apache2/domains/globalcdg.ga.bytes bytes;
        expires        max;
        try_files      $uri @fallback;
    }
}

location /error/ {
    alias   /home/admin/web/globalcdg.ga/document_errors/;
}

location @fallback {
    proxy_pass      http://192.168.0.39:8080;
}

location ~ /\.ht    {return 404;}
location ~ /\.svn/  {return 404;}
location ~ /\.git/  {return 404;}
location ~ /\.hg/   {return 404;}
location ~ /\.bzr/  {return 404;}

include /home/admin/conf/web/nginx.globalcdg.ga.conf*;
}

server {
listen      192.168.0.39:80;
server_name lifepost.site www.lifepost.site;
error_log  /var/log/apache2/domains/lifepost.site.error.log error;

location / {
    proxy_pass      http://192.168.0.39:8080;
    location ~* ^.+\.(jpeg|jpg|png|gif|bmp|ico|svg|tif|tiff|css|js|htm|html|ttf|otf|webp|woff|txt|csv|rtf|doc|docx|xls|xlsx|ppt|pptx|odf|odp|ods|odt|pdf|psd|ai|eot|eps|ps|zip|tar|tgz|gz|rar|bz2|7z|aac|m4a|mp3|mp4|ogg|wav|wma|3gp|avi|flv|m4v|mkv|mov|mp4|mpeg|mpg|wmv|exe|iso|dmg|swf)$ {
        root           /home/admin/web/lifepost.site/public_html;
        access_log     /var/log/apache2/domains/lifepost.site.log combined;
        access_log     /var/log/apache2/domains/lifepost.site.bytes bytes;
        expires        max;
        try_files      $uri @fallback;
    }
}

location /error/ {
    alias   /home/admin/web/lifepost.site/document_errors/;
}

location @fallback {
    proxy_pass      http://192.168.0.39:8080;
}

location ~ /\.ht    {return 404;}
location ~ /\.svn/  {return 404;}
location ~ /\.git/  {return 404;}
location ~ /\.hg/   {return 404;}
location ~ /\.bzr/  {return 404;}

include /home/admin/conf/web/nginx.lifepost.site.conf*;
}

server {
listen      192.168.0.39:80;
server_name cdgmediagroup.com www1.cdgmediagroup.com www.cdgmediagroup.com;
error_log  /var/log/apache2/domains/cdgmediagroup.com.error.log error;

location / {
    proxy_pass      http://192.168.0.39:8080;
    location ~* ^.+\.(jpeg|jpg|png|gif|bmp|ico|svg|tif|tiff|css|js|htm|html|ttf|otf|webp|woff|txt|csv|rtf|doc|docx|xls|xlsx|ppt|pptx|odf|odp|ods|odt|pdf|psd|ai|eot|eps|ps|zip|tar|tgz|gz|rar|bz2|7z|aac|m4a|mp3|mp4|ogg|wav|wma|3gp|avi|flv|m4v|mkv|mov|mp4|mpeg|mpg|wmv|exe|iso|dmg|swf)$ {
        root           /home/admin/web/cdgmediagroup.com/public_html;
        access_log     /var/log/apache2/domains/cdgmediagroup.com.log combined;
        access_log     /var/log/apache2/domains/cdgmediagroup.com.bytes bytes;
        expires        max;
        try_files      $uri @fallback;
    }
}

location /error/ {
    alias   /home/admin/web/cdgmediagroup.com/document_errors/;
}

location @fallback {
    proxy_pass      http://192.168.0.39:8080;
}

location ~ /\.ht    {return 404;}
location ~ /\.svn/  {return 404;}
location ~ /\.git/  {return 404;}
location ~ /\.hg/   {return 404;}
location ~ /\.bzr/  {return 404;}

include /home/admin/conf/web/nginx.cdgmediagroup.com.conf*;
}

server {
listen      192.168.0.39:80;
server_name cartergames.ga www.cartergames.ga;
error_log  /var/log/apache2/domains/cartergames.ga.error.log error;

location / {
    proxy_pass      http://192.168.0.39:8080;
    location ~* ^.+\.(jpeg|jpg|png|gif|bmp|ico|svg|tif|tiff|css|js|htm|html|ttf|otf|webp|woff|txt|csv|rtf|doc|docx|xls|xlsx|ppt|pptx|odf|odp|ods|odt|pdf|psd|ai|eot|eps|ps|zip|tar|tgz|gz|rar|bz2|7z|aac|m4a|mp3|mp4|ogg|wav|wma|3gp|avi|flv|m4v|mkv|mov|mp4|mpeg|mpg|wmv|exe|iso|dmg|swf)$ {
        root           /home/admin/web/cartergames.ga/public_html;
        access_log     /var/log/apache2/domains/cartergames.ga.log combined;
        access_log     /var/log/apache2/domains/cartergames.ga.bytes bytes;
        expires        max;
        try_files      $uri @fallback;
    }
}

location /error/ {
    alias   /home/admin/web/cartergames.ga/document_errors/;
}

location @fallback {
    proxy_pass      http://192.168.0.39:8080;
}

location ~ /\.ht    {return 404;}
location ~ /\.svn/  {return 404;}
location ~ /\.git/  {return 404;}
location ~ /\.hg/   {return 404;}
location ~ /\.bzr/  {return 404;}

include /home/admin/conf/web/nginx.cartergames.ga.conf*;
}

server {
listen      192.168.0.39:80;
server_name oregondate.space www.oregondate.space;
error_log  /var/log/apache2/domains/oregondate.space.error.log error;

location / {
    proxy_pass      http://192.168.0.39:8080;
    location ~* ^.+\.(jpeg|jpg|png|gif|bmp|ico|svg|tif|tiff|css|js|htm|html|ttf|otf|webp|woff|txt|csv|rtf|doc|docx|xls|xlsx|ppt|pptx|odf|odp|ods|odt|pdf|psd|ai|eot|eps|ps|zip|tar|tgz|gz|rar|bz2|7z|aac|m4a|mp3|mp4|ogg|wav|wma|3gp|avi|flv|m4v|mkv|mov|mp4|mpeg|mpg|wmv|exe|iso|dmg|swf)$ {
        root           /home/admin/web/oregondate.space/public_html;
        access_log     /var/log/apache2/domains/oregondate.space.log combined;
        access_log     /var/log/apache2/domains/oregondate.space.bytes bytes;
        expires        max;
        try_files      $uri @fallback;
    }
}

location /error/ {
    alias   /home/admin/web/oregondate.space/document_errors/;
}

location @fallback {
    proxy_pass      http://192.168.0.39:8080;
}

location ~ /\.ht    {return 404;}
location ~ /\.svn/  {return 404;}
location ~ /\.git/  {return 404;}
location ~ /\.hg/   {return 404;}
location ~ /\.bzr/  {return 404;}

include /home/admin/conf/web/nginx.oregondate.space.conf*;
}

server {
listen      192.168.0.39:80;
server_name testdummies.cf www.testdummies.cf;
error_log  /var/log/apache2/domains/testdummies.cf.error.log error;

location / {
    proxy_pass      http://192.168.0.39:8080;
    location ~* ^.+\.(jpeg|jpg|png|gif|bmp|ico|svg|tif|tiff|css|js|htm|html|ttf|otf|webp|woff|txt|csv|rtf|doc|docx|xls|xlsx|ppt|pptx|odf|odp|ods|odt|pdf|psd|ai|eot|eps|ps|zip|tar|tgz|gz|rar|bz2|7z|aac|m4a|mp3|mp4|ogg|wav|wma|3gp|avi|flv|m4v|mkv|mov|mp4|mpeg|mpg|wmv|exe|iso|dmg|swf)$ {
        root           /home/admin/web/testdummies.cf/public_html;
        access_log     /var/log/apache2/domains/testdummies.cf.log combined;
        access_log     /var/log/apache2/domains/testdummies.cf.bytes bytes;
        expires        max;
        try_files      $uri @fallback;
    }
}

location /error/ {
    alias   /home/admin/web/testdummies.cf/document_errors/;
}

location @fallback {
    proxy_pass      http://192.168.0.39:8080;
}

location ~ /\.ht    {return 404;}
location ~ /\.svn/  {return 404;}
location ~ /\.git/  {return 404;}
location ~ /\.hg/   {return 404;}
location ~ /\.bzr/  {return 404;}

include /home/admin/conf/web/nginx.testdummies.cf.conf*;
}

server {
listen      192.168.0.39:80;
server_name cdg.tech www.cdg.tech;
error_log  /var/log/apache2/domains/cdg.tech.error.log error;

location / {
    proxy_pass      http://192.168.0.39:8080;
    location ~* ^.+\.(jpeg|jpg|png|gif|bmp|ico|svg|tif|tiff|css|js|htm|html|ttf|otf|webp|woff|txt|csv|rtf|doc|docx|xls|xlsx|ppt|pptx|odf|odp|ods|odt|pdf|psd|ai|eot|eps|ps|zip|tar|tgz|gz|rar|bz2|7z|aac|m4a|mp3|mp4|ogg|wav|wma|3gp|avi|flv|m4v|mkv|mov|mp4|mpeg|mpg|wmv|exe|iso|dmg|swf)$ {
        root           /home/admin/web/cdg.tech/public_html;
        access_log     /var/log/apache2/domains/cdg.tech.log combined;
        access_log     /var/log/apache2/domains/cdg.tech.bytes bytes;
        expires        max;
        try_files      $uri @fallback;
    }
}

location /error/ {
    alias   /home/admin/web/cdg.tech/document_errors/;
}

location @fallback {
    proxy_pass      http://192.168.0.39:8080;
}

location ~ /\.ht    {return 404;}
location ~ /\.svn/  {return 404;}
location ~ /\.git/  {return 404;}
location ~ /\.hg/   {return 404;}
location ~ /\.bzr/  {return 404;}

include /home/admin/conf/web/nginx.cdg.tech.conf*;
}

server {
listen      192.168.0.39:80;
server_name complexwebs.com www1.complexwebs.com www.complexwebs.com;
error_log  /var/log/apache2/domains/complexwebs.com.error.log error;

location / {
    proxy_pass      http://192.168.0.39:8080;
    location ~* ^.+\.(jpeg|jpg|png|gif|bmp|ico|svg|tif|tiff|css|js|htm|html|ttf|otf|webp|woff|txt|csv|rtf|doc|docx|xls|xlsx|ppt|pptx|odf|odp|ods|odt|pdf|psd|ai|eot|eps|ps|zip|tar|tgz|gz|rar|bz2|7z|aac|m4a|mp3|mp4|ogg|wav|wma|3gp|avi|flv|m4v|mkv|mov|mp4|mpeg|mpg|wmv|exe|iso|dmg|swf)$ {
        root           /home/admin/web/complexwebs.com/public_html;
        access_log     /var/log/apache2/domains/complexwebs.com.log combined;
        access_log     /var/log/apache2/domains/complexwebs.com.bytes bytes;
        expires        max;
        try_files      $uri @fallback;
    }
}

location /error/ {
    alias   /home/admin/web/complexwebs.com/document_errors/;
}

location @fallback {
    proxy_pass      http://192.168.0.39:8080;
}

location ~ /\.ht    {return 404;}
location ~ /\.svn/  {return 404;}
location ~ /\.git/  {return 404;}
location ~ /\.hg/   {return 404;}
location ~ /\.bzr/  {return 404;}

include /home/admin/conf/web/nginx.complexwebs.com.conf*;
}

server {
listen      192.168.0.39:80;
server_name cdghost.xyz www.cdghost.xyz cdghost-xyz.beta.cdg.tech;
error_log  /var/log/apache2/domains/cdghost.xyz.error.log error;

location / {
    proxy_pass      http://192.168.0.39:8080;
    location ~* ^.+\.(jpeg|jpg|png|gif|bmp|ico|svg|tif|tiff|css|js|htm|html|ttf|otf|webp|woff|txt|csv|rtf|doc|docx|xls|xlsx|ppt|pptx|odf|odp|ods|odt|pdf|psd|ai|eot|eps|ps|zip|tar|tgz|gz|rar|bz2|7z|aac|m4a|mp3|mp4|ogg|wav|wma|3gp|avi|flv|m4v|mkv|mov|mp4|mpeg|mpg|wmv|exe|iso|dmg|swf)$ {
        root           /home/admin/web/cdghost.xyz/public_html;
        access_log     /var/log/apache2/domains/cdghost.xyz.log combined;
        access_log     /var/log/apache2/domains/cdghost.xyz.bytes bytes;
        expires        max;
        try_files      $uri @fallback;
    }
}

location /error/ {
    alias   /home/admin/web/cdghost.xyz/document_errors/;
}

location @fallback {
    proxy_pass      http://192.168.0.39:8080;
}

location ~ /\.ht    {return 404;}
location ~ /\.svn/  {return 404;}
location ~ /\.git/  {return 404;}
location ~ /\.hg/   {return 404;}
location ~ /\.bzr/  {return 404;}

include /home/admin/conf/web/nginx.cdghost.xyz.conf*;
}

server {
listen      192.168.0.39:80;
server_name srv39.cdghost.xyz www.srv39.cdghost.xyz srv39-cdghost-xyz.srv39.cdghost.xyz;
error_log  /var/log/apache2/domains/srv39.cdghost.xyz.error.log error;

location /.well-known/acme-challenge {
    alias /etc/letsencrypt/.well-known/acme-challenge;
    location ~ /.well-known/acme-challenge/(.*) {
        add_header Content-Type text/plain;
    }
}

location / {
    proxy_pass      http://192.168.0.39:8080;
    location ~* ^.+\.(jpeg|jpg|png|gif|bmp|ico|svg|tif|tiff|css|js|htm|html|ttf|otf|webp|woff|txt|csv|rtf|doc|docx|xls|xlsx|ppt|pptx|odf|odp|ods|odt|pdf|psd|ai|eot|eps|ps|zip|tar|tgz|gz|rar|bz2|7z|aac|m4a|mp3|mp4|ogg|wav|wma|3gp|avi|flv|m4v|mkv|mov|mpeg|mpg|wmv|exe|iso|dmg|swf)$ {
        root           /home/admin/web/srv39.cdghost.xyz/public_html;
        access_log     /var/log/apache2/domains/srv39.cdghost.xyz.log combined;
        access_log     /var/log/apache2/domains/srv39.cdghost.xyz.bytes bytes;
        expires        max;
        try_files      $uri @fallback;
    }
}

location /error/ {
    alias   /home/admin/web/srv39.cdghost.xyz/document_errors/;
}

location @fallback {
    proxy_pass      http://192.168.0.39:8080;
}

location ~ /\.ht    {return 404;}
location ~ /\.svn/  {return 404;}
location ~ /\.git/  {return 404;}
location ~ /\.hg/   {return 404;}
location ~ /\.bzr/  {return 404;}

include /home/admin/conf/web/nginx.srv39.cdghost.xyz.conf*;
}

server {
listen      192.168.0.39:80;
server_name mlinks.ml www.mlinks.ml mlinks-ml.srv39.cdghost.xyz;
error_log  /var/log/apache2/domains/mlinks.ml.error.log error;

location /.well-known/acme-challenge {
    alias /etc/letsencrypt/.well-known/acme-challenge;
    location ~ /.well-known/acme-challenge/(.*) {
        add_header Content-Type text/plain;
    }
}

location / {
    proxy_pass      http://192.168.0.39:8080;
    location ~* ^.+\.(jpeg|jpg|png|gif|bmp|ico|svg|tif|tiff|css|js|htm|html|ttf|otf|webp|woff|txt|csv|rtf|doc|docx|xls|xlsx|ppt|pptx|odf|odp|ods|odt|pdf|psd|ai|eot|eps|ps|zip|tar|tgz|gz|rar|bz2|7z|aac|m4a|mp3|mp4|ogg|wav|wma|3gp|avi|flv|m4v|mkv|mov|mpeg|mpg|wmv|exe|iso|dmg|swf)$ {
        root           /home/admin/web/mlinks.ml/public_html;
        access_log     /var/log/apache2/domains/mlinks.ml.log combined;
        access_log     /var/log/apache2/domains/mlinks.ml.bytes bytes;
        expires        max;
        try_files      $uri @fallback;
    }
}

location /error/ {
    alias   /home/admin/web/mlinks.ml/document_errors/;
}

location @fallback {
    proxy_pass      http://192.168.0.39:8080;
}

location ~ /\.ht    {return 404;}
location ~ /\.svn/  {return 404;}
location ~ /\.git/  {return 404;}
location ~ /\.hg/   {return 404;}
location ~ /\.bzr/  {return 404;}

include /home/admin/conf/web/nginx.mlinks.ml.conf*;
}

server {
listen      192.168.0.39:80;
server_name zearch.ga www.zearch.ga zearch-ga.srv39.cdghost.xyz;
error_log  /var/log/apache2/domains/zearch.ga.error.log error;

location /.well-known/acme-challenge {
    alias /etc/letsencrypt/.well-known/acme-challenge;
    location ~ /.well-known/acme-challenge/(.*) {
        add_header Content-Type text/plain;
    }
}

location / {
    proxy_pass      http://192.168.0.39:8080;
    location ~* ^.+\.(jpeg|jpg|png|gif|bmp|ico|svg|tif|tiff|css|js|htm|html|ttf|otf|webp|woff|txt|csv|rtf|doc|docx|xls|xlsx|ppt|pptx|odf|odp|ods|odt|pdf|psd|ai|eot|eps|ps|zip|tar|tgz|gz|rar|bz2|7z|aac|m4a|mp3|mp4|ogg|wav|wma|3gp|avi|flv|m4v|mkv|mov|mpeg|mpg|wmv|exe|iso|dmg|swf)$ {
        root           /home/admin/web/zearch.ga/public_html;
        access_log     /var/log/apache2/domains/zearch.ga.log combined;
        access_log     /var/log/apache2/domains/zearch.ga.bytes bytes;
        expires        max;
        try_files      $uri @fallback;
    }
}

location /error/ {
    alias   /home/admin/web/zearch.ga/document_errors/;
}

location @fallback {
    proxy_pass      http://192.168.0.39:8080;
}

location ~ /\.ht    {return 404;}
location ~ /\.svn/  {return 404;}
location ~ /\.git/  {return 404;}
location ~ /\.hg/   {return 404;}
location ~ /\.bzr/  {return 404;}

include /home/admin/conf/web/nginx.zearch.ga.conf*;
}

server {
listen      192.168.0.39:80;
server_name getlifepost.cu.cc www.getlifepost.cu.cc getlifepost-cu-cc.srv39.cdghost.xyz;
error_log  /var/log/apache2/domains/getlifepost.cu.cc.error.log error;

location /.well-known/acme-challenge {
    alias /etc/letsencrypt/.well-known/acme-challenge;
    location ~ /.well-known/acme-challenge/(.*) {
        add_header Content-Type text/plain;
    }
}

location / {
    proxy_pass      http://192.168.0.39:8080;
    location ~* ^.+\.(jpeg|jpg|png|gif|bmp|ico|svg|tif|tiff|css|js|htm|html|ttf|otf|webp|woff|txt|csv|rtf|doc|docx|xls|xlsx|ppt|pptx|odf|odp|ods|odt|pdf|psd|ai|eot|eps|ps|zip|tar|tgz|gz|rar|bz2|7z|aac|m4a|mp3|mp4|ogg|wav|wma|3gp|avi|flv|m4v|mkv|mov|mpeg|mpg|wmv|exe|iso|dmg|swf)$ {
        root           /home/admin/web/getlifepost.cu.cc/public_html;
        access_log     /var/log/apache2/domains/getlifepost.cu.cc.log combined;
        access_log     /var/log/apache2/domains/getlifepost.cu.cc.bytes bytes;
        expires        max;
        try_files      $uri @fallback;
    }
}

location /error/ {
    alias   /home/admin/web/getlifepost.cu.cc/document_errors/;
}

location @fallback {
    proxy_pass      http://192.168.0.39:8080;
}

location ~ /\.ht    {return 404;}
location ~ /\.svn/  {return 404;}
location ~ /\.git/  {return 404;}
location ~ /\.hg/   {return 404;}
location ~ /\.bzr/  {return 404;}

include /home/admin/conf/web/nginx.getlifepost.cu.cc.conf*;
}

server {
listen      192.168.0.39:80;
server_name truos.ga www.truos.ga truos-ga.srv39.cdghost.xyz;
error_log  /var/log/apache2/domains/truos.ga.error.log error;

location /.well-known/acme-challenge {
    alias /etc/letsencrypt/.well-known/acme-challenge;
    location ~ /.well-known/acme-challenge/(.*) {
        add_header Content-Type text/plain;
    }
}

location / {
    proxy_pass      http://192.168.0.39:8080;
    location ~* ^.+\.(jpeg|jpg|png|gif|bmp|ico|svg|tif|tiff|css|js|htm|html|ttf|otf|webp|woff|txt|csv|rtf|doc|docx|xls|xlsx|ppt|pptx|odf|odp|ods|odt|pdf|psd|ai|eot|eps|ps|zip|tar|tgz|gz|rar|bz2|7z|aac|m4a|mp3|mp4|ogg|wav|wma|3gp|avi|flv|m4v|mkv|mov|mp4|mpeg|mpg|wmv|exe|iso|dmg|swf)$ {
        root           /home/admin/web/truos.ga/public_html;
        access_log     /var/log/apache2/domains/truos.ga.log combined;
        access_log     /var/log/apache2/domains/truos.ga.bytes bytes;
        expires        max;
        try_files      $uri @fallback;
    }
}

location /error/ {
    alias   /home/admin/web/truos.ga/document_errors/;
}

location @fallback {
    proxy_pass      http://192.168.0.39:8080;
}

location ~ /\.ht    {return 404;}
location ~ /\.svn/  {return 404;}
location ~ /\.git/  {return 404;}
location ~ /\.hg/   {return 404;}
location ~ /\.bzr/  {return 404;}

include /home/admin/conf/web/nginx.truos.ga.conf*;
}
</code></pre>
","<nginx><configuration><service>","2016-01-21 01:59:00"
"750685","How to use the awk,sed Modify the text?","<p>As follows, some text</p>

<pre><code>16G    16G    1.9G    40G
4G     4G     952M    60G
16G    16G    1.6G    40G
5G     780M   5G      80G
</code></pre>

<p>I want to change all the unit from 'M' to 'G', like this</p>

<pre><code>16G    16G    1.9G    40G
4G     4G     0.92G   60G
16G    16G    1.6G    40G
5G     0.76G  5G      80G
</code></pre>

<p>I can use python to do it, but I don't know how to achieve it with shell?
Can awk,sed,perl... do it?</p>
","<shell><perl><sed><awk>","2016-01-21 07:52:47"
"821802","Is it possible to visit one of the websites that are sharing a same IP by IP address?","<p>I hosted my several websites on Linode, they are all sharing a same IP address.
I use webmin/virtualmin to manage my server and websites. Is it possible to visit one of the websites by IP address?
Currently if I visit the IP, what shows in the browser is just an apache2 default page: <a href=""http://45.79.146.98/"" rel=""nofollow noreferrer"">http://45.79.146.98/</a>. I think this also points to some folder on my server, but I don't know where it is.</p>

<p>P.S. If I can visit my websites by <a href=""http://45.79.146.98/something"" rel=""nofollow noreferrer"">http://45.79.146.98/something</a>, it is also acceptable. But I don't know where to start doing this.</p>
","<ip><webmin><virtualmin>","2016-12-20 12:46:47"
"750941","SetEnv REMOTE ADDR if x-for-forwarded is present","<p>I can translate this in PHP.. but I am curious if it's possible to write this in on .conf file.. background of the setup. We have an ELB behind ec2 on AWS. We found out that REMOTE_ADDR is giving us the ELB ip address and not the client. So we want to fix this. 2 options 1.) in PHP or 2.) If can write in apache..</p>

<p>Can any give me an idea to write the snippet.</p>

<p>Thanks in advance</p>
","<apache-2.4>","2016-01-22 02:18:23"
"965547","Nameserver responding REFUSED on Centos 7","<p>I have domain registered with godaddy. I had set up custom hostname on godaddy and used same nameservers for the domain.
ns1.domain.com and ns2.domain.com (both hostnames are pointing to same ip)</p>

<p>Now I had configured my server with Centos 7 and centos web panel. Below are data of few files which is necessary for setting up DNS. The problem is with DNS that nameserver is not working. Please help me solve this issue. IP: 142.54.176.130 and Domain: getfreereallikes.com</p>

<p>/etc/named.conf:</p>

<pre><code>//
// named.conf
//
// Provided by Red Hat bind package to configure the ISC BIND named(8) DNS
// server as a caching only nameserver (as a any DNS resolver only).
//
// See /usr/share/doc/bind*/sample/ for example named configuration files.
//
// See the BIND Administrator's Reference Manual (ARM) for details about the
// configuration located in /usr/share/doc/bind-{version}/Bv9ARM.html

options {
    listen-on port 53 { any; };
    listen-on-v6 port 53 { ::1; };
    directory   ""/var/named"";
    dump-file   ""/var/named/data/cache_dump.db"";
    statistics-file ""/var/named/data/named_stats.txt"";
    memstatistics-file ""/var/named/data/named_mem_stats.txt"";
    recursing-file  ""/var/named/data/named.recursing"";
    secroots-file   ""/var/named/data/named.secroots"";
    allow-query     { any; };

    /* 
     - If you are building an AUTHORITATIVE DNS server, do NOT enable recursion.
     - If you are building a RECURSIVE (caching) DNS server, you need to enable 
       recursion. 
     - If your recursive DNS server has a public IP address, you MUST enable access 
       control to limit queries to your legitimate users. Failing to do so will
       cause your server to become part of large scale DNS amplification 
       attacks. Implementing BCP38 within your network would greatly
       reduce such attack surface 
    */
    recursion no;

    dnssec-enable yes;
    dnssec-validation no;

    /* Path to ISC DLV key */
    bindkeys-file ""/etc/named.iscdlv.key"";

    managed-keys-directory ""/var/named/dynamic"";

    pid-file ""/run/named/named.pid"";
    session-keyfile ""/run/named/session.key"";
};

logging {
        channel default_debug {
                file ""data/named.run"";
                severity dynamic;
        };
};

zone ""."" IN {
    type hint;
    file ""named.ca"";
};

include ""/etc/named.rfc1912.zones"";
include ""/etc/named.root.key"";


zone ""ns1.getfreereallikes.com"" {type master;file ""/var/named/ns1.getfreereallikes.com.db"";};
zone ""ns2.getfreereallikes.com"" {type master;file ""/var/named/ns2.getfreereallikes.com.db"";};
</code></pre>

<p>/var/named/ns1.getfreereallikes.com.db:</p>

<pre><code>; Panel %version%
; Zone file for ns1.getfreereallikes.com
$TTL 14400
ns1.getfreereallikes.com.      86400      IN      SOA      ns1.getfreereallikes.com.      info.centos-webpanel.com.      (
                    2013071600 ;serial, todays date+todays
                    86400 ;refresh, seconds
                    7200 ;retry, seconds
                    3600000 ;expire, seconds
                    86400 ;minimum, seconds
      )
ns1.getfreereallikes.com. 86400 IN NS ns1.getfreereallikes.com.
ns1.getfreereallikes.com. 86400 IN NS ns2.getfreereallikes.com.
ns1.getfreereallikes.com. 14400 IN A 142.54.176.130
</code></pre>

<p>/var/named/getfreereallikes.com.db:</p>

<pre><code>; Generated by CWP
; Zone file for krushimitra.co.in
$TTL 14400
getfreereallikes.com.      86400      IN      SOA      ns1.getfreereallikes.com.      email.gmail.com.      (
                    2019050200 ;serial, todays date+todays
                        86400 ;refresh, seconds
                        7200 ;retry, seconds
                        3600000 ;expire, seconds
                        86400 
      )


getfreereallikes.com.      86400      IN      NS      ns1.getfreereallikes.com.
getfreereallikes.com.      86400      IN      NS      ns2.getfreereallikes.com.
getfreereallikes.com.      0      IN      A      142.54.176.130
localhost.getfreereallikes.com.      0      IN      A      127.0.0.1
getfreereallikes.com.      0      IN      MX      5      getfreereallikes.com.
mail      0      IN      CNAME      getfreereallikes.com.
www      0      IN      CNAME      getfreereallikes.com.
ftp      0      IN      CNAME      getfreereallikes.com.
;      Add      additional            below      this      line
_dmarc      14400      IN      TXT      ""v=DMARC1; p=none""
ns1      14400      IN      A      142.54.176.130
ns2      14400      IN      A      142.54.176.130
</code></pre>

<p>/etc/resolv.conf:</p>

<pre><code># Generated by NetworkManager
search getfreereallikes.com
nameserver 127.0.0.1
</code></pre>

<p>dig @ 142.54.176.130 www.getfreereallikes.com:</p>

<pre><code>; &lt;&lt;&gt;&gt; DiG 9.9.4-RedHat-9.9.4-73.el7_6 &lt;&lt;&gt;&gt; @142.54.176.130 www.getfreereallikes.com
; (1 server found)
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: REFUSED, id: 2624
;; flags: qr rd; QUERY: 1, ANSWER: 0, AUTHORITY: 0, ADDITIONAL: 1
;; WARNING: recursion requested but not available

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;www.getfreereallikes.com.      IN      A

;; Query time: 0 msec
;; SERVER: 142.54.176.130#53(142.54.176.130)
;; WHEN: Thu May 02 05:56:26 CDT 2019
;; MSG SIZE  rcvd: 53
</code></pre>
","<domain-name-system><centos7><nameserver><dns-zone>","2019-05-02 11:01:32"
"821964","Allow winscp access, restrict direct shell access, allow su - for Linux server local user","<p>Is it possible to restrict access for a particular local user in Linux, with below mentioned conditions:</p>

<ol>
<li>User should have no direct shell access</li>
<li>User should be able to login with command ""su -"" from any other user shell</li>
<li>User should be able to do SFTP via winscp client and copy files from local machine to user's home directory</li>
</ol>
","<ssh><sftp><su><restrictions>","2016-12-21 06:35:27"
"821991","openssl version keeps the old one","<p>I'm trying to update openssl on CentOS 7.3. I tried at the beginning to update it using <code>yum update openssl-devel</code>' but it says : <code>No packages marked for update</code> </p>

<p>When I'm running <code>yum info openssl</code> the version number that appears is : <code>1.0.1e</code></p>

<p>Afterwards I installed openssl 1.0.2i from source using this <a href=""http://www.ehowstuff.com/how-to-install-and-update-openssl-on-centos-6-centos-7/"" rel=""nofollow noreferrer"">guide</a> on CentOS 7.3. I think that my problem is in these two lines:</p>

<pre><code>mv /usr/bin/openssl /root/
ln -s /usr/local/ssl/bin/openssl /usr/bin/openssl
</code></pre>

<p>In both cases when I'm running openssl version after the installation, I'm stil see the old one: </p>

<pre><code>openssl version
OpenSSL 0.9.8l 5 Nov 2009
</code></pre>

<p>What could be the problem?</p>
","<centos7><openssl>","2016-12-21 09:48:11"
"965648","Having Trouble Setting Subdomains On NGINX","<p>I'm currently setting up OpenMediaVault on my Rock64 Single Board PC which is based off Debian and is configured through the web GUI running from NGINX on port 80.</p>

<p>I'm currently trying to install Nextcloud using NGINX, MariaDB/MySQL and PHPMyAdmin, and originally I was hoping to use Docker to install and manage them all. Unfortunately I've had nothing but problems so I reverted to installing the software from the Linux repository. </p>

<p>I have successfully setup a MySQL server with a user and database ready for Nextcloud and have managed to get PHPMyAdmin working on a different port 8080. </p>

<p>Instead of using ports I would like to use subdomains but I can't seem to get them to work. So far I have set the root directories as the following:-</p>

<pre><code>/var/www/openmediavault
/var/www/phpmyadmin
/var/www/nextcloud
</code></pre>

<p>I would like the following subdomains:- N.B I will setup SSL at a later date once I take this over a WAN.</p>

<ul>
<li><a href=""http://rock64.lan"" rel=""nofollow noreferrer"">http://rock64.lan</a> (default OpenMediaVault web GUI)</li>
<li><a href=""http://phpmyadmin.rock64.lan"" rel=""nofollow noreferrer"">http://phpmyadmin.rock64.lan</a> (PHPMyAdmin GUI for MySQL)</li>
<li><a href=""http://nextcloud.rock64.lan"" rel=""nofollow noreferrer"">http://nextcloud.rock64.lan</a> (Nextcloud web GUI)</li>
</ul>

<p>My NGINX is configured to use
<code>/etc/nginx/sites-available</code>
and
<code>/etc/nginx/sites-enabled</code></p>

<p>The following symbolic links will be used with respective names:-</p>

<pre><code>ln -s /etc/nginx/sites-available/openmediavault-webgui /etc/nginx/sites-enabled/openmediavault-webgui
ln -s /etc/nginx/sites-available/phpmyadmin /etc/nginx/sites-enabled/phpmyadmin
ln -s /etc/nginx/sites-available/nextcloud /etc/nginx/sites-enabled/nextcloud
</code></pre>

<p>This is the conetents of the openmediavault-webgui file</p>

<pre><code>server {
    server_name openmediavault-webgui;
    root /var/www/openmediavault;
    index index.php;
    autoindex off;
    server_tokens off;
    sendfile on;
    large_client_header_buffers 4 32k;
    client_max_body_size 25M;
    error_log /var/log/nginx/openmediavault-webgui_error.log error;
    access_log /var/log/nginx/openmediavault-webgui_access.log combined;
    error_page 404 = /404.php;
    location /404.html {
        internal;
    }
    location /extjs6/ {
        alias /usr/share/javascript/extjs6/;
        expires 2d;
        }
    location ~ ^/(css|fonts|js|images)/ {
        expires 2d;
    }
    location /favicon {
        expires 14d;
    }
    location ~ \.php$ {
        try_files $uri =404;
        fastcgi_split_path_info ^(.+\.php)(/.+)$;
        fastcgi_pass unix:/var/run/php-fpm-openmediavault-webgui.sock;
        fastcgi_index index.php;
        fastcgi_read_timeout 60s;
        include fastcgi.conf;
        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
    }
    listen 80 default_server;
    include /etc/nginx/openmediavault-webgui.d/*.conf;
}
</code></pre>

<p>I also have a symbolic link from PHPMyAdmin's location to www directory</p>

<p><code>ln -s /usr/share/phpmyadmin /var/www/phpmyadmin</code></p>

<p>Many thanks</p>

<p>Will</p>

<p><strong>UPDATE 10:18 03/06/2019</strong>
Here is the server block for /etc/nginx/sites-enabled/phpmyadmin
<code>
server {
    listen 80;
    root /var/www/phpmyadmin;
    index index.php;
    server_name phpmyadmin.rock64.lan;
    location / {
        try_files $uri $uri/ =404;
    }
    location ~ \.php {
        include snippets/fastcgi-php.conf;
        fastcgi_pass unix:/var/run/php/php7-0.fpm.sock;
        fastcgi_index index.php;
    }
}
</code></p>

<p><strong>UPDATE 11:25 03/06/2019</strong>
After configuring my OpenWrt router I can do a DNS lookup and <code>http://phpmyadmin.rock64.lan</code> is pointing to the correct IP address. However, I'm getting 502 Bad Gateway. One forum I came across mentioned setting the ownership and permissions. I've had play around with them  but no change.</p>

<p>Under /etc/php/7.0/fpm/pool.d/ I have:</p>

<pre><code>[openmediavault-webgui]
user = openmediavault-webgui
group = openmediavault-webgui

listen = /var/run/php-fpm-openmediavault-webgui.sock
listen.owner = www-data
listen.group = www-data
listen.mode = 0600

pm = ondemand
pm.max_children = 25
pm.process_idle_timeout = 10s

chdir = /

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
; openmediavault php.ini settings ;
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

; Paths and Directories
php_value[include_path] = "".:/usr/share/php:/var/www/openmediavault""

; Pam Authentication Support (see /etc/pam.d)
php_value[pam.servicename] = ""openmediavault-webgui"";

; Maximum allowed size for uploaded files.
; http://php.net/upload-max-filesize
php_value[upload_max_filesize] = 25M

; Maximum size of POST data that PHP will accept.
; http://php.net/post-max-size
php_value[post_max_size] = 25M

; Do not expose to the world that PHP is installed on the server.
; http://php.net/expose-php
php_value[expose_php] = Off

; Name of the session (used as cookie name).
; http://php.net/session.name
php_value[session.name] = X-OPENMEDIAVAULT-SESSIONID

; Default timeout for socket based streams (seconds)
; http://php.net/default-socket-timeout
php_value[default_socket_timeout] = 90

; Maximum execution time of each script, in seconds
; http://php.net/max-execution-time
; Note: This directive is hardcoded to 0 for the CLI SAPI
php_value[max_execution_time] = 90
</code></pre>

<p><strong>UPDATE 09/05/2019</strong>
I have now registered the FQDN as phpmyadmin.rock64.test to eliminate any conflicts and this is registered in my OpenWrt router's DNS. I can ping the domain name and I get reply back from the statically assigned IP address (192.168.1.123) I have linked to the LAN hostname. For other testing purposes I have assigned port 8080 but neither the FQDN or accessing via <a href=""http://192.168.1.123:8080"" rel=""nofollow noreferrer"">http://192.168.1.123:8080</a> works and still results in a bad gateway 502.</p>

<pre><code>server {
    listen 8080;
    root /var/www/phpmyadmin;
    index index.php index.html index.htm;
    server_name phpmyadmin.rock64.test;
    server_tokens off;
    location ~ \.php$ {
        try_files $uri +404;
#       include snippets/fastcgi-php.conf;
        include /etc/nginx/fastcgi.conf;
        include /etc/nginx/fastcgi_params;
#       fastcgi_pass unix:/var/run/php/php7-0.fpm.sock;
        fastcgi_index index.php;
        fastcgi_pass 127.0.0.1:9000;
    }
    location ~ /\. {
        access_log off;
        log_not_found off;
        deny all;

    }
}
</code></pre>
","<nginx><mysql><subdomain><phpmyadmin><nextcloud>","2019-05-02 21:23:32"
"965681","LVM resize / full system encrypt (reinstall no possible)","<p>Seen few questions on this, but the general answer was just reinstall.
Although that is the best and easiest solution, this unfortunately is not possible in my situation.</p>

<p>I was thinking of the following solution.</p>

<p>current partition layout</p>

<pre><code>/dev/vda1      linux/boot  250mb
/dev/vda2      extended    15GiB
  |_/dev/vda5  Linux LVM   15GiB
</code></pre>

<p>LVM has 2 LVs 13GiB /root and 2GiB /swap</p>

<p>it's a clean minimum vps install with a &lt;1gb footprint.
So my idea is to</p>

<ol>
<li>remove the swap drive</li>
<li>shrink LVM partition</li>
<li>create a new temp 2gb partition</li>
<li>move root to temp partition</li>
<li>remove LVM, set up new LVM on LUKS partition.</li>
<li>move temp root to the new LVM on LUKS</li>
<li>remove temp root, Resize LUKS/LVM partition back to 15gb to set up 2GB swap again.</li>
</ol>

<p>Any reason this would not work?
and if not.. I'm stuck on step 2/3</p>

<p>after removing swap lv I resized PV /dev/vda5 to 13GiB
pvdisplay shows 13GiB correctly.</p>

<p>However when I go to cfdisk to create a new temp root partition, it still shows /dev/vda5 as 15GiB</p>

<p>I have the feeling I'm missing a step here to get that 2GiB from the removed swap drive into unpartitioned space?</p>
","<debian><lvm><luks>","2019-05-03 02:09:15"
"751100","SSL proxy nginx cannot reach server by domain name","<p>I am having an odd issue.
I configured a SSL proxy with Nginx serving a website in HTTP which is on 127.0.0.1:1195 (example.com is in my hosts pointing to 127.0.0.1).
For some reason :</p>

<ul>
<li><code>https://example.com:8765</code> doesn't work and gives ERR_NAME_NOT_RESOLVED</li>
<li>while <code>https://www.example.com:8765</code> works</li>
<li>and <code>https://ipaddress:8765</code> works as well.</li>
</ul>

<p>What did I do wrong?
Here is my nginx configuration </p>

<pre><code>  server {
    listen 8765 ssl;
    server_name example.com;

    error_log /var/log/nginx/service.log;

    ssl on;
    ssl_certificate /etc/nginx/certificate.crt;
    ssl_certificate_key /etc/nginx/certificate.key;
    ssl_dhparam /etc/nginx/dhparams.pem;

    ssl_session_timeout 5m;
    ssl_protocols TLSv1 TLSv1.1 TLSv1.2;
    ssl_ciphers 'EECDH+AESGCM:EDH+AESGCM:AES256+EECDH:AES256+$
    ssl_prefer_server_ciphers on;
    ssl_session_cache shared:SSL:10m;

    add_header  X-Robots-Tag ""noindex, nofollow, nosnippet, n$

    location / {
            gzip off;
            proxy_set_header X-Forwarded-Ssl on;
            client_max_body_size 50M;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection ""upgrade"";
            proxy_set_header Host $http_host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_for$
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_set_header X-Frame-Options SAMEORIGIN;
            proxy_pass http://example.com:1195;
    }
</code></pre>

<p>}</p>
","<nginx><ssl><reverse-proxy><domain-name>","2016-01-22 16:46:22"
"822153","CentOS and the IBM x3850 M2","<p>Im kinda new to this sort of stuff so its worth asking for some advice on the matter.
I have recently purchased one second hand IBM x3850 M2 however am slightly confused when it comes to installing the OS.
When i install it using the disk it displays the error ""Cannot find default configuration file"" and the internal usb refuses to load any OS i try to use on it and just displays the word 'Error' 
(Im trying to install CentOS 6.8),</p>
","<centos6><ibm>","2016-12-21 23:17:04"
"751187","Email reception directing the domain request to EC2 instance","<p>I'm not able to receive any emails on my GoDaddy hosted domain after changing the DNS settings for redirecting the domain request to an AWS EC2 instance.</p>

<p>Even though it's a common scenario, I would like to know is there any chances to redirect only the HTTP &amp; HTTPS request to the ec2 instance and the incoming mail requests to the GoDaddy mail server?</p>
","<domain-name-system><amazon-ec2><amazon-web-services><mx-record><godaddy>","2016-01-23 01:33:28"
"965952","MDT Litetouch.vbs Windows Deployment Wizard doesn't show any Task Sequence","<p>I am trying to work on the MDT LTI deployment of Windows 10 in a lab environment according to MSFT document <a href=""https://docs.microsoft.com/en-us/windows/deployment/windows-10-poc-mdt#refresh-a-computer-with-windows-10"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/windows/deployment/windows-10-poc-mdt#refresh-a-computer-with-windows-10</a></p>

<p>Everything else worked fine so far, but now I am stuck at this stage where I am trying to deploy Windows 10 on PC1 that's a Windows 7 machine.</p>

<p>When I run the below scripts;</p>

<p>cscript \\SRV1\MDTProd$\Scripts\Litetouch.vbs</p>

<p>The MDT Starts fine and gets to the Task Sequence selection screen, but there's not task sequence visible that I can select to go to the next step. So I am stuck at this point.</p>

<p>[UPDATE] Ran the command in debug mode and captured debug data.</p>

<p>cscript \\SRV1\MDTProd$\Scripts\litetouch.vbs with /debug:true /debugcapture</p>

<p>Looking at the BDD.log towards the end, I see this message:</p>

<p>[Not Capable of running Platform: x64   W10-X64-001]</p>

<p>Here are 7 last lines of the bdd.log file:</p>

<p>





</p>

<p>Any help would be appreciated.</p>

<p>Thanks in advance.</p>
","<windows-10><mdt>","2019-05-05 14:58:23"
"751400","How to see raid array in Windows Server 2012?","<p>I have just setup Windows 2012 Server Standard on an HP DL380 Gen 9. I have a raid 1 array with 2 drives and a raid 5 array with 4 drives. The drives are all 600GB SAS. This Server 2012 installation will be running Hyper-V and then 2 VMs will be installed, both running Server 2012.</p>

<p>In the server software these raid arrays are setup fine and recognised. I installed the OS successfully to the raid 1 array. </p>

<p>My question is, when I boot to the Server 2012 environment I can see the raid 5 array in disk management but I can't access it with a right click to define a volume. What is the best method to set this up? I'd like to install my VMs on there. Here's a screenshot:</p>

<p><a href=""https://i.sstatic.net/essMk.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/essMk.jpg"" alt=""Raid array not visible""></a></p>

<p>I understand that I also need to define it as a SAN for the VMs later on too, maybe this needs doing now, if so how?</p>
","<windows-server-2012><hyper-v>","2016-01-24 18:35:14"
"889648","Restricting su access","<p>I am using rhel7.2 in our envirement.I do not want any user on system to ""su"" to 2 specific users say ""user3 and user4"".But ""user3 and user4"" can ""su"" to any user in the system.</p>

<p>Let me explain my requirement in detail.Lets say i have 4 users on servers say user1,user2,user3 and user4.</p>

<p>Say i logged in to server with user 1 or user2.
""
My requirement is that it cannot switch user ""su "" to user"" user 3 and user4"".But if i logged in server using  ""user3"" or ""user4"",I can switch user ""su"" to user 1 or user2.</p>

<p>In a nutshell</p>

<p>Login as User1 on server> su - user3   -------This should not work</p>

<p>Login as User3 on server> su - user1 --------This should work</p>
","<su>","2017-12-26 12:30:08"
"751446","DirectAccess in DMZ - internal firewall rules","<p>I'm looking at deploying DirectAccess in our network but have some concerns over the requirement to have the DirectAccess server be domain joined, particularly because it's going to be in the DMZ. The firewall rules on the external firewall are quite straightforward to me (pretty much just TCP443 as it's going to be NATted so 6to4 and teredo ports are not required) but the internal firewall is less clear.</p>
<p>Has anyone else deployed DA here? Short of opening up all IPv4 and IPv6 traffic from the DA server to the internal network as per Microsoft's recommendation (which we aren't going to get approved), what exactly needs to be opened up? All ports required for AD DS operation, plus whichever services/resources our clients need to access?</p>
<p>I thought about using an RODC in the DMZ, but apparently a read/write DC is required (despite threads suggesting it works in some cases).</p>
","<direct-access>","2016-01-25 02:02:34"
"822394","HAProxy: If I bind to ports 32768-65535, the computer lose access to other servers","<p>I need to configure a HAProxy frontend like this:</p>

<pre><code>frontend web-server
    option forwardfor       except 127.0.0.0/8

    bind :8080
    bind :32768-65535

    default_backend service
</code></pre>

<p>But, that configuration don't let me connect to other servers, internal or external.</p>

<pre><code>$ wget www.google.com
--2016-12-22 23:21:13--  http://www.google.com/
Resolving www.google.com (www.google.com)... 172.217.6.196, 2607:f8b0:4006:804::2004
Connecting to www.google.com (www.google.com)|172.217.6.196|:80... failed: Cannot assign requested address.
Connecting to www.google.com (www.google.com)|2607:f8b0:4006:804::2004|:80... failed: Network is unreachable.
</code></pre>

<p><strong>If I comment the line <code>bind :32768_65535</code> and restart HAProxy, I can connect to other servers again.</strong></p>

<p>I think I'm making HAProxy binds to ports that are necessary to start a connection, and that's the reason why that configuration is causing this problem. </p>

<p><strong>How can I configure HAProxy to listen in those ports, without that connection problem?</strong></p>

<p><strong>EDIT:</strong> </p>

<ul>
<li>HAProxy 1.6 </li>
<li>Ubuntu 16.04 (it's a clean installation)</li>
</ul>
","<haproxy>","2016-12-23 04:50:18"
"822521","Why was apache restart required after updating curl version?","<p>I have a server running PHP using mod_php in Apache. Apache is using the Prefork MPM. I had some SSL errors due to using an old version of CURL.</p>

<p>I ran the command 'sudo yum update curl'.</p>

<p>It successfully updated curl, but I noticed I was still getting the SSL errors intermittently. A simple restart of Apache fixed this.</p>

<p>So my 2 questions are..</p>

<ol>
<li><p>Why was an apache restart required? ie php.ini was not modified (unless yum did this behind the scenes)?</p></li>
<li><p>Why did the SSL errors only happen intermittently?. ie why didn't it just not work until apache was restarted?</p></li>
</ol>
","<apache-2.2><php><web-server><curl>","2016-12-23 19:40:58"
"966145","Azure Portal --> Virtual Machine (Default Directory) page issue?","<p>I am very new to Azure Portal.
I have noticed the following for Subject mentioned page.</p>

<p>Once I open the Virtual machines page</p>

<ol>
<li>The (Assign Tags,Start,Restart, Deleted, Services) are comes to be disable and it should be? as I have not selected any of the VM checkbox.</li>
<li>However once I select any of the checkbox then above mentioned buttons are enabled.</li>
<li>I deselect the checkbox but buttons remains enabled.
  So, behavior in *1 and *3 is different. I was expecting that buttons shall disable again as I have not selected any VM.</li>
</ol>

<p>Moreover, I can delete, assign tags etc, even without selecting the VM.</p>

<p>Sorry if this question is not relevant here and please provide any information if my expectation is wrong as I am very new to Azure.</p>
","<azure>","2019-05-07 08:39:40"
"751714","OpenSolaris/OpenIndiana drive full, cannot boot","<p>While downloading my drive became full and now it is non booting.   </p>

<p>I installed a new drive in the workstation then installed exactly the same OS.</p>

<p>I am now trying to go about a simple way of accessing my full drive and getting my data  which is in the root directory <code>/root/virtualbox</code>.
Since both systems have the same rpool structure, what can I do to mount the secondary drive without conflicts?</p>

<p>I have cloned the drive and am using the clone for this. There is more than one way to go about this looking for suggestions.</p>

<p>I am thinking of cloning the drive to a bigger drive,  maybe stretch the partition from 1tb drive to 2tb drive.</p>

<p>Not sure yet if this is a solution. On a ZFS level, what is a good practice? </p>
","<zfs><opensolaris>","2016-01-26 00:45:40"
"890059","Could not connect to Azure Ubuntu VM using Win10 BASH subsystem","<p>Could not connect using Win10 BASH subsystem to newly created Azure Ubuntu server, the Microsoft technicians were unable to help but it appears that there is some kind of key issue. I used the following doc ""<a href=""https://docs.microsoft.com/en-us/azure/virtual-machines/linux/quick-create-portal"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/virtual-machines/linux/quick-create-portal</a>"" to learn Azure using the portal but they and I, could not get it to connect. </p>

<p>Update 1/2/2017:</p>

<p>Was able to connect using the -i ~/.ssh/name_of_the_keyfile appended to the ssh command. Thank You</p>
","<linux><bash><ubuntu-16.04><azure-networking>","2017-12-29 23:22:27"
"822564","Domain leads to Apache default page instead of the desired page","<p>I have set up a server, and created additional virtual hosts with their document roots being /var/www/example.com But whenever I got to example.com it leads to the default ""its working"" Apache page. </p>

<p>If I go to example.com/example.com/index.html however it displays the code that I would have liked to display by default.</p>

<p>Thank you for your help.</p>

<p>---- In Response To Carlos ----</p>

<p>Here is the code you asked for</p>

<pre><code>        &gt; apache2ctl -S  
AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 127.0.1.1. Set the 'ServerName' directive globally to suppress this message  
VirtualHost configuration:  
*:80                   is a NameVirtualHost  
         default server example.com (/etc/apache2/sites-enabled/000-default.conf:1)  
         port 80 namevhost example.com (/etc/apache2/sites-enabled/000-default.conf:1)  
         port 80 namevhost 2.example.com (/etc/apache2/sites-enabled/2.example.com.conf:1)  
         port 80 namevhost example.com (/etc/apache2/sites-enabled/example.com.conf:1)  
                 alias www.example.com  
ServerRoot: ""/etc/apache2""  
Main DocumentRoot: ""/var/www/html""  
Main ErrorLog: ""/var/log/apache2/error.log""  
Mutex default: dir=""/var/lock/apache2"" mechanism=fcntl   
Mutex mpm-accept: using_defaults  
Mutex watchdog-callback: using_defaults  
PidFile: ""/var/run/apache2/apache2.pid""  
Define: DUMP_VHOSTS  
Define: DUMP_RUN_CFG  
User: name=""www-data"" id=33  
Group: name=""www-data"" id=33   
</code></pre>
","<virtualhost><apache2>","2016-12-24 03:45:48"
"751721","404 header error instead of the correct 200","<p>I have just updated a (dynamic) website by sftp/ssh and all the new pages have been considered as 404 error not found. Actually, all the pages, including the old pages which was overwritten.</p>

<p>Also, the website is using cloudflare.</p>

<p>Let me show you a curl of a url:</p>

<pre><code>curl http://example.com/something/

 HTTP/1.1 404 Not Found Date: Tue, 26 Jan 2016 01:25:08 GMT
 Content-Type: text/html; charset=UTF-8 Transfer-Encoding: chunked
 Connection: keep-alive Set-Cookie:
 __cfduid=de5a53a227295f9d9374fdf39bb45514f1453771507; expires=Wed,25-Jan-17 01:25:07 GMT; path=/; domain=.example.com; HttpOnly
 X-Powered-By: PHP/5.5.29 
 X-Pingback: http://example.com/folder-wp-blog/xmlrpc.php 
 X-UA-Compatible: IE=edge 
 Expires: Wed, 11 Jan 1984 05:00:00 GMT 
 Cache-Control: no-cache, must-revalidate, max-age=0 
 Pragma: no-cache Set-Cookie:
 PHPSESSID=c72b8d6fc18775ef85ac8ab1b8bf6e95; path=/ Server:
 cloudflare-nginx 
 CF-RAY: 26a86c0ee0f10f15-IAD
</code></pre>

<p>As you can see, the curl returns a 404 error. BUT the a URL is live and working like a charm. </p>

<p>The problem is on cloudflare I thought. So, I ""purge everything"" when I curled before but the 404 still exist. I added a custom rule to bypass caching on cloudflare on a spesific url but it didn't work at all. </p>

<p>Also, I thought that was some mistake on my .htaccess and I erase everything from there. I restart the apache but the 404 are still exist.</p>

<p>As a result, every search engine and every other bot see a 404 header.
The problem is everywhere, except on the /index.html and on the wordpress blog which is on a /folder-wp-blog/ folder. This WP wasn't changed at all. So, every other html and php file which is in root is flaged as 404...</p>

<p>Any ideas? solutions?</p>
","<cache><wordpress><curl><http-status-code-404><cloudflare>","2016-01-26 01:37:37"
"822585","Baikal Folder not Found","<p>I tried to install baikal. Works perfectly, configuration, the website perfect. But when I try to get my books I get this error:</p>

<pre><code>2016/12/24 10:21:40 [error] 6440#0: *7 ""/var/www/mail/dav/html/dav.php/addressbooks/index.html"" is not found (20: Not a directory), client: 46.223.1.8, server: dav.mailgermania.de, request: ""GET /dav.php/addressbooks/ HTTP/1.1"", host: ""dav.mailgermania.de"", referrer: ""https://dav.mailgermania.de/dav.php""
</code></pre>

<p>This is my nginx config</p>

<pre><code>server {
  listen 80;
  listen [::]:80;
  server_name dav.mailgermania.de
  server_tokens off;
  root /var/www/mail/dav/html;
  return 301 https://$host$request_uri;
}
server {
  listen 443;
  listen [::]:443;
  server_name dav.mailgermania.de
  server_tokens off;
  ssl on;
  ssl_certificate /etc/ssl/mail/mail.crt;
  ssl_certificate_key /etc/ssl/mail/mail.key;
  ssl_prefer_server_ciphers on;
  ssl_protocols TLSv1 TLSv1.1 TLSv1.2;
  ssl_ciphers 'ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-  GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA256:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA:DHE-RSA-AES256-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:AES:CAMELLIA:DES-CBC3-SHA:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!PSK:!aECDH:!EDH-DSS-DES-CBC3-SHA:!EDH-RSA-DES-CBC3-SHA:!KRB5-DES-CBC3-SHA';
  ssl_dhparam /etc/ssl/mail/dhparams.pem;
  add_header Strict-Transport-Security max-age=15768000;
  ssl_session_timeout 30m;
  client_max_body_size 25m;
  root /var/www/mail/dav/html;
  index index.html index.htm index.php;

  add_header X-Frame-Options ""SAMEORIGIN"";
  add_header Content-Security-Policy ""frame-ancestors 'self';"";
  add_header X-Content-Type-Options nosniff;
  add_header X-XSS-Protection ""1; mode=block"";

  rewrite ^/.well-known/caldav /dav.php redirect;
  rewrite ^/.well-known/carddav /dav.php redirect;

  charset utf-8;

  location ~ /(.ht|Core|Specific) {
    deny all;
    return 404;
  }

  location ~ .php$ {
    include fastcgi_params;
    fastcgi_split_path_info ^(.+.php)(/.+)$;
    fastcgi_pass unix:/var/run/php-mail-fpm.sock;
    fastcgi_index index.php;
    fastcgi_param HTTPS on;
    fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
    fastcgi_read_timeout 630;
    fastcgi_keep_conn on;
  }
}
</code></pre>

<p>I do not know what this error means, maybe you could help me?</p>

<p>Thanks for help</p>

<p><strong>Update</strong>
I changed my php block like this:</p>

<pre><code>  location ~ ^(.+\.php)(.*)$ {
try_files $fastcgi_script_name =404;
include        /etc/nginx/fastcgi_params;
fastcgi_split_path_info  ^(.+\.php)(.*)$;
fastcgi_pass   unix:/var/run/php5-fpm.sock;
fastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;
fastcgi_param  PATH_INFO        $fastcgi_path_info;
fastcgi_index index.php;
fastcgi_param HTTPS on; 
fastcgi_read_timeout 630;
fastcgi_keep_conn on; 
}
</code></pre>

<p>Now it works. Thanks.</p>
","<nginx><php>","2016-12-24 09:30:00"
"822608","What modules can be enable/disable in a default Apache 2.4 setup?","<p>I am setting up a new CentOS 7 server with Apache 2.4 and I am planning to add PHP-FPM trough mod_proxy_fcgi (as per this post <a href=""https://serverfault.com/a/629964/124850"">here</a>). I don't think all that's enabled by default is needed in this case but sadly I am not an Apache expert so I need some advice from gurus over here. This is what's enabled by default:</p>

<pre><code>LoadModule access_compat_module modules/mod_access_compat.so
LoadModule actions_module modules/mod_actions.so
LoadModule alias_module modules/mod_alias.so
LoadModule allowmethods_module modules/mod_allowmethods.so
LoadModule auth_basic_module modules/mod_auth_basic.so
LoadModule auth_digest_module modules/mod_auth_digest.so
LoadModule authn_anon_module modules/mod_authn_anon.so
LoadModule authn_core_module modules/mod_authn_core.so
LoadModule authn_dbd_module modules/mod_authn_dbd.so
LoadModule authn_dbm_module modules/mod_authn_dbm.so
LoadModule authn_file_module modules/mod_authn_file.so
LoadModule authn_socache_module modules/mod_authn_socache.so
LoadModule authz_core_module modules/mod_authz_core.so
LoadModule authz_dbd_module modules/mod_authz_dbd.so
LoadModule authz_dbm_module modules/mod_authz_dbm.so
LoadModule authz_groupfile_module modules/mod_authz_groupfile.so
LoadModule authz_host_module modules/mod_authz_host.so
LoadModule authz_owner_module modules/mod_authz_owner.so
LoadModule authz_user_module modules/mod_authz_user.so
LoadModule autoindex_module modules/mod_autoindex.so
LoadModule cache_module modules/mod_cache.so
LoadModule cache_disk_module modules/mod_cache_disk.so
LoadModule data_module modules/mod_data.so
LoadModule dbd_module modules/mod_dbd.so
LoadModule deflate_module modules/mod_deflate.so
LoadModule dir_module modules/mod_dir.so
LoadModule dumpio_module modules/mod_dumpio.so
LoadModule echo_module modules/mod_echo.so
LoadModule env_module modules/mod_env.so
LoadModule expires_module modules/mod_expires.so
LoadModule ext_filter_module modules/mod_ext_filter.so
LoadModule filter_module modules/mod_filter.so
LoadModule headers_module modules/mod_headers.so
LoadModule include_module modules/mod_include.so
LoadModule info_module modules/mod_info.so
LoadModule log_config_module modules/mod_log_config.so
LoadModule logio_module modules/mod_logio.so
LoadModule mime_magic_module modules/mod_mime_magic.so
LoadModule mime_module modules/mod_mime.so
LoadModule negotiation_module modules/mod_negotiation.so
LoadModule remoteip_module modules/mod_remoteip.so
LoadModule reqtimeout_module modules/mod_reqtimeout.so
LoadModule rewrite_module modules/mod_rewrite.so
LoadModule setenvif_module modules/mod_setenvif.so
LoadModule slotmem_plain_module modules/mod_slotmem_plain.so
LoadModule slotmem_shm_module modules/mod_slotmem_shm.so
LoadModule socache_dbm_module modules/mod_socache_dbm.so
LoadModule socache_memcache_module modules/mod_socache_memcache.so
LoadModule socache_shmcb_module modules/mod_socache_shmcb.so
LoadModule status_module modules/mod_status.so
LoadModule substitute_module modules/mod_substitute.so
LoadModule suexec_module modules/mod_suexec.so
LoadModule unique_id_module modules/mod_unique_id.so
LoadModule unixd_module modules/mod_unixd.so
LoadModule userdir_module modules/mod_userdir.so
LoadModule version_module modules/mod_version.so
LoadModule vhost_alias_module modules/mod_vhost_alias.so
</code></pre>

<p>Also I am seeing <code>dav</code>, <code>lua</code>, <code>mpm</code> <code>proxy</code>, <code>systemd</code> and <code>cgi</code> in <code>/etc/httpd/conf.modules.d</code> so I am assuming those ones are loaded by default as well.</p>

<p>What to enable/disable and keep Apache running with PHP-FPM?</p>
","<apache-2.4><centos7><httpd><httpd.conf>","2016-12-24 16:38:08"
"890199","After OS update PHP function file_get_contents() cause SSL error","<p>After my regular updates and a reboot of my server I accessed a web page on it and got the following errors:</p>

<pre><code>Warning: file_get_contents(): SSL operation failed with code 1. OpenSSL Error messages: 
error:14090086:SSL routines:ssl3_get_server_certificate:certificate verify failed in 
/var/www/domain.ca/public_html/page.php on line 51

Warning: file_get_contents(): Failed to enable crypto in /var/www/domain.ca/public_html/
page.php on line 51

Warning: file_get_contents(https://www.cryptopia.co.nz/api/GetMarkets/BTC): failed to open 
stream: operation failed in /var/www/domain.ca/public_html/page.php on line 51
</code></pre>

<p>After some more testing, my other file_get_contents() works well. The only difference I saw with cryptopia is that they use a Comodo certificate. So I guess it's related to the issue but I didn't find a solution for this.</p>

<p>OS is Ubuntu Server 16.04.3 and PHP is version 7.0.22.</p>

<p>Any help is welcome!</p>
","<php><ssl-certificate><openssl><apache2>","2017-12-31 16:07:05"
"890200","P2V causes PPTP to disappear","<p>I have recently completed a P2V of a Win2008R2 server.</p>

<p>It had PPTP server vpn capability before conversion, but it appears something is now blocking / preventing the vpn connection.</p>

<p>If I try to connect to the VPN with a local virtualized windows client (on same ESXi server), it connects.</p>

<p>However clients that are not virtualized on the ESXi server cannot connect. After ""Verifying user name and password"" the connection times out with an Error 619. I can see the VPN port on the server shows ""Authenticating""</p>

<p>All other file and print and security services work fine on this server. L2TP connections work on the server. (and yes I know L2TP is better than PPTP)</p>

<p>I saw a similar error posted about ESXi v5.5, where protocol 47 was being blocked, but nothing on v6.5.</p>

<p>I've subsequently built a new Win2K8R2 server in ESXi and also in Oracle VM using the exact same steps. The PPTP service works fine on Oracle, but again fails on ESXi.</p>

<p>Further process of elimination:  It doesn't appear to be the ESXi firewall.  Set default action to PASS and disabled firewall with no difference.</p>

<p>(via SSH ran following commands)</p>

<pre><code>esxcli network firewall set -e false
esxcli network firewall set -d true
esxcli network firewall get
   Default Action: PASS
   Enabled: false
   Loaded: true
</code></pre>

<p>Any thoughts o wise ones?</p>
","<vpn><vmware-esxi><pptp>","2017-12-31 16:11:52"
"822715","How to access my ftp sever over the internet?","<p>I have Windows Server 2016 running on an Azure VM, and I'm trying to get ftp running on it.
FTP role is installed, ports 21 and 20 are opened in the firewall and in Azure portal.
And still port 20 doesn't seem to be open, since the port scanner times out. My Firewall log doesn't show the attempt, sp I guess it's blocked in AZure somewhere, but my port 20 is definitely opened (inbound. Do I have to do anything outbound?).
I also opened ports 7000-7014 in Azure (Windows Firewall has a predefined FTS passive data rule with 1024-65535, so I didn't add anything here), and put those in the ""FTP Firewall support"" item of the IIS and put in the external IP of my VM.</p>

<p>When going <a href=""ftp://localhost"" rel=""nofollow noreferrer"">ftp://localhost</a> from within my VM, I login with the user I authorized in the IIS, and it works how it should.
When accessing it over dns name over the internet, I tried to login with my credentials, and I get a timeout. i tried username, \username, myPC\username, |username, and they all do the same. THe connection does work over the internet via cli with ftp myDNSname -> |username -> password.</p>

<p>Edit: I did search the internet, and then stackexchange, but didn't find anything that might explain what's going wrong here. So I turned to you, hoping you would help me. </p>

<p><strong>Edit</strong>: Pictures to show the current state.</p>

<p>Another edit: Log from FileZilla:
PWD
257 ""/"" is current directory.
TYPE I
200 Type set to I.
PASV
227 Entering Passive Mode (13,95,157,108,225,30).
LIST
Die Datenverbindung konnte nicht hergestellt werden: ENETUNREACH - Netzwerk nicht erreichbar // It says network unreachable
150 Opening BINARY mode data connection.
// After this comes a timeout</p>

<p><a href=""https://i.sstatic.net/woBVU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/woBVU.png"" alt=""Azure inbound rules""></a></p>

<p><a href=""https://i.sstatic.net/oXNSb.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oXNSb.png"" alt=""Firewall inbound rules""></a></p>

<p><a href=""https://i.sstatic.net/qjWa6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qjWa6.png"" alt=""FTP site bindings""></a></p>
","<firewall><ftp><azure><windows-server-2016>","2016-12-26 10:04:09"
"822717","Routing two network cards","<p>I got one server with two network cards. </p>

<p>NIC1 is 192.168.1.14 and can communicate with internal computers. 
NIC2 is 192.168.2.6 and can communicate with router and Internet. </p>

<p>From internal computers I can ping 192.168.1.14 and 192.168.2.6, but not ping 8.8.8.8 or any Internet site. Any idea? </p>

<p>Edit: I'm using CentOS 7, my default gateway is 192.168.2.2, and I stopped iptables but was unsucessful. 
Internal computers point destination 0.0.0.0 to gateway 192.168.1.14 </p>
","<networking><linux-networking><centos7>","2016-12-26 10:46:28"
"890279","Systematically add fragment to URLs","<p>I have a <code>mean-stack</code> website with <code>html5mode</code> enabled. In <code>index.html</code>, I have <code>&lt;base href=""/1/"" /&gt;</code>. And I have the following nginx setting such that <code>www.myweb.com/home</code> becomes automatically <code>www.myweb.com/1/home</code> (<code>www.myweb.com/js/abc.js</code> remains <code>www.myweb.com/js/abc.js</code>):</p>

<pre><code>location ~* ^/\b(?!1|stylesheets|js)\w+\b/? {
    rewrite .* /1$request_uri redirect;
}
</code></pre>

<p>Now, I have to add a special library, which unfortunately disables <code>html5mode</code>. So I am going to give up <code>html5mode</code> for the whole websites.</p>

<p>So now, I expect a rewrite rule such that</p>

<pre><code>www.myweb.com/home ==&gt; www.myweb.com/1/#/home
www.myweb.com/js/controller.js ==&gt; www.myweb.com/js/controller.js
www.myweb.com/1/abc/def ==&gt; www.myweb.com/1/#/abc/def
www.myweb.com/1/#/abc/def ==&gt; www.myweb.com/1/#/abc/def
</code></pre>

<p>Does anyone know how to modify the above rewriting to enable this? Because my website is already online. I want to make sure the rules before changing the production...</p>

<p><strong>Edit 1:</strong> I guess if I write </p>

<pre><code>location ~* ^/\b(?!1|stylesheets|js)\w+\b/? {
    rewrite .* /1/#$request_uri redirect;
}
</code></pre>

<p>That will result in</p>

<pre><code>www.myweb.com/home ==&gt; www.myweb.com/1/#/home (correct)
www.myweb.com/js/controller.js ==&gt; www.myweb.com/js/controller.js (correct)
www.myweb.com/1/abc/def ==&gt; www.myweb.com/1/abc/def (wrong)
www.myweb.com/1/#/abc/def ==&gt; www.myweb.com/1/#/abc/def (correct)
</code></pre>
","<nginx><rewrite><url><html5>","2018-01-01 20:23:27"
"822753","Database Index files get corrupted if shared folder resides in Win Server 2012 but not if it resides in a regula Win 7 PC Shared folder","<p>Database application programmed for peer to peer access to a shared folder experiences database associated index files corruption if said shared folder resides in Win Server 2008 or Win Server 2012.  If same application share folder resides on a regular Win 7 PC the application runs fine with no Database Index files corruption.  Database tables are currently accessed by 10 Win 7 64bit computers concurrently.  </p>
","<database><corruption>","2016-12-26 15:59:01"
"966301","Nginx letsencrypt certbot ssl page will not load over https but will load over http","<p>I am trying to set up a digital ocean server using this tutorial <a href=""https://www.digitalocean.com/community/tutorials/how-to-set-up-django-with-postgres-nginx-and-gunicorn-on-ubuntu-16-04"" rel=""nofollow noreferrer"">https://www.digitalocean.com/community/tutorials/how-to-set-up-django-with-postgres-nginx-and-gunicorn-on-ubuntu-16-04</a></p>

<p>The site is served just fine over HTTP but will not load from https.
(will not load references this chrome error)</p>

<pre><code>This site can’t be reached kronoswebsolutions.com took too long to respond.
Try:

Checking the connection
Checking the proxy and the firewall
ERR_CONNECTION_TIMED_OUT
</code></pre>

<p>As advised I did check the firewall previous to posting but here is the output of my ufw. I do not have a firewall enabled on my digital ocean droplet web gui.</p>

<pre><code>Nginx Full                 ALLOW       Anywhere                  
22                         ALLOW       *************             
22                         ALLOW       *************             
OpenSSH                    ALLOW       Anywhere                  
Nginx Full (v6)            ALLOW       Anywhere (v6)             
OpenSSH (v6)               ALLOW       Anywhere (v6)     
</code></pre>

<pre><code>Here is the results of nmap from my local machine using the ip of my server...

nmap -Pn -p 443 IPADDRESS
Starting Nmap 7.70 ( https://nmap.org ) at 2019-05-07 23:46 MDT
Nmap scan report for IPADDRESS
Host is up.

PORT    STATE    SERVICE
443/tcp filtered https
</code></pre>

<p>I am using Debian stretch rather than Ubuntu.</p>
","<networking><nginx><ssl><firewall><debian-stretch>","2019-05-08 02:43:04"
"822881","IIS Clusters within a Cluster","<p><strong>Updated to reflect John Mahowald's input</strong></p>

<p>After doing considerable research on the subject and exploring other options for our system, a cluster of clusters is not a good idea for the reasons that Mahowald gave and the following:</p>

<ol>
<li>Our system uses a custom database (not SQL), which can be easily partitioned to fit on a server pair for redundancy.</li>
<li>Client-side load balancing can be used to distribute the load across each pair of servers.</li>
<li>The only part of our system that actually needs the IIS NLB cluster is the dedicated web application servers.</li>
<li>The active-active approach (suggested by Mahowald) for the pair of data servers works for our system. The two servers running in parallel provide lower latency/better performance when both are up and redundancy when one server in a pair fails.</li>
</ol>

<hr>

<p>We would like to have a cluster of two IIS servers as a building block for a larger cluster of 10 such building blocks (in other words, a cluster of clusters using 20 servers).</p>

<p>Each server would have a unique IP Address and a shared IP address for its cluster. Each of these server modules (two servers in a cluster) is responsible for its own data and to serve as a download source for the web application.</p>

<p>The leaf-node server pairs must be collocated and data synchronized using a wake-up timer created with ASP.NET auto-start functionality. This synchronization is done via the second NIC, which is only used for intranet communication. A JavaScript client must be able to access one of these server pairs to read/write data to the two server cluster. It will synchronize any data that changes. The latency of updates between servers is already taken into account in our system. No data is synchronized across the server pairs.</p>

<p><strong>Edited to add more details</strong>
The intent is to have a server pair (leaf-node cluster) that provides redundancy (for its unique dataset) and increased throughput when both servers are available. In the event of a failure, one server would carry the load, at a reduced capacity, until the other one is repaired. </p>

<p>The top-level cluster would handle normal application download functionality, which includes failover redundancy. It would provide NLB for the 10 cluster pairs using the lower-level cluster IP addresses.  The top-level cluster would be effectively managing 10 unique IP nodes, where each IP node would be a leaf-node cluster.</p>

<p>I have not been able to ascertain whether Windows 2012 IIS handles this scenario after doing some searching on the web. The information that I have read does not seem to preclude this scenario. </p>

<p>Is this type of configuration supported? Resource links would be greatly appreciated.</p>

<p>Regards…</p>
","<iis><windows-cluster>","2016-12-27 15:02:18"
"752071","Unable to connect to a port from a remote machine in Centos?","<p>I am not able to connect to <code>9423</code> port of a machine from another machine though the local <code>telnet localhost 9423</code> works. Here is my <code>netstat -putna</code>. Is there anything suspicious? <code>10000</code> works just fine though.</p>

<pre><code>[root@localhost etc]# netstat -putna
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address               Foreign Address             State       PID/Program name   
tcp        0      0 0.0.0.0:5480                0.0.0.0:*                   LISTEN      3285/vami-lighttpd  
tcp        0      0 0.0.0.0:10000               0.0.0.0:*                   LISTEN      3475/perl           
tcp        0      0 0.0.0.0:22                  0.0.0.0:*                   LISTEN      3390/sshd           
tcp        0    300 192.168.5.80:22             172.29.14.11:42646          ESTABLISHED 5835/sshd           
tcp        0      0 ::ffff:127.0.0.1:9423       :::*                        LISTEN      6475/java           
tcp        0      0 :::80                       :::*                        LISTEN      3435/java           
tcp        0      0 :::5488                     :::*                        LISTEN      3183/vami-sfcbd     
tcp        0      0 :::5489                     :::*                        LISTEN      3183/vami-sfcbd     
tcp        0      0 :::22                       :::*                        LISTEN      3390/sshd           
tcp        0      0 ::ffff:192.168.5.80:80      ::ffff:172.29.14.11:43242   FIN_WAIT2   3435/java           
tcp        0      0 ::ffff:192.168.5.80:80      ::ffff:172.29.14.11:40142   ESTABLISHED 3435/java           
tcp        0      0 ::ffff:192.168.5.80:80      ::ffff:172.29.14.11:40402   ESTABLISHED 3435/java           
tcp        0      0 ::ffff:192.168.5.80:80      ::ffff:172.29.14.11:39538   ESTABLISHED 3435/java           
udp        0      0 0.0.0.0:10000               0.0.0.0:*                               3475/perl   
</code></pre>
","<networking><iptables><port>","2016-01-27 11:05:09"
"890501","Can a non-authoritative name server give any response it likes to?","<p>Our domain registrar allows us to either make use of our own DNS servers or to use theirs (configuring DNS entries in their web interface); I suppose this is a common situation.</p>

<p>We have opted to set up our own DNS servers. Now, the registrar's DNS servers (which are no longer authoritative) return a response containing, among other entries, an <code>A</code> record pointing to a ""getting started"" webpage for the web hosting service they offer. So in fact their DNS server returns incorrect information.</p>

<h2>Example DNS lookups and answers</h2>

<p>To hopefully make this easier to follow, assume:</p>

<ul>
<li>I've registered <code>example.com</code> at MyRegistrar.com</li>
<li>I've set up <code>ns1.mydomain.com</code> as the (authoritative) name server for <code>example.com</code></li>
<li><code>ns1.myregistrar.com</code> is one of MyRegistrar.com's name servers</li>
<li>I'm hosting the site for <code>example.com</code> on a server with address <code>1.2.3.4</code></li>
<li>A ""getting started"" landing page for MyRegistrar.com is served from <code>9.8.7.6</code></li>
</ul>

<p>The expected, valid response for <code>dig -t any example.com</code> is then (abridged):</p>

<pre><code>;; ANSWER SECTION:
example.com. 99999  IN  A   1.2.3.4.

;; AUTHORITY SECTION:
example.com. 9999   IN  NS  ns1.mydomain.com.

;; ADDITIONAL SECTION:
ns1.mydomain.com.   99999   IN  A   1.2.3.1.
</code></pre>

<p>However, when querying MyRegistrar.com using <code>dig -t any @ns1.myregistrar.com example.com</code>, I'm getting the following invalid response:</p>

<pre><code>;; ANSWER SECTION:
example.com. 99999  IN  SOA ns1.myregistrar.com. 1 14400 3600 604800 3600
example.com. 9999   IN  A   9.8.7.6.
example.com. 99999  IN  NS  ns1.myregistrar.com.

;; ADDITIONAL SECTION:
ns1.myregistrar.com.    600 IN  A   9.8.7.1.
</code></pre>

<p>When querying specifically for <code>MX</code> records with <code>dig -t mx @ns1.myregistrar.com example.com</code>, I even get an authority section in the response (the answer section is empty as there are no <code>MX</code> records):</p>

<pre><code>;; QUESTION SECTION:
;example.com.   IN  MX

;; AUTHORITY SECTION:
example.com. 3600   IN  SOA ns1.myregistrar.com. 1 14400 3600 604800 3600
</code></pre>

<h2>Wrapping up...</h2>

<p>This is of course no problem in practice, because normal DNS queries won't be directed at <code>ns1.myregistrar.com</code>.  But just as a matter of interest, is MyRegistrar.com's name server operating in accordance with RFCs?</p>

<p>I've browsed documentation and RFCs for what information DNS servers are required / allowed to include in their responses (and in particular if they are allowed to completely deviate from the authoritative response), but haven't been able to find anything relevant. Any pointers would be greatly appreciated.</p>
","<domain-name-system><domain-registrar><rfc><authoritative>","2018-01-03 11:23:32"
"752094","sed + search word with strings and replaced all word","<p>I want to create with sed the following:</p>

<p>for example each word in the file that have the ""ssss...""
Should be replaced (all word) with target string as gggg</p>

<pre><code> echo ""duwdbnhb ssssssmnfkejfnei"" | sed s'/ssssss*/gggg/g'

 duwdbnhb ggggmnfkejfnei
</code></pre>

<p>should be:</p>

<pre><code> duwdbnhb gggg
</code></pre>

<p>remark - string could be with couple of ""s"" strings ( for example ss or sss or ssssss )</p>

<p>example:</p>

<pre><code> echo ""duwdbnhb sssmnfkejfnei"" | sed s'/s*/gggg/g'

 duwdbnhb gggg



 echo ""duwdbnhb sssmnfdej3434bjhhji"" | sed s'/s*/gggg/g'

 duwdbnhb gggg
</code></pre>
","<linux><sed>","2016-01-27 11:56:25"
"822986","Unable to login when the primary DC is down","<p>In my environment, i got 2 DCs Windows Server 2012. But when the primary is down i cant seams to login or find the path in the network to access another PC.</p>

<p>I have checked and all the DCs has GC and when i change the DNS in the PC from primary to secondary then it works, it just the PCs are not going to the second when the primary is down which i have set all the PCs with a secondary DNS and i am using static IPs for all PCs and servers.</p>
","<active-directory><windows-server-2012><internal-dns>","2016-12-28 10:43:19"
"890646","How to increase space of temp in centos , As its 100% used","<p>I have a server with Centos on and when I print <code>df -h</code> I get this output <a href=""https://i.sstatic.net/cduKC.jpg"" rel=""nofollow noreferrer"">OutPut of df -h</a> the issue is that the temp folder is 100% used and I need to add more space to temp folder. I was wondering can any one help me as I have less experience with Linux and I have also tried codes like <code>lvresize</code> but it says insufficient space.</p>

<pre><code>[root@ns1 ~]# df -h
Filesystem            Size  Used Avail Use% Mounted on
/dev/mapper/VolGroup0-root
                  908G   13G  849G   2% /
tmpfs                 7.8G     0  7.8G   0% /dev/shm
/dev/sda1             243M   72M  158M  32% /boot
/dev/mapper/VolGroup0-tmp
                  976M  960M     0 100% /tmp
[root@ns1 ~]#
</code></pre>

<p>Output of <code>ls -l</code> is </p>

<pre><code>-rw-------  1 root root     0 Jan  2 21:49 3UocP8
-rw-r--r--  1 root root     0 Jan  3 21:57 e.mysql
drwx------. 2 root root 16384 Sep 11 03:03 lost+found
-rw-------  1 root root     0 Jan  4 02:10 sess_02lt8blpha2hjd9fvjoimvp543
-rw-------  1 root root     0 Jan  3 06:15 sess_049tsvhh6dgs554b9u2ra3av32
-rw-------  1 root root     0 Jan  4 03:35 sess_070dnkje4lsk92abdgcngo24c4
-rw-------  1 root root     0 Jan  3 04:55 sess_0emeqaakt9153e87p8rsjqqcn2
-rw-------  1 root root     0 Jan  3 05:45 sess_0g7sboh1c24vcqcaguu2h6gjq0
-rw-------  1 root root     0 Jan  3 22:40 sess_0jcmmfbs3qotopfb1nobb7m5d3
-rw-------  1 root root     0 Jan  3 05:05 sess_0kf4tk9d5uod4fin73jheqa0r0
-rw-------  1 root root     0 Jan  3 05:55 sess_0kuvoi5rs5i3f1dtbi0k3gqu91
-rw-------  1 root root     0 Jan  3 04:25 sess_0pu6bsi95erqokftnroh6umar4
-rw-------  1 root root     0 Jan  3 22:35 sess_0tlh93o5nqm5hit7k58laojrp7
-rw-------  1 root root     0 Jan  3 05:35 sess_12emje5ru2ndse53ke74k28u01
-rw-------  1 root root     0 Jan  4 03:10 sess_157gr3hbfdb77mjneja83n3m47
-rw-------  1 root root     0 Jan  3 07:05 sess_1880cq4gesliitso5t15ked254
-rw-------  1 root root     0 Jan  3 07:05 sess_1cn44isoa9cmdeg6b8ak1k62e5
-rw-------  1 root root     0 Jan  3 05:25 sess_1h1ksp9u19c2u98mqfj30d8212
-rw-------  1 root root     0 Jan  3 23:55 sess_1h26v8r9afe1gtud3jlhf51g43
-rw-------  1 root root     0 Jan  3 07:40 sess_1ja7dalbkpjv9bke8l9slnv746
-rw-------  1 root root     0 Jan  4 05:45 sess_1qc2cfmnli5etkgnqbkdufs631
-rw-------  1 root root     0 Jan  3 06:50 sess_1qq5ihlvsghndeubtpn61ba622
-rw-------  1 root root    37 Jan  3 03:40 sess_1s6nbtrd51cqhp0l0orbcmfu35
-rw-------  1 root root     0 Jan  4 01:25 sess_1spaflggra7ec8hcvvkk15g7c2
-rw-------  1 root root     0 Jan  4 05:20 sess_22js4evme87t2es41fppum6eh3
-rw-------  1 root root     0 Jan  4 01:50 sess_25rgiqm98a61eid67i7upccs94
-rw-------  1 root root     0 Jan  3 04:50 sess_2a2rqqvts3j992u3f147ljk9h0
-rw-------  1 root root    37 Jan  3 21:59 sess_2cj34idrrn21bmvc40v9evm210
-rw-------  1 root root     0 Jan  4 00:50 sess_2dsjs0fs6slcrfhip9vchmnth5
-rw-------  1 root root    37 Jan  3 04:00 sess_2loeqdefrfh3qonjui6a7hldq3
-rw-------  1 root root     0 Jan  3 04:55 sess_2p1nvm52c6i4g3o849ukn5t434
-rw-------  1 root root     0 Jan  3 06:50 sess_2tebauu4qg7kdt0klv0f638eh0
-rw-------  1 root root     0 Jan  3 22:25 sess_335cqui2ja89lbad7nnlulsqa6
-rw-------  1 root root     0 Jan  4 02:25 sess_39s38o2v79qjp8ig86qjjkibi3
-rw-------  1 root root     0 Jan  4 01:35 sess_3m4a23ois3amfh4hupmpv0ijf4
-rw-------  1 root root     0 Jan  3 23:25 sess_3mb0s6gajerp3k9jsb0ot5q5b2
-rw-------  1 root root     0 Jan  4 04:10 sess_3vlq4716kpji41ov740i95ph07
-rw-------  1 root root     0 Jan  3 06:55 sess_48gv9crkcv89k1dam4ke5r0k20
-rw-------  1 root root     0 Jan  4 04:00 sess_48hcjoa08p2hrtltljc5rrsvd0
-rw-------  1 root root     0 Jan  4 04:30 sess_4aa5vqslsnq9pu07v53hja5e81
-rw-------  1 root root     0 Jan  3 04:30 sess_4j8tdcrb8dpeq37ssp45j0pl27
-rw-------  1 root root     0 Jan  3 04:20 sess_4jhfgds7h2jio3dsfsavqure11
-rw-------  1 root root     0 Jan  3 07:10 sess_4knainbs51meijfq22e04nagm1
-rw-------  1 root root     0 Jan  3 06:20 sess_4mfknqf1qlommjqho08mcodfq3
-rw-------  1 root root     0 Jan  4 03:45 sess_4p5354gvt6e781gemphtlku0i2
-rw-------  1 root root     0 Jan  3 06:05 sess_52g2q6l3v46ke3ol8vrkm3hmh4
-rw-------  1 root root     0 Jan  4 01:05 sess_5d43ithmjbpa5afgkbbljd36q6
-rw-------  1 root root     0 Jan  3 07:10 sess_5gcru5rtu36ftaepos8t55np72
-rw-------  1 root root     0 Jan  3 23:20 sess_62d00rli602c8vl66c6cba0d26
-rw-------  1 root root     0 Jan  4 00:40 sess_6dij04etr9vbdok7tnuqijkns1
-rw-------  1 root root     0 Jan  3 06:35 sess_6dtcs9tg4aka1et0tgoobdu6g1
-rw-------  1 root root     0 Jan  4 02:30 sess_6jabs7isqgfo5g7phaujk92gk7
-rw-------  1 root root     0 Jan  3 05:10 sess_6lke3ibjb8ohcbtodisdg4p695
-rw-------  1 root root     0 Jan  3 06:00 sess_6piv1d3cnvb9dmufpkecm544h2
-rw-------  1 root root     0 Jan  4 00:00 sess_6rki44erukp51af9aglb1ka2f2
-rw-------  1 root root     0 Jan  3 06:25 sess_6rt7l5n0dr7bfbk2as47f467s4
-rw-------  1 root root     0 Jan  3 05:50 sess_72k1knfboe45t0agphnocmct25
-rw-------  1 root root     0 Jan  3 06:30 sess_75ctr6fuuk9lmoplh5jmqialh3
-rw-------  1 root root    37 Jan  3 03:45 sess_75ldhmccjd9ed2ggcoeek5jlk0
-rw-------  1 root root     0 Jan  4 02:45 sess_77q5rrvle787sadj6cla8bvhh1
-rw-------  1 root root     0 Jan  3 22:55 sess_8764a6t51g8ot786j5ms1uqpb7
-rw-------  1 root root     0 Jan  3 23:45 sess_89f9p6906mbv7siv3gd10vn1k1
-rw-------  1 root root     0 Jan  3 05:30 sess_89vp55p5rjj5mgc5n61inqsar5
-rw-------  1 root root     0 Jan  4 04:40 sess_8c5197s69bn3in4f4urm68sf02
-rw-------  1 root root     0 Jan  3 07:45 sess_8dmafmcac59ajq5c5ug5le3t17
-rw-------  1 root root     0 Jan  4 01:15 sess_8h81uoa506tiho43i68ml70h74
-rw-------  1 root root     0 Jan  3 04:40 sess_8lebbo46fk6v8hi43a6ql3r2s4
-rw-------  1 root root     0 Jan  4 00:05 sess_8qg3ld6356ncn1qmohh22t4vt6
-rw-------  1 root root     0 Jan  3 07:15 sess_8rj4s26l43htgi12bckvs2ed27
-rw-------  1 root root    37 Jan  3 03:50 sess_91kkk3chr0u3r7ldebcuo57jj1
-rw-------  1 root root     0 Jan  4 05:55 sess_921t6a9tsorcr71shs8kl3sfr2
-rw-------  1 root root     0 Jan  3 05:20 sess_950qps7b7ggadbm0nhp6tuh4h2
-rw-------  1 root root     0 Jan  3 06:15 sess_9k0rg99r4lhedl4f8hj6k4tr11
-rw-------  1 root root     0 Jan  3 07:40 sess_9lab77j959v5tqg92viah8r761
-rw-------  1 root root     0 Jan  4 02:55 sess_a762areq9887r7q133dfl1t725
-rw-------  1 root root     0 Jan  4 03:40 sess_aar3b6i7tfgs595fk33cid5ca2
-rw-------  1 root root     0 Jan  3 07:15 sess_abr4evd2d36mcbmj5idriamgt2
-rw-------  1 root root    37 Jan  3 21:57 sess_agj66945pu4os95mjtgot22926
-rw-------  1 root root     0 Jan  4 01:45 sess_ahaeb54244lhjmpnbe8uc5jju5
-rw-------  1 root root     0 Jan  3 06:40 sess_b06bi2uqajqig16huhgbum3dl2
-rw-------  1 root root     0 Jan  4 04:35 sess_bbeofn2c0ovk1sd09fvd2b7i73
-rw-------  1 root root     0 Jan  3 05:00 sess_bn89dnktrir656ou3eq0flmak0
-rw-------  1 root root     0 Jan  4 00:25 sess_cjm5kam9paihpdsmv0ah8qagb4
-rw-------  1 root root     0 Jan  4 04:45 sess_ctjfidldh8kuuokg53o7sp74i2
-rw-------  1 root root     0 Jan  3 06:00 sess_d7tpj8g6ufnlge3o0tup4eke03
-rw-------  1 root root     0 Jan  3 22:15 sess_dd1ld71edav5pll63dt84bpao1
-rw-------  1 root root     0 Jan  3 04:50 sess_dj4keuqq16rms4i9vogppjg673
-rw-------  1 root root     0 Jan  3 04:20 sess_dkjf0nmrhp3b7d2rodoq2obti0
-rw-------  1 root root     0 Jan  3 06:10 sess_dmsgrs27euqgdtbm9g60ihj5m5
-rw-------  1 root root     0 Jan  4 04:25 sess_e13os9rmjpudo6lus9b11qc9c0
-rw-------  1 root root    37 Jan  3 04:10 sess_e4obul8rm8fijuhda8brm53oo6
-rw-------  1 root root     0 Jan  4 01:10 sess_e70crma9i53o397tctss19shv5
-rw-------  1 root root     0 Jan  3 05:05 sess_e9g6fn5ls0qblddq4c2345uvr5
-rw-------  1 root root     0 Jan  3 07:35 sess_ec46ir3tij41am3tp5dra95940
-rw-------  1 root root     0 Jan  3 22:30 sess_ecib2kuhsuvjdkgt3uc0nq2mn1
-rw-------  1 root root     0 Jan  4 01:30 sess_eea3lhthjtvei62q76cnjr25o1
-rw-------  1 root root     0 Jan  4 01:00 sess_eigta62udv1fmq2nvqr39kjpr0
-rw-------  1 root root     0 Jan  4 05:15 sess_ej9komlibdddkv2g7ekcv999l3
-rw-------  1 root root     0 Jan  3 07:00 sess_euvkpt9i6sv0i5p1ictbbggcg2
-rw-------  1 root root     0 Jan  3 06:40 sess_faqmjgi9t3vh87e72c3f69qsj0
-rw-------  1 root root     0 Jan  4 00:45 sess_ffbf5v5p3rs2mm8o0tuemkle01
-rw-------  1 root root     0 Jan  3 05:45 sess_fh0uou2p2q6b885ovv8mljqm45
-rw-------  1 root root     0 Jan  4 04:05 sess_ftl19us1tf27r3guu0rae7nqt1
-rw-------  1 root root     0 Jan  3 07:35 sess_ftlbbn6d7qevuaohni1k15gvm7
-rw-------  1 root root     0 Jan  3 05:15 sess_g9jkbu391bmahpqlffh7jd04g0
-rw-------  1 root root     0 Jan  3 22:20 sess_gq0quof1qb8dvrv136vjghf9o1
-rw-------  1 root root     0 Jan  4 00:55 sess_gv18q4oeopq8q62oarfdqg1m65
-rw-------  1 root root     0 Jan  3 23:35 sess_h6aq86v2uv2v7jqn0dc6j0bsr2
-rw-------  1 root root     0 Jan  3 05:40 sess_hd07djg3i6nktoi1ve69tiqas4
-rw-------  1 root root     0 Jan  3 07:25 sess_hfcchujnt5tcrqd0lu9meiafb5
-rw-------  1 root root     0 Jan  3 06:35 sess_hgncukivodp4jg6pt2a4ihas46
-rw-------  1 root root     0 Jan  4 02:35 sess_hhjld0cjjed00b9c7nvr3onr55
-rw-------  1 root root     0 Jan  4 03:15 sess_hk4ovo41ldjh6n81j6hlpee7h4
-rw-------  1 root root     0 Jan  3 04:30 sess_hm42e3ugncvdl6eqmnqtjkph84
-rw-------  1 root root     0 Jan  3 07:30 sess_hqf3j7uj496o98o9rs30fagh57
-rw-------  1 root root     0 Jan  4 00:20 sess_i50qfjbvgt7akeh04qo5k82pr5
-rw-------  1 root root     0 Jan  3 05:30 sess_icdu1lm6ql23jer1lep417jk73
-rw-------  1 root root     0 Jan  3 07:00 sess_il43g07t0mala3qg9qugb11gb0
-rw-------  1 root root     0 Jan  3 05:35 sess_iqh5vhsoisgkhn6k7hsc4jdb73
-rw-------  1 root root     0 Jan  4 02:40 sess_j1njq4g90cp6a68tkdnub7tdr1
-rw-------  1 root root     0 Jan  4 03:30 sess_j4nnjq1ta1sfpu45lfl9eceks7
-rw-------  1 root root     0 Jan  3 06:05 sess_j5ge74f86p0kr15gpd0v0abec4
-rw-------  1 root root     0 Jan  3 23:05 sess_j605276gm02dcrqpu9bevgl9a2
-rw-------  1 root root     0 Jan  3 23:40 sess_j8re158ote9afqt8frqr6nfos2
-rw-------  1 root root     0 Jan  3 05:55 sess_jt1oho35rv7p1cqpsumqtjuhs6
-rw-------  1 root root     0 Jan  4 03:25 sess_k482qnghmoghtm6utffuvumg15
-rw-------  1 root root     0 Jan  3 04:45 sess_kaufo3m0osmnl10a500824qps1
-rw-------  1 root root     0 Jan  3 22:45 sess_kntt2di41o26b7ba7ft1ke0331
-rw-------  1 root root     0 Jan  3 06:30 sess_krhihtubsc6gjarqbd93khllv1
-rw-------  1 root root     0 Jan  4 00:15 sess_kvjjhav30r91nr20ttb0j1ika6
-rw-------  1 root root    37 Jan  3 03:25 sess_l2p7grri5ucbdflknt1t99vqg5
-rw-------  1 root root     0 Jan  4 02:50 sess_l36m2qon0id4qj4c9novp1h8m1
-rw-------  1 root root     0 Jan  4 01:55 sess_lbhl2al1spqqdou3vef5e828r5
-rw-------  1 root root     0 Jan  3 04:45 sess_lq4lrj3hu8gt54fs9g6es6no72
-rw-------  1 root root     0 Jan  3 23:10 sess_lrgeeadeqt4tvd3bunlq7js4d3
-rw-------  1 root root     0 Jan  3 23:30 sess_m0b6bsknhlqaeqb6j4djuplft5
-rw-------  1 root root     0 Jan  3 04:40 sess_m3j95qbg95qt4p8o2dp5abv7t6
-rw-------  1 root root     0 Jan  3 22:50 sess_mt48f4agfho6214osvmomjn7g2
-rw-------  1 root root    37 Jan  3 03:35 sess_mvfc6lbd0rogv6d16lp3ipot57
-rw-------  1 root root     0 Jan  3 23:15 sess_nq9mpvvb0v37ism961floj7cf6
-rw-------  1 root root     0 Jan  4 02:05 sess_ns5m9eb3eimb3qfnublfenbb61
-rw-------  1 root root     0 Jan  3 05:00 sess_o2olpf4a4blq5iov0d6c82dcp4
-rw-------  1 root root     0 Jan  4 03:50 sess_o441dpmd2a6i82hs8futtmb2e6
-rw-------  1 root root     0 Jan  4 03:20 sess_o68pg76258riio057ra09djvm2
-rw-------  1 root root    37 Jan  3 04:05 sess_o76eaeccjsigopq8uh183av7o6
-rw-------  1 root root     0 Jan  4 00:10 sess_o7oa8k5uajcgncs02hrfld0ia4
-rw-------  1 root root     0 Jan  3 06:25 sess_oaqd72ubrksau63bikdl2hu746
-rw-------  1 root root     0 Jan  3 07:30 sess_oh24835rf6bokhkilaktfgjk77
-rw-------  1 root root     0 Jan  4 04:15 sess_ohncb4c783r1uh9umj7ceaho16
-rw-------  1 root root     0 Jan  3 23:50 sess_ooe9pljed6nbj2gvd6h7s46ds2
-rw-------  1 root root     0 Jan  4 02:20 sess_or5gitru0qr5bvodlrbu2t6md7
-rw-------  1 root root     0 Jan  4 04:50 sess_ouvch03bqi9ts513ue57vrs635
-rw-------  1 root root     0 Jan  3 04:15 sess_ovlr4dl8lsar4nogbeg49fu5u2
-rw-------  1 root root     0 Jan  4 02:15 sess_pg0e2u8fvckdda1opkr9oca276
-rw-------  1 root root     0 Jan  3 04:35 sess_pl1qom0dek5njpf2chm97ji7m5
-rw-------  1 root root     0 Jan  3 23:00 sess_q3beic17o0iu0r0sl5r3m1kql0
-rw-------  1 root root     0 Jan  3 04:25 sess_q3tl9bka49lfm34has9k3atak1
-rw-------  1 root root     0 Jan  4 03:05 sess_q9h8gqbpo2sd6c87lijrho3td4
-rw-------  1 root root     0 Jan  4 04:55 sess_qgi14r7uv8qacvqkn36sij7r40
-rw-------  1 root root     0 Jan  3 05:25 sess_qj947pkvcbbj9o6oarr7sa0mc1
-rw-------  1 root root     0 Jan  3 05:15 sess_ql2fdb0rjqdcorao9sur882kn6
-rw-------  1 root root    37 Jan  3 03:30 sess_qnqff2khldatlgenbg0he3gln3
-rw-------  1 root root     0 Jan  3 06:20 sess_r66cel42emgorborp2a0616bi5
-rw-------  1 root root    37 Jan  3 03:55 sess_ranv99m912nheg310gf4odt5o0
-rw-------  1 root root     0 Jan  4 05:25 sess_rs24ifc5cm9m326s4h1ssu0do6
-rw-------  1 root root     0 Jan  3 06:10 sess_sfe57ljoe30ctgn1rerk9sn026
-rw-------  1 root root     0 Jan  3 06:45 sess_sur6hsp66she4hbt42gemfbul2
-rw-------  1 root root     0 Jan  3 07:20 sess_tatemu4sce98prtugjd5rl4u37
-rw-------  1 root root     0 Jan  3 04:35 sess_thsmkudiu58jb1e4ug3nhgi670
-rw-------  1 root root     0 Jan  4 02:00 sess_tqt59dt7rf4v85esr7465qlbf3
-rw-------  1 root root     0 Jan  4 00:30 sess_tr7v6jahf93nuqk50fbipg3c25
-rw-------  1 root root     0 Jan  3 05:20 sess_u4nvcbfo4mnm2h7848308hv9u6
-rw-------  1 root root     0 Jan  3 07:20 sess_u5c18ibphjb79tvaarcc8ug443
-rw-------  1 root root     0 Jan  3 05:50 sess_u6c69cejn9pbb9vopauu90tfe2
-rw-------  1 root root     0 Jan  4 01:20 sess_ucc80kp9fabag7se17i1ipc8g2
-rw-------  1 root root     0 Jan  4 03:55 sess_ucfronu3ii701r4cdm06iu2tk4
-rw-------  1 root root     0 Jan  4 00:35 sess_ucfsc99nj3sntb0sumeocu2gd7
-rw-------  1 root root     0 Jan  3 07:25 sess_udqbpmies8erhd46kc5lsn3sv0
-rw-------  1 root root     0 Jan  4 01:40 sess_uphu25nlnjkcavvurptd2octb2
-rw-------  1 root root     0 Jan  3 06:55 sess_uu3icb4kc6gp34eel7hs860n02
-rw-------  1 root root     0 Jan  3 05:40 sess_v7cjmlclcbs3f8buqoug0lqdp4
-rw-------  1 root root     0 Jan  3 06:45 sess_v7ocbh5pb06fm1ll0bocdlj607
-rw-------  1 root root     0 Jan  4 04:20 sess_v7ollcu44ak60c6vhmqo6rkf11
-rw-------  1 root root     0 Jan  4 03:00 sess_vn9vitf39ihm2q848r5mljhfu1
-rw-------  1 root root     0 Jan  3 05:10 sess_vr9ka9u0vtrv75rtc35p6l7kq6
drwx------  2 root root  4096 Jan  3 02:58 spamd-2310-init
-rw-------  1 root root     0 Jan  3 20:11 tmp.0GI7SnoAcl
drwx------  5 root root  4096 Jan  3 22:00 tmp.eAf8ywXBlL
drwx------  3 root root  4096 Jan  3 04:10 tmp.fJtAldZyv7
drwx------  5 root root  4096 Jan  4 06:15 tmp.qvcJcczy0R
drwx------  3 root root  4096 Jan  2 19:10 tmp.TBUGbSyr8D
drwx------  5 root root  4096 Jan  3 22:05 tmp.tGQGKmQBq4
-rw-------  1 root root     0 Jan  2 21:19 TOQwhJ
-rw-------  1 root root     0 Jan  3 02:59 tpOB3f
</code></pre>
","<linux><centos><mysql>","2018-01-04 07:56:39"
"966638","Linux refusing mapped image file created with dd as swap device","<p>Getting the error ""<code>WRITE ERROR ON SWAP DEVICE</code>"" on boot but all works fine when I change the line: </p>

<pre><code>truncate -s $swapsize $flPtDev
</code></pre>

<p>by </p>

<pre><code>cp /swap.img $flPtDev
</code></pre>

<p>The complete script is the following:</p>

<pre><code>cd /home/myuser/
mkdir ./.myfolder
cd ./.myfolder  
swapsize='4G'
curdir=$(pwd)
flNmDev=""myfile.img""
flPtDev=""$curdir/$flNmDev"" 
flNmKey=""mykeyfile"" 
flPtKey=""$curdir/$flNmKey"" 
flNmMnt=""myDesiredMappedDeviceName"" 
flPtMnt=""$curdir/$flNmMnt""  
truncate -s $swapsize $flPtDev  # ** THE OFFENDING LINE **
chmod 0600 $flPtDev
chown root $flPtDev 
dd if=/dev/urandom of=$flPtKey bs=4096 count=1 conv=notrunc,noerror
sudo chmod 0600 $flPtKey
chown root $flPtKey
cat &lt;&lt; EOF &gt; /etc/crypttab
# &lt;target name&gt; &lt;source device&gt;         &lt;key file&gt;      &lt;options&gt;
$flNmMnt $flPtDev $flPtKey swap,offset=1024,cipher=aes-xts-plain64
EOF
cryptdisks_start $flNmMnt
rpl ""/swap.img none swap    sw  0   0"" ""#/swap.img none swap    sw  0   0"" /etc/fstab
echo ""/dev/mapper/$flNmMnt none swap sw 0 0"" &gt;&gt; /etc/fstab
</code></pre>
","<linux><filesystems><swap><encrypting-file-system>","2019-05-09 21:54:48"
"966761","What does it mean in log file: month: 1432","<p>I came aross a log format (from a data management product called <a href=""https://www.immuta.com/"" rel=""nofollow noreferrer"">Immuta</a>) in Json which shows:</p>

<pre><code>{
  ""id"": ""5cbe96c0-70fa-11e9-996b-21bef380b159"",
  ""dateTime"": ""1557255621161"",
  ""month"": 1432,
.....
}
</code></pre>

<p>What does it mean by 1432 in the month field? </p>

<p>there isn't much more info to provide here, I post it here in case my understanding is wrong, it seems this is a bug?</p>

<p>Thanks.</p>
","<unix><timestamp>","2019-05-10 18:45:32"
"752494","sharing resources between virtual machines","<p>After some extensive searching I couldn't find the answer to my question. I'm sorry if this is obvious or not, but I just need to know.
I have a server with verry low resources, 2gb of ram and 2 cores. I want to run a few webservices on it and want to do those in at least three linux VM's. Can I set my vm software up so that it shares the core depending on resource need of the guest os'es? My favourite vm software is VmWare, And I like to use that if that's possible.</p>
","<virtual-machines><vmware-server>","2016-01-28 20:46:11"
"890980","Wordpress Cannot Create or Modify .htaccess file","<p>Wordpress cannot write a new .htacess file or modify one that I put in. I've set the owner to apache:apache and the file permissions are 644. </p>

<p>Update - I've read for hours on posts just like mine. It seems the criteria is this: </p>

<ol>
<li>The user running the web service needs to own the .htaccess file (done). </li>
<li>The file permissions need to be set to 644 (done).</li>
</ol>

<p>Output of <code>ls -Z .htaccess</code>:</p>

<pre><code>-rwxr--r--. apache apache unconfined_u:object_r:httpd_sys_content_t:s0 /var/www/html/.htaccess
</code></pre>
","<apache-2.4><.htaccess><centos7>","2018-01-06 05:14:33"
"890983","Error while trying to ssh into Linux VM on GCP","<p>I'd like to know if you've come across this error and possible solution.
I've gone through alot to fix this, but doesn't seem to work.</p>

<p>Permission denied (publickey,gssapi-keyex,gssapi-with-mic).
ERROR: (gcloud.compute.ssh) [/usr/bin/ssh] exited with return code [255].</p>
","<ssh><google-compute-engine>","2018-01-06 08:01:19"
"966990","converting bat to shell script","<p>I have source of batch files which i need to convert to into shell script,i have do
converted most of the code but got stuck with the following code because i could'nt find any shell equivalents for it online so anyone help me in converting the batch script</p>

<p>batch code needed to convert:</p>

<pre><code>setlocal EnableDelayedExpansion
for %%i in (..\..\..\..\res\devices\*.xml) DO set xmls=!xmls! %%i
set xmls=!xmls! ..\..\..\..\res\RuntimeCreatedParams.xml

perl ParamEnumGen.pl ParamIds %xmls%
</code></pre>
","<linux><shell>","2019-05-13 07:59:47"
"752705","Linux mysql server 5.6 will not start, or restart","<p>I have a problem with mysql server; it doesn't want to start
do you have an idea</p>

<p>thank you.</p>

<pre><code>    janv. 29 10:51:15 clicshopping mysqld_safe[20076]: 160129 10:51:15 mysqld_safe Can't log to error log and syslog at the same time.  Remove all --log-error configuration options for --syslog to take effect.
janv. 29 10:51:15 clicshopping mysqld_safe[20076]: 160129 10:51:15 mysqld_safe Logging to '/var/log/mysql/error.log'.
janv. 29 10:51:15 clicshopping mysqld_safe[20076]: 160129 10:51:15 mysqld_safe Starting mysqld daemon with databases from /var/lib/mysql
janv. 29 10:51:15 clicshopping mysqld_safe[20076]: 160129 10:51:15 mysqld_safe mysqld from pid file /var/run/mysqld/mysqld.pid ended
janv. 29 10:51:55 clicshopping sudo[19837]: pam_unix(sudo:session): session closed for user root
</code></pre>

<p>My process</p>

<pre><code>sudo service mysql stop
sudo service apparmor stop
sudo mv /var/lib/mysql /home/mysql
sudo ln -s /home/mysql /var/lib/mysql
sudo chown -R mysql:mysql /home/mysql
vi /etc/apparmor.d/usr.sbin.mysqld (change var/lib/mysql ot /home/mysql)
sudo service apparmor start
sudo service mysql start
</code></pre>

<p>after deleted /var/log/mysql/error.log : results</p>

<pre><code>mysqld_safe Starting mysqld daemon with databases from /var/lib/mysql
2016-01-29 11:21:17 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
2016-01-29 11:21:17 0 [Note] /usr/sbin/mysqld (mysqld 5.6.28-0ubuntu0.15.10.1) starting as process 27093 ...
2016-01-29 11:21:17 27093 [Warning] Buffered warning: Changed limits: max_open_files: 1024 (requested 5000)

2016-01-29 11:21:17 27093 [Warning] Buffered warning: Changed limits: table_open_cache: 431 (requested 2000)

2016-01-29 11:21:17 27093 [Warning] Using unique option prefix myisam-recover instead of myisam-recover-options is deprecated and will be removed in a future release. Please use the full name instead.
2016-01-29 11:21:17 27093 [Note] Plugin 'FEDERATED' is disabled.
/usr/sbin/mysqld: Table 'mysql.plugin' doesn't exist
2016-01-29 11:21:17 27093 [ERROR] Can't open the mysql.plugin table. Please run mysql_upgrade to create it.
2016-01-29 11:21:17 27093 [Note] InnoDB: Using atomics to ref count buffer pool pages
2016-01-29 11:21:17 27093 [Note] InnoDB: The InnoDB memory heap is disabled
2016-01-29 11:21:17 27093 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
2016-01-29 11:21:17 27093 [Note] InnoDB: Memory barrier is not used
2016-01-29 11:21:17 27093 [Note] InnoDB: Compressed tables use zlib 1.2.8
2016-01-29 11:21:17 27093 [Note] InnoDB: Using Linux native AIO
2016-01-29 11:21:17 27093 [Note] InnoDB: Not using CPU crc32 instructions
2016-01-29 11:21:17 27093 [Note] InnoDB: Initializing buffer pool, size = 128.0M
2016-01-29 11:21:17 27093 [Note] InnoDB: Completed initialization of buffer pool
</code></pre>
","<mysql>","2016-01-29 15:55:33"
"967141","Difficulty in assigning resources to KVM virtual machines leading to extremely sluggish performance on VM","<p><strong>Physical Server Setup:</strong></p>

<p><strong>OS:</strong> Ubuntu 16.04</p>

<p><strong>RAM:</strong> 384GB </p>

<p><strong>CPU:</strong> Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz with 2 sockets, 8 cores per socket, and 2 threads per core, so the system reports it has 32 processors. </p>

<hr>

<p>I am using Wok/Kimchi to manage my system.  </p>

<p>At the moment I have 3 virtual machines running with 32GB of ram assigned to each and two of them have only 1 core assigned  and the other has 4 cores assigned, so a total of 6 cores are allocated to VMs.  </p>

<p>They seem to be running just fine <strong><em>but occasionally will hang for a second or two when I am using a putty terminal for example</em></strong>.  </p>

<p><strong><em>I am trying to start a 4th VM with only 1 core and 32GB of Ram but it's performance is so bad that just booting up takes several minutes and then even trying to type a command in a terminal is nearly impossible as the VM is so sluggish.</em></strong>  </p>

<p>The physical server is running two multithreaded Python scripts that use up to 10 cores, but otherwise it is not doing anything other than running the VMs.</p>

<p><strong><em>Does anyone have advice on how to allocate resources better?  I think I should easily be able to run several more VMs.</em></strong></p>
","<ubuntu><kvm-virtualization>","2019-05-14 04:05:31"
"967204","SSH connection between two hosts with differents user","<p>i have two hosts</p>

<p>H1 with <code>ip=10.10.10.15</code> , <code>OS=Windows Server 2016</code> and <code>username=administrator</code></p>

<p>H2 with  <code>ip=10.10.10.16</code> , <code>OS=Centos7</code> and <code>username=user</code></p>

<p>I attempt to create a ssh connection between H1 and H2.</p>

<p>I mean, when i tape from the windows machine : <code>ssh user@10.10.1.16</code> , i don't need to tape the password of <code>user</code> and directly i have a remote access</p>

<p>After generate the key with <code>ssh-keygen</code> , i try with <code>ssh-copy-id -i id_rsa.pub user@10.10.1.16</code> and i enter the password</p>

<pre><code>/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "".ssh/id_rsa.pub""
/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
user@10.10.10.16's password: 

Number of key(s) added: 1

Now try logging into the machine, with:   ""ssh 'user@10.10.10.16'""
and check to make sure that only the key(s) you wanted were added.
</code></pre>

<p>but the console request always the password of <code>user</code></p>

<p>Is there some way to resolve that ?</p>

<p>EDIT : </p>

<p>this the output of <code>ssh -v user@10.10.1.16</code></p>

<pre><code>OpenSSH_7.4p1 Debian-10+deb9u6, OpenSSL 1.0.2r  26 Feb 2019
debug1: Reading configuration data /etc/ssh/ssh_config
debug1: /etc/ssh/ssh_config line 19: Applying options for *
debug1: Connecting to 10.10.1.16 [10.10.1.16] port 22.
debug1: Connection established.
debug1: identity file /home/administrator/.ssh/id_rsa type 1
debug1: key_load_public: No such file or directory
debug1: identity file /home/administrator/.ssh/id_rsa-cert type -1
debug1: key_load_public: No such file or directory
debug1: identity file /home/administrator/.ssh/id_dsa type -1
debug1: key_load_public: No such file or directory
debug1: identity file /home/administrator/.ssh/id_dsa-cert type -1
debug1: key_load_public: No such file or directory
debug1: identity file /home/administrator/.ssh/id_ecdsa type -1
debug1: key_load_public: No such file or directory
debug1: identity file /home/administrator/.ssh/id_ecdsa-cert type -1
debug1: key_load_public: No such file or directory
debug1: identity file /home/administrator/.ssh/id_ed25519 type -1
debug1: key_load_public: No such file or directory
debug1: identity file /home/administrator/.ssh/id_ed25519-cert type -1
debug1: Enabling compatibility mode for protocol 2.0
debug1: Local version string SSH-2.0-OpenSSH_7.4p1 Debian-10+deb9u6
debug1: Remote protocol version 2.0, remote software version OpenSSH_7.4
debug1: match: OpenSSH_7.4 pat OpenSSH* compat 0x04000000
debug1: Authenticating to 10.10.10.16:22 as 'user'
debug1: SSH2_MSG_KEXINIT sent
debug1: SSH2_MSG_KEXINIT received
debug1: kex: algorithm: curve25519-sha256
debug1: kex: host key algorithm: ecdsa-sha2-nistp256
debug1: kex: server-&gt;client cipher: chacha20-poly1305@openssh.com MAC: &lt;implicit&gt; compression: none
debug1: kex: client-&gt;server cipher: chacha20-poly1305@openssh.com MAC: &lt;implicit&gt; compression: none
debug1: expecting SSH2_MSG_KEX_ECDH_REPLY
debug1: Server host key: ecdsa-sha2-nistp256 SHA256:gIsbKkZPUqXJXkIwkjm0rNe+BS98+J0fQ+acbNgEzzI
debug1: Host '10.10.10.16' is known and matches the ECDSA host key.
debug1: Found key in /home/administrator/.ssh/known_hosts:25
debug1: rekey after 134217728 blocks
debug1: SSH2_MSG_NEWKEYS sent
debug1: expecting SSH2_MSG_NEWKEYS
debug1: SSH2_MSG_NEWKEYS received
debug1: rekey after 134217728 blocks
debug1: SSH2_MSG_EXT_INFO received
debug1: kex_input_ext_info: server-sig-algs=&lt;rsa-sha2-256,rsa-sha2-512&gt;
debug1: SSH2_MSG_SERVICE_ACCEPT received
debug1: Authentications that can continue: publickey,gssapi-keyex,gssapi-with-mic,password
debug1: Next authentication method: gssapi-keyex
debug1: No valid Key exchange context
debug1: Next authentication method: gssapi-with-mic
debug1: Unspecified GSS failure.  Minor code may provide more information
No Kerberos credentials available (default cache: FILE:/tmp/krb5cc_1000)

debug1: Unspecified GSS failure.  Minor code may provide more information
No Kerberos credentials available (default cache: FILE:/tmp/krb5cc_1000)

debug1: Next authentication method: publickey
debug1: Offering RSA public key: /home/administrator/.ssh/id_rsa
debug1: Authentications that can continue: publickey,gssapi-keyex,gssapi-with-mic,password
debug1: Trying private key: /home/administrator/.ssh/id_dsa
debug1: Trying private key: /home/administrator/.ssh/id_ecdsa
debug1: Trying private key: /home/administrator/.ssh/id_ed25519
debug1: Next authentication method: password
</code></pre>

<p>the output of sshd log</p>

<pre><code>May 14 14:08:12 dockerjava sshd[17170]: Authentication refused: bad ownership or modes for file /home/user/.ssh/authorized_keys
</code></pre>

<p>the file is</p>

<pre><code>-rw-rw-r--. 1 user user 1999 May 14 14:03 authorized_keys
-rw-------. 1 user user 1675 May 14 12:37 id_rsa
-rw-r--r--. 1 user user  398 May 14 12:37 id_rsa.pub
-rw-r--r--. 1 user user 1314 May 14 14:02 known_hosts
</code></pre>
","<windows-server-2008><ssh><git><public-key>","2019-05-14 12:30:20"
"891242","Cannot log in to the MySQL server after hosting company restarted my server","<p>Today my hosting company decided to patch their systems due to Meltdown/Spectre and restart all servers.</p>

<p>I had my CentOS 7.2 server running for 280 days with no problems of any kind at all.</p>

<p>But now I can't use my website anymore, it says ""Error establishing a database connection"". When I try to log in to phpMyAdmin with my usual credentials, which I haven't changed at all, it says ""Cannot log in to the MySQL server"" without accompanied error number #1045, which it tells usually.</p>

<p>In <code>/var/log/mariadb/mariadb.log</code> I found the following:</p>

<pre><code>170329 18:41:40 [Note] /usr/libexec/mysqld: ready for connections.
Version: '5.5.44-MariaDB'  socket: '/var/lib/mysql/mysql.sock'  port: 3306  MariaDB Server
180108 14:25:52 [Note] /usr/libexec/mysqld: Normal shutdown
180108 14:25:54 [Note] Event Scheduler: Purging the queue. 0 events
180108 14:26:16  InnoDB: Starting shutdown...
180108 14:26:42  InnoDB: Shutdown completed; log sequence number 233621773
180108 14:26:44 [Note] /usr/libexec/mysqld: Shutdown complete
</code></pre>

<p>You see the last activity was on 29.03.2017, in between everything ran smooth, and today InnoDB and MySQL have been shutdown normally. But there are no signs of an attempted start.</p>

<p>When I type in ""<code>systemctl start mariadb</code>"" it yields an error which I analyzed via ""<code>systemctl status mariadb</code>"":</p>

<pre><code>Process: 3738 ExecStartPost=/usr/libexec/mariadb-wait-ready $MAINPID (code=exited, status=1/FAILURE)
Process: 3737 ExecStart=/usr/bin/mysqld_safe --basedir=/usr (code=exited, status=1/FAILURE)
</code></pre>

<p>I'm very confused as to what happened and hope you can shed some light on this problem.</p>
","<mysql><centos7><mariadb><innodb><reboot>","2018-01-08 17:36:27"
"823692","Rescue fstab, invalid argument mounting my disk ext4","<p>I have a problem, I think I wrong to edit /etc/fstab on centos7 and I cannot reboot my remote server.</p>

<p>I'm in rescue mode with freebsd.</p>

<p>I cannot mount my ada0s3 (on centos was sda3) to fix my fstab.</p>

<pre><code>root@rescuefbsd:/mnt # gpart show -p ada0
=&gt;        63  3907029105    ada0  MBR  (1.8T)
          63        1985          - free -  (993K)
        2048        2048  ada0s1  linux-data  (1.0M)
        4096     1024000  ada0s2  linux-data  [active]  (500M)
     1028096  3889223680  ada0s3  linux-data  (1.8T)
  3890251776    16777216  ada0s4  ebr  (8.0G)
  3907028992         176          - free -  (88K)



root@rescuefbsd:/mnt # file -s /dev/ada0s3 
/dev/ada0s3: Linux rev 1.0 ext4 filesystem data, UUID=80281fb1-7589-4a01-b2d0-6e6529e6b09b (needs journal recovery) (extents) (64bit) (large files) (huge files)

root@rescuefbsd:/mnt # dmesg | grep ada0
ada0 at ahcich0 bus 0 scbus0 target 0 lun 0
ada0: &lt;WDC WD2002FAEX-007BA0 05.01D05&gt; ATA-8 SATA 3.x device
ada0: Serial Number WD-WMAY02628053
ada0: 300.000MB/s transfers (SATA 2.x, UDMA6, PIO 8192bytes)
ada0: Command Queueing enabled
ada0: 1907729MB (3907029168 512 byte sectors: 16H 63S/T 16383C)
ada0: Previously was known as ad4
WARNING: mount of ada0s3 denied due to unsupported optional features
WARNING: mount of ada0s3 denied due to unsupported optional features
WARNING: mount of ada0s3 denied due to unsupported optional features
WARNING: mount of ada0s3 denied due to unsupported optional features
</code></pre>

<p>Tried (some solution on google):</p>

<pre><code>root@rescuefbsd:/mnt # mount -t ext4 /dev/ada0s3 /mnt
mount: /dev/ada0s3: Operation not supported by device

root@rescuefbsd:/mnt # mount /dev/ada0s3 /mnt
mount: /dev/ada0s3: Invalid argument

root@rescuefbsd:/mnt # mount -t ext2fs -o ro /dev/ada0s3 /mnt
mount: /dev/ada0s3: Invalid argument
</code></pre>

<p>Some idea please? </p>
","<freebsd><mount><fstab>","2017-01-02 11:39:52"
"752973","Where is the right place to install the ESX vSphere?","<p>I have one server with 6 hotplug 300G SAS HDD.</p>

<p>Where is the right place to install the ESX vSphere?</p>

<p>6 HDD RAID10 for datastore and ESX vSphere together</p>

<p>-------- OR -------- </p>

<p>2 HDD RAID1 for ESX vSphere</p>

<p>4 HDD RAID10 for datastore </p>

<p>Thanks to all helpers</p>
","<raid><vmware-esxi><vmware-vsphere>","2016-01-31 09:04:37"
"753321","Namecheap DNS Cname issue","<p>I am using Namecheap default DNS, where I bought the dns name. However, I am having a issue...</p>

<p>Typically, for AWS and Rackspace I would have:</p>

<pre><code>A Record: foo.com 124.22.22.1
Cname:    www.example.com foo.com
</code></pre>

<p>But for Namecheap I have to use <code>@</code> to show server root:</p>

<pre><code>A Record: @ 124.22.22.1
Cname:    www.example.com example.com
</code></pre>

<p>With Namecheap DNS, <code>http://example.com</code> works but not <code>www.example.com</code> with <code>ERR_NAME_NOT_RESOLVED</code>. Any suggestions?</p>
","<domain-name-system><cname-record><namecheap>","2016-02-01 21:58:58"
"753326","port listening in netstat can't telnet","<p>I'm trying to connect to port 8020 on a CentOS vm but I'm getting <code>telnet: connect to address &lt;public_ip&gt;: Connection refused</code> when I run <code>telnet public_ip 8020</code>. When I run <code>netstat -anp | grep 8020</code> I get </p>

<pre><code>tcp        0      0 127.0.0.1:8020              0.0.0.0:*                   LISTEN      18908/java
tcp        0      0 127.0.0.1:38750             127.0.0.1:8020              TIME_WAIT   -
</code></pre>

<p>I have other ports listening, <code>telnet public_ip 50070</code> allows me to connect. The output for netstat on that is </p>

<pre><code>tcp        0      0 0.0.0.0:50070               0.0.0.0:*                   LISTEN      18908/java
</code></pre>

<p>When I execute <code>telnet localhost 8020</code> I get </p>

<pre><code>Trying ::1...
telnet: connect to address ::1: Connection refused
Trying 127.0.0.1...
Connected to localhost.
</code></pre>

<p>Do I need to do something to change the 127.0.0.1 in the netstat for 8020 to 0.0.0.0 like it is for the successful connection to 50070?</p>

<p>The output of <code>iptables -L -v</code> is </p>

<pre><code>Chain INPUT (policy ACCEPT 84481 packets, 11M bytes)
 pkts bytes target     prot opt in     out     source               destination

Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination

Chain OUTPUT (policy ACCEPT 54190 packets, 6770K bytes)
 pkts bytes target     prot opt in     out     source               destination
</code></pre>

<p>This is a hadoop service, the config that tells it to listen on port 8020 is in core-site.xml</p>

<pre><code>&lt;property&gt;
    &lt;name&gt;fs.defaultFS&lt;/name&gt;
    &lt;value&gt;hdfs://localhost:8020&lt;/value&gt;
&lt;/property&gt;
</code></pre>
","<linux><networking><connection-refused>","2016-02-01 22:23:05"
"967685","Can't install phpMyAdmin due to package dependency issue","<p>When I run the command <code>sudo yum --enablerepo=epel install phpmyadmin</code> it returns the following package dependency error:  </p>

<pre><code>  Error: Package: php-mcrypt-5.3.3-5.el6.x86_64 (epel)
           Requires: php(api) = 20090626
           Installed: php-common-5.4.16-45.amzn2.0.6.x86_64 (@amzn2-core)
               php(api) = 20100412-64
           Available: php-common-5.4.16-43.amzn2.x86_64 (amzn2-core)
               php(api) = 20100412-64
           Available: php-common-5.4.16-43.amzn2.0.1.x86_64 (amzn2-core)
               php(api) = 20100412-64
           Available: php-common-5.4.16-43.amzn2.0.2.x86_64 (amzn2-core)
               php(api) = 20100412-64
           Available: php-common-5.4.16-43.amzn2.0.3.x86_64 (amzn2-core)
               php(api) = 20100412-64
           Available: php-common-5.4.16-43.amzn2.0.4.x86_64 (amzn2-core)
               php(api) = 20100412-64
           Available: php-common-5.4.16-45.amzn2.0.5.x86_64 (amzn2-core)
               php(api) = 20100412-64
Error: Package: php-mcrypt-5.3.3-5.el6.x86_64 (epel)
           Requires: php(zend-abi) = 20090626
           Installed: php-common-5.4.16-45.amzn2.0.6.x86_64 (@amzn2-core)
               php(zend-abi) = 20100525-64
           Available: php-common-5.4.16-43.amzn2.x86_64 (amzn2-core)
               php(zend-abi) = 20100525-64
           Available: php-common-5.4.16-43.amzn2.0.1.x86_64 (amzn2-core)
               php(zend-abi) = 20100525-64
           Available: php-common-5.4.16-43.amzn2.0.2.x86_64 (amzn2-core)
               php(zend-abi) = 20100525-64


           Available: php-common-5.4.16-43.amzn2.0.3.x86_64 (amzn2-core)
               php(zend-abi) = 20100525-64
           Available: php-common-5.4.16-43.amzn2.0.4.x86_64 (amzn2-core)
               php(zend-abi) = 20100525-64
           Available: php-common-5.4.16-45.amzn2.0.5.x86_64 (amzn2-core)
               php(zend-abi) = 20100525-64
Error: Package: php-tcpdf-6.2.26-1.el6.noarch (epel)
           Requires: php-tidy
 You could try using --skip-broken to work around the problem
 You could try running: rpm -Va --nofiles --nodigest
</code></pre>

<p>What do I need to install to make myphpadmin able to download properly?</p>

<p>My php version is 5.4.16 (obviously) and if i run <code>rpm -q phpmyadmin</code> it says that the package is not installed. </p>
","<linux><apache-2.2><amazon-ec2><phpmyadmin>","2019-05-16 21:27:49"
"891767","rocketchat-server snap running but not responding","<p><strong>What I am trying to do</strong>/<strong>What have you tried in order to make it happen?</strong></p>

<p>I followed installation and configuration of ubuntu 17.10 snap ""rocketchat-server"" and ""rocketchat-client"": Now I want to connect them both.</p>

<p>I entered channel <code>--edge</code> to no avail!</p>

<p>I completely reinstalled rocketchat-server and rocketchat-desktop snaps to no avail!</p>

<p><strong>What results did you expect?</strong></p>

<p>I want to connect to my own server URLs, as happened earlier:
<a href=""http://leder.no-ip.org:8080"" rel=""nofollow noreferrer"">RocketChat</a> (please enter this link in rocketchat-desktop, error message <code>No valid server found at the URL</code> in case my server is down)</p>

<p>I even cannot configure and access 
<a href=""http://localhost:8080"" rel=""nofollow noreferrer"">localhost</a> (this link should work when entered in rocketchat-desktop)</p>

<p>The example server works fine here:
<a href=""https://open.rocket.chat/"" rel=""nofollow noreferrer"">example server</a> (please enter this link in rocketchat-desktop)</p>

<p><strong>What actually happened?</strong></p>

<p>Desktop gives error:
<code>No valid server found at the URL</code></p>

<p>And mobile app says:
<code>Invalid server version</code></p>

<p>Service is up and running with:
<code>sudo systemctl status snap.rocketchat-server.rocketchat-caddy</code></p>

<p>Rocketchat-mongo.service is down, why is this?
<code>sudo systemctl status snap.rocketchat-server.rocketchat-mongo.service</code></p>

<p>How do I connect client to server successfully, again?</p>

<p>Thanks in advance!</p>
","<ubuntu><client-server>","2018-01-11 15:59:23"
"753365","Windows Server 2012 R2 DNS internal and external records issue","<p>I am running  into an issue with with DNS on Windows Server 2012 R2 acting as Active Directory domain controller AND DNS server</p>

<p>I want to be able to setup certain records that will resolve internally using the local DNS server but if record is not present, the request is sent for external DNS query. Is this possible with Windows Server 2012R2?</p>

<p>ZONE</p>

<p>[internal records managed by windows server 2012 DNS server] </p>

<p>something.com</p>

<pre><code>sub1    A   10.10.0.1
sub2    A   10.10.0.2
sub3    CNAME sub1.something.com.
</code></pre>

<hr>

<p>[external records managed by external DNS provider]</p>

<p>something.com</p>

<pre><code>sub4    A    54.26.45.24
sub5    A    184.34.56.25
sub6    CNAME   images.cdnprovider.net.
</code></pre>

<p>So basically i want to be to direct users to internal resources if requesting the records(for example sub1.something.com, sub2.something.com and sub3.something.com) in the internal DNS and if those records do not exist (for example sub4.something.com, sub5.something.com and sub6.something.com) then it should go out and resolve appropriately</p>

<p>I am trying to do this so i don't have to copy all records from external DNS provider to internal DNS. So that way i only add the records that need to be resolved locally and for the ones not added locally then it should go out and resolve.</p>

<p>Can i do this?</p>

<p>If my question is not very clear please let me know and i will edit it right away.</p>

<p>UPDATED: 
Basically something that works same way hosts file work on Windows OS(and others as well), where if the records are present then it uses local host records and if not it goes out.</p>

<p>Thanks</p>
","<domain-name-system><active-directory><windows-server-2012-r2>","2016-02-02 02:35:47"
"891788","Why would my Oracle read from disk?","<p>Our Oracle server (version 12c) is running on a RedHat-6.7 server with 128GB of RAM.</p>

<p>The current size of the only database on it is only about 60GB.</p>

<p>Yet, we see plenty of disk reads -- on the device where the database is stored -- in the <code>iostat</code>-output (shown in blue). There are some writes (shown in yellow) too, of course:</p>

<p><a href=""https://i.sstatic.net/pDtJn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pDtJn.png"" alt=""enter image description here""></a></p>

<p>While the writes to permanent storage make sense because we do make modifications to the data some times, the reads do not -- the entire DB can fit in memory twice over... Indeed, we <em>should</em> be able to store it on a USB-stick and still have good read-times.</p>

<p>What parts of the Oracle-configuration should we examine and tune to make the server use all available RAM?</p>

<p><em>Update</em>: we will look at the <a href=""https://blogs.oracle.com/in-memory/getting-started-with-oracle-database-in-memory-part-i-installing-enabling"" rel=""nofollow noreferrer"">INMEMORY</a> settings -- thanks @lenniey. But it would seem, the INMEMORY parameters come into play, when the server has to (because of RAM shortage) decide, what to drop from memory to be re-read back later. In our case <em>nothing</em> should ever be dropped from memory, because there is room for <em>everything</em>. And yet some things are, evidently, being (re)read again and again...</p>
","<oracle><database-performance><oracle-linux>","2018-01-11 17:08:08"
"753385","Websites from the same hosting doesn't open only from my router/proovider","<p>Faced with a strange problem.</p>

<p>I have a hosting with few websites. Starting from today I can't navigate any of them from any device, connected to home wifi. But I can ping each domain.</p>

<p>Chrome fails with Timeout, while mobile browser fails with Connection Refused.</p>

<p>And there are nobody (as far as I know) who experiencing the same problem.</p>

<p>The only changes were made during this period are (for domain config):</p>

<pre><code>1. added TXT domain_key record
2. added TXT @ SPF record
</code></pre>

<p>Tried to restart router, get a new IP, flush DNS, restart hosting.</p>

<p><strong>UPD.</strong></p>

<p>nslookup shows:</p>

<pre><code>nslookup domain.com
Server: unknown
Address: 192.168.1.1

Non-authoritative answer:
Name: domain.com
Address: &lt;correct IP&gt;
</code></pre>

<p>tracert domain.com</p>

<pre><code>output doesn't show any errors and show trace without any problems tight to the domain.com
</code></pre>
","<hosting><timeout><connection-refused><accessibility>","2016-02-02 06:46:04"
"753397","Need some clarification on a NIC functionality on a Dell Poweredge r610","<p>Quick question. I'm trying to understand the NIC functionality of this server. NIC specs from manual: <a href=""https://i.sstatic.net/pZ7q7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pZ7q7.png"" alt=""NIC info""></a></p>

<p>My question is, do the two broadcom controller chips provide fail-over redundancy? For example, is it design intention that if a controller chip dies then the other takes over (with only 2 ports?). Thanks.</p>
","<windows><networking><failover><redundancy><broadcom>","2016-02-02 08:35:16"
"753400","Write on an unavailable nfs share","<p>We have a legacy application which writes files on an nfs share. </p>

<p>Some days ago that nfs share could be down and we have some missing files. </p>

<p>According to the application's assistance when the nfs share is down the files are written on local file system. It sounds weird to me and cannot even understand if it is an application behavior or if it is normal nfs operation.</p>

<p>Anyway at a first glance the missing files are not present on local file system too.
I wonder what happens if I try to mount nfs share on a non empty directory.
Does it fails or files are merged ?</p>
","<nfs>","2016-02-02 08:46:12"
"753408","why am i Forced to increase Exchange 2013 attachment size to 100MB to be able to receive 10MB attachment","<p>A few of my users were getting rejected by a remote server when trying to send/receive attachment of 9-10Mb. </p>

<p>I tried the following command </p>

<pre><code>Set-TransportConfig –MaxSendSize 10MB –MaxReceiveSize 15MB
</code></pre>

<p>This did not work.</p>

<p>I ran the following command </p>

<pre><code>Set-TransportConfig -ExternalDsnMaxMessageAttachSize 100 MB -InternalDsnMaxMessageAttachSize 100MB -MaxReceiveSize 100MB -MaxSendSize 100MB
</code></pre>

<p>This allowed the users to send and receive attachment size in the 10MB size.</p>

<p>My question is why do I have to increase the attachment to 10 times more than what I intended to do for it to work?</p>

<p>Thanks</p>

<p>Ruben</p>
","<exchange-2013>","2016-02-02 09:30:15"
"891874","IBM Notes asking for user id file during setup","<p>I have a test Domino server and client machines. I have problem trying to setup IBM Notes on a client machine using the admin credential(e.g. John Doe). After putting Name (John Doe) and Server name, its asking for the user id file as shown below:</p>

<p><a href=""https://i.sstatic.net/CtXcr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CtXcr.png"" alt=""enter image description here""></a></p>

<p>I've confirmed client machine is able to ping the domino server, also I was able to open the domino server web administrator on client browser with same credential. But one thing I observed during Note setup (in case its related), I was only able to connect using the hostname of domino server. It refused to connect using just the server name in the format: mydomino01/hrteam. </p>

<p>Earlier on, I setup IBM Notes on a Macbook with same credentials and without it prompting me for user id file, I put in the password and it was able to access domino server. I've found a user.id file in my Mac's IBM Notes Data, can I simply import this file into this new client's IBM Notes Data directory?</p>

<p><code>Notes.ini</code> in my My Domino server:</p>

<pre><code>Directory=/local/notesdata
KeyFileName=/local/notesdata/server.id
KeyFileName_Owner=CN=mydomino01/O=hrteam
CertifierIDFile=/local/notesdata/cert.id
MailServer=CN=mydomino01/O=hrteam
</code></pre>

<p>I'm confused about what to do next,how to resolve this issue?</p>
","<ibm-domino><lotus-notes>","2018-01-12 08:43:40"
"891879","Apache does not log internal ip when connected to vpn","<p>I'm having vps which has apache and openvpn at the same time. When I'm connecting vpn with my client computer, my public ip becomes vps's public ip and server assigns an internal ip to my client according to configuration. When I access web server from the client using browser, I am expecting to see client's internal ip or at least public ip which is the same with the server at access_log files, However apache logs router's public ip. Is this behaviour normal or I am having some misconfiguration?</p>
","<apache-2.4><openvpn><centos7>","2018-01-12 09:12:20"
"891887","Remote Server Administration Tool","<p><img src=""https://i.sstatic.net/0kGhI.jpg"" alt=""screenshot""></p>

<p>i am trying to install RSAT ,BUT there is error ,""Installer encountered an error 0x8007052e"" The username and password is incorrect.HOW to fix this to install RSAT</p>
","<windows-server-2012-r2>","2018-01-12 10:21:15"
"891903",".htaccess is not working even when i change Allowoverride to All","<p>I have  searched several docs here and everywhere on Google but to no avail
i have added a .htaccess file to my default document root (/var/www/html) on apache2 running on ubuntu 16.0.
In my /etc/apache2/apache2.conf file  i have also tried doing this
FROM:</p>

<pre><code>&lt;Directory /var/www/&gt;
        Options Indexes FollowSymLinks
        AllowOverride None
        Require all granted
&lt;/Directory&gt;
</code></pre>

<p>To:</p>

<pre><code>&lt;Directory /var/www/&gt;
        Options Indexes FollowSymLinks
        AllowOverride All
        Require all granted
&lt;/Directory&gt;
</code></pre>

<p>But my .htaccess file is not executing...i have tried adding some sort of dummy text to my .htaccess file but it is still not working.I have checked the validity of my .htaccess file on <a href=""http://www.htaccesscheck.com/"" rel=""nofollow noreferrer"">http://www.htaccesscheck.com/</a> and my .htaccess file was ok..</p>

<p>this a copy of my .htaccess file </p>

<pre><code>#Dont touch

# Indexing index.php

    DirectoryIndex index.php index.html


# Error pages 

    ErrorDocument 400 /errorfiles/400.html
    ErrorDocument 401 /errorfiles/401.html
    ErrorDocument 403 /errorfiles/403.html
    ErrorDocument 404 /errorfiles/404.html
    ErrorDocument 500 /errorfiles/500.html

# disable directory browsing

    Options ExecCGI Includes IncludesNOEXEC SymLinksIfOwnerMatch -Indexes
</code></pre>

<p>This is what  my error log says </p>

<pre><code>[Fri Jan 12 11:35:54.273813 2018] [mpm_event:notice] [pid 25427:tid 13978013479$
[Fri Jan 12 11:35:54.273873 2018] [core:notice] [pid 25427:tid 139780134791040]$
[Fri Jan 12 11:39:52.533648 2018] [mpm_event:notice] [pid 25427:tid 13978013479$
[Fri Jan 12 11:39:53.594378 2018] [mpm_event:notice] [pid 25656:tid 13991752895$
[Fri Jan 12 11:39:53.594440 2018] [core:notice] [pid 25656:tid 139917528958848]$
[Fri Jan 12 11:40:49.182465 2018] [mpm_event:notice] [pid 25656:tid 13991752895$
[Fri Jan 12 11:40:50.242188 2018] [mpm_prefork:notice] [pid 27360] AH00163: Apa$
[Fri Jan 12 11:40:50.242239 2018] [core:notice] [pid 27360] AH00094: Command li$
[Fri Jan 12 11:40:51.521464 2018] [mpm_prefork:notice] [pid 27360] AH00169: cau$
[Fri Jan 12 11:40:52.645238 2018] [mpm_prefork:notice] [pid 27476] AH00163: Apa$
[Fri Jan 12 11:40:52.645278 2018] [core:notice] [pid 27476] AH00094: Command li$
[Fri Jan 12 11:41:05.370691 2018] [mpm_prefork:notice] [pid 27476] AH00169: cau$
[Fri Jan 12 11:41:06.427807 2018] [mpm_prefork:notice] [pid 27561] AH00163: Apa$
[Fri Jan 12 11:41:06.427850 2018] [core:notice] [pid 27561] AH00094: Command li$
[Fri Jan 12 11:46:15.963404 2018] [:error] [pid 27597] [client 41.66.255.110:35$
[Fri Jan 12 12:06:58.048553 2018] [mpm_prefork:notice] [pid 27561] AH00169: cau$
[Fri Jan 12 12:06:59.130686 2018] [mpm_prefork:notice] [pid 27713] AH00163: Apa$
[Fri Jan 12 12:06:59.130727 2018] [core:notice] [pid 27713] AH00094: Command li$
[Fri Jan 12 12:39:43.402867 2018] [mpm_prefork:notice] [pid 27713] AH00169: cau$
</code></pre>

<p><strong>sorry about that, this is the continuation</strong></p>

<pre><code>[Fri Jan 12 12:39:43.402867 2018] [mpm_prefork:notice] [pid 27713] AH00169: cau$
[Fri Jan 12 12:39:44.479303 2018] [mpm_prefork:notice] [pid 27935] AH00163: Apa$
[Fri Jan 12 12:39:44.479341 2018] [core:notice] [pid 27935] AH00094: Command li$
[Fri Jan 12 12:39:48.447223 2018] [mpm_prefork:notice] [pid 27935] AH00169: cau$
[Fri Jan 12 12:39:49.503253 2018] [mpm_prefork:notice] [pid 27995] AH00163: Apa$
[Fri Jan 12 12:39:49.503294 2018] [core:notice] [pid 27995] AH00094: Command li$
[Fri Jan 12 12:40:09.668870 2018] [core:alert] [pid 28002] [client 41.66.255.11$
[Fri Jan 12 12:42:08.004183 2018] [mpm_prefork:notice] [pid 27995] AH00169: cau$
[Fri Jan 12 12:42:09.064352 2018] [mpm_prefork:notice] [pid 28058] AH00163: Apa$
[Fri Jan 12 12:42:09.064394 2018] [core:notice] [pid 28058] AH00094: Command li$
[Fri Jan 12 13:14:10.396661 2018] [mpm_prefork:notice] [pid 28058] AH00169: cau$
[Fri Jan 12 13:14:11.444968 2018] [mpm_prefork:notice] [pid 28191] AH00163: Apa$
[Fri Jan 12 13:14:11.445006 2018] [core:notice] [pid 28191] AH00094: Command li$
[Fri Jan 12 13:15:23.882928 2018] [core:alert] [pid 28196] [client 41.66.255.11$
[Fri Jan 12 13:18:06.442135 2018] [mpm_prefork:notice] [pid 28191] AH00169: cau$
[Fri Jan 12 13:19:41.544214 2018] [mpm_prefork:notice] [pid 28282] AH00163: Apa$
[Fri Jan 12 13:19:41.544256 2018] [core:notice] [pid 28282] AH00094: Command li$
[Fri Jan 12 14:12:06.208682 2018] [mpm_prefork:notice] [pid 28282] AH00169: cau$
[Fri Jan 12 14:12:07.286856 2018] [mpm_prefork:notice] [pid 28502] AH00163: Apa$
[Fri Jan 12 14:12:07.286897 2018] [core:notice] [pid 28502] AH00094: Command li$
[Fri Jan 12 14:12:59.228236 2018] [core:alert] [pid 28508] [client 41.66.255.11$
[Fri Jan 12 14:20:17.281456 2018] [core:alert] [pid 28505] [client 41.66.255.11$
[Fri Jan 12 14:20:53.223902 2018] [mpm_prefork:notice] [pid 28502] AH00169: cau$
[Fri Jan 12 14:20:54.282339 2018] [mpm_prefork:notice] [pid 28570] AH00163: Apa$
[Fri Jan 12 14:20:54.282384 2018] [core:notice] [pid 28570] AH00094: Command li$
[Fri Jan 12 14:20:59.132006 2018] [core:alert] [pid 28575] [client 41.66.255.11$
[Fri Jan 12 14:27:17.987127 2018] [core:alert] [pid 28576] [client 41.66.255.11$
[Fri Jan 12 14:27:35.697848 2018] [core:alert] [pid 28574] [client 41.66.255.11$
[Fri Jan 12 14:28:05.961142 2018] [core:alert] [pid 28573] [client 188.166.189.$
</code></pre>

<p><strong>I found my last five errors having this as a  continuation since my errors are truncated and i dont know how to copy everything here</strong></p>

<pre><code> /var/www/html/.htaccess: Either all Options must start with + or -, or no Option may.
</code></pre>
","<ubuntu><apache-2.4>","2018-01-12 13:00:55"
"891988","access local network router page remotely through SSH","<p>I'm trying to access my router page remotely through apache.</p>

<p>I've installed apache with proxy-pass module </p>

<p>httpd.conf file</p>

<pre><code>ServerName localhost
Listen *:80
&lt;VirtualHost *:80&gt;
    &lt;Location /someurl &gt;
        # Password protection omitted for brevity
        ProxyPass http://192.168.10.0
        ProxyPassReverse http://192.168.10.0
    &lt;/Location&gt;
&lt;/VirtualHost&gt;
</code></pre>

<p>I'm trying to reach this page <a href=""http://192.168.10.0"" rel=""nofollow noreferrer"">http://192.168.10.0</a> remotely.</p>

<p>it's working but when I browse <a href=""http://myipaddress/someurl"" rel=""nofollow noreferrer"">http://myipaddress/someurl</a> </p>

<p>I get 503 Service Unavailable, I tried to change the local IP to google website but it redirects me to google website instead of showing it within my IP address. </p>

<p>I really don't know where the problem is. what i'm trying to do is reaching router page remotely to open some port.</p>

<h2>UPDATE</h2>

<p>when I've changed the IP address to 192.168.10.1 its keep connecting and redirect me to 192.168.10.1.</p>

<p>I've changed HTTP protocol to https but I get 500 Internal Server Error</p>

<h2>UPDATE 2</h2>

<p>When I call curl 192.168.10.1 from my terminal I get:</p>

<pre><code>&lt;html&gt;
&lt;head&gt;&lt;title&gt;302 Found&lt;/title&gt;&lt;/head&gt;
&lt;body bgcolor=""white""&gt;
&lt;center&gt;&lt;h1&gt;302 Found&lt;/h1&gt;&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx/1.4.7&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>but not on my browser.</p>
","<networking><apache-2.4>","2018-01-12 21:26:11"
"753628","Why when i use netstat while not connected to internet, it returns some ip addresses?","<p>I read that this command  displays network connections for the Transmission Control Protocol  more info <a href=""https://en.wikipedia.org/wiki/Netstat"" rel=""nofollow noreferrer"">link netstat</a>
,but when i not connected to internet and i turn off XAMMP
server,it returns some ip:</p>

<pre><code>PS F:\Users\ROOT&gt; netstat -a

Active Connections

Proto  Local Address          Foreign Address        State   TCP   
0.0.0.0:111            ROOT-PC:0              LISTENING   TCP    
0.0.0.0:135            ROOT-PC:0              LISTENING   TCP    
0.0.0.0:445            ROOT-PC:0              LISTENING   TCP    
0.0.0.0:902            ROOT-PC:0              LISTENING   TCP    
0.0.0.0:912            ROOT-PC:0              LISTENING   TCP
0.0.0.0:1025           ROOT-PC:0              LISTENING   TCP    
0.0.0.0:1026           ROOT-PC:0              LISTENING   TCP    
0.0.0.0:1027           ROOT-PC:0              LISTENING   TCP    
0.0.0.0:1028           ROOT-PC:0              LISTENING   TCP   
0.0.0.0:1098           ROOT-PC:0              LISTENING   TCP    
0.0.0.0:2809           ROOT-PC:0              LISTENING   TCP    
0.0.0.0:3162           ROOT-PC:0              LISTENING   TCP    
0.0.0.0:11177          ROOT-PC:0              LISTENING   TCP    
0.0.0.0:46680          ROOT-PC:0              LISTENING   TCP    
127.0.0.1:1047         ROOT-PC:0              LISTENING   TCP    
127.0.0.1:1654         ROOT-PC:1655           ESTABLISHED   TCP    
127.0.0.1:1655         ROOT-PC:1654           ESTABLISHED   TCP    
127.0.0.1:8118         ROOT-PC:0              LISTENING   TCP    
127.0.0.1:8118         ROOT-PC:13259          TIME_WAIT   TCP    
127.0.0.1:8118         ROOT-PC:13260          TIME_WAIT   TCP    
127.0.0.1:8118         ROOT-PC:13271          TIME_WAIT   TCP    
127.0.0.1:8118         ROOT-PC:13278          TIME_WAIT   TCP    
127.0.0.1:8118         ROOT-PC:13280          TIME_WAIT   TCP    
127.0.0.1:8118         ROOT-PC:13285          TIME_WAIT   TCP    
127.0.0.1:8118         ROOT-PC:13287          ESTABLISHED   TCP    
127.0.0.1:8118         ROOT-PC:13288          ESTABLISHED   TCP    
127.0.0.1:13267        ROOT-PC:8118           TIME_WAIT   TCP    
127.0.0.1:13269        ROOT-PC:8118           TIME_WAIT   TCP    
127.0.0.1:13286        ROOT-PC:31620          TIME_WAIT   TCP    
127.0.0.1:13287        ROOT-PC:8118           ESTABLISHED   TCP    
127.0.0.1:13288        ROOT-PC:8118           ESTABLISHED   TCP    
127.0.0.1:13290        ROOT-PC:8118           TIME_WAIT   TCP    
127.0.0.1:31620        ROOT-PC:0              LISTENING   TCP    
192.168.86.1:139       ROOT-PC:0              LISTENING   TCP    
192.168.240.1:139      ROOT-PC:0              LISTENING   TCP    
[::]:135               ROOT-PC:0              LISTENING   TCP   
[::]:445               ROOT-PC:0              LISTENING   TCP   
[::]:1025              ROOT-PC:0              LISTENING   TCP   
[::]:1026              ROOT-PC:0              LISTENING   TCP   
[::]:1027              ROOT-PC:0              LISTENING   TCP   
[::]:1028              ROOT-PC:0              LISTENING   TCP   
[::]:1098              ROOT-PC:0              LISTENING   UDP   
0.0.0.0:68             *:*   UDP    
0.0.0.0:111            *:*   UDP        
0.0.0.0:500            *:*   UDP    
0.0.0.0:3162           *:*   UDP    
0.0.0.0:4500           *:*   UDP    
0.0.0.0:5355           *:*   UDP    
0.0.0.0:11177          *:*   UDP    
127.0.0.1:1900         *:*   UDP    
127.0.0.1:51993        *:*   UDP    
127.0.0.1:61732        *:*   UDP    
127.0.0.1:64749        *:*   UDP    
192.168.86.1:137       *:*   UDP    
192.168.86.1:138       *:*   UDP    
192.168.86.1:1900      *:*   UDP    
192.168.86.1:51991     *:*   UDP    
192.168.240.1:137      *:*   UDP    
192.168.240.1:138      *:*   UDP    
192.168.240.1:1900     *:*   UDP    
192.168.240.1:51992    *:*   UDP    
[::]:500               *:*   UDP    
[::]:4500              *:*   UDP    
[::]:5355              *:*   UDP  
[::1]:1900             *:*   UDP    
[::1]:51990            *:*   UDP  
[fe80::1530:4527:9f71:8067%17]:1900  *:*   UDP   
[fe80::1530:4527:9f71:8067%17]:51988  *:*   UDP   
[fe80::7d8b:c704:eab8:f26%12]:546  *:*   UDP   
[fe80::cda6:9321:d50:51d7%18]:1900  *:*   UDP   
[fe80::cda6:9321:d50:51d7%18]:51989  *:*
</code></pre>

<p>These ips and ports and protocols, what does this mean?
I am not connected to internet!!!!</p>

<p>thanks for reply.</p>
","<ip><port><internet><netstat>","2016-02-03 04:13:43"
"753716","How to remove a Domain Controller from a Forest to a Single Domain","<p>I have a server which is a domain controller in a Forest. I want to remove that server from the forest and place it in its own domain so it becomes a single domain controller.</p>

<p>So say we have eight domain controllers in domain A I want to remove one domain controller from domain A and create domain B with the single domain controller on a completely separated network.</p>

<p>I've done a bit of searching and found tips on how to demote or transfer from one forest to another but this doesnt quite fit my scenario.</p>

<p>Any tips ?</p>
","<domain><migration><transfer>","2016-02-03 12:58:10"
"753780","Nginx Not Respecting File Permissions","<p>I'm trying to apply permissions of <code>600</code> to certain files on my web server, but they are still accessible to the world (I can access them by navigating to direct link in browser e.g. <code>domain.com/test.txt</code>)!</p>

<p>The owner of these files is <code>www-data:www-data</code>. Even when I set permissions to <code>000</code>, the files are still accessible to the world (akin to permissions <code>777</code>)!</p>

<p>The only way I can make files inaccessible to the world is to apply <code>chown root:root</code> these files, but then <code>www-data</code> can't access them!</p>

<p>How can I make Nginx respect file permissions?</p>

<p>I'm using Nginx 1.8.1 and PHP 5.6.17.</p>
","<nginx><permissions><file-permissions><user-permissions>","2016-02-03 16:50:41"
"892243","How big is grub2?","<p>Due to ""Meltdown"" I am in the progress of moving our CentOS7-XEN-PV-DomUs to HVM mode. <a href=""https://xen-orchestra.com/blog/meltdown-and-spectre-for-xenserver/"" rel=""nofollow noreferrer"">See here on why I need to migrate to HVM</a></p>

<p>This involves installing a bootloader - unnessesary on PV-DomUs, since they use <strong>pygrub</strong> to emulate the grub/grub2 boot process.</p>

<p>What works: Boot the HVM DomU via PXE into a rescue CentOS7 image, chroot the disks, grub2-install. I can not do this from the Dom0 - since that is SLES11 SP4 - with grub, not grub2.</p>

<p>I would like to automate this by just copying the grub2-sectors via dd from an alredy converted VM to a VM which still needs the step.</p>

<p>Question: How big is grub2 - so how many bytes do I have to copy?</p>

<p>I already found out that 62 sectors with sector-size 512 byte is not enough.</p>

<p><strong>Update</strong> 2018-01-22:
The first answer hinted a potential problem.</p>

<p>In my scenario you will have a uniform disk-layout with identical first partition, starting at about 9 MiB. On that partition /boot is located.</p>

<p>I could do what I wanted by:</p>

<pre><code>sfdisk -d ALREADYCONVERTEDDISK &gt;oldpart.sfdisk
dd if=VM-DISK-WITH-GRUB2 of=NEWDISK bs=1M count=8
sfdisk ALREADYCONVERTEDDISK &lt;oldpart.sfdisk
</code></pre>

<p>The the question boils down to:
What would be the minimal space needed for <strong>grub2</strong>?</p>
","<centos7><xen><grub2><boot-loader><sles11>","2018-01-15 12:03:49"
"892270","VMWare - VM won't start - 5 (Input/output error) - failed drive","<p>Basics:
Server with 4 drives, 2 solid state. One of the solid state drives appears to have failed. Running VMWare 5.0</p>

<p>We tried to distribute the VMs over several disks and using RAID, but I'm not sure if it was setup incorrectly. We tried to ensure that if one of the disks ever failed, we would still be ok. However, it may have had opposite effect. Here is the startup error:</p>

<pre><code>Failed to start the virtual machine.
Module DiskEarly power on failed. 
Cannot open the disk '/vmfs/volumes/54d9758a-23d4381c-9118-40167e7bd317/atlassian.somedomain.com/atlassian.somedomain.com_9-000003.vmdk' 
or one of the snapshot disks it depends on. 
5 (Input/output error)
</code></pre>

<p>under properties for the VM, I can see: 
<a href=""https://i.sstatic.net/RiIwc.png"" rel=""nofollow noreferrer"">Shows Disabled Drive in Settings</a></p>

<p>Here are the drives when SSH'ing into the VMWare server:
<a href=""https://i.sstatic.net/XFTh5.png"" rel=""nofollow noreferrer"">Shows list of available drives</a></p>

<p>Here are the contents of HDD1:
<a href=""https://i.sstatic.net/56ZON.png"" rel=""nofollow noreferrer"">Contents of HDD1's folder</a></p>

<p>Contents of HDD2:
<a href=""https://i.sstatic.net/LkNWZ.png"" rel=""nofollow noreferrer"">Contents of HDD2's folders</a></p>

<p>Contents of SSD1:
<a href=""https://i.sstatic.net/QgvfB.png"" rel=""nofollow noreferrer"">Contents of SSD1's folder</a></p>

<p>Finally, When I look at SSD1s’ atlassian.somedomain.com.vmx file, I can see:</p>

<p><a href=""https://i.sstatic.net/PHZVZ.png"" rel=""nofollow noreferrer"">Contents of VMX file</a></p>

<p>Note the reference to SSD2 (54d9758a-23d4381c-9118-40167e7bd317) looking for atlassian.somedomain.com_9-000003.vmdk</p>

<p>What’s strange is that some of the other VMs don’t have the same problem, even if they do share files on that same failed drive.</p>

<p>I'm not sure how to proceed, and before I make a 'final' error, I wanted to get feedback on next steps.</p>

<p>I could:</p>

<p>1) Delete the affected Hard Disk from the VM's Hardware list:
<a href=""https://i.sstatic.net/8Fmig.png"" rel=""nofollow noreferrer"">delete drive</a></p>

<p>2) Alter SSD1s’ atlassian.somedomain.com.vmx file, instead pointing to version _8 (instead of the missing 9)</p>

<p>3) Any other suggestions?</p>

<p><strong><em>NOTE: The purple you see in the images is my covering up of the actual domain name.</em></strong></p>

<p><strong><em>EDIT: Note that I understand I may end up losing _10, _11 if they are all interdependent - as I may have to move all back to _8. If need be, so be it. I just need to get as much recovered as possible.</em></strong> </p>
","<vmware-esxi><vmware-vsphere><io>","2018-01-15 15:37:50"
"753885","Enable an external website to resolve to a page on my parent website, keeping URL the same","<p>I have a domain, www.example.COM. I have a page on that domain, www.example.com/page. I have another domain, www.example.NET and I want that domain to resolve to the page first mentioned, www.example.com/page</p>

<p>So basically when someone goes to www.example.net it resolves to www.example.com/page but the url remains www.example.net in the address bar.</p>

<p>So far, I can successfully make www.example.net resolve to www.example.com (the home page) through setting the DNS at the registrar. Now that I can do that, is it something with virtual hosts, .htaccess, apache config or something else on my server whereas I can specifically make this domain-b.com resolve to this specific page on domain.com?</p>
","<apache-2.2><virtualhost><.htaccess>","2016-02-04 01:32:59"
"753905","Seperating web server from database over public network","<p>How much of a risk is it splitting nginx web server from mysql database server and having them talk over public network? The database server would be in the same OVH DC but from what I understand there would latency in the equation. The ping between two servers is 0.2ms which seems to be low. I am trying to decide whether to keep everything together and move to a bigger server or move DB away from web server to another server in same DC but there may be network delays which could cause connection issues for users? Important info to note is that server(SQL to be specific) is running out of resources at 12k users so a temporary solution while a proper infrastructure is built is to choose between two options mentioned previously.</p>
","<linux><nginx><web-server><architecture><ovh>","2016-02-04 04:11:17"
"754194","Will CSR become invalid","<p>I have a domain registered with <code>namecheap</code> and server instance is <code>aws ec2</code> (windows machine running node js) , cname record pointing to ec2 instance's public ip.</p>

<p>Now i want to configure ssl on it , I already have a Positive SSl(comodo) in my namecheap product list, which says <em>Enter CSR</em>.</p>

<p>However for some reasons I may need to change my ec2 instance, which inturn will change it's public ip, in that case <strong>will that CSR or ssl cert become invalid</strong> ?</p>

<p>If yes then is there any workaround ?</p>

<p><em>For generating csr i found this <a href=""https://support.comodo.com/index.php?/Default/Knowledgebase/Article/View/1/19/csr-generation-using-openssl-apache-wmod_ssl-nginx-os-x"" rel=""nofollow noreferrer"">link</a></em></p>

<p><em>Sorry if it's something pretty basic but I am new to this</em></p>

<p>Thanks</p>

<p>EDIT: If possible Plz point me to some good resources where I can understand all this</p>
","<amazon-ec2><ssl><csr><namecheap>","2016-02-05 06:26:13"
"754411","Hiding parameters on url with nginx","<p>I have a website with this kind of adresses: </p>

<p>www.mydomain.com/user/userprofile/[userprofile1]?r=[login]&amp;w=[encryptedpassword] </p>

<p>ie: </p>

<p>www.mydomain.com/user/userprofile/toto?r=reqqfdvca&amp;w=skvlsqmg 
www.mydomain.com/user/userprofile/titi?r=re45a&amp;w=slkvldfgmg 
www.mydomain.com/user/userprofile/tutu?r=reqq0krgca&amp;w=s46893ls 
etc... </p>

<p>the parameters are important because there are credentials to access the website but I would like my adresses to look like: </p>

<p>www.mydomain.com/user/userprofile/toto
www.mydomain.com/user/userprofile/titi
www.mydomain.com/user/userprofile/tutu
etc... </p>

<p>I'm new with nginx, and my english is not perfect so please excuse my mistakes, also I'm not familiar at all with regex (that's why I'm writing this url-rewriting question).
but please does anyone have an idea on how to do that?</p>

<p>thanks</p>

<hr>

<p>The point is that I'm afraid it won't be possible to POST these vars.
I mean: to access my website each user may scan a QR code.
This QR code contain a shortened url that redirect to this kind of url (those with credential)
My joomla website detects credential in the url and automaticaly logs the user in.</p>

<p>That's why i would like to hide this vars</p>
","<nginx><url-routing>","2016-02-06 09:52:36"
"754625","Default gateway and ARP confusion","<p>I have the following configuration (I'd post a pic but not enough rep for more than 2):</p>

<p>PC1 (10.0.2.10/24) and PC2 (10.0.2.137/29) are connected to a hub, which is connected to a router on interface FastEthernet0/1 (10.0.2.138/24).</p>

<p>I ran two tests with this configuration.</p>

<p>First test: I sent a ping from PC1 to PC2, having set no default gateways for either host. PC1 thinks that PC2 is in the same subnet, so it sends out an ARP broadcast. It is not though, so it does not receive any replies. So far so good. <a href=""https://i.sstatic.net/tTEaE.png"" rel=""nofollow noreferrer"">Result</a></p>

<p>Second test: I set the router as the default gateway in PC1 and PC2's routing tables. I then sent a ping from PC1 to PC2 once again. This time, PC2 seems to reply to PC1's ARP broadcast directly, and PC1 proceeds to send PC2 the ping request. PC2 then uses its routing table to reply to PC1 via the default gateway and then gets an ICMP redirect to update its routing cache to reply directly to PC1 instead. <a href=""https://i.sstatic.net/zwbio.png"" rel=""nofollow noreferrer"">Result</a></p>

<p>My confusion: how does PC2 reply to PC1's ARP broadcast when it's on a different subnet? How does having a default gateway allow this? At first I thought Proxy ARP is being used, but that doesn't seem to be the case, as the source and destination addresses of the ARP packets are those of PC1 and PC2, not the router's.</p>
","<ip><linux-networking><ping><arp><icmp>","2016-02-08 02:21:27"
"755146","HP Array P410, HDD changed, but still predict to fail soon","<p>On HP Proliant DL G6, one disk in RAID 1 on P410 array broken. It was HP EG0300FBDBR. I changed it on compatible HP model - HP EG0300FAWHV. (both 300 Gb, 10K)</p>

<p>But HP Array Configuration Utility still show me - that new HP EG0300FAWHV - 300 GB 2-Port SAS Drive at Port 1I : Box 1 : Bay 0 is predicted to fail soon.</p>

<p>New disk in server blinking green - is like ""The drive is rebuilding, erasing, or it is part of an array that is undergoing capacity expansion or stripe migration.""</p>

<p>But two days gone and status didn't change.</p>

<p>In RIS Event Log, last info:</p>

<p>*> Event 123 2016-02-08 11:56:54 Hot Plug Physical Drive Change Removed.</p>

<blockquote>
  <p>Physical drive number: 0x09. Configured drive flag: 1. Spare drive
  flag: 0. Big drive: 0x00000009. Enclosure Box: 00. Bay: 00  Event 124
  2016-02-08 12:16:57 Hot Plug Physical Drive Change Inserted. Physical
  drive number: 0x09. Configured drive flag: 1. Spare drive flag: 0. Big
  drive: 0x00000009. Enclosure Box: 00. Bay: 00  Event 125 2016-02-08
  12:16:57 Logical Drive Status State change. State change, logical
  drive 0x0000. Previous logical drive state (0x03): Logical drive is
  degraded. New logical drive state (0x04): Logical drive is ready for
  recovery operation. Spare status (0x00): No spare configured  Event
  126 2016-02-08 12:16:57 Logical Drive Status State change. State
  change, logical drive 0x0000. Previous logical drive state (0x04):
  Logical drive is ready for recovery operation. New logical drive state
  (0x05): Logical drive is currently recovering. Spare status (0x00): No
  spare configured  Event 127 2016-02-08 12:51:51 Logical Drive Status
  State change. State change, logical drive 0x0000. Previous logical
  drive state (0x05): Logical drive is currently recovering. New logical
  drive state (0x00): Logical drive OK. Spare status (0x00): No spare
  configured  Event 128 2016-02-09 03:23:22 Logical Drive Surface
  Analysis Surface Analysis pass information. Block count: 00000000.
  Drive No: 00. Starting Address: 00000848:00000000.*</p>
</blockquote>

<p>I attached ADU report and screen.</p>

<p>Is it mean that new disk still recovering? But why its status as ""predict to fail"" and why so long for recovering? Why HP utility didn't mark it as rebuilding?</p>

<p>Report ADU: <a href=""https://www.dropbox.com/s/70ucdsiafzdwvfr/ADUReport.zip?dl=0"" rel=""nofollow noreferrer"">https://www.dropbox.com/s/70ucdsiafzdwvfr/ADUReport.zip?dl=0</a></p>

<p><a href=""https://i.sstatic.net/MBmTz.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MBmTz.jpg"" alt=""enter image description here""></a></p>
","<raid><hp>","2016-02-10 01:14:35"
"823855","How to resume then send a program back to the background","<p>I know I can run something in the background by adding <code>&amp;</code> to it, such as <code>tail -f log.log</code></p>

<p>I can then resume or send the job to the foregrond with <code>fg</code></p>

<p>However, once I do this, how do I send it to the background again? <code>Ctl-z</code> stops the task, and <code>Ctl-c</code> kills it.</p>

<p>When I type <code>bg</code> nothing happens.</p>
","<background-process><background><job-control>","2017-01-03 10:51:31"
"755221","How to open shared folder with administrator previleges","<p>Using windows 7.
I am administrator.</p>

<p>I want to open folder that is share and I am able to Open the folder if I am logged in.</p>

<p>Now if Some user is logged in and I want to open that folder from that user's login, can I open the share folder to which user is not authorize, with my admin login?
Some thing like right click and open with admin.</p>
","<windows><networking>","2016-02-10 10:22:39"
"823897","I need to discover the proper order of drives to restart a server that are in a raid 5 configuration with 6 hard drives","<p>i have a Dell Power Edge 750 server  machine with 6 hard drives of same size and running on raid 5. unfortunately during a service process of our data center one of the junior team members pulled out all four hard drives without labeling the actual sequence. </p>

<p>now i have following questions in mind:</p>

<ol>
<li>can the raid controller automatically determine the actual sequence and rebuild the array without any data loss.</li>
<li>if not, is there any way to determine the actual order of hard drives</li>
</ol>

<p>i cannot afford to loose the data on my drives. Can anybody suggest some authentic way out of this situation.</p>

<p>Thanks</p>
","<raid>","2017-01-03 15:21:19"
"823965","Group Policy to enable file audit","<p>What am I missing here?  I'm trying to enable file auditing so I can see who deleted a file via security logs in event viewer.
I created the below group policy 
Computer Configuration > Windows Settings > Local Policies/Audit Policy > Audit Object Access. Enabled for success and failure.
The enabled checkbox is checked for the policy.  In the delegation tab the computer account I'm trying to set this up for has read and apply policy selected.
as well as authenticated users.</p>

<p>On the folder itself I've enabled auditing for ""Everyone"" for ""Delete subfolders and files"" as well as ""Delete""  Success and failure are setup for these.
gpresult shows the policy is applied
not sure if it matters but gpedit shows the policy is not applied.</p>

<p>Where else should this be set?</p>
","<windows-server-2008-r2><group-policy><audit><eventviewer>","2017-01-03 21:15:49"
"755320","Amazon ec2 server timeout","<p>I know this question has been asked many times before, but in my case none of the answers seem to solve the question. I have followed all of amazon's trouble shooting guide, but I'm still unable to connect to my amazon instance. Any suggestions what else I can do are appreciated.</p>

<p>I would love to put the screenshots but stack exchange's excessive policing doesn't allow me to do anything until I get 10 reputation points. If you give me one or two upvotes I can post the the rest of the details needed.</p>

<p>thanks</p>

<p><a href=""https://i.sstatic.net/KGOhp.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KGOhp.png"" alt=""Timeout""></a></p>

<p>[Instance][2]</p>

<p>[Security group allowing access from everywhere][3]</p>

<p>[Subnet][4]</p>

<p>[Routing table][5]</p>

<p>[Internet gateway][6]</p>

<p>[Network acl][7]</p>
","<amazon-ec2><timeout>","2016-02-10 16:17:40"
"755404","Ubuntu 14.04 Can Access LAN but not Internet (Verizon Fios)","<p>I have been struggling with this for quite some time. I am away from home a lot and have an Ubuntu 14.04 server (SSH, HTTP, Transmission, Plex) running with the proper ports forwarded through my Verizon Fios router (FiOS-G1100). We recently had a snow storm and for some reason the box that converts fiber optic data to coax had to be replaced by Verizon. Ever since this time, my server can only intermittently connect to the internet (and I can only intermittently connect to it from outside the network). The Fios router is 192.168.1.1. I get ping results like this (where 192.16.31.23 is verizon.com):</p>

<pre><code>$  ping -i 15 verizon.com | while read pong; do echo ""$(date): $pong""; done
Wed Feb 10 15:50:08 EST 2016: 64 bytes from 192.16.31.23: icmp_seq=142 ttl=59 time=9.62 ms
Wed Feb 10 15:50:23 EST 2016: From 192.16.31.23 icmp_seq=143 Destination Net Unreachable
Wed Feb 10 15:50:38 EST 2016: 64 bytes from 192.16.31.23: icmp_seq=144 ttl=59 time=10.0 ms
Wed Feb 10 15:50:53 EST 2016: From 192.16.31.23 icmp_seq=145 Destination Net Unreachable
Wed Feb 10 15:51:08 EST 2016: From 192.16.31.23 icmp_seq=146 Destination Net Unreachable
</code></pre>

<p>...and so on. Here is a traceroute result:</p>

<pre><code>$ traceroute 192.16.31.23
traceroute to 192.16.31.23 (192.16.31.23), 30 hops max, 60 byte packets
 1  192.168.1.1 (192.168.1.1)  0.387 ms  0.564 ms  0.738 ms
 2  192.168.1.1 (192.168.1.1)  0.925 ms !N  1.111 ms !N  1.288 ms !N
</code></pre>

<p>The only reason I can run these commands is because I have TeamViewer set up on one of my Windows machines and I can reliably SSH through there. I can successfully ping and connect to other machines on the LAN. My server is assigned to a static IP both through the router and in <code>/etc/network/interfaces</code></p>

<pre><code>$ cat /etc/network/interfaces
# This file describes the network interfaces available on your system
# and how to activate them. For more information, see interfaces(5).

# The loopback network interface
auto lo
iface lo inet loopback

# The primary network interface
auto eth0
iface eth0 inet static
address 192.168.1.5
netmask 255.255.255.0
gateway 192.168.1.1
</code></pre>

<p>The ports are forwarded correctly. My iptables are flushed. The rest of the computers on the LAN have no problem accessing the internet. What is going on here? Is there anything else I can do to diagnose this better? Could this be a router issue? I looked through all of the config on the router and didn't see anything limiting my server. I have also found that rebooting the router fixes the issue for about a minute (this is variable) before problems start to occur again.</p>

<p>Some other output that may be helpful:</p>

<pre><code>$ sudo route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         192.168.1.1     0.0.0.0         UG    0      0        0 eth0
192.168.1.0     0.0.0.0         255.255.255.0   U     0      0        0 eth0

$ ping 192.168.1.1
PING 192.168.1.1 (192.168.1.1) 56(84) bytes of data.
64 bytes from 192.168.1.1: icmp_seq=1 ttl=64 time=0.370 ms
64 bytes from 192.168.1.1: icmp_seq=2 ttl=64 time=0.342 ms

$ traceroute 192.168.1.1
traceroute to 192.168.1.1 (192.168.1.1), 30 hops max, 60 byte packets
 1  192.168.1.1 (192.168.1.1)  0.484 ms  0.709 ms  0.945 ms

$ ping 192.168.1.4
PING 192.168.1.4 (192.168.1.4) 56(84) bytes of data.
64 bytes from 192.168.1.4: icmp_seq=1 ttl=128 time=0.423 ms
64 bytes from 192.168.1.4: icmp_seq=2 ttl=128 time=0.431 ms

$ traceroute 192.168.1.4
traceroute to 192.168.1.4 (192.168.1.4), 30 hops max, 60 byte packets
 1  * * *
 2  * * *
 ...
30  * * *

$ sudo iptables -L
Chain INPUT (policy ACCEPT)
target     prot opt source               destination

Chain FORWARD (policy ACCEPT)
target     prot opt source               destination

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination
</code></pre>

<p>EDIT: Edited for clarity.</p>
","<ubuntu><networking>","2016-02-10 21:11:51"
"893898","VCenter has failed, DHCP server is down","<p>Our VCenter is crashing and technicians are trying to work on it, however this has resulted in the DHCP server failing, therefore none of the machines on the network can connect to the Domain controller or authenticate (except the people that were connected before hand and have not released their assigned IP address). </p>

<p>Does anyone have an idea on how to temporarily get people connected to the physical Domain controller (except the virtual DC, we also have physical one onsite)/allow them to authenticate? </p>
","<domain-controller><vmware-vcenter><dhcp-server>","2018-01-24 15:27:42"
"824087","Differences in Installation for nagios 3/4 on Ubuntu with apt-get or build in?","<p>I am new to nagios. I think my question is not only specific for nagios but overall regarding unix-systems. </p>

<p>There are many guides for Nagios Installation. Some of them use apt-get install nagios</p>

<p>Others use like build-in or  build-essential with compiler Installation?</p>

<p>What are the differences between these two installation? </p>

<p>What kind of effect could it have on my nagios installation or configuration?</p>
","<networking><monitoring><installation><shell>","2017-01-04 13:07:38"
"824169","Cisco 2948G - No Config Terminal","<p>Recently I inherited a ""<strong>WS-2948G</strong>"" and the <strong>configure terminal</strong> command is missing. What am I missing?</p>

<p>I think this may be some kind of distributed switch that I'm not completely competent with but there so many models of 2948's that I'm not certain. Let me know if you need commands ran.</p>

<h2>show version</h2>

<pre><code>Console&gt; show version
WS-C2948 Software, Version NmpSW: 7.6(2)
Copyright (c) 1995-2003 by Cisco Systems, Inc.
NMP S/W compiled on Jun 25 2003, 23:00:25
GSP S/W compiled on Jun 25 2003, 17:11:56

System Bootstrap Version: 6.1(5)

Hardware Version: 2.2  Model: WS-C2948  Serial #: JAB04100JWL

Mod Port Model              Serial #              Versions
--- ---- ------------------ -------------------- ---------------------------------
1   0    WS-X2948           JAB04100JWL          Hw : 2.2
                                                 Gsp: 7.6(2.0)
                                                 Nmp: 7.6(2)
2   50   WS-C2948G          JAB04100JWL          Hw : 2.2

       DRAM                    FLASH                   NVRAM
Module Total   Used    Free    Total   Used    Free    Total Used  Free
------ ------- ------- ------- ------- ------- ------- ----- ----- -----
1       65536K  38095K  27441K  12288K  10438K   1850K  480K  308K  172K

Uptime is 0 day, 5 hours, 45 minutes
</code></pre>

<h2>show system</h2>

<pre><code>Console&gt; show version
PS1-Status PS2-Status
---------- ----------
ok         none

Fan-Status Temp-Alarm Sys-Status Uptime d,h:m:s Logout
---------- ---------- ---------- -------------- ---------
ok         off        ok         0,00:53:48     20 min

PS1-Type          PS2-Type
----------------- -----------------
internal 120AC    none

Modem   Baud  Traffic Peak Peak-Time
------- ----- ------- ---- -------------------------
disable  9600   0%      0% Wed Jan 4 2017, 17:40:53

Power Capacity of the Chassis: 1 supply

System Name              System Location          System Contact           CC
------------------------ ------------------------ ------------------------ ---
</code></pre>

<h2>show boot</h2>

<pre><code>Console&gt; show boot
BOOT variable = bootflash:cat4000-k8.7-6-2.bin,1;
CONFIG_FILE variable = bootflash:switch.cfg

Configuration register is 0x2
ignore-config: disabled
auto-config: non-recurring
console baud: 9600
boot: image specified by the boot system commands
</code></pre>

<p>What am I missing here? Thanks a lot!</p>
","<networking><cisco>","2017-01-04 17:02:28"
"894351","Changing existing NAT rule on Cisco ASA","<p>This is the exact line I want to edit</p>

<pre><code>nat (inside,outside) source static thatplace thatplace destination static thisplace thisplace no-proxy-arp route-lookup
</code></pre>

<p>All I want to do is enable <code>proxy-arp</code>. 
How can I do that?</p>
","<networking><firewall><cisco><cisco-asa>","2018-01-26 22:14:52"
"824541","Not to show the external I.P address in viewing in mobile","<p>I created subdomain in go daddy account, and I pointed to i.p address of GCE google cloud computing engine.</p>

<p>In DNS Management, I click add button </p>

<p>type = A
host = nameofsubdomain //
point to = i.p address
ttl = 1 hourd //default</p>

<p>in forwarding under subdomain section, I click add button</p>

<p>subdomain = mysubdomain</p>

<p>forward to = htt://i.p address</p>

<p>forward type = 301 (permanent)</p>

<p>settings = forward only</p>

<p>when I tried in url, at first it works fine the name of subdomain is visible in url, after couple of hour or minuetes? when I tried again in url my subdomain will turn into i.p address. how to show name of subdomain I don't want to show i.p addresss.</p>

<p>Thank you in advance.</p>
","<domain-name-system><godaddy>","2017-01-06 02:11:37"
"894383","Redirect all HTTPS requests to HTTP temporary in Apache on Ubuntu Server","<p>My site was hosted in an hosted provider which had enabled certificates so the website in the google results is appearing with the <code>https://</code> in the beginning.</p>

<p>Since, yesterday I have setup my site in a VM (<code>Ubuntu Server 16.04</code> with Apache2) which currently don't have any enabled certificate for the domain. The <code>.conf</code> file is like :</p>

<pre><code>&lt;VirtualHost *:80&gt;

        ServerAdmin  myemail@example.com
        ServerName example.com
        ServerAlias www.example.com
        DocumentRoot /var/www/html/mysite

        ErrorLog ${APACHE_LOG_DIR}/mysite-error.log
        CustomLog ${APACHE_LOG_DIR}/mysite-access.log combined

&lt;/VirtualHost&gt;
</code></pre>

<p>Is possible to edit the <code>.conf</code>. Is it possible to redirect all the traffic from https to http?</p>
","<apache-2.2><virtualhost><https><ubuntu-16.04>","2018-01-27 09:24:07"
"755988","Install CentOS 7 on N54L","<p>currently Im using Scientific Linux 7.2 on my HP Microserver N54L.</p>

<p>To get more ""mainstream"" and out of curiosity (and to try out my ansible-playbooks), I wanted to install CentOS 7 on it.</p>

<p>So I downloaded the current iso-image, dd'd it on my usb drive and bootet from it, but after selecting ""Check disk &amp; Install"" or only ""Install"", I get several errors. These happen directly afterwards.</p>

<p>Please look at <a href=""https://i.sstatic.net/meUIm.jpg"" rel=""nofollow noreferrer"">this error messages</a> The difference is the following boot parameter:</p>

<pre><code>initcall_blacklist=clocksource_done_booting
</code></pre>

<p>I got it from <a href=""https://bugs.centos.org/view.php?id=9860"" rel=""nofollow noreferrer"">this bug report</a></p>

<p>I have no idea, how to approach the problem.</p>

<p>The same usb drive works fine on my laptop.</p>

<p>kind regards,
pwe</p>
","<linux><centos><hardware><installation>","2016-02-13 10:40:19"
"894402","Rolled back SSL config but server still broken","<p>Access server through IP:
/var/www/ip/</p>

<p>Access server through Domain:
/var/www/domain/</p>

<pre><code>ServerRoot ""/etc/httpd""
DocumentRoot ""/var/www/ip""
Listen 80

&lt;VirtualHost *:80&gt;
DocumentRoot ""/var/www/domain""
ServerName example.com
&lt;/VirtualHost&gt;
</code></pre>

<p>After activating SSL with letsencrypt, server was returning 404 for any domain subfolder file. So I rolled back everything.</p>

<p>I used MELD to compare the /etc/httpd files and directory structure, everything is like before.</p>

<p>But it doesn't work like before. Access through ip returns 404 on /var/www/ip files.</p>

<p>I restarted apache, the server, what config files am I missing that I need to revert? /Conf.d/ already checked.</p>
","<apache-2.4><ssl-certificate><centos7>","2018-01-27 13:22:57"
"755990","Memcached server with multiple webservers","<p>I'm struggling for days now, I'm setting up my cloud infrastructure: 
1 nginx Loadbalancer
2 Centos Webservers
1 Centos Memcached server</p>

<p>On the webservers I've installed the php-pecl-memcached packages and made the necessary adjustments to php.ini. The webservers connect to the memcached server over a private network:  </p>

<pre><code>session.save_handler = memcached
session.save_path = ""10.xx.xx.xx:11211""
</code></pre>

<p>The webservers are capable of connecting, tested it with <code>telnet 10.xx.xx.xx 11211</code> and the memcached server is able to output the statistics when you type stats. </p>

<p>Now trying to check if Memcached works as it should results in a PHP error:</p>

<pre><code>HP Warning:  Unknown: Failed to write session data (memcached). Please verify that the current setting of session.save_path is correct (/var/lib/php/session) in Unknown on line 0, referer: https://example.com
</code></pre>

<p>It looks like the session.save_path as mentioned above isn't used, while it should be accessible, see telnet test. </p>
","<centos><memcached>","2016-02-13 11:14:49"
"824853","Apache persists on not wanting to redirect to https","<p>I know many people have ask similar not to say the same question before:</p>

<p><strong>How can I redirect http to https (Using Apache)?/Why does it not do so?</strong></p>

<p>There is even a ""code"" snippet on the <a href=""https://en.wikipedia.org/wiki/HTTP_301"" rel=""nofollow noreferrer"">Wikipedia</a> page of HTTP 301 status code that redirects to https.</p>

<p>But after hours of searching I tried many different methods out of with only one is working: </p>

<pre><code>&lt;meta http-equiv=""refresh"" content=""https://example.org""&gt;
</code></pre>

<p>as a default page at <code>/var/www/html/index.html</code></p>

<p>Even though it successfully redirects it is more than ugly and also not really liked by search engines and increases loading time.</p>

<p>To get us all on one level <a href=""http://pastebin.com/2MEpvVzB"" rel=""nofollow noreferrer"">this</a>
 is my Apache configuration. I also use Django in combination with Apache in case that has any relevance. </p>

<p>So here is a listing of all my attempts:</p>

<ol>
<li><p>Use another VirtualHost</p>

<pre><code>&lt;VirtualHost *:80&gt;
    Redirect permanent / https://example.org/
&lt;/VirtualHost&gt;
</code></pre></li>
<li><p>Use a rewrite that redirect everything that not connects on port 443 to the corresponding https page:</p>

<pre><code>RewriteEngine On
RewriteCond %{SERVER_PORT} !443
RewriteRule ^(/(.*))?$ https://%{HTTP_HOST}/$1 [R=301,L]`
</code></pre></li>
<li><p>Use the implementation mentioned from Wikipedia:</p>

<pre><code>RewriteEngine On
RewriteCond %{HTTPS} off
RewriteCond %{HTTP_HOST} ^www\.(.*)$ [NC]
RewriteRule ^(.*)$ http://%1/$1 [R=301,L]
RewriteCond %{HTTPS} on
RewriteCond %{HTTP_HOST} ^www\.(.*)$ [NC]
RewriteRule ^(.*)$ https://%1/$1 [R=301,L]
RewriteEngine On
RewriteCond %{SERVER_PORT} 80
RewriteRule ^(.*)$ https://example.org/$1 [R,L]
</code></pre></li>
</ol>

<p>I tried these options (where it made sense) in both <code>apache2.conf</code> and <code>.htaccess</code> files. Obviously with no success. Weirdly enough when I visit example.org and then after it has loaded manually type example.org/# the last two methods worked. I have tried also with only example.org/ and the same procedure but without #. But that does not work.</p>

<p>What other options are there (for a 301-Redirect)? Why does this not work? And if it possible: How can I modify it to get it working?</p>
","<apache-2.4><django><301-redirect>","2017-01-07 21:41:25"
"757296","EC2 - Disable CPU Turbo boost technology","<p>I've got a bunch of EC2 instances - I want to disable a CPU feature on one of them (i.e. turbo boost on CPU).</p>

<p>is this possible to do?</p>

<p>I assume I need BIOS access but I can't see anything about it online.</p>

<p>Amazon don't provide 'technical' support for 'basic' plans.</p>

<p>Any ideas?</p>

<p>If I could even do it at OS level it would be good (Windows Server 2012).</p>
","<windows><amazon-web-services><central-processing-unit>","2016-02-15 13:41:25"
"824903","visudo + how to enable user to delete any file under /var/log/http","<p>I want to enable user - ""Ertop"" to delete any file or directory under  /var/log/http by update the visudo</p>

<p>I have redhat machine version 6.x</p>

<p>user name is Ertop</p>

<p>I set the following in visudo but not sure it this syntax is ok</p>

<pre><code>Ertop ALL=(root) NOPASSWD: rm -rf /var/log/http/*
</code></pre>

<p>or</p>

<pre><code>Ertop ALL=(root) NOPASSWD: sudoedit /var/log/http/*
</code></pre>
","<linux><redhat><sudo>","2017-01-08 07:07:16"
"824904","Need to get an output value to a variable in powershell","<p>my Output is like this </p>

<pre><code>MPIO Disk    System Disk  LB Policy    DSM Name
-------------------------------------------------------------------------------
MPIO Disk0   Disk 1       RR           Microsoft DSM
</code></pre>

<p>how can we get only <code>LB policy</code> in a variable to get it in <code>csv</code> output</p>
","<powershell>","2017-01-08 07:14:00"
"824914","How does Apache spool STDOUT from a CGI script?","<p>As part of a content management system I'm developing I have a script which retrieves image files (JPEG, GIF, PNG etc) in response to the browser GETing a URL like <code>http://myserver/getimage.cgi/virtual/path/to/image</code>.  On the server the image files are stored outside DOCUMENT_ROOT as randomly-named blobs and a database keeps track of the metadata, in particular the correspondence between virtual path, blob filename and the MIME type.  The script looks like this:-</p>

<pre><code>#!/usr/bin/perl

use CGI::Simple;
use File::Copy;
use MYSTUFF 'dblookup';

my $q = new CGI::Simple;
my ($mimetype, $filepath) = dblookup($q-&gt;path_info);

$| = 1; # enable autoflush so header is output before calling copy()
print $q-&gt;header(-type=&gt;$mimetype);
copy($filepath, \*STDOUT);
</code></pre>

<p>The <code>dblookup</code> function is exported by MYSTUFF.pm and extracts the mimetype and filepath for the virtual path passed in via the standard CGI $PATH_INFO environment variable.</p>

<p>My concern is how the output of the CGI script is spooled by the Apache server before it starts sending it back to the browser.  If it spools the entire output then potentially there's a need for a huge amount of spool space on the server because the image files can be 10's or 100's of MB, and when I start supporting video files they could get up to GB.</p>

<p>Is the Apache server sensible enough to spool the STDOUT stream from the CGI script only up to the point where it's got all the headers (i.e. the first ""\n\n"", which is generated by <code>$q-&gt;header()</code>), then start copying the data buffer-for-buffer from STDOUT to whatever socket is attached to the HTTP connection back to the browser?  The documentation for <a href=""http://perldoc.perl.org/File/Copy.html"" rel=""nofollow noreferrer"">File::Copy</a> suggests that it will use a 1K buffer, so if Apache behaves in the way I've outlined then I don't really have a problem, do I, as the IPC sockets/pipes will enforce flow control on my CGI script, removing the need for any further spool space beyond the buffers that are already there??</p>
","<apache-2.4><cgi><ipc>","2017-01-08 10:57:00"
"757345","Return yaml value to puppet fact lookup","<p>I'm trying to resolve a YAML value in a module</p>

<pre><code>file { '/boot/config.txt' :
ensure      =&gt; present,
mode        =&gt; '0755',
content =&gt; $::hostname?{
     'serverName1.domain' =&gt; template('template1'),
     default =&gt; template('template2'),
     }
}
</code></pre>

<p>above with hardcoded names works nicely, however I'm trying to something like this:</p>

<pre><code>file { '/boot/config.txt' :
ensure      =&gt; present,
mode        =&gt; '0755',
content =&gt; $::hostname?{
     &lt;%= scope.call_function('hiera',['server_name_in_hiera']) %&gt; =&gt; template('template1'),
     default =&gt; template('template2'),
     }
}
</code></pre>

<p>I'm sure its doable and I'm just doing something stupid.</p>

<p>Thanks</p>
","<puppet><hiera><yaml>","2016-02-15 17:21:14"
"757369","SSG5 Firewall Chrome ERR_SSL_VERSION_OR_CIPHER_MISMATCH Workaround?","<p>We have an environment in which we have about a dozen SSG 5 Firewalls.  We administer these using the https web interface and have done so for several years.  Chrome is generally the browser we like to use but no longer supports earlier ciphers and we have not found any workaround as yet.</p>

<p>Obviously one solution is simply to upgrade all of our firewalls, which we fully expect to do within the next two years.  In the meantime, we use Firefox to administer them and have no difficulty with that approach.</p>

<p>However, our concern is that Mozilla and other browser authors will follow Google’s lead and remove support from their browsers for these earlier ciphers.  Although we’re retaining the current version of Firefox to mitigate such a scenario, it would leave us in a precarious position that we don’t want to be in.</p>
","<ssg5>","2016-02-15 19:27:38"
"824979","Power requirements for SAS enclosures","<p>I have a Supermicro 4u enclosure with dual 1400 watt power supply running on 220v.  There are 24 drive bays.  The power supply is rated at 4amps for the 5v output.  If I have 24 drives that requires .75 amps on the 5v, the math isn't adding up in terms of the power supplied.  Is the SAS expander board converting 12v to 5v to supply the extra 5v requirements above the rates 4amps?</p>
","<hardware><storage><sas><power-supply-unit>","2017-01-08 21:04:13"
"824987","Config Error in HAproxy","<p>I'm having an HAproxy as a loadbalancer on top of 2 backend servers.
I installed the SSL certificate and this is my config which gives me tons of errors when reloading HAproxy. Please help!</p>

<p>This is my configuration:</p>

<pre><code>    global
        log /dev/log    local0
        log /dev/log    local1 notice
        chroot /var/lib/haproxy
        user haproxy
        group haproxy
        tune.ssl.default-dh-param 2048
        daemon

defaults
        log     global
        mode    http
        option  httplog
        option  dontlognull
        timeout connect 5000
        timeout client 50000
        timeout server 50000
        errorfile 400 /etc/haproxy/errors/400.http
        errorfile 403 /etc/haproxy/errors/403.http
        errorfile 408 /etc/haproxy/errors/408.http
        errorfile 500 /etc/haproxy/errors/500.http
        errorfile 502 /etc/haproxy/errors/502.http
        errorfile 503 /etc/haproxy/errors/503.http
        errorfile 504 /etc/haproxy/errors/504.http

frontend kontentwebsite_in
        bind *:80
        mode http
        redirect location https://example.com/
        redirect prefix http://example.com code 301 if { hdr(host) -$
        default_backend exampleservers_http

frontend www-https
        bind *:443 ssl crt /etc/haproxy/example.crt
        reqadd X-Forwarded-Proto:\ https
        redirect prefix http://example.com code 301 if { hdr(host) -i www$
        default_backend exampleservers_http

    backend exampleservers_http
            redirect prefix  https code 301 if !{ ssl_fc }
            appsession PHPSESSID len 64 timeout 3h request-learn prefix
            balance roundrobin
            mode http
            server web1 ip:80 check
            server web2 ip:80 check
</code></pre>

<p>and the errors are as follow:</p>

<pre><code>root@....:~# service haproxy reload
 * Reloading haproxy haproxy                                                     [ALERT] 007/221946 (2288) : parsing [/etc/haproxy/haproxy.cfg:7] : unknown keyword 'tune.ssl.default-dh-param' in 'global' section
[ALERT] 007/221946 (2288) : parsing [/etc/haproxy/haproxy.cfg:30] : 'redirect': error detected while parsing redirect condition.
[ALERT] 007/221946 (2288) : parsing [/etc/haproxy/haproxy.cfg:34] : 'bind' only supports the 'transparent', 'defer-accept', 'name', 'id', 'mss' and 'interface' options.
[ALERT] 007/221946 (2288) : parsing [/etc/haproxy/haproxy.cfg:36] : 'redirect': error detected while parsing redirect condition.
[ALERT] 007/221946 (2288) : parsing [/etc/haproxy/haproxy.cfg:40] : 'redirect': error detected while parsing redirect condition.
[ALERT] 007/221946 (2288) : Error(s) found in configuration file : /etc/haproxy/haproxy.cfg
[ALERT] 007/221946 (2288) : Fatal errors found in configuration.
                                                                          [fail]
</code></pre>

<p>Just FYI - HAproxy Version</p>

<pre><code>HA-Proxy version 1.4.24 2013/06/17
Copyright 2000-2013 Willy Tarreau &lt;w@1wt.eu&gt;

Build options :
  TARGET  = linux2628
  CPU     = generic
  CC      = gcc
  CFLAGS  = -g -O2 -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security -D_FORTIFY_SOURCE=2
  OPTIONS = USE_PCRE=1

Default settings :
  maxconn = 2000, bufsize = 16384, maxrewrite = 8192, maxpollevents = 200

Encrypted password support via crypt(3): yes

Available polling systems :
     sepoll : pref=400,  test result OK
      epoll : pref=300,  test result OK
       poll : pref=200,  test result OK
     select : pref=150,  test result OK
Total: 4 (4 usable), will use sepoll.
</code></pre>
","<ubuntu><ssl><haproxy>","2017-01-08 21:51:24"
"894941","What can root read that a regular user cannot?","<p>A contractor is asking me to provide him with root access from day one.</p>

<p>Skipping the obvious ""no-no""... as he's arguing that in order to do a full review of the system and identifying SPOF/etc, he needs root access, <em>what can a root user read/see that a regular user cannot</em> (excluding the root folder and similar)?</p>

<p>To say it in another way: ""what can't a regular user read?"", ""what does he need root access for?""</p>

<p><strong>Please don't down vote without explaining why</strong></p>
","<linux><security><permissions>","2018-01-31 09:57:29"
"825153","Configuring OpenVPN and SSH access (for another country)","<p>So I am using OpenVPN to connect to an internal network. I have some users that require access from a different country (India). I created an SSH user and generated VPN certs for them to connect. </p>

<ol>
<li>They are able to connect to the VPN with no issues</li>
<li>They are able to ping the designated VM via its private static IP address</li>
<li>I had them do an nmap and they do see port 22 but it says ""filtered""</li>
<li>I've added rules to my firewall appliance (Sophos) to let all incoming SSH traffic through for testing purposes</li>
<li>They get a connection refused error when trying to connect to SSH</li>
<li>I can connect just fine using their SSH user (while VPNed from my workstation)</li>
<li>When I do a nmap from my workstation (in Canada), while VPNed in, I see the state of ssh to be ""Open"". When they do the same thing from India, they see ""Filtered"" as the SSH state.</li>
</ol>

<p>I know that opening up SSH connections is a security issue but I'd prefer to get these guys connected, and then add in the necessary security measures to secure/limit SSH logins (no root password, ssh keys only, etc) after the fact. </p>

<p>I'm a budding sysadmin so sorry if this is total noob territory. Essentially, I need to know what else I can do to diagnose this situation and determine why they are unable to connect. I have a feeling it's firewall related, but I don't have any country blocking enabled, and SSH is wide open so I am unsure what the issue is. </p>

<p>Thanks!</p>
","<ssh><openvpn>","2017-01-09 19:35:00"
"825183","DirectAccess DNS warning: Enterprise DNS servers (192.168.100.33,::1) used by DirectAccess clients for name resolution are not responding","<p>I'm attempting to set up DirectAccess Remote Access on a standalone Windows Server 2012 server (DC, DHCP, DNS etc.) are handled on another server. </p>

<p>I used the wizard but DNS kept throwing warning icons in the status viewer. I've tried setting it manually to the servers local IPv4 address, local IPv6 address, the domain controllers local IPv4 address, and also by deleting the entry and manually hitting Detect and then Verify (passed) but once saved the warning icon comes back with the same error message.</p>

<blockquote>
  <p>Enterprise DNS servers (192.168.100.33,::1) used by DirectAccess
  clients for name resolution are not responding. This might affect
  DirectAccess client connectivity to corporate resources.</p>
</blockquote>

<p><a href=""https://i.sstatic.net/342qu.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/342qu.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.sstatic.net/2m0zk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2m0zk.png"" alt=""enter image description here""></a></p>

<p>How do I sort it?</p>
","<vpn><windows-server-2012><windows-server-2012-r2><remote-access><direct-access>","2017-01-09 22:14:17"
"825197","AH00094: Command line: '/usr/sbin/apache2'","<p>i have installed ldap and phpldapadmin 3 weeks ago and everything was good. yesterday i wanted to configure web server, so the web server works fine, and the command</p>

<pre><code>service apache2 restart 
</code></pre>

<p>it gives as result :</p>

<pre><code> * Restarting web server apache2                                         [ OK ] 
</code></pre>

<p>but when i check the file   /var/log/apache2/error.log  i get : </p>

<pre><code>[Mon Jan 09 23:07:56.392253 2017] [mpm_event:notice] [pid 10447:tid 3074607744] AH00489: Apache/2.4.7 (Ubuntu) configured -- resuming normal operations
[Mon Jan 09 23:07:56.392355 2017] [core:notice] [pid 10447:tid 3074607744] AH00094: Command line: '/usr/sbin/apache2'
</code></pre>

<p>when this result appears in this file, i could not be able any more to access to my_IP_address/phpldapadmin, it gives as result 
""The requested URL /phpldapadmin/ was not found on this server. "" </p>

<p>Also:</p>

<pre><code>$ service apache2 status
* apache2 is running

$ pgrep apache
10642
10645
10646

$ telnet localhost 80
Trying 127.0.0.1...
Connected to localhost.
Escape character is '^]'.

$ ps -A | grep 'apache2'
 10642 ?        00:00:00 apache2
 10645 ?        00:00:00 apache2
 10646 ?        00:00:00 apache2
</code></pre>

<p>so any help ?</p>
","<apache-2.4><ldap><openldap><apache2>","2017-01-09 23:22:52"
"895104","Is TLS necessary for port 25 for MX delivery?","<p>I am in the process of implementing a new Postfix implementation on an existing environment which is extremely old. While doing so I am requiring all clients to connect securely on either 465 or 587 for relay access. </p>

<p>The existing postfix environment that I inherited has the following setting:</p>

<pre><code>smtpd_tls_security_level = may
</code></pre>

<p>According to Postfix documentation, which cites RFC 2487:</p>

<blockquote>
  <p>A publicly-referenced SMTP server MUST NOT require use of the STARTTLS
  extension in order to deliver mail locally. This rule prevents the
  STARTTLS extension from damaging the interoperability of the
  Internet's SMTP infrastructure. A publicly-referenced SMTP server is
  an SMTP server which runs on port 25 of an Internet host listed in the
  MX record (or A record if an MX record is not present) for the domain
  name on the right hand side of an Internet mail address.</p>
</blockquote>

<p>Given that I want outbound SMTP relay access to be exclusively performed on 465/587, my question is simply this:</p>

<p><strong>Is there any circumstance that TLS is used for local mail delivery for an MX host?</strong> (I have the required overrides in master.cf to enable ssl/tls, startls).</p>

<p>I can't find a definitive answer if other MTA's establish secure connections to MX hosts in order to deliver locally.</p>

<p>My goal is to prevent any relay access (Authenticated or otherwise) on port 25, however I do not want to disable tls security on 25 if it will interfere with local delivery from the internet.</p>

<p>Thanks in advance.</p>

<p>main.cf</p>

<pre><code>#
# Postscreen
#

#postscreen_dnsbl_whitelist_threshold = -1 # Pending Postfix 2.11

postscreen_access_list          = permit_mynetworks, cidr:/etc/postfix/postscreen/access.cidr
postscreen_greet_banner         = $myhostname [POSTSCREEN] ESMTP
postscreen_blacklist_action     = drop
postscreen_dnsbl_action         = enforce
postscreen_dnsbl_threshold      = 3
postscreen_greet_action         = enforce
postscreen_dnsbl_reply_map      = pcre:$config_directory/postscreen/dnsbl_reply_map.pcre
postscreen_dnsbl_sites          = zen.spamhaus.org*3
        b.barracudacentral.org*2
        bl.spameatingmonkey.net*2
        bl.spamcop.net
        dnsbl.sorbs.net
        psbl.surriel.com
        bl.mailspike.net
        #swl.spamhaus.org*-4
postscreen_whitelist_interfaces = static:all
postscreen_bare_newline_action  = enforce
postscreen_bare_newline_enable  = no
postscreen_non_smtp_command_enable = no
postscreen_pipelining_enable    = no

#
# Server
#

myhostname                      = relay1.sanitized.fqdn
mydomain                        = sanitized.fqdn
myorigin                        = $myhostname
mynetworks                      = 127.0.0.0/8, 192.168.0.0/24
mydestination                   = $myhostname, localhost.$mydomain, localhost
sendmail_path                   = /usr/sbin/sendmail.postfix
relay_domains                   =
local_recipient_maps            =
inet_interfaces                 = localhost, 192.168.0.12
inet_protocols                  = ipv4
strict_rfc821_envelopes         = yes
disable_vrfy_command            = yes
local_recipient_maps            =

#
# SMTPD
#

append_dot_mydomain             = no
biff                            = no
relayhost                       =
smtpd_banner                    = $myhostname ESMTP
#smtpd_client_restrictions      = sleep 5
smtpd_delay_reject              = no
smtpd_sender_restrictions       = reject_unknown_sender_domain
smtpd_error_sleep_time          = 30
smtpd_soft_error_limit          = 10
smtpd_hard_error_limit          = 20
smtpd_recipient_restrictions    = permit_sasl_authenticated, permit_mynetworks, reject_unauth_destination, permit
smtp_helo_timeout               = 5
#smtp_connect_timeout           = 5
smtp_host_lookup                = native


#
# SMTP-AUTH configuration
#
smtp_sasl_auth_enable           = yes
smtp_sasl_password_maps         = proxy:mysql:/etc/postfix/mysql/mailboxes.cf
smtpd_sasl_local_domain         = $mydomain
smtpd_sasl_auth_enable          = yes
smtpd_sasl_type                 = cyrus
smtpd_sasl_path                 = smtpd
smtpd_sasl_authenticated_header = yes
smtpd_sasl_security_options     = noanonymous
smtpd_sasl_tls_security_options = noanonymous
smtpd_sasl_exceptions_networks  = $mynetworks
broken_sasl_auth_clients        = yes
smtpd_sasl_authenticated_header = no
#smtp_sasl_mechanism_filter     = plain, login

#
# TLS configuration
#

smtpd_tls_security_level        = may
smtpd_tls_cert_file             = /etc/pki/tls/certs/godaddy.crt
smtpd_tls_key_file              = /etc/pki/tls/private/godaddy.key
smtpd_tls_CAfile                = /etc/pki/tls/certs/gd_bundle-g2-g1.crt
smtpd_tls_loglevel              = 1
smtpd_tls_received_header       = yes
smtpd_tls_session_cache_timeout = 10800s
smtpd_tls_session_cache_database = btree:/var/lib/postfix/smtpd_scache
tls_random_source               = dev:/dev/urandom
tls_random_exchange_name        = /var/lib/postfix/prng_exch
smtpd_tls_auth_only             = yes

#
# Virtual configuration
#

virtual_alias_domains           =
virtual_mailbox_base            = /
virtual_mailbox_domains         = proxy:mysql:/etc/postfix/mysql/domains.cf
virtual_mailbox_maps            = proxy:mysql:/etc/postfix/mysql/mailboxes.cf
virtual_alias_maps              = proxy:mysql:/etc/postfix/mysql/autoreply.cf, proxy:mysql:/etc/postfix/mysql/aliases.cf, proxy:mysql:/etc/postfix/mysql/groups.cf
virtual_minimum_uid             = 110532
virtual_uid_maps                = static:110532
virtual_gid_maps                = static:101
proxy_read_maps                 = $virtual_mailbox_maps $virtual_alias_maps $virtual_mailbox_domains $smtp_sasl_password_maps
</code></pre>

<p>master.cf</p>

<pre><code>#
# Postfix master process configuration file.  For details on the format
# of the file, see the master(5) manual page (command: ""man 5 master"").
#
# Do not forget to execute ""postfix reload"" after editing this file.
#
# ==========================================================================
# service type  private unpriv  chroot  wakeup  maxproc command + args
#               (yes)   (yes)   (yes)   (never) (100)
# ==========================================================================
#smtp      inet  n       -       n       -       1      smtpd 
smtp      inet  n       -       n       -       1       postscreen
smtpd     pass  -       -       n       -       -       smtpd
dnsblog   unix  -       -       n       -       0       dnsblog
tlsproxy  unix  -       -       n       -       0       tlsproxy
submission inet n       -       n       -       -       smtpd -v
  -o syslog_name=postfix/submission
  -o smtpd_tls_security_level=encrypt
  -o smtpd_sasl_auth_enable=yes
  -o smtpd_reject_unlisted_recipient=no
  -o smtpd_recipient_restrictions=permit_sasl_authenticated,reject
  -o milter_macro_daemon_name=ORIGINATING
  #-o smtpd_sender_restrictions=reject_sender_login_mismatch
  #-o smtpd_recipient_restrictions=reject_non_fqdn_recipient, reject_unknown_recipient_domain, permit_sasl_authenticated, reject
  #-o smtpd_client_restrictions=$mua_client_restrictions
  #-o smtpd_helo_restrictions=$mua_helo_restrictions
  #-o smtpd_sender_restrictions=$mua_sender_restrictions
  #-o smtpd_sender_login_maps=proxy:mysql:/etc/postfix/mysql_login_maps.cf
  #-o smtpd_sender_sasl_password_maps=proxy:mysql:/etc/postfix/mysql_login_maps.cf
smtps     inet  n       -       n       -       -       smtpd
  -o syslog_name=postfix/smtps
  -o smtpd_tls_wrappermode=yes
  -o smtpd_sasl_auth_enable=yes
  -o smtpd_reject_unlisted_recipient=no
  -o smtpd_recipient_restrictions=permit_sasl_authenticated,reject
  -o milter_macro_daemon_name=ORIGINATING
  #-o smtpd_client_restrictions=$mua_client_restrictions
  #-o smtpd_helo_restrictions=$mua_helo_restrictions
  #-o smtpd_sender_restrictions=$mua_sender_restrictions
#628       inet  n       -       n       -       -       qmqpd
pickup    unix  n       -       n       60      1       pickup
cleanup   unix  n       -       n       -       0       cleanup
qmgr      unix  n       -       n       300     1       qmgr
#qmgr     unix  n       -       n       300     1       oqmgr
tlsmgr    unix  -       -       n       1000?   1       tlsmgr
rewrite   unix  -       -       n       -       -       trivial-rewrite
bounce    unix  -       -       n       -       0       bounce
defer     unix  -       -       n       -       0       bounce
trace     unix  -       -       n       -       0       bounce
verify    unix  -       -       n       -       1       verify
flush     unix  n       -       n       1000?   0       flush
proxymap  unix  -       -       n       -       -       proxymap
proxywrite unix -       -       n       -       1       proxymap
smtp      unix  -       -       n       -       -       smtp
relay     unix  -       -       n       -       -       smtp
       -o smtp_helo_timeout=5 -o smtp_connect_timeout=5
showq     unix  n       -       n       -       -       showq
error     unix  -       -       n       -       -       error
retry     unix  -       -       n       -       -       error
discard   unix  -       -       n       -       -       discard
local     unix  -       n       n       -       -       local
virtual   unix  -       n       n       -       -       virtual
lmtp      unix  -       -       n       -       -       lmtp
anvil     unix  -       -       n       -       1       anvil
scache    unix  -       -       n       -       1       scache
policy    unix  -   n   n   -   0   spawn user=nobody
  argv=/bin/python /usr/libexec/postfix/policyd-spf
#
# ====================================================================
# Interfaces to non-Postfix software. Be sure to examine the manual
# pages of the non-Postfix software to find out what options it wants.
#
# Many of the following services use the Postfix pipe(8) delivery
# agent.  See the pipe(8) man page for information about ${recipient}
# and other message envelope options.
# ====================================================================
#
# maildrop. See the Postfix MAILDROP_README file for details.
# Also specify in main.cf: maildrop_destination_recipient_limit=1
#
#maildrop  unix  -       n       n       -       -       pipe
#  flags=DRhu user=vmail argv=/usr/local/bin/maildrop -d ${recipient}
#
# ====================================================================
#
# Recent Cyrus versions can use the existing ""lmtp"" master.cf entry.
#
# Specify in cyrus.conf:
#   lmtp    cmd=""lmtpd -a"" listen=""localhost:lmtp"" proto=tcp4
#
# Specify in main.cf one or more of the following:
#  mailbox_transport = lmtp:inet:localhost
#  virtual_transport = lmtp:inet:localhost
#
# ====================================================================
#
# Cyrus 2.1.5 (Amos Gouaux)
# Also specify in main.cf: cyrus_destination_recipient_limit=1
#
#cyrus     unix  -       n       n       -       -       pipe
#  user=cyrus argv=/usr/lib/cyrus-imapd/deliver -e -r ${sender} -m ${extension} ${user}
#
# ====================================================================
#
# Old example of delivery via Cyrus.
#
#old-cyrus unix  -       n       n       -       -       pipe
#  flags=R user=cyrus argv=/usr/lib/cyrus-imapd/deliver -e -m ${extension} ${user}
#
# ====================================================================
#
# See the Postfix UUCP_README file for configuration details.
#
#uucp      unix  -       n       n       -       -       pipe
#  flags=Fqhu user=uucp argv=uux -r -n -z -a$sender - $nexthop!rmail ($recipient)
#
# ====================================================================
#
# Other external delivery methods.
#
#ifmail    unix  -       n       n       -       -       pipe
#  flags=F user=ftn argv=/usr/lib/ifmail/ifmail -r $nexthop ($recipient)
#
#bsmtp     unix  -       n       n       -       -       pipe
#  flags=Fq. user=bsmtp argv=/usr/local/sbin/bsmtp -f $sender $nexthop $recipient
#
#scalemail-backend unix -       n       n       -       2       pipe
#  flags=R user=scalemail argv=/usr/lib/scalemail/bin/scalemail-store
#  ${nexthop} ${user} ${extension}
#
#mailman   unix  -       n       n       -       -       pipe
#  flags=FR user=list argv=/usr/lib/mailman/bin/postfix-to-mailman.py
#  ${nexthop} ${user}
</code></pre>
","<postfix><starttls>","2018-02-01 03:29:54"
"757688","Docker hostname not working","<pre><code>docker run -d -v /srv/dockervolume/openssh-git/srv:/srv --name node-basickarl -h node-basickarl basickarl/node-basickarl
</code></pre>

<p>ping:</p>

<pre><code>root@domain:/srv/dockervolume/haproxy/etc/haproxy# ping node-basickarl
ping: unknown host node-basickarl
</code></pre>

<p>It works when I ping the ipv4 address though. Why isn't hostname working?</p>
","<docker>","2016-02-17 00:56:52"
"895126","In OS X ssh to vps I had changed the port in /etc/ssh/sshd_config, shut computer,","<p>That's about what happened, save for what had happened prior: I was following a tutorial that said to ssh to itself... somehow slightly differently but now I forget and can't track it down -- and I did.
I had also set PasswordAuthentication no.</p>

<p>The local prompt in Terminal is <code>mycomputer:~ user$</code> but window title is <code>user@mi.pad.dre.ds:~</code>. But searching up on it this seems to be normal enough behavior. My primary concern is with connecting through ssh right now.</p>

<p>So I try </p>

<pre><code>$ ssh -vp xxxx root@xx.xxx.xxx.xx
OpenSSH_6.9p1, LibreSSL 2.1.8
debug1: Reading configuration data /etc/ssh/ssh_config
debug1: /etc/ssh/ssh_config line 20: Applying options for *
debug1: /etc/ssh/ssh_config line 53: Applying options for *
debug1: Connecting to xx.xxx.xxx.xx [xx.xxx.xxx.xx] port xxxx.
debug1: Connection established.
debug1: identity file /Users/bitch/.ssh/id_rsa type 1
debug1: key_load_public: No such file or directory
debug1: identity file /Users/bitch/.ssh/id_rsa-cert type -1
debug1: key_load_public: No such file or directory
debug1: identity file /Users/bitch/.ssh/id_dsa type -1
debug1: key_load_public: No such file or directory
debug1: identity file /Users/bitch/.ssh/id_dsa-cert type -1
debug1: key_load_public: No such file or directory
debug1: identity file /Users/bitch/.ssh/id_ecdsa type -1
debug1: key_load_public: No such file or directory
debug1: identity file /Users/bitch/.ssh/id_ecdsa-cert type -1
debug1: key_load_public: No such file or directory
debug1: identity file /Users/bitch/.ssh/id_ed25519 type -1
debug1: key_load_public: No such file or directory
debug1: identity file /Users/bitch/.ssh/id_ed25519-cert type -1
debug1: Enabling compatibility mode for protocol 2.0
debug1: Local version string SSH-2.0-OpenSSH_6.9
debug1: Remote protocol version 2.0, remote software version OpenSSH_7.4
debug1: match: OpenSSH_7.4 pat OpenSSH* compat 0x04000000
debug1: Authenticating to 23.254.247.78:1231 as 'root'
debug1: SSH2_MSG_KEXINIT sent
debug1: SSH2_MSG_KEXINIT received
debug1: kex: server-&gt;client chacha20-poly1305@openssh.com &lt;implicit&gt; none
debug1: kex: client-&gt;server chacha20-poly1305@openssh.com &lt;implicit&gt; none
debug1: expecting SSH2_MSG_KEX_ECDH_REPLY
debug1: Server host key: ecdsa-sha2-nistp256 SHA256:LjPAnfLg4EL/B4CucIQEkAiloSREqKzGoYCpLnHsVdQ
debug1: checking without port identifier
debug1: Host '23.254.247.78' is known and matches the ECDSA host key.
debug1: Found key in /Users/bitch/.ssh/known_hosts:6
debug1: found matching key w/out port
debug1: SSH2_MSG_NEWKEYS sent
debug1: expecting SSH2_MSG_NEWKEYS
debug1: SSH2_MSG_NEWKEYS received
debug1: SSH2_MSG_SERVICE_REQUEST sent
debug1: SSH2_MSG_SERVICE_ACCEPT received
debug1: Authentications that can continue: publickey,gssapi-keyex,gssapi-with-mic
debug1: Next authentication method: publickey
debug1: Offering RSA public key: /Users/bitch/.ssh/id_rsa
debug1: Authentications that can continue: publickey,gssapi-keyex,gssapi-with-mic
debug1: Trying private key: /Users/bitch/.ssh/id_dsa
debug1: Trying private key: /Users/bitch/.ssh/id_ecdsa
debug1: Trying private key: /Users/bitch/.ssh/id_ed25519
debug1: No more authentication methods to try.
Permission denied (publickey,gssapi-keyex,gssapi-with-mic).
</code></pre>
","<ssh><centos7><vps>","2018-02-01 07:42:01"
"895166","Change External NIC to Public on Windows Server 2016","<p>I've got a cloud network: 192.168.100.0/24 with:</p>
<ul>
<li>Pfsense GW (IPSec site-to-site)</li>
<li>NIC1 = WAN-IP1 (fully blocked except IPSec)</li>
<li>NIC2 = 192.168.100.1</li>
<li><strong>DC (WinServer2016)</strong></li>
<li>NIC1 = WAN-IP2 (external)</li>
<li>NIC2 = 192.168.100.2 with Pfsense as GW (internal)</li>
</ul>
<p>In Windows Network Sharing Center I've disabled the External NIC for obvious security reasons.</p>
<p>However I would like to change my NIC1 &amp; firewall that it can get updates and other important traffic. Why not over the Pfsense? Due to bandwith limitations of my provider and VPS's.</p>
<p><strong>How can I change my external NIC (NIC1 on WinServer2016) to only allow outgoing updates traffic?</strong></p>
<p>edit: Removed suggestion to use Public profile</p>
","<domain-controller><windows-server-2016><nic><windows-firewall>","2018-02-01 12:16:41"
"757790","How does Apache Load balancing work if DNS Caching is involved?","<p>Assuming the simple scenario of an Intranet with lot of users, there will be a DNS Server which translates IP address for the end user system. Now you also have a <code>yourapp.intranet</code> that is deployed over Apache for load balancing.</p>

<p>How will the load balancing work if each client maitains a dns cache like Windows does? The first request will get a ""load-balanced"" response but the next request onwards its going to go directly bypassing the Apache? Also would Apapche be able to track all these requests if the users directly hit the Node since it already knows the IP?</p>

<pre><code>C1-----*-----S1
       |
      DNS
       |
C2-----*-----S1
</code></pre>

<p>Or is it like DNS <strong>only</strong> knows the IP of the Apache server, and the load balancing is at the level of Apache? So as far as the DNS is concerned it doesn't know the nodes at all. Which also means it kind of NATing that happens internally?</p>

<pre><code>C1----*      S1
      |      |
     DNS-----*
      |      |
C2----*      S2
</code></pre>
","<apache-2.2><domain-name-system><load-balancing><web-applications>","2016-02-17 11:01:45"
"757845","Log LAN bandwidth per IP (with command line output)","<p>I have a local samba server and I need from which IP most bandwidth is generated. I would like to be able to pump that data do influx so I need a commandline tool giving me some sort of straightforward output that I could work with using python for example.</p>

<p>There are some tools I've already exploread but all of them are either realtime monitors (which I don't want) or are uncapable of giving me a ""per ip"" statistics.</p>

<p>My case is simple -> single server, single network card so it's a central point to log data. All I need is a simple output in return, like:</p>

<pre><code>10.10.10.10 -&gt; 35234234 bytes sent 
</code></pre>

<p>(average throughput will be fine as well).
Any clues ? (I'm not using ip tables)</p>

<p>I looked at iptraf-ng , itop - none of those allowed me go get command line output. (i don't need realtime bandwith)</p>
","<linux><networking><monitoring>","2016-02-17 14:49:32"
"895265","Minimum requirements for enabling reset password functionality in Azure","<p>I recently got back from Holidays to discover that I am no longer able to login to my work (Azure/Microsoft?) account. This account was added through 'Active Directory' (free) as a guest.</p>

<p>When I attempt to reset the account password I get a message stating that Admin has not enabled/configured reset functionality.</p>

<p>My fellow co-worker is the 'Global Administrator' for our Azure account/services but the 'Reset Password' option is disabled for all users within Azure dashboard, including themselves, and the option to enable external 'Password Reset' is not available in 'Active Directory' without upgrading to 'Premium'.</p>

<p>We would like to know the simplest way to re-enable my account. Is there are way to reset an account password for free or do we have to enable some paid Azure services to do so?</p>

<p>Additional info:</p>

<p>I've tried deleting the 'Quest' account in 'Active Directory' but if you attempt to re-add a guest account with the same email address it prompts for a password (the first time I did this it prompted to set a password). I still don't know the password so I still cannot log-in.</p>
","<active-directory><azure>","2018-02-02 00:16:50"
"825396","Cent OS 6 restarts immediately after booting","<p>I have a server with CentOs 6 which restarts in loop.</p>

<p>It loads all modules correctly, then login prompt appears and then it immediately starts umount ing drives and goes to restart.</p>

<p>Does anyone knows how to stop it from restarting? <code>CTRL</code>+<code>C</code>  doesn't help.
Or maybe i should alter boot modules somehow?</p>

<p>I am thinking about booting from some live cd, but i'm not sure where to look to disable restart in the end.</p>
","<linux><centos><centos6>","2017-01-10 18:43:50"
"825411","How to manually select a specific disk in the UDI Wizard when using MDT 2013?","<p>Is there a way to manually select a disk or partition as part of the MDT Deployment UDI Wizard rather than MDT automatically selecting one.</p>

<p>I know you can select one in the Task Sequence but I'm specifically looking for a way to do this in the wizard.</p>

<p>Thanks for any help you can provide!</p>

<p>Joshua</p>
","<hard-drive><wds><mdt-2013>","2017-01-10 19:20:27"
"895415","Changing Hostname on CentOS 7","<p>What is difference between those two methods for changing host name on CentOS 7?
I want to setup this centOS with a FQDN.</p>

<p>The first way is <code>vi /etc/sysconfig/network</code> and <code>vi /etc/hosts</code> files. but this way somehow wont work, after I changed the host name in this 2 locations, rebooted, run <code>hostname</code> it still shows me hostname as ""localhost""</p>

<p>The second way after research is using <code>hostnamectl set-hostname</code> or <code>hostname set-hostname xxx.xxx --static</code> command.</p>

<p>If using 2nd way to change hostname, it modifies which file?</p>
","<linux><centos>","2018-02-02 18:30:36"
"825531","Fail2Ban on Centos is blocking connections from Cygwin and WinSCP","<p>I got over 3k failed login attempts yesterday morning which was the most ive ever seen. I did some research and Fail2Ban seems to be a good step to stopping this. I have installed it and it seems to be going ok, but i have noticed that it is blocking access from anything that isn't an SSH client.</p>

<p>I use windows and i like to keep backups of the server on my laptop. I use Cygwin to give me access to rsync. I also use WinSCP to log into the server to modify any files as I am completely useless at using vi. When Fail2Ban is running, i can no longer access my server using both of these application. </p>

<p>I am running Centos6 and i am using the root user to login with both applications. Is there a way to get around this without causing security issues?</p>

<p>This is the error i get from WinSCP.</p>

<blockquote>
  <p>Cannot initialize SFTP protocol. Is the host running a SFTP server?</p>
</blockquote>
","<ssh><security><centos6><fail2ban>","2017-01-11 10:23:36"
"758162","MySQL crashes due to memory related errors, but roughly 50% memory is unused","<p>MySQL keeps crashing because of: <code>InnoDB: Fatal error: cannot allocate memory for the buffer pool</code>. This looks like a memory problem and the internet seems to confirm that. Also, my box has only 1GB of RAM, so another hint that this might be the problem.</p>

<p>However, my monitoring tools (from rackspace) say I've only used about 608MB of my memory at the time of the crash. Also, if I check out my memory usage through the <code>free</code> command, I see something like this:</p>

<pre><code>           total       used       free     shared    buffers     cached
Mem:       1018872     832144     186728      86608      22992     183276
-/+ buffers/cache:     625876     392996
Swap:            0          0          0
</code></pre>

<p>At this point, please note that I'm very inexperienced with Linux's way of handling memory. But with Googles help I've found that 'cached' memory should be considered free as it is given back by the system when needed (is that right?).</p>

<p>If that's the case, then why does MySQL still crash?</p>

<p>Edit: Some more info. the INNODB part of mysqltuner is this:</p>

<pre><code>-------- InnoDB Metrics ----------------------------
[--] InnoDB is enabled.
[OK] InnoDB buffer pool / data size: 128.0M/20.2M
[OK] InnoDB buffer pool instances: 1
[!!] InnoDB Used buffer: 11.83% (969 used/ 8191 total)
[OK] InnoDB Read buffer efficiency: 92.74% (12364 hits/ 13332 total)
[!!] InnoDB Write buffer efficiency: 0.00% (0 hits/ 1 total)
[OK] InnoDB log waits: 0.00% (0 waits / 1 writes)
</code></pre>

<p>My.cnf (I've removed all commented out lines to make it more readable):</p>

<pre><code>[client]
port        = 3306
socket      = /var/run/mysqld/mysqld.sock

[mysqld_safe]
socket      = /var/run/mysqld/mysqld.sock
nice        = 0

[mysqld]
user        = mysql
pid-file    = /var/run/mysqld/mysqld.pid
socket      = /var/run/mysqld/mysqld.sock
port        = 3306
basedir     = /usr
datadir     = /var/lib/mysql
tmpdir      = /tmp
lc-messages-dir = /usr/share/mysql
skip-external-locking
bind-address        = 127.0.0.1
key_buffer      = 16M
max_allowed_packet  = 16M
thread_stack        = 192K
thread_cache_size       = 8
myisam-recover         = BACKUP
query_cache_limit   = 1M
query_cache_size        = 16M

log_error = /var/log/mysql/error.log

expire_logs_days    = 10
max_binlog_size         = 100M

[mysqldump]
quick
quote-names
max_allowed_packet  = 16M

[mysql]

[isamchk]
key_buffer      = 16M

!includedir /etc/mysql/conf.d/
</code></pre>
","<linux><mysql><memory>","2016-02-18 16:38:24"
"825807","OpenVPN left a SSL certificate… How to remove it?","<p>Well a while ago I tried <em>OpenVPN Access Point</em> to use my server as VPN.
Then I wanted to get letsencrypt
then everything failed
then I removed OpenVPN Access Point
then the stupid certificate is still there</p>

<p>I have tried purging OpenSSL... Nothing</p>

<p>Help...</p>

<p>EDIT:<br />
This is on Ubuntu 16.04 Xenial. DigitalOcean. 512MB RAM.<br />
Also, I followed <a href=""https://openvpn.net/index.php/access-server/docs/quick-start-guide.html"" rel=""nofollow noreferrer"">this</a>.</p>
","<ssl><openvpn><certificate>","2017-01-12 10:42:22"
"825808","How to undo after copying a file in centos","<p>I wanted to copy contents of one file to another but by mistake i copied the new file to file having contents. So now file which had data is overwritten and is empty. Is there a way to revert this process back without backup. Any way?</p>

<p>centos 6.4</p>
","<centos><centos6>","2017-01-12 10:46:25"
"758374","using Curl for Rest API calls - Authenication Failure","<p>I have an authentication problem with a REST API Call with curl, I think its to do with the formatting of my curl input. </p>

<pre><code>curl -v https://api.ananthsdomain.com/version1/testaccount/test \
     -H ""Accept: text/xml \
         User-Agent: MyIn Interface \
         X-Api-Signature:  XXXXXXX+u:`date +%Y%m%d%H%M%S`:`echo XXXXXXX$(date +%Y%m%d%H%M%S)XXXXXXX | openssl dgst -binary -sha1 | openssl base64`""
</code></pre>

<p>I have to construct X-Api-Signature to match requirements from servers which includes API key, current date and sha1hash of the API key. </p>

<p>I'm receiving authentication error and I think it's because the signature is being set in the same line as User-Agent (output below) </p>

<pre><code>Accept: text/xml
User-Agent: MyIn Interface           X-Api-Signature:  XXXXXXX+u:20160210112839:O9bXXXXXXXXXXXlVAXXXXXXXXXX=
</code></pre>

<p>The output I want to see is meant to be like this. </p>

<pre><code>Accept: text/xml
User-Agent: MyIn Interface
X-Api-Signature: XXXXXXX+u:20160210112839:O9bXXXXXXXXXXXlVAXXXXXXXXXX=
</code></pre>

<p>How can I make curl produce the output so it appears like the above. </p>
","<curl>","2016-02-19 13:13:14"
"895832","MySQL replication not working","<p>I have been trying everything I could find and my servers are not replicating! Can someone look at my config files and offer some guidance?</p>

<hr>

<p>My,ini master on Windows:</p>

<hr>

<pre><code># Other default tuning values
# MySQL Server Instance Configuration File
# ----------------------------------------------------------------------
# Generated by the MySQL Server Instance Configuration Wizard
#
#
# Installation Instructions
# ----------------------------------------------------------------------
#
# On Linux you can copy this file to /etc/my.cnf to set global options,
# mysql-data-dir/my.cnf to set server-specific options
# (@localstatedir@ for this installation) or to
# ~/.my.cnf to set user-specific options.
#
# On Windows you should keep this file in the installation directory 
# of your server (e.g. C:\Program Files\MySQL\MySQL Server X.Y). To
# make sure the server reads the config file use the startup option 
# ""--defaults-file"". 
#
# To run run the server from the command line, execute this in a 
# command line shell, e.g.
# mysqld --defaults-file=""C:\Program Files\MySQL\MySQL Server X.Y\my.ini""
#
# To install the server as a Windows service manually, execute this in a 
# command line shell, e.g.
# mysqld --install MySQLXY --defaults-file=""C:\Program Files\MySQL\MySQL Server X.Y\my.ini""
#
# And then execute this in a command line shell to start the server, e.g.
# net start MySQLXY
#
#
# Guildlines for editing this file
# ----------------------------------------------------------------------
#
# In this file, you can use all long options that the program supports.
# If you want to know the options a program supports, start the program
# with the ""--help"" option.
#
# More detailed information about the individual options can also be
# found in the manual.
#
# For advice on how to change settings please see
# http://dev.mysql.com/doc/refman/5.7/en/server-configuration-defaults.html
#
#
# CLIENT SECTION
# ----------------------------------------------------------------------
#
# The following options will be read by MySQL client applications.
# Note that only client applications shipped by MySQL are guaranteed
# to read this section. If you want your own MySQL client program to
# honor these values, you need to specify it as an option during the
# MySQL client library initialization.
#
[client]

# pipe
# socket=0.0
port=3306

[mysql]
no-beep

default-character-set=utf8


# SERVER SECTION
# ----------------------------------------------------------------------
#
# The following options will be read by the MySQL Server. Make sure that
# you have installed the server correctly (see above) so it reads this 
# file.
#
# server_type=3
[mysqld]

# The next three options are mutually exclusive to SERVER_PORT below.
# skip-networking

# enable-named-pipe

# shared-memory

# shared-memory-base-name=MYSQL

# The Pipe the MySQL Server will use
# socket=MYSQL

# The TCP/IP Port the MySQL Server will listen on
port=3306

# Path to installation directory. All paths are usually resolved relative to this.
# basedir=""C:/Program Files/MySQL/MySQL Server 5.7/""

# Path to the database root
datadir=C:/ProgramData/MySQL/MySQL Server 5.7/Data

# The default character set that will be used when a new schema or table is
# created and no character set is defined
character-set-server=utf8

# The default storage engine that will be used when create new tables when
default-storage-engine=INNODB

# Set the SQL mode to strict
sql-mode=""STRICT_TRANS_TABLES,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION""

# Enable Windows Authentication
# plugin-load=authentication_windows.dll

# General and Slow logging.
log-output=FILE
general-log=0
general_log_file=""FL51LTHLGX2H2.log""
slow-query-log=1
slow_query_log_file=""FL51LTHLGX2H2-slow.log""
long_query_time=10

# Binary Logging.
# log-bin

# Error Logging.
log-error=""FL51LTHLGX2H2.err""

# Server Id.
server-id = 2

# Secure File Priv.
secure-file-priv=""C:/ProgramData/MySQL/MySQL Server 5.7/Uploads""

# The maximum amount of concurrent sessions the MySQL server will
# allow. One of these connections will be reserved for a user with
# SUPER privileges to allow the administrator to login even if the
# connection limit has been reached.
max_connections=151

# The number of open tables for all threads. Increasing this value
# increases the number of file descriptors that mysqld requires.
# Therefore you have to make sure to set the amount of open files
# allowed to at least 4096 in the variable ""open-files-limit"" in
# section [mysqld_safe]
table_open_cache=2000

# Maximum size for internal (in-memory) temporary tables. If a table
# grows larger than this value, it is automatically converted to disk
# based table This limitation is for a single table. There can be many
# of them.
tmp_table_size=16M

# How many threads we should keep in a cache for reuse. When a client
# disconnects, the client's threads are put in the cache if there aren't
# more than thread_cache_size threads from before.  This greatly reduces
# the amount of thread creations needed if you have a lot of new
# connections. (Normally this doesn't give a notable performance
# improvement if you have a good thread implementation.)
thread_cache_size=10

#*** MyISAM Specific options
# The maximum size of the temporary file MySQL is allowed to use while
# recreating the index (during REPAIR, ALTER TABLE or LOAD DATA INFILE.
# If the file-size would be bigger than this, the index will be created
# through the key cache (which is slower).
myisam_max_sort_file_size=100G

# If the temporary file used for fast index creation would be bigger
# than using the key cache by the amount specified here, then prefer the
# key cache method.  This is mainly used to force long character keys in
# large tables to use the slower key cache method to create the index.
myisam_sort_buffer_size=8M

# Size of the Key Buffer, used to cache index blocks for MyISAM tables.
# Do not set it larger than 30% of your available memory, as some memory
# is also required by the OS to cache rows. Even if you're not using
# MyISAM tables, you should still set it to 8-64M as it will also be
# used for internal temporary disk tables.
key_buffer_size=8M

# Size of the buffer used for doing full table scans of MyISAM tables.
# Allocated per thread, if a full scan is needed.
read_buffer_size=0

read_rnd_buffer_size=0

#*** INNODB Specific options ***
# innodb_data_home_dir=0.0

# Use this option if you have a MySQL server with InnoDB support enabled
# but you do not plan to use it. This will save memory and disk space
# and speed up some things.
# skip-innodb

# If set to 1, InnoDB will flush (fsync) the transaction logs to the
# disk at each commit, which offers full ACID behavior. If you are
# willing to compromise this safety, and you are running small
# transactions, you may set this to 0 or 2 to reduce disk I/O to the
# logs. Value 0 means that the log is only written to the log file and
# the log file flushed to disk approximately once per second. Value 2
# means the log is written to the log file at each commit, but the log
# file is only flushed to disk approximately once per second.
innodb_flush_log_at_trx_commit=1

# The size of the buffer InnoDB uses for buffering log data. As soon as
# it is full, InnoDB will have to flush it to disk. As it is flushed
# once per second anyway, it does not make sense to have it very large
# (even with long transactions).
innodb_log_buffer_size=1M

# InnoDB, unlike MyISAM, uses a buffer pool to cache both indexes and
# row data. The bigger you set this the less disk I/O is needed to
# access data in tables. On a dedicated database server you may set this
# parameter up to 80% of the machine physical memory size. Do not set it
# too large, though, because competition of the physical memory may
# cause paging in the operating system.  Note that on 32bit systems you
# might be limited to 2-3.5G of user level memory per process, so do not
# set it too high.
innodb_buffer_pool_size=8M

# Size of each log file in a log group. You should set the combined size
# of log files to about 25%-100% of your buffer pool size to avoid
# unneeded buffer pool flush activity on log file overwrite. However,
# note that a larger logfile size will increase the time needed for the
# recovery process.
innodb_log_file_size=48M

# Number of threads allowed inside the InnoDB kernel. The optimal value
# depends highly on the application, hardware as well as the OS
# scheduler properties. A too high value may lead to thread thrashing.
innodb_thread_concurrency=17

# The increment size (in MB) for extending the size of an auto-extend InnoDB system tablespace file when it becomes full.
innodb_autoextend_increment=64

# The number of regions that the InnoDB buffer pool is divided into.
# For systems with buffer pools in the multi-gigabyte range, dividing the buffer pool into separate instances can improve concurrency,
# by reducing contention as different threads read and write to cached pages.
innodb_buffer_pool_instances=8

# Determines the number of threads that can enter InnoDB concurrently.
innodb_concurrency_tickets=5000

# Specifies how long in milliseconds (ms) a block inserted into the old sublist must stay there after its first access before
# it can be moved to the new sublist.
innodb_old_blocks_time=1000

# It specifies the maximum number of .ibd files that MySQL can keep open at one time. The minimum value is 10.
innodb_open_files=300

# When this variable is enabled, InnoDB updates statistics during metadata statements.
innodb_stats_on_metadata=0

# When innodb_file_per_table is enabled (the default in 5.6.6 and higher), InnoDB stores the data and indexes for each newly created table
# in a separate .ibd file, rather than in the system tablespace.
innodb_file_per_table=1

# Use the following list of values: 0 for crc32, 1 for strict_crc32, 2 for innodb, 3 for strict_innodb, 4 for none, 5 for strict_none.
innodb_checksum_algorithm=0

# The number of outstanding connection requests MySQL can have.
# This option is useful when the main MySQL thread gets many connection requests in a very short time.
# It then takes some time (although very little) for the main thread to check the connection and start a new thread.
# The back_log value indicates how many requests can be stacked during this short time before MySQL momentarily
# stops answering new requests.
# You need to increase this only if you expect a large number of connections in a short period of time.
back_log=80

# If this is set to a nonzero value, all tables are closed every flush_time seconds to free up resources and
# synchronize unflushed data to disk.
# This option is best used only on systems with minimal resources.
flush_time=0

# The minimum size of the buffer that is used for plain index scans, range index scans, and joins that do not use
# indexes and thus perform full table scans.
join_buffer_size=256K

# The maximum size of one packet or any generated or intermediate string, or any parameter sent by the
# mysql_stmt_send_long_data() C API function.
max_allowed_packet=4M

# If more than this many successive connection requests from a host are interrupted without a successful connection,
# the server blocks that host from performing further connections.
max_connect_errors=100

# Changes the number of file descriptors available to mysqld.
# You should try increasing the value of this option if mysqld gives you the error ""Too many open files"".
open_files_limit=4161

# If you see many sort_merge_passes per second in SHOW GLOBAL STATUS output, you can consider increasing the
# sort_buffer_size value to speed up ORDER BY or GROUP BY operations that cannot be improved with query optimization
# or improved indexing.
sort_buffer_size=256K

# The number of table definitions (from .frm files) that can be stored in the definition cache.
# If you use a large number of tables, you can create a large table definition cache to speed up opening of tables.
# The table definition cache takes less space and does not use file descriptors, unlike the normal table cache.
# The minimum and default values are both 400.
table_definition_cache=1400

# Specify the maximum size of a row-based binary log event, in bytes.
# Rows are grouped into events smaller than this size if possible. The value should be a multiple of 256.
binlog_row_event_max_size=8K

# If the value of this variable is greater than 0, a replication slave synchronizes its master.info file to disk.
# (using fdatasync()) after every sync_master_info events.
sync_master_info=10000

# If the value of this variable is greater than 0, the MySQL server synchronizes its relay log to disk.
# (using fdatasync()) after every sync_relay_log writes to the relay log.
sync_relay_log=10000

# If the value of this variable is greater than 0, a replication slave synchronizes its relay-log.info file to disk.
# (using fdatasync()) after every sync_relay_log_info transactions.
sync_relay_log_info=10000

# Indicates how is the InnoDB Cluster configured as (Classic, Sandbox, Master or Slave).
# innodbclustertypeselection=ClassicMySQLReplication

# Indicates how is the InnoDB Cluster is/will be named.
# innodbclustername=""sandboxCluster""

# Indicates how many instances will the InnoDB cluster sandbox will have.
# innodbclusterinstances=0

# Holds the InnoDB Cluster Username.
# innodbclusterusername

# Indicates the InnoDB Cluster URI.
# innodbclusteruri

# Indicates the InnoDB Cluster Port.
# innodbclusterport=3306

# Load mysql plugins at start.""plugin_x ; plugin_y"".
# plugin_load

# MySQL server's plugin configuration.
# loose_mysqlx_port=33060

enforce_gtid_consistency = WARN
show-slave-auth-info

# 2018-01-26
log-bin = C:\ProgramData\MySQL\MySQL Server 5.7\Data\mysql.bin


binlog-do-db = equip, units

replicate-do-db = equip, units
</code></pre>

<hr>

<p>mysqld.cnf (slave)</p>

<pre><code>#
# The MySQL database server configuration file.
#
************************************************

[mysqld_safe]
socket      = /var/run/mysqld/mysqld.sock
nice        = 0

[mysqld]
#
# * Basic Settings
#
user        = mysql
pid-file    = /var/run/mysqld/mysqld.pid
socket      = /var/run/mysqld/mysqld.sock
port        = 3306
basedir     = /usr
datadir     = /var/lib/mysql
tmpdir      = /tmp
lc-messages-dir = /usr/share/mysql
skip-external-locking
#
# Instead of skip-networking the default is now to listen only on
# localhost which is more compatible and is not less secure.

#bind-address       = 192.168.56.1

#
# * Fine Tuning
#
key_buffer_size     = 16M
max_allowed_packet  = 16M
thread_stack        = 192K
thread_cache_size       = 8
# This replaces the startup script and checks MyISAM tables if needed
# the first time they are touched
myisam-recover-options  = BACKUP
#max_connections        = 100
#table_cache            = 64
#thread_concurrency     = 10
#
# * Query Cache Configuration
#
query_cache_limit   = 1M
query_cache_size        = 16M
#
# * Logging and Replication
#
# Both location gets rotated by the cronjob.
# Be aware that this log type is a performance killer.
# As of 5.1 you can enable the log at runtime!
#general_log_file        = /var/log/mysql/mysql.log
#general_log             = 1
#
# Error log - should be very few entries.
#
log_error = /var/log/mysql/error.log
#
# Here you can see queries with especially long duration
#log_slow_queries   = /var/log/mysql/mysql-slow.log
#long_query_time = 2
#log-queries-not-using-indexes
#
# The following can be used as easy to replay backup logs or for replication.
# note: if you are setting up a replication slave, see README.Debian about
#       other settings you may need to change.

server-id       = 3

relay-log       = /var/log/mysql/mysql-relay-bin.log
expire_logs_days    = 10
max_binlog_size     = 100M
#binlog_do_db       = equip
#binlog_do_db       = units
replicate-do-db     = units
replicate-do-db     = equip

#binlog_ignore_db   = include_database_name
#log_slave_updates
auto-increment-increment = 2
auto-increment-offset   = 1

#
# * InnoDB
#
# InnoDB is enabled by default with a 10MB datafile in /var/lib/mysql/.
# Read the manual for more InnoDB related options. There are many!
#
# * Security Features
#
# Read the manual, too, if you want chroot!
# chroot = /var/lib/mysql/
#
# For generating SSL certificates I recommend the OpenSSL GUI ""tinyca"".
#
# ssl-ca=/etc/mysql/cacert.pem
# ssl-cert=/etc/mysql/server-cert.pem
# ssl-key=/etc/mysql/server-key.pem
</code></pre>
","<mysql><database><sql><replication>","2018-02-06 07:55:19"
"825878","process user differently using ssh key","<p>I wonder if it is possible to chroot/chdir/process the same unix user differently using the ssh key used in the ssh connexion...</p>

<p>The use case could be a rsync server using only one linux/local user for many virtual account.</p>

<p>It's somewhat how github/gitlab manages users.</p>

<p>EDIT: chrooting is not the main point. The point is to differentiate 'virtual' account using the same *nix user but with different SSH keys. Chrooting is a bonus.</p>

<p>PS: I've cooked a quick POC on a F25 server.</p>
","<ssh><rsync><chroot>","2017-01-12 14:28:58"
"758518","Error when I run ""bucardo add database mybd""","<p>This is the message error:</p>

<pre><code>install_driver(Pg) failed: Can't locate DBD/Pg.pm in @INC (@INC contains: /usr/local/lib/perl5 /usr/local/share/perl5 /usr/lib/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib/perl5 /usr/share/perl5 .) at (eval 9) line 3.
Perhaps the DBD::Pg perl module hasn't been fully installed, or perhaps the capitalisation of 'Pg' isn't right.
Available drivers: DBM, ExampleP, File, Gofer, Proxy, Sponge, mysql. at /usr/local/bin/bucardo line 308
</code></pre>

<p>Please could someone help me with this issue,
Thanks</p>
","<centos6><postgresql><database-replication>","2016-02-19 23:05:06"
"826000","Why should my web server be on a seperate network? (Azure)","<p>Just to start, I fully understand that a webserver should be on a seperate network, in the DMZ, not connected to the domain (or at least in a different forest). However, Im website developer and making this argument to networking. </p>

<p>I have made the argument that if someone was to gain control of the Web server, you wouldn't want a trust relationship to exists between said server and the DC. But I don't fully understand how to validate that this trust relationship would cause a problem. </p>

<p>In other words, how would one go about hacking an enterprise network by gaining access to an on-network web server and how do these principles change when talking about an azure VNET with a site to site VPN to the local network. </p>
","<security><vpn><web-server><azure><azure-networking>","2017-01-13 01:26:28"
"758593","Sharing internal storage of ESXi host via fiber channel protocol","<p>Imagine a pair of servers with fiber channel adapters. Normally each one can connect to the shared SAN storage using its HBA and fiber cords,</p>

<p>Now, eliminating the SAN, is it possible to interconnect these two ESXi servers directly with fiber cord and mount each others LUN in a shared manner? </p>

<p>In other words [I think!] : Can we publish the internal storage of an ESXi server with a given World Wide Name to the other directly attached ESXi? (If not, what about iSCSI?)</p>

<p>Anyway I want the storage traffic travel through the fiber.</p>
","<vmware-esxi><mount><storage-area-network><lun>","2016-02-20 11:53:19"
"826024","Apache subdomain (html) directory to external ip addr","<p>Wondering how to redirect subdomain public html directory to external server pu lic html directory? E.g </p>

<p>When user comes to this link: Sub.domain.com/abc</p>

<p>Contents of abc would get displayed from another external server html directory.</p>

<p><strong>Edit</strong>:</p>

<p>Os -- ubuntu
Environment-- vps</p>

<p>Sub.domain.com/abc ----> server1
Sub.domain.com/efg ----> server1
Sub.domain.com/xyz ----> server2</p>

<p>Do i need to point both server ip addr to same subdomain?
Do i need to define subdomain in both servers</p>

<p>In short what i suppose to do:)</p>
","<subdomain>","2017-01-13 04:18:01"
"896035","Iptables: How can I allow only one IP address to access a Centos 7 server?","<p>I have 5 IP addresses for 5 different computers. All of them can access the server. I only want one of them to have access, how should I go about this with iptables?</p>
","<iptables>","2018-02-07 07:19:41"
"896121","Will using SMTP without encryption reduce my email deliverability?","<p>Will using SMTP without encryption mechanisms like TLS or SSL, MIME reduce email deliverability?</p>
","<ssl><smtp>","2018-02-07 15:44:23"
"826132","OpenVPN Client connects, but DNS setting are not correct (cannot resolve domain names)","<p>I've an active subscription with <code>VPNUnlimited</code>, and they have sent a few settings in order to be able to use <code>OpenVPN</code> client with their service. (they have sent me pre-made <code>.ovpn</code> files for each of their servers)</p>

<p>the thing is, I've followed a tutorial to setup <code>OpenVPN</code> in my raspberry pi, and whenever I start the <code>OpenVPN</code> client with the <code>.ovpn</code> file, it authenticate and connects successfully.</p>

<p>this looks great but the problem is after a successfull connection, it looks like DNS configuration is not correct as any trial to navigate/ping any domain name fails, although pinging any IP is succeeded, so it's normally state that there is a DNS problem.</p>

<p>disconnecting the session will restore back the ability to resolve domain names normally and use the built-in web browser to browse websites.</p>

<p>I've searched for a while and found that one possible fix is to add the following line (provided from this <a href=""https://serverfault.com/questions/416708/how-to-ensure-openvpn-connection-uses-specific-dns"">question</a>) to the <code>client.conf</code> file in order to force the client to use Google DNS:</p>

<p><code>dhcp-option DNS 8.8.8.8</code></p>

<p>but unfortunately it's not working.</p>

<p>this is what is on my screen as a result on connecting to openvpn:
<a href=""https://i.sstatic.net/uJI6E.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/uJI6E.png"" alt=""enter image description here""></a></p>

<p>and this is the <code>ifconfig</code> after the connection is established:
<a href=""https://i.sstatic.net/AEx4r.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AEx4r.png"" alt=""enter image description here""></a></p>

<p>and this is the <code>ifconfig</code> result when I disconnect the <code>OpenVPN</code> client:
<a href=""https://i.sstatic.net/3OOUn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3OOUn.png"" alt=""enter image description here""></a></p>

<p>I hope I could clarify the situation, and if anyone can help me to setup the proper configuration to make my raspberry pi use google DNS (or anything that would work) I would much appreciate it as I can't figure it out yet.</p>
","<domain-name-system><vpn><openvpn><linux-networking><raspbian>","2017-01-13 15:05:46"
"758791","rewrite after rewrite in nginx","<p>i have:</p>

<ol>
<li>nginx server </li>
<li>site based on joomla </li>
<li><p>rewrite like this in server block</p>

<pre><code>location / {
    # Support Clean (aka Search Engine Friendly) URLs
    try_files $uri $uri/ /index.php?$args;
}
</code></pre></li>
</ol>

<p>this produces ""sef like"" links /abb/aaa/ccc</p>

<p>what i want now is to make map of short url's (of those rewrite links) </p>

<p>like this:</p>

<pre><code>map @rewrite_url $rewrited_url {
    /a    /abb/aaa/ccc
    /b    /aaa/adddd/aaa
}
</code></pre>

<p>and this map to be applied </p>

<p><strong>edit:</strong> </p>

<p>in location / try_files this : </p>

<pre><code>/index.php?q=$request_uri; 
</code></pre>

<p>makes joomla url:</p>

<pre><code>/index.php?bla=1&amp;blab=2  ===&gt; like ""/aaa/aaa/aaa""  - &gt; without  ?,&amp; and =  
</code></pre>

<p>and i wanna make short form of this friendly(s) link(s) as include map to use:</p>

<pre><code>/a instead of /bla/bla/comelink
/b instead of /bla/aaa/someotherlink
</code></pre>

<p>example (alias):</p>

<pre><code>/changelog  /some/adres/to change-log/on/joomla/page
</code></pre>
","<nginx><rewrite>","2016-02-21 19:48:28"
"758834","Nginx: How to redirect domain on lang?","<p>I'm trying to figure out how to switch/redirect to nanother domain based on language in the URL.</p>

<p>I have a site using two domain names domain1.tld and domain2.tld.
The site is on wordpress with WPML, and the URLs are formed this way:
domain1.tld/en-US/page1
domain2.tld/es-ES/page1</p>

<p>Crappy solution for now: In order to avoid some troubles with search engines, I put in the index.php a script that redirects to the domain associated with a language.
But each time I update wordpress, I have to rewrite the file.</p>

<p>How can I associate a domain to a language in Nginx ? 
I mean, if a visitor lands on domain1.tld/es-ES/page1, he should be redirected to domain2.tld/es-ES/page1.</p>

<p>I need to implement this because of trademarks that are not the same in all countries.
So I need to link one or more languages to a domain.</p>

<p>To clarify a bit, the website is divided in languages, ie /fr-CA/pages, /fr-FR/pages, /en-US/pages, /en-CA/pages, /es-ES/pages and so on.</p>

<p>When a visitor arrives on the site, he's redirected by the domain name to a base language. domain1.tld/ redirect by default to domain1.tld/en-US/home, and domain2.tld/ redirect to domain2.tld/fr-CA/acceuil</p>

<p>He's not redirected on his browser lang.</p>

<p>If he decides to change site language, the domain name might change based on the language he choose.</p>

<p>Each language on the site is associated to a domain name. Thus every domain name can be linked to one or more languages.</p>

<p>If a visitor requests domain1,tld, he's redirected to domain1.tld/en-US.
If now he wants US spanish from a drop down menu, he's redirected to domain1.tld/es-US/.
If he changes again and want it canadian french, he's redirected to domain2.tld/fr-CA/ because fr-CA is linked to domain2.tld</p>

<p>I was looking for something like this:</p>

<pre><code>location ~* /(es-ES|fr-FR/fr-CA)/ {
  # if user is on domain1.tld, redirect to domain2.tld
}
location ~* /(es-US|en-CA|en-US)/ {
  # if user is on domain2.tld, redirect to domain1.tld
}
</code></pre>

<p>I'd like to detect the language in the URL path and redirect to the domain linked to the language.</p>

<p>I know it's a bit complicated, but that's marketing things and I can't avoid it.</p>

<p>Thanks.</p>
","<nginx><domain><redirection><language>","2016-02-22 03:22:48"
"758871","MX Record for a physical server","<p>I've a physical server which is used as a Mail Transfer Agent(MTA), I'm using <code>postfix</code> as SMTP server and <code>cyrus</code> as IMAP. I've a static IP for the server and have SSH access to it. </p>

<p>Postfix and cyrus seem to be configured properly, because I can send emails to others. But I can't receive any emails. Probably the reason is I haven't updated the MX Record of my domain which is bought from AWS. </p>

<p>I can't understand <strong>how to update MX Record to receive emails from others</strong>. I mean, what should be the values to be put as MX Record, I only have  the IP address of my server. </p>

<p>More information:<br>
I used <code>mailx</code> command to send emails and I successfully received the sent emails in my gmail/outlook/yahoo. But when I reply to those it says:</p>

<pre><code>The recipient server did not accept our requests to connect. Learn more at https://support.google.com/mail/answer/7720
[example.com 54.175.152.x: socket error]
[example.com 107.21.3.x: socket error] 
</code></pre>
","<domain-name-system><amazon-web-services><smtp><email-server><mx-record>","2016-02-22 08:02:41"
"967912","Can not import table to MySQL database on linux","<p>I am trying to install cacti. When I want to import <code>mysql_test_data_timezone.sql</code> to MySQL database on Linux CentOS, I get an error.</p>

<p>My command is</p>

<pre><code>mysql -u root -p mysql &lt; /usr/share/mysql/mysql_test_data_timezone.sql
</code></pre>

<p>I get error</p>

<pre><code>No such a file or directory ;
</code></pre>

<p>Does this directory have to exist or do I have to create it myself?</p>
","<linux><centos><mysql><mariadb><cacti>","2019-05-18 20:35:46"
"967995","Unknown domain owner?","<p>If a domain returns as picture shows who owns said domain and how would I be able to purchase it?</p>

<p><img src=""https://i.sstatic.net/bYx7O.png"" alt=""PICTURE""></p>
","<domain>","2019-05-19 20:14:50"
"826411","Why there are so many ""ss"" process on my server (CentOS with nginx serving the web)?","<p>There are about 500 concurrent users. It's because some famous people on social network repost a link.</p>

<p>This happened before. And our server were performing good.</p>

<p>But this time, the server is getting very slow and I noticed that the load is very high. In this picture the load is only 83.22 but that's because I rebooted it. Before rebooting it is 600+ (after editing this post, it's climbing to 200+, and it's still climbing, I guess it'd be above 600 soon)</p>

<p>I noticed that there're so many ""ss"" process. What are them? Any further advice on this case?</p>

<p>Thanks!</p>

<p><a href=""https://i.sstatic.net/w4O4A.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/w4O4A.jpg"" alt=""enter image description here""></a></p>

<p>Update: after posting this question, it's getting higher:</p>

<p><a href=""https://i.sstatic.net/xca5Y.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xca5Y.png"" alt=""enter image description here""></a></p>

<p>Update:</p>

<p>I tried to find the source of this ""ss"" but it's like this:</p>

<p><a href=""https://i.sstatic.net/vliwW.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vliwW.jpg"" alt=""enter image description here""></a></p>

<p>Update: added another picture for <code>ps auxw</code></p>

<p>Update:</p>

<p>Used <code>ps aux | grep ss</code> and listed this:</p>

<p><code>/bin/sh -c ss -nlp | grep ""[,=]27846,"" || netstat -nlp | grep ""[[:space:]]27846/""</code></p>

<p><a href=""https://i.sstatic.net/TbZao.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TbZao.png"" alt=""enter image description here""></a></p>
","<cpu-usage><high-load>","2017-01-15 15:18:06"
"896439","What's the difference between /var/spool/cron and /var/spool/cron/crontabs?","<p>This went unmentoned anywhere else I could find, not even in the man page of crontab. what's the difference between them?</p>

<p>some people say they don't have /var/spool/cron/crontabs, but one can easily find lots of systems with this path. for example, <a href=""https://docs.oracle.com/cd/E19455-01/805-7229/6j6q8svfo/index.html"" rel=""nofollow noreferrer"">here</a>.</p>

<p>note here I'm not talking about one specific linux distro, if these two paths come from different distros, I would like to know the reason behind this.</p>
","<linux><cron><scheduled-task><incrontab>","2018-02-09 02:58:14"
"826490","CNAME working - but not for directories","<p>I'm currently trying to resolve a problem with getting a domain pointed onto another domain, but for some reason it isn't working as it should be - any help here would be fantastic.</p>

<p>The CNAME as been set and is, as far as I can tell, working.<br>
We've pointed <code>example.co.uk</code> to <code>example2.co.uk</code></p>

<p>Going to <code>example.co.uk</code> shows the page of <code>example2.co.uk</code><br>
However, directories are not functioning.</p>

<p>So regularly, <code>example2.co.uk/directory</code> - would work,<br>
But <code>example.co.uk/directory</code> - does not work</p>

<p>What is happening here?</p>
","<domain-name-system><cname-record><domain-name>","2017-01-16 09:03:42"
"826629","Problems creating a static route (or any route)","<p>I have been using this website to read about routing tables and static routes, but I can't seem to get my own static route going.</p>

<p>Here is the situation:</p>

<p>-> PC:<br>
1x Wifi NIC 192.168.1.19 with mask 255.255.255.0 and gateway 192.168.1.1<br>
1x LAN NIC 172.16.0.1 with mask 255.255.0.0</p>

<p>-> ESXi Server<br>
LAN NIC 172.16.0.3 with mask 255.255.0.0 (Management Network)</p>

<p>-> Windows Server 2012 VM (runs on ESXi Server)<br>
LAN NIC 172.16.0.4 with mask 255.255.0.0 and gateway 172.16.0.1 (???)</p>

<p>The nodes in the 172.16.0.0 network can ping one another.</p>

<p>Now I want the VM to be able to connect to the internet, so I figured I had to make a route on my PC, since it has 2 interfaces.</p>

<p>I did <i>route add 172.16.0.0 mask 255.255.0.0 192.16.1.19 </i>but that did not work.<br>
What am I not doing correctly?<br>
Or can I not use a static route here?</p>
","<routing><virtual-machines><internet><interface>","2017-01-16 20:45:49"
"759021","Why does ntpq report reachability (reach) in Octal?","<p>I'm not sure if this is better suited for ServerFault or Stack Overflow.  </p>

<p>The return of <code>ntpq -p</code> reports ""reach"" as an 8-bit octal value, 1-377, as in the following example:</p>

<pre><code> remote           refid      st t when poll reach   delay   offset  jitter

==============================================================================

*serv1       123.123.123.1  4 u   23  256  377    0.462    0.230   0.072

serv2        1.1.1.1        5 u  142  256  377    1.209   -0.600   0.050

serv3        1.1.1.1        5 u  134  256  377    0.452   -0.055   0.012

serv4        1.1.1.1        5 u  148  256  377    1.477    0.283   0.061
</code></pre>

<p>""reach"" is a circular buffer of the last eight NTP transactions.  For example, <code>11111011</code> (<code>373</code>) would mean that the last two transactions were successful, but the previous one failed.  Here's a great article on how this works:<br>
<a href=""http://www.linuxjournal.com/article/6812"" rel=""nofollow noreferrer"">Understanding NTP Reachability Statistics</a></p>

<p>I can't see the benefit of reporting this transaction history in octal.  It seems like printing it as the 8-bit binary string would more directly show the user what the statistic represents - the history of the last eight transactions.  Printing it as octal seems inconvenient at best.</p>

<p>What am I missing?</p>
","<ntp><user-experience>","2016-02-22 20:04:47"
"826756","Trying to connect to vsftpd, Failed to retrieve directory listing","<p>I set up a FTP server on a virtual machine in my private network. I have an external ip address - <code>x.x.x.x</code> and use <code>pfSense</code> to forward ports. My FTP server is <code>vsftpd</code>, my OS is newest <code>Ubuntu 16.04</code>. </p>

<p>FTP internal address is <code>192.168.1.34</code>, external address of server itself is <code>x.x.x.x</code>.</p>

<p>I know I should forward both port 21 and 20, and I did it. Here'e NAT rules from <code>pfSense</code>:</p>

<p><a href=""https://i.sstatic.net/ii2vl.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ii2vl.png"" alt=""enter image description here""></a></p>

<pre><code>listen=NO
listen_ipv6=YES
anonymous_enable=NO
local_enable=YES
write_enable=YES
local_umask=022
dirmessage_enable=YES
use_localtime=YES
xferlog_enable=YES
connect_from_port_20=YES
secure_chroot_dir=/var/run/vsftpd/empty
pam_service_name=vsftpd

rsa_cert_file=/etc/ssl/certs/ssl-cert-snakeoil.pem
rsa_private_key_file=/etc/ssl/private/ssl-cert-snakeoil.key
ssl_enable=NO

allow_writeable_chroot=YES

port_enable=YES
pasv_addr_resolve=NO
connect_from_port_20=YES
pasv_enable=YES
pasv_min_port=40000
pasv_max_port=40100
pasv_address=x.x.x.x
</code></pre>

<p>For me everything seems fine, but when I try to connect to my server, I'm getting <code>Error:   Connection timed out. Error:   Failed to retrieve directory listing.</code></p>

<p><a href=""https://i.sstatic.net/C0jZ7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/C0jZ7.png"" alt=""enter image description here""></a></p>

<p>I did <code>sudo ufw disable</code>.</p>
","<ftp><nat><pfsense><vsftpd>","2017-01-17 14:08:25"
"826790","What does the domain in vm host TCP/IP stack DNS config do?","<p>We are pulling an Active Directory server out and modifying DNS settings on all systems. Attempting to update DNS list on a vm host results in a validation error that the domain value is not filled. How could this have been setup originally without, what does it do and is there any harm in adding our domain?</p>
","<vmware-esxi><vmware-vcenter>","2017-01-17 16:07:48"
"826814","Nginx proxy depending on the domain name","<p>I'm using a EC2 from AWS with Nginx.</p>

<p>Let's say I have three domain name: domain1.com, domain2.com, domain3.com.
Let's also say that I have three rails application on three different port: 3000, 3001, 3002.</p>

<p>All of these domains are linked to my server on the port 80.</p>

<p>Depending on which URL is requested, I want my port 80 to redirect the request to the correct port.</p>

<p>How should I configure my virtual host file? I know that the If block exist but Nginx seem to not recommend it. My plan is to have multiple sites on my EC2.</p>

<p>I can't make server block on my rails app port since it would block the rails server.</p>
","<nginx><amazon-ec2><ruby-on-rails>","2017-01-17 17:04:11"
"968531","Can I ping all AWS buckets in the world?","<p>I have 2 questions,</p>

<ol>
<li><p>Can I ping all the AWS buckets in the world? I would use a simple shell script which will curl with a dictionary of names/number combinations or some other method.</p></li>
<li><p>Will my IP be blocked if I do this? If so, why? This is not a DOS attack, is it?</p></li>
</ol>

<p>I searched in Google and Stack Overflow, but couldn’t find any related question or answer.</p>
","<amazon-web-services><ip><ping><ip-blocking>","2019-05-23 08:18:00"
"826846","Possible DOS Attack or computer ""freak out""","<p>I am a dev-ops web developer with a site running two ec2.smalls behind a load balancer on AWS. </p>

<p>Recently we saw 3-4 requests per second take down our clients site. </p>

<p>The site was down and would not come back after multiple server reboots and errors log scans for any scripts that might be causing the issue, even though no changes were recently pushed.</p>

<p>After I turned on load balancer logging I saw that 1000s of requests to a single page were coming from one IP address.</p>

<p>We forwarded the request from the load balancer to the server handling the request using X-forwarded-for and blocked the IP using an .htaccess rule.</p>

<p>While in communication with clients IT, they were notified that the IP address responsible for the flood of requests was in fact one of their internal company machines.</p>

<p>The responsible machine was remotely rebooted and all requests stopped.  The site came back online.</p>

<p>The official explanation for this was ""the computer was freaking out"".</p>

<p>Is it possible for a web browser or windows machine to make 3-4 requests per second to a load balanced web page and take it down for 5+ hours? </p>

<p>Here is what the requests looked like:</p>

<pre><code>2017-01-14T01:00:46.170447Z west-ssl XX.XXX.XX.XXX:33370 - -1 -1 -1 503 0 0 0 ""GET https://www.example.com:443/example/12 HTTP/1.1"" ""Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko"" ECDHE-RSA-AES128-SHA256 TLSv1.2
</code></pre>
","<amazon-web-services><load-balancing><ddos><denial-of-service><load-testing>","2017-01-17 18:38:21"
"968578","A parameter cannot be found that matches parameter name -ComputerName","<p>I written a script to verify the services were succefully stopped If not it will kill the running processes. Below is my script.</p>

<p>Let me know what needs be added. I am very new to powershell.</p>

<pre><code>$configData = (Get-Content (Join-Path $PSScriptRoot ""config\gp.processes.Json"") -Raw | ConvertFrom-Json)
$svcProcessList = $configData.UI

foreach ($svcProcess in $svcProcessList) {
    Write-Output ""::INFO::Looking for $svcProcess on $computerName to kill""
    $sumOfReturnValues += stop-Process -processName $svcProcess -computerName $computerName
}
if($sumOfReturnValues -ne 0)
{
    Write-Error ""Unable to stop Services as I am unable to terminate all the related processes""
    Exit 1
}

</code></pre>

<p>Error message ""</p>

<p>A parameter cannot be found that matches parameter name 'computerName'</p>
","<powershell><powershell-v4.0>","2019-05-23 13:56:04"
"759171","How to disable unwanted services in vyos?","<p>I am using VYOS server. I used nmap to scan all the open ports.</p>

<pre><code>PORT      STATE    SERVICE 
22/tcp    open     ssh
111/tcp   filtered rpcbind
1521/tcp  filtered oracle
1525/tcp  filtered orasrv
14520/tcp filtered unknown
</code></pre>

<p>According to nmap document, the filtered state is something where nmap is unable to decide whether port is open or not.</p>

<p>I used curl to connect to the port. I see SYN is coming but VYOS is not responding back?</p>

<p>How can I do any one of these?</p>

<ul>
<li>Block these ports in vyos permanently.</li>
<li>Disable the services.</li>
</ul>
","<linux><nmap><vyatta>","2016-02-23 12:03:38"
"759177","MySQL wont start after updating","<p>My MySQL server (Located in /mysql) refuses to start after i updated it from 5.1 to 5.6.</p>

<p>Permissions:</p>

<pre><code>-rwxr-xr-x. 1 mysql mysql 10485760 Dec 29 18:20 ibdata1
drwx---rwx. 2 mysql mysql    16384 Dec 29 15:56 lost+found
drwxr-xrwx. 2 mysql mysql     4096 Feb 23 06:48 mysql
drwx---rwx. 2 mysql mysql     4096 Feb 23 06:48 performance_schema
drwx---rwx. 2 mysql mysql     4096 Feb 23 06:48 test
</code></pre>

<p>Commands:</p>

<pre><code>service mysqld start
MySQL Daemon failed to start.
Starting mysqld:                                           [FAILED]
</code></pre>

<p>Log:
<a href=""http://pastebin.com/yWxSYeWp"" rel=""nofollow noreferrer"">http://pastebin.com/yWxSYeWp</a></p>

<p>my.cnf:</p>

<pre><code>[mysqld]
datadir=/mysql
socket=/mysql/mysql.sock
user=mysql
innodb_file_per_table
innodb_flush_method=O_DIRECT
innodb_log_file_size=1G
innodb_buffer_pool_size=4G

# Disabling symbolic-links is recommended to prevent assorted security risks
symbolic-links=0

[client]
socket=/mysql/mysql.sock

[mysqld_safe]
log-error=/var/log/mysqld.log
pid-file=/var/run/mysqld/mysqld.pid
</code></pre>

<p>Using Centos 6.7</p>
","<mysql>","2016-02-23 12:33:46"
"759196","Subversion access control for repo sub-folders","<p>Platform/Versions:<br>
   Windows Server 2008 R2 Std SP1 (64 bit)<br>
   CollabNet Subversion Apache 2.2.23 (win32)<br>
   CollabNet Subversion Client Svnserve V1.7.8<br>
   Authentication using SSPI (active directory)<br>
   Users run Tortoise SVN Client V1.8.11 (64 bit) or higher versions on Windows 7 Pro  </p>

<p>We use an access control file to limit who has access to subversion top-level repositories.  We are now wanting to limit access to certain folders under the top-level. We’ve referenced the following examples, but they don’t seem to work for us:</p>

<p><a href=""https://nithint.wordpress.com/2009/12/17/format-of-svn-access-file-for-path-based-authorization/"" rel=""nofollow noreferrer"">https://nithint.wordpress.com/2009/12/17/format-of-svn-access-file-for-path-based-authorization/</a><br>
<a href=""https://www.open.collab.net/community/subversion/svnbook/svn.serverconfig.pathbasedauthz.html"" rel=""nofollow noreferrer"">https://www.open.collab.net/community/subversion/svnbook/svn.serverconfig.pathbasedauthz.html</a>  </p>

<p>We have found that if you have access at the top level, then you get access to all lower level folders including the folders where someone should be denied access. Additionally, if you don’t have access at the top level but are given access at the lower level, you are still blocked from accessing the lower level. (We’re ok with the latter but find it curious as it seems to contradict what the websites say regarding permissions, that is, permissions at lower levels should override permissions at upper levels.) The access control at the lower level seems to be ignored.  </p>

<p>What we want:</p>

<p>Junk_repo – top-level, grant Beth, Eric and Joe access<br>
Junk_repo/Commercial – grant Beth, Eric and Joe access<br>
Junk_repo/Military – grant Beth &amp; Eric access. Deny Joe access  </p>

<p>We tried using groups and setting the permissions by group. (We use domain\userid active directory authentication therefore our access control file also uses domain\userid format.)  </p>

<p>The groups:</p>

<p>Junk_repo_team = domain\beth, domain\eric, domain\joe<br>
Junk_repo_comm_team = domain\beth, domain\eric, domain\joe<br>
Junk_repo_mil_team = domain\beth, domain\eric   (No Joe)  </p>

<p>The access:    </p>

<pre><code>[Junk_repo:/]
@Junk_repo_team = rw   //Beth, Eric Joe have access 

[Junk_repo:/Commercial]  
@Junk_repo_comm_team = rw   // commercial team (Beth, Eric, Joe) has access  

[Junk_repo:/Military]  
@Junk_repo_mil_team = rw    // military team (Beth, Eric) has access. No access for Joe
</code></pre>

<p>The above failed so we then tried setting permissions by listing the userids at each level:  </p>

<pre><code>[Junk_repo:/]  
domain\beth = rw   //Beth, Eric and Joe have access  
domain\eric = rw   
domain\joe = rw

[Junk_repo:/Commercial]  
domain\beth = rw   //Beth, Eric and Joe have access  
domain\eric = rw   
domain\joe = rw  

[Junk_repo:/Military]  
//Beth and Eric have access.  Joe does not have access. 
domain\beth = rw
domain\eric = rw   
domain\joe=
//We even tried  specifically denying Joe access to the Military folder by listing his id without anything after the equal sign. 
</code></pre>

<p>In both scenarios Joe was able to access the Junk_repo/Military folder where we specifically did not want him to have access to it.   </p>

<p>Do you have any experience with blocking access at various levels of a repo? If so, do you see anything obvious that we’re doing wrong?</p>

<p>SVN-location</p>

<pre><code>&lt;Location /Junk_repo&gt;
DAV svn
SVNPath E:/svn_repository/Junk_repo
AuthType Basic
AuthName ""Subversion Junk_repo repository""
Require valid-user
ErrorDocument 404 default
&lt;/Location&gt;
</code></pre>
","<apache-2.2><active-directory><svn><access-control-list><repository>","2016-02-23 14:19:54"
"826923","Apache VirtualHost Hell - fails to redirect sometimes, throws odd SSL error other times: What am I doing wrong?","<p>I'm trying to configure my website to redirect all traffic to the www subdomain, and redirect all http requests to https requests. I know it's not a problem with by DNS records (which include an A record for www and cloud) because my configuration worked perfectly until about a week ago when I reinstalled everything. The problem experienced is that it behaves very funkily.</p>
<p>(I've replaced my domain with &quot;example.com&quot; to sanitize the post.)</p>
<h2>What doesn't work:</h2>
<pre><code>http://example.com        - redirects to https://www.example.com,
                            but yields SSL_PROTOCOL_ERROR
http://cloud.example.com  - redirects to https://www.example.com/myfiles/,
                            but yields SSL_PROTOCOL_ERROR
https://example.com       - no redirect to www, and yields SSL_PROTOCOL_ERROR
https://www.example.com   - yields SSL_PROTOCOL_ERROR
https://cloud.example.com - no redirect to www, and yields SSL_PROTOCOL_ERROR
</code></pre>
<h2>What does work:</h2>
<pre><code>http://www.example.com - works like a dream
</code></pre>
<h1>All of my Apache Configuration Steps (from a clean install):</h1>
<pre><code>sudo echo -e &quot;\\ndeb http://ftp.debian.org/debian jessie-backports main&quot; &gt;&gt; /etc/apt/sources.list

sudo apt-get update &amp;&amp; sudo apt-get -y upgrade

# Because you'll want the latest certbot...
sudo apt-get -y install python-certbot-apache -t jessie-backports

sudo apt-get -y install apache2 php5 libapache2-mod-php5 php5-mcrypt php5-mysql php5-cli

sudo a2enmod rewrite
</code></pre>
<h2>Put the following (as root) at the end of /etc/apache2/apache2.conf</h2>
<pre><code>ServerName example.com

&lt;VirtualHost *:80&gt;
    ServerName example.com
    Redirect permanent / https://www.example.com/
&lt;/VirtualHost&gt;
&lt;Directory /var/www/(.*)&gt;
  RewriteEngine On
  RewriteCond %{HTTPS} off
  RewriteRule (.*) https://%{HTTP_HOST}%{REQUEST_URI}
&lt;/Directory&gt;
</code></pre>
<h2>Creating the VirtualHosts and Certificates</h2>
<pre><code>sudo tee /etc/apache2/sites-available/www.conf &lt;&lt; &quot;EOP&quot;
&lt;VirtualHost *:80&gt;
  ServerName www.example.com
  Redirect / https://www.example.com/
&lt;/VirtualHost&gt;
&lt;VirtualHost *:443&gt;
  DocumentRoot /var/www/html/
  ServerName www.example.com
&lt;/VirtualHost&gt;
EOP

sudo a2ensite www

sudo service apache2 restart

sudo certbot --apache --domain www.example.com
#I apply this to /etc/apache2/sites/enabled/default-ssl.conf

sudo mkdir -p /var/www/html/myfiles/

sudo tee /etc/apache2/sites-available/cloud.conf &lt;&lt; &quot;EOP&quot;
&lt;VirtualHost *:80&gt;
  ServerName cloud.example.com
  Redirect / https://www.example.com/myfiles/
&lt;/VirtualHost&gt;
&lt;VirtualHost *:443&gt;
  ServerName cloud.example.com
  Redirect / https://www.example.com/myfiles/
&lt;/VirtualHost&gt;
EOP

sudo a2ensite cloud

sudo service apache2 restart

sudo apache2ctl configtest #which reports everything's ok
</code></pre>
<h2>More Information about Server:</h2>
<p>The server is a clean install of Debian 8.6 (amd64), and the Apache version number is 2.4, so stability of the software isn't the issue.</p>
<h2>My SSL Problem</h2>
<p>A <code>cat /var/log/apache2/error.log</code> throws me the following:</p>
<pre><code>[ssl:warn] [pid 11737] AH01909: www.example.com:443:0 server certificate does NOT include an ID which matches the server name
</code></pre>
","<apache-2.4><virtualhost><ssl-certificate><lets-encrypt><certbot>","2017-01-18 04:24:39"
"896936","What can cause a local administrator account to be unable to access a local folder, even when using special privileges that bypass NTFS security?","<p>I have a Windows 2012 R2 server where local administrator accounts (or at least one particular one) cannot access the ""System Volume Information"" folder. This works for the same account with the same exact permissions on another server. We use a program that scans the server's local disks which I am sure uses the backup privilege in doing so because it is normally able to bypass NTFS ACLs, but not in this case.</p>

<p>These are the things that I verified all the servers (working and non-working) have in common: The (service) account in question is a member of the local administrators group. The folder permissions are exactly the same (default): Allow Full Control to SYSTEM, applies to this folder, subfolders and files. UAC is also completely disabled. This to me indicates that this local admin account is not able to exert the backup privilege on this server for some reason.</p>

<p>My question is mainly how can this happen, and what can I do to troubleshoot it? I don't know enough about this depth of Windows security mechanisms to even know where to start. If an account is a local administrator I would expect this to work. I've searched high and low and don't see anything out of the ordinary with security groups, NTFS permissions, or group policies. I've also searched the net but unfortunately haven't been able to find anyone mentioning this kind of scenario or anything related that gives me a clue. Any help would be appreciated.</p>

<p>Please note that some other folders on this server suffer from the same issue, but I'm specifically discussing the system volume information folder because it is present and has the same default permissions on all the servers, making it easier to compare. Also, I have now checked the user rights assignments on the local security policy of the server with the issue, and it does grant ""Back up files and directories"" policy set to Administrators and Backup Operators, which seems to be the default.</p>
","<windows><security><windows-server-2012-r2><permissions><ntfs>","2018-02-12 19:24:08"
"826937","How to resolve ""fail to connect to MySQL Server at localhost:3306 with user root""?","<p>I have tried over and over to connect to the local server using MySQL Workbench 6.3 on my computer using the following parameters, but it won't let me connect. Do you think it is a firewall issue? </p>

<p>Here are my parameters: </p>

<pre><code>hostname: localhost
port: 3306
username: root
password: [not_really]
</code></pre>

<p><a href=""https://i.sstatic.net/S0UMw.jpg"" rel=""nofollow noreferrer"">Here is the picture of my problem for anyone's viewing.</a></p>
","<mysql><firewall>","2017-01-18 06:35:43"
"826957","I accidentally deactivated the internet of my VPS (How to enable it again if I lost the connection?)","<p>I accessed the: Control Panel > Network and Internet > Network and Sharing Center. So, I clicked in ""Ethernet"". Oponed the ""Ethernet Status"" for me and I clicked in ""Disable"".</p>

<p>So, I lost my connection!!!</p>

<p>My intention was change my IP, but now, I can't login anymore, I need help!</p>

<p><a href=""https://i.sstatic.net/Gt8Pj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Gt8Pj.png"" alt=""network settings screenshot""></a></p>

<p>PS1: My VPS is a Free VPS of Amazon.</p>

<p>PS2: I'm asking help here because the Amazon's Forum didn't allow me open a thread there. I received the following msg: ""Your account is not ready for posting messages yet. Please try again later.""</p>

<p>PS3: I ask help for Amazon's technical support, but, probably they will not answer me, because the Amazon don't give tech support for free VPS.</p>
","<vps><internet>","2017-01-18 09:18:12"
"827008","Find and rm command not deleting recursively","<p>We have a simple scripts that suppose to delete old folders.
The problems is that it doesn't delete them recursively.</p>

<p>This is the command:</p>

<pre><code>find $PWD -maxdepth 5 -mtime +80 | xargs -I{} rm -Rvf {}
</code></pre>

<p>It removes the files and folders but suddenly tries to remove a file from a directory it already deleted.</p>

<p>The server is running <em>CentOS</em>, and the folders are mounted using <em>NFS</em> from the storage.</p>

<p>What can we do?</p>
","<bash><centos6><find>","2017-01-18 12:51:26"
"827038","Script to search bots from log file","<p>I have a little log-file, where i should find the bots.</p>

<p>File:</p>

<pre><code>Mon, 22 Aug 2016 13:15:39 +0200|178.57.66.225|fxsciaqulmlk| - |user logged in| -
Mon, 22 Aug 2016 13:15:39 +0200|178.57.66.225|fxsciaqulmlk| - |user changed password| -
Mon, 22 Aug 2016 13:15:39 +0200|178.57.66.225|fxsciaqulmlk| - |user logged off| -
Mon, 22 Aug 2016 13:15:42 +0200|178.57.66.225|faaaaaa11111| - |user logged in| -
Mon, 22 Aug 2016 13:15:49 +0200|178.57.66.215|terdsfsdfsdf| - |user logged in| -
Mon, 22 Aug 2016 13:15:49 +0200|178.57.66.215|terdsfsdfsdf| - |user changed password| -
Mon, 22 Aug 2016 13:15:49 +0200|178.57.66.215|terdsfsdfsdf| - |user logged off| -
Mon, 22 Aug 2016 13:15:59 +0200|178.57.66.205|erdsfsdfsdf| - |user logged in| -
Mon, 22 Aug 2016 13:15:59 +0200|178.57.66.205|erdsfsdfsdf| - |user logged in| -
Mon, 22 Aug 2016 13:15:59 +0200|178.57.66.205|erdsfsdfsdf| - |user changed password| -
Mon, 22 Aug 2016 13:15:59 +0200|178.57.66.205|erdsfsdfsdf| - |user logged off| -
Mon, 22 Aug 2016 13:17:50 +0200|178.57.66.205|abcbbabab| - |user logged in| -
Mon, 22 Aug 2016 13:17:50 +0200|178.57.66.205|abcbbabab| - |user changed password| -
Mon, 22 Aug 2016 13:17:50 +0200|178.57.66.205|abcbbabab| - |user changed profile| -
Mon, 22 Aug 2016 13:17:50 +0200|178.57.66.205|abcbbabab| - |user logged off| -
Mon, 22 Aug 2016 13:19:19 +0200|178.56.66.225|fxsciulmla| - |user logged in| -
Mon, 22 Aug 2016 13:19:19 +0200|178.56.66.225|fxsciulmla| - |user changed password| -
Mon, 22 Aug 2016 13:19:19 +0200|178.56.66.225|fxsciulmla| - |user logged off| -
Mon, 22 Aug 2016 13:20:42 +0200|178.57.67.225|faaaa0a1111| - |user logged in| -
</code></pre>

<p>In this file, i have a lot ip, logins, usernames. first of all, i thought about uniq and count of ip. 
So, what i should do. I should use events only: user logged in, user changed off, user logged off. Next, i should show the users, that logged in the same time. First three lines, user has logged in, changed password, logged off. This time is 13:15:39. And user fxsciaqulmlk from ip 178.57.66.225 is a bot, because operation of event has made at the same time. My script:</p>

<pre><code>log_file=/root/log
log_after=/root/after_log
temp_file=/root/temp
temp_file2=/root/temp2
uniq_file=/root/uniq
uniq_file2=/root/uniq2
result_uniq=/root/result_uniq
result_file=/root/result

cat /dev/null &gt; $log_after
cat /dev/null &gt; $temp_file
cat /dev/null &gt; $temp_file2
cat /dev/null &gt; $uniq_file
cat /dev/null &gt; $uniq_file2
cat /dev/null &gt; $result_file

grep ""changed password\|logged in\|logged off"" $log_file &gt; $log_after
cat $log_after | awk '{print $6}' | awk -F ""|"" '{print $2,$3}' | tail -n 20 &gt; $temp_file
cat $log_after | awk '{print $5}' | tail -n 20 &gt; $temp_file2
uniq -c $temp_file | awk '{print $1}' &gt; $uniq_file
uniq -c $temp_file2 | awk '{print $1}' &gt; $uniq_file2

awk 'FNR==NR{a[$1]++;next}!a[$1]' $uniq_file  $uniq_file2  &gt; $result_uniq

if [ -s $result_uniq ] &amp;&amp; [ -f $result_uniq ]; then
 echo ""File is not  empty""
 echo ""Differences:""
 cat $result_uniq
 echo ""Need to think""
 exit 0
else
 echo ""File is empty""
 echo ""We can use one file from uniq""
fi

for i in `uniq -c $temp_file | awk '{print $1}'`; do
if [ $i -gt 2 ]; then
s=`uniq -c $temp_file | awk '$1 == '$i | awk '{print $3}'`
ss=`uniq -c $temp_file | awk '$1 == '$i | awk '{print $2}'`
echo ""Tho boot is user $s with ip $ss""
fi
done
</code></pre>

<p>This question is an exact duplicate of:</p>

<pre><code>Script to search bots from log file 1 answer
</code></pre>

<p>I have written this message before. So, i have done my task, but i have a little not correct output. So... I will start. I have a log file:</p>

<p>Mon, 22 Aug 2016 13:15:39 +0200|178.57.66.225|fxsciaqulmlk| - |user logged in| -
Mon, 22 Aug 2016 13:15:39 +0200|178.57.66.225|fxsciaqulmlk| - |user changed password| -
Mon, 22 Aug 2016 13:15:39 +0200|178.57.66.225|fxsciaqulmlk| - |user logged off| -
Mon, 22 Aug 2016 13:15:42 +0200|178.57.66.225|faaaaaa11111| - |user logged in| -
Mon, 22 Aug 2016 13:15:49 +0200|178.57.66.215|terdsfsdfsdf| - |user logged in| -
Mon, 22 Aug 2016 13:15:49 +0200|178.57.66.215|terdsfsdfsdf| - |user changed password| -
Mon, 22 Aug 2016 13:15:49 +0200|178.57.66.215|terdsfsdfsdf| - |user logged off| -
Mon, 22 Aug 2016 13:15:59 +0200|178.57.66.205|erdsfsdfsdf| - |user logged in| -
Mon, 22 Aug 2016 13:15:59 +0200|178.57.66.205|erdsfsdfsdf| - |user logged in| -
Mon, 22 Aug 2016 13:15:59 +0200|178.57.66.205|erdsfsdfsdf| - |user changed password| -
Mon, 22 Aug 2016 13:15:59 +0200|178.57.66.205|erdsfsdfsdf| - |user logged off| -
Mon, 22 Aug 2016 13:17:50 +0200|178.57.66.205|abcbbabab| - |user logged in| -
Mon, 22 Aug 2016 13:17:50 +0200|178.57.66.205|abcbbabab| - |user changed password| -
Mon, 22 Aug 2016 13:17:50 +0200|178.57.66.205|abcbbabab| - |user changed profile| -
Mon, 22 Aug 2016 13:17:50 +0200|178.57.66.205|abcbbabab| - |user logged off| -
Mon, 22 Aug 2016 13:19:19 +0200|178.56.66.225|fxsciulmla| - |user logged in| -
Mon, 22 Aug 2016 13:19:19 +0200|178.56.66.225|fxsciulmla| - |user changed password| -
Mon, 22 Aug 2016 13:19:19 +0200|178.56.66.225|fxsciulmla| - |user logged off| -
Mon, 22 Aug 2016 13:20:42 +0200|178.57.67.225|faaaa0a1111| - |user logged in| -</p>

<p>So, what i should do. I should use events only: user logged in, user changed off, user logged off. Next, i should show the users, that logged in the same time. First three lines, user has logged in, changed password, logged off. This time is 13:15:39. And user fxsciaqulmlk from ip 178.57.66.225 is a bot, because operation of event has made at the same time. My script:</p>

<pre><code>#!/bin/bash
# you should add it script in crontab, like this
#*/2 * * * *      /name_of_this_script.sh
# you should change variable way in $log_file to your own way

log_file=/root/log
log_after=/root/after_log
temp_file=/root/temp
temp_file2=/root/temp2
uniq_file=/root/uniq
uniq_file2=/root/uniq2
result_uniq=/root/result_uniq
result_file=/root/result

cat /dev/null &gt; $log_after
cat /dev/null &gt; $temp_file
cat /dev/null &gt; $temp_file2
cat /dev/null &gt; $uniq_file
cat /dev/null &gt; $uniq_file2
cat /dev/null &gt; $result_file

grep ""changed password\|logged in\|logged off"" $log_file &gt; $log_after
cat $log_after | awk '{print $6}' | awk -F ""|"" '{print $2,$3}' | tail -n 20 &gt; $temp_file
cat $log_after | awk '{print $5}' | tail -n 20 &gt; $temp_file2
uniq -c $temp_file | awk '{print $1}' &gt; $uniq_file
uniq -c $temp_file2 | awk '{print $1}' &gt; $uniq_file2

awk 'FNR==NR{a[$1]++;next}!a[$1]' $uniq_file  $uniq_file2  &gt; $result_uniq

if [ -s $result_uniq ] &amp;&amp; [ -f $result_uniq ]; then
 echo ""File is not  empty""
 echo ""Differences:""
 cat $result_uniq
 echo ""Need to think""
 exit 0
else
 echo ""File is empty""
 echo ""We can use one file from uniq""
fi

for i in `uniq -c $temp_file | awk '{print $1}'`; do
if [ $i -gt 2 ]; then
s=`uniq -c $temp_file | awk '$1 == '$i | awk '{print $3}'`
ss=`uniq -c $temp_file | awk '$1 == '$i | awk '{print $2}'`
echo ""Tho boot is user $s with ip $ss""
fi
done
</code></pre>

<p>Everything ok. But i have bad result:</p>

<pre><code>Tho boot is user fxsciaqulmlk
terdsfsdfsdf
abcbbabab
fxsciulmla with ip 178.57.66.225
178.57.66.215
178.57.66.205
178.56.66.225
Tho boot is user fxsciaqulmlk
terdsfsdfsdf
abcbbabab
fxsciulmla with ip 178.57.66.225
178.57.66.215
178.57.66.205
178.56.66.225
Tho boot is user erdsfsdfsdf with ip 178.57.66.205
Tho boot is user fxsciaqulmlk
terdsfsdfsdf
abcbbabab
fxsciulmla with ip 178.57.66.225
178.57.66.215
178.57.66.205
178.56.66.225
Tho boot is user fxsciaqulmlk
terdsfsdfsdf
abcbbabab
fxsciulmla with ip 178.57.66.225
178.57.66.215
178.57.66.205
178.56.66.225
</code></pre>

<p>Where i have a mistake ? I can't understand, where i have a mistake in code ? I am so sorry for repeating question, but i have done this script, i need a little help.</p>
","<linux><bash><scripting><shell><shell-scripting>","2017-01-18 14:22:29"
"897073","Creating a user on exchange through LDAP","<p>I have managed to use the <code>LDAP module</code> that comes standard with <code>PHP</code> to <code>find, add, modify, and authenticate user accounts and groups</code> to <code>Active Directory</code>. </p>

<p>The only thing that has me stumped is <code>creating a mailbox</code> for a new user in exchange.</p>

<p>Now I notice that people point to <code>ews, adLDAP, ldaptools</code>.</p>

<p>I am wondering if it is possible to just use <code>LDAP</code> to accomplish this goal. Has anyone attempted this?</p>

<p>Also when adding <code>user mailboxs</code> in <code>exchange</code>, back-end uses <code>load balancing</code> and automatically distributes the accounts between the available DBs, can this be utilized when adding <code>user mailboxes</code> with <code>php</code>?</p>

<p>Is this even possible through plain <code>LDAP</code>? if not what are my choices?</p>

<p><strong>Response to BastianW:</strong></p>

<p>I have tried using <code>power-shell</code> and executing it from my <code>php script</code>:</p>

<p>1- php script resides on a webserver</p>

<p>2-exchange resides on another server</p>

<p>3-the command I built to execute on <code>remote-exchange</code>:</p>

<pre><code>$command1 =  'C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -command'. "". 'C:\Program Files\Microsoft\Exchange Server\V15\bin\RemoteExchange.ps1'; Connect-ExchangeServer -auto -ClientApplication:ManagementShell; enable-mailuser -identity tmeow@comp.com -ExternalEmailAddress tmeow@comp.com"";
shell_exec($command1);
</code></pre>

<p>I can run this in <code>power-shell</code> with no problem, but in <code>php</code> I can not run it successfully.</p>
","<php><exchange><ldap>","2018-02-13 16:04:57"
"968812","Relative URL is relative to wrong absolute URL on web","<h3>Background</h3>

<p>I am building a website. The files are:</p>

<pre><code>/index.php
/pic.jpg
/dir/index.php
/dir/pic.jpg
</code></pre>

<p>As we know, people can visit my site's <code>dir/index.php</code> page using different URLs: </p>

<pre><code>1. site.com/dir
2. site.com/dir/ 
3. site.com/dir/index.php
</code></pre>

<p>I have a link in <code>/dir/index.php</code></p>

<pre><code>&lt;a target=""_blank"" href=""pic.jpg""&gt;
</code></pre>

<p>which I want to relativly link to <code>/dir/pic.jpg</code></p>

<h3>The Problem</h3>

<pre><code>---------------------/-------------------------------                    
                             index.php
           pic.jpg
              ^
              |
        ------|-----/dir/------------------------
        |     |                                 |
        |     | 1                               |
        |     |                                 |
        |     |       2 or 3                    |
        | index.php ---------&gt; pic.jpg          |
        -----------------------------------------
-----------------------------------------------------
</code></pre>

<p>The <code>href=""pic.jpg""</code> leads people to different places when people visit the web via different urls.</p>

<p>People using url 1 get <code>/pic.jpg</code> , and others using url 2 or 3 get <code>/dir/pic.jpg</code> </p>

<p>The same problem goes with not just <code>&lt;a&gt;</code> but other HTML tags.</p>

<p>How can I write a relative URL in <code>/dir/index.php</code> that links to <code>/dir/pic.jpg</code> whatever URL visitors use?</p>

<p>I need to keep folder <code>dir</code> portable (I may change <code>dir</code>'s name, or use <code>dir</code> folder on another website, so it may have different path). We need a solution other than hardcoding <code>href=""/dir/pic.jpg""</code>.</p>
","<httpd><website><web><url><html>","2019-05-25 09:37:12"
"827134","Windows firewall blocks port connection even it's allowed","<p>I have Windows firewall rules to allow port 443 and 9000 inbound and outbound, and allowed the apache httpd.exe.</p>
<p>As shown by my firewall log, both ports are dropped. Below are screenshots of firewall rules for inbound and outbound, firewall logs showing the connection being dropped, and IP configuration on the Windows server.</p>
<p><a href=""https://i.sstatic.net/G57M1.jpg"" rel=""nofollow noreferrer"">Firewall rules</a></p>
<p><a href=""https://i.sstatic.net/bJo8p.jpg"" rel=""nofollow noreferrer"">Firewall logs and IP config</a></p>
","<apache-2.2><windows><firewall><vmware-vsphere>","2017-01-18 19:39:58"
"759450","Vanity nameserver problems","<p>After creating a nameserver subdomain named <em>1/2/3.buzztera.gr</em> with the help from a <a href=""https://www.digitalocean.com/community/tutorials/how-to-create-vanity-or-branded-nameservers-with-digitalocean-cloud-servers"" rel=""nofollow noreferrer"">Digital Ocean</a> tutorial and creating glue records, I changed my nameservers to point to ns1/2/3.buzztera.gr from the registrar but this <a href=""http://www.webdnstools.com/dnstools/check-domain-results"" rel=""nofollow noreferrer"">test</a> says </p>

<blockquote>
  <p>One or more name servers is not authoritative for this domain (or did not respond in time).</p>
</blockquote>

<p>and</p>

<p>ns1.buzztera.gr : no nameserver</p>

<p>ns2.buzztera.gr : no nameserver</p>

<p>ns3.buzztera.gr : no nameserver</p>

<p>what else do i need to do ?</p>

<p>My current setup is</p>

<p><strong>Glue Records :</strong></p>

<p>ns1.buzztera.gr A (digital ocean ns1.digitalocean.com ip)</p>

<p>ns2.buzztera.gr A (digital ocean ns2.digitalocean.com ip)</p>

<p>ns3.buzztera.gr A (digital ocean ns3.digitalocean.com ip)</p>

<p><strong>DNS Settings (from DO)</strong></p>

<p>ns1.buzztera.gr NS</p>

<p>ns2.buzztera.gr NS </p>

<p>ns3.buzztera.gr NS </p>

<p>ns1.buzztera.gr A (digital ocean ns1.digitalocean.com ip)</p>

<p>ns2.buzztera.gr A (digital ocean ns2.digitalocean.com ip)</p>

<p>ns3.buzztera.gr A (digital ocean ns3.digitalocean.com ip)</p>

<p><strong>Registar nameserver:</strong></p>

<p>ns1.buzztera.gr</p>

<p>ns2.buzztera.gr</p>

<p>ns3.buzztera.gr</p>
","<domain-name-system><nameserver>","2016-02-24 13:42:33"
"759512","Linux + how to know why Linux machine is hung from the messages on the console","<p>As in the picture below, my Linux machine was hung and I couldn't login.</p>

<p>How do I identify the reason for the ""hang"", according to the messages on the console? </p>

<p>I searched <code>/var/log/messages</code> for more info (but I get lost in there, can't find anything useful), but I do not know exactly where to find core files.</p>

<p>What are the other files that we can find info in for this situation ?</p>

<p><img src=""https://i.sstatic.net/RTglJ.png"" alt=""enter image description here""></p>

<p><img src=""https://i.sstatic.net/3lV0W.png"" alt=""enter image description here""></p>
","<linux><log-files><kernel-panic><dmesg><message-queuing>","2016-02-24 17:34:05"
"759568","How do I redirect all folder requests to a data source in nginx?","<p>I have to common resource folders <code>images</code> and <code>reveal.js</code> on my server that I'm trying to redirect urls that's include those folder names in their url to the same source.  For example: </p>

<pre><code>/images/1.gif --&gt; /path/to/images/1.gif
/some/other/url/images/1.gif --&gt; /path/to/images/1.gif
</code></pre>

<p>similar for <code>reveal.js</code>:</p>

<pre><code>/reveal.js/path/to/element.js --&gt; /path/to/reveal.js/path/to/element.js
/some/other/url/reveal.js/element.js --&gt; /path/to/reveal.js/element.js
</code></pre>

<p>How do I do this using Nginx?</p>

<hr>

<pre><code>server {
    location / {
        root /data/www;
    }

    location ~* ""^.+\/images\/"" {
        root /data/images; 
    }
}
</code></pre>

<p>This is my configuration file.  Requests to <code>site.com/images/image.gif</code> and site.com/some/other/url/images/image.gif<code>should all point to the file</code>/data/images/image.gif`</p>
","<nginx><redirect>","2016-02-24 20:51:55"
"897366","postfix fails to autheticate aginst saslauthd","<p>I have a weird problem trying to autheticate postfix against saslauthd.</p>

<p>I am on a standard linux system:
Linux smswtc 3.16.0-4-amd64 #1 SMP Debian 3.16.51-3 (2017-12-13) x86_64 GNU/Linux</p>

<p>postfix configuration is very basic</p>

<pre><code>root@smswtc:/etc/postfix# cat /etc/postfix/main.cf |grep -v ""#"" |grep -v ^$

smtpd_banner = $myhostname ESMTP $mail_name (Debian/GNU) 

biff = no    

append_dot_mydomain = no 

readme_directory = no    

smtpd_tls_cert_file=/etc/ssl/certs/ssl-cert-snakeoil.pem    

smtpd_tls_key_file=/etc/ssl/private/ssl-cert-snakeoil.key    

smtpd_use_tls=yes 

smtpd_tls_session_cache_database =   btree:${data_directory}/smtpd_scache 

smtp_tls_session_cache_database    = btree:${data_directory}/smtp_scache 

smtpd_relay_restrictions = permit_mynetworks permit_sasl_authenticated defer_unauth_destination    

myhostname = smswtc 

alias_maps = hash:/etc/aliases 

alias_database = hash:/etc/aliases 

myorigin = /etc/mailname 

mydestination = wtc99.com, localhost, localhost.localdomain, localhost relayhost = out.alice.sm    

mynetworks = 127.0.0.0/8 [::ffff:127.0.0.0]/104 [::1]/128 10.0.0.0/8     

mailbox_command = procmail -a ""$EXTENSION"" 

mailbox_size_limit = 0    

recipient_delimiter = + 

inet_interfaces = all 

smtpd_sasl_local_domain = """" 

smtpd_sasl_auth_enable = yes 

broken_sasl_auth_clients = yes 

relay_domains = wtc99.com 

smtpd_recipient_restrictions = permit_sasl_authenticated, permit_mynetworks, reject_unauth_destination

root@smswtc:/etc/postfix# cat sasl/smtpd.conf 

pwcheck_method: saslauthd

mech_list: PLAIN LOGIN

saslauthd configuration is again very basic:
</code></pre>

<p>/etc/default/saslauthd-postfix</p>

<pre><code>root@smswtc:/etc/postfix# cat /etc/default/saslauthd-postfix 

START=yes

DESC=""SASL Auth. Daemon for Postfix""

NAME=""saslauthd""      # max. 15 char.

OPTIONS=""-c -m /var/spool/postfix/var/run/saslauthd""
</code></pre>

<p>I have started saslauthd in debug mode and I made a little test</p>

<p>1) basic sasl auth test ----------> sasl auth test is OK</p>

<p>request -</p>

<pre><code>root@smswtc:/var/log# testsaslauthd  -u user -p pass -r smswtc -f /var/spool/postfix/var/run/saslauthd/mux
0: OK ""Success.""
</code></pre>

<ul>
<li><p>saslauthd[9759] :rel_accept_lock : released accept lock</p>

<p>saslauthd[9758] :get_accept_lock : acquired accept lock</p>

<p>saslauthd[9759] :cache_get_rlock : attempting a read lock on slot: 138</p>

<p>saslauthd[9759] :cache_lookup    : [login=user] [service=imap] 
[realm=smswtc]: found with valid passwd</p>

<p>saslauthd[9759] :cache_un_lock   : attempting to release lock on slot: 138</p>

<p>saslauthd[9759] :do_auth         : auth success (cached): [user=user] [service=imap] [realm=smswtc]</p>

<p>saslauthd[9759] :do_request      : response: OK</p></li>
</ul>

<p>The I have tried authentication through in postfix. No success:</p>

<p>2) postfix auth ---------> NOT OK</p>

<p>request -</p>

<pre><code>smpp@smsbg:~/smsqueue$ telnet 10.1.0.249 25

Trying 10.1.0.249...

Connected to 10.1.0.249.

Escape character is '^]'.

220 smswtc ESMTP Postfix (Debian/GNU)

ehlo smpp

250-smswtc

250-PIPELINING 

250-SIZE 10240000

250-VRFY

250-ETRN

250-STARTTLS

250-AUTH PLAIN LOGIN

250-AUTH=PLAIN LOGIN

250-ENHANCEDSTATUSCODES

250-8BITMIME

250 DSN

AUTH PLAIN dXNlcm5hbWUAdXNlcgBwYXNz

535 5.7.8 Error: authentication failed: authentication failure

quit

221 2.0.0 Bye

Connection closed by foreign host.
</code></pre>

<p>postfix mail log -</p>

<pre><code>root@smswtc:/etc/postfix# cat /var/log/mail.log

Feb 15 10:24:31 smswtc postfix/smtpd[10399]: connect from unknown[10.1.1.249]
Feb 15 10:24:40 smswtc postfix/smtpd[10399]: warning: SASL authentication failure: Requested identity not authenticated identity
Feb 15 10:24:40 smswtc postfix/smtpd[10399]: warning: unknown[10.1.1.249]: SASL PLAIN authentication failed: authentication failure
Feb 15 10:24:42 smswtc postfix/smtpd[10399]: disconnect from unknown[10.1.1.249]
</code></pre>

<p>Postfix complains about failed authentication but log from saslauthd shows that authetication was successful:</p>

<p>s</p>

<pre><code>aslauthd[9767] :get_accept_lock : acquired accept lock

saslauthd[9768] :rel_accept_lock : released accept lock

saslauthd[9768] :cache_get_rlock : attempting a read lock on slot: 2

saslauthd[9768] :cache_lookup    : [login=user] [service=smtp] [realm=""""]: found with valid passwd

saslauthd[9768] :cache_un_lock   : attempting to release lock on slot: 2

saslauthd[9768] :do_auth         : auth success (cached): [user=user] [service=smtp] [realm=""""]

saslauthd[9768] :do_request      : response: OK
</code></pre>

<p>Also from strace log of the smtpd process of process it is clear that it receives OK back from saslauthd</p>

<pre><code>root@smswtc:/etc/postfix# strace -f -s 512 -p 10399

.....

write(10, ""250-smswtc\r\n250-PIPELINING\r\n250-SIZE 10240000\r\n250-VRFY\r\n250-ETRN\r\n250-STARTTLS\r\n250-AUTH PLAIN LOGIN\r\n250-AUTH=PLAIN LOGIN\r\n250-ENHANCEDSTATUSCODES\r\n250-8BITMIME\r\n250 DSN\r\n"", 173) = 173

poll([{fd=10, events=POLLIN}], 1, 300000) = 1 ([{fd=10, revents=POLLIN}])
read(10, ""AUTH PLAIN dXNlcm5hbWUAdXNlcgBwYXNz\r\n"", 4096) = 37

socket(PF_LOCAL, SOCK_STREAM, 0)        = 12

connect(12, {sa_family=AF_LOCAL, sun_path=""/var/run/saslauthd/mux""}, 110) = 0

**writev(12, [{""\0\4user\0\4pass\0\4smtp\0\2\""\"""", 22}], 1) = 22

read(12, ""\0\2"", 2)                     = 2

read(12, ""OK"", 2)                       = 2

close(12)                               = 0**

sendto(8, ""&lt;20&gt;Feb 15 10:24:40 postfix/smtpd[10399]: warning: SASL authentication failure: Requested identity not authenticated identity"", 125, MSG_NOSIGNAL, NULL, 0) = 125

sendto(8, ""&lt;20&gt;Feb 15 10:24:40 postfix/smtpd[10399]: warning: unknown[10.1.1.249]: SASL PLAIN authentication failed: authentication failure"", 128, MSG_NOSIGNAL, NULL, 0) = 128

poll([{fd=10, events=POLLOUT}], 1, 300000) = 1 ([{fd=10, revents=POLLOUT}])

....
</code></pre>

<p>Any idea?</p>
","<postfix><saslauthd>","2018-02-15 10:03:07"
"827350","OpenVPN from AWS into Office","<p>I'm trying to build an OpenVPN gateway <strong>from</strong> my <strong>VPC</strong> -> <strong>into</strong> the <strong>office</strong> network. I've successfully set up a VPN client on one of my EC2 instances (let's name it ""gateway"") and now it has VPN virtual interface ""tun0"".</p>

<p>Now I want to route all office-related traffic (dst 172.20.0.0/16) from the rest of EC2 instances in the VPC to ""gateway""'s network interface (10.0.0.100).</p>

<p>I've tried 2 different approaches:</p>

<ul>
<li>add a new rule into the related AWS Route Table: 172.20.0.0/16 -> eni-XXX (where eni-XXX is an id of ""gateway""'s interface);</li>
<li>update EC2's route table: route add -net 172.20.0.0 netmask 255.255.0.0 gw 10.0.0.100</li>
</ul>

<p>Both variants seem to be failed because running ""tcpdump -i eth0 'src port not 22 and dst port not 22'"" on the gateway and curling/pinging internal office ips shows nothing :(</p>

<p>Does anyone have an idea about what's wrong? Or may be has a batter solution for my problem? </p>

<p>And the second question. Once I get my traffic on gateway's eth0, I plan to forward it into the VPN connection using the following IpTables commands:</p>

<pre><code>iptables -t nat -A POSTROUTING -o tun0  -j MASQUERADE
iptables -A FORWARD -i eth0 -s 10.0.0.0/16 -o tun0 -j ACCEPT
iptables -A FORWARD -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
</code></pre>

<p>Should I expect any problems here (except enabling ip forwarding)?</p>
","<amazon-web-services><openvpn><amazon-vpc>","2017-01-19 17:28:57"
"759650","Cannot access a folder without using the port","<p>I have created a new folder inside already existed folder in my application.
When I use the port and access this folder it is working fine. But without the port it cannot be accessed.</p>

<p>Ex :</p>

<p>ip:port/sample/newFolder/sample-1.js --> can be accessed</p>

<p>ip/sample/newFolder/sample-1.js      --> 404 error</p>

<p>But there was another folder in the same path and the js files inside that can be accessed.</p>

<p>Ex : </p>

<p>ip:port/sample/existedFolder/sample-1.js --> can be accessed</p>

<p>ip/sample/existedFolder/sample-1.js      --> can be accessed</p>

<p>Can anyone please help me how can I access js files inside the newFolder.</p>
","<apache-2.2>","2016-02-25 07:10:13"
"827382","Powershell script to find and delete DNS records across multiple zones","<p>We have multiple DNS zones, and what to find static DNS records that no longer respond to pings and delete them.</p>

<p>I've written a PS script, but I'm having two problems.</p>

<p>1 - test-connection fails unless it uses the FQDN. I can't seem to find out how to put the FQDN of failed DNS records into a variable that works with test-connection.<br>
2 - when I try to use test-connection with a large domain, it returns a Quota violation error.  </p>

<p>Here's my code:</p>

<hr>

<pre><code>$zones = Get-Content ""C:\Users\ddavidson.EMF\Documents\Test\zones.txt""
$OutFile = Write-Output ""C:\Users\ddavidson.EMF\Documents\Test\logfile.txt""
$hostn = Get-dnsserverresourcerecord -ComputerName ""DNSBox"" -zonename ""ourzone.com"" -RRType A
$zone = Get-DnsServerZone -ComputerName ""DNSBox""
$zones = $zone.zonename
$names = $hostn.hostname
$hostna = $names + ""."" + $zone""

foreach ($name in $names){
 if (Test-Connection -ComputerName $names -Count 1 -ErrorAction SilentlyContinue){
    Write-Host ""$name, Up""
    #Write-Output ""$name, Up"" | Out-File $OutFile -Append
  }
  else{
    Write-Host ""$name,Down""
    #Write-Output ""$name"" | Out-File $OutFile -Append
    Remove-DnsServerResourceRecord -zonename ""ourzone.com"" -ComputerName rklw303pv -name $names -Force
  }
}
</code></pre>

<hr>

<p>For now, I've left out getting the zones from the server and just hard coded it.</p>

<p>I'm incredibly new to PS, and I've been searching for things to help me out, but haven't been able to find anything.</p>

<p>TIA to any and all.</p>
","<domain-name-system><powershell><fqdn>","2017-01-19 19:56:43"
"759672","What is the internet Bandwidth needed to support 2000 concurrent requests?","<p>For one our projects, we need to calculate the internet bandwidth required to support 2000 concurrent requests/per second. Following is the scenario detail:  </p>

<ul>
<li>If 1 user accesses the page, the response page size comes around 8 KB/Second  </li>
<li>So if we have to support 2000 concurrent request, the 2000 x 8 KB = 16000 KB/Second would have to be supported.  </li>
<li>If I convert 16000 KB into Mbps, it comes to 128 Mbps  </li>
</ul>

<p>So my question is: do I really need 128 Mbps internet bandwidth? Or is there something wrong with my calculation?</p>
","<capacity-planning><bandwidth-measuring><concurrency>","2016-02-25 08:48:44"
"969223","AWS Application load balancer in switching between two frontends","<p>I have two angular6 frontends, one running on port 4000, the second running on 4100.</p>

<p>Do a Dockerized push to ECR and create two task definitions ... create cluster and then 2 services of these two frontends, both are running on their ports well and good.</p>

<p>Now I create an application load balancer with ssl, default traffic to port 4000 frontend on domain driver.com. Then add a rule for the second frontend, like:</p>

<pre><code>Host driver.com
Path /secondfrontend
</code></pre>

<p>so it becomes <code>driver.com/secondfrontend</code> </p>

<p>I get the secondfrontend but it's blank</p>
","<amazon-web-services><docker><load-balancing><amazon-alb><angular>","2019-05-28 20:39:43"
"827451","Can't access Apache 2.4 server except on localhost","<p>I recently installed Apache 2.4 on a Windows Server 2012 machine.</p>

<p>Apache is listening on port 8079 (IIS is installed on that machine too)</p>

<p>It works fine if I access the web server with <a href=""http://localhost:8079"" rel=""nofollow noreferrer"">http://localhost:8079</a>. However I can't access it on other computers on the network. I can't even access it with my own (server's) network ip address (192.168.9.3). Apache doesn't generate any error (403 or something). The connection just times out after a few seconds without talking to Apache.</p>

<p>IIS works fine on port 80, even on the LAN.</p>

<p>Port 8079 seems to be opened.</p>

<pre><code>C:\ms4w\Apache\bin&gt;netstat -a -n | find ""8079""
  TCP    0.0.0.0:8079           0.0.0.0:0              LISTENING
</code></pre>

<p>I added a firewall rule to accept incoming connections on port 8079. According to the logs, it is accepting the connection.</p>

<p>Part of my httpd.conf file:</p>

<pre><code>#Listen 12.34.56.78:80
Listen 8079
</code></pre>

<p>....</p>

<pre><code>&lt;Directory /&gt;
    AllowOverride none
    Require all denied
    Allow from all
&lt;/Directory&gt;
</code></pre>

<p>....</p>

<pre><code>&lt;Directory ""C:/ms4w/Apache/htdocs""&gt;
    Options Indexes FollowSymLinks
    AllowOverride all
    Order deny,allow
    Allow from all
    Require all granted
&lt;/Directory&gt;
</code></pre>

<p>Well now I am clueless... Any idea what's wrong?</p>
","<apache-2.4>","2017-01-20 05:17:15"
"969268","Amazon S3 mysterious(unwanted) files","<p>I recently created my first bucket on S3. I had a folder named backups there which when I opened I saw these about 300 small(from few bytes to 4 or so Kbs) in that folder. I hadn't even uploaded anything yet.</p>
<p>I delete those files but they reappear again after some time.</p>
<p>Can anyone please explain what is(might be) going on. The content of those files is something about encryption stuff.</p>
<p>You can see <a href=""https://i.sstatic.net/EaTUq.png"" rel=""nofollow noreferrer"">in this screenshot</a></p>
<p>If i don't delete them they are just gonna multiply and use my space.</p>
<p>Please help</p>
<p>Thank you</p>
","<amazon-web-services><amazon-s3>","2019-05-29 08:43:48"
"759786","Nginx redirect 2 level subpaths","<hr>

<p>I have the following redirect rule:</p>

<pre><code>        rewrite ^/path1/(.+)$ /path1/index.php?/$1 last;
        rewrite ^/path2/(.+)$ /path2/index.php?/$1 last;
        rewrite ^/path3/(.+)$ /path3/index.php?/$1 last;
        rewrite ^/path4/(.+)$ /path4/index.php?/$1 last;
        rewrite ^/path5/(.+)$ /path5/index.php?/$1 last;
        rewrite ^/path6/(.+)$ /path6/index.php?/$1 last;
        rewrite ^/path7/(.+)$ /path7/index.php?/$1 last;
        rewrite ^/path8/(.+)$ /path8/index.php?/$1 last;
        rewrite ^/path9/(.+)$ /path9/index.php?/$1 last;
</code></pre>

<p>The path is just a sample, but the real redirect has different names at the path.</p>

<p>There's a more generic rule to catch only this situation with 2 level paths? </p>

<p>Thank you for your time in checking. :-)</p>
","<nginx><redirect><rewrite><regex>","2016-02-25 15:53:34"
"827489","HP Proliant server DL180 g6 smart array p410 fatal error","<p>I HAVE ONE HP PROLIANT SERVER DL 180 G6. THIS SERVER RESTART SUDDENLY, SOME TIME TWO OR THREE TIME IN  A DAY SOME TIME AFTER 2 DAYS OR AFTER 7 DAYS. (<a href=""https://i.sstatic.net/RdumI.jpg"" rel=""nofollow noreferrer"">Reboot screenshot</a>)</p>

<p>I have run the hp smart storage administrator tool (<a href=""https://i.sstatic.net/bSlHk.png"" rel=""nofollow noreferrer"">see screenshot</a>).</p>

<pre><code>ADU REPORT


ADU Version                             2.30.6.0
Diagnostic Module Version               8.3.6.0
Time Generated                          Friday January 20, 2017 2:22:11PM

Device Summary:
   Smart Array P410 in Embedded Slot

Consolidated Error Report:
   Controller: Smart Array P410 in Embedded Slot
      Message: An unknown cache error was detected.
   Controller: Smart Array P410 in Embedded Slot
      Message: The cache is permanently disabled.

Report for Smart Array P410 in Embedded Slot
--------------------------------------------

Smart Array P410 in Embedded Slot : Device Error Report

Device                            Severity Error                                
--------------------------------- -------- ------------------------------------ 
Smart Array P410 in Embedded Slot Warning  An unknown cache error was detected.
Smart Array P410 in Embedded Slot Critical The cache is permanently disabled.
</code></pre>
","<windows-server-2008><hp><hp-proliant>","2017-01-20 09:22:43"
"827523","Redirect HTTP to HTTPS with WWW in Nginx","<p>I am using letsencrypt ssl. When i access site with domain.com, it gets redirected to <a href=""https://domain.com"" rel=""nofollow noreferrer"">https://domain.com</a> but i want <a href=""https://www.domain.com"" rel=""nofollow noreferrer"">https://www.domain.com</a> !</p>

<p>I have added www as alias for domain.com in dns panel for accessing site with www. but Now i am getting double redirection. </p>

<p>First redirection : domain.com to https:// domain.com, 
Second redirection : https:// domain.com to https:// www.domain.com </p>

<p>I want to achieve this in single 301 redirection.
already tried multiple solutions given in SF. but it didn't work.
Anyone got idea ?</p>
","<linux><nginx><http><redirect><https>","2017-01-20 11:34:42"
"897671","Dig DNS vs Whois","<p>I have a client that has their name services done through some third part.  The domain was registered through Godaddy (yet a different third party).  The name service provider had their domain names hijacked (don't know how) so the name servers  my client uses became unreachable or compromised.    </p>

<p>Some how the name service provided was able to get the DNS to resolve using different name servers.  I am baffled. How is it that the service provider can alter the name servers for the client's domain?</p>

<p>Abbreviated explaination:
*Godaddy Registrar for example.com lists ns2.ispnameserver.com and ns2.ispnameserver.com as name servers.</p>

<p>*ISP provides name server for example.com, manages ns1.ispnameserver.com and ns2.ispnameserver.com  </p>

<p>ISP loses control of *ispnameserver.com
Somehow ISP is able to provide new name servers ns1.newispname.com ns2.newispname.com and magically DNS uses ns1.newispname.com and ns2.newispname.com to resolve queries for example.com.</p>

<p>In essense ISP was able to highjack control of client's example.com domian.  Whois still list ns1.ispnameserver.com.  </p>

<p>How was the ISP able to do that?  What organization can provide that service?</p>

<p><a href=""https://i.sstatic.net/EPB8D.jpg"" rel=""nofollow noreferrer"">Actual Results</a> from dig and whois.  Note it's been this way for at least a week.</p>
","<domain-name-system><dig><whois>","2018-02-17 13:07:15"
"827544","How add new domains automatically using PHP script?","<p>I am offering to my costumers a web based app hosted on dedicated servers, so clients can register on the website and use the service by using the URL : mywebsite.com/client.</p>

<p><em>The service is running and I have no problem with it !</em></p>

<p>Actually, I would like to add a new option to the app and let costumers use their own domain, <strong>BUT</strong> have the app still hosted on my servers !</p>

<p>Now I would like to know how I can add new domains to my server (I know they should be pointed to my name servers) automatically and by using PHP scripts ?</p>

<p><em>So clients add the domain, they get NS URL's and IP's and after they changed the domain name servers, they can use the service using their domain name</em></p>

<p>I'm pretty sure I need to edit Apache config file for that, but I have no idea, what modification should be done !</p>

<p>In general, I would like all clients domains will be added as an addon domain to my domain where the app is hosted ! So I can get the domain name as user identifier !</p>

<p>*I use Centos and also centos web panel</p>
","<domain-name-system><php><domain>","2017-01-20 13:45:27"
"897675","MySQL access slow from windows clients on virtual server","<p>I have an installation of UniCenta open source POS on three till computers at a pub I used to work at.</p>

<p>The clients are pointed to a virtual server running CentOS 6 and MySQL 5.6.39 with phpMyAdmin for management.</p>

<p>The application is written in Java and runs against MySQL</p>

<p>A couple of months ago the installation started running really slowly with buttons on the screen taking several seconds to respond, in a busy pub this is a serious problem.</p>

<p>I tried changing settings in my.cnf on the server to tweak performance of the MySQL server, thinking this was the issue initially, but this made no difference.</p>

<p>I have upgraded to the latest version of the UniCenta software to see if that would resolve things, but still no joy.</p>

<p>I thought I would try the client from my Mac running OS-X El Capitan and this works perfectly without any slowness.</p>

<p>I have tried installing different versions of Java on the client computers, but this has had no effect.</p>

<p>I have changed the skip_name_resolve to on after seeing this mentioned in several posts online, but this still does not resolve the issue.</p>

<p>I have tested a windows 7 virtual PC on my mac against a local database and this works fine without slowing down, so the issue is somewhere between the clients and the virtual server.</p>

<p>I have tried disabling NetBIOS over TCP/IP on the Windows clients, but this makes no difference.</p>

<p>I am thinking that a wireshark trace is the next step in figuring out the problem but thought I would post this up and see if anyone else might have some clever ideas regarding the Windows 7 TCP/IP stack or other networking issues that might be causing the problem.</p>

<p>18-02-2018
Further to this, I have installed the client software on another Windows 7 machine at the same location as the tills and this works perfectly against the VPS hosted database.</p>

<p>I have now copied the VPS database over to a local database and the three till computers work perfectly well against this local database.</p>

<p>The issue seems to be with certain computers when accessing the VPS hosted database.</p>

<p>Wireshark doesn't show any issues with connectivity between the clients and the server so troubleshooting is problematic.</p>

<p>Further update:
Possible fix is to disable an auto-refresh setting on the application. This setting performs a big refresh task every time it runs and on a number of screen re-loads therefore creating large amounts of traffic to the SQL server.</p>

<p>Having disabled this option seems to have resolved the speed issue and the application is running normally and responding in a timely fashion.</p>
","<mysql><windows-7><java>","2018-02-17 14:05:12"
"827548","Router's IP and devices external IP are different","<p>Is that normal if my router's IP address and external IP that my devices shows me are different? What does that mean?</p>

<p><a href=""https://i.sstatic.net/sfdpz.png"" rel=""nofollow noreferrer"">laptop screeshot</a></p>

<p><a href=""https://i.sstatic.net/Na8U6.jpg"" rel=""nofollow noreferrer"">router screeshot</a></p>
","<ip><router>","2017-01-20 14:06:44"
"969440","/dev/mapper Partition 1 does not start on physical sector boundary","<p>How to fix ""Partition 1 does not start on physical sector boundary"" for ""Disk /dev/mapper/isw_ddfdcbhjcj_Volume0""?</p>

<pre><code>sudo fdisk -l

Disk /dev/mapper/isw_ddfdcbhjcj_Volume0: 935.4 GiB, 1004400541696 bytes, 1961719808 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 4096 bytes
I/O size (minimum/optimal): 4096 bytes / 4096 bytes
Disklabel type: dos
Disk identifier: 0x00000000

Device                                   Boot Start        End    Sectors Size Id Type
/dev/mapper/isw_ddfdcbhjcj_Volume0-part1          1 4294967295 4294967295   2T ee GPT

Partition 1 does not start on physical sector boundary.
</code></pre>
","<linux><lvm><partition>","2019-05-30 05:00:27"
"969444","GPO cannot be accessed/is disabled/or has no extensions","<p>GPO is not applying and in the logs it shows that it cannot be accessed. But it is accessible from \domain.com\sysvol\policies{1C95124D-D9BE-4C67-B9E4-36EDC98DE5BF}.</p>

<p>Delegation looks same as other gpo. Any suggestion on how to debug the issue?</p>

<pre><code>GPSVC(3b4.d74) 05:42:13:923 EvalList: Object &lt;cn={1C95124D-D9BE-4C67-B9E4-36EDC98DE5BF},cn=policies,cn=system,DC=abc,DC=abc&gt; cannot be accessed/is disabled/or has no extensions
GPSVC(3b4.d74) 05:42:13:923 ProcessGPO(User):  ==============================
</code></pre>

<p>There's no google result for the exact same issue. I've even recreated the GPO but still I get the same error.</p>
","<group-policy>","2019-05-30 06:05:42"
"827553","How can I create a log when a program is installed in Windows Server?","<p>Is there a way to log when a program is installed on windows server 2008?</p>

<p>I look at my logs, at application logs, but dont see when I install something like for example WinSCP.</p>
","<windows><logging>","2017-01-20 14:18:11"
"969563","Finding the menu.lst that is being used by grub on a multiboot/multipartition system","<p>How do I find which partition holds the menu.lst being used at boot time?</p>

<p>Not looking for multiple locations , as where the file could be, but where it is definitely to be found. (e.t. partition)
Is the file located on partition 1,2, or partition 8 ? 
The file in /boot/grub/menu.lst was used to display a grub-gretting screen, but which partition it was read from ? without going in to each and every partition and dig, what about a system that holds 8 or even more partitions ? How to find out ?</p>

<p>So the output should list a single location for the file .</p>

<p>grub version 0.97 (grub legacy)</p>
","<grub><legacy>","2019-05-30 21:14:08"
"897820","Windows 2016 HyperV Edition rescan hardware change","<p>I have some problem with Windows 2016 HyperV Edition. It is Core only, and no GUI. Problem is that the mainboard is broken, and I need to replace it. But after replacing it, Windows doesn't recognize anything. Onboard LAN dead, GPU is not recognized, and PCIe LAN is also dead. Even USB is not working. Network and internet is out of question. Keyboard is PS/2 only, no USB flash drive. So it must be something inside Windows Server itself. Therefore Devcon is also out of question. </p>

<p>How to perform hardware rescan? Any idea to revive this? Reinstall HyperV? Running an upgrade? Anything without reinstalling virtual machines will do. </p>

<p>Thank you.</p>
","<hyper-v><hyper-v-server-2016>","2018-02-19 07:03:32"
"969597","ClearLinux on Openstack","<p>I'm having trouble getting ClearLinux to work on OpenStack.</p>

<p>From their <a href=""https://download.clearlinux.org/image/"" rel=""nofollow noreferrer"">download source</a>, based on their sites download page &amp; documentation, I've tried clear-29690-cloud.img, clear-29690-cloud-native.img, clear-29690-live-server.img, and clear-29690-live-server.iso after running it through conversion. I can't tell if this is something specific with clearlinux or not. </p>

<p><a href=""https://clearlinux.org/documentation/clear-linux/guides/maintenance/image-types"" rel=""nofollow noreferrer"">Their documentation</a> states the one for OpenStack is the cloud-img file. </p>

<p>We reference <a href=""https://docs.openstack.org/image-guide/convert-images.html"" rel=""nofollow noreferrer"">this documentation</a> on how to convert an image to a raw format by the documentation of the internal cluster, but even after converting I'm still running into either ""Booting from Hard Drive"" or ""Boot failed: not a bootable disk - No bootable device"".</p>

<p>After trying these images, I run <code>openstack image create --private --disk-format raw --container-format bare</code> and have passed the images themselves and the qcow2 images over. I would think the images would be in their proper formats and that I may be doing the step of converting them to qcow2 types needlessly, but in all scenarios nothing works and I'm having the <em>worst</em> of luck here. Any help would be appreciated.</p>

<p>Edit:</p>

<p><a href=""https://rancher.com/docs/os/v1.2/en/running-rancheros/cloud/openstack/"" rel=""nofollow noreferrer"">Rancher releases as qcow2</a>. just noticed I was also converting from what was given to qcow2 for half of my attempts. Maybe that is the key. Is it a standard to release in qcow2 for openstack images? Testing this now. </p>

<p>Also lets be adults and not downvote without comment here. </p>

<p>Edit 2:
TIL (Tonight(at 3am) I learned)</p>

<pre><code>qemu-img convert -f qcow2 -O raw clear-29690-cloud.img clear-29690-cloud.raw
</code></pre>

<p>I was not only doing the conversion backward but qcow2 is the format given by Clear Linux's OpenStack image and maybe the common one for OpenStack in general. Fun venture. Thanks <a href=""https://rancher.com/docs/os/v1.2/en/running-rancheros/cloud/openstack/"" rel=""nofollow noreferrer"">Rancher</a>! </p>

<p>Edit 3: Spoke too soon. RancherOS works. Clear Linux still stalls at ""Booting form harddisk""</p>

<pre><code>+------------------+------------------------------------------------------------------------------------------------------------------------------------------------                                                                                                           --------------------------------------------------------------------------------------------------------------------------------------------------------------------                                                                                                           ---------------------------------------------------------------------------------------------------------------------------+
| Field            | Value                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
+------------------+------------------------------------------------------------------------------------------------------------------------------------------------                                                                                                           --------------------------------------------------------------------------------------------------------------------------------------------------------------------                                                                                                           ---------------------------------------------------------------------------------------------------------------------------+
| checksum         | ada1f029ffbaee41f10b4fc2422d9f38                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| container_format | bare                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| created_at       | 2019-05-31T08:09:06Z                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| disk_format      | raw                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| file             | /v2/images/33dc7258-27ff-4807-b552-a57a823e1fd7/file                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| id               | 33dc7258-27ff-4807-b552-a57a823e1fd7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| min_disk         | 0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| min_ram          | 0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| name             | clear-29690-cloud-raw-actual                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| owner            | 61af61cc80b04e14a185f5a08e763d3c                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| properties       | direct_url='rbd://bd0b115f-4871-4bc0-a5e8-0207e6e69f18/images/33dc7258-27ff-4807-b552-a57a823e1fd7/snap', locations='[{u'url': u'rbd://bd0b115f                                                                                                           -4871-4bc0-a5e8-0207e6e69f18/images/33dc7258-27ff-4807-b552-a57a823e1fd7/snap', u'metadata': {}}]', os_hash_algo='sha512', os_hash_value='44fb841206b98e1505524e829f                                                                                                           4189bf464aac1659686dc7fdb8fd40135a36bad41b1859c124e76ce375f24c3dfde3718ee20dcf7681a75ed61d86846d6f3cc5', os_hidden='False' |
| protected        | False                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| schema           | /v2/schemas/image                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| size             | 907018240                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| status           | active                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| tags             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| updated_at       | 2019-05-31T08:09:53Z                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| virtual_size     | None                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| visibility       | private                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
+------------------+------------------------------------------------------------------------------------------------------------------------------------------------                                                                                                           --------------------------------------------------------------------------------------------------------------------------------------------------------------------                                                                                                           ---------------------------------------------------------------------------------------------------------------------------+
</code></pre>
","<linux><openstack><disk-image>","2019-05-31 06:37:50"
"969858","I want to display the GUI of prometheus and grafana on kubernetes","<p><a href=""https://qiita.com/miyacomaru/items/6fa4121e7ba765e9efd6"" rel=""nofollow noreferrer"">prometheusによるkubernetesのクラスタ監視</a></p>

<p><a href=""https://github.com/giantswarm/prometheus"" rel=""nofollow noreferrer"">Kubernetes Setup for Prometheus and Grafana</a></p>

<p>The following comment was executed referring to</p>

<pre><code>kubectl apply \
  --filename https://raw.githubusercontent.com/giantswarm/kubernetes-prometheus/master/manifests-all.yaml
</code></pre>

<pre><code>[root@instance-1 ~]# kubectl get pods --namespace=monitoring
NAME                                  READY   STATUS      RESTARTS   AGE
alertmanager-78cbf8f796-crk8k         1/1     Running     0          42m
grafana-core-7f65444f84-2rg6q         1/1     Running     0          42m
grafana-import-dashboards-h4bp5       0/1     Completed   0          42m
kube-state-metrics-5f4c7f9d47-s2ndv   1/1     Running     0          42m
node-directory-size-metrics-57lm5     2/2     Running     0          42m
node-directory-size-metrics-5ncxd     2/2     Running     0          42m
prometheus-core-5c96ddd598-srk4l      1/1     Running     0          42m
prometheus-node-exporter-b8wfz        1/1     Running     0          42m
prometheus-node-exporter-rbfkh        1/1     Running     0          42m
</code></pre>

<pre><code>[root@instance-1 ~]# kubectl get svc --namespace=monitoring
NAME                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
alertmanager               NodePort    10.19.254.177   &lt;none&gt;        9093:30576/TCP   44m
grafana                    NodePort    10.19.244.179   &lt;none&gt;        3000:31362/TCP   44m
kube-state-metrics         ClusterIP   10.19.241.158   &lt;none&gt;        8080/TCP         44m
prometheus                 NodePort    10.19.241.218   &lt;none&gt;        9090:30472/TCP   44m
prometheus-node-exporter   ClusterIP   None            &lt;none&gt;        9100/TCP         44m
</code></pre>

<p>I did, but I can not see the GUI of prometheus and grafana.
You will not be able to access this site.
Please tell me how to use GUI.</p>

<p>If the necessary part is not enough, add it.
Thank you.</p>

<pre><code>Cluster setting method
[root@instance-1 ~]curl https://sdk.cloud.google.com | bash
[root@instance-1 ~]gcloud init
[root@instance-1 ~]sudo yum update kubectl
[root@instance-1 ~]cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF
yum install -y kubectl
[root@instance-1 ~]gcloud container clusters create cluster-1 \
    --num-nodes 1 \
    --machine-type n1-standard-2
</code></pre>

<pre><code>after that
[root@instance-1 ~]kubectl apply \
  --filename https://raw.githubusercontent.com/giantswarm/kubernetes-prometheus/master/manifests-all.yaml
[root@instance-1 ~]# kubectl get pods --namespace=monitoring
[root@instance-1 ~]# kubectl get svc --namespace=monitoring
[root@instance-1 ~]#kubectl port-forward svc/prometheus 9090:9090 -n monitoring
</code></pre>

<pre><code>[root@instance-1 ~]# kubectl get svc --namespace=monitoring← I tried setting without using this command, but it was useless.
</code></pre>

<p>Browser screen</p>

<pre><code>I can not access this site Connection denied on localhost.
Please try the following

Check connection
Check proxy and firewall
ERR_CONNECTION_REFUSED
</code></pre>
","<kubernetes><google-kubernetes-engine>","2019-06-02 18:41:15"
"969885","How to protect host fabric stability in Hyper-V under high load","<p>We have a server, running Windows Server 2016 with 4 physical cores (8 hyperthreads) and has Hyper-V installed.</p>

<p>Now, let's say that there are two VMs, each configured in Hyper-V to have a certain number of cores X.  Let's say that each of those VMs is under very high load - 80% to 100% CPU load.</p>

<p>If I want to safely preserve a margin for the host to stay healthy so that I can remote desktop to the server to manage it, what is the maximum number of cores I can assign to each VM in Hyper-V?  Do I need to leave one core ""free"" so that the host stays stable/accessible?  If so, can I leave a partial core free instead of a whole core?</p>

<p>Is there a separate setting to specify how much CPU should be preserved for the host machine?</p>

<p>Also, in Hyper-V does one core mean one physical core (of which there are 4 in this example) or one logical core (of which there are 8 in this example)?</p>

<p>Any help with this is greatly appreciated.</p>
","<virtualization><virtualhost><hyper-v><windows-server-2016><hyper-v-server-2016>","2019-06-03 05:30:20"
"828103","Windows Server 2016 Standard Processor Groups","<p>I'm using vmware and assigned 64 logical processors and 64 sockets. But the operating system is dividing the number of processors into two different uneven processor groups. </p>

<p>From reading Microsoft documentation it stats that I should only have 1 processor group for 64 or less processors. </p>

<p>See Photo below, what will be the cause of this, and how can I force it to be 1 processor group? </p>

<p><a href=""https://i.sstatic.net/FDdbZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/FDdbZ.png"" alt=""Photo of Processor Groups""></a></p>
","<vmware-esxi><windows-server-2016>","2017-01-23 19:11:52"
"828282","What is the meaning of the sysContact object in snmpd.conf?","<p>I have the following snmpd.conf file in my linux machine</p>

<pre><code>rocommunity blmlom 127.0.0.1
rocommunity blmlom hty.corp
rocommunity blmlom  hty.corp
syscontact totadmin@hju.com
syslocation UNKNOWN
trapcommunity blmlom
trapsink  hty.corp blmlom     
</code></pre>

<p>what is the meaning of <code>sysContact</code> in the file? What is good for ?</p>
","<linux><snmp><snmpd>","2017-01-24 14:21:36"
"970160","How to set Starting program section and Remote control section by powershell for AD user","<p>I have used the ""New-ADUser"" command to create a batch of new user. 
However, I need to set the ""Program file name"" in ""Starting program"" to ""c:\windows\system32\logoff.exe"" and ""Start in"" to ""c:\windows\system32\"".</p>

<p><img src=""https://i.sstatic.net/bnvgd.png"" alt=""Starting program""></p>

<p>Also I need to untick ""Require user's permission"" in ""Remote control""</p>

<p><img src=""https://i.sstatic.net/g6Pt9.png"" alt=""Remote control""></p>

<p>How can I set these by powershell?
Many thanks.</p>
","<active-directory><powershell>","2019-06-05 05:07:26"
"828290","reset mysql password without knowing current password","<p>I am unable to remember the password for mysql database.</p>

<p>Using the <code>ALTER USER</code> statement does not work to update the password.</p>

<p>does anyone know how to do this?</p>

<p>I am entering:</p>

<pre><code>ALTER USER 'root'@'localhost' IDENTIFIED BY 'passwordhere';
</code></pre>

<p>the error I am receiving is</p>

<pre><code>""You must reset your password using the ALTER USER 
statement before executing this statement."" 
</code></pre>
","<mysql>","2017-01-24 14:54:38"
"898535","Are thousands of expired keys bad on performance for Redis?","<p>I'm new to Redis, so I don't know which metrics I should care about and how to monitor its healthiness and responsiveness.</p>

<p>My colleagues and I are maintaining a software, which uses Redis heavily for caching. Sometimes Redis will slow down since too many requests are happening at the same time, but sadly I'm not sure which of our processes to blame (it's a legacy software project with dozens of processes running at the same time plus time triggered processes via cron).</p>

<p>I can see, that Redis has thousands to 1.5 millions of expired keys in its store, does this somehow harm the performance while adding or querying new/other keys?</p>
","<performance><monitoring><cache><redis>","2018-02-23 07:51:37"
"970300","The task includes an option with an undefined variable. The error was: 'dict object' has no attribute 'stdout","<p>I am getting issues on my below task. </p>

<pre><code>- name: Deploy incremental changes using rsync
     command: ""sh run""
    register: data
</code></pre>

<p>I am trying to print data down below but keeps getting the above issues, this is a snippet of my code which sends an email after the successful completion of job </p>

<pre><code>        --------------------
         Components Deployed
        --------------------
        {{ data.stdout }}
         ---------------------
         Please verify
</code></pre>
","<ansible><task>","2019-06-05 23:44:07"
"970323","i am struggling to read aws s3 bucket event log getting error","<p>ClientError:</p>

<blockquote>
  <p>An error occurred (AllAccessDisabled) when calling the GetObject operation: All access to this object has been disabled</p>
</blockquote>

<p>My code is:</p>

<pre><code>s3resource = boto3.resource('s3')
my_bucket = s3.Bucket(bucket_name)
s3client = boto3.client('s3')
for s3_object in my_bucket.objects.all():
    path, file1 = os.path.split(s3_object.key)
    obj = s3client.get_object(Bucket='bucket_name',Key=file1)
    j = json.loads(obj['Body'].read())
</code></pre>

<p>And the bucket policy is:</p>

<pre><code>{
    ""Version"": ""2012-10-17"",
    ""Id"": ""Policy1559802940321"",
    ""Statement"": [
        {
            ""Sid"": ""Stmt1559802821882"",
            ""Effect"": ""Allow"",
            ""Principal"": {
                ""Service"": ""cloudtrail.amazonaws.com""
            },
            ""Action"": ""s3:*"",
            ""Resource"": ""arn:aws:s3:::bucketname""
        }
    ]
}
</code></pre>
","<amazon-web-services><aws-cli>","2019-06-06 07:14:42"
"899137","How should I name my server in nginx if I have a redirect server and the main server?","<p>Suppose my main server is to serve <code>https://www.example.com</code> and the redirect server is to redirect <code>http://www.example.com</code> to <code>https://www.example.com</code>. How should I name these two servers? Is there a sort of best practice for this?</p>

<p>Is server name mostly a personal preference such that I can name anything?</p>
","<nginx><web-server><web>","2018-02-27 15:14:37"
"828931","How to properly use SMTP authentication?","<p>I have been trying to configure Postfix to use SMTP authentication. When I telnet on port 587, I appear to be authenticating correctly, but the mail fails to reach its destination and instead comes back as 553 rejected by Spamhaus because my IP is on the PBL. When I read the documentation on Spamhaus, I am told that being on the PBL is not a block, I just need to ensure that I authenticate correctly (<a href=""https://www.spamhaus.org/faq/section/Spamhaus%20PBL#253"" rel=""nofollow noreferrer"">https://www.spamhaus.org/faq/section/Spamhaus%20PBL#253</a>). </p>

<p>I have searched extensively, but have not found a way to ensure mail is delivered successfully from this server. </p>

<p>Would anyone know what I might be missing here?</p>

<p>Here is the result of my telnet test:</p>

<pre><code>ubuntu@dev-server:~$ telnet api.mijnvitalefuncties.com 587
Trying 192.168.0.11...
Connected to api.mijnvitalefuncties.com.
Escape character is '^]'.
220 dev-server ESMTP Postfix (Ubuntu)
ehlo api.mijnvitalefuncties.com
250-dev-server
250-PIPELINING
250-SIZE 10240000
250-VRFY
250-ETRN
250-STARTTLS
250-AUTH PLAIN LOGIN
250-AUTH=PLAIN LOGIN
250-ENHANCEDSTATUSCODES
250-8BITMIME
250 DSN
AUTH LOGIN
334 VXNlcm5hbWU6
***
334 UGFzc3dvcmQ6
***
235 2.7.0 Authentication successful
MAIL FROM:&lt;someone@api.mijnvitalefuncties.com&gt;
250 2.1.0 Ok
RCPT TO:&lt;someone@example.com&gt;
250 2.1.5 Ok
DATA
354 End data with &lt;CR&gt;&lt;LF&gt;.&lt;CR&gt;&lt;LF&gt;
.
250 2.0.0 Ok: queued as B70A764235
quit
221 2.0.0 Bye
Connection closed by foreign host.
</code></pre>

<p>Here is the email I receive informing me that the mail cannot be delivered:</p>

<pre><code>This is the mail system at host dev-server.

I'm sorry to have to inform you that your message could not
be delivered to one or more recipients. It's attached below.

For further assistance, please send mail to postmaster.

If you do so, please include this problem report. You can
delete your own text from the attached returned message.

               The mail system

&lt;someone@example.com&gt;: host cluster5.eu.messagelabs.com[193.109.255.99]
said: 553-mail rejected because your IP is in the PBL. See 553
http://www.spamhaus.org/pbl (in reply to RCPT TO command) 
</code></pre>

<p>Here is the error I get when I try to disable port 25 so as to force mail through submission (port 587).</p>

<pre><code>Jan 27 11:53:26 dev-server postfix/qmgr[16821]: warning: connect to transport private/smtp: Connection refused
Jan 27 11:53:26 dev-server postfix/error[16841]: 5137E64232: to=&lt;peter.heylin@ie.fujitsu.com&gt;, relay=none, delay=19, delays=19/0/0/0.01, dsn=4.3.0, status=deferred (mail transport unavailable)
</code></pre>
","<postfix><smtp><smtp-auth>","2017-01-27 09:31:45"
"899170","How to find the battery type on a server UPS Socomec Mastery BC","<p>I am in a situation where I have to change the UPS batteries of a Server and I cannot even find the type of the batteries that are compatible.</p>

<p>Here is what I have tried until now:</p>

<ul>
<li>I have found the UPS model which is a <a href=""https://www.socomec.com/range-ups-three-single-phase_en.html?product=/ups-masterys-bc-15-20_en.html"" rel=""nofollow noreferrer"">Socomec Master BC</a></li>
<li>Found the <a href=""http://upserve.com.au/assets/downloads/socomec/gb-masterys_bc_15_40-operating_manual.pdf"" rel=""nofollow noreferrer"">Installation and operating manual</a></li>
<li>Found the <a href=""https://www.socomec.com/files/live/sites/systemsite/files/DOCUMENTATION/UPS_hors_cata/dcg_130023.pdf"" rel=""nofollow noreferrer"">Socomec Product technical Guide</a></li>
<li>Contacted Manufacturer but I am still waiting for a reply...</li>
</ul>

<p>However, I have no clue what batteries I should order. In other models in the site there is a reference to <code>Battery Type</code> but in the specific model there is none. You can check the link of the model above. I have not found anything in the manuals as well.</p>

<ol>
<li>How can I find the battery type without any help from the manufacturer?</li>
<li>Can I open the cabinet to check the type of batteries while the UPS is up and running?</li>
</ol>

<p>Thanks in advance.</p>

<p>PS: If this thread is not for <code>serverfault</code> please point the right stackexchange and vote for it to be transfered there.</p>
","<installation><ups><battery>","2018-02-27 17:59:50"
"759952","Unable to resolve DNS provided by azure for redhat linux VM","<p>I am not a server guy and trying to configure a redhat linux virtual machine on azure for hosting a php website. While creating the VM I got a public IP and option to save a name for the DNS, which I saved. When trying to access that url <a href=""http://omgchicks.southeastasia.cloudapp.azure.com/"" rel=""nofollow noreferrer"">http://omgchicks.southeastasia.cloudapp.azure.com/</a> I get nothing.</p>

<p>Upon further researching I came to conclusion that some inbound rules has to be applied like we used to create end points in classic mode. Which I did. Currently I allowed all the ports. Still I get nothing when I hit that IP.</p>

<p>I have already installed apache , php on the VM which I am able to access through PUTTY.</p>

<p>I know nothing from here. What are the steps to get this working?</p>
","<linux><domain-name-system><virtual-machines><azure>","2016-02-26 07:27:05"
"899447","sshd: unrecognized service","<p>I am trying to start sshd but i am getting an error which says</p>

<pre><code>sshd: unrecognized service
</code></pre>

<p>However, it seems that sshd is installed properly (?)</p>

<pre><code># rpm -qa | grep ssh
libssh2-1.4.2-2.el6_7.1.x86_64
openssh-5.3p1-117.el6.x86_64
openssh-clients-5.3p1-117.el6.x86_64

# which sshd
/usr/sbin/sshd
</code></pre>

<p>is there something that i need to do for service to recognize sshd?
i see this in /etc as well</p>

<pre><code># ls /etc/ssh
moduli       sshd_config           ssh_host_key      ssh_host_rsa_key.pub
ssh_config   ssh_host_dsa_key      ssh_host_key.pub
sshd-banner  ssh_host_dsa_key.pub  ssh_host_rsa_key
</code></pre>
","<ssh><centos6>","2018-03-01 08:14:26"
"971150","How to show differnet port number to user which is not same as the backend server","<p>I am trying to show the user 443 port number .
I have two url like below
<code>https://mainadmin.dev.example:8443/mainadmin</code>
<code>https://studentblog.dev.example:8443/Studentblogpage</code>
but now , I want use change one url like 
<code>https://studentblog.dev.example/Studentblogpage</code> or <code>https://studentblog.dev.example:443/Studentblogpage</code> </p>

<p>Is it possible to change like below code ?</p>

<pre><code>frontend port_8443
bind *:8443 ssl crt /etc/haproxy/ssl/certificate/crt.txt
reqadd X-Forwarded-Proto:\ https
acl mainadm path_reg -i ^/MainAdm.*$
acl StudentBlog path_reg -i ^/StudentSite.*$
use_backend MainAdm-Devl if { sslsni admmain.dev.example } Mainadm
use_backend Studentblog-Devl if { sslsni studentblog.dev.example } StudentBlog
mode http
option tcplog
backend MainAdm-Devl
mode http  
balance roundrobin
cookie SERVERID insert indirect nocache
server webserver01 server-01.dev.example:8443 ssl check verify none cookie a1
server webserver02 server-02.dev.example:8443 ssl check verify none cookie a2
backend Studentblog-Devl
mode http
balance roundrobin
option httpchk GET /Studentblogpage
cookie SERVERID insert indirect nocache
server webserver04 server-04.dev.example:443 ssl check verify none
server webserver05 server-05.dev.example:443 ssl check verify none backup
</code></pre>
","<load-balancing><haproxy>","2019-06-12 17:39:43"
"760023","Samba PDC - Create groups","<p>I'm researching on Samba and have created a PDC using it. I have created user and computer accounts and things are working.
I'm having problems, however, in finding how to create groups and add users to them. Samba apparently doesn't create AD default groups like ""Domain Users"" or ""Domain Admins"". Does it use Unix groups for that or I have to create them using some Samba tool?</p>

<p>Are these groups ""searchable"" in Windows workstations as users are (or like when using AD)?</p>

<p>I'm running CentOS 6.7 and Samba 3.6.23.</p>

<p>I'm using tdbsam to store user data.</p>
","<domain><samba><groups>","2016-02-26 13:11:44"
"899498","Power server when power is on (No Bios access)","<p>I have 3 servers and one is old hardware. I have frequent power outage due a Kw limit.</p>

<p>I know there is an option in the BIOS to power on a machine when there is power.
2 servers does have this AC Power Recovery option, but the last one I can access it since I don't have PS/2 keyboard.</p>

<p>Could there be an hardware solution to this?
I was thinking of an Arduino powered on an outlet that checks if detects power through the USB port of the server. If there's power, then wait else if there's no power send a signal to the <code>Power SW</code> pins on the MOBO?</p>

<p>Any advice appreciated.</p>
","<hardware><electrical-power><power-supply-unit>","2018-03-01 15:52:32"
"760115","Jenkins execute shell scp","<p>I have a Jenkins build where I am trying to copy a war file from one vm to another.  I want to do this using <code>scp</code>.  I've tried running this in the shell outside of Jenkins and everything works fine.  Trying to automate it is a little tricky.  I need to provide a password in order to use <code>sudo</code> and also need to enter a password for the vm I am copying to.  </p>

<p>In the Jenkins build logs, I am seeing that the build is failing after I execute the custom shell script.</p>

<p>The command I am running in the Jenkins build is</p>

<pre><code>echo 'password' | sudo -S scp -P 5555 application.war user@15.15.15.15:/JBoss/standalone/deployments
</code></pre>

<p>The output is</p>

<pre><code>[sudo] password for jenkins: Sorry, try again.
[sudo] password for jenkins: 
sudo: 1 incorrect password attempt
</code></pre>

<p>I don't have any users setup for Jenkins, so I am not understanding the <code>[sudo] password for jenkins</code> error.  </p>

<p>I realize that there are Jenkins plugins to achieve what I am trying to do, but none of them are working for me at the moment, which is probably due to bad server configuration.  That is why I want to use <code>scp</code> in a custom shell script.</p>
","<scp><jenkins>","2016-02-26 19:39:21"
"760192","Openstack cluster for parallel execution?","<p>I want to try Ubuntu OpenStack with <a href=""http://www.ubuntu.com/cloud/maas"" rel=""nofollow noreferrer"">Metal-as-a-Service (MaaS)</a> for web application purposes.  I would be using Apache2, PHP, MySQL and Postgres.</p>

<p>If I got 5 or more machines setup and installed all those application would it be able work as cluster, for parallel execution?</p>

<p>For example, would a MySQL insert of 1500 rows be able to execute using resources (RAM, CPU, disk, etc) from multiple machines in the cluster?  For another example, if I got 1000 requests to the site, would Apache and PHP be able to use multiple machines to handle the requests?</p>

<p>Basically, could I use OpenStack to create a cluster that would perform a lot faster than a single server without OpenStack would?</p>
","<linux><apache-2.2><cluster><openstack><web-applications>","2016-02-27 09:38:28"
"829560","Disk goes to recovery mode","<p>We are running an application with 3 disks in a server. OS is CentOS 7. Intermittently one of the disks goes to recovery mode. CentOS file <code>/sys/class/scsi_host/host0/state</code> changes to recovery automatically and the disk is not accessible. On reboot it is normal again. Have anyone faced this?</p>
","<hard-drive><centos7>","2017-01-31 02:11:31"
"760386","ISCSI, MPIO and Target-IPs","<p>Today I played around with ISCSI, MPIO and performed some tests. Some went as expected, someothers delivered results I cannot explain.</p>
<p>The Test-Environment was 2 virtual machines, named <code>A</code> and <code>B</code>. Both VMs have been assigned with 2 NICS, let's call them <code>A-1</code>, <code>A-2</code>, <code>B-3</code> and <code>B-4</code>, to make it clear which Host was utilizing which IP Adresses.</p>
<p>I limited each Nic to 100 Mbps to have enough time to observe results.</p>
<p>On Host <code>B</code>, I created 2 virtual Disks, assigned them both to the same ISCSI-Target.</p>
<h3>First Test: Single LU , Single Session</h3>
<p>When I established the ISCSI Connection between <code>A</code> and <code>B</code> with a single session.</p>
<p>The result was as expected: about 12 MB/s Transfer Speed.</p>
<h3>Second Test: Single LU , Two distinct Sessions</h3>
<p>In this test, I created 2 ISCSI Sessions, namely (<code>A-1</code>, <code>B-3</code>) and (<code>A-2</code>, <code>B-4</code>)</p>
<p>The results were as expected:</p>
<ul>
<li>Regular Transfer showed around 24 MB/s</li>
<li>Disabling any of the NICs during transfer on any device lowered the transferspeed to 12 MB/s again, while every vm only utilizes one nic for the remaining file part.</li>
</ul>
<h3>Third Test: Single LU , Four Sessions (Full Join)</h3>
<p>After creating the sessions (<code>A-1</code>, <code>B-3</code>), (<code>A-1</code>, <code>B-4</code>), (<code>A-2</code>, <code>B-3</code>) and (<code>A-2</code>, <code>B-4</code>), the result was as expected:</p>
<ul>
<li>Regular Transfer showed around 24 MB/s</li>
<li>Disabling any of the SENDING nics, leads to 100 Mbps on the remaining nic, and 50 Mbps on each receiving nic. -&gt; 12 MB/s</li>
<li>Disabling any of the RECEIVING nics, leads to 100 Mbps on the remaining nic, and 50 MBps on each sending nic. -&gt; 12 MB/s</li>
</ul>
<h3>Fourth Test: Two LU (same ISCSI-Target), Four Sessions (Full Join)</h3>
<p>This is the test that failed - even before I could start to test the fail-over Szenarios.</p>
<ul>
<li>Distinct File Transfer to any disk worked as expected. (200 Mbps)</li>
<li>However, when I initiated another Transfer to the SECOND LU, while the first Transfer was running - the first Transfer dropped to 0 Mbps until the second Transfer finished... ?</li>
</ul>
<p>Is this by design, cause the session cannot hold 2 different conversations - or is this just some &quot;strange&quot; effect of running everything virtualized on the same Hyper-V host?</p>
<hr />
<p>The Initiator:</p>
<p><a href=""https://i.sstatic.net/TVcTv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TVcTv.png"" alt=""enter image description here"" /></a></p>
<p>The Target:</p>
<p><a href=""https://i.sstatic.net/19bKM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/19bKM.png"" alt=""enter image description here"" /></a></p>
<p>The Mentioned &quot;copy-problem&quot; with 2 concurrent Transfers (Screenshot was made with nics @ 200 Mbps - thought maybe 100 is just to Little for the tests):</p>
<p><a href=""https://i.sstatic.net/AIzG7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AIzG7.png"" alt=""enter image description here"" /></a></p>
","<iscsi><mpio>","2016-02-29 00:33:56"
"829699","Not getting the console login prompt in HP ILO 3","<p>Not able to get the console login in the ILO 3. When I am entering the vsp command in the ilo it is not showing the console login prompt.</p>

<p>Tried to reset the ilo(from the Diagnostic Option in the Web GUI) but still the console login prompt is not coming.</p>

<p>Need help to get the console login prompt.</p>
","<hp><login><ilo><prompt>","2017-01-31 16:04:33"
"829712","MYSQL Log Files Eating Disk Space","<p>I have a server that has recently had some issues and after rebooting it last night, we are back at it again today. </p>

<p>I noticed today however that the disk is completely full. So I used the <code>du</code> command to find out where all my space had gone and turns out, there is a file under <code>/var/www/websitename/localfolder/etc/log</code> and in that log folder there are several hundred <code>.sql</code> files with dates going as far back as 2015. </p>

<p>I am not opposed to deleting them and I've read some things on how to turn off the logs and do that etc but all those documents assume the logs are kept in the mysql folder. I'm wondering if I should be concerned that these are in my website directory and if it would be the same process that is found here for example: <a href=""https://stackoverflow.com/questions/7381320/in-mysql-how-can-i-delete-flush-clear-all-the-logs-that-are-not-necessary"">https://stackoverflow.com/questions/7381320/in-mysql-how-can-i-delete-flush-clear-all-the-logs-that-are-not-necessary</a> </p>

<p>Sorry again for the noob question but I just recently inherited all this. </p>

<p>As it was suggested, it is possible these are backups, but uder the same directory as the logs folder, there is a sql folder that contains the websitename.sql</p>

<p>The files in the logs folder follow this convention. websitename-2017-01-09T00:50:04.146Z.sql.</p>
","<linux><nginx><mysql><logging>","2017-01-31 16:39:47"
"760644","Windows 8.1 - Group Policy Settings set, but not executet when offline","<p>Enviremental information: Windows Domain with 3 2012 R2 Domain Controlers.</p>

<p>I am trying to build a Windows 8.1 in kiosk mode, only a browser with access to specific websites.</p>

<p>With the help of GPO - partially with user settings and GPO Loopback Processing enabled - i made Internet Explorer (with -k option -> fullscreen Kiosk mode) the Custom User Interface, so there will be no Windows Explorer, just IE, most Windows shortcuts won't work, windows key on keyboard is enabled. The user (Domain Account in group ""Domain Guests"") is loged in automaticaly. On the computer, i removed the ""Domain Users"" group from users, and added the kiosk user directly.</p>

<p>This works fine, as long as the computer is online. When i unplug the network cable, windows is booting the explorer instead of the custom user interface, and the task that should run in the background - also configured via GPO - are not started. I can see all the settings, i.e. the registry key for the Custom User Interface is set to IE Kiosk mode, so Windows should know all the settings... but they are just ignored, as long as the computer is not connected to our DC!</p>

<p>This is not only a problem with this kiosk computer - i always tought that GPOs are cached AND executed localy, so i.e. a laptop will get the settings from our DC while in our network, an when the Laptop leaves the building and the user is working on the train, our settings should still be applied! Am i wrong? Or what am i doing wrong?</p>

<p><strong>EDIT:</strong></p>

<p>Tried without Loopback Processing, without success.
Disabled Loopback Processing -> no User Settings after a reboot - just as expected.
created a new GPO for the USER, linked it, enabled it -> Reboot, and the user settings are applied.
BUT: Reboot without network connection, and the settings are not applied.</p>

<p><strong>EDIT2:</strong></p>

<p>Some more tests, and it seams as if the Computer Settings are applied properly, but the User Settings are ignored. They even vanishe from the registry (i am shure that this did not happen before...).</p>
","<active-directory><group-policy><windows-8.1>","2016-03-01 07:48:00"
"760654","How to connect to a VM on ESXi by command line?","<p>I want to connect to a VM running on an ESXi host by command line.</p>
<p>With VMWare Workstation, I can use this command to start an view a local VM:</p>
<pre><code>vmware.exe -X -q &lt;path&gt;\MyVM.vmx
</code></pre>
<p>With ESXi, I managed to connect to the host:</p>
<pre><code>VpxClient.exe -i -s &lt;adress&gt; -u &lt;user&gt; -p &lt;password&gt;
</code></pre>
<p>But, how can I connect directly to a VM running on that host?</p>
","<vmware-esxi>","2016-03-01 08:36:24"
"900094","Security issues with SMTP","<p>I am looking at SMTP and POP3 and am trying to find the security issues with both protocols, I have found that SMTP can be easily impersonated on port 25. However I am struggling to find <strong>security</strong> issues, only standard issues like internet connection etc. </p>

<p>Can anyone help me find security issues with both protocols or point me in a good direction to research?</p>
","<networking><smtp><email-server><pop3><network-protocols>","2018-03-05 18:08:52"
"829951","SCCM - PowerShell: All Systems collection not available","<p>I try to import a computer to SCCM with the cmdlet ""Import-CMComputerInformation"" (<a href=""https://technet.microsoft.com/en-us/library/jj821991(v=sc.20).aspx"" rel=""nofollow noreferrer"">https://technet.microsoft.com/en-us/library/jj821991(v=sc.20).aspx</a>), using</p>

<pre><code>Import-CMComputerInformation -CollectionName ""All Systems"" -ComputerName ""Computer1"" -MacAddress ""FA:FA:FA:FA:FA:FA""
</code></pre>

<p>Also tried with</p>

<pre><code>Import-CMComputerInformation -ComputerName ""Computer1"" -MacAddress ""FA:FA:FA:FA:FA:FA""
</code></pre>

<p>But I get this error</p>

<pre><code>Import-CMComputerInformation : No object corresponds to the specified parameters.
At line:1 char:1
+ Import-CMComputerInformation -ComputerName ""Computer1"" -MacAddress ""FA:FA:FA:FA:FA:FA""
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : ObjectNotFound: (Microsoft.Confi...ormationCommand:ImportComputerInformationCommand) [Import-CMComputerInformation], ItemNotFoundException
    + FullyQualifiedErrorId : ItemNotFound,Microsoft.ConfigurationManagement.Cmdlets.Oob.Commands.ImportComputerInformationCommand
</code></pre>

<p>This command also gives no output:</p>

<pre><code>Get-CMDeviceCollection -Name ""All Systems""
</code></pre>

<p>I am able to query certain other collections (our IT is divided in departments, you don't have permissions on everything).
If I try to add a computer with the SCCM console, I am able to import a computer to All Systems, but I can also not find the All Systems-collection if I search for it.</p>

<p>Using WMI, comparable to this:</p>

<pre><code>#New computer account information
$WMIConnection = ([WMIClass]""\\SERVER100\root\SMS\Site_PRI:SMS_Site"")
    $NewEntry = $WMIConnection.psbase.GetMethodParameters(""ImportMachineEntry"")
    $NewEntry.MACAddress = $MACAddress
    $NewEntry.NetbiosName = $ResourceName
    $NewEntry.OverwriteExistingRecord = $True
$Resource = $WMIConnection.psbase.InvokeMethod(""ImportMachineEntry"",$NewEntry,$null)
</code></pre>

<p>I am able to import it. </p>

<p>A PC seems to move to certain collections after importing, defined by rules.</p>

<p>Is there a way to use the Import-CMComputerInformation command? It seems a bit silly to write your own function if there's a perfectly fine cmdlet already. 
I'd also rather not write it directly to a certain collection, as those rules might possibly change, and it -should- be possible to import it to All Systems.</p>

<p>SCCM is managed by a central IT, we are a decentral and use their infrastructure (in this case). That means we don't have full access to SCCM. We wanted a more automated way to import computers, and they wouldn't adjust their import-tools to accept command line arguments, so I decided to write my own import-tool. </p>

<p>EDIT, verbose output:</p>

<pre><code>PS L01:\&gt; Import-CMComputerInformation -ComputerName $pcName -MacAddress $macAddress -Verbose
Import-CMComputerInformation : No object corresponds to the specified parameters.
At line:1 char:1
+ Import-CMComputerInformation -ComputerName $pcName -MacAddress $macAddress -Verb ...
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : ObjectNotFound: (Microsoft.Confi...ormationCommand:ImportComputerInformationCommand) [Import-CMComputerInformation], ItemNotFoundException
    + FullyQualifiedErrorId : ItemNotFound,Microsoft.ConfigurationManagement.Cmdlets.Oob.Commands.ImportComputerInformationCommand


PS L01:\&gt; $macAddress
98:90:96:AE:64:97

PS L01:\&gt; $pcName
SET-D-DI-00804
</code></pre>
","<powershell><sccm>","2017-02-01 17:42:23"
"900159","How to determine what quota I'm violating?","<p>Ok, so I'm developing an application for eventual deployment to App Engine and I'm working locally, on my laptop, and I keep getting these errors:</p>

<pre><code>Insufficient tokens for quota 'logging.googleapis.com/read_requests' and limit 'ReadRequestsPerMinutePerProject' of service 'logging.googleapis.com' for consumer 'project_number:111111111'.
</code></pre>

<p>So how would I sort out which quota I'm violating? I logged into the GCP console and I click the <code>APIs &amp; Services</code> > <code>Dashboard</code>. But none of the graph numbers are anywhere near their quota limits. (example, the graph shows 5 ""Read requests"" and the <code>Read requests per 100 seconds</code> quota next to it says 2,000). </p>

<p>I thought it might be in the <code>IAM &amp; Admin</code> > <code>Quotas</code> section, but nothing there is even remotely close to being violated and it is sorted on ""Used"". </p>

<p>With that all said, how do I look at this error message and match it to a ""Quota"" in the console? </p>

<p>For what it's worth, this is a small Node.js app and I'm using the <code>@google-cloud/logging</code> and <code>@google-cloud/compute</code> packages. My laptop is a Macbook. </p>
","<google-cloud-platform><google-app-engine><google-stackdriver>","2018-03-06 00:37:03"
"760822","MS Server R2 Standard & Hyper V","<p>We have a physical 2012 R2 Standard DC.
Can the Hyper-V Manager be loaded and two Virtual servers be created within the licensing of MS 2012 R2 licensing?</p>
","<windows-server-2012-r2><hyper-v-server-2012>","2016-03-01 20:25:56"
"760903","samba openldap user file premissions","<p>we are testing openldap server with samba3 backend for a new domain in our company.
How can we implement users with different permissions on home directory.
we want some users to have full access to save data on desktop and home directory while others won't be able to save or modify any data.</p>

<p>We are planning to replace all windows machines from the environment with Centos 6 including servers and workstations.</p>

<p>Any suggestion for implementation using open source technologies?</p>
","<linux><permissions><centos6><samba><openldap>","2016-03-02 06:47:20"
"972154","What does postfix do when receiving emails?","<p>Okay, so here's the situation: I'm new to this, but after a few days of Googling I finally managed to successfully setup postfix on a server. I use two subdomains for email. The first subdomain is used by the server just to send email, and the second subdomain is used to forward email. In other words, let's say one email is noreply@servernotifs.example.com, and the other subdomain is used to create forwarders, i.e. admin@fwd.example.com gets forwarded to admin@example2.net and info@fwd.example.com gets forwarded to salesdept@example.net.</p>

<p>Now that you understand the basic setup, my question is: What does postfix do when it receives an email to an address that isn't setup for receiving, or an address that isn't setup to forward email? On the server, I have no MDA or MUA installed, just postfix, as that is all that's necessary to send and forward emails. Normally, if the server was just sending emails, I'd edit the main.cf file to listen only on the loopback interface, but I can't do that since I also need to receive emails in order to forward them. My concern is that if someone replies to the noreply@servernotifs.example.com email, because there is no MDA, postfix will cache the email somewhere. On a server that has just enough memory and diskspace to work with, I want to make sure that the email will simply be ignored or discarded by postfix, instead of retained in memory or written to a hard disk.</p>

<p>TLDR When postfix receives an email that it can't ""hand off,"" does it discard the email? If not, is there a way to make it ignore those emails?</p>

<p>Thanks!
(By the way, if you need any more or simply more-specific info, let me know!)</p>
","<postfix>","2019-06-19 19:50:25"
"900441","Default A record for custom nameserver","<p>I have quite a few domains parked that are currently not in use, so I am looking into a dead simple way to maintain them and display a custom parking-page. I do not want to update every single one the DNS-records on all of the domains separately – instead I want the ability to just have default nameservers / NS-records for all the domains and to have ""the rest handle itself"". </p>

<p>What I am looking for is setting up a custom nameserver (ie. <code>ns.example.com</code>) which I can apply for each domains; what I want to happen then is that all domains with that nameserver automatically gets an A-record which I can point to the server of my liking.</p>

<p>On that ""target""-server I can then just check the URL and display an appropriate parking-page (if needed, or just serve a default one).</p>

<p>I've begun reading up on BIND setting up a custom nameserver – is this the way to go? If so, anything special I should look into to speed up the time to deployment? </p>

<p>Or even better, are there any ""ready to use""-solutions that would fit my needs? (Preferably free, but I'm more than willing to pay for good systems).</p>
","<domain-name-system><bind><nameserver><domain-parking>","2018-03-07 12:16:07"
"972244","Can I get all the virtual IP addresses given the CNAME","<p>Is there a way to find all the aliases of the host?</p>

<pre><code>$ nslookup x
x is an alias for y
y is an alias for z
z has address 192.0.2.45
</code></pre>

<p>So if I have z, is there a way of using nslookup/host/dig commands to get x and y too?</p>
","<cname-record><ip-address><nslookup><dig><virtual-ip>","2019-06-20 14:00:18"
"830258","cannot obtain certificate from let's encrypt","<p>I have a problem with obtaining a certificate from let's encrypt. My website is: www.webuilder.co.uk. All the time I am receiving an error like this:</p>

<p><a href=""https://i.sstatic.net/IhKLj.jpg"" rel=""nofollow noreferrer"">Error</a></p>

<p>My settings are like this:</p>

<p><a href=""https://i.sstatic.net/He2u5.jpg"" rel=""nofollow noreferrer"">Ubuntu Virtualbox</a></p>

<p>If any other information are required please let me know, help me solve this problem as I am struggling with it for days now</p>

<p>Edit://
I think I have found the problem but now need solution</p>

<p>when I type: nslookup -type=soa webuilder.co.uk I get:</p>

<pre><code>;; Got recursion not available from 217.160.82.109, trying next server
Server: 87.237.17.182
Address: 87.237.17.182

Non-authoritative answer:
webuilder.co.uk

origin = ns1088.ui-dns.de
mail addr = hostmaster.1and1.com
serial = 101720101
refresh = 28800
retry = 7200
expire = 604800
minimum = 600

Authoritative answers can be found from:
webuilder.co.uk nameserver = ns1120.ui-dns.biz
webuilder.co.uk nameserver = ns1109.ui-dns.com
webuilder.co.uk nameserver = ns1103.ui-dns.org
webuilder.co.uk nameserver = ns1088.ui-dns.de
ns1109.ui-dns.com internet address = 217.160.82.109
</code></pre>

<p>So is that the issue? These are the default nameserver for 1&amp;1, should I change it? What to?</p>
","<networking><web-server><linux-networking><web-hosting><lets-encrypt>","2017-02-02 21:03:53"
"830259","How to unlock account with expired password while still prohibiting passwords from their password-history","<p>I have a password policy issue that I need help with. The password policy I am using is the following: </p>

<pre><code>objectClass: top
objectClass: device
objectClass: pwdPolicyChecker
objectClass: pwdPolicy
pwdAttribute: userPassword
pwdLockoutDuration: 1800
pwdMaxFailure: 5
pwdLockout: TRUE
pwdFailureCountInterval: 900
structuralObjectClass: device
pwdMinAge: 86400
pwdMaxAge: 7776000
pwdAllowUserChange: TRUE
pwdInHistory: 3
</code></pre>

<p>Here is what I have working:</p>

<ul>
<li>Accounts are locked for 30 mins after 5 failed authentications within 15 mins.</li>
<li>Prohibiting password changes where a user provides a new password that is listed in their password history, only works using ldappasswd binding as said user, using ldapmodify while binding as said user does not work.</li>
<li>Unlocking an account locked due to failed authentications by manually removing the <code>pwdAccountLockedTime</code>.</li>
</ul>

<p>This does not work:</p>

<ul>
<li>Unlocking an account that was locked due to expired password while still enforcing password history.</li>
</ul>

<p>I try using ldappasswd to change the user's password, binding as the locked user using current password, give it a new password that is not in it's password history, and the operation fails saying expired password. Can anyone help me with this?</p>
","<openldap>","2017-02-02 21:07:21"
"972288","What is the cause of most top-vendor SSD crashes?","<p>I'm referring to <em>total</em> crashes where the SSD doesn't work anymore. Not IO errors.</p>

<p>SSDs have a limited lifespan. So if nothing else goes wrong, eventually they die of NAND wear. My question is what are the statistics (as far as we know) of what <em>actually</em> destroyed an SSD. According to <a href=""https://serverfault.com/a/883710/147633"">this answer</a> on this site, ""SSDs rarely die due to NAND wear."" Therefore, the writer advises to put a priority on ""vendor track record"" (and more).</p>

<p>So, what if I buy a top-vendor SSD, is there much of a point of buying a larger capacity SSD because in that case NAND wear <strong><em>is</em></strong> what is expected to total the drive, or will the drive likely be totaled by one of the other failures anyway? [<strong>In regards to crash-protection only!</strong> The answer there mentions that ""space is never enough"". But that's not the point of my question.]</p>
","<ssd><capacity-planning>","2019-06-20 18:30:46"
"830299","Vnstat shows 500MB daily of RX transfer on non operational server","<p>I'm running Centos7 on a VPS with <strong>apache</strong> and a <strong>1KB php application</strong> deployed that i use <strong>for tests</strong>, the logs don't have any entries other than my own and a couple of bots.</p>

<p>I have <strong>vnstat</strong> for tracking traffic and it used to be <strong>20MB daily of RX</strong>, i'm always on ssh.</p>

<p><strong>After</strong> i installed <strong>LetsEncrypt ssl</strong> with certbot it jumped to Nearly <strong>500MB daily of RX</strong> traffic. TX is at 90mb.</p>

<p>I installed <strong>nethogs</strong> to track traffic on eth0 by application but the higher rx rate comes from ssh and it's only a couple of MB/S after hours.</p>

<p><strong>Update:</strong> 
 Tcpdump and tshark output shows dozens of ARP requests per second:</p>

<pre><code>Broadcast ARP 60 Who has X.X.X.X? Tell X.X.X.X
</code></pre>

<p><strong>Update2:</strong>
ARP Packets detailed format:</p>

<pre><code>TIME `MyMAC` (oui Unknown) &gt; `UnknownMAC` (oui Unknown), ethertype IPv4 (0x800), length 87: vps.xxxcloud.com &gt; resolver.dns.xxxx.com PTR someIP.in-addr.arpa (45)
</code></pre>

<p><strong>Update3:</strong>
I'm ignoring ARP packets based on <a href=""https://serverfault.com/questions/266547/how-to-find-the-process-which-causes-the-arp-request"">this</a>. vnstat estimated RX transfer for the month is 10.21GB on a testing environment, still a mystery for me.</p>
","<networking><linux-networking><centos7><local-area-network>","2017-02-03 00:22:23"
"972307","Connecting Blade OS to Internet with Cisco 3020 Switch Module","<p>I am very new to server networking, and appreciate any help.</p>

<p>I have an HP c7000 with Proliant G8 blades. I have installed RHEL, Centos, and Windows Server on different blades, trying to get any of them to connect. I have 4 Cisco 3020 switch modules to the top 4 slots of the enclosure. I've tried connecting Ethernet from my router to each port of the switches, but none of the OSs find a network.</p>

<p>Do I need to map IPs or something more than simply plugging in the switch modules? I am running the OSs over iLO, which I connect to over LAN, would that conflict with the connectivity?</p>

<p>Thanks, and sorry if this is a bad question.</p>
","<centos><redhat><hp-proliant><bridge><ilo>","2019-06-20 20:56:57"
"761229","Openldap Password change at first login","<p>I have implemented openldap in centos 6, I want to push password change policy at first login. Which pwdPolicy ObjectClass and Attributes can accomplish this task ?</p>

<pre><code>    # MyOrgPPolicy, Policies, eostest.com
    dn: cn=MyOrgPPolicy,ou=Policies,dc=eostest,dc=com
    cn: MyOrgPPolicy
    pwdInHistory: 4
    pwdMinLength: 9
    pwdFailureCountInterval: 0
    objectClass: pwdPolicy
    objectClass: device
    objectClass: top
    objectClass: pwdPolicyChecker
    pwdMustChange: TRUE
    pwdMaxFailure: 3
    pwdCheckQuality: 1
    pwdAllowUserChange: TRUE
    pwdAttribute: userPassword
    pwdLockout: TRUE
    pwdSafeModify: FALSE
    pwdExpireWarning: 1
    pwdGraceAuthNLimit: 5
    pwdLockoutDuration: 60
    pwdMaxAge: 2592000
    pwdMinAge: 300
    pwdCheckModule: pwcheck.la
</code></pre>
","<linux><centos6><openldap><password-policy>","2016-03-03 07:41:51"
"830461","Browser keeps downloading php instead of executing it","<p>First of all, I know this question has been duplicated about a thousand times but I've been trying to wrap my head around this for a week and I'm still totally lost.</p>

<p>Here's the server block in question</p>

<pre><code>server {
    listen 4002;

    root /var/customer-sites/bellside7/wordpress;
    index index.php;
    server_name 192.241.151.113;

    location / {
            #try_files $uri $uri/ =404;
            try_files $uri $uri/ /index.php$is_args$args;
    }

    location = /favicon.ico {
            log_not_found off;
            access_log off;
    }

    location = /robots.txt {
            log_not_found off;
            access_log off;
            allow all;
    }

    location ~ \.php$ {
            include /etc/nginx/fastcgi_params;
            fastcgi_index index.php;
            fastcgi_pass unix:/run/php/php7.0-fpm.sock;
    }

    location ~ /\.ht {
            deny all;
    }
}
</code></pre>

<p>So like I said, instead of executing PHP, my browser prompts me to download the page instead.</p>

<p>Now here's a really weird thing that happens too. If I stop the nginx process, this server block still works, and asks me to download the page, (but my other servers still go down). If I change the port to say 4003, now the page will do the same thing on port 4003 AND 4002. You can test this yourself now; If you go to the IP with port 4001, it still prompts to download the page even though I already changed the port to 4002. Now port 4002 and 4001 are doing the same thing even after I changed the port to 4002, even though there is no longer any configuration that tells anything to be served on 4001. It's like the server gets stuck in a state of limbo or something.</p>

<p>NOW THE REALLY MESSED UP PART</p>

<p>I tried shutting down the server, like totally turning off the computer. And it still serves a page and asks me to download it on port 4001 and 4002. My other servers still go down though. While the server was turned off I was still even able to access the page from any other computer, still with the same problem.</p>

<p>edit:</p>

<p><code>sudo netstat -planet</code> output:</p>

<pre><code>Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       User       Inode       PID/Program name
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      0          59947       19324/nginx     
tcp        0      0 0.0.0.0:21              0.0.0.0:*               LISTEN      0          9593        689/vsftpd      
tcp        0      0 0.0.0.0:25              0.0.0.0:*               LISTEN      0          14617       1470/master     
tcp        0      0 0.0.0.0:443             0.0.0.0:*               LISTEN      0          59948       19324/nginx     
tcp        0      0 127.0.0.1:9000          0.0.0.0:*               LISTEN      0          13058       1062/php-fpm.conf)
tcp        0      0 127.0.0.1:27017         0.0.0.0:*               LISTEN      0          64487       20307/mongod    
tcp        0      0 127.0.0.1:9001          0.0.0.0:*               LISTEN      0          13057       1062/php-fpm.conf)
tcp        0      0 127.0.0.1:3306          0.0.0.0:*               LISTEN      106        36534       8326/mysqld     
tcp        0      0 127.0.0.1:11211         0.0.0.0:*               LISTEN      110        14188       1347/memcached  
tcp        0      0 127.0.0.1:41233         127.0.0.1:27017         ESTABLISHED 0          65677       20539/node      
tcp        0      0 127.0.0.1:27017         127.0.0.1:41253         ESTABLISHED 0          64952       20307/mongod    
tcp        0      0 127.0.0.1:27017         127.0.0.1:41287         ESTABLISHED 0          65029       20307/mongod    
tcp        0      0 127.0.0.1:41250         127.0.0.1:27017         ESTABLISHED 0          65687       20567/node      
tcp        0      0 127.0.0.1:41289         127.0.0.1:27017         ESTABLISHED 0          65034       20539/node      
tcp        0      0 127.0.0.1:41280         127.0.0.1:27017         ESTABLISHED 0          65004       20539/node      
tcp        0      0 127.0.0.1:41234         127.0.0.1:27017         ESTABLISHED 0          65679       20539/node      
tcp        0      0 127.0.0.1:27017         127.0.0.1:41283         ESTABLISHED 0          65014       20307/mongod    
tcp        0      0 127.0.0.1:27017         127.0.0.1:41290         ESTABLISHED 0          65038       20307/mongod    
tcp        0      0 127.0.0.1:41283         127.0.0.1:27017         ESTABLISHED 0          65013       20539/node      
tcp        0      0 127.0.0.1:27017         127.0.0.1:41280         ESTABLISHED 0          65005       20307/mongod    
tcp        0      0 127.0.0.1:27017         127.0.0.1:41251         ESTABLISHED 0          64948       20307/mongod    
tcp        0      0 127.0.0.1:27017         127.0.0.1:41281         ESTABLISHED 0          65008       20307/mongod    
tcp        0      0 127.0.0.1:27017         127.0.0.1:41235         ESTABLISHED 0          65682       20307/mongod    
tcp        0      0 127.0.0.1:41232         127.0.0.1:27017         ESTABLISHED 0          65675       20539/node      
tcp        0      0 127.0.0.1:27017         127.0.0.1:41233         ESTABLISHED 0          65678       20307/mongod    
tcp        0      0 127.0.0.1:27017         127.0.0.1:41252         ESTABLISHED 0          64950       20307/mongod    
tcp        0      0 127.0.0.1:27017         127.0.0.1:41250         ESTABLISHED 0          64946       20307/mongod    
tcp        0      0 127.0.0.1:41253         127.0.0.1:27017         ESTABLISHED 0          65690       20567/node      
tcp        0      0 127.0.0.1:41249         127.0.0.1:27017         ESTABLISHED 0          65686       20567/node      
tcp        0      0 127.0.0.1:41252         127.0.0.1:27017         ESTABLISHED 0          65689       20567/node      
tcp        0      0 127.0.0.1:41251         127.0.0.1:27017         ESTABLISHED 0          65688       20567/node      
tcp        0      0 127.0.0.1:41287         127.0.0.1:27017         ESTABLISHED 0          65028       20539/node      
tcp        0      0 127.0.0.1:27017         127.0.0.1:41231         ESTABLISHED 0          65674       20307/mongod    
tcp        0      0 127.0.0.1:27017         127.0.0.1:41249         ESTABLISHED 0          64944       20307/mongod    
tcp        0      0 127.0.0.1:27017         127.0.0.1:41232         ESTABLISHED 0          65676       20307/mongod    
tcp        0      0 127.0.0.1:41281         127.0.0.1:27017         ESTABLISHED 0          65007       20539/node      
tcp        0   5348 192.241.151.113:22      50.141.211.238:62814        ESTABLISHED 0          65031       20539/node      
tcp        0      0 127.0.0.1:27017         127.0.0.1:41234         ESTABLISHED 0          65680       20307/mongod    
tcp        0      0 127.0.0.1:27017         127.0.0.1:41291         ESTABLISHED 0          65041       20307/mongod    
tcp        0      0 127.0.0.1:27017         127.0.0.1:41279         ESTABLISHED 0          65002       20307/mongod    
tcp        0      0 127.0.0.1:27017         127.0.0.1:41282         ESTABLISHED 0          65011       20307/mongod    
tcp        0      0 127.0.0.1:41231         127.0.0.1:27017         ESTABLISHED 0          65673       20539/node      
tcp        0      0 127.0.0.1:41279         127.0.0.1:27017         ESTABLISHED 0          65001       20539/node      
tcp        0      0 127.0.0.1:27017         127.0.0.1:41288         ESTABLISHED 0          65032       20307/mongod    
tcp        0      0 127.0.0.1:41291         127.0.0.1:27017         ESTABLISHED 0          65040       20539/node      
tcp        0      0 127.0.0.1:27017         127.0.0.1:41289         ESTABLISHED 0          65035       20307/mongod    
tcp        0      0 127.0.0.1:41290         127.0.0.1:27017         ESTABLISHED 0          65037       20539/node      
tcp        0      0 127.0.0.1:41235         127.0.0.1:27017         ESTABLISHED 0          65681       20539/node      
tcp        0      0 127.0.0.1:41282         127.0.0.1:27017         ESTABLISHED 0          65010       20539/node      
tcp6       0      0 :::80                   :::*                    LISTEN      0          59949       19324/nginx     
tcp6       0      0 :::3000                 :::*                    LISTEN      0          65691       20567/node      
tcp6       0      0 :::25                   :::*                    LISTEN      0          14618       1470/master     
tcp6       0      0 :::3002                 :::*                    LISTEN      0          64880       20539/node
</code></pre>
","<nginx><php>","2017-02-03 17:10:07"
"830484","How to use Map Network Drive to store files on Windows Server 2008 R2","<p>I use Windows Server 2008 R2.</p>

<p>The server is on a virtual machine.</p>

<p>How can I share a folder on the server, and map a drive to it? </p>
","<file-sharing>","2017-02-03 20:16:00"
"830528","Shell script to store server list for ssh remote connection","<p>I want to store all servers in the local server(Linux Jumpbox), so I can connect by simply searching for specific hostname and ssh remotely from my Linux jump box.</p>

<p>So, the requirement is, input the server hostname, which should search in existing server list, if found, perform ssh connection, if not found, store under specific folder and perform ssh connection.</p>

<p>I tried the following simple if condition, but I actually want to categorize store servers under DEV, QA, UAT, PROD etc., using either under folder or LABEL.</p>

<p>Please share your suggestions/recommendations best way to manage the server list.
Also suggest whether IF or CASE would be better approach for this.</p>

<p>***Script****</p>

<pre><code>#!/bin/bash
if test -f /home/oracle/remote_hosts.lst
then
        echo ""The File /home/oracle/remote_hosts.lst exists""
else
        echo ""The File /home/oracle/remote_hosts.lst does not exist""
        exit 1
fi

echo ""Please enter the Host Name of Remote Host System: \c""

read rhost

if grep $rhost /home/oracle/remote_hosts.lst 1&gt; /dev/null 2&gt;&amp;1
then
        ssh -l oracle $rhost
else
        echo $rhost &gt;&gt; /home/oracle/remote_hosts.lst
        ssh -l oracle $rhost
fi
</code></pre>

<hr>
","<linux><ssh><remote>","2017-02-04 04:03:06"
"900768","Puzzling change of language in windows server 2012","<p>I'm using WSUS on Windows Server 2012.</p>

<p>Initially the language was Italian.  I then switched to English.</p>

<p>In the list of services all are in English but one. There is still one that is listed ""Servizio di registrazione W3C"".  What is the English version of that service (maybe ""W3C validation service""?)</p>

<p>Any idea why this is happening?</p>
","<windows-server-2012><wsus>","2018-03-09 01:06:45"
"761396","Postfix no longer reciving mail","<p>I installed and configured a mail server with postfix(3.1.0-1)+dovecot(2.2.21) on archlinux(4.4.3-1) in December, and on which in confirmed it to be working, both receiving and sending. </p>

<p>About a week ago  I lost the ability to receive email and to send from Thunderbird. When trying to send, Thunderbird claims that it con't connect to the server over SMTP.</p>

<p>I can send using the <strong>sendmail recipient</strong> from the server and receive email locally. This made med think about wrongly forwarded ports but they are all still point to the right position. Secondly I thought about DNS - issues, but the .zone has not been changed.</p>

<p>I have been looking around but found nothing that helps to explain this. journalctl does not show any signs of errors.</p>

<p>Below follows my current settings.</p>

<pre><code>postconf -n
--------
alias_database = $alias_maps
alias_maps = hash:/etc/postfix/aliases
command_directory = /usr/bin
compatibility_level = 2
daemon_directory = /usr/lib/postfix/bin
data_directory = /var/lib/postfix
debug_peer_level = 2
debugger_command = PATH=/bin:/usr/bin:/usr/local/bin:/usr/X11R6/bin ddd    $daemon_directory/$process_name $process_id &amp; sleep 5
home_mailbox = Maildir/
html_directory = no
inet_protocols = ipv4
mail_owner = postfix
mailq_path = /usr/bin/mailq
manpage_directory = /usr/share/man
meta_directory = /etc/postfix
mydestination = $myhostname, localhost.$mydomain, localhost, $mydomain
mydomain = gserv.se
myhostname = mail.gserv.se
mynetworks_style = host
myorigin = $mydomain
newaliases_path = /usr/bin/newaliases
queue_directory = /var/spool/postfix
readme_directory = /usr/share/doc/postfix
relay_domains =
sample_directory = /etc/postfix
sendmail_path = /usr/bin/sendmail
setgid_group = postdrop
shlib_directory = /usr/lib/postfix
smtp_tls_note_starttls_offer = yes
smtp_use_tls = yes
smtpd_recipient_restrictions = permit_mynetworks, permit_sasl_authenticated, reject_unauth_destination
smtpd_tls_CAfile = /etc/ssl/certs/cacert.pem
smtpd_tls_auth_only = no
smtpd_tls_cert_file = /etc/ssl/certs/mail.gserv.se.crt
smtpd_tls_key_file = /etc/ssl/private/mail.gserv.se.key
smtpd_tls_loglevel = 1
smtpd_tls_received_header = yes
smtpd_tls_session_cache_timeout = 3600s
smtpd_use_tls = yes
tls_random_source = dev:/dev/urandom
unknown_local_recipient_reject_code = 550
</code></pre>

<p>This have been driving me crazy for a while now, it would be great to have another perspective.</p>

<p>Thanks</p>

<p>EDIT:
Adding current versions</p>
","<postfix><smtp><email-server>","2016-03-03 18:24:36"
"761426","Trouble logging into Sphere 3D Glassware 2.0 containers on Azure","<p>I've created a few Sphere 3D Glassware 2.0 VM's using the Azure marketplace, set the username password, and have not been able to log into the Sphere 3D admin panel on any of them with my username and password.</p>

<p>I used the default settings for everything, but still no luck logging into Glassware admin with the passwords I have set - it always prompts ""Invalid username and/or password."" When using a web browser to log-in.</p>

<p>Has anyone had this problem? Is there something for the container or virtual machine that I have not configured properly in Glassware 2.0 or Sphere 3D's G-Cloud?</p>
","<azure><containers>","2016-03-03 20:42:15"
"761535","nginx ssl ERR_CONNECTION_REFUSED","<p>I'm getting an error on the SSL, my site is </p>

<p><a href=""https://pontebuso.com/"" rel=""nofollow noreferrer"">https://pontebuso.com/</a></p>

<p>But it always give me ERR_CONNECTION_REFUSED</p>

<p><em>sudo netstat -anp |grep :443| grep LISTEN</em></p>

<pre><code>tcp    0   0 0.0.0.0:443      0.0.0.0:*          LISTEN      1165/nginx 
tcp6   0   0 :::443               :::*           LISTEN      1165/nginx
</code></pre>

<p><em>sudo netstat --numeric -l -p -A ip | grep tcp</em></p>

<pre><code>tcp        0  0 0.0.0.0:22          0.0.0.0:*     LISTEN      1130/sshd       
tcp        0  0 0.0.0.0:443         0.0.0.0:*     LISTEN      1165/nginx      
tcp        0  0 127.0.0.1:9000      0.0.0.0:*     LISTEN      1241/php-fpm.conf)
tcp        0  0 0.0.0.0:3306        0.0.0.0:*     LISTEN      1214/mysqld     
tcp        0  0 0.0.0.0:80          0.0.0.0:*     LISTEN      1165/nginx 
</code></pre>

<p><em>root@05A:~# sudo ufw status verbose</em></p>

<pre><code>Status: active
Logging: on (low)
Default: deny (incoming), allow (outgoing), disabled (routed)
New profiles: skip

To                         Action      From

--                         ------      ----
443/tcp                    ALLOW IN    Anywhere
22/tcp                     ALLOW IN    Anywhere
80                         ALLOW IN    Anywhere
443                        ALLOW IN    Anywhere
80/tcp                     ALLOW IN    Anywhere
23.253.246.119 443/tcp     ALLOW IN    Anywhere
443/tcp (v6)               ALLOW IN    Anywhere (v6)
22/tcp (v6)                ALLOW IN    Anywhere (v6)
80 (v6)                    ALLOW IN    Anywhere (v6)
443 (v6)                   ALLOW IN    Anywhere (v6)
80/tcp (v6)                ALLOW IN    Anywhere (v6)
</code></pre>

<p>-------------------nginx.conf---------------------------</p>

<pre><code>user www-data;
worker_processes 4;
pid /run/nginx.pid;

events {
        worker_connections 768;
        # multi_accept on;
}

http {
    sendfile on;
    autoindex off;

    tcp_nopush on;
    tcp_nodelay on;

    types_hash_max_size 2048;

    fastcgi_buffers 8 16k;
    fastcgi_buffer_size 32k;

    #include /etc/nginx/mime.types;
    default_type application/octet-stream;


    access_log /var/log/nginx/access.log;
    error_log /var/log/nginx/error.log;

    gzip  on;
    gzip_comp_level 2;
    gzip_proxied any;
    gzip_types      text/plain text/css application/x-javascript text/xml application/xml application/xml+rss text/javascript;

    include       /etc/nginx/mime.types;
    include       /etc/nginx/conf.d/*.conf;
    include       /etc/nginx/sites-enabled/*;

    log_format  main  '$remote_addr - $remote_user [$time_local] ""$request ""'
                      '$status $body_bytes_sent ""$http_referer"" '
                      '""$http_user_agent"" ""$http_x_forwarded_for""';

    map $scheme $fastcgi_https { ## Detect when HTTPS is used
        default off;
        https on;
    }

    keepalive_timeout  10;

    # Load config files from the /etc/nginx/conf.d directory
    include /etc/nginx/conf.d/*.conf;
}
</code></pre>

<p>-------------------default---------------------------</p>

<pre><code>server {
        listen 80;

        server_name pontebuso.com;
        add_header Access-Control-Allow-Origin sub.pontebuso.com;
        root /home/sites/pontebuso/;

        autoindex off;

        location / {
            index index.php index.html index.htm;
            try_files $uri $uri/ /index.php?$args; 
            access_log off;
            expires max; 
        }

        ## These locations would be hidden by .htaccess normally
        location ^~ /app/                { deny all; }
        location ^~ /includes/           { deny all; }
        location ^~ /lib/                { deny all; }
        location ^~ /media/downloadable/ { deny all; }
        location ^~ /pkginfo/            { deny all; }
        location ^~ /report/config.xml   { deny all; }
        location ^~ /var/                { deny all; }

        access_log /var/log/nginx/nginx.vhost.access.log;
        error_log /var/log/nginx/nginx.vhost.error.log;

        location /var/export/ {
            auth_basic           ""Restricted"";
            auth_basic_user_file htpasswd;
            autoindex            on;
        }

        location  /. {
            return 404;
        }

        location @handler {
            rewrite / /index.php;
        }

        location ~ .php/ {
            rewrite ^(.*.php)/ $1 last;
        }

        location ~ .php$ {
            if (!-e $request_filename) { rewrite / /index.php last; }
            expires        off;
            fastcgi_split_path_info ^(.+\.php)(/.+)$;
            fastcgi_pass 127.0.0.1:9000;
            fastcgi_index index.php;
            fastcgi_param  SCRIPT_FILENAME $document_root$fastcgi_script_name;
            fastcgi_param  HTTPS $fastcgi_https;
            fastcgi_param  MAGE_RUN_CODE default;
            fastcgi_param  MAGE_RUN_TYPE store;
            include        fastcgi_params;
        }

        location ~ .php/ {
            rewrite ^(.*.php)/ $1 last;
         }


         location ~ .php$ { 
            if (!-e $request_filename) { rewrite / /index.php last; }
            expires        off; ## Do not cache dynamic content
            fastcgi_pass   127.0.0.1:9000;
            fastcgi_param  HTTPS $fastcgi_https;
            fastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;
            fastcgi_param  MAGE_RUN_CODE default;
            fastcgi_param  MAGE_RUN_TYPE store;
            include        fastcgi_params; ## See /etc/nginx/fastcgi_params
        }

        location ~ \.css {
            add_header  Content-Type    text/css;
        }

        location ~ \.js {
            add_header  Content-Type    application/x-javascript;
        }


    }
</code></pre>

<p>-------------------pontebuso.conf---------------------------</p>

<pre><code>#server {
#    listen 80;
#    server_name localhost;
#    rewrite ^/(.*) https://pontebuso.com/$1 permanent;
#    }

server {
       listen 443 ssl;

        ssl on;
        ssl_certificate /etc/nginx/ssl/pontebuso.com.chained.crt;
        ssl_certificate_key /etc/nginx/ssl/pontebuso.key;   

        server_name pontebuso.com;

        ssl_session_cache  builtin:1000  shared:SSL:10m;
        ssl_protocols  TLSv1 TLSv1.1 TLSv1.2;
        ssl_ciphers HIGH:!aNULL:!eNULL:!EXPORT:!CAMELLIA:!DES:!MD5:!PSK:!RC4;
        ssl_prefer_server_ciphers on;

        fastcgi_param   HTTPS               on;
        fastcgi_param   HTTP_SCHEME         https;

        add_header Access-Control-Allow-Origin sub.pontebuso.com;
        root /home/sites/pontebuso/;

        autoindex off;

        access_log /var/log/nginx/nginx.vhost.access.log;
        error_log /var/log/nginx/nginx.vhost.error.log;

        location / {
            index index.php index.html index.htm;
            try_files $uri $uri/ /index.php?$args; 
            access_log off;
            expires max; 
        }


        ## These locations would be hidden by .htaccess normally
        location ^~ /app/                { deny all; }
        location ^~ /includes/           { deny all; }
        location ^~ /lib/                { deny all; }
        location ^~ /media/downloadable/ { deny all; }
        location ^~ /pkginfo/            { deny all; }
        location ^~ /report/config.xml   { deny all; }
        location ^~ /var/                { deny all; }


        location /var/export/ {
            auth_basic           ""Restricted"";
            auth_basic_user_file htpasswd;
            autoindex            on;
        }

        location  /. {
            return 404;
        }

        location @handler {
            rewrite / /index.php;
        }

        location ~ .php/ {
            rewrite ^(.*.php)/ $1 last;
        }

        location ~ .php$ {
            if (!-e $request_filename) { rewrite / /index.php last; }
            expires        off;
            fastcgi_split_path_info ^(.+\.php)(/.+)$;
            fastcgi_pass 127.0.0.1:9000;
            fastcgi_index index.php;
            fastcgi_param  SCRIPT_FILENAME $document_root$fastcgi_script_name;
            fastcgi_param  HTTPS $fastcgi_https;
            fastcgi_param  MAGE_RUN_CODE default;
            fastcgi_param  MAGE_RUN_TYPE store;
            include        fastcgi_params;
        }

        location ~ .php/ {
            rewrite ^(.*.php)/ $1 last;
         }


         location ~ .php$ { 
            if (!-e $request_filename) { rewrite / /index.php last; }
            expires        off; ## Do not cache dynamic content
            fastcgi_pass   127.0.0.1:9000;
            fastcgi_param  HTTPS $fastcgi_https;
            fastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;
            fastcgi_param  MAGE_RUN_CODE default;
            fastcgi_param  MAGE_RUN_TYPE store;
            include        fastcgi_params; ## See /etc/nginx/fastcgi_params
        }

        location ~ \.css {
            add_header  Content-Type    text/css;
        }

        location ~ \.js {
            add_header  Content-Type    application/x-javascript;
        }


    }
</code></pre>
","<ubuntu><nginx><ssl><ubuntu-14.04>","2016-03-04 09:51:47"
"830739","Link subdomain to digital ocean server","<p>My website (landing page for node.js app) runs on a shared hosting, I also have the node.js app running on a DigitalOcean Droplet (Ubuntu).</p>

<p>I want to run my website (landing page on shared hosting) on visualizr.be but when surfing to play.visualizr.be my node app (let's say IP is 192.168.0.10) needs to be showed.</p>

<p>This is the DNS Management screen of my shared hosting provider:
<a href=""https://i.sstatic.net/dw8Ki.jpg"" rel=""nofollow noreferrer"">DNS Management of the shared hosting</a></p>

<p>The DigitalOcean domain servers are</p>

<p>ns1.digitalocean.com, ns2.digitalocean.com, ns3.digitalocean.com</p>

<p>What do I need to edit to make this work?</p>
","<domain-name-system>","2017-02-05 19:22:44"
"830781","Too many redirects for subdirectory excluded from SSL","<p>I have a subdirectory /blog/ served from reverse proxy. The rest of the site is served over https. </p>

<p>I'm trying to get the /blog/ served over http without ssl but I keep getting too_many_redirects errors. Here is the config (edit, it worked in the past but since I did a SSL update something change somewhere - maybe with Forge - idk but don't think it matters to find solution): </p>

<pre><code># FORGE CONFIG
include forge-conf/example.com/before/*;

server {
    listen 80;
    server_name example.com;

    location /blog {
        proxy_set_header X-Original-Host $host;
        proxy_set_header X-Is-Reverse-Proxy ""true"";
        proxy_pass_header Set-Cookie;
        proxy_cookie_path / /blog/;
        proxy_pass http://blog.example.com/;
        expires off;
    }
    # FORGE CONFIG (DOT NOT REMOVE!)
    include forge-conf/example.com/server/*;

    location / {
        return 301 https://example.com$request_uri;
    }
}

server {
    listen 443 ssl;
    server_name .example.com;
    root /home/forge/example.com/public;

    # FORGE SSL (DO NOT REMOVE!)
    ssl_certificate /etc/nginx/ssl/example.com/166494/server.crt;
    ssl_certificate_key /etc/nginx/ssl/example.com/166494/server.key;

    ssl_protocols TLSv1 TLSv1.1 TLSv1.2;

    index index.html index.htm index.php;

    charset utf-8;

    location /blog {
         return 301 http://example.com$request_uri;
    }
    # FORGE CONFIG (DOT NOT REMOVE!)
    include forge-conf/example.com/server/*;
</code></pre>

<p>If I add the reverse proxy block in the 443 listen part it serves correctly over https - but I can't have it like that because of mixed content (original server being http for the blog)</p>

<p>Edit: More info, here is the setup - not ideal but no choice: blog.example.com is hosted on apache on a different server that doesn't have SSL. example.com/blog/ serves in reverse proxy the blog located blog.example.com. example.com has an SSL certificate. I don't want to serve the blog over https otherwise I need another certificate for blog.example.com</p>
","<nginx><ssl><https>","2017-02-06 02:56:38"
"830814","Error executing help manual pages on Fedora 25 for all kind of man pages","<pre><code>[user1@testMachine ~]$ lsb_release -a

LSB Version:    :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch
Distributor ID: Fedora
Description:    Fedora release 25 (Twenty Five)
Release:    25
Codename:   TwentyFive

[user1@testMachine ~]$ man wget
sed: can't read 2&gt;/dev/null: No such file or directory
sed: can't read |: No such file or directory
sed: can't read less: No such file or directory

man: command exited with status 2: sed -e '/^[[:space:]]*$/{ N; /^[[:space:]]*\n[[:space:]]*$/D; }' | (cd &lt;fd 3&gt; &amp;&amp; LESS=-ix8RmPm Manual page wget(1) ?ltline %lt?L/%L.:byte %bB?s/%s..?e (END):?pB %pB\%.. (press h for help or q to quit)$PM Manual page wget(1) ?ltline %lt?L/%L.:byte %bB?s/%s..?e (END):?pB %pB\%.. (press h for help or q to quit)$-iMSx4 -FXR MAN_PN=wget(1) sed s/\([[:space:]]\+[0-9.\-]\+\)$/\1/;s/\([[:space:]]\+[0-9.\-]\+[[:space:]]\)/\1/g;s/|/|/g;s/^\([-+]\+\)/\1/ 2&gt;/dev/null | less)
</code></pre>

<p>Even lot of help commands are not working, and showing the same error page.</p>
","<linux><fedora><sed><command><man>","2017-02-06 09:51:53"
"901061","No such file or directory linux shell script","<pre><code>#!/bin/bash
mysql -uroot -pwelcome1 &lt; /home/sai/first.sql

echo ""The program has completed""
</code></pre>

<p>I am trying to call a sql file from shell script ,I am running the script as <code>bash run.sh</code>, but it telling that there is no such directoryfirst.sql, but if i run that command separately as <code>mysql -uroot -pwelcome1 &lt; /home/sai/first.sql</code>, it is working, is there a different way to write?</p>
","<linux><bash><docker><shell>","2018-03-11 07:01:03"
"830852","Can domain records exist in multiple registrars?","<p>I have a client that wants to maintain control of his domain (fair enough) - he also wants to maintain control of his MX records (fair enough).</p>
<p>He is with CrazyDomains and the current records kind of look like this:</p>
<h2>Before</h2>
<p><strong>CrazyDomains:</strong></p>
<ul>
<li>A Name: @ 111.222.333.444</li>
<li>MX: 1ASPMX.L.GOOGLE.COM</li>
<li>MX: 2ASPMX.L.GOOGLE.COM</li>
<li>DS: ns1.crazydomains.com</li>
<li>DS: ns2.crazydomains.com</li>
</ul>
<h2>After</h2>
<p><strong>CrazyDomains:</strong></p>
<ul>
<li>MX: 1ASPMX.L.GOOGLE.COM</li>
<li>MX: 2ASPMX.L.GOOGLE.COM</li>
<li>DS: ns1.crazydomains.com (for MX records)</li>
<li>DS: ns2.crazydomains.com ( &quot;&quot; )</li>
<li>DS: ns1.digitalocean.com. (for AName records)</li>
<li>DS: ns2.digitalocean.com. ( &quot;&quot; )</li>
</ul>
<p><strong>Digital Ocean</strong> (I control via Network tab):</p>
<ul>
<li>A Name: @ 666.777.888.999</li>
</ul>
<p>Essentially I can maintain the A Name records (via Digital Ocean) and he can maintain the MX records (via CrazyDomain).</p>
<p>Alternatively - is it possible to set a static ip, that I can map onto different servers, without the client needing to update their records. Digital Ocean has floating IP's - but they must point to DO instances.</p>
","<domain-name-system><ip><domain-registrar>","2017-02-06 13:28:25"
"761732","Apache Error (AH01276), Cannot serve directory /var/www/project/:","<p>I am trying to deploy a Django web app on a newly installed centos7 server.
for this i am using apache2.4 with mod_wsgi.</p>
<p>i am stuck at serving the static files and because i am new to this, this is so confusing to me.</p>
<p>here is my configuration files if you can help me with this please.</p>
<h3>/etc/httpd/conf/httpd.conf:</h3>
<p>(i removed everything i am not using here and removed the commented lines)</p>
<pre><code>Listen 80
Include conf.modules.d/*.conf
User apache
Group apache
ServerAdmin admin@serv.com
&lt;Files &quot;.ht*&quot;&gt;
    Require all denied
&lt;/Files&gt;
ErrorLog &quot;logs/error_log&quot;
LogLevel info

&lt;IfModule log_config_module&gt;
    LogFormat &quot;%h %l %u %t \&quot;%r\&quot; %&gt;s %b \&quot;%{Referer}i\&quot; \&quot;%{User-Agent}i\&quot;&quot; combined
    LogFormat &quot;%h %l %u %t \&quot;%r\&quot; %&gt;s %b&quot; common
    &lt;IfModule logio_module&gt;
      LogFormat &quot;%h %l %u %t \&quot;%r\&quot; %&gt;s %b \&quot;%{Referer}i\&quot; \&quot;%{User-Agent}i\&quot; %I %O&quot; combinedio
    &lt;/IfModule&gt;
    CustomLog &quot;logs/access_log&quot; combined
&lt;/IfModule&gt;
&lt;IfModule mime_module&gt;
    TypesConfig /etc/mime.types
    AddType application/x-compress .Z
    AddType application/x-gzip .gz .tgz
    AddType text/html .shtml
    AddOutputFilter INCLUDES .shtml
&lt;/IfModule&gt;
AddDefaultCharset UTF-8

&lt;IfModule mime_magic_module&gt;
    MIMEMagicFile conf/magic
&lt;/IfModule&gt;
EnableSendfile on
IncludeOptional sites-enabled/*.conf
LoadModule wsgi_module modules/mod_wsgi.so
</code></pre>
<p>in my sites-enabled folder, i have only one config file for my project</p>
<h3>/etc/httpd/sites-enabled/project.conf:</h3>
<pre><code>Alias /favicon.ico /var/www/cvctools/staticfiles/templates/images/favicon.ico
Alias /static/ /var/www/cvctools/staticfiles/
&lt;VirtualHost *:80&gt;
    DocumentRoot /var/www/cvctools
    ServerName proj.ma
    ServerAlias www.proj.ma
    ServerAdmin admin@proj.ma

    &lt;Directory /var/www/cvctools/staticfiles&gt;
        AllowOverride None
        Require all granted
    &lt;/Directory&gt;

    &lt;IfModule wsgi_module&gt;
        WSGIScriptAlias / /var/www/cvctools/CapValue/wsgi.py
        WSGIScriptReloading On
        WSGIDaemonProcess CapValue processes=1 threads=1 maximum-requests=5000 display-name=my-wsgi
        WSGIProcessGroup CapValue
        WSGIPythonPath /var/www/cvctools
        &lt;Directory /var/www/cvctools/CapValue&gt;
            &lt;Files wsgi.py&gt;
                Require all granted
            &lt;/Files&gt;
        &lt;/Directory&gt;
    &lt;/IfModule&gt;
&lt;/VirtualHost&gt;
</code></pre>
<p>in my project folder located at /var/www/cvctools i have this..</p>
<p>./CapValue/wsgi.py:</p>
<pre><code>import django.core.handlers.wsgi
import os,sys

sys.path.append('/vat/www/cvctools')
os.environ.setdefault(&quot;DJANGO_SETTINGS_MODULE&quot;, &quot;CapValue.settings&quot;)
application = django.core.handlers.wsgi.WSGIHandler()
</code></pre>
<p>my staticfiles settings in ./CapValue/settings.py:</p>
<pre><code>PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
STATIC_ROOT = os.path.join(os.getcwd(), 'staticfiles')
STATIC_URL = '/static/'
STATICFILES_DIRS = (os.path.join(PROJECT_ROOT, 'static'),)
STATICFILES_FINDERS = (
    'django.contrib.staticfiles.finders.FileSystemFinder',
    'django.contrib.staticfiles.finders.AppDirectoriesFinder',
    # 'compressor.finders.CompressorFinder',
)
STATICFILES_STORAGE = 'whitenoise.django.GzipManifestStaticFilesStorage'
TEMPLATES = [
    {
        'BACKEND' : 'django.template.backends.django.DjangoTemplates',
        'DIRS'    : [os.path.join(STATIC_ROOT, 'templates'), ],
        'APP_DIRS': True,
        'OPTIONS' : {
            'context_processors': [
                'django.template.context_processors.debug',
                'django.template.context_processors.request',
                'django.contrib.auth.context_processors.auth',
                'django.contrib.messages.context_processors.messages',
            ],
        },
    },
]
</code></pre>
<p>./CapValue/views.py:</p>
<pre><code>from django.views.decorators.csrf import ensure_csrf_cookie
from django.views.generic.base import TemplateView
from django.utils.decorators import method_decorator


class IndexView(TemplateView):
    template_name = 'index.html'

    @method_decorator(ensure_csrf_cookie)
    def dispatch(self, *args, **kwargs):
        return super(IndexView, self).dispatch(*args, **kwargs)
</code></pre>
<p>./CapValue/urls.py:</p>
<pre><code>#imports...
urlpatterns = [
    #...
    url('^.*$', IndexView.as_view(), name='index')
]
</code></pre>
<p>and after running manage.py collectstatic, in my project directory i have staticfiles folder with this contents:</p>
<p>/var/www/cvctools/staticfiles/ :</p>
<pre><code>drwxr-xr-x.  6 admin admin    47 Mar  4 22:36 admin
drwxr-xr-x.  2 admin admin  4096 Mar  4 22:36 css
drwxr-xr-x.  5 admin admin    35 Mar  4 22:36 django_extensions
-rwxr-xr-x.  1 admin admin     0 Mar  4 22:36 human.d41d8cd98f00.txt
-rwxr-xr-x.  1 admin admin     0 Mar  4 22:36 human.txt
drwxr-xr-x.  2 admin admin  4096 Mar  4 22:36 images
drwxr-xr-x. 12 admin admin  4096 Mar  4 22:36 javascript
drwxr-xr-x.  2 admin admin  4096 Mar  4 22:36 notifications
drwxr-xr-x.  6 admin admin    47 Mar  4 22:36 rest_framework
-rwxr-xr-x.  1 admin admin 12925 Mar  4 22:36 staticfiles.json
drwxr-xr-x.  6 admin admin  4096 Mar  4 22:36 templates
</code></pre>
<p>but when i'm trying to access the application, in my apache log i get this error:</p>
<pre><code> [autoindex:error] [pid 3150] [client 192.168.0.126:57623] AH01276: Cannot serve directory /var/www/cvctools/: No matching DirectoryIndex (index.html) found, and server-generated directory index forbidden by Options directive
</code></pre>
<p>what exactly i am doing wrong ?</p>
<h1>Edit</h1>
<p>apparently it have something to do with mod_wsgi, beacause i changed my DcumentRoot to /var/www/cvctools/staticfiles/templates
then i was able to see my index page, but it was like this:</p>
<pre><code>{% load staticfiles %} {% include 'stylesheets.html' %}
{% include 'navbar.html' %}
{% include 'menu.html' %}
{#
#} {# {% include 'breadcrumb.html' %}#} {#
#}
{% include 'footer.html' %}
{% include 'javascripts.html' %} 
</code></pre>
<p>it is not picking my template tags !</p>
","<apache-2.4><python><django><centos7><mod-wsgi>","2016-03-05 03:04:59"
"830952","DNS does not resolve on certain ISPs","<p>We are currently in the process of building a cloud service and we are having DNS issues. </p>

<p>DNS <strong>is resolving correctly</strong> via
 - Cellular/LTE/Verizon
 - Wifi/Cable (tested with different ISPs)</p>

<p>DNS <strong>is not</strong> resolving via cellular/LTE/AT&amp;T </p>

<p><strong>Edit</strong>: Problem occurs with iPhone on AT&amp;T/LTE. Verizon works fine, also on iPhone. The problem was reproduced with Chrome + Safari, also after flushing the DNS cache (airplane mode on + off). Chrome on iOS shows the error: ERR_NAME_NOT_RESOLVED, Safari shows not detailed information.</p>

<p><strong>Edit 2</strong>: I unfortunately can't share uncensored DNS settings but perhaps that helps (these settings have been up for over a week and the TTLs have not been changed):</p>

<pre><code>A   @   &lt;censored ip 1&gt; 600 seconds
A   sub1    &lt;censored ip 2&gt; 1 Hour
A   sub2    &lt;censored ip 2&gt; 1 Hour
CNAME   ftp @   1 Hour
CNAME   issue   @   1 Hour
CNAME   mail    @   1 Hour
CNAME   smtp    @   1 Hour
CNAME   sub3    @   1 Hour
CNAME   www @   1 Hour
CNAME   _domainconnect  _domainconnect.gd.domaincontrol.com 1 Hour  Edit
MX  @   mail.domain.tld (Priority: 0)   1 Hour  Edit
MX  @   smtp.domain.tld (Priority: 10)  1 Hour  Edit
TXT @   google-site-verification=&lt;censored key&gt; 1 Hour  Edit
TXT @   v=spf1 a mx ip4:&lt;censored ip1&gt; ~all 1 Hour  Edit
TXT selector._domainkey k=rsa; p=&lt;censored key&gt; 1 Hour  Edit
TXT _domainkey  o=~; r=noreply@truerec.io   1 Hour  Edit
NS  @   ns47.domaincontrol.com  1 Hour  
NS  @   ns48.domaincontrol.com  1 Hour
</code></pre>

<p>Do we need to add more name servers or is it a different issue?</p>
","<domain-name-system><nameserver><godaddy><mobile-devices>","2017-02-06 21:48:00"
"831042","Prevent file access via web","<p>My mobile app will be serving images from my Ubuntu VPS.</p>

<p>Basically I have the images organized in a folder <code>\DATA</code></p>

<p>What happens is, whenever an image is needed from the mobile application, a request comes to my back end, where it fetches the link of the image and sends it back to the user so that the link would open the image on the phone. For example, <a href=""http://myexamplesite.com/Data/Image001.jpg"" rel=""nofollow noreferrer"">http://myexamplesite.com/Data/Image001.jpg</a></p>

<p>The images that I have are in sequential order. (Image001, Image002, Image003, etc). </p>

<p>What I want to achieve is, if someone finds the link <a href=""http://myexamplesite.com/Data"" rel=""nofollow noreferrer"">http://myexamplesite.com/Data</a>, and tries to access it via the browser, to deny him access. My goal is to make the images accessible only when it comes via a request from the mobile.</p>

<p>I am using
Server version: Apache/2.4.18 (Ubuntu 16.04)
Server built:   2016-07-14 T12:32:26
MySQL:          V5.7</p>

<p>Is such a thing possible on Linux? </p>
","<linux>","2017-02-07 10:26:11"
"831150","Troubleshooting a DNS issue","<p>I created a virtual configuration in VirtualBox for practice but can't get it to work. I have an Ubuntu 14.04 Server as a server and Xubuntu 16.04 as a client. Server has one network interface as a bridge (192.168.1.192) and another in a local network (192.168.56.101), client only one interface in a local network(192.168.56.200). I am trying to set up bind in a DNS forwarder configuration, but the client won't connect to the server.<br>
Server configuration:<br>
-iptables</p>

<pre><code>*filter
:INPUT DROP [1824:109808]
:FORWARD DROP [0:0]
:OUTPUT ACCEPT [776:81374]
-A INPUT -p tcp -m tcp --dport 22 -j ACCEPT
-A INPUT -p icmp -j ACCEPT
-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT
-A INPUT -i lo -j ACCEPT
-A FORWARD -m state --state RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -p icmp -j ACCEPT
-A FORWARD -p udp -m multiport --ports 53 -j ACCEPT
-A FORWARD -p tcp -m multiport --ports 53,80,8080,443,110,25,21 -j ACCEPT
COMMIT
# Completed on Mon Feb  6 19:29:11 2017
# Generated by iptables-save v1.4.21 on Mon Feb  6 19:29:11 2017
*nat
:PREROUTING ACCEPT [1983:119530]
:INPUT ACCEPT [2:144]
:OUTPUT ACCEPT [19:1244]
:POSTROUTING ACCEPT [19:1244]
-A POSTROUTING -s 192.168.56.0/24 -j MASQUERADE
COMMIT
</code></pre>

<p>/etc/bind/named.conf  </p>

<pre><code>// This is the primary configuration file for the BIND DNS server named.
//
// Please read /usr/share/doc/bind9/README.Debian.gz for information on the 
// structure of BIND configuration files in Debian, *BEFORE* you customize 
// this configuration file.
//
// If you are just adding zones, please do that in /etc/bind/named.conf.local

include ""/etc/bind/named.conf.options"";
include ""/etc/bind/named.conf.local"";
include ""/etc/bind/named.conf.default-zones"";
# Use with the following in named.conf, adjusting the allow list as needed:
 key ""rndc-key"" {
    algorithm hmac-md5;
    secret ""PuLMSMi7CDsY1tG9qGSQBQ=="";
 };
# 
 controls {
    inet 127.0.0.1 port 953
        allow { 127.0.0.1; } keys { ""rndc-key""; };
 };
# End of named.conf
</code></pre>

<p>/etc/bind/rndc.conf </p>

<pre><code>#Start of rndc.conf
key ""rndc-key"" {
    algorithm hmac-md5;
    secret ""PuLMSMi7CDsY1tG9qGSQBQ=="";
};

options {
    default-key ""rndc-key"";
    default-server 127.0.0.1;
    default-port 953;
};
# End of rndc.conf
</code></pre>

<p>/etc/bind/named.conf.options </p>

<pre><code>acl goodclients {
        192.168.56.0/24;
        localhost;
        localnets;
};

options {
    directory ""/var/cache/bind"";

    recursion yes;
    allow-query { goodclients; };
    // If there is a firewall between you and nameservers you want
    // to talk to, you may need to fix the firewall to allow multiple
    // ports to talk.  See http://www.kb.cert.org/vuls/id/800113

    // If your ISP provided one or more IP addresses for stable 
    // nameservers, you probably want to use them as forwarders.  
    // Uncomment the following block, and insert the addresses replacing 
    // the all-0's placeholder.

    forwarders {
    8.8.8.8;
    8.8.4.4;
    };
    forward only;
    listen-on port 53 { 192.168.56.101; };

    //========================================================================
    // If BIND logs error messages about the root key being expired,
    // you will need to update your keys.  See https://www.isc.org/bind-keys
    //========================================================================
    dnssec-enable yes;
    dnssec-validation yes;

    auth-nxdomain no;    # conform to RFC1035
    listen-on-v6 { any; };
};
</code></pre>

<p>/etc/resolv.conf</p>

<pre><code># Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8)
#     DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN
nameserver 8.8.8.8
</code></pre>

<p>The bind seems to work fine</p>

<pre><code>sudo service bind9 restart 
 * Stopping domain name service... bind9                                                                                 WARNING: key file (/etc/bind/rndc.key) exists, but using default configuration file (/etc/bind/rndc.conf)
waiting for pid 1046 to die
                                                                                                                  [ OK ]
 * Starting domain name service... bind9                                                                          [ OK ] 
</code></pre>

<p>But named does not, in fact named isn't found at all</p>

<pre><code>sudo service named status
named: unrecognized service
</code></pre>

<p>Client config</p>

<p>/etc/resolv.conf</p>

<pre><code># Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8)
#     DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN
</code></pre>

<p>I have tried adding 192.168.56.101 (server's interface on the inside network) to resolv.conf, nothing happened.</p>

<p>/etc/network/interfaces</p>

<pre><code># interfaces(5) file used by ifup(8) and ifdown(8)
auto lo
iface lo inet loopback
auto enp0s3
iface enp0s3 inet static
    address 192.168.56.200
    netmask 255.255.255.0
    broadcast 192.168.56.255
    network 192.168.56.0
    gateway 192.168.56.101
</code></pre>

<p>/etc/hosts</p>

<pre><code>127.0.0.1   localhost
127.0.1.1   xubuntu-VirtualBox

# The following lines are desirable for IPv6 capable hosts
::1     ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
</code></pre>

<p>Investigation</p>

<pre><code>ping 8.8.8.8

PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.
64 bytes from 8.8.8.8: icmp_seq=1 ttl=40 time=148 ms
64 bytes from 8.8.8.8: icmp_seq=2 ttl=40 time=105 ms
64 bytes from 8.8.8.8: icmp_seq=3 ttl=40 time=78.9 ms
64 bytes from 8.8.8.8: icmp_seq=4 ttl=40 time=79.2 ms
^C
--- 8.8.8.8 ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3004ms
rtt min/avg/max/mdev = 78.928/102.951/148.389/28.314 ms
</code></pre>

<p>Ping to the outside world works</p>

<pre><code>telnet 8.8.8.8 53
Trying 8.8.8.8...
Connected to 8.8.8.8.
Escape character is '^]'.
Connection closed by foreign host.
</code></pre>

<p>As well as telnet.</p>

<pre><code>ping 192.168.56.101
PING 192.168.56.101 (192.168.56.101) 56(84) bytes of data.
64 bytes from 192.168.56.101: icmp_seq=1 ttl=64 time=0.735 ms
64 bytes from 192.168.56.101: icmp_seq=2 ttl=64 time=0.492 ms
64 bytes from 192.168.56.101: icmp_seq=3 ttl=64 time=0.483 ms
64 bytes from 192.168.56.101: icmp_seq=4 ttl=64 time=0.610 ms
64 bytes from 192.168.56.101: icmp_seq=5 ttl=64 time=0.357 ms
^C
--- 192.168.56.101 ping statistics ---
5 packets transmitted, 5 received, 0% packet loss, time 3997ms
rtt min/avg/max/mdev = 0.357/0.535/0.735/0.129 ms
</code></pre>

<p>Ping to the inside server interface suceeds.</p>

<pre><code>telnet 192.168.56.101 53
Trying 192.168.56.101...
telnet: Unable to connect to remote host: Connection timed out
</code></pre>

<p>However the telnet doesn't</p>

<pre><code>netstat -anp | grep '53'
(No info could be read for ""-p"": geteuid()=1000 but you should be root.)
tcp        0      0 192.168.56.101:53       0.0.0.0:*               LISTEN      -               
tcp        0      0 127.0.0.1:953           0.0.0.0:*               LISTEN      -               
tcp6       0      0 :::53                   :::*                    LISTEN      -               
udp        0      0 192.168.56.101:53       0.0.0.0:*                           -               
udp6       0      0 ::1:53908               ::1:48239               ESTABLISHED -               
udp6       0      0 :::53                   :::*                                -               
udp6       0      0 ::1:48239               ::1:53908               ESTABLISHED -
</code></pre>

<p>But the server is listening on this port.</p>

<p>I enabled forwarding in the firewall, set up forwarding in named.conf, disabled ufw on both ends. What else can go awry?</p>

<p>UPDATE: Edited the firewall rules to:</p>

<pre><code># Generated by iptables-save v1.4.21 on Tue Feb  7 21:04:56 2017
*nat
:PREROUTING ACCEPT [1:60]
:INPUT ACCEPT [1:60]
:OUTPUT ACCEPT [6:435]
:POSTROUTING ACCEPT [6:435]
-A POSTROUTING -s 192.168.56.0/24 -j MASQUERADE
COMMIT
# Completed on Tue Feb  7 21:04:56 2017
# Generated by iptables-save v1.4.21 on Tue Feb  7 21:04:56 2017
*filter
:INPUT DROP [0:0]
:FORWARD DROP [0:0]
:OUTPUT ACCEPT [123:14050]
-A INPUT -p tcp -m tcp --dport 22 -j ACCEPT
-A INPUT -p icmp -j ACCEPT
-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT
-A INPUT -p tcp -m multiport --ports 53 -j ACCEPT
-A INPUT -p udp -m multiport --ports 53 -j ACCEPT
-A FORWARD -m state --state RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -p icmp -j ACCEPT
-A FORWARD -p tcp -m multiport --ports 80,8080,443,110,25,21 -j ACCEPT
COMMIT
# Completed on Tue Feb  7 21:04:56 2017
</code></pre>

<p>Telnet connects now, however DNS still doesn't.</p>

<pre><code>dig google.com

; &lt;&lt;&gt;&gt; DiG 9.10.3-P4-Ubuntu &lt;&lt;&gt;&gt; google.com
;; global options: +cmd
;; connection timed out; no servers could be reached
</code></pre>

<p>Elaboration: firewall and bind are hosted on the same server.</p>
","<domain-name-system><bind>","2017-02-07 18:01:01"
"901569","Apache 2.2.3, DISABLE redirect port 80 to 443","<p>Sorry if I'm not clear, 
I'm newbie...</p>

<p>We have a server Apache 2.2.3 on RHEL.
In the httpd.conf there is no VirtualHost configuration,
And the iptables service is stopped</p>

<p>Based on this, 
<a href=""https://wiki.apache.org/httpd/RedirectSSL"" rel=""nofollow noreferrer"">https://wiki.apache.org/httpd/RedirectSSL</a>
I added in the httpd.conf file:</p>

<pre><code>NameVirtualHost *:80
&lt;VirtualHost *:80&gt;

    ServerName mysite.address.com

&lt;/VirtualHost&gt;
</code></pre>

<p>But when I try to access <a href=""http://mysite.address.com"" rel=""nofollow noreferrer"">http://mysite.address.com</a>
I still get <a href=""https://mysite.address.com"" rel=""nofollow noreferrer"">https://mysite.address.com</a></p>

<p>(Thanks Simon Greenwood)
There is a .htaccess with this</p>

<pre><code>RewriteEngine On
RewriteCond %{HTTPS} off
RewriteRule (.*) https://%{HTTP_HOST}%{REQUEST_URI}
</code></pre>

<p>It seems for me that </p>

<pre><code>RewriteRule (.*) https://%{HTTP_HOST}%{REQUEST_URI}
</code></pre>

<p>redirect <em>all</em> to <em>https</em>
Isn't it?</p>

<p>If yes, is it enouth to comment this line to stop the redirect?</p>

<p>Thanks</p>
","<apache-2.2><port>","2018-03-14 11:09:46"
"831387","Disable all NFS Client cache on CentOS","<p>I have a netapp server exportting NFS share (NFS3).<br>
I am monitoring the fpolicy on the netapp server, this means, I listen to all File read operation of the volume (and share..).  </p>

<p>From a CentOS machine, I mount that NFS and perform File read (cat) operation.<br>
In the first time I will do ""cat"", in the netapp I can see that there was a FILE READ event.  </p>

<p>But then if I do additional ""cat filename"", I dont get the event FILE READ counter to increase.<br>
If I got to a different machine, and mount the NFS Share, and do cat, I will see the counter increase by one.<br>
I assume - the NFS Client is having internal cache mechanism.  </p>

<p>I want to disable the NFS Client Cache.  </p>

<p>I mounted the NFS with the following command:  </p>

<pre><code>mount -o lookupcache=none,actimeo=0,noac 10.10.10.1:/vol/vol0_unix /mnt/nfs1
</code></pre>

<p>Notice the lookupcache=none,actimeo=0,noac options - <a href=""https://serverfault.com/questions/638601/nfs-caching-issue"">taken from link</a>.  </p>

<p>Are there additional NFS Client cache mechanism i am missing ?  </p>

<p>My NFS Client is:  Linux CENTOS 6.4 2.6.32-358.el6.x86_64  </p>

<p>Machine NFS Version:</p>

<pre><code>[ilan@DevCore-Centos6 nfs1]$ rpm -qa|grep -i nfs
nfs-utils-1.2.3-36.el6.x86_64
nfs-utils-lib-1.1.5-6.el6.x86_64
nfs4-acl-tools-0.3.3-6.el6.x86_64
[ilan@DevCore-Centos6 nfs1]$
</code></pre>

<p>I Assume that <a href=""https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Storage_Administration_Guide/fscachesetup.html"" rel=""nofollow noreferrer"">cachefilesd</a> is not running by default.</p>
","<centos><nfs><mount><netapp>","2017-02-08 17:06:24"
"831390","dns dig - no ANSWER unless ANY option set","<p>So wired issue.
Ubuntu 16.04 - I receive no ANSWER of the dig command (for specific domain), unless ANY is set in the dig command</p>

<p>Of course regular dns queries does not work too :/</p>

<pre><code>; &lt;&lt;&gt;&gt; DiG 9.10.3-P4-Ubuntu &lt;&lt;&gt;&gt; @8.8.8.8 stg-test102.example.net ANY
; (1 server found)
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 18534
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 512
;; QUESTION SECTION:
;stg-test102.example.net.           IN      ANY

;; ANSWER SECTION:
stg-test102.example.net.    599     IN      A       172.16.x.x

;; Query time: 53 msec
;; SERVER: 8.8.8.8#53(8.8.8.8)
;; WHEN: Wed Feb 08 17:58:50 CET 2017
;; MSG SIZE  rcvd: 64

root@uhost:/home/user# dig @8.8.8.8 stg-test102.example.net  

; &lt;&lt;&gt;&gt; DiG 9.10.3-P4-Ubuntu &lt;&lt;&gt;&gt; @8.8.8.8 stg-test102.example.net
; (1 server found)
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 36491
;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 512
;; QUESTION SECTION:
;stg-test102.example.net.           IN      A

;; Query time: 47 msec
;; SERVER: 8.8.8.8#53(8.8.8.8)
;; WHEN: Wed Feb 08 17:58:56 CET 2017
;; MSG SIZE  rcvd: 48
</code></pre>

<p>tcpdumps:</p>

<p><code>tcpdump -i any port 53 -A -n -w /tmp/t.pcap</code></p>

<p>regular dig, my host:</p>

<pre><code>Frame 2: 92 bytes on wire (736 bits), 92 bytes captured (736 bits)
Linux cooked capture
Internet Protocol Version 4, Src: 8.8.8.8, Dst: 192.168.x.x
User Datagram Protocol, Src Port: 53 (53), Dst Port: 33205 (33205)
Domain Name System (response)
    [Request In: 1]
    [Time: 0.053160000 seconds]
    Transaction ID: 0x4b37
    Flags: 0x8180 Standard query response, No error
        1... .... .... .... = Response: Message is a response
        .000 0... .... .... = Opcode: Standard query (0)
        .... .0.. .... .... = Authoritative: Server is not an authority for domain
        .... ..0. .... .... = Truncated: Message is not truncated
        .... ...1 .... .... = Recursion desired: Do query recursively
        .... .... 1... .... = Recursion available: Server can do recursive queries
        .... .... .0.. .... = Z: reserved (0)
        .... .... ..0. .... = Answer authenticated: Answer/authority portion was not authenticated by the server
        .... .... ...0 .... = Non-authenticated data: Unacceptable
        .... .... .... 0000 = Reply code: No error (0)
    Questions: 1
    Answer RRs: 0
    Authority RRs: 0
    Additional RRs: 1
    Queries
        stg-test102.example.net: type A, class IN
            Name: stg-test102.example.net
            [Name Length: 19]
            [Label Count: 3]
            Type: A (Host Address) (1)
            Class: IN (0x0001)
    Additional records
        &lt;Root&gt;: type OPT
            Name: &lt;Root&gt;
            Type: OPT (41)
            UDP payload size: 512
            Higher bits in extended RCODE: 0x00
            EDNS0 version: 0
            Z: 0x0000
                0... .... .... .... = DO bit: Cannot handle DNSSEC security RRs
                .000 0000 0000 0000 = Reserved: 0x0000
            Data length: 0
</code></pre>

<p>dig ANY, my host</p>

<pre><code>Frame 4: 108 bytes on wire (864 bits), 108 bytes captured (864 bits)
Linux cooked capture
Internet Protocol Version 4, Src: 8.8.8.8, Dst: 192.168.x.x
User Datagram Protocol, Src Port: 53 (53), Dst Port: 34839 (34839)
Domain Name System (response)
    [Request In: 3]
    [Time: 0.046263000 seconds]
    Transaction ID: 0xe8eb
    Flags: 0x8180 Standard query response, No error
        1... .... .... .... = Response: Message is a response
        .000 0... .... .... = Opcode: Standard query (0)
        .... .0.. .... .... = Authoritative: Server is not an authority for domain
        .... ..0. .... .... = Truncated: Message is not truncated
        .... ...1 .... .... = Recursion desired: Do query recursively
        .... .... 1... .... = Recursion available: Server can do recursive queries
        .... .... .0.. .... = Z: reserved (0)
        .... .... ..0. .... = Answer authenticated: Answer/authority portion was not authenticated by the server
        .... .... ...0 .... = Non-authenticated data: Unacceptable
        .... .... .... 0000 = Reply code: No error (0)
    Questions: 1
    Answer RRs: 1
    Authority RRs: 0
    Additional RRs: 1
    Queries
        stg-test102.example.net: type ANY, class IN
            Name: stg-test102.example.net
            [Name Length: 19]
            [Label Count: 3]
            Type: * (A request for all records the server/cache has available) (255)
            Class: IN (0x0001)
    Answers
        stg-test102.example.net: type A, class IN, addr 172.16.z.y
            Name: stg-test102.example.net
            Type: A (Host Address) (1)
            Class: IN (0x0001)
            Time to live: 599
            Data length: 4
            Address: 172.16.z.y
    Additional records
        &lt;Root&gt;: type OPT
            Name: &lt;Root&gt;
            Type: OPT (41)
            UDP payload size: 512
            Higher bits in extended RCODE: 0x00
            EDNS0 version: 0
            Z: 0x0000
                0... .... .... .... = DO bit: Cannot handle DNSSEC security RRs
                .000 0000 0000 0000 = Reserved: 0x0000
            Data length: 0
</code></pre>

<p>regular dig, some other host</p>

<pre><code>Frame 128: 98 bytes on wire (784 bits), 98 bytes captured (784 bits)
Linux cooked capture
Internet Protocol Version 4, Src: 216.239.x.x, Dst: 192.168.x.x
User Datagram Protocol, Src Port: 53 (53), Dst Port: 33085 (33085)
Domain Name System (response)
    [Request In: 127]
    [Time: 0.023883000 seconds]
    Transaction ID: 0x5576
    Flags: 0x8400 Standard query response, No error
        1... .... .... .... = Response: Message is a response
        .000 0... .... .... = Opcode: Standard query (0)
        .... .1.. .... .... = Authoritative: Server is an authority for domain
        .... ..0. .... .... = Truncated: Message is not truncated
        .... ...0 .... .... = Recursion desired: Don't do query recursively
        .... .... 0... .... = Recursion available: Server can't do recursive queries
        .... .... .0.. .... = Z: reserved (0)
        .... .... ..0. .... = Answer authenticated: Answer/authority portion was not authenticated by the server
        .... .... ...0 .... = Non-authenticated data: Unacceptable
        .... .... .... 0000 = Reply code: No error (0)
    Questions: 1
    Answer RRs: 1
    Authority RRs: 0
    Additional RRs: 0
    Queries
        stg-test102.example.net: type A class IN,
            Name: stg-test102.example.net
            [Name Length: 20]
            [Label Count: 3]
            Type: A (Host Address) (1)
            Class: IN (0x0001)
    Answers
        stg-test102.example.net: type A, class IN, addr 172.16.z.y
            Name: stg-test102.example.net
            Type: A (Host Address) (1)
            Class: IN (0x0001)
            Time to live: 600
            Data length: 4
            Address: 172.16.x.193
</code></pre>

<p>Nice commands page: <a href=""http://linoxide.com/how-tos/useful-options-dig/"" rel=""nofollow noreferrer"">http://linoxide.com/how-tos/useful-options-dig/</a></p>
","<domain-name-system><dig>","2017-02-08 17:14:20"
"831393","ansible task generating weird syntax for logrotate definition file","<p>I am using ansible logrotate definition for logrotate in ubuntu 14.04 which is having definition as below</p>

<pre><code>---

- name: dependencies
  apt: pkg={{item}} state=latest
  with_items:
    - unzip
    - jq

- name: check if already downloaded
  stat: path={{nomad_download_folder}}/{{nomad_archive}}
  register: nomad_archive_stat

- name: download
  get_url: &gt;
    url={{nomad_download}}
    dest={{nomad_download_folder}}
    sha256sum={{nomad_checksum}}
  register: nomad_downloaded
  when: nomad_archive_stat.stat.exists == false

- name: group
  group: &gt;
    name={{nomad_group}}
    state=present
  register: nomad_group_created

# On Nomad schedulers
- name: user
  user: &gt;
    home={{nomad_home}}
    name={{nomad_user}}
    system=yes
    groups={{nomad_group}}
    append=yes
  when: (nomad_group_created | changed) and (nomad_is_server == true)

# On Nomad runners
- name: user
  user: &gt;
    home={{nomad_home}}
    name={{nomad_user}}
    system=yes
    groups={{nomad_group}},docker
    append=yes
  when: (nomad_group_created | changed) and (nomad_is_server == false)

- name: directories
  file: &gt;
    state=directory
    path={{item}}
    owner={{nomad_user}}
    group={{nomad_group}}
  with_items:
    - ""{{nomad_home}}""
    - ""{{nomad_home}}/bin""
    - ""{{nomad_config_dir}}""

- name: check for log directory
  stat: path={{nomad_log_file | dirname}}
  register: nomad_log_directory_stat

- name: create log directory
  file: &gt;
    state=directory
    path={{nomad_log_file | dirname}}
    owner={{nomad_user}}
    group={{nomad_group}}
  when: not nomad_log_directory_stat.stat.exists

- name: touch log file
  file: &gt;
    state=touch
    path={{nomad_log_file}}
    owner={{nomad_user}}
    group={{nomad_group}}
  changed_when: false

- name: install
  unarchive: &gt;
    src={{nomad_download_folder}}/{{nomad_archive}}
    dest={{nomad_home}}/bin
    copy=no
  when: nomad_downloaded | changed

- name: link executable in PATH
  file: &gt;
    state=link
    src={{nomad_home}}/bin/nomad
    dest=/usr/local/bin/nomad

- name: set ownership
  file: &gt;
    state=directory
    path={{nomad_home}}
    owner={{nomad_user}}
    group={{nomad_group}}
    recurse=yes
  when: nomad_downloaded | changed

- name: nomad config file
  template: &gt;
    src=nomad.conf.j2
    dest={{nomad_config_file}}
    owner={{nomad_user}}
    group={{nomad_group}}
    mode=0755
  notify:
    - restart nomad

- name: copy nomad upstart script
  template: &gt;
    src=nomad.upstart.conf.j2
    dest=/etc/init/nomad.conf
    owner={{nomad_user}}
    group={{nomad_group}}
    mode=0755
  notify:
    - restart nomad

- name: rotate log file
  logrotate: name=nomad path={{nomad_log_file}}
  args:
    options:
      - daily
      - missingok
      - rotate 3
      - compress
      - delaycompress
      - copytruncate
      - notifempty
</code></pre>

<p>Which causes to generate file with weird syntax which is below.</p>

<pre><code>sudo cat /etc/logrotate.d/nomad

# Generated by Ansible.
# Local modifications will be overwritten.

/var/log/nomad.log {
  [
  '
  d
  a
  i
  l
  y
  '
  ,

  '
  m
  i
  s
  s
  i
  n
  g
  o
  k
  '
  ,

  '
  r
  o
  t
  a
  t
  e

  3
  '
  ,

  '
  c
  o
  m
  p
  r
  e
  s
  s
  '
  ,

  '
  d
  e
  l
  a
  y
  c
  o
  m
  p
  r
  e
  s
  s
  '
  ,

  '
  c
  o
  p
  y
  t
  r
  u
  n
  c
  a
  t
  e
  '
  ,

  '
  n
  o
  t
  i
  f
  e
  m
  p
  t
  y
  '
  ]
}
</code></pre>

<p>Due to this syntax error I am getting so many emails every morning. Below are more details about server.</p>

<pre><code>$sudo ansible --version
ansible 2.2.1.0
  config file = /etc/ansible/ansible.cfg
  configured module search path = Default w/o overrides
$lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 14.04.4 LTS
Release:    14.04
Codename:   trusty
</code></pre>
","<linux><ubuntu><ubuntu-14.04><ansible><logrotate>","2017-02-08 17:28:40"
"973423","LetsEncrypt certificate (renewing) installing error","<p>I'm trying to renew my LetsEncrypt certificate on my Raspberry Pi running apache2, but I seem to get an error when CertBot tries to generate the certificate.</p>
<pre><code>Expected sha256 a988718abfad80b6b157acce7bf130a30876d27603738ac39f140993246b25b3
         Got        6162f1c328e42d9bd4e74ca234a575748f199e40f56d5ec3204d55cd130cfd7e
</code></pre>
<p>I've tried manually running it by updating my DNS records on my domain but I seem to get this error</p>
<pre><code>    Failed authorization procedure. theflyingrat.com (dns-01): urn:ietf:params:acme:error:dns :: DNS problem: NXDOMAIN looking up TXT for _acme-challenge.theflyingrat.com

IMPORTANT NOTES:
 - The following errors were reported by the server:

   Domain: mydomain.com
   Type:   None
   Detail: DNS problem: NXDOMAIN looking up TXT for
   _acme-challenge.mydomain.com
</code></pre>
<p>If I can, I want to be able to know how to install a Wildcard certificate, but that also gives me errors. I've tried the --certonly parameter on a Raspberry Pi Zero to hopefully give me the certificate there so I can migrate it over to my main Pi Server, but that (as you guessed) also gives me the SAME errors.</p>
<p>Thanks in advance, P.s, the (non-wildcard) certificate expired yesterday, so a fast response would be very grateful.</p>
<p>Thanks, Rat</p>
","<ssl-certificate><apache2><lets-encrypt><ssl-certificate-errors><ssl-certificate-renewal>","2019-06-30 02:48:10"
"761750","MSSQL Server 2008r2 database restoration issue","<p>Let me explain the issue briefly.</p>

<p>I am using MSSQL server 2008r2 on which there is a existing database with name ""portaluser2"" i have created one more database with dbname ""chandigarh"".</p>

<p>I took the back up of database ""portaluser2"" and tried to restore it to database ""chnadigarh"" at time of restoration i am getting error, which i have listed below. Kindly keep in mind that i want both my databases online and unning. Kindly assist if anyone have any idea on this.</p>

<pre><code>Error: TITLE: Microsoft SQL Server Management Studio
Restore failed for Server '64.31.20.2,7426'. (Microsoft.SqlServer.SmoExtended)

For help, click: http://go.microsoft.com/fwlink?ProdName=Microsoft+SQL+Server&amp;ProdVer=10.50.4042.0+((KJ_SP2_GDR).150326-2110+)&amp;EvtSrc=Microsoft.SqlServer.Management.Smo.ExceptionTemplates.FailedOperationExceptionText&amp;EvtID=Restore+Server&amp;LinkId=20476

ADDITIONAL INFORMATION:

System.Data.SqlClient.SqlError: The backup set holds a backup of a database other than the existing 'chandigarh' database. (Microsoft.SqlServer.Smo)

For help, click: http://go.microsoft.com/fwlink?ProdName=Microsoft+SQL+Server&amp;ProdVer=10.50.4042.0+((KJ_SP2_GDR).150326-2110+)&amp;LinkId=20476
</code></pre>

<p>++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</p>
","<sql-server>","2016-03-05 07:14:13"
"973439","How to force installation of specific mysql server version (5.6) in Ubuntu 18.04","<p>I have installed Ubuntu 18.04.</p>

<p>I want to install mysql server version 5.6, but <code>apt-get</code> shows that version 5.7 will be installed:</p>

<pre><code>apt-get install mysql-server
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following additional packages will be installed:
  libaio1 libcgi-fast-perl libcgi-pm-perl libedit2 libencode-locale-perl libevent-core-2.1-6 libfcgi-perl libgdbm-compat4 libgdbm5 libhtml-parser-perl
  libhtml-tagset-perl libhtml-template-perl libhttp-date-perl libhttp-message-perl libio-html-perl liblwp-mediatypes-perl libnuma1 libperl5.26
  libtimedate-perl liburi-perl libwrap0 mysql-client-5.7 mysql-client-core-5.7 mysql-common mysql-server-5.7 mysql-server-core-5.7 netbase perl
  perl-modules-5.26 psmisc
Suggested packages:
  gdbm-l10n libdata-dump-perl libipc-sharedcache-perl libwww-perl mailx tinyca perl-doc libterm-readline-gnu-perl | libterm-readline-perl-perl make
The following NEW packages will be installed:
  libaio1 libcgi-fast-perl libcgi-pm-perl libedit2 libencode-locale-perl libevent-core-2.1-6 libfcgi-perl libgdbm-compat4 libgdbm5 libhtml-parser-perl
  libhtml-tagset-perl libhtml-template-perl libhttp-date-perl libhttp-message-perl libio-html-perl liblwp-mediatypes-perl libnuma1 libperl5.26
  libtimedate-perl liburi-perl libwrap0 mysql-client-5.7 mysql-client-core-5.7 mysql-common mysql-server mysql-server-5.7 mysql-server-core-5.7 netbase
  perl perl-modules-5.26 psmisc
0 upgraded, 31 newly installed, 0 to remove and 7 not upgraded.
Need to get 27.8 MB of archives.
After this operation, 204 MB of additional disk space will be used.
</code></pre>

<p>How can I force the installation of mysql version 5.6?</p>
","<ubuntu-18.04>","2019-06-30 08:49:41"
"973474","Can I use a different nameserver for a domain that is a nameserver itself as ns1.domain.com","<p>My example.com domain used as the name server for all my other websites and domains, the following name servers have been setup as nameservers on different IP address and servers:</p>

<pre><code>ns1.example.com
ns2.example.com
ns3.example.com
ns4.example.com
</code></pre>

<p>Can I use the following name servers for all my other website but set another 2 nameservers on CloudFlare to route example.com itself. </p>

<p>Right now the nameservers for example.com are the same 4 nameservers, another 100 websites use these nameservers as well. I now want to keep everything the same but use CloudFlare for example.com as name servers. I am wondering if this is a problem and if the root domain of nameservers must point to them as it's authoritative name servers.</p>

<p>Is this safe to do this? Or does it go against any RFC rules?</p>
","<domain-name-system><bind><nameserver><glue-record>","2019-06-30 19:39:16"
"761821","Installing self-signed certificate on tomcat","<p>down vote
favorite
The question: I have an application running under tomcat which calls another application under the same tomcat via a gateways which has a self signed SSL certificate installed. When you call the link from your browser it warns you about potentially dangerous site, but you have an option to proceed anyway. However the tomcat cannot proceed anyway until it trusts the certificate. The certificate is issued by KEMP technologies. So how do I make tomcat to trust the certificate? I have .cer and .key files.</p>

<p>OS: Windows 2008R2</p>

<p>Tomcat 7</p>

<p>Thanks.</p>
","<windows-server-2008-r2><ssl><tomcat><self-signed-certificate>","2016-03-05 16:49:18"
"831507","Why can't I see http request in wireshark?","<p>I am working in wireshark. I am monitoring the wifi traffic on the same network. We have 6 pc's there and one machine was installed on wireshark to  capture wifi traffic. I got my team mates Ip address in the endpoint list.  My machine have installed wiresahrk. whenever they ping to my machine. my machine ip : 192.168.1.214. and others are: 192.168.1.31, 164,188,242. and they ping a request to my m/c I can capture that request.
And if 192.168.1.188 machine send a http request to a local server to other ip like 164, 31 . I couldn't capture that http request. Its hows only mdns protocol.. here is the screenshot..</p>

<p>Why I can't capture the http request ? Why other team mates IP (188-->164.31) was not capture ?  Anyone please help me </p>

<p>I am working in ubuntu machine.</p>

<p>My router type is : <code>Belkin Surf N150 Wireless Modem Router, F9J1001 v1.</code></p>

<p><a href=""https://i.sstatic.net/X6LGy.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","<wireshark>","2017-02-09 04:54:27"
"762069","Does NTP should update time after manual change?","<p>I have configured <code>NTP</code> service and I've checked it by the following way:</p>

<ol>
<li>Disable NTP</li>
<li>Adjust time</li>
<li>Enable NTP</li>
</ol>

<p>Host's time <strong>fixed</strong> in few seconds. But if I do:</p>

<ol>
<li>Enable NTP</li>
<li>Adjust time</li>
</ol>

<p>Time is <strong>not fixed</strong>. Should be time fixed in this case?<br>
I'm setting time by command like  <code>/bin/date -s ""4:2"" &amp;&amp; /sbin/hwclock -w</code><br>
OS: <code>Debian 3.2.68-1+deb7u5</code><br>
ntp.conf (all default, except <code>server</code> line):  </p>

<pre><code># /etc/ntp.conf, configuration for ntpd; see ntp.conf(5) for help

driftfile /var/lib/ntp/ntp.drift


# Enable this if you want statistics to be logged.
#statsdir /var/log/ntpstats/

statistics loopstats peerstats clockstats
filegen loopstats file loopstats type day enable
filegen peerstats file peerstats type day enable
filegen clockstats file clockstats type day enable


# You do need to talk to an NTP server or two (or three).
server 0.pool.ntp.org iburst
server 1.pool.ntp.org iburst
server 2.pool.ntp.org iburst
server 3.pool.ntp.org iburst

# pool.ntp.org maps to about 1000 low-stratum NTP servers.  Your server will
# pick a different set every time it starts up.  Please consider joining the
# pool: &lt;http://www.pool.ntp.org/join.html&gt;
#server 0.debian.pool.ntp.org iburst
#server 1.debian.pool.ntp.org iburst
#server 2.debian.pool.ntp.org iburst
#server 3.debian.pool.ntp.org iburst


# Access control configuration; see /usr/share/doc/ntp-doc/html/accopt.html for
# details.  The web page &lt;http://support.ntp.org/bin/view/Support/AccessRestrictions&gt;
# might also be helpful.
#
# Note that ""restrict"" applies to both servers and clients, so a configuration
# that might be intended to block requests from certain clients could also end
# up blocking replies from your own upstream servers.

# By default, exchange time with everybody, but don't allow configuration.
restrict -4 default kod notrap nomodify nopeer noquery
restrict -6 default kod notrap nomodify nopeer noquery
# Local users may interrogate the ntp server more closely.
restrict 127.0.0.1
restrict ::1

# Clients from this (example!) subnet have unlimited access, but only if
# cryptographically authenticated.
#restrict 192.168.123.0 mask 255.255.255.0 notrust


# If you want to provide time to your local subnet, change the next line.
# (Again, the address is an example only.)
#broadcast 192.168.123.255

# If you want to listen to time broadcasts on your local subnet, de-comment the
# next lines.  Please do this only if you trust everybody on the network!
#disable auth
#broadcastclient
</code></pre>
","<ntp>","2016-03-07 12:53:57"
"973723","cors post laravel vuejs (405 Method Not Allowed)","<p>People let me ask the same post method as to why the place is not. I'm calling api post between laravel and vuejs. thanks all read the article!</p>

<p><a href=""https://i.sstatic.net/ZpjEe.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZpjEe.jpg"" alt=""enter image description here""></a></p>

<p><a href=""https://i.sstatic.net/DfghG.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/DfghG.jpg"" alt=""enter image description here""></a></p>

<p><a href=""https://i.sstatic.net/hSYTf.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/hSYTf.jpg"" alt=""enter image description here""></a></p>
","<cors>","2019-07-02 16:57:25"
"831867","How to limit number of connections user may use?","<p>I have a process <code>greedyprocess</code> that is always run as user <code>greedyuser</code> that uses too many NAT connections. They aren't using too much bandwidth, instead they are clogging up the router's NAT table. Is it possible to limit the user to, say, 1000 open connections? I would prefer some kind of kernel limit on the number of sockets that <code>greedyuser</code> can have open.</p>

<p>Server running debian jessie (stable branch at the time of writing).
No, I cannot work around the NAT as I do not control the router.</p>

<p>User's open file limit is 166,384 and I prefer not to reduce this if possible. I only want to limit open sockets, not all open files.</p>
","<linux><connlimit>","2017-02-10 17:25:53"
"762242","How do I determine the version of the SCOM agent installed on a server?","<p>Context first: We have two version of SCOM installed. An SCOM 2007 installation and a recently installed 2012 R2. After much swearing and dirty looks from a colleague one cubicle over I'm starting to figure this product out...</p>

<p>I can see that SCOM agent is installed on a particular server by checking for the windows service HealthService.</p>

<p>What I want to know is how do I tell which version of SCOM the agent is talking to?</p>
","<system-center><scom>","2016-03-08 04:50:56"
"832048","SQL 2008 cluster","<p>I have a problem with the SQL cluster 2008 after I backup one of the volume, format the volume and restore the volume, then the cluster SQL agent can't start anymore. I did so many times but everything was always fine. Now if I do the restore one Volume for the transaction log then SQL instance can't startup at all.<br>
I'm having two errors:<br>
1) msdb can't start.<br>
2) login failure of the SQL service account.<br>
Is it related? the volume for SQL data hasn't change at all.<br>
I can't even restore the msdb database.</p>

<p><a href=""https://i.sstatic.net/QHPgP.jpg"" rel=""nofollow noreferrer"">restore msdb</a></p>

<p><a href=""https://i.sstatic.net/cdxBw.jpg"" rel=""nofollow noreferrer"">when repairing sql installation</a></p>
","<sql>","2017-02-11 14:01:41"
"974278","When should one enable ""Do not require Kerberos Preauthentication""?","<p>In active directory, there's an option <code>Do not require Kerberos Preauthentication</code>.</p>
<p><a href=""https://i.sstatic.net/n56xn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/n56xn.png"" alt=""enter image description here"" /></a></p>
<p>Does anyone know the use-case when it's checked?</p>
","<active-directory>","2019-07-07 14:42:26"
"832785","Server with very high load","<p>I have a problem with my server, which hosts several sites with circa 400 concurrent visits at peak time.<br>
The problem is that every few hours the load of the server gets very high for a minute or so, and then it will get back to normal again; this load peak makes my sites really slow to reach or not reachable at all. This is the output of top during a high load spell:<br></p>

<p><a href=""https://i.sstatic.net/cf5qh.png"" rel=""nofollow noreferrer"">Server top output</a><br></p>

<p>I also noticed that sometimes this high load not related to the peak time.
I also tried to tune the MYSQL but nothing changed.</p>

<p>My sever details:<br>
Processor   1x Intel Quad-Core Xeon X3440 [ 4 Core(s) ]<br>
RAM 16 GB<br></p>
","<mysql><httpd><high-load>","2017-02-15 21:09:26"
"974780","What is meant by outstanding IO request in a host controller (e.g. HBA/RAID card)","<p>In context of a host controller (HBA/RAID card) which sends IO traffic to SAS/SATA drives which are part of logical volume - so in such setup, generally I hear a term ""Outstanding commands"". I did got somewhat understanding that these are number of IO requests which a controller can accept even if it already has some pending IOs which are not completed yet. So two questions here,</p>

<p>1) What would be a more complete (or more comprehensive) definition for <code>outstanding</code> IO commands ?</p>

<p>2) What is generic behavior of host controller when more IO commands arrives which is larger than its <code>queue depth</code> for holding outstanding commands ?</p>

<p>3) How <code>non NCQ</code> commands handled in such design ? Is it possible that in case of SATA drives where we have <code>NCQ</code> concept, a <code>non NCQ</code> command can starve for arbitrary amount of time ?</p>
","<storage><sata><hba><raid-controller>","2019-07-11 05:10:44"
"974784","Pointing to another domain using CNAME","<p>I would like to point a subdomain to be displayed in another external domain using CNAME flattening.</p>

<p>e.g. <code>book.webbangkok.com</code> to be displayed at <code>property.thailandbusinessconsultant.com</code></p>

<p>In <code>webbangkok.com</code> DNS, I have created the CNAME blog to the value of <code>property.thailandbusinessconsultant.com</code>. What do I need to do within the DNS for <code>property.thailandbusinessconsultant.com</code> to display it?</p>
","<subdomain><nameserver><cname-record><hostname><dns-hosting>","2019-07-11 05:55:29"
"762559","phpMyAdmin - make user which only sees databases they create","<p>I am aware of how to grant a user access to only one database using phpMyAdmin, and I am aware of how to give a user access to create a new database using phpMyAdmin (using the Create privilege).</p>

<p>However, granting the Create privilege to a user also allows them to see all databases. Is there a way to allow users to create databases, but only view the databases that they have created?</p>

<p>I realise that this can be done with actual PHP / MySQL queries when creating the database (i.e. <code>GRANT</code>, also see <a href=""https://serverfault.com/questions/309401/dont-list-the-databases-which-dont-belong-to-the-user"">here</a>), however can it be done using only the phpMyAdmin interface (thus I also don't want answers involving a <em>different</em> interface)? </p>
","<mysql><phpmyadmin>","2016-03-09 07:05:58"
"762586","ESXi host memory usage seems high with no VMs running","<p>I have set up an ESXi 6.0 Update 1 installation on a HP ProLiant Microserver Gen8.  This is using the HP-provided image for ProLiant servers.</p>

<p>With no VMs running the host memory usage is 1714 MB, which seems high to me. The only config changes I have made on the ESXi host are SSH was enabled and NTP client was configured.</p>

<p>I've run esxtop in memory (M) mode and the two processes using the most memory are hostd and vpxa - I don't see any custom HP processes in the list eating RAM.</p>

<p>I've tried disabling CIM and memory usage dropped to 1666 MB, which still seems high for a bare ESXi server! Is this normal and if not, how can I reduce the memory usage?
<a href=""https://i.sstatic.net/m2GOa.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/m2GOa.png"" alt=""enter image description here""></a></p>
","<virtualization><vmware-esxi><memory>","2016-03-09 09:41:24"
"762607","Migration between Oracle and Azure SQL Data warehouse using SSMA","<p>I am facing the below error while syncing table from Oracle to an Azure SQL data warehouse using SSMA tool (a tool used to migrate data from Oracle to Azure SQL). I am able to connect to the database and convert the schema from Oracle database but, am getting an error while syncing it with the Azure SQL data warehouse. Please suggest a solution.</p>
<blockquote>
<p>Errors: USE statement is not supported to switch between databases. Use a new connection to connect to a different Database.</p>
<p>Synchronization error: USE statement is not supported to switch between databases. Use a new connection to connect to a different Database.</p>
</blockquote>
<p>I am unable to identify the rectification of this error in SSMA. Would help lots to know the resolution.</p>
","<sql-server><azure><oracle-11g><data-warehouse>","2016-03-09 11:10:39"
"762870","RAID 10 (2+2) and (4+4) - terminology or physical setup?","<p>I understand what RAID10 (2+2) is (stripped mirror) whereby there are data are striped across 2 groups and each group have a pair of mirror devices (2 disks).</p>

<p>This is how i have understand ""2+2""</p>

<p>For RAID10 4+4,  is it</p>

<ol>
<li>striping across 4 groups, each group still having 2 mirrored
devices or </li>
<li>striping across 2 groups, each group have 4 mirrored devices</li>
<li>striping across 4 groups, each group have 4 mirrored
devices</li>
</ol>

<p>2 and 3 just doesn't make sense.</p>

<p>How is 1) a 4+4 then ?</p>

<p>Regards,
Noob</p>
","<raid><hard-drive><storage><storage-area-network><network-attached-storage>","2016-03-10 11:45:25"
"762899","Move Unallocated Space Between Disks","<p>I am running Ubuntu's latest LTS.
In /dev/sda I have sda1 and sda2, both are using a couple hundreds of MiB with a couple of GiB allocated. So far so good.</p>

<p>In /dev/sdb I have just one partition, sdb1 - using over 300 GiB, about 98% of the available space.</p>

<p>I have about 350 GiB of unallocated space in /dev/sda - what is the easiest way to both move it from sda to sdb and allocate it?</p>
","<ubuntu><vmware-esxi><storage><partition><disk-space-utilization>","2016-03-10 13:48:46"
"975314","Can't delete windows.old folder from inside .wim file","<p>Someone created this image without cleaning up the C drive first. I have taken this over at a new job, and, the .wim file we have to use for re-imaging is absolutely massive due to it.</p>
<p>I mounted the .wim file with DISM, and, managed to delete a few folders through Windows Explorer, however, some of the stuff in it just won't delete.</p>
<p>The error I get is: &quot;You require permission from TrustedInstaller to make changes to this file&quot;.</p>
<p>And so I have to skip all of them which then leaves me with about 17 GB of files. I am logged in as a domain admin.</p>
<p>I've even changed the owner of the whole folder (and it's 200.000+ files) to myself and given myself explicit permission over them, yet, when I now try to delete the folder, it says that I need permission from my own username to make changes to the file.</p>
<p>I'm logged in as the user that it is saying it needs permissions from, which is really weird.</p>
<p>If I try rmdir in an elevated command prompt it just says that access is denied.</p>
<p>Any thoughts anyone?</p>
<p>Host OS is 2008 W2.</p>
","<windows-server-2008><uac><wim>","2019-07-15 13:33:16"
"763046","how can i proxy :80/abc to :8080 by nginx","<p>how can i proxy :80/abc to :8080 by nginx. here is my config and it got 404 </p>

<pre><code>server {
  listen 80;
  server_name xx.com;
  location /abc/ {
    proxy_set_header Host $host;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_redirect http://127.0.0.1:8080/;
  } 
}
</code></pre>
","<nginx>","2016-03-11 00:19:35"
"975463","Windows Server 2016 disconnecting every hour","<p>I have a Windows Server 2016 running a SQL Server and every hour I get several errors about connection problems.</p>

<p>I found out that the network adapter has <code>Lease Obtained = 2:44 PM</code> and has <code>Lease Expires = 3:44 PM</code> for example, that means that I had network issues at 2:44, 1:44, 12:44, ... and I will have another one at 3:44 PM the next time it renews.  The IP is fixed though in AWS. So while the server is set to DHCP, the IP should not change. </p>

<p>To be clear, the server disconnects for a (or a few) second only, then everything goes back to normal. Also, if I set the IP to Static IP, the problem doesn't happen anymore.</p>

<p><strong><code>ipconfig /all:</code></strong></p>

<pre><code>Windows IP Configuration

   Host Name . . . . . . . . . . . . : SQL02
   Primary Dns Suffix  . . . . . . . :
   Node Type . . . . . . . . . . . . : Hybrid
   IP Routing Enabled. . . . . . . . : No
   WINS Proxy Enabled. . . . . . . . : No
   DNS Suffix Search List. . . . . . : us-east-1.ec2-utilities.amazonaws.com
                                       ec2.internal

Ethernet adapter Ethernet 2:

   Connection-specific DNS Suffix  . : ec2.internal
   Description . . . . . . . . . . . : Amazon Elastic Network Adapter
   Physical Address. . . . . . . . . : 0A-4D-28-5B-73-E8
   DHCP Enabled. . . . . . . . . . . : Yes
   Autoconfiguration Enabled . . . . : Yes
   Link-local IPv6 Address . . . . . : fe80::4878:2803:afe:7ac0%16(Preferred)
   IPv4 Address. . . . . . . . . . . : 172.30.1.11(Preferred)
   Subnet Mask . . . . . . . . . . . : 255.255.255.0
   Lease Obtained. . . . . . . . . . : Tuesday, July 16, 2019 2:44:48 PM
   Lease Expires . . . . . . . . . . : Tuesday, July 16, 2019 3:44:48 PM
   Default Gateway . . . . . . . . . : 172.30.1.1
   DHCP Server . . . . . . . . . . . : 172.30.1.1
   DHCPv6 IAID . . . . . . . . . . . : 252333352
   DHCPv6 Client DUID. . . . . . . . : 00-01-00-01-20-A9-10-5C-0A-4D-28-5B-73-E8
   DNS Servers . . . . . . . . . . . : 172.30.0.2
   NetBIOS over Tcpip. . . . . . . . : Enabled

Tunnel adapter Local Area Connection* 3:

   Connection-specific DNS Suffix  . :
   Description . . . . . . . . . . . : Teredo Tunneling Pseudo-Interface
   Physical Address. . . . . . . . . : 00-00-00-00-00-00-00-E0
   DHCP Enabled. . . . . . . . . . . : No
   Autoconfiguration Enabled . . . . : Yes
   IPv6 Address. . . . . . . . . . . : 2001:0:34f1:8072:1c8d:304a:53e1:fef4(Preferred)
   Link-local IPv6 Address . . . . . : fe80::1c8d:304a:53e1:fef4%14(Preferred)
   Default Gateway . . . . . . . . . : ::
   DHCPv6 IAID . . . . . . . . . . . : 134217728
   DHCPv6 Client DUID. . . . . . . . : 00-01-00-01-20-A9-10-5C-0A-4D-28-5B-73-E8
   NetBIOS over Tcpip. . . . . . . . : Disabled

Tunnel adapter isatap.ec2.internal:

   Media State . . . . . . . . . . . : Media disconnected
   Connection-specific DNS Suffix  . : ec2.internal
   Description . . . . . . . . . . . : Microsoft ISATAP Adapter
   Physical Address. . . . . . . . . : 00-00-00-00-00-00-00-E0
   DHCP Enabled. . . . . . . . . . . : No
</code></pre>

<p>I would appreciate if anyone can point me in the right direction of why this is happening and whether it's a Windows or an AWS issue.</p>

<p><strong>UPDATE:</strong></p>

<p>When the DHCP update happens, I get these events in the Event Viewer, in this order (all within 10 seconds):</p>

<ol>
<li>lphlpsvc:  Isatap interface isatap.ec2.internal is no longer active.</li>
<li>Time-Service: NtpClient was unable to set a manual peer to use as a time source because of DNS resolution error on 'time.windows.com,0x8'. NtpClient will try again in 15 minutes and double the reattempt interval thereafter. The error was: No such host is known. (0x80072AF9)</li>
<li>Service Control Manager:  The Network Setup Service service entered the running state.</li>
<li>lphlpsvc:  Isatap interface isatap.ec2.internal with address fe80::5efe:172.30.1.11 has been brought up.</li>
<li>Time-Service: The time provider NtpClient is currently receiving valid time data from time.windows.com,0x8 (ntp.m|0x8|0.0.0.0:123->13.70.22.122:123).</li>
<li>Time-Service: The time service is now synchronizing the system time with the time source time.windows.com,0x8 (ntp.m|0x8|0.0.0.0:123->13.70.22.122:123).</li>
<li>Time-Service: The time provider NtpClient is currently receiving valid time data from time.windows.com,0x8 (ntp.m|0x8|0.0.0.0:123->13.70.22.122:123).</li>
</ol>
","<networking><amazon-ec2><windows-server-2016>","2019-07-16 15:36:05"
"763235","Staging architecture on AWS","<p>We are using AWS (EC2-classic) and I will migrate our servers to AWS VPC, we will use AuroraDB, too. </p>

<p>Staging ENV has 8servers (Linux Ubuntu 14.04), and on these servers are running RoR &amp; node.js applications.
I was thinking to consolidate these servers into one server.</p>

<p>Pros</p>

<ul>
<li>easier maintenance</li>
<li>we will be able to save money (cca $1,5k which I can theoretically spend in production ENV :) )</li>
</ul>

<p>Cons</p>

<ul>
<li>can be harder to troubleshoot of issues on servers</li>
<li>more apps on one server can theoretically cause more problems</li>
</ul>

<p>I looked on usage of performance for last couple weeks and we should be OK on based of performance with new server</p>

<p>It is staging ENV, so we dont need high availability for this. </p>

<p>what is your opinion or experience? </p>
","<linux><amazon-web-services><ruby-on-rails><node.js><consolidation>","2016-03-11 20:39:49"
"833607","strange behaviour while creating NFS datastore on load balanced servers","<blockquote>
<p>BACKGROUND</p>
</blockquote>
<ul>
<li>vSphere client version = 5.1.0</li>
<li>VMWare ESXi version = 5.1.0</li>
<li>Server OS = RHEL version 6.8</li>
<li>virtualip = virtual IP</li>
<li>server1 = ip address of server 1</li>
<li>server2 = ip address of server 2</li>
<li>lvsserver = ip address of lvs router</li>
<li>IPVSADM rules
<ul>
<li>ipvsadm -A -t virtualip:0 -s rr -p</li>
<li>ipvsadm -a -t virtualip:0 -r server2:0 -g</li>
<li>ipvsadm -a -t virtualip:0 -r server1:0 -g</li>
<li>ipvsadm -A -t virtualip:2049 -s rr</li>
<li>ipvsadm -a -t virtualip:2049 -r server1:2049 -g</li>
<li>ipvsadm -a -t virtualip:2049 -r server2:2049 -g</li>
</ul>
</li>
</ul>
<p><strong>/etc/exports</strong> on both servers (server1, server2)</p>
<pre><code>/vm0    *(rw,sync,no_root_squash)
</code></pre>
<p><strong>Both servers contains /vm0 folder</strong></p>
<blockquote>
<p>ACTUAL PROBLEM</p>
<p>scenario 1: on server1</p>
</blockquote>
<pre><code>root@server1 ~# mount -t nfs server2:/vm0 /vm0
root@server1 ~# service nfs restart
</code></pre>
<p>tried to create NFS datastore using following credentials</p>
<pre><code>server = virtualip
Folder = /vm0
Datastore name = vm0
</code></pre>
<p><strong>RESULT success!</strong></p>
<blockquote>
<p>scenario 2:</p>
</blockquote>
<pre><code>//ON SERVER 1
    root@server1 ~# umount vm0

//ON SERVER 2
    root@server2 ~# mount -t nfs server1:/vm0 /vm0
    root@server2 ~# service nfs restart
</code></pre>
<p>tried to create NFS datastore using following credentials</p>
<p>server = virtualip
Folder = /vmo
Datastore name = vm0
RESULT Failed!</p>
<p><strong>ERROR MESSAGE on ESXi server</strong></p>
<blockquote>
<p>Call &quot;HostDatastoreSystem.CreateNasDatastore&quot; for object &quot;ha-datastoresystem&quot; on ESXi (ESXi_ip) failed.
NFS mount (virtualip):/vm0 failed: The mount request was denied by the NFS server. Check that the export exists and that the client is permitted to mount it.</p>
</blockquote>
<p><strong>logs traced using tshark</strong></p>
<blockquote>
<p>33.916294107 (virtualip) -&gt; (ESXi_ip) MOUNT 100 V3 MNT Reply (Call In 41) Error:ERR_ACCESS</p>
</blockquote>
","<vmware-esxi><load-balancing><nfs><lvs><datastore>","2017-02-20 06:14:25"
"763308","What is the minimum requirements to receive mail for a specific domain?","<p>I think it is:</p>

<ul>
<li>A <code>MTA - Mail transfer Agent</code> software on the server</li>
<li>A <code>MX</code> record, a <code>DNS</code> record specifying which server receives mail for this domain.</li>
</ul>

<p>This would deliver mail to a file on the server for example:</p>

<p><code>/var/mail/my-domain.com/username</code></p>

<p>Are there any extra requirements to receive the email on another computer/device?</p>
","<domain-name-system><email><email-server><mx-record>","2016-03-12 10:17:50"
"763383","Configure Cisco 2801 that has NAT overload to allow access to WAN from LAN","<p>I have a server behind my Cisco (IOS, not ASA) that is running NAT overload (PAT?) with port forwarding for specific services (http/https) to a statically IP'ed server that is on the same subnet as my normal internal LAN. How can I configure the router to allow my local users to access the public interface so that they can get to the web server. (Split DNS is not an option for me at this time)</p>

<p>Details:
Router Public IP: 1.2.3.4/30 (DHCP Assigned)
Router Private LAN IP : 10.10.10.1/24
Server behind firewall IP: 10.10.10.25/24</p>

<p>What I am trying to do is allow my internal LAN to access a website that is locally hosted:</p>

<p>www.example.com 1.2.3.4</p>

<p>When I try to access www.example.com from behind the router, it fails, since the router will not allow the traffic to pass out and then back in.</p>

<p>Let me know if there are any further details that you need</p>
","<cisco>","2016-03-12 22:35:52"
"763448","How to find the missing inodes?","<p><code>df -i</code> reports</p>

<pre><code>Filesystem     Inodes  IUsed IFree IUse% Mounted on
/dev/simfs     300000 250697 49303   84% /
</code></pre>

<p>so around 250K inodes in use.</p>

<p><code>du -shx --inodes /</code> reports 70K</p>

<p>to be sure no inodes was hidden behind mounts, i tested</p>

<pre><code>mkdir /mnt/test/
mount --bind / /mnt/test/
du -shx --inodes /mnt/test/
</code></pre>

<p>That also report 70K.</p>

<p>So where is the rest of them, missing around 180K (250%).</p>

<p>What more tests can I do? Do I need to ask the OpenVZ-support? if so, what to ask?</p>

<p>in case its useful, <code>mount</code> reports</p>

<blockquote>
  <p>/vz/private/1210881 on / type simfs (rw,relatime,usrquota,grpquota)</p>
</blockquote>
","<openvz><linode><df><du>","2016-03-13 13:07:16"
"763541","dnsmasq failed to restart","<p>Whenever I want to restart <code>dnsmasq</code> service, I receive this error</p>

<pre><code>root@ipm:/tftpboot# /etc/init.d/dnsmasq restart
[....] Restarting dnsmasq (via systemctl): dnsmasq.serviceJob for
dnsmasq.service failed because the control process exited with error code.
See ""systemctl status dnsmasq.service"" and ""journalctl -xe"" for details.
 failed!
</code></pre>

<p>There is no useful information in those commands as stated in the error message.</p>

<pre><code># systemctl status dnsmasq.service
● dnsmasq.service - dnsmasq - A lightweight DHCP and caching DNS server
   Loaded: loaded (/lib/systemd/system/dnsmasq.service; enabled; vendor preset: enabled)
  Drop-In: /run/systemd/generator/dnsmasq.service.d             └─50-dnsmasq-$named.conf, 50-insserv.conf-$named.con 
   Active: failed (Result: exit-code) since  2016-03-4 11:04:42 IRST; 3min 53s ago
  Process: 18806 ExecStartPre=/usr/sbin/dnsmasq --test (code=exited, status=1/FAILURE)

 14 11:04:42  systemd[1]: Starting dnsmasq - A lightweight DHCP .....
 14 11:04:42  dnsmasq[18806]: dnsmasq: bad dhcp-range at line 4 o...f
 14 11:04:42  systemd[1]: dnsmasq.service: Control process exite...=1
 14 11:04:42  systemd[1]: Failed to start dnsmasq - A lightweigh...r.
 14 11:04:42  systemd[1]: dnsmasq.service: Unit entered failed state.
 14 11:04:42  systemd[1]: dnsmasq.service: Failed with result 'e...'.
Hint: Some lines were ellipsized, use -l to show in full.
</code></pre>

<p>The content of the <code>dnsmasq.conf</code> is </p>

<pre><code> # cat /etc/dnsmasq.conf
 interface=enp3s0
 domain=hpclab
 expand-hosts
 dhcp-range=192.168.1.1,192.168.1.20,static
 dhcp-option=42,0.0.0.0
 dhcp-boot=pxelinux.0
 enable-tftp
 tftp-root=/tftpboot
 dhcp-host=00:e0:81:c5:19:64,ws01,192.168.1.1
</code></pre>

<p>So, line 4 is <code>dhcp-range=192.168.1.1,192.168.1.20,static</code>. I used the same content with ubuntu-12.04. </p>

<p>UPDATE:</p>

<p>Anyway, the correct syntax is <code>dhcp-range=192.168.1.0,static</code>. I don't know how that was working with 12.04!</p>
","<dnsmasq><ubuntu-15.10>","2016-03-14 07:41:29"
"833915","VPS vs Real Hardware performance correlation","<p>Assuming I have application running in a VPS and consuming 50% of assigned CPU resource. Will it correct to assume that on real hardware with same properties CPU usage will not be more than 50% running same application ?
Thanks.</p>
","<networking><performance><cloud><site-to-site-vpn><virtual-appliances>","2017-02-21 12:51:27"
"763830","CentOS suspicious process","<p>One of my web server is running on CentOS and I have been spotted that one of the processes is taking almost all swap memory.
Mention process is:<br/></p>

<blockquote>
  <p>/opt/opsware/agent/bin/pyton
  /opt/opsware/agent/pylibs/shadowbot/deamonbot.pyc --conf
  /etc/opt/opsware/agent/agent.args</p>
</blockquote>

<p><br/>
I've tried to google it but I can't find anything.<br/>
Process is not using CPU only memory.<br/>
Do you know that I can kill it without affecting any of web services?<br/></p>
","<linux><centos><process>","2016-03-15 12:23:24"
"976021","SSH always fails to start and status is activating","<p>I am running open-ssh 7.8p1 on Redhat 7.0. I downloaded the package and executed the commands in order to install the package:</p>

<p>./configure --prefix=/usr --sysconfdir=/etc/ssh --with-privsep-path=/var/lib/sshd  --with-pam --with-xauth=/usr/bin/xauth</p>

<p>make</p>

<p>sudo make install</p>

<p>sshd/config: </p>

<pre><code>#AddressFamily any
#ListenAddress 0.0.0.0
#ListenAddress ::

HostKey /etc/ssh/ssh_host_rsa_key
#HostKey /etc/ssh/ssh_host_dsa_key
HostKey /etc/ssh/ssh_host_ecdsa_key
HostKey /etc/ssh/ssh_host_ed25519_key

# Ciphers and keying
#RekeyLimit default none

# Logging
#SyslogFacility AUTH
SyslogFacility AUTHPRIV
#LogLevel INFO

# Authentication:

#LoginGraceTime 2m
#PermitRootLogin yes
#StrictModes yes
#MaxAuthTries 6
#MaxSessions 10

#PubkeyAuthentication yes

# The default is to check both .ssh/authorized_keys and .ssh/authorized_keys2
# but this is overridden so installations will only check .ssh/authorized_keys
AuthorizedKeysFile      .ssh/authorized_keys

#AuthorizedPrincipalsFile none

#AuthorizedKeysCommand none
#AuthorizedKeysCommandUser nobody

# For this to work you will also need host keys in /etc/ssh/ssh_known_hosts
#HostbasedAuthentication no
# Change to yes if you don't trust ~/.ssh/known_hosts for
# HostbasedAuthentication
#IgnoreUserKnownHosts no
# Don't read the user's ~/.rhosts and ~/.shosts files
#IgnoreRhosts yes

# To disable tunneled clear text passwords, change to no here!
#PasswordAuthentication yes
#PermitEmptyPasswords no
PasswordAuthentication yes

# Change to no to disable s/key passwords
#ChallengeResponseAuthentication yes
ChallengeResponseAuthentication no

# Kerberos options
#KerberosAuthentication no
#KerberosOrLocalPasswd yes
#KerberosTicketCleanup yes
#KerberosGetAFSToken no
#KerberosUseKuserok yes

# GSSAPI options
#GSSAPIAuthentication yes
#GSSAPICleanupCredentials no
#GSSAPIStrictAcceptorCheck yes
#GSSAPIKeyExchange no
#GSSAPIEnablek5users no

# Set this to 'yes' to enable PAM authentication, account processing,
# and session processing. If this is enabled, PAM authentication will
# be allowed through the ChallengeResponseAuthentication and
# PasswordAuthentication.  Depending on your PAM configuration,
# PAM authentication via ChallengeResponseAuthentication may bypass
# the setting of ""PermitRootLogin without-password"".
# If you just want the PAM account and session checks to run without
# PAM authentication, then enable this but set PasswordAuthentication
# and ChallengeResponseAuthentication to 'no'.
# WARNING: 'UsePAM no' is not supported in Red Hat Enterprise Linux and may cause several
# problems.
UsePAM yes

#AllowAgentForwarding yes
#AllowTcpForwarding yes
#GatewayPorts no
X11Forwarding yes
#X11DisplayOffset 10
#X11UseLocalhost yes
#PermitTTY yes
#PrintMotd yes
#PrintLastLog yes
#TCPKeepAlive yes
#UseLogin no
#UsePrivilegeSeparation sandbox
#PermitUserEnvironment no
#Compression delayed
ClientAliveInterval 20m
#ClientAliveCountMax 3
#ShowPatchLevel no
#UseDNS yes
#PidFile /var/run/sshd.pid
#MaxStartups 10:30:100
#PermitTunnel no
#ChrootDirectory none
#VersionAddendum none

# no default banner path
#Banner none

# Accept locale-related environment variables
AcceptEnv LANG LC_CTYPE LC_NUMERIC LC_TIME LC_COLLATE LC_MONETARY LC_MESSAGES
AcceptEnv LC_PAPER LC_NAME LC_ADDRESS LC_TELEPHONE LC_MEASUREMENT
AcceptEnv LC_IDENTIFICATION LC_ALL LANGUAGE
AcceptEnv XMODIFIERS

# override default of no subsystems
Subsystem       sftp    /usr/libexec/openssh/sftp-server

# Example of overriding settings on a per-user basis
#Match User anoncvs
#       X11Forwarding no
#       AllowTcpForwarding no
#       PermitTTY no
#       ForceCommand cvs server

</code></pre>

<p><a href=""https://i.sstatic.net/k7NYk.png"" rel=""nofollow noreferrer"">sshd status</a></p>

<p>ssh debug mode result:</p>

<pre><code>debug2: load_server_config: done config len = 669
debug2: parse_server_config: config /etc/ssh/sshd_config len 669
debug3: /etc/ssh/sshd_config:22 setting HostKey /etc/ssh/ssh_host_rsa_key
debug3: /etc/ssh/sshd_config:24 setting HostKey /etc/ssh/ssh_host_ecdsa_key
debug3: /etc/ssh/sshd_config:25 setting HostKey /etc/ssh/ssh_host_ed25519_key
debug3: /etc/ssh/sshd_config:32 setting SyslogFacility AUTHPRIV
debug3: /etc/ssh/sshd_config:47 setting AuthorizedKeysFile .ssh/authorized_keys
debug3: /etc/ssh/sshd_config:65 setting PasswordAuthentication yes
debug3: /etc/ssh/sshd_config:69 setting ChallengeResponseAuthentication no
debug3: /etc/ssh/sshd_config:96 setting UsePAM yes
debug3: /etc/ssh/sshd_config:101 setting X11Forwarding yes
debug3: /etc/ssh/sshd_config:112 setting ClientAliveInterval 20m
debug3: /etc/ssh/sshd_config:126 setting AcceptEnv LANG LC_CTYPE LC_NUMERIC LC_TIME LC_COLLATE LC_MONETARY LC_MESSAGES
debug3: /etc/ssh/sshd_config:127 setting AcceptEnv LC_PAPER LC_NAME LC_ADDRESS LC_TELEPHONE LC_MEASUREMENT
debug3: /etc/ssh/sshd_config:128 setting AcceptEnv LC_IDENTIFICATION LC_ALL LANGUAGE
debug3: /etc/ssh/sshd_config:129 setting AcceptEnv XMODIFIERS
debug3: /etc/ssh/sshd_config:132 setting Subsystem sftp /usr/libexec/openssh/sftp-server
debug1: sshd version OpenSSH_7.8, OpenSSL 1.0.2k-fips  26 Jan 2017
debug1: private host key #0: ssh-rsa SHA256:Gd4H0gToGhergccDgoCrmH03UPAfWcUd1NKusBGlls4
debug1: private host key #1: ecdsa-sha2-nistp256 SHA256:NxfjZIJ7oRPjfsBJKeSw/N3kf4iZMedZFnjePbLbyoc
debug1: private host key #2: ssh-ed25519 SHA256:CWwG5eZVSaU3hSizraB1blaeYfws4KI6NOWn1I8KC9Y
debug1: rexec_argv[0]='/usr/sbin/sshd'
debug1: rexec_argv[1]='-ddd'
debug3: oom_adjust_setup
debug1: Set /proc/self/oom_score_adj from 0 to -1000
debug2: fd 3 setting O_NONBLOCK
debug1: Bind to port 22 on 0.0.0.0.
Server listening on 0.0.0.0 port 22.
debug2: fd 4 setting O_NONBLOCK
debug3: sock_set_v6only: set socket 4 IPV6_V6ONLY
debug1: Bind to port 22 on ::.
Server listening on :: port 22.
</code></pre>

<p>I am trying to figure the problem and I tried to run sshd in debug mode and it didn't show any error. 
After 1 min the sshd service fails then it auto restarts.</p>

<p>I am able to login to the server, but when it fails and before starting again, I get Connection refused.</p>

<p>So If anyone please can suggest a solution for this issue?</p>
","<ssh><redhat><systemd>","2019-07-20 11:55:26"
"834364","LVS router stopped working after reboot","<p>I am using RHEL 6.8 for load balancing using LVS direct routing. For some reason I had to restart my LVS router.
after rebooting, LVS service was not working</p>
<p>I have done following things, still it is not working</p>
<pre><code>/sbin/chkconfig --level 0123456 iptables on
/sbin/chkconfig --level 0123456 piranha-gui on
/sbin/chkconfig --level 0123456 pulse on
/sbin/chkconfig --level 0123456 sshd on
service httpd restart
service piranha-gui restart
service pulse restart
ipvsadm -A -t 10.209.104.60:80 -s rr
ipvsadm -a -t 10.209.104.60:80 -r 10.209.104.7:80 -g
ipvsadm -a -t 10.209.104.60:80 -r 10.209.104.5:80 -g
</code></pre>
<p>ifconfig command is <strong>not showing</strong>  VIP as a alias to eth0 (which I think, should be automatically configured from /etc/sysconfig/ha/lvs.cf file)</p>
<p>LOG file entries</p>
<blockquote>
<p>lvsprimary pulse[3143]: STARTING PULSE AS MASTER</p>
<p>lvsprimary pulse[3143]: partner dead: activating lvs</p>
<p>lvsprimary lvsd[3147]: starting virtual service loadbalancer active:80</p>
<p>lvsprimary kernel: IPVS: __ip_vs_del_service: enter</p>
<p>lvsprimary kernel:IPVS: __ip_vs_del_service: enter</p>
<p>lvsprimary lvsd[3147]: create_monitor for loadbalancer/server1 running as pid 3151</p>
<p>lvsprimary lvsd[3147]: create_monitor for loadbalancer/server2 running as pid 3152</p>
<p>lvsprimary nanny[3152]: starting LVS client monitor for 10.###.###.##:80 -&gt; 10.2##.###.7:0</p>
<p>lvsprimary nanny[3151]: starting LVS client monitor for 10.###.###.##:80 -&gt; 10.2##.###.5:0</p>
<p>lvsprimary pulse[3154]: gratuitous lvs arps finished</p>
</blockquote>
","<load-balancing><rhel6><lvs><piranha>","2017-02-23 06:44:02"
"834372","i install dns in centos machine but it not resolve ip to name and name to ip","<p>I have some problem regarding DNS. I install DNS in CentOS 7 machine and my CentOS machine ip is 192.168.0.155 . If I run nslookup FQDN (centos7.unixmen.local) name then it resolves the IP, which means my forward zone is running, and my output is</p>

<pre><code>nslookup centos7.unixmen.local

Server:     192.168.0.155
Address:    192.168.0.155#53

Name:   centos7.unixmen.local

Address: 192.168.0.155
</code></pre>

<p>but if i run</p>

<pre><code>nslookup 192.168.0.155

Server:     192.168.0.155
Address:    192.168.0.155#53

** server can't find 155.0.168.192.in-addr.arpa.: NXDOMAIN
</code></pre>

<p>that means reverse zone not resolve to IP to name. </p>

<h2>My /etc/named.conf  file</h2>

<pre><code>options {

        listen-on port 53 {
                127.0.0.1;
                192.168.0.155;
                };
        listen-on-v6 port 53 { ::1; };
        directory       ""/var/named"";
        dump-file       ""/var/named/data/cache_dump.db"";
        statistics-file ""/var/named/data/named_stats.txt"";
        memstatistics-file ""/var/named/data/named_mem_stats.txt"";
        allow-query     { any; };

*/
        recursion yes;

        dnssec-enable yes;
        dnssec-validation yes;

        /* Path to ISC DLV key */
        bindkeys-file ""/etc/named.iscdlv.key"";

        managed-keys-directory ""/var/named/dynamic"";

        pid-file ""/run/named/named.pid"";
        session-keyfile ""/run/named/session.key"";
        forwarders {
                8.8.8.8;
                8.8.8.4;
                };
        forward first;
};


logging {

        channel default_debug {
                file ""data/named.run"";
                severity dynamic;
        };

};

zone ""."" IN {
        type hint;
        file ""named.ca"";
};


zone ""unixmen.local"" IN {

      type master;
      file ""forward.unixmen"";
      allow-update { none;};
};

zone ""0.168.192.in-add.arpa"" IN {

      type master;
      file ""reverse.unixmen"";
      allow-update { none; };
};

include ""/etc/named.rfc1912.zones"";

include ""/etc/named.root.key
</code></pre>

<h2>My Reverse zone configuration file: /var/named/reverse.unixmen</h2>

<pre><code>$TTL 86400

@       IN SOA  centos7.unixmen.local. root.unixmen.local.(

                                        2011071001      ; serial
                                        3600            ; refresh
                                        1800            ; retry
                                        604800          ; expire
                                        86400           ; minimum TTL
)

@                IN     NS      centos7.unixmen.local.

@                IN    PTR      unixmen.local.

@                IN      A      192.168.0.155

@                IN     PTR     192.168.0.155

155              IN     PTR     centos7.unixmen.local.
</code></pre>

<h2>My forward zone file at /var/named/forward.unixmen</h2>

<pre><code>$TTL 86400

@       IN SOA  centos7.unixmen.local. root.unixmen.local. (

                                        2011071001      ; serial
                                              3600      ; refresh
                                              1800      ; retry
                                             604800     ; expire
                                             86400      ; minimum TTL
)

@                  IN        NS   centos7.unixmen.local.

@                  IN        A       192.168.0.155

centos7            IN        A       192.168.0.155
</code></pre>

<p>In my CentOS 7 machine only one ethernet port and he direct connected to router using ethernet cable and all client window system connected to switch
and my</p>

<ul>
<li>ip is 192.168.0.155</li>
<li>gateway 192.168.0.1</li>
<li>dns-192.168.0.1 </li>
</ul>

<p>Recently if I set my own IP 192.168.0.155 as a DNS IP in CentOS 7 machine  then internet browsing working and also forward zone means name to IP is resolving but IP to name is not resolving.</p>

<p>Please help me resolve my problem.</p>
","<linux><domain-name-system><centos><linux-networking><dns-zone>","2017-02-23 07:34:40"
"764160","DNS issues resolving specific site","<p>We seem to be having some DNS issues.  At certain times, we loose they ability to get to two particular websites. The problem only seems to be with these two websites that are both hosted by the same company.  At the time we are 
experiencing the issue, NSLOOKUP can not find the sites.  We can access the site via the IP.  Also, if I change the DNS on the local machine to Google's, it works.  That is why I think it may be a problem with our DNS servers.  When I clear the cache on the DNS servers, everything comes back.  NSLOOKUP can find the sites and our users can access them.  After awhile, they go out again.  Can anyone please give me some ideas as to how to find the error.  I really have no idea where to go from here.  </p>

<p>Edit:
We are running Windows Server2008 R2.  There are no error messages or reports from the server.  Users just can't get to these two sites.  I was looking for some ideas of what I could research to try and find out why this is happening.  The name of the site is my.providersportal.com.</p>

<p>Edit 2:
I think I may have confused you.  This is not my domain.  Our users are just trying to access it.  Do you have any idea why clearing the cache would fix the issue if it is an issue with the domains NS?  Thanks for the help.</p>
","<domain-name-system><internal-dns>","2016-03-16 16:39:53"
"901635","MySqld Service wont start on Redhat7 after a yum update","<p>I'm having a problem with Redhat7 after a yum update.  Various services wont start and are giving permissions errors.</p>

<p>Similar problem to 
<a href=""https://serverfault.com/questions/790327/cant-start-any-service-after-installing-iredmail-on-centos"">Can&#39;t start any service after installing iRedMail on CentOS</a>
but different OS, so maybe different problem.</p>

<p>Googling the error message brought me to the link above, so I tried some of the same fixes, see output of each below.  Please help!</p>

<hr>

<pre><code>service mysqld restart
</code></pre>

<p>Redirecting to /bin/systemctl restart mysqld.service
Error getting authority: Error initializing authority: Exhausted all available authentication mechanisms (tried: EXTERNAL, DBUS_COOKIE_SHA1, ANONYMOUS) (available: EXTERNAL, DBUS_COOKIE_SHA1, ANONYMOUS) (g-io-error-quark, 0)
Job for mysqld.service failed because the control process exited with error code. See ""systemctl status mysqld.service"" and ""journalctl -xe"" for details.</p>

<hr>

<p>the yum update that I performed yesterday included these packages:</p>

<pre><code>grep ""Mar 13.*"" /var/log/yum.log  | awk '{ print $5 }' | grep policy
</code></pre>

<ul>
<li>selinux-policy-3.13.1-166.el7_4.9.noarch</li>
<li>selinux-policy-targeted-3.13.1-166.el7_4.9.noarch</li>
</ul>

<hr>

<p>Tried reinstalling, but no change, also tried reinstalling the following, as well as the selinux policies above. (no joy)</p>

<ul>
<li>rpm -qi dbus </li>
<li>rpm -qi polkit</li>
</ul>

<hr>

<pre><code>systemctl status dbus
</code></pre>

<p>â dbus.service - D-Bus System Message Bus
   Loaded: loaded (/usr/lib/systemd/system/dbus.service; static; vendor preset: disabled)
   Active: active (running) since Wed 2018-03-14 15:12:45 UTC; 47min ago
 Main PID: 677 (dbus-daemon)
   CGroup: /system.slice/dbus.service
           ââ677 /bin/dbus-daemon --system --address=systemd: --nofork --nopidfile --systemd-...</p>

<p>Mar 14 15:12:45 systemd[1]: Started D-Bus System Message Bus.<br>
Mar 14 15:12:45 systemd[1]: Starting D-Bus System Message Bus...<br>
Mar 14 15:29:10 dbus-daemon[677]: dbus[677]: [system] Unable to reload config...ed<br>
Mar 14 15:29:10 dbus-daemon[677]: Unable to reload configuration:  Failed to o...ed<br>
Mar 14 15:29:10 dbus-daemon[677]: dbus[677]: [system] Unable to reload config...ed  </p>

<pre><code>journalctl -u dbus
</code></pre>

<p>Mar 14 15:12:45 systemd[1]: Started D-Bus System Message Bus.<br>
Mar 14 15:12:45 systemd[1]: Starting D-Bus System Message Bus...<br>
Mar 14 15:29:10 dbus-daemon[677]: dbus[677]: [system] Unable to reload configuration: Failed to open ""/etc/dbus-1/system.conf"": Permission denied<br>
Mar 14 15:29:10 dbus-daemon[677]: Unable to reload configuration: Failed to open ""/etc/dbus-1/system.conf"": Permission denied    </p>

<pre><code>ls -l /etc/dbus-1/system.conf
</code></pre>

<p>-rw-r--r-- 1 root root 2991 Sep 12  2016 /etc/dbus-1/system.conf</p>

<hr>

<pre><code>systemctl start polkit
</code></pre>

<p>Error getting authority: Error initializing authority: Exhausted all available authentication mechanisms (tried: EXTERNAL, DBUS_COOKIE_SHA1, ANONYMOUS) (available: EXTERNAL, DBUS_COOKIE_SHA1, ANONYMOUS) (g-io-error-quark, 0)
Job for polkit.service failed because a timeout was exceeded. See ""systemctl status polkit.service"" and ""journalctl -xe"" for details.</p>

<hr>

<pre><code>namei -l /etc/dbus-1/system.conf  
</code></pre>

<p>f: /etc/dbus-1/system.conf<br>
drwxr-xr-x root root /<br>
drw------- root root etc<br>
drwxr-xr-x root root dbus-1<br>
-rwxr-xr-x root root system.conf</p>

<hr>

<p>Additionally, here's the full list of things that got upgraded...
I'm thinking I should restore from backup...</p>

<ul>
<li>1dhclient-4.2.5-58.el7_4.3.x86_64  </li>
<li>1dhcp-common-4.2.5-58.el7_4.3.x86_64  </li>
<li>1dhcp-libs-4.2.5-58.el7_4.3.x86_64  </li>
<li>3bind-libs-9.9.4-51.el7_4.2.x86_64  </li>
<li>3bind-libs-lite-9.9.4-51.el7_4.2.x86_64  </li>
<li>3bind-license-9.9.4-51.el7_4.2.noarch  </li>
<li>3bind-utils-9.9.4-51.el7_4.2.x86_64  </li>
<li>accountsservice-0.6.45-3.el7_4.1.x86_64  </li>
<li>accountsservice-libs-0.6.45-3.el7_4.1.x86_64  </li>
<li>apr-1.4.8-3.el7_4.1.x86_64  </li>
<li>at-3.1.13-22.el7_4.2.x86_64  </li>
<li>binutils-2.25.1-32.base.el7_4.2.x86_64  </li>
<li>colordiff-1.0.13-2.el7.noarch  </li>
<li>copy-jdk-configs-2.2-5.el7_4.noarch  </li>
<li>cryptsetup-1.7.4-3.el7_4.1.x86_64  </li>
<li>cryptsetup-libs-1.7.4-3.el7_4.1.x86_64  </li>
<li>cryptsetup-python-1.7.4-3.el7_4.1.x86_64  </li>
<li>curl-7.29.0-42.el7_4.1.x86_64  </li>
<li>device-mapper-multipath-0.4.9-111.el7_4.2.x86_64  </li>
<li>device-mapper-multipath-libs-0.4.9-111.el7_4.2.x86_64  </li>
<li>device-mapper-persistent-data-0.7.0-0.1.rc6.el7_4.1.x86_64  </li>
<li>dleyna-server-0.5.0-2.el7_4.x86_64  </li>
<li>dracut-033-502.el7_4.1.x86_64  </li>
<li>dracut-config-rescue-033-502.el7_4.1.x86_64  </li>
<li>dracut-network-033-502.el7_4.1.x86_64  </li>
<li>epel-release-7-11.noarch  </li>
<li>firefox-52.6.0-1.el7_4.x86_64  </li>
<li>gdb-7.6.1-100.el7_4.1.x86_64  </li>
<li>gdm-3.22.3-13.el7_4.x86_64  </li>
<li>glibc-2.17-196.el7_4.2.i686  </li>
<li>glibc-2.17-196.el7_4.2.x86_64  </li>
<li>glibc-common-2.17-196.el7_4.2.x86_64  </li>
<li>gtk3-3.22.10-5.el7_4.x86_64  </li>
<li>gtk3-immodule-xim-3.22.10-5.el7_4.x86_64  </li>
<li>gtk-update-icon-cache-3.22.10-5.el7_4.x86_64  </li>
<li>initscripts-9.49.39-1.el7_4.1.x86_64  </li>
<li>iptables-1.4.21-18.3.el7_4.x86_64  </li>
<li>iptables-services-1.4.21-18.3.el7_4.x86_64  </li>
<li>iwl1000-firmware-39.31.5.1-58.el7_4.noarch  </li>
<li>iwl100-firmware-39.31.5.1-58.el7_4.noarch  </li>
<li>iwl105-firmware-18.168.6.1-58.el7_4.noarch  </li>
<li>iwl135-firmware-18.168.6.1-58.el7_4.noarch  </li>
<li>iwl2000-firmware-18.168.6.1-58.el7_4.noarch  </li>
<li>iwl2030-firmware-18.168.6.1-58.el7_4.noarch  </li>
<li>iwl3160-firmware-22.0.7.0-58.el7_4.noarch  </li>
<li>iwl3945-firmware-15.32.2.9-58.el7_4.noarch  </li>
<li>iwl4965-firmware-228.61.2.24-58.el7_4.noarch  </li>
<li>iwl5000-firmware-8.83.5.1_1-58.el7_4.noarch  </li>
<li>iwl5150-firmware-8.24.2.2-58.el7_4.noarch  </li>
<li>iwl6000-firmware-9.221.4.1-58.el7_4.noarch  </li>
<li>iwl6000g2a-firmware-17.168.5.3-58.el7_4.noarch  </li>
<li>iwl6000g2b-firmware-17.168.5.2-58.el7_4.noarch  </li>
<li>iwl6050-firmware-41.28.5.1-58.el7_4.noarch  </li>
<li>iwl7260-firmware-22.0.7.0-58.el7_4.noarch  </li>
<li>java-1.7.0-openjdk-1.7.0.171-2.6.13.0.el7_4.x86_64  </li>
<li>java-1.7.0-openjdk-headless-1.7.0.171-2.6.13.0.el7_4.x86_64  </li>
<li>java-1.8.0-openjdk-1.8.0.161-0.b14.el7_4.x86_64  </li>
<li>java-1.8.0-openjdk-headless-1.8.0.161-0.b14.el7_4.x86_64  </li>
<li>kernel-3.10.0-693.21.1.el7.x86_64  </li>
<li>kernel-tools-3.10.0-693.21.1.el7.x86_64  </li>
<li>kernel-tools-libs-3.10.0-693.21.1.el7.x86_64  </li>
<li>kmod-20-15.el7_4.7.x86_64  </li>
<li>kmod-libs-20-15.el7_4.7.x86_64  </li>
<li>kpartx-0.4.9-111.el7_4.2.x86_64  </li>
<li>kpatch-0.4.0-2.el7_4.noarch  </li>
<li>libblkid-2.23.2-43.el7_4.2.x86_64  </li>
<li>libcurl-7.29.0-42.el7_4.1.x86_64  </li>
<li>libdb-5.3.21-21.el7_4.i686  </li>
<li>libdb-5.3.21-21.el7_4.x86_64  </li>
<li>libdb-utils-5.3.21-21.el7_4.x86_64  </li>
<li>libgcc-4.8.5-16.el7_4.2.i686  </li>
<li>libgcc-4.8.5-16.el7_4.2.x86_64  </li>
<li>libgomp-4.8.5-16.el7_4.2.x86_64  </li>
<li>libgudev1-219-42.el7_4.10.x86_64  </li>
<li>liblouis-2.5.2-12.el7_4.x86_64  </li>
<li>liblouis-python-2.5.2-12.el7_4.noarch  </li>
<li>libmount-2.23.2-43.el7_4.2.x86_64  </li>
<li>libpciaccess-0.13.4-3.1.el7_4.x86_64  </li>
<li>libreswan-3.20-5.el7_4.x86_64  </li>
<li>libsmbclient-4.6.2-12.el7_4.x86_64  </li>
<li>libsss_idmap-1.15.2-50.el7_4.11.x86_64  </li>
<li>libsss_nss_idmap-1.15.2-50.el7_4.11.x86_64  </li>
<li>libstdc++-4.8.5-16.el7_4.2.i686  </li>
<li>libstdc++-4.8.5-16.el7_4.2.x86_64  </li>
<li>libstoragemgmt-1.4.0-5.el7_4.x86_64  </li>
<li>libstoragemgmt-python-1.4.0-5.el7_4.noarch  </li>
<li>libstoragemgmt-python-clibs-1.4.0-5.el7_4.x86_64  </li>
<li>libteam-1.25-6.el7_4.3.x86_64  </li>
<li>libtevent-0.9.31-2.el7_4.x86_64  </li>
<li>libuuid-2.23.2-43.el7_4.2.x86_64  </li>
<li>libvirt-client-3.2.0-14.el7_4.9.x86_64  </li>
<li>libvirt-daemon-3.2.0-14.el7_4.9.x86_64  </li>
<li>libvirt-daemon-config-network-3.2.0-14.el7_4.9.x86_64  </li>
<li>libvirt-daemon-driver-interface-3.2.0-14.el7_4.9.x86_64  </li>
<li>libvirt-daemon-driver-network-3.2.0-14.el7_4.9.x86_64  </li>
<li>libvirt-daemon-driver-nodedev-3.2.0-14.el7_4.9.x86_64  </li>
<li>libvirt-daemon-driver-nwfilter-3.2.0-14.el7_4.9.x86_64  </li>
<li>libvirt-daemon-driver-qemu-3.2.0-14.el7_4.9.x86_64  </li>
<li>libvirt-daemon-driver-secret-3.2.0-14.el7_4.9.x86_64  </li>
<li>libvirt-daemon-driver-storage-3.2.0-14.el7_4.9.x86_64  </li>
<li>libvirt-daemon-driver-storage-core-3.2.0-14.el7_4.9.x86_64  </li>
<li>libvirt-daemon-driver-storage-disk-3.2.0-14.el7_4.9.x86_64  </li>
<li>libvirt-daemon-driver-storage-gluster-3.2.0-14.el7_4.9.x86_64  </li>
<li>libvirt-daemon-driver-storage-iscsi-3.2.0-14.el7_4.9.x86_64  </li>
<li>libvirt-daemon-driver-storage-logical-3.2.0-14.el7_4.9.x86_64  </li>
<li>libvirt-daemon-driver-storage-mpath-3.2.0-14.el7_4.9.x86_64  </li>
<li>libvirt-daemon-driver-storage-rbd-3.2.0-14.el7_4.9.x86_64  </li>
<li>libvirt-daemon-driver-storage-scsi-3.2.0-14.el7_4.9.x86_64  </li>
<li>libvirt-daemon-kvm-3.2.0-14.el7_4.9.x86_64  </li>
<li>libvirt-libs-3.2.0-14.el7_4.9.x86_64  </li>
<li>libwbclient-4.6.2-12.el7_4.x86_64  </li>
<li>linux-firmware-20170606-58.gitc990aae.el7_4.noarch  </li>
<li>microcode_ctl-2.1-22.5.el7_4.x86_64  </li>
<li>mutter-3.22.3-12.el7_4.x86_64  </li>
<li>mysql-community-client-5.7.21-1.el7.x86_64  </li>
<li>mysql-community-common-5.7.21-1.el7.x86_64  </li>
<li>mysql-community-libs-5.7.21-1.el7.x86_64  </li>
<li>mysql-community-libs-compat-5.7.21-1.el7.x86_64  </li>
<li>mysql-community-server-5.7.21-1.el7.x86_64  </li>
<li>nautilus-3.22.3-4.el7_4.x86_64  </li>
<li>nautilus-extensions-3.22.3-4.el7_4.x86_64  </li>
<li>net-snmp-libs-5.7.2-28.el7_4.1.x86_64  </li>
<li>nfs-utils-1.3.0-0.48.el7_4.2.x86_64  </li>
<li>python-dmidecode-3.12.2-1.1.el7.x86_64  </li>
<li>python-gobject-3.22.0-1.el7_4.1.x86_64  </li>
<li>python-gobject-base-3.22.0-1.el7_4.1.x86_64  </li>
<li>python-perf-3.10.0-693.21.1.el7.x86_64  </li>
<li>qemu-img-1.5.3-141.el7_4.6.x86_64  </li>
<li>qemu-kvm-1.5.3-141.el7_4.6.x86_64  </li>
<li>qemu-kvm-common-1.5.3-141.el7_4.6.x86_64  </li>
<li>rhnsd-5.0.13-7.3.el7_4.x86_64  </li>
<li>samba-client-libs-4.6.2-12.el7_4.x86_64  </li>
<li>samba-common-4.6.2-12.el7_4.noarch  </li>
<li>samba-common-libs-4.6.2-12.el7_4.x86_64  </li>
<li>samba-common-tools-4.6.2-12.el7_4.x86_64  </li>
<li>samba-libs-4.6.2-12.el7_4.x86_64  </li>
<li>selinux-policy-3.13.1-166.el7_4.9.noarch  </li>
<li>selinux-policy-targeted-3.13.1-166.el7_4.9.noarch  </li>
<li>sos-3.4-13.el7_4.noarch  </li>
<li>spice-glib-0.33-6.el7_4.1.x86_64  </li>
<li>spice-gtk3-0.33-6.el7_4.1.x86_64  </li>
<li>sssd-client-1.15.2-50.el7_4.11.x86_64  </li>
<li>swiagent-1.8.0.551-835342.x86_64  </li>
<li>systemd-219-42.el7_4.10.x86_64  </li>
<li>systemd-libs-219-42.el7_4.10.i686  </li>
<li>systemd-libs-219-42.el7_4.10.x86_64  </li>
<li>systemd-python-219-42.el7_4.10.x86_64  </li>
<li>systemd-sysv-219-42.el7_4.10.x86_64  </li>
<li>systemtap-runtime-3.1-5.el7_4.x86_64  </li>
<li>teamd-1.25-6.el7_4.3.x86_64  </li>
<li>tigervnc-license-1.8.0-2.el7_4.noarch  </li>
<li>tigervnc-server-minimal-1.8.0-2.el7_4.x86_64  </li>
<li>tuned-2.8.0-5.el7_4.2.noarch  </li>
<li>tzdata-2018c-1.el7.noarch  </li>
<li>tzdata-java-2018c-1.el7.noarch  </li>
<li>util-linux-2.23.2-43.el7_4.2.x86_64  </li>
<li>webkitgtk4-2.14.7-3.el7.x86_64  </li>
<li>webkitgtk4-jsc-2.14.7-3.el7.x86_64  </li>
<li>webkitgtk4-plugin-process-gtk2-2.14.7-3.el7.x86_64  </li>
<li>xorg-x11-server-common-1.19.3-11.el7_4.2.x86_64  </li>
<li>xorg-x11-server-Xorg-1.19.3-11.el7_4.2.x86_64</li>
</ul>
","<linux><mysql><redhat><selinux>","2018-03-14 16:14:15"
"901706","Squid proxy not working when upgrade to 3.5","<p>Please someone help. I've spent 2 weeks trying to get proxy script that was always used on a different server with 128 ips working on the new server with 253 ips.</p>

<pre><code>acl manager proto cache_object
acl localhost src 127.0.0.1/32
acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1
acl localnet src 10.0.0.0/8 # RFC1918 possible internal network
acl localnet src 172.16.0.0/12 # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7 # RFC 4193 local private network range
acl localnet src fe80::/10 # RFC 4291 link-local (directly plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT


http_access allow manager localhost
http_access deny manager

# http_access deny !Safe_ports
# http_access deny CONNECT !SSL_ports
# http_access deny all

http_access allow localnet
http_access allow localhost

# hierarchy_stoplist cgi-bin ?



# Uncomment and adjust the following to add a disk cache directory.
cache_dir ufs /var/spool/squid 300 16 256
coredump_dir /var/spool/squid

 refresh_pattern ^ftp:           1440    20%     10080
 refresh_pattern ^gopher:        1440    0%      1440
 refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
 refresh_pattern .               0       20%     4320

 auth_param basic program /usr/lib/squid3/basic_ncsa_auth

 auth_param basic children 5
 auth_param basic realm Squid proxy-caching web server
 auth_param basic credentialsttl 2 hours
 acl ncsa_auth proxy_auth REQUIRED
 http_access allow ncsa_auth

# http_port 3000 

http_port 164.163.XXX.2:3000 intercept name=3000
http_port 164.163.XXX.3:3000 intercept name=3001


acl ip1 myportname 3000
acl ip2 myportname 3001

tcp_outgoing_address 164.163.XXX.2 ip1
tcp_outgoing_address 164.163.XXX.3 ip2



forwarded_for off

#request_header_access Allow allow all
#request_header_access Authorization allow all
#request_header_access WWW-Authenticate allow all
#request_header_access Proxy-Authorization allow all
#request_header_access Proxy-Authenticate allow all
#request_header_access Cache-Control allow all
#request_header_access Content-Encoding allow all
#request_header_access Content-Length allow all
#request_header_access Content-Type allow all
#request_header_access Date allow all
#request_header_access Expires allow all
#request_header_access Host allow all
#request_header_access If-Modified-Since allow all
#request_header_access Last-Modified allow all
#request_header_access Location allow all
#request_header_access Pragma allow all
#request_header_access Accept allow all
#request_header_access Accept-Charset allow all
#request_header_access Accept-Encoding allow all
#request_header_access Accept-Language allow all
#request_header_access Content-Language allow all
#request_header_access Mime-Version allow all
#request_header_access Retry-After allow all
#request_header_access Title allow all
#request_header_access Connection allow all
#request_header_access Proxy-Connection allow all
#request_header_access User-Agent allow all
#request_header_access Cookie allow all
#request_header_access All deny all
</code></pre>

<p>After a week I got 128 proxies working on squid 3.1 but was told to do compile of squid 3.5 with the following configure options for 253 proxies to work on the same server:</p>

<p>configure options: '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--target=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--verbose' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-logdir=$(localstatedir)/log/squid' '--with-pidfile=$(localstatedir)/run/squid.pid' '--disable-dependency-tracking' '--enable-follow-x-forwarded-for' '--enable-auth' '--enable-auth-basic=DB,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam' '--enable-auth-ntlm=smb_lm,fake' '--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper' '--enable-external-acl-helpers=wbinfo_group,kerberos_ldap_group' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' '--enable-delay-pools' '--enable-epoll' '--enable-icap-client' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-storeio=aufs,diskd,ufs,rock' '--enable-wccpv2' '--enable-esi' '--enable-ssl-crtd' '--enable-icmp' '--with-aio' '--with-default-user=squid' '--with-filedescriptors=16384' '--with-dl' '--with-openssl' '--with-pthreads' '--with-included-ltdl' '--disable-arch-native' '--without-nettle' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'target_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic' 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -fPIC' 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig' '--enable-ltdl-convenience' 'CXXFLAGS=-DMAXTCPLISTENPORTS=256'</p>

<p>Can someone please help me because IDK what the proxy script is really doing with:</p>

<p>http_port 164.163.XXX.2:3000 intercept name=3000
http_port 164.163.XXX.3:3000 intercept name=3001</p>

<p>acl ip1 myportname 3000
acl ip2 myportname 3001</p>

<p>tcp_outgoing_address 164.163.XXX.2 ip1
tcp_outgoing_address 164.163.XXX.3 ip2</p>

<p>Right now, on squid 3.5 I cant get any of them to work anymore.</p>
","<squid>","2018-03-15 00:54:24"
"764402","How to route all client traffic without setting it's gateway","<p>I've 3 devices where for some reasons I cannot change their default gateway and neither their ip and netmask.</p>

<p>All the devices has the following ip: 169.254.11.22 and netmask 255.255.255.0</p>

<p>I want to put a router for each client to ""change"" their visible ip address and then forward the port 80 of the router to port 80 of the internal client.</p>

<p>Actually that solution doesn't work because I cannot set the client gateway.</p>

<p>Does exists a device or some software to install on ddwrt routers that allow me to force the gateway of the traffic generated by the clients?</p>

<p>The only solution I've found by now is to estabilish an ssh tunnel and the forward the ports but that cannot be a solution to be used in a production environment.. </p>
","<router><port-forwarding><gateway>","2016-03-17 17:08:53"
"834661","smtp relay shows temporary lookup failure","<p>We deploy a new smtp relay server in ubuntu. When send test email from the root the mail flows properly. But when we use it as relay server it shows ""temporary lookup failure""</p>

<p>Where should we need to config in the postfix. Need some help. Here is our postfixs main.cf file</p>

<pre><code>#myorigin = /etc/mailname

smtpd_banner = $myhostname ESMTP $mail_name (Ubuntu)
biff = no

# appending .domain is the MUA's job.
append_dot_mydomain = no

# Uncomment the next line to generate ""delayed mail"" warnings
#delay_warning_time = 4h

readme_directory = no
# TLS parameters
smtpd_tls_cert_file=/etc/ssl/certs/ssl-cert-snakeoil.pem
smtpd_tls_key_file=/etc/ssl/private/ssl-cert-snakeoil.key
smtpd_use_tls=yes
smtpd_tls_session_cache_database = btree:${data_directory}/smtpd_scache
smtp_tls_session_cache_database = btree:${data_directory}/smtp_scache

# See /usr/share/doc/postfix/TLS_README.gz in the postfix-doc package for
# information on enabling SSL in the smtp client.

smtpd_relay_restrictions = permit_mynetworks permit_sasl_authenticated defer_unauth_destination
mydomain = efftelsolutions.net
myhostname = netexsmtp.efftelsolutions.net
alias_maps = hash:/etc/aliases
alias_database = hash:/etc/aliases
myorigin = /etc/mailname
mydestination = $myhostname, smtp.solutions.net, localhost.solutions.net, , localhost
relayhost =
mynetworks = 127.0.0.0/8 88.82.175.33/27 88.82.175.225 88.82.175.65/27
mailbox_command = procmail -a ""$EXTENSION""
mailbox_size_limit = 0
recipient_delimiter = +
inet_interfaces = all
inet_protocols = all
</code></pre>
","<smtp>","2017-02-24 10:01:51"
"764479","Sending many record on client startup. Nginx 502 bad gateway too many open files","<p>I have nodejs app that sending > 800 mongodb documents on client startup (execute only when a client access my app for the first time).</p>

<p>Nginx as reverse proxy in front of node server.</p>

<p><strong>App server spec</strong></p>

<ul>
<li>Digital Ocean</li>
<li>CentOS 7.2</li>
<li>2GB Ram</li>
<li>2CPU</li>
</ul>

<p><strong>MongoDB server spec</strong></p>

<ul>
<li>Digital Ocean</li>
<li>Ubuntu 14.04</li>
<li>512 RAM</li>
<li>1 CPU</li>
</ul>

<p>nginx -v // nginx version: nginx/1.8.1</p>

<p><strong>nginx config</strong></p>

<pre><code>user  nginx;
worker_processes  2;
worker_rlimit_nofile 100480;

error_log  /var/log/nginx/error.log;

pid        /run/nginx.pid;

events {
  worker_connections  1024;
}

http {
  include   /etc/nginx/mime.types;
  default_type  application/octet-stream;

  log_format  main  '$remote_addr - $remote_user [$time_local] ""$request"" '
  '$status $body_bytes_sent ""$http_referer"" '
  '""$http_user_agent"" ""$http_x_forwarded_for""';

  access_log  /var/log/nginx/access.log  main;

  sendfile        on;

  index   index.html index.htm;

  server {
    server_name 128.199.139.xxx;

    root /var/www/myapp/bundle/public;

    module_app_type node;

    module_startup_file main.js;

    module_env_var MONGO_URL mongodb://{username}:{password}@128.199.139.xxx:27017/;

    module_env_var ROOT_URL http://128.199.139.xxx;

    location / {
      proxy_pass http://128.199.139.xxx;
      proxy_http_version 1.1;

      #proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      #proxy_set_header X-Real-IP $remote_addr;

      # pass the host header - http://wiki.nginx.org/HttpProxyModule#proxy_pass
      #proxy_set_header Host $host;

      # WebSocket proxying - from http://nginx.org/en/docs/http/websocket.html
      proxy_set_header Upgrade ""upgrade"";
      proxy_set_header Connection $http_upgrade;

      #add_header 'Access-Control-Allow-Origin' '*';
    }
  }

  server {
    listen       80 default_server;
    server_name  localhost;
    root         /usr/share/nginx/html;

    #charset koi8-r;

    #access_log  /var/log/nginx/host.access.log  main;

    # Load configuration files for the default server block.
    include /etc/nginx/default.d/*.conf;

    location / {
    }

    # redirect server error pages to the static page /40x.html
    #
    error_page  404              /404.html;
    location = /40x.html {
    }

    # redirect server error pages to the static page /50x.html
    #
    error_page   500 502 503 504  /50x.html;
    location = /50x.html {
    }
  }
}
</code></pre>

<p><strong>Error Log</strong></p>

<p>Below error log:</p>

<pre><code>2016/03/17 09:46:00 [crit] 10295#0: accept4() failed (24: Too many open files)
2016/03/17 09:46:01 [crit] 10295#0: accept4() failed (24: Too many open files)
2016/03/17 09:46:01 [crit] 10295#0: accept4() failed (24: Too many open files)
.....many duplicate error as above
2016/03/17 09:47:35 [error] 10295#0: *4064 recv() failed (104: Connection reset by peer) while reading response header from upstream, client: 128.199.139.160, server: 128.199.139.1$
2016/03/17 09:47:35 [alert] 10295#0: accept4() failed (9: Bad file descriptor)
[ 2016-03-17 09:53:47.8144 10403/7f2833c9a700 age/Ust/UstRouterMain.cpp:422 ]: Signal received. Gracefully shutting down... (send signal 2 more time(s) to force shutdown)
[ 2016-03-17 09:53:47.8145 10403/7f2839500880 age/Ust/UstRouterMain.cpp:492 ]: Received command to shutdown gracefully. Waiting until all clients have disconnected...
[ 2016-03-17 09:53:47.8146 10403/7f2833c9a700 Ser/Server.h:464 ]: [UstRouter] Shutdown finished
2016/03/17 09:54:11 [alert] 10549#0: 1024 worker_connections are not enough
2016/03/17 09:54:11 [error] 10549#0: *1021 recv() failed (104: Connection reset by peer) while reading response header from upstream, client: 128.199.139.160, server: 128.199.139.1$
2016/03/17 09:54:12 [alert] 10549#0: 1024 worker_connections are not enough
2016/03/17 09:54:12 [error] 10549#0: *2043 recv() failed (104: Connection reset by peer) while reading response header from upstream, client: 128.199.139.160, server: 128.199.139.1$
2016/03/17 11:43:20 [alert] 10549#0: 1024 worker_connections are not enough
2016/03/17 11:43:20 [error] 10549#0: *3069 recv() failed (104: Connection reset by peer) while reading response header from upstream, client: 128.199.139.160, server: 128.199.139.1$
2016/03/17 13:49:54 [error] 10549#0: *3071 open() ""/usr/share/nginx/html/robots.txt"" failed (2: No such file or directory), client: 180.97.106.xx, server: localhost, request: ""GET
</code></pre>

<p>Appreciated for any help or clue</p>

<p><strong>PROBLEM</strong></p>

<p>The problem is I'm getting following error according to above setup:</p>

<ul>
<li>Too many open files</li>
<li>worker_connections are not enough</li>
<li>(104: Connection reset by peer) while reading response header from upstream</li>
</ul>

<p>How to solve this issue? Thanks</p>
","<nginx><mongodb><node.js><meteor>","2016-03-17 22:28:49"
"834723","do I have to upgrade all servers to 2016 at once?","<p>Do I?</p>

<p>Currently, we have a mix of 2003, 2008, 2008R2 and 2012.
I want to move to 2016 AD for DCs only.
I'm going to purchase CALs and Server 2016 Standard licenses.</p>

<p>I assume not?</p>

<p>Thanks for any help!!</p>
","<active-directory><windows-server-2003><upgrade><windows-server-2016>","2017-02-24 15:08:49"
"764498","How to do ""dynamic apex"" right in DNS/BIND?","<blockquote>
<p>I want to retain dynamic control of the apex, but not break standard
handling of other RRs (NS, MX).</p>
</blockquote>
<h2>Context</h2>
<p>A domain name (<em>exampleA.net</em>) is controlled by the domain owner (via the domain registrar). The website shall be placed in <em>exampleB.net</em> cloud.</p>
<p>I want to use dynamic addressing (not calling it CNAME at this point), so the system doesn't stall waiting for the manual update of &quot;A&quot;s in the registrar records.</p>
<p>A complete &quot;NS&quot; zone delegation is not applicable.</p>
<p>The logical, simple configuration, which is invalid:</p>
<pre><code>@   CNAME exampleB.net.
www CNAME exampleB.net.
@   MX    mx
@   NS    ns0
; ...setting the SOA, A's
</code></pre>
<h2>&quot;Can't do, breaches RFC&quot;</h2>
<p>CNAME per the <a href=""https://www.rfc-editor.org/rfc/rfc2181"" rel=""nofollow noreferrer"">RFC 2181</a> simply forbids you multiple RRs, barring you to use apex-CNAMEs, because of SOA and NS.</p>
<p>The <a href=""http://www.rfc-editor.org/rfc/rfc1912.txt"" rel=""nofollow noreferrer"">&quot;dns error&quot; rfc 1912</a> calls this practice <em>&quot;often attempted by inexperienced administrators&quot;.</em></p>
<p>Well, I doubt that was true even in 1996, it was just the need for a &quot;dynamic&quot; RR (which CNAME is believed to be, but it's not, for these very reasons).</p>
<p>In fact, it's a fundamental flaw of the domain-naming-system. Besides the inception of the holy apex, it really messes up the www.appendectomy. I'm not taking the &quot;canonical no&quot; for an answer here.</p>
<blockquote>
<p>it can be accomplished using a preprocessor such as m4 on your host
files.</p>
</blockquote>
<p>Yeah, right...</p>
<h2>Real World Issues</h2>
<p>BIND with file-based zones will complain and fail a zonecheck if you try this. But using the <strong>DLZ will pass</strong> and work, as described. Other DNS software might or might not accept this, or they use some special types (ANAME, ALIAS) for this.</p>
<p>Still, if you manage to pull this off, you have been warned.</p>
<p>The headache starts when queries of any types for <em>exampleA.net</em> will <strong>sometimes</strong> get resolved as <em>CNAME exampleB.net.</em> instead of the configured record.</p>
<p>That might work, will usually fail, or worse, for example in some MTAs lead to the change/redelivery of mail@exampleA.net to mail@exampleB.net</p>
<h2>Incomplete Solutions</h2>
<p>Instead of a compliant failure, the recommended workaround is, by setting (delegating) the RR into the CNAME'd record itself.</p>
<p>If you also manage the particular sub-system, you can &quot;pipe&quot; it:</p>
<pre><code>exampleB.net. MX mx.exampleB.net.
</code></pre>
<p>or you can &quot;bounce it back&quot;:</p>
<pre><code>; pointing the apex CNAME to a more specific exampleA.exampleB.net.

exampleA.exampleB.net. MX mx.exampleA.net.
</code></pre>
<p>That's a hotfix at best, <strong>doesn't solve the dynamics</strong> and leaves the zone exposed to stale configurations and migration booby traps.</p>
<p><strong>Related questions</strong><br />
<a href=""https://stackoverflow.com/questions/656009/how-to-overcome-root-domain-cname-restrictions"">https://stackoverflow.com/questions/656009/how-to-overcome-root-domain-cname-restrictions</a><br />
<a href=""https://serverfault.com/questions/608895/cname-in-bind"">CNAME in @ (BIND)</a><br />
<a href=""https://serverfault.com/questions/55528/set-root-domain-record-to-be-a-cname"">Set root domain record to be a CNAME</a></p>
","<domain-name-system><bind><cname-record><dns-zone>","2016-03-18 01:12:59"
"764508","Zoneminder homepage not loading","<p>I'm trying to set up Zoneminder using <a href=""http://tuxradar.com/content/build-your-own-surveillance-zoneminder#null"" rel=""nofollow noreferrer"">this tutorial</a>.</p>

<p>I've started with even installing Ubuntu Server (14.04), so it's a fresh install.</p>

<p>The first problem I came across might be relevant, so I'll mention it first: conf.d didn't exist, so the line </p>

<pre><code>sudo ln -s /etc/zm/apache.conf /etc/apache2/conf.d/zoneminder.conf
</code></pre>

<p>didn't work for me.  I just used mkdir to create the conf.d directory, based on something I read online.  This might be what's causing the problem (in which case, I still don't know how to fix it).</p>

<p>But I came to the first show-stopper problem at the part that says to go to ""zm"" in the browser (in my case, 192.168.1.155).  The Apache ""It works"" test page works fine, but 192.168.1.155/zm, it returns a 404 error.</p>

<p>Any idea on how to fix it?  I've already tried uninstalling and reinstalling php5-common.  (And I also reinstalled Zoneminder, of course, because php5-common is a dependency.)</p>
","<ubuntu-14.04>","2016-03-18 02:57:28"
"901853","Create a fake job for SGE to fill all nodes before launching a job that will run on complete nodes","<p>The problem of submitting jobs on SGE to run on complete nodes was addressed before in this forum. Several solutions have been suggested, one of which is to configure SGE to allow for the usage of the option -l excl=TRUE, another solution is to ask SGE for hard memory or load limits.</p>

<p>I'm using the cluster of my university for my master thesis, the parallel environment openmpi is configured with the fill-up strategy. Typically the nodes of the cluster contain 16 or 20 cores each, the problem is that some of the users instead of launching computations with a number of cores that is multiple of 16 (or 20), they launch their jobs with an arbitrary number of cores. As a result, when I launch a job with -pe openmpi 16, sometimes SGE will reserve the processors on 3 nodes (e.g. 6 + 1 + 10) which makes the computations very slow.</p>

<p>I asked the administrator to configure the cluster to allow for -l excl=TRUE but he refused to change the configuration before making tests (I don't know for how long).</p>

<p>Now I have a new idea that may allow me to have a similar result as (-l excl=TRUE) but without changing the cluster:</p>

<ol>
<li>Write a script that will scan the queue and estimate the number of cores that must be asked to SGE so that he fills all the running nodes and let only completely free nodes.</li>
<li>Launch a fake job with the computed number of cores that will wait for a certain amount of time.</li>
<li>launch my true job (e.g -pe openmpi 2*16=32).</li>
<li>Delete the fake job to allow other users to use its cores</li>
</ol>

<p>Can someone provide me an example of such code ?</p>
","<cluster>","2018-03-15 16:06:40"
"976473","Client can't authenticate to IIS site using Kerberos","<p>We have several IIS hosted sites using windows authentication. Some of our users can logon in one of the sites, but getting never-ending authentication challenge in another (the second one is used in an iframe of first). We found out that users that can't logon are using Kerberos authentication (others NTLM). All of the sites are using the same authorization settings (useAppPoolCredentials set to true). Thus users can access one site but can't access the second one with the same settings. App pool identity user is in the administrator group and IIS_IUSRS group. I also tried to use the domain user account to logon to site from VM and got same never-ending authentication prompt because of Kerberos. I've read Chiranth Ramaswamy's article about IIS authentication but unfortunately couldn't find solution to problem. Is there any way to solve the problem?</p>

<p>EDIT: We also have 2nd server with the same sites and settings.</p>

<p>EDIT2: I found out that I can logon if I'm using same domain user account if I <strong>don't</strong> write domain in login. Thus ""UserName"" works and ""DomainName\UserName"" doesn't</p>
","<active-directory><iis><windows-server-2012-r2><authentication><kerberos>","2019-07-24 08:18:02"
"764574","IPv6 + PTR + gmail returning error","<p>I have a problem with mail connected to gmail account. Every time when i wanna send mail i got that error:</p>

<blockquote>
  <p>Our system has detected that this message does not meet IPv6 sending guidelines regarding PTR records and authentication</p>
</blockquote>

<p>From server providoer i got IPv6 addreses: 2001:41D0:2:DFb7::/64</p>

<p>I add records to file piorkowski.pl.db:</p>

<pre><code>piorkowski.pl.   IN   AAAA   2001:41D0:2:DFb7::1
1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.7.b.f.d.2.0.0.0.0.d.1.4.1.0.0.2.ip6.arpa. IN PTR piorkowski.pl.
</code></pre>

<p>And on server provider i add reverse for IPv6.</p>

<p>Command</p>

<pre><code>dig AAAA piorkowski.pl +nocomments
</code></pre>

<p>return valid ip addres</p>

<p>the entire contents of piorkowski.pl.db:</p>

<pre><code>$TTL 14400
@       IN      SOA     ns318558.ovh.net.      hostmaster.piorkowski.pl. (
                                                2014072700
                                               14400
                                                3600
                                                1209600
                                                86400 )

piorkowski.pl.   14400   IN   NS   ns318558.ovh.net.
piorkowski.pl.   14400   IN   NS   sdns2.ovh.net.

*.piorkowski.pl.   14400   IN   A   188.165.23.11
ftp   14400   IN   A   188.165.23.11
localhost   14400   IN   A   127.0.0.1
mail   14400   IN   A   188.165.23.11
piorkowski.pl.   14400   IN   A   188.165.23.11
pop   14400   IN   A   188.165.23.11
smtp   14400   IN   A   188.165.23.11
www   14400   IN   A   188.165.23.11
piorkowski.pl.   14400   IN   AAAA   2001:41D0:2:DFb7::1
1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.7.b.f.d.2.0.0.0.0.d.1.4.1.0.0.2.ip6.arpa. IN     PTR piorkowski.pl.
piorkowski.pl.   14400   IN   MX   10 mail



piorkowski.pl.   14400   IN   TXT   ""v=spf1 a mx ip4:188.165.23.11 ~all""

localhost   14400   IN   AAAA   ::1
</code></pre>

<p>How stop quotes?</p>
","<centos><domain><ipv6><exim>","2016-03-18 11:24:52"
"834824","I faced this error after trying to install mongodb on ubuntu 16.10","<p>When trying to install mongodb on ubuntu 16.10</p>

<p>When I type this:</p>

<pre><code>$ sudo systemctl start mongodb
Failed to start mongodb.service: Unit mongodb.service is masked
</code></pre>

<p>I have followed all the steps on the following <a href=""http://www.digitalocean.com/community/tutorials/how-to-install-mongodb-on-ubuntu-16-04"" rel=""nofollow noreferrer"">link</a>.</p>
","<ubuntu><mongodb>","2017-02-24 22:32:38"
"903326","Resource group on google cloud console","<p>I want to know if a ""Resource group"" feature i.e. a logical container for a collection of resources that can be treated as one logical instance, which helps me to keep all the resource types like Virtual machine, Network interface, Public IP address,Storage account,Virtual network and gateway etc. for a particular resource under a single resource name; has been implemented on Google Cloud Platform. This feature has been implemented in Azure and AWS. </p>
","<google-cloud-platform>","2018-03-19 03:42:08"
"976935","Reinstalling the uninstalled packages using a bash script to make things back to normal","<p>Well, I created a bash script which seems to work on my situation. Here I have installed a random small program to catch other error messages &amp; saved it as ""<strong>ls.sh</strong>"" <em>filename</em>.</p>

<pre><code>for package in $(apt remove hello --purge 2&gt;&amp;1 |
 grep ""warning: files list file for package '"" |
 grep -Po ""[^'\n ]+'"" | grep -Po ""[^']+"");
 do apt-get install --reinstall ""$package"";
 done 
</code></pre>

<p>To solve this kinds of big time consuming problem:</p>

<pre><code>dpkg: warning: files list file for package 'package-name 1' missing; assuming package has no files currently installed
....
....
dpkg: warning: files list file for package 'package-name 2000' missing; assuming package has no files currently installed
</code></pre>

<p>Day by day, the number of ""packages"" to install decreases depending upon my internet speed and whether its SSD or HDD. We all know, fast internet + SDD = Less time. But that it's in my case. I have HDD and 500kbps download speed.</p>

<p>So every day I keep track of the number of the remaining packages to be installed in <a href=""https://pastebin.com/hruWCeYJ"" rel=""nofollow noreferrer"">this</a> way, where I read the last line number.</p>

<p><strong>Now my main problem is this:</strong></p>

<pre><code>pranav@inspiron-5548L:~/Documents/command$ sudo /bin/bash ls.sh

&lt;no output like it used to show before, but it is running though&gt;
</code></pre>

<p>where I clearly don't see what going on. No output whatsoever. Here can't keep track of remaining packages to be reinstalled since I don't see anything. <strong>Help me see it</strong>. <em>It seems to be running but waiting for all packages to be re-installed and then only say something.</em></p>
","<ubuntu><bash><package-management>","2019-07-27 07:29:03"
"764675","SCSI to SATA conversion","<p>I have Hitachi dk514c-38 drive, which is producing ticking sound. These drives not available anymore. Can I connect a SATA drive instead of this with some converter? 
I also want to know if these is some box or casing available to do this. </p>
","<scsi>","2016-03-18 19:43:22"
"977005","CentOS7: Multiple Interfaces, default route","<p>[First post here...]</p>

<p>Hello everyone,</p>

<p>I have a CentOS 7 box which is supposed to become a node of a ganeti-cluster. It has 4 interfaces:
eno1, eno2 - on the mainboard
ens23s0f0, ens23s0f1 - a dualport 10G Intel adapter</p>

<p>I have renamed the 10G interfaces to eth0/eth1 by adding two lines to /usr/lib/udev/rules.d/60-net.rules like this:</p>

<pre><code>ACTION==""add"", SUBSYSTEM==""net""m DRIVERS==""?*"", ATTR{type}==""1"", ATTR{ADDRESS}==""xx:xx:xx:xx:xx:xx"", RESULT==""?*"", NAME=""eth0""    
ACTION==""add"", SUBSYSTEM==""net""m DRIVERS==""?*"", ATTR{type}==""1"", ATTR{ADDRESS}==""xx:xx:xx:xx:xx:xy"", RESULT==""?*"", NAME=""eth1""
</code></pre>

<p>3 interfaces are in the same subnet 172.16.66.0/24 - eno1, eth0, eth1. That may not be the optimum setup, but let's just accept it for now, OK? eno2 is not needed.</p>

<p>The patchcables of eno1 and eth1 are plugged into the same switch, default VLAN. eth0 is plugged into another switch with a seperate VLAN for cluster data replication.</p>

<p>What I want is that the 10G-interface eth1 persistently becomes the default gateway for the box. However, as long as eno1 is up, the box will pick it as the default gateway. And if it's down, it will pick eth0.</p>

<p>I have no entries in /etc/sysconfig/network or anywhere else. 
Also I have no files /etc/sysconfig/network-scripts/route-devN.</p>

<p>The ifcfg-xxxN files are basically identical:</p>

<pre><code>HWADDR=xx:xx:xx:xx:xx:xx
NM_CONTROLLED=no
DEVICE=devN
NAME=devN
TYPE=ETHERNET
BOOTPROTO=static
ONBOOT=yes
IPADDR=172.16.66.nn
PREFIX=24
</code></pre>

<p>In ifcfg-eth1 I added</p>

<pre><code>GATEWAY=172.16.66.200
DNS1=172.16.66.200
</code></pre>

<p>So this is the only file with a GATEWAY directive, however it seems to be ignored.</p>

<p>I am able to set the default route temporarily with</p>

<pre><code># ip route add default via 172.16.66.200 dev eth1
</code></pre>

<p>but as expected this does not survive a reboot.</p>

<p>Finally my routing table would always look like this:</p>

<pre><code># ip r
default via 172.16.66.200 dev eno1
169.254.0.0/16 dev eno1 scope link metric 1002
169.254.0.0/16 dev eth0 scope link metric 1004
169.254.0.0/16 dev eth1 scope link metric 1005
172.16.66.0/24 dev eno1 proto kernel scope link src 172.16.66.n1
172.16.66.0/24 dev eth0 proto kernel scope link src 172.16.66.n2
172.16.66.0/24 dev eth1 proto kernel scope link src 172.16.66.n3
</code></pre>

<p>Any help on how to make eth1 the default gateway is really appreciated.</p>

<p>Thanks in advance,
Andreas</p>

<hr>

<p>EDIT:
OK, so I decided to start it all from scratch. Here's my log and questions:</p>

<p>I installed CentOS 7.6.1810, Kernel 3.10.0-957.
Originally I intended to install 7.2.1511 because this is the version of the other clusternodes, but it's too old for the server-CPU.</p>

<p>I unplugged all patchcables except for the 10G-interface that I want to become the default gateway. In the graphical installer I configured a static IP (172.16.66.11) for that interface. In the dialog there's also a button to set a route. There I set a route for 172.16.66.0, gateway 172.16.66.200 which is the internet router, metric 100.</p>

<p>After the basic installation the internet connection worked.
I only installed the MidnightCommander and did not rename any interfaces. 
I checked the routing table:</p>

<pre><code># ip r
default via 172.16.66.200 dev enp23S0f1 proto static metric 100
172.16.66.0/24 dev enp23s0f1 proto kernel scope link src 172.16.66.11 metric 100
172.16.66.0/24 via 172.16.66.200 dev enp23s0f1 proto static metric 100
</code></pre>

<p>This is what I had expected. After I cleared and edited some stuff the ifcfg-enp23s0f1 looked like this (the quotes were set automatically by the installer):</p>

<pre><code>TYPE=""Ethernet""
BOOTPROTO=""static""
DEFROUTE=""yes""
NAME=""enp23s0f1""
DEVICE=""enp23s0f1""
ONBOOT=""yes""
HWADDR=""XX:XX:XX:XX:XX:XX""
IPADDR=""172.16.66.11""
PREFIX=""24""
GATEWAY=""172.16.66.200""
DNS1=""172.16.66.200""
</code></pre>

<p>I noticed that the installer created a file /etc/sysconfig/network-scripts/route-enp23s0f1. The format however is different from what Jenny suggested and what I also found elsewhere:</p>

<pre><code>ADDRESS0=172.16.66.0
NETMASK0=255.255.255.0
GATEWAY0=172.16.66.200
METRIC0=100
</code></pre>

<p>As everything was working fine, I added a second interface now (eno1, mainboard, 1Gb/s). I edited ifcfg-eno1 like this (the installer did not set quotes here, probably because I didn't edit that connection upon installation?):</p>

<pre><code>TYPE=Ethernet
BOOTPROTO=static
DEFROUTE=no
NAME=eno1
DEVICE=eno1
ONBOOT=yes
HWADDR=YY:YY:YY:YY:YY:YY
IPADDR=172.16.66.22
PREFIX=24
</code></pre>

<p>After restart of the network the routing table looked like this:</p>

<pre><code># systemctl restart network
# ip r
default via 172.16.66.200 dev enp23S0f1 proto static metric 101
172.16.66.0/24 dev eno1 proto kernel scope link src 172.16.66.22 metric 100
172.16.66.0/24 via 172.16.66.200 dev enp23s0f1 proto static metric 100
172.16.66.0/24 dev enp23s0f1 proto kernel scope link src 172.16.66.11 metric 101
</code></pre>

<p>So the system automatically sets the metric for eno1 to 100 although I have assigned this vaue to enp23s0f1 in the route-enp23s0f1 file. The metric for enp23s0f1 is then automatically set to 101. The system seems to insist on giving the mainboard-interfaces priority over the PCIe-adapter. </p>

<p>The above happened with NetworkManager still being active. I deactivated NM then</p>

<pre><code># systemctl stop NetworkManager
# systemctl disable NetworkManager
</code></pre>

<p>and added to ifcfg-enp23s0f1 and -eno1 as first line</p>

<pre><code>NM_CONTROLLED=no
</code></pre>

<p>The routing table now looks like this:</p>

<pre><code># systemctl restart network
# ip r
default via 172.16.66.200 dev eno1
169.254.0.0/16 dev eno1 scope link metric 1002
169.254.0.0/16 dev enp23s0f1 scope link metric 1005
172.16.66.0/24 dev eno1 proto kernel scope link src 172.16.66.22
172.16.66.0/24 dev enp23s0f1 proto kernel scope link src 172.16.66.11
</code></pre>

<p>Again eno1 is set as default route. The file route-enp23s0f1 is obviously being ignored and only seems to be relevant with NetworkManager enabled.</p>

<p>Do I need to enable NetworkManager at all, is that advisable? I thought not.</p>

<p>Why does the system automatically change the metric in favor of the mainboard-if even with NM enabled and an explicit value given in the route-file?</p>

<p>Thanks and best regards
Andreas</p>
","<routing><centos7><gateway><ip-routing>","2019-07-28 08:35:28"
"764706","OpenVPN won't start on Ubuntu","<p>I was trying to setup an OpenVPN server on LinuxMint 17.1 using the following guide: <a href=""https://www.digitalocean.com/community/tutorials/how-to-set-up-an-openvpn-server-on-ubuntu-14-04"" rel=""nofollow noreferrer"">Click me</a>.</p>

<p>I followed up to the part where you start up OpenVPN with the following command: <code>service openvpn start</code></p>

<p>That brought the following response: </p>

<p><code>* Starting virtual private network daemon(s)...                                                                                                                           *   Autostarting VPN 'server'                                                                                                                                            foo-server random-folder #</code></p>

<p>But when I check the service status it says that OpenVPN isn't running. Any ideas? This is my <code>/var/log/syslog</code>: <a href=""http://pastebin.com/giiUrtzj"" rel=""nofollow noreferrer"">PasteBin</a>.</p>

<p>Thanks.</p>
","<ubuntu><openvpn><linuxmint>","2016-03-18 22:15:55"
"977060","3gbps / 6gbps / 12gbps HDD - does interface speed really matter?","<p>Currently there are no mechanical hard drives (non-ssd/sshd) that can max out or even get close to 3gbps data transfer, what's the point of having drives with 6 or even 12gbps interface speed?</p>
","<hard-drive>","2019-07-29 01:57:21"
"834924","How to support more HTTPS Ciphers?","<p>I am debugging an IoT device that I am making. I am having it try and connect to my website (EC2 hosted) over HTTPS. The device is running into an issue negotiating the TLS handshake, and it is throwing a Handshake Failure (40). The IoT device supports the following Cipher Suites.</p>

<pre><code>TLS_RSA_WITH_AES_128_CBC_SHA
TLS_RSA_WITH_AES_256_CBC_SHA
TLS_RSA_WITH_RC4_128_SHA
TLS_RSA_WITH_RC4_128_MD5
</code></pre>

<p>How can I make my server support one of these Cipher Suites? Do I need a different SSL cert? My current one is from <a href=""https://www.sslforfree.com/"" rel=""nofollow noreferrer"">https://www.sslforfree.com/</a></p>

<p>My website is <a href=""https://sniffergps.com/"" rel=""nofollow noreferrer"">https://sniffergps.com/</a> and <a href=""https://app.sniffergps.com/"" rel=""nofollow noreferrer"">https://app.sniffergps.com/</a> (Amazon API Gateway)</p>

<p><strong>Termination details:</strong>
For app.sniffergps.com, I am using AWS API Gateway, custom domain. I just input my certificate details.</p>

<p>For sniffergps.com, its Elastic beanstalk, so the certificate is on the load balancer.</p>
","<ssl><ssl-certificate><ssl-certificate-errors><cipher>","2017-02-25 21:15:29"
"903494","Setting up an Internal DNS Server, Inherited a Workgroup Environment","<p>I'm currently getting ready to move my company to a domain, they are on a workgroup using their IP's DNS servers. I need to have users resolve internal addresses and thinking about using a Forwarder to resolve external addresses as needed. No external users need to use this DNS server to resolve anything, it will be used for internal users behind the firewall only. So my predecessor said I would need to register my DNS server (w/an outside agency)if I was gonna have an internal DNS server in AD. I don't believe this is true, but correct me if I'm wrong?  Do I need to do any special ""registering"" of the internal DNS server if I just need it to resolve internal requests &amp; set up a forward on it for external resolutions?</p>

<p>What do I really need to do to get this DNS server up &amp; running!</p>
","<domain-name-system>","2018-03-20 01:32:40"
"834927","Back panel display ports on Z800, connecting to VGA monitor","<p>I work for a charity, we've received a HP Z800 workstation. Need to connect a 15-pin VGA monitor. However the the back panel has only the following ports (as shown in the two images) connected to display. Can anyone tell me what these ports are called and what the connectors or adapters required to connect the VGA monitor (a Dell one) cable ?</p>

<p><strong>Workstation :</strong></p>

<p>On the workstation, 9-pin-port :
<a href=""https://i.sstatic.net/TJidp.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TJidp.jpg"" alt=""enter image description here""></a></p>

<p>On the workstation, display ports: 
<a href=""https://i.sstatic.net/TMfPR.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TMfPR.jpg"" alt=""enter image description here""></a></p>

<p>On the workstation : Individual view :
<a href=""https://i.sstatic.net/Vej8P.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Vej8P.jpg"" alt=""enter image description here""></a></p>

<p>On the workstation : Individual view :
<a href=""https://i.sstatic.net/9JXXa.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9JXXa.jpg"" alt=""enter image description here""></a></p>

<p><strong>Monitor :</strong></p>

<p>For more info, here are the ports on the monitor :
<a href=""https://i.sstatic.net/eWoyK.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eWoyK.jpg"" alt=""enter image description here""></a></p>

<p><a href=""https://i.sstatic.net/HrKx3.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HrKx3.jpg"" alt=""enter image description here""></a></p>
","<hp><cable><display><adapter><monitors>","2017-02-25 21:34:11"
"834959","Move my.cnf to Different Mount Point on MySQL Server","<p>I am working on a RHEL 6.6 server which runs MySQL version 5.7.17 which I had installed by the following command:</p>

<pre><code>yum install mysql-server
</code></pre>

<p>My client want me to move the <code>my.cnf</code> from it's default location <code>/etc/my.cnf</code> to <code>/app/mysql/my.cnf</code>. If I move, the mysql server fails.</p>

<p>I cannot find any way to do this. </p>

<p>If anyone know the way, please help me.</p>
","<mysql><my.cnf>","2017-02-26 05:38:37"
"834964","What does the Let's Encrypt apache plugin do when proving domain ownership?","<p>I want to run</p>

<pre><code>$ letsencrypt --apache certonly
</code></pre>

<p>to get a new cert, but I can't find anywhere in the documentation that explains what this actually does other than that it uses the ""tls-sni-01 challenge type"". What does it actually change in my Apache config, and does it delete the changes after completing the challenge? I am nervous about running a script that makes automatic changes to my Apache configuration.</p>

<p>If I run it on a live site will it result in any downtime?</p>
","<apache-2.4><ubuntu-16.04><lets-encrypt>","2017-02-26 06:29:41"
"977202","554 5.7.1 <user@domain.com>: Relay access denied","<p>I've looked at the postings for this similar issue and tried the suggestions with no luck. </p>

<p>Please help me figure out why no emails can be received by my postfix server. </p>

<p>I have my MX record pointing to my postfix server </p>

<p>Here is my main.cf</p>

<pre><code>smtpd_banner = $myhostname ESMTP $mail_name (Ubuntu)
biff = no

# appending .domain is the MUA's job.
append_dot_mydomain = no

# Uncomment the next line to generate ""delayed mail"" warnings
#delay_warning_time = 4h

readme_directory = no

# See http://www.postfix.org/COMPATIBILITY_README.html -- default to 2 on
# fresh installs.
compatibility_level = 2

# TLS parameters
smtpd_tls_cert_file=/etc/ssl/certs/ssl-cert-snakeoil.pem
smtpd_tls_key_file=/etc/ssl/private/ssl-cert-snakeoil.key
smtpd_use_tls=yes
smtpd_tls_session_cache_database = btree:${data_directory}/smtpd_scache
smtp_tls_session_cache_database = btree:${data_directory}/smtp_scache

# See /usr/share/doc/postfix/TLS_README.gz in the postfix-doc package for
# information on enabling SSL in the smtp client.

smtpd_relay_restrictions = permit_mynetworks permit_sasl_authenticated defer_unauth_destination
myhostname = cumail.circircuitsunited.com
alias_maps = hash:/etc/aliases
alias_database = hash:/etc/aliases
myorigin = /etc/mailname
mydestination = localhost.$mydomain, localhost, $mydomain
relayhost =
mynetworks = 127.0.0.0/8 [::ffff:127.0.0.0]/104 [::1]/128 localhost
mailbox_size_limit = 0
recipient_delimiter = +
inet_interfaces = all
inet_protocols = all
virtual_alias_maps = hash:/etc/postfix/virtual
sender_bcc_maps = hash:/etc/postfix/bcc
mailbox_command = /usr/bin/procmail-wrapper -o -a $DOMAIN -d $LOGNAME
home_mailbox = Maildir/
smtpd_sasl_auth_enable = yes
smtpd_sasl_security_options = noanonymous
broken_sasl_auth_clients = yes
smtpd_recipient_restrictions = permit_mynetworks permit_sasl_authenticated reject_unauth_destination
smtp_tls_security_level = may
allow_percent_hack = no
</code></pre>
","<postfix>","2019-07-30 01:11:16"
"903655","How to point external domain to a subdomain?","<p>I have the domain <code>example.com</code> and have set up the Ubuntu Apache server in a way that it points any subdomain to a directory with the same name.</p>

<pre><code>&lt;VirtualHost *:80&gt;
    ServerAlias *.example.com
    VirtualDocumentRoot /var/www/html/%1
&lt;/VirtualHost&gt;
</code></pre>

<p>So <code>subdomain.example.com</code> will point to <code>example.com/subdomain</code>.</p>

<p>I am trying to allow users point their own custom domain to a subdomain of their choice, how could I achieve this and what records would be required and what would have to be done to the <strong>Virtual Host</strong>?</p>

<p>I have tried the following record:</p>

<ul>
<li><code>sampledomain.example</code> (<strong>CNAME</strong> record) - <code>@</code> --> <code>sample.example.com</code></li>
<li><code>sampledomain.example</code> <strong>(A</strong> record) - <code>@</code> --> (<code>IP</code> Address of example.com)</li>
</ul>

<p>I have also tried using <code>www</code> for the CNAME along with removing the <code>A record</code>.</p>

<p><code>sample.example.com</code> successfully points to <code>example.com/sample</code>. However, <code>sampledomain.example</code> acts like the root site for some reason. </p>

<p>So if I visit <code>sampledomain.example/sample</code>, I will get the contents of <code>example.com/sample</code> rather than pointing to <code>sample.example.com</code>.</p>

<p>How could I sort this out? Which records would be needed and what could be done to the Virtual Host?</p>

<p>I used a CNAME record so that I can retrieve the custom domain name via PHP when the user visits that - is the CNAME required?</p>
","<apache-2.2><virtualhost><ip><cname-record><a-record>","2018-03-20 19:03:49"
"903784","how to protect from hotlink in nginx","<p>I need to protect from hotlinks in my php site.? how Can i achieve this ?? Iam using nginx as webserver ..
  If I try to view <strong>site.com/uploads/image.jpg</strong> without login I can able to see images in browser ,I need to prevent direct folder accessing without login .</p>

<p><strong>my project structure</strong></p>

<pre><code>site
    -index.php
    -uploads/image.jpg....
    -css/
    -js/
</code></pre>

<p><strong>site.conf</strong></p>

<pre><code>server {

    listen site.com;

    server_name site.com;



    root /home/vijo/Music/PHP/site;

    index index.php index.html index.htm;



    keepalive_timeout 70;

    access_log /home/vijo/Music/PHP/site/log/access.log;

    error_log /home/vijo/Music/PHP/site/log/error.log;





    # Make site accessible from http://localhost/



   location / {

            try_files $uri $uri/ @rewrite;

            expires max;

    }



    location @rewrite {

            rewrite ^ /index.php;
    }



    location ~ \.php$ {

            fastcgi_split_path_info ^(.+\.php)(/.+)$;

            fastcgi_pass unix:/var/run/php/php7.0-fpm.sock;

            fastcgi_index index.php;

            fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;

            include fastcgi_params;

    }



    location ~ /\.ht {

            deny all;

    }
</code></pre>

<p>}</p>
","<nginx><php-fpm><website><php7><hotlinking>","2018-03-21 12:26:30"
"835178","Changed network connection for Windows server running on Google Cloud and can't connect","<p>So I changed the IP for my windows server 2012 running on Google Cloud Platform in network sharing center and ""tab"" the subnet mask which was 255.0.0.0 and I got disconnected from the server and I can't reconnect!</p>

<p>Is it possible to get the auto IP again?</p>
","<windows-server-2012><ip><google-cloud-platform><static-ip>","2017-02-27 14:45:47"
"977573","Can't ping switch from WLC Cisco 3750G","<p>Working with a Catalyst 3750G with the integrated WLAN controller. Management VLAN ID is 55 on both the switch and the controller, both on the same VLAN and subnet with the same default gateway. However, I am unable to reach or connect to the controller from the switch and vice versa. If I directly connect to the switch I can ping default gateway, reach internet, etc. However, when I try to ping the management IP address for the controller I am unable to.</p>

<p>I was able to reach the device manager GUI after resetting the config on the controller but unable to reach or connect to it aside from serial console.</p>

<p>edit:
Switch config: <a href=""https://pastebin.com/u3bYC5Kw"" rel=""nofollow noreferrer"">https://pastebin.com/u3bYC5Kw</a></p>

<p>Will get WLC config later today, sorry</p>
","<networking><cisco>","2019-08-01 12:36:37"
"835391","BIngbot on my network IP","<p>I was looking my apache logs and I find out this.</p>

<pre><code>192.168.1.2 - - [30/Nov/2016:15:46:52 +0100] ""GET http://www.Mywebsite.... HTTP/1.1"" 200 5539 ""-"" ""Mozilla/5.0 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)""
</code></pre>

<p>This happen a few times with that IP, but I cannot understand how is bingbot executing from that IP.</p>

<p>Thanks in advance</p>
","<logging><apache2>","2017-02-28 12:34:57"
"904152","recompile nginx with build name and preserve current configurations","<p>So I've been running nginx servers for while and I've made a nice bunch of changes to get it just right for one of my clients. They have recently employed pen testers and want me to harden security. As such I need to hide the build name of nginx to meet their requirements. However it seems I need to recompile nginx to do that. I just wondering if there's a way to keep all my changes to config etc when recompiling with the build name option.</p>
","<nginx><security>","2018-03-23 02:33:50"
"904154","attack from China on the mail server","<p>What is the best way to avoid this kind of attack on the mail server?</p>

<p><code>Mar 22 21:47:46 mail dovecot: imap-login: Disconnected (auth failed, 1 attempts in 9 secs): user=&lt;andree_fontaine@mydomain&gt;, method=PLAIN, rip=60.173.149.237, lip=172.16.16.1, TLS, session=&lt;Ds/cmwpoCwA8rZXt&gt;
Mar 22 21:47:52 mail dovecot: imap-login: Disconnected (auth failed, 1 attempts in 9 secs): user=&lt;camilla_blanc@mydomain&gt;, method=PLAIN, rip=61.185.139.72, lip=172.16.16.1, TLS, session=&lt;dsk9nApoggA9uYtI&gt;
Mar 22 21:53:41 mail dovecot: imap-login: Disconnected (auth failed, 1 attempts in 11 secs): user=&lt;emerick_christianne@mydomain&gt;, method=PLAIN, rip=122.117.63.83, lip=172.16.16.1, TLS: Disconnected, session=&lt;eUvvsApoFAB6dT9T&gt;</code></p>

<p>those ip addresses after <strong>rip=</strong> are from China</p>

<p>this is a fragment of /etc/postfix/main.cf:</p>

<pre><code>myhostname = mail.mydomain
alias_maps = hash:/etc/aliases
alias_database = hash:/etc/aliases
mydestination = mail, localhost.localdomain, localhost&lt;br&gt;
relayhost =
mynetworks = 127.0.0.0/8 [::ffff:127.0.0.0]/104 [::1]/128 192.168.0.0/24 172.16.16.0/24
mailbox_size_limit = 0
</code></pre>

<p>I have tried to solve the problem by capturing the ip address and adding it to the ip tables rules, but the intrusion continues with new ip addresses, all from China</p>

<p><code>cat /var/log/syslog |grep 'Disconnected (auth failed' |awk -F'=' '{print $4}' |sed 's/.\{5\}$//' |sort -u |awk '{print ""$IPTABLES -I INPUT -s "" $0 ""/16 -j DROP""}'</code></p>

<p>the output of this command is like this</p>

<pre><code>$IPTABLES -I INPUT -s 111.75.167.157/16 -j DROP
$IPTABLES -I INPUT -s 112.101.220.75/16 -j DROP
$IPTABLES -I INPUT -s 112.16.214.182/16 -j DROP
$IPTABLES -I INPUT -s 112.26.82.52/16 -j DROP
$IPTABLES -I INPUT -s 114.104.158.172/16 -j DROP
$IPTABLES -I INPUT -s 116.248.41.190/16 -j DROP
$IPTABLES -I INPUT -s 116.248.41.55/16 -j DROP
$IPTABLES -I INPUT -s 117.35.207.102/16 -j DROP
$IPTABLES -I INPUT -s 118.112.180.237/16 -j DROP
</code></pre>
","<email><iptables><postfix>","2018-03-23 02:45:33"
"835561","CentOS 7.3 stuck when reboot after disabling SELinux and editing the fstab","<p>I disabled SELinux and edited the <code>fstab</code> to mount an LVM partition. After that I restart the machine but stuck in this process </p>

<p><img src=""https://i.sstatic.net/1azRk.png"" alt=""enter image description here""></p>

<p>The operating system is CentOS 7.3.</p>

<p>Does anyone have same issue and have the solution?</p>
","<linux><centos7><lvm><selinux>","2017-03-01 07:25:58"
"904180","Azure Free Services Linux Virtual Machine","<p>Am a new user to Azure, trying to learn Azure.
The first thing I tried is Azure free services and I have created 3 Linux Virtual machines using Ubuntu 16.04 LTS in 3 different resource groups. Am not sure its strange or that's how it is all 3 Virtual machines getting same Private IP's. Please see the Pics attached.</p>

<ol start=""2"">
<li>Also, on above created resource groups, Azure is not allowing to create more virtual machines from free services.</li>
</ol>

<p>My question is, is this is how Azure is for free services or I am doing it wrong or its a Bug which Azure needs to fix.</p>

<p>Please help.<a href=""https://i.sstatic.net/OVhat.jpg"" rel=""nofollow noreferrer"">Private_ip_1</a><a href=""https://i.sstatic.net/R6efe.jpg"" rel=""nofollow noreferrer"">Private_ip_2</a><a href=""https://i.sstatic.net/GfQPT.jpg"" rel=""nofollow noreferrer"">Private_ip_3</a><a href=""https://i.sstatic.net/W8Cx9.jpg"" rel=""nofollow noreferrer"">Resource_group</a></p>
","<azure><virtual-machines><linux-networking>","2018-03-23 10:09:17"
"904204","SSH server grinding to a halt when users grep large files - high NET_RX and high softirqs - How to limit?","<p>I have a server running fedora 14 - Linux-2.6.35 with a 1Gbps NIC. It has NFSv3 mounts supplied by an Isilon server.</p>

<p>It freezes whenever any user works with a large file(grep or similar) that exists on the NFS server. I could clearly see that just before the server freeze, </p>

<ol>
<li>The network usage hits close to 1 Gbps, </li>
<li>interupts from NET_RX overwhelms the cpu cores, and </li>
<li>High amounts of soft_irqs. </li>
</ol>

<p>Any ideas to approach these stalls?</p>

<p>ifconfig on the NIC shows </p>

<pre><code>[root@interactive ~]# ifconfig eth1 &lt;redacted&gt;
eth1      Link encap:Ethernet  HWaddr 00:00:00:00:00:00  
      inet addr:xx.yy.zz.aa  Bcast:A.B.C.255  Mask:255.255.252.0
      inet6 addr: aaaa::bbb:ccc:ddd:eee/64 Scope:Link
      UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
      RX packets:685902495 errors:6256 dropped:28226 overruns:0 frame:6256
      TX packets:661268729 errors:0 dropped:0 overruns:0 carrier:0
      collisions:0 txqueuelen:1000 
      RX bytes:973006411081 (906.1 GiB)  TX bytes:146154890021 (136.1 GiB)
      Interrupt:19 Memory:d8000000-d8012800 
</code></pre>

<p>Are the Errors too high?</p>

<p>Is there a way to limit a SSH user's capability to saturate the network interface?</p>
","<linux><networking><ssh><performance><nfs>","2018-03-23 12:16:09"
"978106","How to use asterisk in a shell program","<p>I want to use Asterisk in a shell Linux program more or less like this :</p>

<pre><code>asterisk -r &amp;&amp; console dial 101
</code></pre>

<p>How can I do that ?
Thank you.</p>
","<linux><shell><asterisk>","2019-08-06 08:38:03"
"978142","NS change but MX points to domain","<p>I will be changing the nameservers for a client to my server for a new website. I will be using Cloudflare as it also adds SSL.</p>

<p>The problem is that he has an MX record pointing to <code>mail.example.com</code>. If I change the <code>NS</code> records for that domain, and point the <code>A</code> record of <code>example.com</code> to my server, the <code>MX</code> record will also point to my server rather than the old server, correct?</p>

<p>How can I get Cloudflare to point the mail back to his old server?</p>

<p>Here are the DNS settings I will be using on Cloudflare:</p>

<pre><code>example.com. A 192.168.0.89
www CNAME my.server.example
example.com. MX mail.example.com (priority 10)
</code></pre>
","<domain-name-system><mx-record><ns-record>","2019-08-06 12:52:27"
"904633","site is not accessable on single ip","<h3>OUR SIDE:</h3>
<p>The website is hosted by WordPress on DreamHost server. There are not any filters in WordPress and I can't see any IP blocks on Dreamhost server.</p>
<h3>CLIENT SIDE:</h3>
<p>Client has a single internet connection with 3 public IP. There are no firewalls or filters. Routers are reset to default. All the computers are fine.</p>
<h3>PROBLEM:</h3>
<p>Out of these three IP, our website is not accessible from one IP. In fact, not just website, email or anything that is hosted by Dreamhost is not accessible. I can't ping, nslookup, even can't ping IP.</p>
<p>But with the same line with another IP on the same PC works fine. Already contacted ISP. They didn't resolve it. Where might be the problem?</p>
","<router><wordpress><internet><isp><dreamhost>","2018-03-26 15:11:00"
"836017","How to export zpool from alternate disk slice?","<p>I need some help in recovering ZFS pool. Here is scenerio. There are two disks -
c0t0d0 - This is good disk. I cloned it from other server and boot server from this disk.
c0t1d0 - This is original disk of this server, having errors. I am able to mount it on /mnt. So that I can copy required data from this to c0t0d0</p>

<p>Below pool is not imported yet and copy of another server, from where I have cloned</p>

<pre><code># zpool import
  pool: zplctpool
    id: 11623878967666942759
 state: DEGRADED
status: The pool was last accessed by another system.
action: The pool can be imported despite missing or damaged devices.  The
        fault tolerance of the pool may be compromised if imported.
   see: http://www.sun.com/msg/ZFS-8000-EY
config:

        zplctpool          DEGRADED
          mirror      DEGRADED
            c0t0d0s7  FAULTED  corrupted data
            c0t0d0s7  ONLINE
</code></pre>

<p>I do not want this zplctpool, it can be deleted. Instead, I want zplctpool, which is sitting on c0t1d0s7</p>

<p>Regards</p>
","<zfs><solaris-10><zpool>","2017-03-03 06:26:39"
"904701","sending email to Postfix SMTP: connection lost in middle of transaction without VPN - IPv6","<p>I connected an Postfix/Dovecot smtp server through an OpenVPN tunnel to an VPS to the Internet. When i send short (like 3-4 sentences) emails using my regular internet connection to it (Port 587), it works fine. But if my emails are longer, Thunderbird throws an connection lost in middle of transaction error. If i connect my machine to the same VPN&amp;VPS, sending works just fine. All machines are running Dual Stack, also the the VPS &amp; OpenVPN tunnel (using tun, no tap). Receiving long emails from other email-servers works just fine. How can i fix the issue? All servers are running on Debian 9, Client is running Win10.<br>
Thi issue arises on IPv6 only. So far i've only tested it from my home network. tun-mtu is set to 1280 now. tracepath correctly recognizes the set MTU.<br>
Here are my ip6tables:  </p>

<pre><code># Generated by ip6tables-save v1.6.0 on Tue Mar 27 23:51:39 2018
*filter
:INPUT ACCEPT [5:608]
:FORWARD ACCEPT [1217:402964]
:OUTPUT ACCEPT [0:0]
-A INPUT -i lo -j ACCEPT
-A INPUT -p ipv6-icmp -j ACCEPT
-A INPUT -i tun0 -j ACCEPT
-A INPUT -i eth0 -m state --state RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -i tun0 -j ACCEPT
-A FORWARD -m state --state NEW,RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -i tun+ -o eth0 -m state --state NEW,RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -i eth0 -o tun+ -m state --state NEW,RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -p ipv6-icmp -j ACCEPT
-A FORWARD -s [MYSUBNET]/64 -j ACCEPT
-A OUTPUT -o lo -j ACCEPT
-A OUTPUT -p ipv6-icmp -j ACCEPT
-A OUTPUT -o tun0 -j ACCEPT
-A OUTPUT -j ACCEPT
COMMIT
</code></pre>
","<postfix><openvpn><ipv6>","2018-03-26 23:25:25"
"836070","Local DNS server without specify domain name","<p>I am hosting a local DNS server on a Synology with domain name <em>example.com</em></p>

<p>My configuration is the most basic possible:</p>

<pre><code>example.com          NS   ns.example.com
ns.example.com       NS   8.8.8.8
server1.example.com   A    192.168.0.10
</code></pre>

<p>Currently to access <em>server1</em>, my users have to write <em><a href=""http://server1.example.com"" rel=""nofollow noreferrer"">http://server1.example.com</a></em> or <code>ping server1.example.com</code></p>

<p>What kind of configuration should I do in my DNS to access by this way <em><a href=""http://server1"" rel=""nofollow noreferrer"">http://server1</a></em> or ping <em>server1</em> without having to specify the domain name?</p>
","<domain-name-system>","2017-03-03 10:52:32"
"836156","how to copy file from local to remote via ssh","<p>this is how to shh to server</p>

<p><code>ssh -p 2222 thatilike@192.185.21.105</code></p>

<p>I want to copy file <code>logo.png</code> to <code>public_html</code> folder on server</p>

<p>This is what I tried</p>

<p><code>scp ./logo.png -p 2222 thatilike@192.185.21.105:/public_html/</code></p>

<p>Don't know why not work</p>
","<ssh><scp>","2017-03-03 17:55:46"
"904878","Apache Timeout directive correct but requests never timeout (not working)","<p>I am having trouble getting Timeout directive to work, I have reproduced the issue in the below instructions and official Docker container. The server is using the standard MPM prefork model with default configs. My understanding is that the Timeout should return a 500 error after the timeout or some sort of 4xx or 5xx response code but it doesn't. Is my understanding incorrect?</p>

<p><strong>Setup:</strong>  </p>

<pre><code>mkdir /tmp/test; cd /tmp/test
echo ""&lt;?php sleep(10);"" &gt; index.php
docker run -d -p 80:80 --name apache-php -v ""/tmp/test"":/var/www/html php:5.6-apache
time curl http://localhost/ # should take 10 seconds
</code></pre>

<p><strong>Now, lower timeout to 1 second:</strong>  </p>

<pre><code>docker cp apache-php:/etc/apache2/apache2.conf . # Copy to host, since container doesn't have editor.  
echo ""Timeout 1"" &gt;&gt; apache2.conf # Set Timeout to 1, default is 300.  
docker cp apache2.conf apache-php:/etc/apache2/apache2.conf # Copy conf file back to container  
docker exec -it apache-php /bin/bash
service apache2 reload
exit # exit docker container. 
</code></pre>

<p><strong>Test for error:</strong>  </p>

<pre><code>time curl http://localhost/ # Here is where I expect a timeout error, but it works just the same as before and takes 10 seconds.
</code></pre>

<p>p.s. I have read that AcceptFilter can affect things until the request gets past it. We do have the below config as well:</p>

<pre><code>AcceptFilter http none
AcceptFilter https none
</code></pre>
","<apache-2.4><php5>","2018-03-27 21:01:41"
"904915","lost connection with smtp.strato.de[81.169.145.133] while receiving the initial server greeting","<p>49BAC4185F     3057 Sat Mar 24 06:02:32  MAILER-DAEMON
(delivery temporarily suspended: lost connection with smtp.strato.eu[81.169.145.133] while receiving the initial server greeting)</p>

<p>3400 mails in queue</p>

<p>i don't get a solution, please anybody guide me to solve the issue</p>

<p>mailbox_size_limit = 51200000</p>

<p>but i have edited the mailbox_size_limit in main.cf as 102100000</p>
","<postfix><smtp><email-server><linux-networking>","2018-03-28 03:57:00"
"978549","How do I get the public DNS for an Amazon Workspace (virtual desktop)?","<p>Running nginx reverse proxy on an Amazon Workspace (virtual desktop).  How can I get a public DNS or a stable public ip address (like an elastic IP?)</p>
","<nginx><amazon-web-services><reverse-proxy><virtual-machines>","2019-08-08 23:21:51"
"904963","Server 2008 SP2 (Non R2) TLS 1.1 / 1.2","<p>I've been asked to look at a server for a customer as they are having trouble getting TLS 1.1 or 1.2 support to work. Their server integrates with a payment provider who will be requiring 1.2 soon.</p>

<p>According to Microsoft, this should be supported since they released update 4019276. <a href=""https://support.microsoft.com/en-us/help/4019276/update-to-add-support-for-tls-1-1-and-tls-1-2-in-windows"" rel=""nofollow noreferrer"">https://support.microsoft.com/en-us/help/4019276/update-to-add-support-for-tls-1-1-and-tls-1-2-in-windows</a></p>

<p>However, Windows Update does not seem to list this update, and attempting to install manually just returns ""The update does not apply to your system""</p>

<p>I'm at a loss as to why this update refuses to install.</p>

<p>Edit: Answering some questions from the first answer</p>

<ul>
<li>The update does not appear to be installed unless there's something
broken causing it to not appear in Programs/Features</li>
<li>The requirements for the update list only SP2 which is installed</li>
<li>The customer is using IISCrypto to manage settings but this does not list 1.1 or 1.2 as an option.</li>
</ul>
","<windows-server-2008><ssl>","2018-03-28 10:09:11"
"904978","Ansible: Dynamic list creation","<p>I am trying to create a list on the fly during the playbook execution, as per the below code the list is not getting updated</p>

<pre><code>- debug: var=builds[{{item}}]
  with_sequence: start={{ exactlen }} end={{ buildslen }}
- set_fact: upg_version_list = upg_version_list +  builds[{{item}}]
  with_sequence: start={{ exactlen }} end={{ buildslen }}
</code></pre>

<p>Here builds is a list, exactlen &amp; buildslen are the index value of the list.</p>

<p>If i display the upg_version_list it has empty value</p>
","<ansible>","2018-03-28 11:41:01"
"905058","nginx URL rewriting regex when URL contains a specific word but not other","<p>Please consider the following URLs.</p>

<ol>
<li><a href=""https://products.example.com/family/child"" rel=""nofollow noreferrer"">https://products.example.com/family/child</a></li>
<li><a href=""https://products.example.com/products/family/child"" rel=""nofollow noreferrer"">https://products.example.com/products/family/child</a></li>
<li><a href=""https://products.example.com/family/children"" rel=""nofollow noreferrer"">https://products.example.com/family/children</a></li>
<li><a href=""https://products.example.com/products/family/children"" rel=""nofollow noreferrer"">https://products.example.com/products/family/children</a></li>
</ol>

<p>Among the four, I need to redirect one as follow: <a href=""https://products.example.com/family/children"" rel=""nofollow noreferrer"">https://products.example.com/family/children</a></p>

<p>Which means that I want to redirect the URLs that do not contain the word <code>/products/</code> and <code>/children</code>. If the URL does not contain the word <code>/products/</code> but contain <code>/child</code> at the end, I need to replace the <code>/child</code> with <code>/children</code>. </p>

<p>UPDATE 1: I have 2 problems as below.</p>

<ol>
<li>Identify the URL that are to be substituted. </li>
<li>Modify the identified URL as required.</li>
</ol>

<p>I can identify the URL with <a href=""https://regex101.com/r/54LotX/1"" rel=""nofollow noreferrer"">this regex</a> but not able to substitute.</p>

<p><a href=""https://regex101.com/r/54LotX/3"" rel=""nofollow noreferrer"">Update 2</a>: I think I found a way to identify:  <code>^(?!.*\/products(?:\/))(?:(.*)\/|$)?(?:child)$</code> and substitute: <code>$1/children</code></p>

<p>Next problem is: the regex works with option <code>/m</code> only. Can this be modified to work without it as:</p>

<pre><code>rewrite ^(?!.*\/products(?:\/))(?:(.*)\/|$)?(?:child)$ $1/children permanent;
</code></pre>
","<nginx><regex><url><pcre>","2018-03-28 18:07:35"
"905319","want to know server reboot via manual command or H/W switch","<p>I have the linux centos 7 server in datacenter. Server suddently rebooted  but we didn't restart the server and datacenter people also claim that he didn't reboot the server.</p>

<p>Now I want to know how server suddently restared ?? </p>

<p>Is anybody run linux command reboot via command prompt or anybody have have directly press the reboot switch in datacenter. </p>

<p>I checked /var/log/message but not able to conculed anything. </p>

<p>Please guid how I can identified via log that its hard booted or via command ???</p>
","<linux><reboot>","2018-03-30 10:31:00"
"905335","unable to jail ssh fail2ban 0.9.6","<p>I am on Centos 6.9 with fail2ban 0.9.6. I have the below SSH jail but after restarting fail2ban it is not enabled when running fail2ban-client status.</p>

<p>The conf I used is /etc/fail2ban/jail.local. I have multiple jails in there.</p>

<pre><code>[ssh]
enabled = true
filter = sshd
action = iptables-multiport[name=SSH, port=""2222"", protocol=tcp]
maxretry = 3
logpath  = /var/log/secure
bantime = 86400
</code></pre>

<p>Any help much appreciated.</p>

<p>Thanks.</p>

<p><strong>UPDATE</strong></p>

<p>I got it resolved after all.. It was the space in front of the directives. </p>
","<ssh><centos6><fail2ban>","2018-03-30 12:37:01"
"978977","Global authorized_keys associated with a user?","<p>Is it possible to have a global authorized_keys file that associates each user to their own public key?</p>

<p>For example, I would like to create multiple user accounts by their Github user name and public key. Instead of iterating through each user &amp; key, can I dump them to a single global authorized_keys file? e.g. githubusername: publickey</p>
","<ssh><ssh-keys><user-management>","2019-08-12 20:28:53"
"836652","Mail undeliverable giving ""Diagnostic-Code: smtp; 553 Relaying disallowed"" error","<p>I am running ubuntu 16.04 on an AWS EC2 instance. I've setup LAMP and configured apache to run multiple domains on the server. I plan to run a small number of low traffic websites on the same box.</p>

<p>I've changed name servers for one domain to point to the route 53 setup that I've configured. I've setup email with a provider zoho. I'm trying to route email through this provider.</p>

<p>An elastic ip address has been assigned to the EC2 instance and I've created records inside my hosted zone like so:</p>

<pre><code>domain.co.uk.   A   {elastic-ip-address}    

domain.co.uk.   MX  10 mx.zoho.com 20 mx2.zoho.eu
domain.co.uk. NS    
ns-xxx.awsdns-xx.com. 
ns-xxx.awsdns-xx.co.uk. 
ns-xxx.awsdns-xx.net. 
ns-xxxx.awsdns-xx.org.

domain.co.uk. SOA   ns-xxx.awsdns-xx.com. awsdns-hostmaster.amazon.com. x xxxx xxx xxxxxxx xxxxx

www.domain.co.uk. A {elastic-ip-address}

{unique-zoho-code}.domain.co.uk. CNAME zmverify.zoho.eu
</code></pre>

<p>To my knowledge I've followed all instructions laid out by Zoho, this is reflected in the records above. Although I've not correctly set the spf and dkim records, I don't think this is the cause of the issue I'm facing since the advice mentioned that these records would help improve deliverability, rather than enable it all together. I'm no expert on this though.</p>

<p>Apache seems to be configured correctly as after changing my hosts file I can see each website running.</p>

<p>When I try to email the address setup with zoho such as user@domain.co.uk, the email is returned un-delivered with the following error:</p>

<pre><code>Final-Recipient: rfc822; user@domain.co.uk
Action: failed
Status: 5.0.0
Remote-MTA: dns; mx.zoho.com. ({ip-address-B}, the server for the domain domain.co.uk.)
Diagnostic-Code: smtp; 553 Relaying disallowed
</code></pre>

<p>Zoho have sent me an automated email stating that the current MX records are as follows:</p>

<p>Priority    MX Records  IP Address
20  mx2.zoho.eu.    {ip-address-A}
10  mx.zoho.com.    {ip-address-B}</p>

<p>I don't recognise either of the ip addresses mentioned although the second is the same as the ip address mentioned in the undeliverable message.</p>

<p>Is it possible to configure records in this way even though I'm running multiple domains on through the one version of apache?</p>

<p>How can I correct my setup to enable receiving email?</p>
","<email><email-server><email-bounces>","2017-03-06 20:48:13"
"905379","HP DL360 G5. Doesn't start if power cable plugged in PSU1","<p>I bought an used server. When I plugged in a first PSU (the one near sidewall) a power cable, some fans turned on, a led on the front panel light orange, on mobo green leds works. When I press a power button nothing happens. 
When I plugged off the cable from the first PSU and plugged in the second PSU, some fans turned on, led on the front panel light orange, on mobo green leds works. If I press the power button the server boots with a warning, that power is not redundant.
Is PSU 1 dead?</p>
","<hardware><hp><hp-proliant><power-supply-unit>","2018-03-30 18:39:35"
"836662","How many anycast ips can be created per /24 net?","<p>How many anycast ips can I create with one ipv4 /24 net? I know I need a whole /24 net to do anycast, but how many of those ips can I do with one? With a anycast ip address I mean one address for doke servers.</p>

<p>Here an example what I mean:</p>

<pre><code>What I got:
  - 1.1.1.0/24
  - 6 Servers (a-f)

What I want to do:
  - 1.1.1.1 is for server a, b and c
  - 1.1.1.2 is for server d, e and f
</code></pre>

<p>I searched a long time but I couldnt find an answer. </p>
","<ipv4><anycast>","2017-03-06 22:03:25"
"764745","I can not start HTTP server Admin on AS400","<p>The error message is... </p>

<pre><code> CPF9898 Escape 40 19/03/16 11:11:55.552230 QLWISVRA QSYS *STMT QLWIIHSMOD QHTTPSVR *STMT 

 From module . . . . . . . . : QLWIMSG 5761SS1 V6R1M0 080215 Job Log MLCNAS05 19/03/16 11:11:55 Page 2 

 Job name . . . . . . . . . . : ADMIN 

 User . . . . . . : QTMHHTTP 

 Number . . . . . . . . . . . : 022548 

 Job description . . . . . . : QZHBHTTP 

 Library . . . . . : QHTTPSVR 

 MSGID TYPE SEV DATE TIME FROM PGM LIBRARY INST TO PGM LIBRARY INST

 From procedure . . . . . . : sendStartStopMessage__10LwiMessageFPcsN31 

 Statement . . . . . . . . . : 55 

 To module . . . . . . . . . : MOD_IBMLWI

 To procedure . . . . . . . : startLwiServer__FP10apr_pool_tP10lwi_conf_t 

 Statement . . . . . . . . . : 105 

 Message . . . . : INTEGRATED WEB APPLICATION SERVER Admin1 FAILED TO START. SERVER Admin1 NOT FOUND IN REGISTRY. 

 Cause . . . . . : This message is used by application programs as a general escape message. 
</code></pre>

<p>Where the Admin1 configure? 
 How can I resolve it?</p>
","<ibm-midrange><http-server>","2016-03-19 05:08:35"
"905484","Apache2 default page displayed after disabling all the sites/VirtualHosts","<p>The <code>Apache2 Debian Default Page</code> is displayed when I navigate my browser to a domain name pointing to my web server even though I have disabled all the sites/VirtualHosts on my web server. What I expect to see is a ""Cannot connect to the server"" message.</p>

<p>In other words: Why is Apache displaying the default page when no VirtualHosts are enabled.</p>

<p>The browser is not loading the page from the cache.</p>

<p>I'm using <code>Apache 2.4.33</code></p>

<p><strong>Edit (when closed as duplicate):</strong></p>

<p><a href=""https://serverfault.com/questions/662262/apache-accepting-requests-to-other-servers"">This question</a> is not a duplicate because their VirtualHost files are not disabled.</p>

<p><a href=""https://serverfault.com/a/520201/462994"">This answer</a> does answer my question but the answer is to a broader question. I suggest it would be useful to have a similar answer here since my question is more direct and would help others who are wondering the same thing.</p>
","<amazon-web-services><apache-2.4><ubuntu-16.04>","2018-04-01 04:52:17"
"905502","ACPu cache with a cron job","<p>My sites homepage first time access load time is between 5-8 seconds.</p>

<p>Once I'm on my website all pages load fast. And if I access the homepage directly again it will load instantly.</p>

<p>I set up a <code>warmcache.sh</code> script to have just the hompage cached.</p>

<p>Looking at my <code>apc.php</code> dashboard, the cache is flushed after some minutes after I have accessed my site. Yet the load time is still low, even after 5 minutes accessing my homepage. But I don't know for how long?</p>

<p>How often should I set the <code>warmcachach.sh</code> to run in a cron job to keep the load time low for the homepage?</p>

<p>The question is that I don't know how long it's kept in cache, because the <code>apc.php</code> dashboard indicates that cache has been flushed after some minutes and the only change in the diagram shows:</p>

<blockquote>
  <p>Used: 32.2 KBytes (0.1%)</p>
</blockquote>
","<cron><apc>","2018-04-01 10:56:18"
"764878","How is the request using wordpress site to requests config files?","<p>I was looking through my apache <code>error.log</code> and I noticed there were a few attempts made to get <code>wp-config.php</code> which failed. </p>

<p>Strangely it said the requested was the site itself.</p>

<p>How is the attacker/bot doing this?</p>

<p>Here is the log [Replaced actual url with example]:</p>

<pre><code>[Sun Mar 20 09:06:40 2016] [error] [client 105.228.84.134] PHP Fatal error:  Call to undefined function _deprecated_file() in /var/www/example/wp-includes/rss-functions.php on line 8, referer: http://example.co.za/
[Sun Mar 20 09:06:41 2016] [error] [client 105.228.84.134] File does not exist: /var/www/example/wp-content/debug.log, referer: http://example.co.za/
[Sun Mar 20 09:06:42 2016] [error] [client 105.228.84.134] File does not exist: /var/www/example/wp-config.php~, referer: http://example.co.za/
[Sun Mar 20 09:06:42 2016] [error] [client 105.228.84.134] File does not exist: /var/www/example/#wp-config.php#, referer: http://example.co.za/
[Sun Mar 20 09:06:42 2016] [error] [client 105.228.84.134] File does not exist: /var/www/example/wp-config.php.save, referer: http://example.co.za/
[Sun Mar 20 09:06:44 2016] [error] [client 105.228.84.134] File does not exist: /var/www/example/wp-config.old, referer: http://example.co.za/
[Sun Mar 20 09:06:42 2016] [error] [client 105.228.84.134] File does not exist: /var/www/example/wp-config.php.swp, referer: http://example.co.za/
[Sun Mar 20 09:06:42 2016] [error] [client 105.228.84.134] File does not exist: /var/www/example/wp-config.php.swo, referer: http://example.co.za/
[Sun Mar 20 09:06:42 2016] [error] [client 105.228.84.134] File does not exist: /var/www/example/.wp-config.php.swp, referer: http://example.co.za/
[Sun Mar 20 09:06:42 2016] [error] [client 105.228.84.134] File does not exist: /var/www/example/wp-config.bak, referer: http://example.co.za/
[Sun Mar 20 09:06:42 2016] [error] [client 105.228.84.134] File does not exist: /var/www/example/wp-config.php_bak, referer: http://example.co.za/
[Sun Mar 20 09:06:42 2016] [error] [client 105.228.84.134] File does not exist: /var/www/example/wp-config.php.bak, referer: http://example.co.za/
[Sun Mar 20 09:06:44 2016] [error] [client 105.228.84.134] File does not exist: /var/www/example/wp-config.save, referer: http://example.co.za/
[Sun Mar 20 09:06:44 2016] [error] [client 105.228.84.134] File does not exist: /var/www/example/wp-config.php.old, referer: http://example.co.za/
[Sun Mar 20 09:06:45 2016] [error] [client 105.228.84.134] File does not exist: /var/www/example/wp-config.php.orig, referer: http://example.co.za/
[Sun Mar 20 09:06:45 2016] [error] [client 105.228.84.134] File does not exist: /var/www/example/wp-config.php.original, referer: http://example.co.za/
[Sun Mar 20 09:06:45 2016] [error] [client 105.228.84.134] File does not exist: /var/www/example/wp-config.orig, referer: http://example.co.za/
[Sun Mar 20 09:06:45 2016] [error] [client 105.228.84.134] File does not exist: /var/www/example/wp-config.original, referer: http://example.co.za/
[Sun Mar 20 09:06:47 2016] [error] [client 105.228.84.134] File does not exist: /var/www/example/wp-config.txt, referer: http://example.co.za/
</code></pre>
","<apache-2.2><wordpress>","2016-03-20 08:13:50"
"905556","How to Find and delete File Older than 3 years?","<p>I'm new with server maintenance job.</p>

<p>Now I must find and delete .JPG File older than 3 years. on linux terminal</p>

<p>I have googled and found a script like this</p>

<pre><code>find /path/to/files* -mtime +365 -exec rm {} \;

or

find /path/to/files* -mtime +365 -delete;
</code></pre>

<p>I have tried this</p>

<pre><code>find /path/to/files* -mtime +1095 -exec rm {} \;

or

find /path/to/files* -mtime +1095 -delete;
</code></pre>

<p>But it didn't work, I think my file is too big to be found.</p>

<p>Can anyone help me to fix this..?</p>

<p>Or maybe there's another way?</p>

<p>I would appreciate your answer a lot.</p>

<p>Thanks</p>
","<linux><terminal>","2018-04-02 01:22:29"
"979199","Subdomain A record Resolves, but Nameservers won't","<p>I have a domain xyz.com, which resolves to an IP: 192.168.1.128</p>

<pre><code>Nameservers: ns1.named.com
             ns2.named.com
</code></pre>

<p>Now I have a subdomain, webmail.xyz.com which points to 192.168.1.129</p>

<p>The zone file xyz.con.zone records is provided below:</p>

<pre><code>;; A Records
xyz.com.            IN  A   192.168.1.128
mail.xyz.com.       IN  A   192.168.1.129
webmail.xyz.com.    IN  A   192.168.1.129

;; CNAME Records
www.xyz.com.    IN  CNAME   xyz.com.

;; NS Records
ns1.named.com.    IN    NS  named.com
ns2.named.com.    IN    NS  named.com
</code></pre>

<p>P.S: It's for an Intranet which is completely disconnected from the Internet. </p>

<p>The NS as well as the A records of the main domain xyz.com resolves when checking with any DNS tools like nslookup or dig.</p>

<p>But for mail.xyz.com and webmail.xyz.com the A records are resolved properly, but the NS records doesn't seem to resolve.</p>

<p>Any guess what is the issue ??? </p>
","<domain-name-system><bind><subdomain><nameserver>","2019-08-14 07:38:28"
"764906","GET COMMAND in HTTP Request","<p>I cannot use GET command to get the web site body content only. For example:</p>

<pre><code>[root@mrtg home]# echo -e ""GET / HTTP/\r\nHost: test.com:80\r\n\r\n"" | nc 192.168.1.201 80
HTTP/1.1 200 OK
Date: Sun, 20 Mar 2016 13:45:49 GMT
Server: Apache/2.2.15 (CentOS)
Last-Modified: Sun, 20 Mar 2016 10:28:08 GMT
ETag: ""6068a-10-52e786faf8f79""
Accept-Ranges: bytes
Content-Length: 16
Connection: close
Content-Type: text/html; charset=UTF-8

test.com-server
[root@mrtg home]#
</code></pre>

<p>If I use this command <code>echo -e ""GET / /\r\nHost: test.com:80\r\n\r\n"" | nc 192.168.1.201 80</code></p>

<p>I can get the default host body content only. I have 3 x Virtualhost in my Apache2.4 server.</p>

<p>Please advise.</p>
","<apache-2.4><http><host>","2016-03-20 14:14:58"
"905594","CNAME query fails, but A query returns CNAME","<p>I'm running into the following situation and am wondering how to properly resolve it:</p>

<p>A nameserver is configured to <strong>not</strong> return CNAME records (not under my control), only if an A record is requested (for the same name) the CNAME is returned in the additional section.</p>

<p><strong>Question 1</strong>: why would someone configure a DNS server to act like this? It doesn't make sense to me? (asked them several times, never got an answer)</p>

<p>Practical problem: When we resolve a hostname, say <code>testa.hosta.com</code>, we query an A record. In the response we get:</p>

<pre><code># query for A of `hosta.com`
testa.hosta.com CNAME testb.hostb.com  TTL=5
testb.hostb.com A     1.2.3.4          TTL=60
</code></pre>

<p>So we're storing both records in our cache. Since we didn't get the A record we were looking for, we now try a CNAME record. But this one is in the cache, it points to <code>testb.hostb.com</code>. Now trying an A record for <code>testb.hostb.com</code>, which is also in the cache, so we finally resolve to <code>1.2.3.4</code>. So all is fine up till now.</p>

<p>After 5 seconds the TTL of the CNAME expires. Because the data is stale, we need to refresh it. So we query for <code>testa.hosta.com</code> CNAME, and this delivers an empty response. Now our lookup is broken...</p>

<p><strong>Question 2</strong>: is this lookup logic faulty? I went through many rfc's to figure it out, but couldn't find anything. What would the proper lookup order be?</p>

<p>Any help is greatly appreciated.</p>
","<domain-name-system><cname-record>","2018-04-02 11:06:12"
"764944","Config to allow SQL Server to be connected through CloudFlare?","<p>I have already known the risk of exposing SQL Server database on the Internet, but please look at my situation: I have two instances of SQL Server: production (default instance SQLEXPRESS) and dev (SQLEXPRESSDEV). I have disabled both <code>Administrator</code> account of Windows and <code>sa</code> accounts of SQL Server. The server also has some running websites. <em>EDIT</em>: The server is running MS Windows 2008 R2 and both instances are SQL Server 2014 Express.</p>

<p>The production instance is kept private and can only be logged in with Windows Authentication. Now I want to expose the Dev instance, but I do not want to expose the IP Address. Therefore, I was using Cloudflare service for the websites, and is intend to create DNS record for <code>sql.mydomain.com</code> for accessing SQL Server.</p>

<p>However, I cannot figure out how. <a href=""https://support.cloudflare.com/hc/en-us/articles/200169156-Which-ports-will-CloudFlare-work-with-"" rel=""nofollow noreferrer"">According to CloudFlare</a>, there are those ports that can be forwarded, and I am intending to use port 2086, knowing I will never need that port. I have tried to set the TCP/IP port in both SQL Native Client Configuration and SQL Server Network Configuration to 2086, adding that port to both Inbound and Outbound of Firewall rule. In my own computer, I tried to connect using <code>sql.domain.com,2086</code>, but I receive this message:</p>

<blockquote>
  <h2>TITLE: Connect to Server</h2>
  
  <p>Cannot connect to sql.domain.com,2086\SQLEXPRESSDEV.</p>
  
  <p>------------------------------ ADDITIONAL INFORMATION:</p>
  
  <p>A connection was successfully established with the server, but then an
  error occurred during the pre-login handshake. (provider: TCP
  Provider, error: 0 - An existing connection was forcibly closed by the
  remote host.) (Microsoft SQL Server, Error: 10054)</p>
  
  <p>For help, click:
  <a href=""http://go.microsoft.com/fwlink?ProdName=Microsoft%20SQL%20Server&amp;EvtSrc=MSSQLServer&amp;EvtID=10054&amp;LinkId=20476"" rel=""nofollow noreferrer"">http://go.microsoft.com/fwlink?ProdName=Microsoft%20SQL%20Server&amp;EvtSrc=MSSQLServer&amp;EvtID=10054&amp;LinkId=20476</a></p>
  
  <hr>
  
  <p>An existing connection was forcibly closed by the remote host</p>
</blockquote>

<p>Is it because the CloudFlare proxy? Is there anyway I can achieve this (hiding IP and exposing only one instace)? I left the TCP/IP settings on the production instance Disabled. </p>
","<domain-name-system><security><sql-server><forwarding><cloudflare>","2016-03-20 19:59:40"
"764987","Ubuntu: Postgresql not using ram","<p>I have a 4GB 2 core Ubuntu droplet that doesn't seem to properly make use of the RAM and CPU.</p>

<p>I have tried to optimize the settings in <code>postgresql.conf</code> to match the instance with (based on pgtune):</p>

<pre><code>max_connections = 20
shared_buffers = 1GB
effective_cache_size = 3GB
work_mem = 26214kB
maintenance_work_mem = 512MB
checkpoint_completion_target = 0.9
wal_buffers = 16MB
default_statistics_target = 500
</code></pre>

<p>... After inserting this I reloaded the configs and restarted the server.</p>

<p>However, when opening <code>htop</code> the process (which is owned by the <code>postgres</code> user) only uses 1 core and approx. 5% of the available RAM.</p>

<p><a href=""https://i.sstatic.net/Of2EZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Of2EZ.png"" alt=""htop screenshot""></a></p>

<p>Here is <code>top</code></p>

<pre><code>Tasks: 124 total,   2 running, 120 sleeping,   2 stopped,   0 zombie
%Cpu(s): 50,0 us,  0,2 sy,  0,0 ni, 49,8 id,  0,0 wa,  0,0 hi,  0,0 si,  0,0 st
KiB Mem:   4048268 total,  3890804 used,   157464 free,    24012 buffers
</code></pre>

<p>Here is <code>free -m</code></p>

<pre><code>             total       used       free     shared    buffers     cached
Mem:          3953       3800        153       1065         23       3367
-/+ buffers/cache:        409       3544
Swap:            0          0          0
</code></pre>

<p>Any suggestions what's wrong?</p>
","<ubuntu><memory><postgresql>","2016-03-21 01:59:08"
"765005","Postfix email server sending in junk folder","<p>I’m trying to configure a mail server with Ubuntu 14.04 (Postfix+ dovecot, hostname=idcmail.idc.healthcare) but my emails are being received in recipient’s spam folders. I know there are many answers to this question but I have tried almost everything according to answers to no avail. I’ve done following steps</p>

<ol>
<li>PTR record is set for domain</li>
<li>SPF record is set in DNS</li>
<li>DKIM is configured</li>
<li>IP address is checked for black list via spamhaus but it is also clear.</li>
<li>Abuse address is configured at abuse.net</li>
</ol>

<p>The email headers I am receiving in my Gmail email box says that my SPF and DKIM is passed. Below is the header</p>

<pre><code>    Delivered-To: umair.naqvi73@gmail.com
Received: by 10.79.72.1 with SMTP id v1csp1761228iva;
        Sun, 13 Mar 2016 04:34:08 -0700 (PDT)
X-Received: by 10.98.34.205 with SMTP id p74mr21651199pfj.93.1457868848225;
        Sun, 13 Mar 2016 04:34:08 -0700 (PDT)
Return-Path: &lt;bobby@idc.healthcare&gt;
Received: from idcmail.idc.healthcare (115-186-155-204.nayatel.pk. [115.186.155.204])
        by mx.google.com with ESMTPS id l9si8897121pfb.158.2016.03.13.04.34.07
        for &lt;umair.naqvi73@gmail.com&gt;
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Sun, 13 Mar 2016 04:34:07 -0700 (PDT)
Received-SPF: pass (google.com: domain of bobby@idc.healthcare designates 115.186.155.204 as permitted sender) client-ip=115.186.155.204;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of bobby@idc.healthcare designates 115.186.155.204 as permitted sender) smtp.mailfrom=bobby@idc.healthcare;
       dkim=pass header.i=@idc.healthcare
Received: from [192.168.8.4] (unknown [39.47.91.89])
    (using TLSv1.2 with cipher ECDHE-RSA-AES128-GCM-SHA256 (128/128 bits))
    (No client certificate requested)
    by idcmail.idc.healthcare (Postfix) with ESMTPSA id D3C1C80D36;
    Sun, 13 Mar 2016 04:33:30 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=simple/simple; d=idc.healthcare;
    s=dkim; t=1457868812;
    bh=6imp6PpJMW+8Q5OtYGkLm8HP7ihBSc3AC9SAqXIvKYc=;
    h=To:Cc:From:Subject:Date:From;
    b=XaNuuKJ5/rdtcvDuTZKCaNUfluHOFeJgn7m2O6KfZ2gfEI7/ztVMd61b3Bvx3WUZO
     zbLc8vt4UMaaWe5C4c5DqikNjqJ3VSmFqkzFdeS2pkv9e3F+n7LZ+wnINjAUHWFJsU
     bfiq7KpVI9MWveOCAuTbsg4DLluxTBhNrYGvGykY=
To: ab.sattar@idc.net.pk
Cc: rizwanuppal@hotmail.com, doctoruppal@hotmail.com, rizwanuppal@idc.net.pk,
 arfan.qadir@live.com, umair.naqvi73@gmail.com, umair@acetechnologies.biz
From: bobby &lt;bobby@idc.healthcare&gt;
Subject: Email server
Message-ID: &lt;56E55008.1040309@idc.healthcare&gt;
Date: Sun, 13 Mar 2016 16:33:28 +0500
User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:38.0) Gecko/20100101
 Thunderbird/38.6.0
MIME-Version: 1.0
Content-Type: multipart/alternative;
 boundary=""------------010202010701020208060102""

This is a multi-part message in MIME format.
--------------010202010701020208060102
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 7bit
</code></pre>

<p>Dig command returns with following details</p>

<pre><code>; &lt;&lt;&gt;&gt; DiG 9.9.5-3ubuntu0.8-Ubuntu &lt;&lt;&gt;&gt; idc.healthcare
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 37910
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;idc.healthcare.                        IN      A

;; ANSWER SECTION:
idc.healthcare.         583     IN      A       115.186.155.204

;; Query time: 2 msec
;; SERVER: 192.168.1.4#53(192.168.1.4)
;; WHEN: Mon Mar 21 09:40:41 PKT 2016
;; MSG SIZE  rcvd: 59
</code></pre>

<p>MX record test returns following details from <a href=""http://mxtoolbox.com/SuperTool.aspx"" rel=""nofollow noreferrer"">MX record Tester</a></p>

<p>I also tried to test my email for spam using different free services but somehow my email does not delivered to the temporary email addresses provided by these services, however, for gmail, yahoo, hotmail it is working. One of the returned messages to one such service is </p>

<pre><code>Reporting-MTA: dns; idcmail.idc.healthcare
X-Postfix-Queue-ID: 2B41380CF5
X-Postfix-Sender: rfc822; bobby@idc.healthcare
Arrival-Date: Sun, 13 Mar 2016 15:25:24 +0500 (PKT)

Final-Recipient: rfc822; web-Y8ogB7@mail-tester.com
Original-Recipient: rfc822;web-Y8ogB7@mail-tester.com
Action: failed
Status: 4.4.1
Diagnostic-Code: X-Postfix; connect to mail-tester.com[94.23.206.89]:25:
    Connection timed out
</code></pre>

<p>Here my server can ping the IP mentioned in the returned email header i.e. 94.23.206.89.
My current MX record entries are below and i am receiving emails</p>

<pre><code>10  @   @   1 Hour
30  smtp    @   1 Hour
0   smtp.idcmail    @   1 Hour
</code></pre>

<p>Previously i tried with following MX records from which i was receiving emails too</p>

<pre><code>10  @   @   1 Hour
30  @   smtp    1 Hour
0   @   smtp.idcmail    1 Hour
</code></pre>

<p>There must be something i am missing. Any help would be highly appreciated.</p>

<p><strong>UPDATE</strong>: After configuring DKIM, SPF and IP not in black list what else could have been done to avoid emails going in spam. It is a test server and no lists have been sent to audience.</p>

<p><strong>Update:</strong> My reverse DNS is also ok but why am i not able to send to testing services such as mail-tester.com etc but i can send to gmail, yahoo or hotmail (though in spam). It always has a timeout message in logs but i can ping to these address successfully.</p>

<pre><code>    Mar 24 19:58:36 idcmail postfix/qmgr[987]: D059481247: from=&lt;bobby@idc.healthcare&gt;, size=755, nrcpt=1 (queue active)
Mar 24 19:59:06 idcmail postfix/smtp[1796]: connect to mail.isnotspam.com[74.208.174.205]:25: Connection timed out
Mar 24 19:59:36 idcmail postfix/smtp[1796]: connect to mx01.1and1.com[74.208.5.21]:25: Connection timed out
Mar 24 20:00:06 idcmail postfix/smtp[1796]: connect to mx00.1and1.com[74.208.5.3]:25: Connection timed out
Mar 24 20:00:06 idcmail postfix/smtp[1796]: D059481247: to=&lt;ins-f1vtzbop@isnotspam.com&gt;, relay=none, delay=696, delays=606/0.02/90/0, dsn=4.4.1, status=deferred (connect to mx00.1and1.com[74.208.5.3]:25: Connection timed out)
Mar 24 20:03:36 idcmail postfix/qmgr[987]: 12D2A81258: from=&lt;bobby@idc.healthcare&gt;, size=755, nrcpt=1 (queue active)
Mar 24 20:04:06 idcmail postfix/smtp[1799]: connect to mail-tester.com[94.23.206.89]:25: Connection timed out
Mar 24 20:04:06 idcmail postfix/smtp[1799]: 12D2A81258: to=&lt;web-BqH7aD@mail-tester.com&gt;, relay=none, delay=531, delays=501/0.02/30/0, dsn=4.4.1, status=deferred (connect to mail-tester.com[94.23.206.89]:25: Connection timed out)
</code></pre>
","<domain-name-system><email>","2016-03-21 06:39:20"
"765013","Enable access-based enumeration adminshare on Windows 2012","<p>I can't find this option for adminshare on windows 2012 ( I can enable on folder) . I don't want to change file and folder structure so please help me to enable this function.</p>

<p><a href=""https://i.sstatic.net/oXm4x.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oXm4x.png"" alt=""enter image description here""></a></p>
","<windows>","2016-03-21 07:39:57"
"905791","Trying to set-up Authoritative-Only DNS Servers. My domains are not directed to the server","<p>Guys I am hoping I can find help here.</p>

<p>I am having trouble setting up Authoritative-Only DNS Servers.</p>

<p>One server is a Cloud Virtual Server XL (master server and web server - has Plesk) the other one is Cloud Virtual Server S (slave server).</p>

<p><strong>Please note that servers have different IPS and aren't in the same range but they are something like this: The XL Virtual Server(8X.1XX.1XX.2XX) and the S Virtual Server (9X.9X.2XX.9X).</strong></p>

<p>I am aware that there are more types of DNS, but with some light reading and googling I figured Authoritative-Only are best for my use.</p>

<p>The current problem I am having is that my domains aren't getting through to my nameservers. I'm getting a server not found on all of them, the <strong>main domain however is pointed correctly and resolves to the main machine.</strong></p>

<p>The guide I used: <a href=""https://www.digitalocean.com/community/tutorials/how-to-configure-bind-as-an-authoritative-only-dns-server-on-ubuntu-14-04"" rel=""nofollow noreferrer"">https://www.digitalocean.com/community/tutorials/how-to-configure-bind-as-an-authoritative-only-dns-server-on-ubuntu-14-04</a>.</p>

<p>I followed it from word to word, only part that I left out is <strong>Reverse Zones (in-addr.arpa)</strong> I am not sure on how to proceed on those. However, I did set my hostnames and Reverse DNS in their Control panel (1and1 Control Panel) to vps1.xxx.space and vps2.xxx.space <strong>(Hopefully my hunch was on point on this one and that should be solved? - again I am probably wrong)</strong>.</p>

<p>My <strong>db.xxx.space</strong> file looks like this:</p>

<pre><code>root@vps1:~# sudo nano /etc/bind/zones/db.xxx.xxx
  GNU nano 2.5.3                    File: /etc/bind/zones/db.xxx.space

; BIND data file for local loopback interface
;
$TTL    604800
@       IN      SOA     ns1.xxx.space. admin.xxx.space. (
                              5         ; Serial
                         604800         ; Refresh
                          86400         ; Retry
                        2419200         ; Expire
                         604800 )       ; Negative Cache TTL
;
; Name Servers
xxx.space.  IN      NS      ns1.xxx.space.
xxx.space.  IN      NS      ns2.xxx.space.

; A records for name servers
ns1     IN      NS      8X.1XX.1XX.2XX
ns2     IN      NS      9X.9X.2XX.9X

; other A records
@       IN      A       8X.1XX.1XX.2XX
www     IN      A       8X.1XX.1XX.2XX
vps1.xxx.space.     IN      A       8X.1XX.1XX.2XX
vps2.xxx.space.     IN      A       9X.9X.2XX.9X
xxx.space.  IN      MX      10      mail.xxx.space.
mail.xxx.space.     IN      A       8X.1XX.1XX.2XX
ns1.xxx.space.      IN      A       8X.1XX.1XX.2XX
ns2.xxx.space.      IN      A       9X.9X.2XX.9X
www.xxx.space.      IN      CNAME   xxx.space.
</code></pre>

<p>My <strong>/etc/bind/named.conf.local</strong> file (master):</p>

<pre><code>//
// Do any local configuration here
//

// Consider adding the 1918 zones here, if they are not used in your
// organization
//include ""/etc/bind/zones.rfc1918"";

zone ""xxx.space"" {
        type master;
        file ""/etc/bind/zones/db.xxx.space"";
        allow-transfer { 9X.9X.2XX.9X; };
};
</code></pre>

<p>My <strong>/etc/bind/named.conf.options</strong> file:</p>

<pre><code>options {
        directory ""/var/cache/bind"";

        recursion no;
        allow-transfer { none; };

        // If there is a firewall between you and nameservers you want
        // to talk to, you may need to fix the firewall to allow multiple
        // ports to talk.  See http://www.kb.cert.org/vuls/id/800113

        // If your ISP provided one or more IP addresses for stable
        // nameservers, you probably want to use them as forwarders.
        // Uncomment the following block, and insert the addresses replacing
        // the all-0's placeholder.

        // forwarders {
        //      0.0.0.0;
        // };

        //========================================================================
        // If BIND logs error messages about the root key being expired,
        // you will need to update your keys.  See https://www.isc.org/bind-keys
        //========================================================================
        dnssec-validation auto;

        auth-nxdomain no;    # conform to RFC1035
        listen-on-v6 { any; };
};
</code></pre>

<p>My <strong>/etc/bind/named.conf</strong> file:</p>

<pre><code>// This is the primary configuration file for the BIND DNS server named.
//
// Please read /usr/share/doc/bind9/README.Debian.gz for information on the
// structure of BIND configuration files in Debian, *BEFORE* you customize
// this configuration file.
//
// If you are just adding zones, please do that in /etc/bind/named.conf.local

include ""/etc/bind/named.conf.options"";
include ""/etc/bind/named.conf.local"";
include ""/etc/bind/named.conf.default-zones"";
</code></pre>

<p>My <strong>/etc/bind/named/conf.local (slave)</strong> file:</p>

<pre><code>//
// Do any local configuration here
//

// Consider adding the 1918 zones here, if they are not used in your
// organization
//include ""/etc/bind/zones.rfc1918"";

zone ""xxx.space"" {
        type slave;
        file ""slaves/db.xxx.space"";
        masters { 8X.1XX.1XX.2XX; }; #ns1 private IP
};
</code></pre>

<p>My advanced DNS settings for the main domain (xxx.space) that is supposed to have nameservers.<a href=""https://i.sstatic.net/MVnV7.png"" rel=""nofollow noreferrer"">Advanced Panel at namecheap.com</a></p>

<p>Rest of the domains have their custom name servers option set and point to the ns1.xxx.space and ns2.xxx.space nameservers.</p>

<p>I registered the name servers with namecheap.com as well, through the advanced DNS panel!</p>

<p>My Plesk DNS configuration: <a href=""https://i.sstatic.net/L4Fa5.png"" rel=""nofollow noreferrer"">Plesk Screenshot</a></p>

<p>If you have any more questions feel free to ask.</p>

<p><em>Misc information:</em></p>

<ul>
<li>All domains are hosted (except a few - not working as well) at namecheap.com</li>
<li>Servers are hosted at 1and1.com - Germany</li>
<li>Everything on the servers is up to date. (BIND etc.)</li>
<li>Servers are running Ubuntu 16.04 </li>
</ul>

<p>My questions:</p>

<ul>
<li><strong>What could be causing these issues for me?</strong></li>
<li><strong>Could there be an issue because I am using IPs that aren't in the same range?</strong></li>
<li><strong>How should I address Reverse Zones for two single IPs in the files?</strong></li>
</ul>

<p>I appreciate any help, suggestions or pointers!</p>
","<ubuntu><bind><nameserver><plesk><namecheap>","2018-04-03 14:55:56"
"765151","Java process stay in running state and consume CPU","<p>I have a jar running in background on a Ubuntu server. </p>

<p>In a certain moment, the application start consumming too much CPU (400%) and 4 child process stay in R state:</p>

<p><img src=""https://i.sstatic.net/p7tYD.jpg"" alt=""HTOP state before/after problem""></p>

<p>N.B: the problem is generating NOT because of an amount of using, it is caused just after a certain time (3-4 days). We have to kill java and re-run it.</p>

<p><strong>EDIT ADD GC Log:</strong></p>

<p>I did java -verbose:gc and here is what I got between restarting the app and when the application show the problem explained before.</p>

<p><img src=""https://i.sstatic.net/vzEcK.png"" alt=""Graph Line of the GC""></p>
","<ubuntu><java><process><cpu-usage><memory-usage>","2016-03-21 17:23:51"
"837344","Windows Network - Can't refer to \\Server","<p>We're setting up a small network, 2 or 3 computers. We designated one computer as Server1. Computer2 can access Server1 like so: \\Server1\Folder1</p>

<p>However because of an application configuration issue, on Server1, it needs to reference itself in the same way. But on Server1, windows doesn't recognize \\Server1\Folder1 for some reason.</p>

<p>I'm having trouble getting that working.</p>
","<windows>","2017-03-09 20:14:45"
"765411","Netgear Switches GS748Tv4 and v5 ports go down and up and Spanning Tree Topology Change","<p>I'm getting notifications on my Syslog server from the Netgear switches GS748Tv4 and v5. These are the notifications:</p>

<pre><code>traputil.c(696) 122305 %% Link Down: g30                                
traputil.c(696) 122323 %% Link Up: g30
traputil.c(739) 122307 %% Spanning Tree Topology Change: 0, Unit: 1
traputil.c(739) 122308 %% Spanning Tree Topology Change Initiated: 0, Interface: g30
</code></pre>

<p>Not only for the port g30 but for other ports as well. STP is enabled on both swithces. I don't know exactly what is causing this but when this happens, the user loses connection.</p>

<p>Any ideas? </p>

<p>Thank you,</p>
","<networking><switch><netgear><stp>","2016-03-22 19:01:50"
"837391","Stop wget clobbering local file is server unavailable","<p>I have a cron job on a server which once a day uses <code>wget</code> to download ""Earth Orientation Parameters"" and leapsecond data from a NASA server. Specifically:</p>

<pre><code>wget https://gemini.gsfc.nasa.gov/500/oper/solve_apriori_files/usno_finals.erp -O .eops
wget https://gemini.gsfc.nasa.gov/500/oper/solve_apriori_files/ut1ls.dat -O .ut1ls
</code></pre>

<p>This works fine. However it seems when the server is unavailable, <code>wget</code> clobbers my local files (filesize 0).  Is there anyway to tell <code>wget</code> to abort if the server not available and leave the local file unaffected. (The files contain predictions of a couple of months, so missing the update for a few days until the server comes back is not a problem).</p>
","<wget>","2017-03-10 03:54:14"
"979896","How do I enable a ""VPN-bypassing"" dummy localhost HTTP/SOCKS proxy with OpenVPN?","<p>For as long as I've been using VPNs, I've had this problem: it's either ""all VPN"" or ""all home connection"". There is no obvious (to me) way to, for example, set up a certain Firefox profile to use my ""straight home connection"", or my cURL script controlled by PHP to ""skip VPN"" for a particular request.</p>

<p>This ""binary"" limitation has caused me countless headaches, ranging from having countless shopping orders declined (because they stupidly think it's ""suspicious"" if you don't use your home connection when ordering something) to problems with automating an ""account balance check"" bot for my bank's ""Internet bank"" (because, again, it uses the VPN and its proxies instead of my normal ISP connection).</p>

<p>The ideal and only sensible solution, as I figure it, would be if OpenVPN automatically set up a localhost-only HTTP or SOCKS5 proxy server on some port, allowing you to use this as a ""VPN bypass"". I say HTTP/SOCKS5 because these protocols are what's supported by individual applications (if any proxy is supported at all), including Firefox and my cURL scripts. If it would simply provide this, this would not be a problem. However, as far as I can tell, there isn't any such feature.</p>

<p>I've had so many serious problems resulting from this over the years, but I never found any acceptable/sane solution to this. They really make it impossible for me to do what ought to be the most obvious, basic feature one could possibly imagine. It's really maddening.</p>

<p>Please note that I'm talking about OpenVPN. I refuse to download/install any vendor-specific VPN ""wrapper apps"" which may support this, but I don't trust them for one second, and for good reason. They spy on you even more than when you use the more ""neutral"" and ""industrial"" OpenVPN client...</p>

<p>Also, I've heard things like ""split a tunnel"" or ""set up this and that and blablabla"" and it's always something awfully complex, often Linux-specific (I use Windows), and just ""doesn't sit right with me"". It seems too weird to me that they wouldn't provide some means to bypass the VPN connection, other than of course disconnecting from it, which would be a <em>massive</em> PITA to do constantly, not to mention it would break all kinds of things which require a constant connection.</p>

<p>Please don't suggest that I do things like those. If at all possible, this should be done in a very straight-forward and simple and obvious manner in OpenVPN, perhaps even a single configuration option that is for some reason disabled by default? I <strong><em>dream</em></strong> of something like:</p>

<pre><code>bypass-vpn-proxy-server = 127.0.0.1:9876;HTTP
</code></pre>

<p>Then I would set up a Firefox profile called ""Home connection"", with the proxy set to use 127.0.0.1 on port 9876, and then any connection made in that Firefox profile would be using my actual home ISP connection and not go through the VPN as connected to via OpenVPN. Or SOCKS5, which Firefox also supports. (In fact, it supports both at the same time. Not sure what happens if you specify both...)</p>

<p>I really, really need to figure this out. It's ridiculous. You'd think this would be the first thing they'd think of when designing some kind of VPN client (OpenVPN)... but apparently not? I can't find anything as usual, but that says nothing.</p>
","<windows><vpn><openvpn>","2019-08-19 22:26:21"
"837596","Trying to Setup an internal dns","<p>On my job's intranet we have internal applications that are not available in public for example: app1.example.com, app2.example.com whilst there is the example.com public facing website. Both app1.example.com and app2.example.com are resolving into intranet's ip.</p>

<p>So as far as I searched, I found that this is possible by having a local DNS server into our intranet.</p>

<p>Therefore I wanted to replicate that by using Virtualbox VMs, so used 3 Ubuntu flavored Vm's One Xubuntu, One Lubuntu and one Ubuntu Budgie Edition leftover form previous 'experiments'. All of them are having 2 network Adapters: </p>

<ul>
<li>One setup as NAT and </li>
<li>Another one as 'Internal Network' with static having ips from <code>192.0.0.0/24</code> network. </li>
</ul>

<p>On Xubuntu one I installed the bind9 and a webserver and I try to simulate by by typing into Xubuntu and Budgie Edition Vms' browser app1.intranet.example.com and app2.intranet.example.com to serve 2 different sites. These sites won't be available outside the network (of theese 3 Vms) wont even be able to even resolve the DNS entries for theese 2 sites.</p>

<p>As for Now on the vm running the bind (The Xubuntu One) Has these settings:</p>

<pre><code>options {
        directory ""/var/cache/bind"";


        // If there is a firewall between you and nameservers you want
        // to talk to, you may need to fix the firewall to allow multiple
        // ports to talk.  See http://www.kb.cert.org/vuls/id/800113

        // If your ISP provided one or more IP addresses for stable
        // nameservers, you probably want to use them as forwarders.
        // Uncomment the following block, and insert the addresses replacing
        // the all-0's placeholder.

         forwarders {
                208.67.222.222;
                208.67.220.220;
         };

        //========================================================================
        // If BIND logs error messages about the root key being expired,
        // you will need to update your keys.  See https://www.isc.org/bind-keys
        //========================================================================
        dnssec-validation auto;

        auth-nxdomain no;    # conform to RFC1035
        listen-on-v6 { any; };
};

acl ""intranet"" { 192.0.0.1/24; };
view ""intranetView"" {
        match-clients { ""intranet""; };
        recursion yes;
        zone ""intranet.example.com"" {
                type master;
                file ""/etc/bind/db.intranet""
        }
}

view ""outside"" {
        match-clients { any; }
        recursion no;
}
</code></pre>

<p>Also on <code>/etc/bind/db.intranet</code> I have the following entries:</p>

<pre><code>;
; BIND data file for local loopback interface
;
$TTL    604800
@   IN  SOA intranet.example.com. root.example.com. (
                  2     ; Serial
             604800     ; Refresh
              86400     ; Retry
            2419200     ; Expire
             604800 )   ; Negative Cache TTL
;
@   IN  NS  192.0.0.2
@   IN  A   192.0.0.2
app1    IN  A   192.0.0.2
app2    IN  A   192.0.0.2
</code></pre>

<p>But for some reason when I try to restart the bind It fails. Can you help me out to figure out the problem?</p>
","<ubuntu><bind><virtualbox><split-dns>","2017-03-10 23:33:45"
"837620","How to debug these internal server errors?","<p>I am not a server admin but do have some experience with Linux servers but mostly in Django and Nginx. Someone at our company has left suddenly for personal reasons and we have a site running on PHP with two pages that are showing internal server error messages that I need to fix.</p>

<p>I have done a fair bit of searching here and on Google and tried a few things already but still stuck, these are the urls to the pages (domain removed):</p>

<p><a href=""http://dev.mydomain.com/user/login/index.php"" rel=""nofollow noreferrer"">http://dev.mydomain.com/user/login/index.php</a></p>

<p><a href=""http://dev.mydomain.com/ew-admin/submit.php"" rel=""nofollow noreferrer"">http://dev.mydomain.com/ew-admin/submit.php</a></p>

<p>I first did a grep search for the index.php url in the Apache error logs and found this:</p>

<pre><code>[Fri Mar 10 06:15:51.083887 2017] [core:error] [pid 24087]    [client 121.97.206.98:54740] AH00124: Request exceeded the    limit of 10 internal redirects due to probable configuration error.     Use 'LimitInternalRecursion' to increase the limit if necessary.     Use 'LogLevel debug' to get a backtrace., referer:        http://dev.mydomain.com/user/login/index.php
</code></pre>

<p>and then after Googling those errors I found this thread:</p>

<p><a href=""https://stackoverflow.com/questions/19071324/request-exceeded-the-limit-of-10-internal-redirects"">https://stackoverflow.com/questions/19071324/request-exceeded-the-limit-of-10-internal-redirects</a></p>

<p>this is my .htaccess file</p>

<pre><code># BEGIN WordPress
&lt;IfModule mod_rewrite.c&gt;
RewriteEngine On
RewriteBase /
RewriteRule ^index\.php$ - [L]
RewriteCond %{REQUEST_FILENAME} !-f
RewriteCond %{REQUEST_FILENAME} !-d
RewriteRule . /index.php [L]
&lt;/IfModule&gt;
# END WordPress
</code></pre>

<p>and I tried adding the line that was suggested in the SO thread to the .htaccess file but that didnt seem to change anything.</p>

<p>So next I checked the file and folder permissions for where those files are stored, they weren't set to 755 so I changed them to that, still getting internal server error though.</p>

<p>I should all that the public_html/ directory has these two directories in them:</p>

<p>dev/
old_mydomain/dev</p>

<p>the dev/ directory is empty and old_mydomain/dev directory seems to have the files for these two urls that are showing errors so I tried moving all the files from old_mydomain/dev to the empty dev/ directory seeing as the url has the 'dev.' subdomain but that didnt work either so I moved the files back.</p>

<p>I found an error_log within the old_mydomain/dev directory too with the following in but this seems to have stopped logging errors in the summer of last year and I know that these two urls were working just a few months back:</p>

<pre><code>[21-Jul-2016 11:45:20 UTC] PHP Warning:  fsockopen(): unable to connect to toolbarqueries.google.com:80 (Network is unreachable) in /home/mydomain/public_html/dev/pagerank.php on line 85
[29-Jul-2016 06:03:09 UTC] PHP Warning:  mysql_connect(): Can't connect to local MySQL server through socket '/var/lib/mysql/mysql.sock' (111) in /home/mydomain/public_html/dev/hide/init.php on line 14
[30-Jul-2016 13:22:40 UTC] PHP Warning:  mysql_connect(): Can't connect to local MySQL server through socket '/var/lib/mysql/mysql.sock' (111) in /home/mydomain/public_html/dev/hide/init.php on line 14
[31-Jul-2016 10:38:20 UTC] PHP Warning:  mysql_connect(): Can't connect to local MySQL server through socket '/var/lib/mysql/mysql.sock' (111) in /home/mydomain/public_html/dev/hide/init.php on line 14
[07-Aug-2016 13:32:34 UTC] PHP Warning:  fsockopen(): unable to connect to toolbarqueries.google.com:80 (Network is unreachable) in /home/mydomain/public_html/dev/pagerank.php on line 85
[08-Aug-2016 20:37:55 UTC] PHP Warning:  mysql_connect(): Can't connect to local MySQL server through socket '/var/lib/mysql/mysql.sock' (111) in /home/mydomain/public_html/dev/hide/init.php on line 14
[09-Aug-2016 23:58:17 UTC] PHP Warning:  mysql_connect(): Can't connect to local MySQL server through socket '/var/lib/mysql/mysql.sock' (111) in /home/eatonweb/public_html/dev/hide/init.php on line 14
[11-Aug-2016 00:04:53 UTC] PHP Warning:  mysql_connect(): Can't connect to local MySQL server through socket '/var/lib/mysql/mysql.sock' (111) in /home/mydomain/public_html/dev/hide/init.php on line 14
[16-Aug-2016 22:27:06 UTC] PHP Warning:  mysql_connect(): Can't connect to local MySQL server through socket '/var/lib/mysql/mysql.sock' (2) in /home/mydomain/public_html/dev/hide/init.php on line 14
[17-Aug-2016 23:57:40 UTC] PHP Warning:  mysql_connect(): Can't connect to local MySQL server through socket '/var/lib/mysql/mysql.sock' (2) in /home/mydomain/public_html/dev/hide/init.php on line 14 
</code></pre>

<p>Thats all I can think of for debugging and Im stumped right now, any help would be appreciated!</p>
","<php><apache2>","2017-03-11 04:57:23"
"837662","Why PHP is not installing in my Ubuntu server and how can I install it now?","<p>Everytime when I try to install php using this command, it shows me ""E: Package 'php5-mcrypt' has no installation candidate"".</p>

<p>Here is the command below what I used for installing php-</p>

<pre><code>sudo apt-get install php5 libapache2-mod-php5 php5-mcrypt
</code></pre>
","<ubuntu><php><php5>","2017-03-11 13:00:20"
"765712","Make copy of OVA before restoration or not","<p>Good afternoon,</p>

<p>When having to restore a virtual machine from a backed-up OVA file, should you select the source as the actual file itself or should you create a copy of the backed up file before importing? I am not sure if anything during the restoration process could potentially affect the OVA file itself (loss of network connection, cancelled import, etc.) or if the restoration process is simply reading from the OVA file.
Ex. </p>

<p>OVA Location = /backup/mainServer.ova</p>

<p>Should you import an OVA directly from <strong>/backup/mainServer.ova</strong> or should you...</p>

<pre><code>cp /backup/mainServer.ova /tmp/mainServer.ova
</code></pre>

<p>and THEN proceed to import the OVA from <strong>/tmp/mainServer.ova</strong></p>

<p>From my understanding an OVA is made up of the files that are needed individually from VM/ESXI although it is not apparent if while importing the software is able to merely read the file and pull the necessary information or if the file is modified/opened or accessed in in any way that could have a  possibility of corrupting the file based off errors or unexpected actions during the import process itself.</p>

<p>I have checked the file during uploading and I cannot see a way to tell if the file is modified/access or just read from. Hash sums are the same before/after so nothing is ""modified"". But that does not mean that the file isn't accessed in a way that could corrupt it from the actual import process.</p>
","<linux><redhat><virtual-machines><backup-restoration>","2016-03-23 19:37:05"
"765767","Procedure for GoDaddy and AWS Route53","<p>My boss host a domain e.g. <code>example.com</code> in GoDaddy. (I do not have access to GoDaddy this case)</p>

<p>In AWS, I want to setup a load-balancer (ELB) and point to 2 EC2 servers.</p>

<p>Unfortunately, I can't setup ""@"" A record to ELB due to DNS features.</p>

<p>I want to utilize Route53 to do it but my boss don't want to transfer the domain to Amazon. Is it possible?</p>

<p>My question is, whether I need to transfer <code>example.com</code> from GoDaddy to Amazon. (Of course, if I need to make some changes in GoDaddy side, it make sense to my boss). Is it ok? Thanks.</p>
","<amazon-web-services><load-balancing><godaddy><amazon-route53><domain-transferring>","2016-03-24 02:35:52"
"765865","Minimal requirements to firewall rules for running dhcpd server in linux container","<p>I want to run isc dhcpd server in lxd (lxc) container. But it does not recive clients requests (DHCPDISCOVER, DHCPREQUEST) from clients without this rule:</p>

<pre><code>iptables -I FORWARD -m physdev --physdev-is-bridged -j ACCEPT
</code></pre>

<p>I had prepared new rules to replace:</p>

<pre><code>iptables -A FORWARD -d $DHCP -p udp --dport 67:68 --sport 67:68 -j ACCEPT
iptables -A FORWARD -s $DHCP -p udp --dport 67:68 --sport 67:68 -j ACCEPT
iptables -A FORWARD -i br-eth0 -m pkttype --pkt-type multicast -j ACCEPT
</code></pre>

<p>But it seems to me that this is not enough. To be precise - this is not enough.</p>

<p>I must allow broadcast another way ?</p>
","<iptables><dhcp><bridge>","2016-03-24 14:08:47"
"980209","How to increase the Java Memory in Windows Server 2012 R2?","<p>I am trying to follow <a href=""http://www.messiahpsychoanalyst.org/wikihow/index.php/How_to_Increase_Java_Memory_in_Windows"" rel=""nofollow noreferrer"">This Link</a> to increase the memory that is available to the Java Runtime Environment, because I encountered the &quot;insufficient memory for the Java Runtime Environment to continue&quot; when running Tableau Server. But my question is: I don't even have the &quot;Java&quot; section as shown in the image below at all.
<a href=""https://i.sstatic.net/DGaSI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/DGaSI.png"" alt=""enter image description here"" /></a></p>
<p>On this Windows Server 2012R2, I do find that the &quot;Java Mission Control&quot; installed on the machine but I don't know how to change the Java memory size inside it.</p>
<p>Can anyone give me some suggestions/advice? Thanks!</p>
","<windows-server-2012-r2><java><environment-variables><virtual-memory><heapspace>","2019-08-21 21:04:52"
"980312","Webserver with merged disks and backup optimal Solution","<p>I have to set up an HPE server GEN10 with 2x240GB SSD &amp; 3x1.2TB HDDs as a webserver.</p>

<p>Client needs a production repo (live on apache folder), multiple development repos and backup.</p>

<p>My idea is 
1) merge the two SSDs into 1disk, so 1Disk with ~400GB total,
2) merge 2x1.2TB HDDs having all the development repos there
3) and 1.2TB backup by <code>rsync</code> the merged SSD. </p>

<p>Is there any issue with the above?  Do you think any other optimal solution ?</p>
","<centos><hard-drive><hpe>","2019-08-22 12:40:52"
"766002","Restore RAID 5 HP P410","<p>I have:</p>

<ul>
<li>P410 + 512MB Battery Backup</li>
<li>4x4TB in RAID 5 </li>
</ul>

<p>I changed the PSU today and after starting OS (Windows 2008 R2) the array is gone.</p>

<p>ACU sees 2 HDD as unassigned and original RAID 5 as 2 OK and 2 HDD missing.</p>

<p>How can I restore the original array without losing data? </p>
","<raid><hp-smart-array><data-recovery>","2016-03-24 23:44:54"
"980344","Running out of memory in Docker inside a Virtual Machine","<p>I am running Docker on an Oracle virtual machine (VM), and running a Drupal instance, where I do migrations from version 7 to version 8 database, both of which are there. The migrations were running fine but lately I am getting a memory error. Looking at the VM settings, I have plenty of memory, however.</p>

<p>This is the error:</p>

<pre><code>Drush command terminated abnormally due to an unrecoverable error. [error] Error: Allowed memory size of 404750336 bytes exhausted (tried to allocate 199233536 bytes) in /var/www/html/web/core/lib/Drupal/Core/Database/Connection.php, line 686
</code></pre>
","<docker><memory>","2019-08-22 15:04:45"
"838205","Hundreds of CLOSE_WAIT","<p>I am getting file leaks when I run my code on Google Compute Engine to what appears to be Google servers: </p>

<pre><code>php        3630         www-data  873u     IPv4              34632       0t0        TCP xxxx.internal:43328-&gt;vu-in-f139.1e100.net:https (CLOSE_WAIT)
php        3630         www-data  874u     IPv4              34640       0t0        TCP xxxx.internal:39500-&gt;vh-in-f139.1e100.net:https (CLOSE_WAIT)
php        3630         www-data  875u     IPv4              34648       0t0        TCP xxxx:43336-&gt;vu-in-f139.1e100.net:https (CLOSE_WAIT)
</code></pre>

<p>I am not making calls to those servers directly.  I believe Google's API Library may be making those calls, but it doesn't appear to be closing it.  I have run this same code on a baremetal server and I don't get these file leaks.  </p>

<p>Does anyone have any idea what is causing this?</p>
","<google-cloud-platform><google-compute-engine>","2017-03-14 14:36:23"
"838253","Using NGINX for wildcard denial","<p>I have a URL...</p>

<pre><code>/index.php?option=com_docman&amp;task=doc_download&amp;gid= // many gid's
</code></pre>

<p>I want to use an NGINX location block with a wildcard to <strong>""DENY ALL""</strong> any urls containing ""option=com_docman""</p>

<p>In various regex testers....</p>

<pre><code>^(.*)(option)(.*)(com_docman)$ // Works fine for normal regex
</code></pre>

<p>However when testing in my nginx.conf file... the following does not work.</p>

<pre><code>location ~ ^(.*)(option)(.*)(com_docman)(.*)$ {
    Deny All;
}
</code></pre>

<p><strong>Bonus</strong> Is there a way to get robots.txt to address wildcard urls in the same way?</p>

<p><strong>Edit</strong> Not sure why downvoted... anyone want to tell me what obvious thing i'm doing wrong here?</p>
","<nginx><web-server><regex>","2017-03-14 17:35:19"
"838286","Conditional redirect with domain in .htaccess","<p>I would like to make a redirect in Apache with a domain based rule. For example</p>

<p>If a user access to the page from <code>example.com</code> or another related page (<code>example.com/another-url/</code>), then redirect to <code>example.com/page.html</code>. Else, show the normal page.</p>

<p>I write in the <code>.htaccess</code>:</p>

<pre><code>&lt;IfModule mod_rewrite.c&gt;    
    RewriteEngine on
    RewriteCond %{REMOTE_ADDR} !^example.com
    RewriteRule .* /page.html [R=302,L]    
&lt;/IfModule&gt;
</code></pre>

<p>But it doesn't work.</p>
","<.htaccess><mod-rewrite><apache2>","2017-03-14 21:00:35"
"838306","Instance level access on AWS?","<p>I want to give full access on AWS EC2 instance for a single instance, and deny any action on other instances.</p>

<p>Can it be done ?</p>

<p>Thanks.</p>
","<amazon-web-services>","2017-03-14 23:03:37"
"980702","i cannot login to IMAP/POP3 from different location","<p>[<strong><em>Please read before marking as duplicate</em></strong>]</p>

<p>I have installed <code>Postfix</code> and <code>dovecot</code> on <code>Ubuntu</code> (for virtual Users).</p>

<p>So i can login into the mail server from home using cellphone(mail app) and even on windows(Windows Mail).</p>

<p>but when i move to a different location, i cannot login anymore.</p>

<p>So i tried to change <code>mynetwork</code> to <code>0.0.0.0</code> in <code>/etc/postfix/main.cf</code> and it's not helping:</p>

<pre><code>#mynetworks = 127.0.0.0/8 [::ffff:127.0.0.0]/104 [::1]/128
mynetworks = 0.0.0.0
</code></pre>

<p>How can i fix this problem?</p>

<hr>

<p><strong>Edit</strong></p>

<blockquote>
  <p>i followed this tutorial: <a href=""https://linuxize.com/post/install-and-configure-postfix-and-dovecot/"" rel=""nofollow noreferrer"">linuxize</a></p>
</blockquote>

<hr>

<p><strong>Mail Client on cellphone</strong></p>

<pre><code>The email server (mail.example.net) is not responding.
you can continue the setup process but you'll not be able to receive emails until your account has been verified by the server
</code></pre>

<pre><code>root@mail:~# ufw status
Status: active

To                         Action      From
--                         ------      ----
22                         ALLOW       Anywhere
Nginx Full                 ALLOW       Anywhere
80                         ALLOW       Anywhere
443                        ALLOW       Anywhere
21/tcp                     ALLOW       Anywhere
924                        ALLOW       Anywhere
25                         ALLOW       Anywhere
25/tcp                     ALLOW       Anywhere
Anywhere                   ALLOW       168.167.80.116
22 (v6)                    ALLOW       Anywhere (v6)
Nginx Full (v6)            ALLOW       Anywhere (v6)
80 (v6)                    ALLOW       Anywhere (v6)
443 (v6)                   ALLOW       Anywhere (v6)
21/tcp (v6)                ALLOW       Anywhere (v6)
924 (v6)                   ALLOW       Anywhere (v6)
25 (v6)                    ALLOW       Anywhere (v6)
25/tcp (v6)                ALLOW       Anywhere (v6)
995 (v6)                   ALLOW       Anywhere (v6)
993 (v6)                   ALLOW       Anywhere (v6)
</code></pre>

<p><strong><em>/var/log/dovecot-debug.log</em></strong></p>

<pre><code>Aug 26 09:28:12 auth: Debug: auth client connected (pid=0)
Aug 26 09:28:12 auth: Debug: auth client connected (pid=0)
Aug 26 09:28:12 auth: Debug: auth client connected (pid=0)
Aug 26 09:28:12 auth: Debug: auth client connected (pid=0)
</code></pre>

<p><strong><em>/var/log/mail.log</em></strong></p>

<pre><code>Aug 26 09:29:03 mail postfix/smtpd[2179]: connect from fml.ips.net[ip.address.here]
Aug 26 09:29:03 mail postfix/smtpd[2178]: connect from fml.ips.net[ip.address.here]
Aug 26 09:29:03 mail postfix/smtpd[2180]: connect from fml.ips.net[ip.address.here]
Aug 26 09:29:03 mail postfix/smtpd[2181]: connect from fml.ips.net[ip.address.here]
Aug 26 09:29:03 mail postfix/smtpd[2178]: lost connection after EHLO from fml.ips.net[ip.address.here]
Aug 26 09:29:03 mail postfix/smtpd[2178]: disconnect from fml.ips.net[ip.address.here]
Aug 26 09:29:03 mail postfix/smtpd[2179]: lost connection after EHLO from fml.ips.net[ip.address.here]
Aug 26 09:29:03 mail postfix/smtpd[2179]: disconnect from fml.ips.net[ip.address.here]
Aug 26 09:29:03 mail postfix/smtpd[2180]: lost connection after EHLO from fml.ips.net[ip.address.here]
Aug 26 09:29:03 mail postfix/smtpd[2180]: disconnect from fml.ips.net[ip.address.here]
Aug 26 09:29:03 mail postfix/smtpd[2181]: lost connection after EHLO from fml.ips.net[ip.address.here]
Aug 26 09:29:03 mail postfix/smtpd[2181]: disconnect from fml.ips.net[ip.address.here]
</code></pre>

<p><strong><em>/etc/postfix/master.cf</em></strong></p>

<pre><code># Postfix master process configuration file.  For details on the format
# of the file, see the master(5) manual page (command: ""man 5 master"" or
# on-line: http://www.postfix.org/master.5.html).
#
# Do not forget to execute ""postfix reload"" after editing this file.
#
# ==========================================================================
# service type  private unpriv  chroot  wakeup  maxproc command + args
#               (yes)   (yes)   (yes)   (never) (100)
# ==========================================================================
smtp      inet  n       -       -       -       -       smtpd
#smtp      inet  n       -       -       -       1       postscreen
#smtpd     pass  -       -       -       -       -       smtpd
#dnsblog   unix  -       -       -       -       0       dnsblog
#tlsproxy  unix  -       -       -       -       0       tlsproxy
submission inet n       -       y       -       -       smtpd
  -o syslog_name=postfix/submission
  -o smtpd_tls_security_level=encrypt
  -o smtpd_sasl_auth_enable=yes
#  -o smtpd_reject_unlisted_recipient=no
  -o smtpd_client_restrictions=permit_sasl_authenticated,reject
#  -o smtpd_helo_restrictions=$mua_helo_restrictions
#  -o smtpd_sender_restrictions=$mua_sender_restrictions
#  -o smtpd_recipient_restrictions=
#  -o smtpd_relay_restrictions=permit_sasl_authenticated,reject
  -o milter_macro_daemon_name=ORIGINATING
smtps     inet  n       -       y       -       -       smtpd
  -o syslog_name=postfix/smtps
  -o smtpd_tls_wrappermode=yes
  -o smtpd_sasl_auth_enable=yes
#  -o smtpd_reject_unlisted_recipient=no
  -o smtpd_client_restrictions=permit_sasl_authenticated,reject
#  -o smtpd_helo_restrictions=$mua_helo_restrictions
#  -o smtpd_sender_restrictions=$mua_sender_restrictions
#  -o smtpd_recipient_restrictions=
#  -o smtpd_relay_restrictions=permit_sasl_authenticated,reject
  -o milter_macro_daemon_name=ORIGINATING
#628       inet  n       -       -       -       -       qmqpd
pickup    unix  n       -       -       60      1       pickup
cleanup   unix  n       -       -       -       0       cleanup
qmgr      unix  n       -       n       300     1       qmgr
#qmgr     unix  n       -       n       300     1       oqmgr
tlsmgr    unix  -       -       -       1000?   1       tlsmgr
rewrite   unix  -       -       -       -       -       trivial-rewrite
bounce    unix  -       -       -       -       0       bounce
defer     unix  -       -       -       -       0       bounce
trace     unix  -       -       -       -       0       bounce
verify    unix  -       -       -       -       1       verify
flush     unix  n       -       -       1000?   0       flush
proxymap  unix  -       -       n       -       -       proxymap
proxywrite unix -       -       n       -       1       proxymap
smtp      unix  -       -       -       -       -       smtp
relay     unix  -       -       -       -       -       smtp
#       -o smtp_helo_timeout=5 -o smtp_connect_timeout=5
showq     unix  n       -       -       -       -       showq
error     unix  -       -       -       -       -       error
retry     unix  -       -       -       -       -       error
discard   unix  -       -       -       -       -       discard
local     unix  -       n       n       -       -       local
virtual   unix  -       n       n       -       -       virtual
lmtp      unix  -       -       -       -       -       lmtp
anvil     unix  -       -       -       -       1       anvil
scache    unix  -       -       -       -       1       scache
#
# ====================================================================
# Interfaces to non-Postfix software. Be sure to examine the manual
# pages of the non-Postfix software to find out what options it wants.
#
# Many of the following services use the Postfix pipe(8) delivery
# agent.  See the pipe(8) man page for information about ${recipient}
# and other message envelope options.
# ====================================================================
#
# maildrop. See the Postfix MAILDROP_README file for details.
# Also specify in main.cf: maildrop_destination_recipient_limit=1
#
maildrop  unix  -       n       n       -       -       pipe
  flags=DRhu user=vmail argv=/usr/bin/maildrop -d ${recipient}
#
# ====================================================================
#
# Recent Cyrus versions can use the existing ""lmtp"" master.cf entry.
#
# Specify in cyrus.conf:
#   lmtp    cmd=""lmtpd -a"" listen=""localhost:lmtp"" proto=tcp4
#
# Specify in main.cf one or more of the following:
#  mailbox_transport = lmtp:inet:localhost
#  virtual_transport = lmtp:inet:localhost
#
# ====================================================================
#
# Cyrus 2.1.5 (Amos Gouaux)
# Also specify in main.cf: cyrus_destination_recipient_limit=1
#
#cyrus     unix  -       n       n       -       -       pipe
#  user=cyrus argv=/cyrus/bin/deliver -e -r ${sender} -m ${extension} ${user}
#
# ====================================================================
# Old example of delivery via Cyrus.
#
#old-cyrus unix  -       n       n       -       -       pipe
#  flags=R user=cyrus argv=/cyrus/bin/deliver -e -m ${extension} ${user}
#
# ====================================================================
#
# See the Postfix UUCP_README file for configuration details.
#
uucp      unix  -       n       n       -       -       pipe
  flags=Fqhu user=uucp argv=uux -r -n -z -a$sender - $nexthop!rmail ($recipient)
#
# Other external delivery methods.
#
ifmail    unix  -       n       n       -       -       pipe
  flags=F user=ftn argv=/usr/lib/ifmail/ifmail -r $nexthop ($recipient)
bsmtp     unix  -       n       n       -       -       pipe
  flags=Fq. user=bsmtp argv=/usr/lib/bsmtp/bsmtp -t$nexthop -f$sender $recipient
scalemail-backend unix  -       n       n       -       2       pipe
  flags=R user=scalemail argv=/usr/lib/scalemail/bin/scalemail-store ${nexthop} ${user} ${extension}
mailman   unix  -       n       n       -       -       pipe
  flags=FR user=list argv=/usr/lib/mailman/bin/postfix-to-mailman.py
  ${nexthop} ${user}


</code></pre>

<p><strong><em>/etc/dovecot/conf.d/10-master.conf</em></strong></p>

<pre><code>#default_process_limit = 100
#default_client_limit = 1000

# Default VSZ (virtual memory size) limit for service processes. This is mainly
# intended to catch and kill processes that leak memory before they eat up
# everything.
#default_vsz_limit = 256M

# Login user is internally used by login processes. This is the most untrusted
# user in Dovecot system. It shouldn't have access to anything at all.
#default_login_user = dovenull

# Internal user is used by unprivileged processes. It should be separate from
# login user, so that login processes can't disturb other processes.
#default_internal_user = dovecot

service imap-login {
  inet_listener imap {
    #port = 143
  }
  inet_listener imaps {
    port = 993
    ssl = yes
  }

  # Number of connections to handle before starting a new process. Typically
  # the only useful values are 0 (unlimited) or 1. 1 is more secure, but 0
  # is faster. &lt;doc/wiki/LoginProcess.txt&gt;
  #service_count = 1

  # Number of processes to always keep waiting for more connections.
  #process_min_avail = 0

  # If you set service_count=0, you probably need to grow this.
  #vsz_limit = $default_vsz_limit
}

service pop3-login {
  inet_listener pop3 {
    #port = 110
  }
  inet_listener pop3s {
    port = 995
    ssl = yes
  }
}

service submission-login {
  inet_listener submission {
    port = 587
  }
}

service lmtp {
  unix_listener /var/spool/postfix/private/dovecot-lmtp {
    mode = 0600
    user = postfix
    group = postfix
  }

  # Create inet listener only if you can't use the above UNIX socket
  #inet_listener lmtp {
    # Avoid making LMTP visible for the entire internet
    #address =
    #port =
  #}
}

service imap {
  # Most of the memory goes to mmap()ing files. You may need to increase this
  # limit if you have huge mailboxes.
  #vsz_limit = $default_vsz_limit

  # Max. number of IMAP processes (connections)
  #process_limit = 1024
}

service pop3 {
  # Max. number of POP3 processes (connections)
  #process_limit = 1024
}

service submission {
  # Max. number of SMTP Submission processes (connections)
  #process_limit = 1024
}

service auth {
  # auth_socket_path points to this userdb socket by default. It's typically
  # used by dovecot-lda, doveadm, possibly imap process, etc. Users that have
  # full permissions to this socket are able to get a list of all usernames and
  # get the results of everyone's userdb lookups.
  #
  # The default 0666 mode allows anyone to connect to the socket, but the
  # userdb lookups will succeed only if the userdb returns an ""uid"" field that
  # matches the caller process's UID. Also if caller's uid or gid matches the
  # socket's uid or gid the lookup succeeds. Anything else causes a failure.
  #
  # To give the caller full permissions to lookup all users, set the mode to
  # something else than 0666 and Dovecot lets the kernel enforce the
  # permissions (e.g. 0777 allows everyone full permissions).
  unix_listener auth-userdb {
    mode = 0600
    user = vmail
    group = vmail
  }

  # Postfix smtp-auth
  unix_listener /var/spool/postfix/private/auth {
    mode = 0666
    user = postfix
    group = postfix
  }

  # Auth process is run as this user.
  #user = $default_internal_user
}

service auth-worker {
  # Auth worker process is run as root by default, so that it can access
  # /etc/shadow. If this isn't necessary, the user should be changed to
  # $default_internal_user.
  user = vmail
}

service dict {
  # If dict proxy is used, mail processes should have access to its socket.
  # For example: mode=0660, group=vmail and global mail_access_groups=vmail
  unix_listener dict {
    mode = 0600
    user = vmail
    group = vmail
  }
}
</code></pre>
","<ubuntu><postfix><email-server><ip><dovecot>","2019-08-26 08:38:31"
"766298","SMB2? Traffic Crashing Network Windows Server 2012 R2","<p>I am running a network at a school. The server is running Windows Server 2012 R2. Out of the blue about a week ago, I started experiencing network issues.</p>

<p>Examples:</p>

<ul>
<li>Can't log into roaming profile</li>
<li>Desktop background or icons fail to load on login</li>
<li>Network drives are inaccessible</li>
<li>Can't use the internet (DNS Server Failure? I've tried adding the router ip as the fallback. We will see if that helps)</li>
</ul>

<p>I was really excited when I found this post:
<a href=""https://serverfault.com/questions/455998/smb2-traffic-crashes-network"">SMB2 traffic crashes network?</a></p>

<p>As it seems to match my issue exactly. I am seeing the same types of SMB2 ""Create Request File"" packets to files that are not open on the computer. I am not however using the File Resource Manager. I tried disabling the quota system by adding a registry key anyway, but haven't seen a difference.</p>

<p>One thing that was interesting, I saw a teacher's computer receiving these packets when the network was not working. I unplugged it, and everything worked well for a bit, then saw the same types of packets going to mine.</p>

<p>I'm at a loss of what to try next. Any help would be appreciated.</p>

<p>Thank you!</p>

<p>-Joel</p>

<p><strong>Edit</strong></p>

<p><em>ipconfig /all</em></p>

<pre><code>Microsoft Windows [Version 10.0.10586]
(c) 2015 Microsoft Corporation. All rights reserved.

C:\Users\joelp_000&gt;ipconfig /all
Wireless LAN adapter Wi-Fi:

Connection-specific DNS Suffix  . : immanuelmission.local
Description . . . . . . . . . . . : Intel(R) Dual Band Wireless-AC 3160
Physical Address. . . . . . . . . : D0-7E-35-3D-F1-DA
DHCP Enabled. . . . . . . . . . . : Yes
Autoconfiguration Enabled . . . . : Yes
Link-local IPv6 Address . . . . . : fe80::af:865f:72e0:78e7%3(Preferred)
IPv4 Address. . . . . . . . . . . : 10.0.53.55(Preferred)
Subnet Mask . . . . . . . . . . . : 255.0.0.0
Lease Obtained. . . . . . . . . . : Saturday, March 26, 2016 8:36:12 PM
Lease Expires . . . . . . . . . . : Monday, March 28, 2016 9:06:15 AM
Default Gateway . . . . . . . . . : 10.0.53.1
DHCP Server . . . . . . . . . . . : 10.0.53.7
DHCPv6 IAID . . . . . . . . . . . : 147881525
DHCPv6 Client DUID. . . . . . . . : 00-01-00-01-1C-10-7F-04-54-EE-75-39-FD-11
DNS Servers . . . . . . . . . . . : 10.0.53.7
                                    10.0.53.1
NetBIOS over Tcpip. . . . . . . . : Enabled
</code></pre>

<p>10.0.53.7 is the server IP which acts as the DNS server.
10.0.53.1 is the router</p>

<p>I haven't had any internet failures after changing the secondary DNS server to the router.</p>

<p>The forwarders on the DNS server are:</p>

<pre><code>8.8.8.8
8.8.4.4
148.78.200.7
148.78.200.8
</code></pre>

<p>The IPv4 config on the server:</p>

<pre><code>IP address:           10.0.53.7
Subnet mask:          255.0.0.0
Default gateway:      10.0.53.1
Preferred DNS server: 10.0.53.7
</code></pre>

<p><strong>Update 3/28</strong></p>

<p>Still having random network loss, but haven't seen the internet drop. When monitoring the network with Wireshark, I still see a lot of SMB2 traffic right now for instance, it is constantly querying the Microsoft folder in App Data of students using publisher. The teacher trying to log in did not have access immediately, but it did seem to show up after a bit.</p>

<p>I was able to fix the missing background by typing ipconfig /release. It showed up immediately. I then typed in renew.</p>
","<windows-server-2012-r2><server-message-block><windows-8.1><roaming-profile>","2016-03-27 02:27:46"
"838445","Google Compute Engine: how to notify / email from unattended-upgrades?","<p>I use the unattended-upgrades package on my Debian Google Compute Engine instance to do security updates.  It has the option of sending an email to notify me when a security upgrade was done.  Do I need to go through the 3rd party bulk email services just to notify myself, or is there an easier way?</p>

<p>On google app engine, it was easy to do this via:</p>

<pre><code>from google.appengine.api import mail
mail.send_mail(....
</code></pre>

<p>which generated an email coming from cron@cron-1022.appspotmail.com.
I was hoping there was something correspondingly simple on compute engine.</p>

<p>I'd actually like to setup notifications / emails on other custom security events on my server.</p>

<p>Thanks.</p>
","<google-compute-engine>","2017-03-15 14:19:44"
"766315","What are common strategies for multiple database management in shared hosting?","<p>I've read a lot of in internet including canonical answers at serverfault. However, I still can't find the answer to the question - <code>What are common strategies for multiple database management in shared hosting?</code>.</p>

<p>What I've found out up till now is that shared hosting companies as a rule for SQL databases keep separate servers. Ok. But we can easily understand every database grows in size and earlier or later the space required for all databases can exceed the space of one certain server.  </p>

<p>The only solution came to my mind is the following - when a client make an order he specifies the size of the database (for example 50 MB). Hosting company having for example 500 GB server knows how many databases it can have because the clients specify the space in advance. However, this solution has a very serious disadvantage - when a client databases grows and he needs more space but the current server is out the size the support will have to stop client database to move it to another server. Besides it will require additional settings on site (minimum IP). However, according to the contract hosting company must provide 24/365 database work.</p>
","<hosting><web-hosting><capacity-planning>","2016-03-27 08:00:16"
"766336","How to route LAN traffic for OpenVPN?","<p><a href=""https://i.sstatic.net/9tI6b.png"" rel=""nofollow noreferrer"">Network Environment Setup Image</a></p>

<p>Speedtest.net without OpenVPN - 300Mbps<br>
Speedtest.net with OpenVPN - 10Mbps<br>
Accessing \\192.168.0.223 through Windows Explorer with OpenVPN - 80-100MB/s</p>

<p>Is accessing \\192.168.0.223 encrypted? If not, how do I encrypt the connection?</p>

<p>OpenVPN Server configuration</p>

<pre><code>port 443
proto tcp
dev tun
ca /etc/openvpn/easy-rsa/keys/ca.crt
cert /etc/openvpn/easy-rsa/keys/server.crt
key /etc/openvpn/easy-rsa/keys/server.key
dh /etc/openvpn/easy-rsa/keys/dh2048.pem
server 10.8.0.0 255.255.255.0
ifconfig-pool-persist ipp.txt
log-append /var/log/openvpn.log
push ""redirect-gateway def1""
push ""dhcp-option DNS 8.8.8.8""
push ""dhcp-option DNS 208.67.222.222""
keepalive 10 120
cipher AES-128-CBC
comp-lzo
persist-key
persist-tun
status openvpn-status.log
verb 3
</code></pre>

<p>iptables start-up script</p>

<pre><code>iptables -t filter -F
iptables -t nat -F
iptables -t nat -A POSTROUTING -s 10.8.0.0/24 -o eth0 -j SNAT --to 192.168.1.10
iptables -A FORWARD -m state --state RELATED,ESTABLISHED -j ACCEPT
iptables -A FORWARD -s ""10.8.0.0/24"" -j ACCEPT
iptables -A FORWARD -j REJECT
iptables -t nat -A POSTROUTING -s ""10.8.0.0/24"" -j MASQUERADE
</code></pre>
","<linux><iptables><routing><openvpn>","2016-03-27 11:56:18"
"766394","Why do different packet analyzers sometimes produce different results?","<p>I ran wireshark and windump at the same time. Both packet analyzers use the same winpcap library.</p>

<p>However after doing a row by row comparison of the results I noticed both every column between the 2 matches except for the protocol and info columns, 40% of the protocol column values did not match even though all the source, destination, length columns did. </p>

<p>So I was wondering why is there a 40% difference between the protocol columns when both analyzers use the same winpcap library and which packet capture should I trust to be most accurate?</p>
","<networking><wireshark><packet-capture><packet-sniffer>","2016-03-27 23:40:49"
"766404","Hot-hot systems and its effect on RPO/RTO","<p>How is hot-hot or live-live system related with RPO and/or RTO?</p>

<p>I want to understand what exactly a hot-hot system means? Is it really achievable? How does it affect the RPO and RTO?</p>

<p><a href=""https://stackoverflow.com/questions/36117050/live-live-system-and-rpo-rto"">Here's the link</a> of the question on stackoverflow which recommended to post the question on this forum.</p>

<p><strong>Note</strong>: <a href=""http://www.corexchange.com/blog/disaster-recovery-hot-warm-cold-sites-key-differences"" rel=""nofollow noreferrer"">Here's a link</a> for hot/live and cold environments.
<a href=""http://www.druva.com/blog/understanding-rpo-and-rto"" rel=""nofollow noreferrer"">Here's one</a> for RPO and RTO. Let me know if I should elaborate on any other terms</p>
","<networking><hosting>","2016-03-28 04:00:15"
"766415","ubuntu server 14.04 LTS unusual outgoing traffic","<p>I have virtual private host with <code>ubuntu server 14.04 LTS</code>. On this server, we serve web service. We run <code>apache2</code> and <code>tomcat</code> as web server.
<code>apache2</code> + <code>django1.8</code> and <code>tomcat8</code> recently, I observed the server uses 800G out-going traffic per day, but this server is not operational and has no requests.   </p>

<ol>
<li>How can I analyse this problem using simple methods and how can I trace packets?   </li>
<li>Using htop, there is a process <code>123.lock</code>. This process uses 100% cpu. I killed this process but after a few minuets it restarted. What should I do?
<a href=""https://i.sstatic.net/Xr1Yr.png"" rel=""nofollow noreferrer"">Relevant Picture</a></li>
</ol>
","<apache-2.2><ubuntu-14.04><traffic><htop><tomcat8>","2016-03-28 06:44:39"
"766431","EventLog ""Access Denied"" after installing an additional DC in a 2012 environment","<p>After promoting a new DC into an existing domain, Eventviewer shows the following:</p>

<p><a href=""https://i.sstatic.net/GGrqI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/GGrqI.png"" alt=""enter image description here""></a></p>

<p>I might as well mention that the Installation itself did not experience any issues</p>

<p>When looking at the C:\Windows\System32\winevt\Logs directory I do not see any entries related to AD\DFS\DS etc. Also comparing the permissions on the folder with a working DC did not show any differences.</p>
","<active-directory><domain-controller><windows-event-log>","2016-03-28 09:30:19"
"766656","Map shared network folder as servername","<p>We have an application that is regular deployment to live.
Of course we try to test it as much as possible before.
Many settings is stored in a ini-file</p>

<p>An example setting for a path</p>

<p><strong>Live</strong></p>

<pre><code>AccountingSystemExportPath=\\org2000Appserv\UserFolders\Econet
</code></pre>

<p><strong>Test</strong></p>

<pre><code>AccountingSystemExportPath=\\TestAppServ\Attracs\Peura\RC\UserFolders\Econet
</code></pre>

<p>So the problem is that setting is different in live and test.
What I want is a way to map a servername (\\Org2000AppServ in this case) to a shared network folder.</p>

<p>So something like</p>

<pre><code>SUBST Org2000AppServ \\TestAppServ\Attracs\Peura\RC
</code></pre>

<p>If above command work I could have same setting to live and test and testing would be more reliable. Any hints ?</p>
","<windows><network-share><mappeddrive>","2016-03-29 07:26:00"
"838831","AWS Load balancer configuration - EC2 instances should be of same size and OS?","<p>I want to create an AWS Elastic Classic Load balancer with different EC2 instances. I have different EC2 instances: For example m3 instance with Ubuntu OS and the other instance is m4-large instance with RHEL7 OS. </p>

<p>Is it possible to pull these both instances under same load balancer? What configuration should be done if its different instances?</p>
","<amazon-ec2><amazon-web-services><load-balancing>","2017-03-17 06:52:46"
"838873","VMs are stealing hostnames of other running VMs","<p>When I deploy a fresh CentOS VM with DHCP enabled onto our corporate sandbox ESXi host, it sometimes steals the hostname of another running VM while booting up. My gut feeling is that this is a serious security problem or at least a nasty misconfiguration of our network.</p>

<p>I have checked that the IP addresses of the VM with the stolen name and the original VM are different.</p>

<p>Why could this be happening an how should we fix it?</p>
","<networking><dhcp><hostname>","2017-03-17 09:31:15"
"981221","Who listens to the port","<p>I have a conflict between the mail server and one more service.</p>

<pre><code>netstat -ltnp | grep -w ': 25'
</code></pre>

<p>Conclusion:</p>

<pre><code>(Not all processes could be identified, non-owned process info
  will not be shown, you would have to be root to see it all.)
tcp6 0 0 ::: 25 ::: * LISTEN -
The problem is that I do not know who is listening on port 25. How can this be determined?
</code></pre>

<hr>

<p>I fixed it:</p>

<pre><code>sudo netstat -tulpn | grep LISTEN
[sudo] password for eurvanov: 
tcp        0      0 127.0.0.1:63342         0.0.0.0:*               LISTEN      2967/java           
tcp        0      0 127.0.0.1:5939          0.0.0.0:*               LISTEN      1445/teamviewerd    
tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      583/systemd-resolve 
tcp        0      0 127.0.0.1:631           0.0.0.0:*               LISTEN      709/cupsd           
tcp        0      0 0.0.0.0:25              0.0.0.0:*               LISTEN      7935/master         
tcp        0      0 127.0.0.1:9050          0.0.0.0:*               LISTEN      955/tor             
tcp        0      0 127.0.0.1:6942          0.0.0.0:*               LISTEN      2967/java           
tcp        0      0 0.0.0.0:45577           0.0.0.0:*               LISTEN      2967/java           
tcp6       0      0 ::1:631                 :::*                    LISTEN      709/cupsd           
tcp6       0      0 :::25                   :::*                    LISTEN      7935/master         
tcp6       0      0 :::10012                :::*                    LISTEN      2199/docker-proxy  
</code></pre>
","<ubuntu><networking><tcp>","2019-08-29 18:49:30"
"766851","External Relay Server when server is on Blacklist","<p>I have a Server with Cpanel , that is in a network with Blacklist (no way to change the Ip , all dirty ) , what i want to do , is install another server in another network that all emails sended can pass trough it. I dont know exactly the words that i can use to look for something like this. Any help will be welcome. </p>

<p>Thanks </p>
","<exim><blacklist>","2016-03-29 19:13:02"
"838986","Can't restart MySQL server on CentOS 7 because of mariadb.service failing","<p>I have MySQL server installed on a CentOS 7. However, I stopped it once and now I can't restart it because I get this error:</p>

<pre><code>Failed to start database :

Redirecting to /bin/systemctl start  mariadb.service
Job for mariadb.service failed because the control process exited with error code. See ""systemctl status mariadb.service"" and ""journalctl -xe"" for details.
</code></pre>

<p>I have worked with MySQL for many years now but this is a new server I am moving to and I have never heard about MariaDB so I have no idea what it is. Can you please help me fix this error?</p>

<p>Thank you.</p>

<hr>

<p><em>FULL ERROR LOG</em></p>

<pre><code>â— mariadb.service - MariaDB database server
   Loaded: loaded (/usr/lib/systemd/system/mariadb.service; enabled; vendor preset: disabled)
   Active: failed (Result: exit-code) since Fri 2017-03-17 13:48:21 EDT; 2min 13s ago
  Process: 10775 ExecStartPost=/usr/libexec/mariadb-wait-ready $MAINPID (code=exited, status=1/FAILURE)
  Process: 10774 ExecStart=/usr/bin/mysqld_safe --basedir=/usr (code=exited, status=1/FAILURE)
  Process: 10746 ExecStartPre=/usr/libexec/mariadb-prepare-db-dir %n (code=exited, status=0/SUCCESS)
 Main PID: 10774 (code=exited, status=1/FAILURE)

Mar 17 13:48:20 server mysqld_safe[10774]: 170317 13:48:20 mysqld_safe Starting mysqld daemon                 with databases from /var/lib/mysql
Mar 17 13:48:20 server mysqld_safe[10774]: /usr/bin/mysqld_safe: line 139: /var/log/mariadb/ma                riadb.log: Permission denied
Mar 17 13:48:20 server mysqld_safe[10774]: /usr/bin/mysqld_safe: line 183: /var/log/mariadb/ma                riadb.log: Permission denied
Mar 17 13:48:20 server mysqld_safe[10774]: 170317 13:48:20 mysqld_safe mysqld from pid file /v                ar/run/mariadb/mariadb.pid ended
Mar 17 13:48:20 server mysqld_safe[10774]: /usr/bin/mysqld_safe: line 139: /var/log/mariadb/ma                riadb.log: Permission denied
Mar 17 13:48:20 server systemd[1]: mariadb.service: main process exited, code=exited, status=1                /FAILURE
Mar 17 13:48:21 server systemd[1]: mariadb.service: control process exited, code=exited status                =1
Mar 17 13:48:21 server systemd[1]: Failed to start MariaDB database server.
Mar 17 13:48:21 server systemd[1]: Unit mariadb.service entered failed state.
Mar 17 13:48:21 server systemd[1]: mariadb.service failed.
</code></pre>
","<mysql><mariadb>","2017-03-17 17:34:34"
"766866","nagios 4.1.1 can't start in Centos 6.0","<p>after runing:</p>

<pre><code>/usr/local/nagios/bin/nagios -v /usr/local/nagios/etc/nagios.cfg

Nagios Core 4.1.1
Copyright (c) 2009-present Nagios Core Development Team and Community Contributors
Copyright (c) 1999-2009 Ethan Galstad
Last Modified: 08-19-2015
License: GPL

Website: https://www.nagios.org
Reading configuration data...
Warning: use_embedded_perl_implicitly is deprecated and will be removed.
Warning: enable_embedded_perl is deprecated and will be removed.
Warning: p1_file is deprecated and will be removed.
Warning: sleep_time is deprecated and will be removed.
Warning: external_command_buffer_slots is deprecated and will be removed. All commands are always processed upon arrival
Warning: command_check_interval is deprecated and will be removed. Commands are always handled on arrival
   Read main config file okay...
Warning: failure_prediction_enabled is obsoleted and no longer has any effect in host type objects (config file '/usr/local/nagios/etc/objects/templates.cfg', starting at line 52)
Warning: failure_prediction_enabled is obsoleted and no longer has any effect in service type objects (config file '/usr/local/nagios/etc/objects/templates.cfg', starting at line 206)
   Read object config files okay...

Running pre-flight check on configuration data...

Checking objects...
        Checked 241 services.
        Checked 102 hosts.
        Checked 23 host groups.
        Checked 0 service groups.
        Checked 4 contacts.
        Checked 4 contact groups.
        Checked 37 commands.
        Checked 5 time periods.
        Checked 0 host escalations.
        Checked 0 service escalations.
Checking for circular paths...
        Checked 102 hosts
        Checked 0 service dependencies
        Checked 0 host dependencies
        Checked 5 timeperiods
Checking global event handlers...
Checking obsessive compulsive processor commands...
Checking misc settings...

Total Warnings: 0
Total Errors:   0

Things look okay - No serious problems were detected during the pre-flight check
</code></pre>

<p>This was upgrade from 3.5 to 4.1.1 which is almost new configuration with old commands that I used. Is there any trick in 4.1.1 ?</p>
","<nagios>","2016-03-29 20:31:25"
"839035","MySQL is using too much CPU","<p>I have i7 3770 dedicated server with 32GB ram, and Centos7 is installed, below are CPU details</p>

<pre><code>[root@server ~]# lscpu
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                8
On-line CPU(s) list:   0-7
Thread(s) per core:    2
Core(s) per socket:    4
Socket(s):             1
NUMA node(s):          1
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 58
Model name:            Intel(R) Core(TM) i7-3770 CPU @ 3.40GHz
Stepping:              9
CPU MHz:               1712.218
BogoMIPS:              6799.29
Virtualization:        VT-x
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              8192K
NUMA node0 CPU(s):     0-7
</code></pre>

<p>and mysql is using too much CPU on it. I am using it for web hosting and there is no so demanding account with a lot of visits. Databases are mixed, some are InnoDB and other are MyISAM, and I can't make them all to InnoDB or MyISAM, it needs to stay like this.</p>

<p>This is my mysql configuration</p>

<pre><code>[mysqld]
default-storage-engine=InnoDB
performance-schema=0
max_allowed_packet=268435456
join_buffer_size = 24M
explicit_defaults_for_timestamp = 1
read_rnd_buffer_size=1024K
read_buffer_size=4M
sort_buffer_size=4M
table_open_cache=8000
key_buffer_size=2G
local-infile=0
tmp_table_size=32M
max_heap_table_size=32M
slow_query_log=1
slow_query_log_file=""/var/log/mysql-slow-queries.log""
long_query_time=10

innodb_buffer_pool_size=10G
innodb_buffer_pool_instances = 10
innodb_log_file_size=1024M
innodb_file_per_table=1
innodb_flush_method=O_DIRECT
innodb_autoinc_lock_mode=0
innodb_read_io_threads=4
innodb_write_io_threads=8
</code></pre>

<p>Mysql tuner results are this</p>

<pre><code> -------- Storage Engine Statistics -----------------------------------------------------------------
[--] Status: +ARCHIVE +BLACKHOLE +CSV -FEDERATED +InnoDB +MEMORY +MRG_MYISAM +MyISAM +PERFORMANCE_SCHEMA
[--] Data in MyISAM tables: 911M (Tables: 2990)
[--] Data in InnoDB tables: 3G (Tables: 2349)
[--] Data in MEMORY tables: 0B (Tables: 38)
[OK] Total fragmented tables: 0

-------- Security Recommendations ------------------------------------------------------------------
[OK] There are no anonymous accounts for any database users
[OK] All database users have passwords assigned
[!!] User 'darssopt_mi@%' hasn't specific host restriction.
[!!] User 'darssoptions@%' hasn't specific host restriction.
[!!] There is no basic password file list!

-------- CVE Security Recommendations --------------------------------------------------------------
[--] Skipped due to --cvefile option undefined

-------- Performance Metrics -----------------------------------------------------------------------
[--] Up for: 6h 22m 27s (1M q [79.246 qps], 264K conn, TX: 6G, RX: 190M)
[--] Reads / Writes: 93% / 7%
[--] Binary logging is disabled
[--] Physical Memory     : 31.0G
[--] Max MySQL memory    : 17.0G
[--] Other process memory: 1.4G
[--] Total buffers: 12.0G global + 33.2M per thread (151 max threads)
[--] P_S Max memory usage: 0B
[--] Galera GCache Max memory usage: 0B
[OK] Maximum reached memory usage: 12.8G (41.33% of installed RAM)
[OK] Maximum possible memory usage: 17.0G (54.76% of installed RAM)
[OK] Overall possible memory usage with other process is compatible with memory available
[OK] Slow queries: 0% (21/1M)
[OK] Highest usage of available connections: 15% (23/151)
[OK] Aborted connections: 0.08%  (219/264034)
[!!] name resolution is active : a reverse name resolution is made for each new connection and can reduce performance
[OK] Query cache is disabled by default due to mutex contention on multiprocessor machines.
[OK] Sorts requiring temporary tables: 3% (8K temp sorts / 235K sorts)
[!!] Joins performed without indexes: 3128
[OK] Temporary tables created on disk: 19% (22K on disk / 115K total)
[OK] Thread cache hit rate: 99% (775 created / 264K connections)
[OK] Table cache hit rate: 99% (5K open / 6K opened)
[OK] Open file limit used: 15% (6K/40K)
[OK] Table locks acquired immediately: 99% (1M immediate / 1M locks)

-------- Performance schema ------------------------------------------------------------------------
[--] Performance schema is disabled.

-------- ThreadPool Metrics ------------------------------------------------------------------------
[--] ThreadPool stat is disabled.

-------- MyISAM Metrics ----------------------------------------------------------------------------
[!!] Key buffer used: 18.6% (399M used / 2B cache)
[OK] Key buffer size / total MyISAM indexes: 2.0G/181.8M
[OK] Read Key buffer hit rate: 100.0% (79M cached / 8K reads)
[!!] Write Key buffer hit rate: 93.5% (460K cached / 29K writes)

-------- AriaDB Metrics ----------------------------------------------------------------------------
[--] AriaDB is disabled.

-------- InnoDB Metrics ----------------------------------------------------------------------------
[--] InnoDB is enabled.
[OK] InnoDB buffer pool / data size: 10.0G/3.4G
[OK] InnoDB buffer pool instances: 10
[--] InnoDB Buffer Pool Chunk Size not used or defined in your version
[OK] InnoDB Read buffer efficiency: 100.00% (4991812913 hits/ 4991885443 total)
[!!] InnoDB Write Log efficiency: 41.29% (38836 hits/ 94048 total)
[OK] InnoDB log waits: 0.00% (0 waits / 55212 writes)

-------- TokuDB Metrics ----------------------------------------------------------------------------
[--] TokuDB is disabled.

-------- Galera Metrics ----------------------------------------------------------------------------
[--] Galera is disabled.

-------- Replication Metrics -----------------------------------------------------------------------
[--] Galera Synchronous replication: NO
[--] No replication slave(s) for this server.
[--] This is a standalone server.

-------- Recommendations ---------------------------------------------------------------------------
General recommendations:
    Restrict Host for user@% to user@SpecificDNSorIp
    MySQL started within last 24 hours - recommendations may be inaccurate
    Configure your accounts with ip or subnets only, then update your configuration with skip-name-resolve=1
    Adjust your join queries to always utilize indexes. Please note this
    calculation is made by adding Select_full_join + Select_range_check
    status values and triggered when the total &gt;250
Variables to adjust:
    join_buffer_size (&gt; 24.0M, or always use indexes with joins)
</code></pre>

<p>results of top</p>

<pre><code>  top - 14:51:35 up 4 days, 22:36,  3 users,  load average: 5.88, 5.85, 6.11
Tasks: 275 total,   3 running, 271 sleeping,   0 stopped,   1 zombie
%Cpu(s):  9.1 us,  0.8 sy,  0.1 ni, 67.1 id, 22.9 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem : 32460092 total,  1248560 free,  5231404 used, 25980128 buff/cache
KiB Swap: 16760828 total, 16506348 free,   254480 used. 26023012 avail Mem

    PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 670307 mysql     20   0 15.638g 3.728g   8680 S  62.1 12.0 103:40.03 mysqld
 792649 adv+      20   0  235404  29852   8320 R   3.3  0.1   0:00.10 php-cgi
 792670 bdd       20   0  226492  20820   8292 R   1.3  0.1   0:00.04 php-cgi
   3359 root      30  10  277760  30508   2720 D   0.7  0.1  30:35.10 python2.7
     17 root      20   0       0      0      0 S   0.3  0.0  13:27.67 rcu_sched
     19 root      20   0       0      0      0 S   0.3  0.0  12:00.05 rcuos/1
     21 root      20   0       0      0      0 S   0.3  0.0   3:48.60 rcuos/3
    469 root       0 -20       0      0      0 S   0.3  0.0   8:35.86 kworker/+
    545 root      20   0       0      0      0 D   0.3  0.0   3:40.02 jbd2/md2+
   3336 root      30  10  417280   9084   1244 S   0.3  0.0  16:16.79 python2.7
 599324 root      20   0  107804  28732   2080 S   0.3  0.1   0:06.44 tailwatc+
 670290 root      20   0  186500  18572   3344 S   0.3  0.1   0:02.42 cPhulkd +
 792376 nobody    20   0  216284  24376   2428 S   0.3  0.1   0:00.02 httpd
 792449 superst   20   0   19972   2852   2116 S   0.3  0.0   0:00.05 pop3
 792498 root      20   0  155924   2356   1544 R   0.3  0.0   0:00.07 top
      1 root      20   0  338544   3812   2176 S   0.0  0.0   2:43.82 systemd
      2 root      20   0       0      0      0 S   0.0  0.0   0:00.18 kthreadd
</code></pre>

<p>This is result of iostat</p>

<pre><code>[root@server ~]# iostat
Linux 3.10.0-427.36.1.lve1.4.40.el7.x86_64 (server.connect.rs)  03/23/2017     _x86_64_ (8 CPU)

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           4.12    0.42    0.68   16.87    0.00   77.92

Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn
sdb              65.24      2957.95       981.59 1264085940  419483991
sda              68.70      2749.45       981.59 1174981406  419483991
md1               0.00         0.05         0.00      23171         38
md0               0.70         0.87         1.92     370128     820580
md3              42.31       496.39       318.45  212134125  136090304
md2              68.18       104.71       650.46   44746045  277975140
</code></pre>

<p>Mysql is almost always above 40%, up to 100%</p>

<p>Yesterday I added disabled mysql services for 5 minutes and load was below 1.</p>

<p>please advise how to improve it</p>
","<centos><mysql><cpu-usage>","2017-03-17 21:35:59"
"839036","regex excluding lines with x number of \'s","<p>I am sure this has been asked and solved before but I am stuck on this simple regex expression. I am using powershell and .net for the regex.</p>

<p>Let's say I have a file which contains these lines:
C:\share\my file.txt
C:\share\folder1\anotherfile.txt
C:\share\folder1\yetanotherfile.txt</p>

<p>What regex expression will allow me to only return C:\share\my file.txt while ignoring C:\share\folder1... I don't know how to tell regex to ignore the line if it contains more than 2 \'s. </p>

<p>Thanks in advance.</p>
","<powershell><regex>","2017-03-17 21:36:46"
"766921","how to increase the size of the drive in aws","<p>Could you please let us know how to increase the size of the drive (c drive). This is Windows 2008 server R2. Please let me know the steps to be followed in AWS console to increase the disk size. The X person will initiate AMI, backup &amp; snapshot of a server. After this process he asked us to increase the size of the C drive.</p>
","<windows-server-2008-r2><amazon-web-services>","2016-03-30 05:14:12"
"839206","Free Port 80 for Caddy","<p>I'm trying to use port 80 on caddy but I'm getting an error saying the port is already in use. <code>listen tcp :80: bind: address already in use.</code> When I check to see what is using it this is what I get. </p>

<pre><code>sonar@SonarQube:~$ netstat -an | grep "":80""
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN     
tcp        0      0 10.1.1.5:42214          168.63.129.16:80        TIME_WAIT  
tcp        0      0 10.1.1.5:42222          168.63.129.16:80        TIME_WAIT  
tcp6       0      0 :::80                   :::*                    LISTEN     
</code></pre>

<p>How can I kill whatever is running on 80? Could running <code>sudo setcap 'cap_net_bind_service=+ep' /usr/local/bin/caddy cause it?</code> If so how do I undo it ?</p>

<h3>update (after adding -p)</h3>

<pre><code>tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      -               
tcp        0      0 10.1.1.5:42768          168.63.129.16:80        TIME_WAIT   -               
tcp        0      0 10.1.1.5:42760          168.63.129.16:80        TIME_WAIT   -               
tcp6       0      0 :::80                   :::*                    LISTEN      -   
</code></pre>

<h3>update 2</h3>

<pre><code>tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      1316/nginx -g daemo
tcp        0      0 10.1.1.5:39152          91.189.95.83:80         TIME_WAIT   -               
tcp        0      0 10.1.1.5:49340          168.63.129.16:80        TIME_WAIT   -               
tcp        0      0 10.1.1.5:46654          91.189.88.161:80        TIME_WAIT   -               
tcp        0      0 10.1.1.5:48740          91.189.88.162:80        TIME_WAIT   -               
tcp        0      0 10.1.1.5:49314          52.176.58.79:80         TIME_WAIT   -               
tcp        0      0 10.1.1.5:49284          168.63.129.16:80        TIME_WAIT   -               
tcp6       0      0 :::80                   :::*                    LISTEN      1316/nginx -g daemo
</code></pre>
","<linux><ubuntu><nginx><web-server><tcp>","2017-03-19 07:27:06"
"839212","install windows 10 in esxi 6 but disk speed in machine","<p>I have 2 Windows 10 in esxi 6 on ssd datastor but hard speed in task manager is 3 mb/s 
I install esxi in dl380 g9 server .
Is it normall speed ? 
How i can solve this problem?</p>
","<vmware-esxi><ssd><windows-10><hpe>","2017-03-19 08:19:14"
"839238","Issue with resolving domains on CentOS 6","<p>I was having an issue with updating YUM and also issues with CURL. This led me to find issues with resolving:</p>

<pre><code>root@server [~]# host google.com
;; connection timed out; trying next origin
;; connection timed out; no servers could be reached
root@server [~]# host google.com
google.com has address 216.58.209.238
;; connection timed out; trying next origin
Host google.com not found: 3(NXDOMAIN)
google.com mail is handled by 20 alt1.aspmx.l.google.com.
google.com mail is handled by 10 aspmx.l.google.com.
google.com mail is handled by 40 alt3.aspmx.l.google.com.
google.com mail is handled by 30 alt2.aspmx.l.google.com.
google.com mail is handled by 50 alt4.aspmx.l.google.com.
root@server [~]# host google.com
;; connection timed out; trying next origin
;; connection timed out; no servers could be reached
root@server [~]# host google.com
;; connection timed out; trying next origin
;; connection timed out; no servers could be reached
root@server [~]# host google.com
google.com has address 216.58.209.238
google.com has IPv6 address 2a00:1450:4007:80f::200e
google.com mail is handled by 50 alt4.aspmx.l.google.com.
google.com mail is handled by 10 aspmx.l.google.com.
google.com mail is handled by 20 alt1.aspmx.l.google.com.
google.com mail is handled by 40 alt3.aspmx.l.google.com.
google.com mail is handled by 30 alt2.aspmx.l.google.com.
root@server [~]# host google.com
;; connection timed out; trying next origin
;; connection timed out; no servers could be reached
root@server [~]# host google.com
;; connection timed out; trying next origin
;; connection timed out; no servers could be reached
root@server [~]# host google.com
;; connection timed out; trying next origin
;; connection timed out; no servers could be reached
root@server [~]# host google.com
google.com has address 216.58.209.238
;; connection timed out; trying next origin
;; connection timed out; no servers could be reached
;; connection timed out; trying next origin
;; connection timed out; no servers could be reached
root@server [~]# host google.com
;; connection timed out; trying next origin
Host google.com not found: 3(NXDOMAIN)
root@server [~]# host google.com
google.com has address 216.58.209.238
;; connection timed out; trying next origin
Host google.com not found: 3(NXDOMAIN)
;; connection timed out; trying next origin
;; connection timed out; no servers could be reached
root@server [~]# host google.com
google.com has address 216.58.209.238
google.com has IPv6 address 2a00:1450:4007:80f::200e
google.com mail is handled by 40 alt3.aspmx.l.google.com.
google.com mail is handled by 20 alt1.aspmx.l.google.com.
google.com mail is handled by 50 alt4.aspmx.l.google.com.
google.com mail is handled by 10 aspmx.l.google.com.
google.com mail is handled by 30 alt2.aspmx.l.google.com.
</code></pre>

<p>resolv.conf:</p>

<pre><code>root@server [~]# cat /etc/resolv.conf
search ovh.net
nameserver 8.8.8.8
nameserver 8.8.4.4
</code></pre>

<p>Yum error:</p>

<pre><code>root@server [~]# yum update
Loaded plugins: fastestmirror, rhnplugin
Setting up Update Process
Loading mirror speeds from cached hostfile
 * cloudlinux-x86_64-server-6: cl-mirror.ptisp.com
http://download.fedoraproject.org/pub/epel/6/x86_64/repodata/repomd.xml: [Errno 14] PYCURL ERROR 6 - ""Couldn't resolve host 'download.fedoraproject.org'""
Trying other mirror.
Error: Cannot retrieve repository metadata (repomd.xml) for repository: epel. Please verify its path and try again
</code></pre>

<p>Can anyone point me in the right direction of a solution please?</p>
","<networking><centos>","2017-03-19 11:55:45"
"906311","Hp Proliant dl380 g6: over temp error","<p>Hey guys I just received a dl380 from a auction and can’t get it to boot because after 30 seconds it shuts down and the over temp led is solid Amber on the front. I have removed everything but a processor in slot one and 2 sticks of ram. I reapplied thermal paste on the processor and reseated the cpu heat sink but yet I’m still getting the over heat error. I was wondering if you guys know a way for me to narrow down what’s wrong and hopefully fix it. Thank you in advanced and i can upload pictures and anything that is needed.</p>
","<hp-proliant>","2018-04-06 02:22:40"
"767498","Creating DNS CNAME or MX aliases","<p>How can I create a CNAME or an MX record alias, for example, I need smtp.mydomain.com to resolve to smtp.yandex.com so that on my email client I should enter smtp.mydomain.com instead of smtp.yandex.com. I am using CloudFlare as my DNS manager and my domain is registered on Godaddy.</p>

<p>Here is my CloudFlare settings page</p>

<p><a href=""https://i.sstatic.net/uoVAE.jpg"" rel=""nofollow noreferrer"">CloudFlare DNS settings</a></p>

<p>Thank you</p>
","<domain-name-system><mx-record><cname-record><cloudflare>","2016-04-01 07:09:27"
"767662","What are these black plastic things","<p>I just bought my first <a href=""http://www.logic-case.com/products/rackmount-chassis/3u/3u-standard-chassis-14-x-35-hdd-sc-39650g-atx/"" rel=""nofollow noreferrer"">server case</a> (Yay  me). With it came a box full of these plastic things that I have no idea what they are for or what they are called. There are twenty of these things in total.</p>

<p><a href=""https://i.sstatic.net/zW5OA.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zW5OA.jpg"" alt=""Box of twenty plastic things""></a></p>
","<hardware>","2016-04-01 21:43:05"
"906942","How to enable default Nginx object caching to all sites at once?","<p>Say I have 20 sites and I wish to automate the process of pasting 1X1 <code>nginx.conf</code> caching directives and 1X20 caching directives per 20 different virtual hosts I have?</p>

<p>Also, my virtual hosts have already been modified by Certbot and became a bit messy (Certbot <code>0.21.1-1+ubuntu16.04.1+certbot+0.2</code> adds many spaces and comments), so I'm now even less sure how to efficiently enable Nginx object caching for all my sites.</p>

<p>How would you do so efficiently?</p>
","<nginx><cache><certbot>","2018-04-10 12:29:41"
"768054","Clone a RHEL 5 box into a smaller disk","<p>I'm trying to clone a physical RHEL 5 box into VMware. That box contains a 1 TB disk which is hardly used (80 GB). Thus I want the new virtual machine to have a 100 GB disk instead of wasting a new 1 TB.</p>

<p>The free disk space in the VM host is only 200 GB, not 1 TB. Therefore, the Vmware converter client cannot run successfully -- it requires 1 TB of free disk space in the VMware host.</p>

<p>Firstly I tried <code>mondo-mindi</code> software to backup/restore because it does not need to power off the physical box and it allows create ISO images, but after two weeks of failed attempts I got fed up and I eventually quit.</p>

<p>Do you know any software to clone a RHEL 5 server?</p>

<p>Thanks in advance</p>
","<redhat><rhel5><cloning>","2016-04-04 15:04:46"
"840147","I can’t connect to my Azure VM","<p>After I used this command:</p>

<pre><code>ifdown eth0
</code></pre>

<p>can't connect to my VM (CentOS 6) with ssh on cloud Microsoft azure. 
How can I fix it?</p>
","<azure><linux-networking><centos7>","2017-03-23 13:20:59"
"907209","Whitelist domain name for speedtest.net to allow a test to run","<p>Using a 4G Router (HUAWEI B315) with a domain name whitelist to prevent excessive data usage.</p>

<p>It will only allow our custom API through. <br/>
However when testing the router at new venues it would be great to run speed tests without removing the domain name whitelist everytime.</p>

<p>So this is an example of the current whitelisted domains</p>

<pre><code>api.example.com
speedtest.net
</code></pre>

<p>Running a speed test fails, it simply doesn't connect to a server.
Is it possible to white-list speedtest.net? to allow the speedtest Android App to complete a test?</p>

<p>If not is there another speed test service which could be white-listed easily?</p>
","<http><router><domain-name><whitelist>","2018-04-11 16:20:23"
"840279","Server DNS Resolve issue","<p>So I'm currently using VMware Pro 12 for the purpose of a lab environment. The problem I have now is that on my domain controller I can't ping my client, although the client is configured properly.</p>

<p>As a network adapter I use bridge mode.</p>

<p>The IPs are the following:</p>

<p>hostname: server01<br>
domain: contoso.com<br>
IP: 192.168.1.115<br>
Subnet mask: 255.255.255.0<br>
DNS: 192.168.1.115  </p>

<p>the client:</p>

<p>IP: 192.168.1.138<br>
Subnet Mask: 255.255.255.0<br>
DNS: 192.168.1.115  </p>

<p>Although I managed to join the domain after long troubleshooting and DNS resolve problems, on the server site I still can't ping the address from the client.</p>

<p>I have some experience with server 2012 but I wanted to build my own lab environment with Server 2012 R2. I'm not quite sure where I could find the error what's causing this problem....</p>
","<windows><domain-name-system><windows-server-2012-r2><domain-controller><reverse-dns>","2017-03-23 21:07:42"
"768295","Batch import local user in Windows Server 2012 R2 with a CSV file","<p>I've got a Windows Server 2012 r2 without domain and active users.
No Remote Server Administration Tools (RSAT) are installed and no Csvde command.
Is there a way to import users from a csv file ?
Thank you in advance.
Alessandro</p>
","<windows-server-2012-r2><users><import>","2016-04-05 13:51:58"
"768368","ubuntu network manager openvpn overwrites default route","<p>I am running VPN server on aws account which hosted under Account A. 
I have another aws Account B, where I have a Ubuntu ec2 instance. On this instance I have installed Openvpn and configured client settings. </p>

<p>Whenever I start my Openvpn client on Account B Ubuntu ec2 instance, I am getting kicked out of server as Ubuntu network manager Openvpn overwrites default route. Also I am unable to ssh connection again without rebooting my host. </p>

<p>My default route before starting vpn client :</p>

<pre><code>route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         172.0.16.1     0.0.0.0         UG    0      0        0 eth0
172.0.16.0     0.0.0.0         255.255.240.0   U     0      0        0 eth0
</code></pre>

<p>Default route after starting openvpn client:</p>

<pre><code>route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         10.8.0.9        128.0.0.0       UG    0      0        0 tun0
0.0.0.0         172.0.16.1     0.0.0.0         UG    0      0        0 eth0
10.0.0.0        10.8.0.9        255.255.0.0     UG    0      0        0 tun0
10.8.0.1        10.8.0.9        255.255.255.255 UGH   0      0        0 tun0
10.8.0.9        0.0.0.0         255.255.255.255 UH    0      0        0 tun0
128.0.0.0       10.8.0.9        128.0.0.0       UG    0      0        0 tun0
172.0.16.0     0.0.0.0         255.255.240.0   U     0      0        0 eth0
</code></pre>

<p>I did <a href=""https://www.google.com.sg/search?q=network-manager-openvpn%20command&amp;rct=j#q=ubuntu%20network%20manager%20openvpn%20overwrites%20default%20route"" rel=""nofollow noreferrer"">Googled</a> it and found that it is known bug and the workaround provided is not working in my case as it is <strong>Ubuntu server</strong>(<strong>CLI not GUI</strong>) on aws with dhcp settings, which I would not like to alter to fix the issue. As there can be multiple server in future. </p>

<p>Have any faced this issue and solved this? Any suggestion would be a great help. </p>

<p><strong>Just for information</strong> : Same openvpn setting on the server and client config works for my laptop(running windows though) and other machine and I can connect to aws ec2 instances from my lappy without any issue.</p>
","<ubuntu><openvpn>","2016-04-05 18:39:36"
"768372","How to keep authorized_keys in a custom folder","<p>I would like to keep authorized_keys of OpenSSH for Windows in a custom folder and not in a C:\Users\SomeUserName folder.</p>

<p>Is it possible?</p>

<p>The manual page does not help <a href=""http://www.openssh.com/manual.html"" rel=""nofollow noreferrer"">http://www.openssh.com/manual.html</a></p>
","<windows><ssh>","2016-04-05 19:12:46"
"840472","one script to monitor incoming ssh connections to the server","<p>I want to make a script to monitor incoming ssh connections to the server.
Try ForceCommand in sshd_config but it will not let me in if I put it there
my script.
<br>
Match User * <br>
X11Forwarding no <br>
AllowTcpForwarding no <br>
ForceCommand ssh /usr/local/include/ssh.sh<br></p>

<pre><code>#!/bin/bash
DP=/usr/local/include;
DT=$(w | grep pts | grep -v :0.0 &gt; $DP/tmp.log);
FDATE=$(date +%Y-%m-%d);
while read -r values
do
PTSU=$(echo $values |  awk '{print $2}');
IP=$(echo $values |  awk '{print $3}');
LTIME=$(echo $values |  awk '{print $4}');
USA=$(echo $values |  awk '{print $1}');


  PROC=$( cat /proc/net/arp | grep $IP );
  MAC=$(echo $PROC |  awk '{print $4}');

  echo ""$USA $PTSU $IP $MAC $FDATE $LTIME"" &gt;&gt;$DP/ssh.log;

done &lt; $DP/tmp.log
rm $DP/tmp.log
</code></pre>
","<linux><bash><monitoring><scripting>","2017-03-24 20:01:23"
"983970","Secondary SSL Certificate on same IIS application","<p>On Azure web apps, there is a default SSL certificate signed by Azure. If you add a custom domain and a certificate for that domain the Azure certificate still exists and it is visible with ssl checker <a href=""https://www.ssllabs.com/ssltest/"" rel=""nofollow noreferrer"">https://www.ssllabs.com/ssltest/</a>. This is something that suite me for a specific application.</p>

<p>How can I do the same thing on an o premise server with Windows 2012 R2 and IIS 8.5? I want to have 2 certificates for the same domain. Is this possible?</p>
","<ssl><iis><windows-server-2012-r2>","2019-09-12 11:33:07"
"840615","Issue with high server load centos","<p>I'm having some major issues with one of my servers.</p>

<p>Server load is extremely high: load average: 142.28, 144.80, 139.6</p>

<p>The server is running Centos 6 and has a Perl image sharing script running on it. The server has moderate traffic but for some reason the load is extremely high.</p>

<p>Running iotop I see just a bunch of httpd requests but there's no iowait or problems with disc R/W. </p>

<p>Running the top command there's lots of CGI Zombie processes that are all ""Defunct"". I'm not sure how to handle them because I've set keepalive to off to prevent something like this from occurring.</p>

<p>I need some advise. I've looked at many questions on here and I'm not sure what to do. I've installed CSF and configured it to block possible DDOS attacks. I've also installed mod_evasive.</p>

<p>The server's obviously super sluggish. Any advise would be appreciated.</p>

<p><strong>Edited</strong></p>

<p>Running netstat -an | wc -l shows 13132 connections.</p>
","<apache-2.2><centos><cgi>","2017-03-26 02:29:01"
"768585","Container (Apache) exit after 1 seconds","<p>I've made a mistake in my apache2.conf and so every time I try to start the container, it exits:</p>

<pre><code>#docker start portail
portail
#docker ps -a
    CONTAINER ID        IMAGE                   COMMAND                CREATED             STATUS                      PORTS                                                  NAMES
0c2cd80ab3fc        jnyryan/simplesamlphp   ""/usr/sbin/apache2ct   25 hours ago        Exited (1) 1 seconds ago                                                          portail
</code></pre>

<p>So how can I resolve my problem? I've made many changes in the container and I don't want to lose it.</p>

<p>Do I need to change the start command of the container? If so, how?</p>
","<docker><containers><start>","2016-04-06 15:18:07"
"907646","Does Vagrant introduce another layer between host OS and guest OS","<p>If I use Vagrant, does it introduce another layer of resource users inserting itself between host OS and VM? Does it always run between host OS and VMWare/Vbox? Should I provide for extra RAM / CPU if I plan on using Vagrant to facilitate extra Vagrant processes running together with every VM it starts?</p>

<p>Or is it, in the simplest terms, just a collection of commands that tells VM how to assemble and run itself and there is no Vagrant process running with every VM instance it starts? </p>

<p><strong>Edit</strong>: this is one question, just split into several questions to better describe the intent. The RAM / CPU question does not relate to running VM's per se, it is just another way of asking if Vagrant itself consumes any extra resources.</p>
","<virtualization><virtual-machines><vagrant>","2018-04-14 11:47:38"
"840680","What causes a server to be slow to respond when it has a very low load average?","<p>I have an Ubuntu server that has recently been intermittently slow to respond - 30s+ to connect to ssh or web servers, and New Relic shows regular interruptions in the data (but no actual activity).</p>

<p>Every time I look at it the load average is under 0.1. I can't see signs of excessive activity in any of the services running.</p>

<p>If there is no sign of activity on the server how can I find what is making it slow to respond?</p>
","<ubuntu>","2017-03-26 16:15:42"
"984106","google vm instances does not seem to be on correct region","<p>I created one google vm machine engine instance in eu-west1-b region. In GCP Console seems to be in eu-west1-b region. However when I try to gelocate my ip's they seem to be in somewhere in China.
The IP of the server is: 35.241.151.245 </p>

<p>I read a similar case in: <a href=""https://stackoverflow.com/questions/44829340/my-google-app-instances-does-not-seem-to-be-on-correct-region#"">https://stackoverflow.com/questions/44829340/my-google-app-instances-does-not-seem-to-be-on-correct-region#</a></p>

<p>But, i dont see a solution</p>

<p>Thank you</p>
","<ip><virtual-machines><google-cloud-platform><geolocation>","2019-09-13 09:42:35"
"840747","Possible to have registered or unregistered RAM on same host?","<p>When looking for refurbished HP ram for a DL360p with an Intel Xeon 2603v2 the following comes up.</p>

<p>There is already a registered Samsung and registered HP ram in the server, but it passes memtest86.</p>

<p>The support for the host have expired so I am using it just for testing software and are therefore temped to just go with the cheapest solution. In this case it is to add unregistered memory.</p>

<p>It is of course not best practice to mix ram brands and speed, but it works, so the question is:</p>

<p><strong>Question</strong></p>

<p>Would unregistered ram work in a host with all registered ram?</p>

<p>Registered DIMMs</p>

<ul>
<li>713985-B21    HP 16GB (1x16GB) Dual Rank x4 PC3L-12800R (DDR3-1600) Registered CAS-11 Low Voltage Memory Kit</li>
<li>713985-S21    HP 16GB (1x16GB) Dual Rank x4 PC3L-12800R (DDR3-1600) Registered CAS-11 LP Memory Kit/S-Buy</li>
<li>708641-B21    HP 16GB (1x16GB) Dual Rank x4 PC3-14900R (DDR3-1866) Registered CAS-13 Memory Kit</li>
<li>708641-S21    HP 16GB (1x16GB) Dual Rank x4 PC3-14900R (DDR3-1866) Registered CAS-13 Memory Kit/S-Buy</li>
</ul>

<p>Unbuffered with EEC Dimms</p>

<ul>
<li>647901-B21 HP 16GB (1x16GB) Dual Rank x4 PC3L-10600R (DDR3-1333) Registered CAS-9 Low Voltage Memory Kit</li>
<li>647901-S21 HP 16GB (1x16GB) Dual Rank x4 PC3L-10600R (DDR3-1333) Registered CAS-9 Low Voltage Memory Kit/S-Buy</li>
<li>672631-B21 HP 16GB (1x16GB) Dual Rank x4 PC3-12800R (DDR3-1600) Registered CAS-11 Memory Kit </li>
<li>672631-S21 HP 16GB (1x16GB) Dual Rank x4 PC3-12800R (DDR3-1600) Registered CAS-11 Memory Kit S-Buy</li>
</ul>
","<memory><hardware><hp>","2017-03-27 07:01:45"
"984232","Whitelist ICMP traffic with iptables","<p>I'm configuring iptables, for an Ubuntu Server VPS. It runs sshd, and various Dockerised web apps. It is not a router, and is not part of a complicated network.</p>

<p>After researching the topic, I decided to respect ICMP.</p>

<p>However, I'm using a whitelist, and only <code>ACCEPT</code> specific traffic:</p>

<pre><code>:INPUT DROP [0:0]
:FORWARD DROP [0:0]
:OUTPUT ACCEPT [0:0]
# ...then ACCEPT specific incoming traffic
</code></pre>

<p>What about ICMP? I could <code>REJECT</code> a few types and whitelist the rest:</p>

<pre><code>-A INPUT -p icmp -m icmp --icmp-type redirect -j REJECT
-A INPUT -p icmp -j ACCEPT
</code></pre>

<p>But that defeats the whitelist... So I want to do it the other way round.</p>

<p><a href=""https://en.wikipedia.org/wiki/Internet_Control_Message_Protocol#Control_messages"" rel=""nofollow noreferrer"">Most ICMPv4 types</a> have been deprecated. So creating a whitelist is easy, I just need guidance.</p>

<p>The major types, from <code>iptables -p icmp -h</code>:</p>

<pre><code> 0  =  echo-reply (pong)        # indirectly accepted by ESTABLISHED,RELATED rule
 3  =  destination-unreachable  # `ACCEPT`, especially code 4
 4  =  source-quench
 5  =  redirect
 8  =  echo-request (ping)      # `ACCEPT`
 9  =  router-advertisement
10  =  router-solicitation
11  =  time-exceeded            # indirectly accepted by ESTABLISHED,RELATED rule
12  =  parameter-problem        # `ACCEPT`
13  =  timestamp
14  =  timestamp reply
17  =  address-mask-request
18  =  address-mask-reply
...    many more
</code></pre>

<p>What I'll do:</p>

<ul>
<li>types 3, 8, 12: must <code>ACCEPT</code></li>
<li>types 0, 11: automatically <code>ACCEPT</code> by separate <code>ESTABLISHED,RELATED</code> rule</li>
<li>other types: default policy will <code>REJECT</code> (rather than <code>DROP</code>), with message <code>--reject-with icmp-proto-unreachable</code></li>
</ul>

<p><strong>Which other types should I <code>ACCEPT</code>?</strong> (And am I accepting types or codes that I should not?)</p>

<hr>

<p>UPDATE 1</p>

<p>No this is not a duplicate. It is about whitelisting important incoming ICMP traffic, and rejecting the rest.</p>

<p>Maybe as per @poige's comments, some items in my list are unnecessary as they are responses (like echo-reply). That is part of my question, please advise me what to put in the whitelist. If it is already covered by <code>ESTABLISHED,RELATED</code> then please advise me to remove it from the whitelist.</p>

<hr>

<p>UPDATE 2</p>

<p>To avoid more unnecessary confrontation as below with @poige, here is the question put simply:</p>

<p><em>""I'm using a whitelist approach - so by default everything is dropped. But I don't want icmp traffic to be dropped. So I'd like advice as to what to put into the whitelist.""</em></p>
","<linux><ubuntu><iptables><icmp><whitelist>","2019-09-14 13:43:22"
"768835","Email Being Blocked Being Sent in Own Domain","<p>I'm getting a server when I try to send an email - within my own domain.</p>

<p>The error:</p>

<p>Your message wasn't delivered due to a permission or security issue. It may have been rejected by a moderator, the address may only accept e-mail from certain senders, or another restriction may be preventing delivery.
The following organization rejected your message: 72.38.169.218.</p>

<blockquote>
  <p>Diagnostic information for administrators:<br>
  Generating server: server-4.bemta-12.messagelabs.com<br>
  info@tkhservices.ca<br>
  72.38.169.218 #&lt;72.38.169.218 #5.7.1 smtp; 550 5.7.1 Message rejected as spam by Content Filtering.><br>
   #SMTP#<br>
  Original message headers:<br>
  Return-Path: <br>
  Received: from [216.82.249.211] by server-4.bemta-12.messagelabs.com id 19/89-08444-AE166075; Thu, 07 Apr 2016 13:34:34 +0000<br>
  X-Env-Sender: info@tkhservices.ca<br>
  X-Msg-Ref: server-15.tower-53.messagelabs.com!1460036071!55681150!1<br>
  X-Originating-IP: [120.57.226.93]<br>
  X-StarScan-Received:<br>
  X-StarScan-Version: 8.28; banners=-,-,-<br>
  X-VirusChecked: Checked<br>
  Received: (qmail 22869 invoked from network); 7 Apr 2016 13:34:32 -0000<br>
  Received: from unknown (HELO ?120.57.226.93?) (120.57.226.93)
    by server-15.tower-53.messagelabs.com with SMTP; 7 Apr 2016 13:34:32 -0000<br>
  Message-ID: &lt;32834DBA81FC7476C709FEC5B8303283@tkhservices.ca><br>
  From: <br>
  To: <br>
  Subject: Hello<br>
  Date: Thu, 7 Apr 2016 22:53:29 +0400<br>
  MIME-Version: 1.0<br>
  Content-Type: text/plain<br>
  X-Mailer: Sltxykd uahxq 6.9  </p>
</blockquote>

<p>Any idea with what's causing the issue?</p>
","<exchange-2010><windows-sbs-2011>","2016-04-07 15:46:37"
"984261","Postifx - temporary lookup failure","<p>I have problem with my mail server. I have postfix and dovecot with current configuration:
/etc/postfix/main.cf</p>

<pre><code># See /usr/share/postfix/main.cf.dist for a commented, more complete version


# Debian specific:  Specifying a file name will cause the first
# line of that file to be used as the name.  The Debian default
# is /etc/mailname.
#myorigin = /etc/mailname

smtpd_banner = $myhostname ESMTP $mail_name (Debian/GNU)
biff = no

# appending .domain is the MUA's job.
append_dot_mydomain = no

# Uncomment the next line to generate ""delayed mail"" warnings
#delay_warning_time = 4h

readme_directory = no

# See http://www.postfix.org/COMPATIBILITY_README.html -- default to 2 on
# fresh installs.
compatibility_level = 2

# TLS parameters
smtpd_tls_cert_file=/etc/ssl/certs/ssl-cert-snakeoil.pem
smtpd_tls_key_file=/etc/ssl/private/ssl-cert-snakeoil.key
smtpd_use_tls=yes
smtpd_tls_session_cache_database = btree:${data_directory}/smtpd_scache
smtp_tls_session_cache_database = btree:${data_directory}/smtp_scache

smtpd_relay_restrictions = permit_mynetworks permit_sasl_authenticated defer_unauth_destination
myhostname = learn-linux.eu
alias_maps = hash:/etc/aliases
alias_database = hash:/etc/aliases
myorigin = learn-linux.eu
mydestination =  localhost, mail.learn-linux.eu, learn-linux.eu
relayhost = 
mynetworks = 127.0.0.0/8 [::ffff:127.0.0.0]/104 [::1]/128
mailbox_size_limit = 0
recipient_delimiter = +
inet_interfaces = all
inet_protocols = all

virtual_transport = lmtp:unix:private/dovecot-lmtp


virtual_mailbox_domains = mysql:/etc/postfix/mysql-virtual-domains-maps.cf
virtual_mailbox_maps = mysql:/etc/postfix/mysql-virtual-mailbox-maps.cf
virtual_alias_maps = mysql:/etc/postfix/mysql-virtual-alias-maps.cf,
        mysql:/etc/postfix/mysql-virtual-email2email.cf
</code></pre>

<p>master.cf</p>

<pre><code>#
# Postfix master process configuration file.  For details on the format
# of the file, see the master(5) manual page (command: ""man 5 master"" or
# on-line: http://www.postfix.org/master.5.html).
#
# Do not forget to execute ""postfix reload"" after editing this file.
#
# ==========================================================================
# service type  private unpriv  chroot  wakeup  maxproc command + args
#               (yes)   (yes)   (no)    (never) (100)
# ==========================================================================
smtp      inet  n       -       y       -       -       smtpd
#smtp      inet  n       -       y       -       1       postscreen
#smtpd     pass  -       -       y       -       -       smtpd
#dnsblog   unix  -       -       y       -       0       dnsblog
#tlsproxy  unix  -       -       y       -       0       tlsproxy
submission inet n       -       y       -       -       smtpd
  -o syslog_name=postfix/submission
  -o smtpd_tls_security_level=encrypt
  -o smtpd_sasl_auth_enable=yes
#  -o smtpd_reject_unlisted_recipient=no
  -o smtpd_client_restrictions=permit_sasl_authenticated,reject
#  -o smtpd_helo_restrictions=$mua_helo_restrictions
#  -o smtpd_sender_restrictions=$mua_sender_restrictions
#  -o smtpd_recipient_restrictions=
#  -o smtpd_relay_restrictions=permit_sasl_authenticated,reject
  -o milter_macro_daemon_name=ORIGINATING
smtps     inet  n       -       y       -       -       smtpd
  -o syslog_name=postfix/smtps
  -o smtpd_tls_wrappermode=yes
  -o smtpd_sasl_auth_enable=yes
#  -o smtpd_reject_unlisted_recipient=no
  -o smtpd_client_restrictions=permit_sasl_authenticated,reject
#  -o smtpd_helo_restrictions=$mua_helo_restrictions
#  -o smtpd_sender_restrictions=$mua_sender_restrictions
#  -o smtpd_recipient_restrictions=
#  -o smtpd_relay_restrictions=permit_sasl_authenticated,reject
  -o milter_macro_daemon_name=ORIGINATING
#628       inet  n       -       y       -       -       qmqpd
pickup    unix  n       -       y       60      1       pickup
cleanup   unix  n       -       y       -       0       cleanup
qmgr      unix  n       -       n       300     1       qmgr
#qmgr     unix  n       -       n       300     1       oqmgr
tlsmgr    unix  -       -       y       1000?   1       tlsmgr
rewrite   unix  -       -       y       -       -       trivial-rewrite
bounce    unix  -       -       y       -       0       bounce
defer     unix  -       -       y       -       0       bounce
trace     unix  -       -       y       -       0       bounce
verify    unix  -       -       y       -       1       verify
flush     unix  n       -       y       1000?   0       flush
proxymap  unix  -       -       n       -       -       proxymap
proxywrite unix -       -       n       -       1       proxymap
smtp      unix  -       -       y       -       -       smtp
relay     unix  -       -       y       -       -       smtp
#       -o smtp_helo_timeout=5 -o smtp_connect_timeout=5
showq     unix  n       -       y       -       -       showq
error     unix  -       -       y       -       -       error
retry     unix  -       -       y       -       -       error
discard   unix  -       -       y       -       -       discard
local     unix  -       n       n       -       -       local
virtual   unix  -       n       n       -       -       virtual
lmtp      unix  -       -       y       -       -       lmtp
anvil     unix  -       -       y       -       1       anvil
scache    unix  -       -       y       -       1       scache
#
# ====================================================================
# Interfaces to non-Postfix software. Be sure to examine the manual
# pages of the non-Postfix software to find out what options it wants.
#
# Many of the following services use the Postfix pipe(8) delivery
# agent.  See the pipe(8) man page for information about ${recipient}
# and other message envelope options.
# ====================================================================
#
# maildrop. See the Postfix MAILDROP_README file for details.
# Also specify in main.cf: maildrop_destination_recipient_limit=1
#
maildrop  unix  -       n       n       -       -       pipe
  flags=DRhu user=vmail argv=/usr/bin/maildrop -d ${recipient}
#
# ====================================================================
#
# Recent Cyrus versions can use the existing ""lmtp"" master.cf entry.
#
# Specify in cyrus.conf:
#   lmtp    cmd=""lmtpd -a"" listen=""localhost:lmtp"" proto=tcp4
#
# Specify in main.cf one or more of the following:
#  mailbox_transport = lmtp:inet:localhost
#  virtual_transport = lmtp:inet:localhost
#
# ====================================================================
#
# Cyrus 2.1.5 (Amos Gouaux)
# Also specify in main.cf: cyrus_destination_recipient_limit=1
#
#cyrus     unix  -       n       n       -       -       pipe
#  user=cyrus argv=/cyrus/bin/deliver -e -r ${sender} -m ${extension} ${user}
#
# ====================================================================
# Old example of delivery via Cyrus.
#
#old-cyrus unix  -       n       n       -       -       pipe
#  flags=R user=cyrus argv=/cyrus/bin/deliver -e -m ${extension} ${user}
#
# ====================================================================
#
# See the Postfix UUCP_README file for configuration details.
#
uucp      unix  -       n       n       -       -       pipe
  flags=Fqhu user=uucp argv=uux -r -n -z -a$sender - $nexthop!rmail ($recipient)
#
# Other external delivery methods.
#
ifmail    unix  -       n       n       -       -       pipe
  flags=F user=ftn argv=/usr/lib/ifmail/ifmail -r $nexthop ($recipient)
bsmtp     unix  -       n       n       -       -       pipe
  flags=Fq. user=bsmtp argv=/usr/lib/bsmtp/bsmtp -t$nexthop -f$sender $recipient
scalemail-backend unix  -   n   n   -   2   pipe
  flags=R user=scalemail argv=/usr/lib/scalemail/bin/scalemail-store ${nexthop} ${user} ${extension}
mailman   unix  -       n       n       -       -       pipe
  flags=FR user=list argv=/usr/lib/mailman/bin/postfix-to-mailman.py
  ${nexthop} ${user}
</code></pre>

<p>When i connect from telnet to port 25  i write mail from all it's ok, but when i write rcpt to, then receive temporary lookup failure. For mail server configuration i used this tutorial <a href=""https://www.linode.com/docs/email/postfix/email-with-postfix-dovecot-and-mysql/"" rel=""nofollow noreferrer"">https://www.linode.com/docs/email/postfix/email-with-postfix-dovecot-and-mysql/</a></p>

<p>I can connect to pop3 without any problem, all works fine, but smtp i have problem and sending  e-mail.</p>

<p>I don't know what is wrong with my configuration.
Log from /var/log/mail.log</p>

<pre><code>Sep 15 15:08:05 learn-linux postfix/cleanup[10103]: 2EEC43EAA9: message-id=&lt;20190915130805.2EEC43EAA9@learn-linux.eu&gt;
Sep 15 15:08:05 learn-linux postfix/cleanup[10103]: warning: mysql:/etc/postfix/mysql-virtual-email2email.cf lookup error for ""root@learn-linux.eu""
Sep 15 15:08:05 learn-linux postfix/cleanup[10103]: warning: 2EEC43EAA9: virtual_alias_maps map lookup problem for root@learn-linux.eu -- message not accepted, try again later
Sep 15 15:08:05 learn-linux postfix/pickup[10100]: 2F3743EAA9: uid=0 from=&lt;pawel@learn-linux.eu&gt;
Sep 15 15:08:05 learn-linux postfix/cleanup[10103]: 2F3743EAA9: message-id=&lt;20190915130805.2F3743EAA9@learn-linux.eu&gt;
Sep 15 15:08:05 learn-linux postfix/cleanup[10103]: warning: mysql:/etc/postfix/mysql-virtual-email2email.cf lookup error for ""pawel.cyrklaf@gmail.com""
Sep 15 15:08:05 learn-linux postfix/cleanup[10103]: warning: 2F3743EAA9: virtual_alias_maps map lookup problem for pawel.cyrklaf@gmail.com -- message not accepted, try again later
Sep 15 15:08:05 learn-linux postfix/pickup[10100]: warning: 2F5D93EAA9: message has been queued for 1 days
Sep 15 15:08:05 learn-linux postfix/pickup[10100]: 2F5D93EAA9: uid=0 from=&lt;root&gt;
Sep 15 15:08:05 learn-linux postfix/cleanup[10103]: 2F5D93EAA9: message-id=&lt;20190915130805.2F5D93EAA9@learn-linux.eu&gt;
Sep 15 15:08:05 learn-linux postfix/cleanup[10103]: warning: mysql:/etc/postfix/mysql-virtual-email2email.cf lookup error for ""root@learn-linux.eu""
Sep 15 15:08:05 learn-linux postfix/cleanup[10103]: warning: 2F5D93EAA9: virtual_alias_maps map lookup problem for root@learn-linux.eu -- message not accepted, try again later
Sep 15 15:08:05 learn-linux postfix/pickup[10100]: warning: 2F9843EAA9: message has been queued for 6 days
Sep 15 15:08:05 learn-linux postfix/pickup[10100]: 2F9843EAA9: uid=0 from=&lt;root&gt;
Sep 15 15:08:05 learn-linux postfix/cleanup[10103]: 2F9843EAA9: message-id=&lt;20190915130805.2F9843EAA9@learn-linux.eu&gt;
Sep 15 15:08:05 learn-linux postfix/cleanup[10103]: warning: mysql:/etc/postfix/mysql-virtual-email2email.cf lookup error for ""root@learn-linux.eu""
Sep 15 15:08:05 learn-linux postfix/cleanup[10103]: warning: 2F9843EAA9: virtual_alias_maps map lookup problem for root@learn-linux.eu -- message not accepted, try again later
Sep 15 15:08:05 learn-linux postfix/pickup[10100]: warning: 306D23EAA9: message has been queued for 1 days
Sep 15 15:08:05 learn-linux postfix/pickup[10100]: 306D23EAA9: uid=0 from=&lt;root&gt;
Sep 15 15:08:05 learn-linux postfix/cleanup[10103]: 306D23EAA9: message-id=&lt;20190915130805.306D23EAA9@learn-linux.eu&gt;
Sep 15 15:08:05 learn-linux postfix/cleanup[10103]: warning: mysql:/etc/postfix/mysql-virtual-email2email.cf lookup error for ""root@learn-linux.eu""
Sep 15 15:08:05 learn-linux postfix/cleanup[10103]: warning: 306D23EAA9: virtual_alias_maps map lookup problem for root@learn-linux.eu -- message not accepted, try again later
Sep 15 15:08:05 learn-linux postfix/pickup[10100]: warning: 30B583EAA9: message has been queued for 1 days
Sep 15 15:08:05 learn-linux postfix/pickup[10100]: 30B583EAA9: uid=0 from=&lt;root&gt;
Sep 15 15:08:05 learn-linux postfix/cleanup[10103]: 30B583EAA9: message-id=&lt;20190915130805.30B583EAA9@learn-linux.eu&gt;
Sep 15 15:08:05 learn-linux postfix/cleanup[10103]: warning: mysql:/etc/postfix/mysql-virtual-email2email.cf lookup error for ""root@learn-linux.eu""
Sep 15 15:08:05 learn-linux postfix/cleanup[10103]: warning: 30B583EAA9: virtual_alias_maps map lookup problem for root@learn-linux.eu -- message not accepted, try again later
Sep 15 15:08:11 learn-linux dovecot: master: Warning: Killed with signal 15 (by pid=10111 uid=0 code=kill)
Sep 15 15:08:12 learn-linux dovecot: log: Warning: Killed with signal 15 (by pid=1 uid=0 code=kill)
Sep 15 15:08:12 learn-linux dovecot: master: Dovecot v2.2.27 (c0f36b0) starting up for imap, pop3, lmtp (core dumps disabled)
Sep 15 15:09:05 learn-linux postfix/pickup[10100]: warning: 3F96D3EA21: message has been queued for 2 days
Sep 15 15:09:05 learn-linux postfix/pickup[10100]: 3F96D3EA21: uid=0 from=&lt;root&gt;
Sep 15 15:09:05 learn-linux postfix/cleanup[10103]: 3F96D3EA21: message-id=&lt;20190915130905.3F96D3EA21@learn-linux.eu&gt;
Sep 15 15:09:05 learn-linux postfix/cleanup[10103]: warning: mysql:/etc/postfix/mysql-virtual-email2email.cf lookup error for ""root@learn-linux.eu""
Sep 15 15:09:05 learn-linux postfix/cleanup[10103]: warning: 3F96D3EA21: virtual_alias_maps map lookup problem for root@learn-linux.eu -- message not accepted, try again later
Sep 15 15:09:05 learn-linux postfix/pickup[10100]: warning: 3FFD23EA21: message has been queued for 1 days
Sep 15 15:09:05 learn-linux postfix/pickup[10100]: 3FFD23EA21: uid=0 from=&lt;root&gt;
Sep 15 15:09:05 learn-linux postfix/cleanup[10105]: 3FFD23EA21: message-id=&lt;20190915130905.3FFD23EA21@learn-linux.eu&gt;
Sep 15 15:09:05 learn-linux postfix/cleanup[10105]: warning: connect to mysql server 127.0.0.1: Access denied for user 'root'@'localhost'
Sep 15 15:09:05 learn-linux postfix/cleanup[10105]: warning: mysql:/etc/postfix/mysql-virtual-email2email.cf lookup error for ""root@learn-linux.eu""
Sep 15 15:09:05 learn-linux postfix/cleanup[10105]: warning: 3FFD23EA21: virtual_alias_maps map lookup problem for root@learn-linux.eu -- message not accepted, try again later
Sep 15 15:09:05 learn-linux postfix/pickup[10100]: warning: 413CA3EA21: message has been queued for 6 days
Sep 15 15:09:05 learn-linux postfix/pickup[10100]: 413CA3EA21: uid=0 from=&lt;root&gt;
Sep 15 15:09:05 learn-linux postfix/cleanup[10103]: 413CA3EA21: message-id=&lt;20190915130905.413CA3EA21@learn-linux.eu&gt;
Sep 15 15:09:05 learn-linux postfix/cleanup[10103]: warning: mysql:/etc/postfix/mysql-virtual-email2email.cf lookup error for ""root@learn-linux.eu""
Sep 15 15:09:05 learn-linux postfix/cleanup[10103]: warning: 413CA3EA21: virtual_alias_maps map lookup problem for root@learn-linux.eu -- message not accepted, try again later
Sep 15 15:09:05 learn-linux postfix/pickup[10100]: warning: 4165F3EA21: message has been queued for 1 days
Sep 15 15:09:05 learn-linux postfix/pickup[10100]: 4165F3EA21: uid=0 from=&lt;root&gt;
Sep 15 15:09:05 learn-linux postfix/cleanup[10103]: 4165F3EA21: message-id=&lt;20190915130905.4165F3EA21@learn-linux.eu&gt;
Sep 15 15:09:05 learn-linux postfix/cleanup[10103]: warning: mysql:/etc/postfix/mysql-virtual-email2email.cf lookup error for ""root@learn-linux.eu""
Sep 15 15:09:05 learn-linux postfix/cleanup[10103]: warning: 4165F3EA21: virtual_alias_maps map lookup problem for root@learn-linux.eu -- message not accepted, try again later
Sep 15 15:09:05 learn-linux postfix/pickup[10100]: warning: 41A5E3EA21: message has been queued for 1 days
Sep 15 15:09:05 learn-linux postfix/pickup[10100]: 41A5E3EA21: uid=0 from=&lt;root&gt;
Sep 15 15:09:05 learn-linux postfix/cleanup[10103]: 41A5E3EA21: message-id=&lt;20190915130905.41A5E3EA21@learn-linux.eu&gt;
Sep 15 15:09:05 learn-linux postfix/cleanup[10103]: warning: mysql:/etc/postfix/mysql-virtual-email2email.cf lookup error for ""root@learn-linux.eu""
Sep 15 15:09:05 learn-linux postfix/cleanup[10103]: warning: 41A5E3EA21: virtual_alias_maps map lookup problem for root@learn-linux.eu -- message not accepted, try again later
Sep 15 15:09:05 learn-linux postfix/pickup[10100]: warning: 41D153EA21: message has been queued for 2 days
Sep 15 15:09:05 learn-linux postfix/pickup[10100]: 41D153EA21: uid=0 from=&lt;root&gt;
Sep 15 15:09:05 learn-linux postfix/cleanup[10105]: 41D153EA21: message-id=&lt;20190915130905.41D153EA21@learn-linux.eu&gt;
Sep 15 15:09:05 learn-linux postfix/cleanup[10105]: warning: mysql:/etc/postfix/mysql-virtual-email2email.cf lookup error for ""root@learn-linux.eu""
Sep 15 15:09:05 learn-linux postfix/cleanup[10105]: warning: 41D153EA21: virtual_alias_maps map lookup problem for root@learn-linux.eu -- message not accepted, try again later
Sep 15 15:09:05 learn-linux postfix/pickup[10100]: 41F863EA21: uid=0 from=&lt;pawel@learn-linux.eu&gt;
Sep 15 15:09:05 learn-linux postfix/cleanup[10103]: 41F863EA21: message-id=&lt;20190915130905.41F863EA21@learn-linux.eu&gt;
Sep 15 15:09:05 learn-linux postfix/cleanup[10103]: warning: mysql:/etc/postfix/mysql-virtual-email2email.cf lookup error for ""pawel.cyrklaf@gmail.com""
Sep 15 15:09:05 learn-linux postfix/cleanup[10103]: warning: 41F863EA21: virtual_alias_maps map lookup problem for pawel.cyrklaf@gmail.com -- message not accepted, try again later
Sep 15 15:09:05 learn-linux postfix/pickup[10100]: warning: 422C63EA21: message has been queued for 1 days
Sep 15 15:09:05 learn-linux postfix/pickup[10100]: 422C63EA21: uid=0 from=&lt;root&gt;
Sep 15 15:09:05 learn-linux postfix/cleanup[10105]: 422C63EA21: message-id=&lt;20190915130905.422C63EA21@learn-linux.eu&gt;
Sep 15 15:09:05 learn-linux postfix/cleanup[10105]: warning: mysql:/etc/postfix/mysql-virtual-email2email.cf lookup error for ""root@learn-linux.eu""
Sep 15 15:09:05 learn-linux postfix/cleanup[10105]: warning: 422C63EA21: virtual_alias_maps map lookup problem for root@learn-linux.eu -- message not accepted, try again later
Sep 15 15:09:05 learn-linux postfix/pickup[10100]: warning: 4253D3EA21: message has been queued for 6 days
Sep 15 15:09:05 learn-linux postfix/pickup[10100]: 4253D3EA21: uid=0 from=&lt;root&gt;
Sep 15 15:09:05 learn-linux postfix/cleanup[10103]: 4253D3EA21: message-id=&lt;20190915130905.4253D3EA21@learn-linux.eu&gt;
Sep 15 15:09:05 learn-linux postfix/cleanup[10103]: warning: mysql:/etc/postfix/mysql-virtual-email2email.cf lookup error for ""root@learn-linux.eu""
Sep 15 15:09:05 learn-linux postfix/cleanup[10103]: warning: 4253D3EA21: virtual_alias_maps map lookup problem for root@learn-linux.eu -- message not accepted, try again later
Sep 15 15:09:05 learn-linux postfix/pickup[10100]: warning: 42A4E3EA21: message has been queued for 1 days
Sep 15 15:09:05 learn-linux postfix/pickup[10100]: 42A4E3EA21: uid=0 from=&lt;root&gt;
Sep 15 15:09:05 learn-linux postfix/cleanup[10105]: 42A4E3EA21: message-id=&lt;20190915130905.42A4E3EA21@learn-linux.eu&gt;
Sep 15 15:09:05 learn-linux postfix/cleanup[10105]: warning: mysql:/etc/postfix/mysql-virtual-email2email.cf lookup error for ""root@learn-linux.eu""
Sep 15 15:09:05 learn-linux postfix/cleanup[10105]: warning: 42A4E3EA21: virtual_alias_maps map lookup problem for root@learn-linux.eu -- message not accepted, try again later
Sep 15 15:09:05 learn-linux postfix/pickup[10100]: warning: 42C713EA21: message has been queued for 1 days
Sep 15 15:09:05 learn-linux postfix/pickup[10100]: 42C713EA21: uid=0 from=&lt;root&gt;
Sep 15 15:09:05 learn-linux postfix/cleanup[10103]: 42C713EA21: message-id=&lt;20190915130905.42C713EA21@learn-linux.eu&gt;
Sep 15 15:09:05 learn-linux postfix/cleanup[10103]: warning: mysql:/etc/postfix/mysql-virtual-email2email.cf lookup error for ""root@learn-linux.eu""
Sep 15 15:09:05 learn-linux postfix/cleanup[10103]: warning: 42C713EA21: virtual_alias_maps map lookup problem for root@learn-linux.eu -- message not accepted, try again later
</code></pre>
","<postfix><dovecot>","2019-09-14 22:51:15"
"907931","Docker Networking","<p>Disclosure: I'm new to docker.</p>

<p>I am having trouble connecting my docker containers to the network. I have the container started and in its bash I use <code>ping6</code> to another address on the network and I get destination unreachable. I have no network access so I can't <code>yum install wireshark</code> to see what's going on.</p>

<p>I have two subnets (<code>fc8c:979b:1f4e:6ec7::/64</code> [my lan] and <code>fd45:73cb:008d:16b7::/64</code> [for docker-containers]). If these were physical servers I would assign the docker-host as ip <code>fc8c:979b:1f4e:6ec7:6ae9:a0dc:840f:1a81</code> and <code>fd45:73cb:8d:16b7:694f:c5d1:818:4651</code> to be the gateway. Then I would assign static, randomly generated, IPs from <code>fd45:73cb:8d:16b7::/64</code> to the docker-containers and setup the appropriate routes.</p>

<p>The question then .. How do I get docker-host to act as my gateway? How do I get docker-host to send the router advertisement (or equivalent) to the docker-containers? </p>

<p>I see a <code>docker0</code> ethernet adapter, but I don't want to configure it with <code>ip</code> if I need to use <code>docker</code> commands.</p>
","<docker><centos7><ipv6>","2018-04-16 20:08:41"
"907944","Changes made to users in ADAC vs ADUC not happening","<p>In my organization I have just discovered something: When making certain changes (to users) in the AD Administrative Center (ADAC) GUI, they do not happen, versus making those same changes in ADUC GUI. </p>

<p>For instance, Connect:-ing a Home folder in the GUI for ADUC will create the Home folder as per the server setup. But doing the exact same thing in ADAC (connecting a drive location for the home folder) results in no folder creation. Any idea why?</p>

<p>TL:DR: Why would connecting a Home folder in one Active Directory GUI (the ADAC) not create the folder, while doing the same action in ADUC does create the Home folder? They should just be two GUIs performing the same back-end manipulation.</p>
","<active-directory>","2018-04-16 21:09:08"
"907956","Testing DNS glue records for hidden nameservers that aren't live yet","<p>How do I test the glue records for a nameserver that isn't live yet?</p>

<p>I am more than happy testing glue records for domains that are live.</p>

<p>This time however we have just started the roll out of a large number of new nameservers.  While we have created the glue records via the registrar's I am struggling to find a way to test the glue records without actually assigning them to a domain.  Is this possible?</p>

<p>For clarification using real data (as of the time of writing)...  </p>

<p>One of our current nameservers is <code>ns12.dogsbodyhosting.net.</code> so running a <code>dig -t a @g.gtld-servers.net. ns12.dogsbodyhosting.net.</code> correctly tells us that <code>g.gtld-servers.net.</code> cannot give as an ANSWER but that the AUTHORITY we need to speak to is <code>ns12.dogsbodyhosting.net.</code> and the ADDITIONAL (Glue) information we need of A and AAAA records.  All good :-)</p>

<p>The problem: We have just created <code>ns12.dogsdns.net.</code> (as well as around 40 other new nameservers) and want to test that the glue for <code>ns12.dogsdns.net.</code> exists on the <code>g.gtld-servers.net.</code> and the other <code>.net</code> servers.  However running <code>dig -t a @g.gtld-servers.net. ns12.dogsdns.net.</code> only shows us the glue records for the old DNS curently mapped to this domain and not the glue for <code>ns12.dogsdns.net.</code> that we are after.</p>

<p>To restate; We have created the glue records via the registrar but have no way of knowing if the registrar has done anything or if we made a typo entering all the records. So, how to we test these glue records without putting a domain live and hoping?</p>

<p>Even if we set up a burner domain using the new nameservers we would couldn't add all 60 nameservers to one domain. </p>

<p>Thank you in advance for any responses.</p>
","<domain-name-system><nameserver><glue-record>","2018-04-16 22:35:51"
"907963","Robocopy in powershell","<p>I want to copy the csv files from two different servers and then paste those files at some location on 3rd server.</p>

<p>CSV File location on 1st server is - <code>E:\HC_Disk\Sample\""+""DiskSpace-""+(Get-Date -format yyyyMMdd)+"".csv</code></p>

<p>CSV file location on 2nd server - <code>E:\HC_Disk\Sample\""+""DiskSpace-""+(Get-Date -format yyyyMMdd)+"".csv</code></p>

<p>I have the script which generates the files everyday. My main concern is to club the two files together in the powershell.</p>

<p>Can you please help me with that. Thanks  </p>
","<powershell><robocopy>","2018-04-17 00:04:00"
"984358","Why this ip range is only showing * * *?","<p>I've got a question.</p>

<p>When I try to do this on GNU/Linux:</p>

<pre><code>traceroute -m 255 -I [the_ip]
</code></pre>

<p>Then I get this:</p>

<pre><code>192.0.0.1 (192.0.0.1)  6.788 ms  6.774 ms  6.781 ms
62.214.36.201 (62.214.36.201)  7.672 ms  7.869 ms  7.872 ms
62.214.37.202 (62.214.37.202)  12.351 ms  12.460 ms  12.467 ms
80.249.210.13 (80.249.210.13)  42.244 ms  42.202 ms  42.385 ms
87.110.223.130 (87.110.223.130)  45.054 ms  42.502 ms  42.456 ms
87.110.254.237 (87.110.254.237)  46.880 ms  46.434 ms  46.394 ms
* * *
HostBaltic.balt-ix.net (77.241.206.86)  42.506 ms  42.175 ms  42.117 ms
* * *
* * *
* * *
* * *
* * *
[...]
</code></pre>

<p><a href=""https://pastebin.com/tXq78aEV"" rel=""nofollow noreferrer"">https://pastebin.com/tXq78aEV</a></p>
","<ip><traceroute>","2019-09-16 07:40:02"
"768956","Fallback between datacenters","<p>What is the best way to do a fallback between datacenters. We have a dedicated server in one of the data centers and as our traffic increased i wanted to add more then backup i wanted redundancy and load balancing. </p>

<p>The thing that happened in the meantime is our hosting provider got hit with some DDOS attacks. We were not the target, somebody else was but the attackers managed to make anybody available on that hosting provider. After 2 od those DDOS attacks, the past behind particularly bad i leaved a bad taste in my mouth. I want the redundancy from two different data centers and two different hosting providers.</p>

<p>The only thing that i can think of is DNS fallback, but we have also non HTTP traffic. </p>

<p>I wanted to put something in front of everything that would route/proxy that traffic but then i have also create a redundancy for that</p>

<p>PS. im in europe and have to have the servers in my country because of latency.</p>
","<proxy><ddos><datacenter><redundancy><fallbackresource>","2016-04-08 06:58:01"
"768964","Unable to use SFTP via FileZilla?","<p>I created a Linux VM on Microsoft Azure via this blog <a href=""https://www.jeff.wilcox.name/2013/06/secure-linux-vms-with-ssh-certificates/"" rel=""nofollow noreferrer"">Creating secure Linux VMs in Azure with SSH key pairs</a></p>

<p>In summary it simply says:</p>

<p><code>openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout filename.key -out filename.pem</code></p>

<p>After that we have to upload our <code>.pem</code> file for our Linux VM</p>

<p><a href=""https://i.sstatic.net/3GUsC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3GUsC.png"" alt=""i uploaded filename.pem file""></a></p>

<p>After that, whenever I want to ssh into my vm, all I've to do is this:</p>

<p><code>ssh -i .ssh/filename.key user@subdomain.cloudapp.net</code></p>

<p>and I'm able to get inside my Linux VM, but I'm unable to use FileZilla to transfer file, I tried adding the <code>filename.key</code> file in FileZilla sftp settings but it is not accepting. No error just file open dialog closes without any error whatsoever.</p>

<p>I would really love if anyone can tell how can I connect via FileZilla. What I'm doing wrong, or some steps I've missed?</p>
","<linux><ubuntu><ssh><openssl><sftp>","2016-04-08 07:45:42"
"908008","Virtualhost does not work on Fedora with 403","<p>I have created a project under my home directory. Then, I created a virtual host in a new httpd conf file (I named it z-httpd.conf) I created under /etc/httpd/conf.d. The problem is that I cannot access the named virtual host that I created. When I browse to myservice.localhost, which I have also added to /etc/hosts, I get a 403 error. The z-httpd.conf file is as follows:  </p>

<pre><code>&lt;Directory /&gt;
    AllowOverride none
    Require all denied
&lt;/Directory&gt;


DocumentRoot ""/var/www/html""

#
# Relax access to content within /var/www.
#
&lt;Directory ""/home/johndoe/src""&gt;
    AllowOverride None
    # Allow open access:
    Require all granted
&lt;/Directory&gt;


# Further relax access to the default document root:
&lt;Directory ""/home/johndoe/src/webservice""&gt;
    #
    Options Indexes FollowSymLinks

    AllowOverride None

    #
    Require all granted
&lt;/Directory&gt;


#Add a virtual host
NameVirtualHost *:80
&lt;VirtualHost *:80&gt;
    ServerName myservice.localhost
    DocumentRoot /home/johndoe/src/webservice/public
    ServerPath /home/johndoe/src/webservice/public
    SetEnv APPLICATION_ENV ""development""
    &lt;Directory /home/johndoe/src/webservice/public&gt;
        DirectoryIndex index.php
        AllowOverride All
        Require all granted
    &lt;/Directory&gt;
&lt;/VirtualHost&gt;
</code></pre>

<p>I reassured that z-httpd.conf is included in the main Apache conf file and I also have an index.php file in my DocumentRoot. Also tried <code>setenforce 0</code>. Didn't work either.<br>
Output of <code>ls -lZa</code> on the DocumentRoot is as follows:  </p>

<pre><code>drwxr-xr-x. 6 johndoe johndoe unconfined_u:object_r:user_home_t:s0 4096 Apr 16 12:31 .
drwxr-xr-x. 7 johndoe johndoe unconfined_u:object_r:user_home_t:s0 4096 Apr 17 10:23 ..
drwxr-xr-x. 2 johndoe johndoe unconfined_u:object_r:user_home_t:s0 4096 Apr 16 12:31 css
drwxr-xr-x. 2 johndoe johndoe unconfined_u:object_r:user_home_t:s0 4096 Apr 16 12:31 fonts
-rwxr-xr-x. 1 johndoe johndoe unconfined_u:object_r:user_home_t:s0  748 Apr 16 12:31 .htaccess
drwxr-xr-x. 2 johndoe johndoe unconfined_u:object_r:user_home_t:s0 4096 Apr 16 12:31 img
-rwxr-xr-x. 1 johndoe johndoe unconfined_u:object_r:user_home_t:s0 1255 Apr 16 12:31 index.php
drwxr-xr-x. 2 johndoe johndoe unconfined_u:object_r:user_home_t:s0 4096 Apr 16 12:31 js
-rwxr-xr-x. 1 johndoe johndoe unconfined_u:object_r:user_home_t:s0 1032 Apr 16 12:31 web.config
</code></pre>

<p>In response to one of the comments, I'm adding httpd error_log output:   </p>

<pre><code>[Tue Apr 17 13:02:45.838159 2018] [core:error] [pid 9883:tid 140703779542784] (13)Permission denied: [client ::1:43674] AH00035: access to / denied (filesystem path '/home/johndoe/src') because search permissions are missing on a component of the path
[Tue Apr 17 13:06:11.389019 2018] [mpm_event:notice] [pid 9879:tid 140704428511232] AH00492: caught SIGWINCH, shutting down gracefully
[Tue Apr 17 13:06:12.498547 2018] [core:notice] [pid 10363:tid 140006580465664] SELinux policy enabled; httpd running as context system_u:system_r:httpd_t:s0
[Tue Apr 17 13:06:12.499134 2018] [suexec:notice] [pid 10363:tid 140006580465664] AH01232: suEXEC mechanism enabled (wrapper: /usr/sbin/suexec)
AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using localhost.localdomain. Set the 'ServerName' directive globally to suppress this message
[Tue Apr 17 13:06:12.515517 2018] [lbmethod_heartbeat:notice] [pid 10363:tid 140006580465664] AH02282: No slotmem from mod_heartmonitor
[Tue Apr 17 13:06:12.519962 2018] [mpm_event:notice] [pid 10363:tid 140006580465664] AH00489: Apache/2.4.29 (Fedora) OpenSSL/1.1.0g-fips mod_perl/2.0.10 Perl/v5.26.1 configured -- resuming normal operations
[Tue Apr 17 13:06:12.519991 2018] [core:notice] [pid 10363:tid 140006580465664] AH00094: Command line: '/usr/sbin/httpd -D FOREGROUND'
[Tue Apr 17 13:29:23.288313 2018] [core:error] [pid 10368:tid 140005758412544] (13)Permission denied: [client ::1:44974] AH00035: access to / denied (filesystem path '/home/johndoe/src') because search permissions are missing on a component of the path
[Tue Apr 17 13:29:29.223311 2018] [core:error] [pid 10368:tid 140005783590656] (13)Permission denied: [client 127.0.0.1:44392] AH00035: access to / denied (filesystem path '/home/johndoe/src') because search permissions are missing on a component of the path
</code></pre>

<p>And .htaccess content (located in <code>public</code>):  </p>

<pre><code>RewriteEngine On
# The following rule tells Apache that if the requested filename
# exists, simply serve it.
RewriteCond %{REQUEST_FILENAME} -s [OR]
RewriteCond %{REQUEST_FILENAME} -l [OR]
RewriteCond %{REQUEST_FILENAME} -d
RewriteRule ^.*$ - [L]
# The following rewrites all other queries to index.php. The 
# condition ensures that if you are using Apache aliases to do
# mass virtual hosting or installed the project in a subdirectory,
# the base path will be prepended to allow proper resolution of
# the index.php file; it will work in non-aliased environments
# as well, providing a safe, one-size fits all solution.
RewriteCond %{REQUEST_URI}::$1 ^(/.+)/(.*)::\2$
RewriteRule ^(.*) - [E=BASE:%1]
RewriteRule ^(.*)$ %{ENV:BASE}/index.php [L]
</code></pre>

<p>Additional info:<br>
Server version: Apache/2.4.29 (Fedora)
kernel-release: 4.15.4-300.fc27.x86_64</p>
","<virtualhost><httpd><fedora>","2018-04-17 08:47:41"
"840991","root is not able to change permission of folder/files","<p>I need to your help . I am trying to  changing  the  folder/files permissions but it is not working . what is the issue ? i am login on putty with root user.</p>

<p>Please help me .</p>
","<vps><godaddy>","2017-03-28 09:54:54"
"908128",".htaccess redirect to another video if HTTP_ORIGIN and user agent are not like this","<p>i want users to access my videos only if </p>

<blockquote>
  <p>RULE1:</p>
</blockquote>

<p><strong>HTTP_ORIGIN</strong>  is <strong><em>Example.Com</em></strong>   and  <strong>HTTP_REFERER</strong> is <strong><em>Exammple.Com/xxxxxxxx</em></strong></p>

<blockquote>
  <p>RULE 2</p>
</blockquote>

<p><strong>HTTP_USER_AGENT</strong> is <strong><em>useragent1</em></strong> OR <strong><em>useragent2</em></strong>  OR <strong><em>useragent3</em></strong> </p>

<ul>
<li>So if one rule is correct  then will videos will open normal </li>
</ul>

<p>if both rules are inccorect then  redirect to another video example redirect.mp4</p>

<pre><code>   RewriteRule ^(.*)$ http://example.com/redirect.mp4[R=302,L]
</code></pre>
","<.htaccess>","2018-04-17 21:36:47"
"769085","Adding additional memory in HP server","<p>I am planning the installation of additional memory in a HP DL360 server.</p>

<p>Currently, this server has two processors and 8 x 8GB stick of RAM. I have not got access to the server yet so I will assume this is balanced across the two processors. </p>

<p>We are adding another 16GB of RAM. I have been given a 16GB RAM module but Was expecting 2 x 8GB modules.</p>

<p>What would be the recommend way to add this. Should I take 2 of the 8GB sticks from processor 1 and move them to processor 2, and then install the 16GB stick in processor 1?</p>
","<memory><hp><hp-proliant>","2016-04-08 16:07:00"
"908230","Extend lvm logical volume","<p>I have to increase an lvm partition, then the fact is this I have an lvm centos with 3 file systems (you see them in the image) then the first one is /dev/mapper/centos-root which is what I should increase while /dev/mapper/centos-home which is what I should reduce to assign the spation to /dev/mapper/ centos-root! The problem is that when I resize / dev/mapper/centos-home with lvmresize and reboot the machine the filesystem gets damaged and it will not let me start the machine anymore!
How can I solve this? in the picture!</p>

<p><img src=""https://i.sstatic.net/n1mFn.png"" alt=""Image of filesystem""></p>
","<linux><centos><lvm><partition>","2018-04-18 11:03:46"
"769200","How to do static routing via next hop address?","<p>i know the command is <code>ip route [remote address] [next hop address]</code> but how do i implement it?<br><br>
Currently these are the only ones that are connected:</p>

<pre><code>CALOOCAN - MAKATI
MAKATI - NAVOTAS
SWITCH0-PC0-PC1
SWITCH1-PC2-PC3
SWITCH2-PC4-PC5
</code></pre>

<p>Here's the visualization:</p>

<p><img src=""https://i.sstatic.net/26uLp.png"" alt=""Here""></p>
","<cisco><ip-routing><static-routes>","2016-04-09 12:35:53"
"908242","Access Denied when accessing Service controller from application","<p>I have a web application written in c# mvc which is trying to check the status of windows service whether it is stopped or started. I have written the code but the issue is the code works fine on my local dev machine but when pushed to the server the code errors out stating access is denied.</p>

<p>I have the following code:</p>

<pre><code>            try
            {
                using (ServiceController sc = new ServiceController(""Service""))
                {
                    if (sc.Status == ServiceControllerStatus.Running)
                    {
                        //do something
                    }
                }
            }
               catch (Exception ex) { }; 
           } 
</code></pre>

<p>The above code keeps failing at Service controller itself.</p>

<p>I also tried to use the below code:</p>

<pre><code>    StringBuilder sb = new StringBuilder();
    Process process = new System.Diagnostics.Process();
    ProcessStartInfo startInfo = new System.Diagnostics.ProcessStartInfo();

    startInfo.FileName = @""sc"";
    startInfo.Arguments = ""query \""My Service\"""";
    startInfo.UseShellExecute = false;
    process.StartInfo = startInfo;
    process.OutputDataReceived += (sender, args) =&gt; 
    sb.AppendLine(args.Data);
    process.StartInfo.RedirectStandardOutput = true;
    process.Start();
    process.BeginOutputReadLine();
    process.WaitForExit();
</code></pre>

<p>Both of the above code keep failing at the Service controller / SC itself.</p>

<p>The error is</p>

<blockquote>
  <p>Cannot open Service Control Manager on computer '.'. This operation might require other privileges.</p>
</blockquote>

<p>Our application is running in IIS under service account and we have given admin rights and remote access rights to this account.</p>

<p>Not sure what else needs to be given here. Is there any specific rights to access windows service?</p>
","<windows><windows-server-2012><windows-service>","2018-04-18 12:01:16"
"769236","DNS Inaccessible","<p>I know this might seem a duplicate but wait!<br>
We have a Windows Server 2012 R2 domain controller, it obviously is a DNS server which hosts one AD-integrated domain. recently, it stopped responding and receiving DNS data a couple of days ago, for an unknown reason. It cannot resolve domain names and cannot reverse-lookup an IP, it means it cannot access root hints, forwarders, other websites and ...<br>
All internet resources are accessible by IP and I can ping root hints and forwarders using their IP addresses.<br>
I've already tried: setting up reverse lookup zones, disabling firewall, disabling IPv6, disabling EDNS0, shutting RRAS down, using an external DNS, rebooting, disallowing suffix appending...<br>
I have no clue about what's going on, until when the RRAS service was active, I could dial the server and resolve all internal FQDNs, but not an external one. This server has one IP address which is completely accessible remotely.<br>
I hope you can help.</p>

<p><strong>UPDATE: ARP TEST</strong>  </p>

<pre><code>Interface: 87.236.214.XXX --- 0xc
  Internet Address      Physical Address      Type
  87.236.214.254        64-64-9b-30-20-81     dynamic
  87.236.214.255        ff-ff-ff-ff-ff-ff     static
  224.0.0.22            01-00-5e-00-00-16     static
  224.0.0.252           01-00-5e-00-00-fc     static
  224.0.1.24            01-00-5e-00-01-18     static
</code></pre>

<p><strong>UPDATE: IPCONFIG</strong></p>

<pre><code>Windows IP Configuration

   Host Name . . . . . . . . . . . . : CFS
   Primary Dns Suffix  . . . . . . . : somedomain.co.uk
   Node Type . . . . . . . . . . . . : Hybrid
   IP Routing Enabled. . . . . . . . : No
   WINS Proxy Enabled. . . . . . . . : No
   DNS Suffix Search List. . . . . . : somedomain.co.uk

Ethernet adapter Local Area Connection:

   Media State . . . . . . . . . . . : Media disconnected
   Connection-specific DNS Suffix  . :
   Description . . . . . . . . . . . : TAP-Windows Adapter V9 #2
   Physical Address. . . . . . . . . : 00-FF-C2-28-3C-93
   DHCP Enabled. . . . . . . . . . . : Yes
   Autoconfiguration Enabled . . . . : Yes

Ethernet adapter Ethernet 2:

   Media State . . . . . . . . . . . : Media disconnected
   Connection-specific DNS Suffix  . :
   Description . . . . . . . . . . . : TAP-Windows Adapter V9
   Physical Address. . . . . . . . . : 00-FF-C7-1E-C3-2A
   DHCP Enabled. . . . . . . . . . . : Yes
   Autoconfiguration Enabled . . . . : Yes

Ethernet adapter Ethernet:

   Connection-specific DNS Suffix  . :
   Description . . . . . . . . . . . : Intel(R) 82574L Gigabit Network Connectio
n
   Physical Address. . . . . . . . . : 00-50-56-88-B4-CB
   DHCP Enabled. . . . . . . . . . . : No
   Autoconfiguration Enabled . . . . : Yes
   Link-local IPv6 Address . . . . . : fe80::65fa:4976:4508:552d%12(Preferred)
   IPv4 Address. . . . . . . . . . . : 87.236.214.XXX(Preferred)
   Subnet Mask . . . . . . . . . . . : 255.255.255.0
   Default Gateway . . . . . . . . . : 87.236.214.254
   DHCPv6 IAID . . . . . . . . . . . : 302010454
   DHCPv6 Client DUID. . . . . . . . : 00-01-00-01-1C-C8-30-F4-00-50-56-88-B4-CB

   DNS Servers . . . . . . . . . . . : 127.0.0.1
   NetBIOS over Tcpip. . . . . . . . : Disabled

Tunnel adapter isatap.{A05CB42A-B4DE-4675-B1AB-2FF643A39C8F}:

   Connection-specific DNS Suffix  . :
   Description . . . . . . . . . . . : Microsoft ISATAP Adapter
   Physical Address. . . . . . . . . : 00-00-00-00-00-00-00-E0
   DHCP Enabled. . . . . . . . . . . : No
   Autoconfiguration Enabled . . . . : Yes
   Link-local IPv6 Address . . . . . : fe80::200:5efe:87.236.214.163%30(Preferre
d)
   Default Gateway . . . . . . . . . :
   DHCPv6 IAID . . . . . . . . . . . : 503316480
   DHCPv6 Client DUID. . . . . . . . : 00-01-00-01-1C-C8-30-F4-00-50-56-88-B4-CB

   DNS Servers . . . . . . . . . . . : 127.0.0.1
   NetBIOS over Tcpip. . . . . . . . : Disabled

Tunnel adapter isatap.{C71EC32A-4699-4F87-898D-82BB58445CB5}:

   Media State . . . . . . . . . . . : Media disconnected
   Connection-specific DNS Suffix  . :
   Description . . . . . . . . . . . : Microsoft ISATAP Adapter #2
   Physical Address. . . . . . . . . : 00-00-00-00-00-00-00-E0
   DHCP Enabled. . . . . . . . . . . : No
   Autoconfiguration Enabled . . . . : Yes

Tunnel adapter 6TO4_Adapter:

   Connection-specific DNS Suffix  . :
   Description . . . . . . . . . . . : Microsoft 6to4 Adapter
   Physical Address. . . . . . . . . : 00-00-00-00-00-00-00-E0
   DHCP Enabled. . . . . . . . . . . : No
   Autoconfiguration Enabled . . . . : Yes
   IPv6 Address. . . . . . . . . . . : 2002:57ec:d6a3::57ec:d6a3(Preferred)
   Default Gateway . . . . . . . . . :
   DHCPv6 IAID . . . . . . . . . . . : 536870912
   DHCPv6 Client DUID. . . . . . . . : 00-01-00-01-1C-C8-30-F4-00-50-56-88-B4-CB

   DNS Servers . . . . . . . . . . . : 127.0.0.1
   NetBIOS over Tcpip. . . . . . . . : Disabled

Tunnel adapter isatap.{C2283C93-6E5F-433C-9775-AA9E9B54F989}:

   Media State . . . . . . . . . . . : Media disconnected
   Connection-specific DNS Suffix  . :
   Description . . . . . . . . . . . : Microsoft ISATAP Adapter #3
   Physical Address. . . . . . . . . : 00-00-00-00-00-00-00-E0
   DHCP Enabled. . . . . . . . . . . : No
   Autoconfiguration Enabled . . . . : Yes
</code></pre>
","<domain-name-system><windows-server-2012-r2><domain-controller>","2016-04-09 19:14:00"
"769319","Can't connect to socket from outside","<p>I have a problem on my server when I start an application that listen to a port: I can connect to it from the same machine, but not from the ouside world.
I noticed this problem when trying to create an irc bouncer with irssiproxy (I successfully did it on another server before). irssiproxy is supposed to listen to a port that I specify and another instance of irssi can connect to it. This works properly if I start another instance of irssi on my server, but it doesn't work from any other machine on the internet. (remark: I used successfully both 127.0.0.1 and the public ip when trying from the same machine).</p>

<p>First I was thinking that the problem came from my irssi setup, but I tried to listen to a port with netcat and to connect to it, and I had a similar issue:</p>

<pre><code>nc -l -v -p XXXX
</code></pre>

<p>then from the same machine:
<code>telnet 127.0.0.1 XXXX</code> (or <code>telnet [public-ip] XXXX</code>). In both case I successfully connected:</p>

<pre><code>Trying 127.0.0.1...
Connected to 127.0.0.1.
Escape character is '^]'.
</code></pre>

<p>but doing the same from my home PC or another server (using the public ip this time ofc), I always end up with a timeout:</p>

<pre><code>Trying XXX.XXX.XXX.XXX...
telnet: Unable to connect to remote host: Connection timed out
</code></pre>

<p>About my server: it's a dedicated server running on Debian (jessie that I upgraded from wheezy, but never really used before). I have a working apache2 on it, no problem to connect through port 80 even with telnet. The machine properly answers to ping as well.</p>

<p>I tried to run nc both from the root account and my user account.</p>

<p>I am not a Linux expert but is there anything that could prevent a port on which a program listen to to be accessed from outside?</p>

<p>Also, just in case there's a hint in it, when I start <code>nc</code> I always have this message: <code>nnetfd reuseport failed : Protocol not available</code>.</p>
","<linux><port><socket><netcat>","2016-04-10 15:12:53"
"769339","Can't query most DNS servers from ESXi guests","<p>I'm having some issues doing DNS lookups against most DNS servers from the guests on my ESXi 5.5 host. I'm saying <em>most</em> because I can do DNS lookups just fine against the DNS servers I've configured under ""DNS and Routing"" in the vSphere Client, but against the DNS servers that are not configured the DNS request times out.</p>

<p>Here's an example of a <code>nslookup</code> from the ESXi guest (the IP of the configured DNS server is replaced with x.x.x.x):</p>

<pre><code>user@guest:~$ nslookup example.com
Server:         x.x.x.x
Address:        x.x.x.x#53

Non-authoritative answer:
Name:   example.com
Address: 93.184.216.34
</code></pre>

<p>Doing the same <code>nslookup</code> from the same ESXi guest against e.g. Google Public DNS instead results in this:</p>

<pre><code>user@guest:~$ nslookup example.com 8.8.8.8
;; connection timed out; no servers could be reached
</code></pre>

<p>If I however SSH into the ESXi host instead and try to do a <code>nslookup</code> from there against Google Public DNS it works just fine, as seem below:</p>

<pre><code>~ # nslookup nslookup example.com 8.8.8.8
Server:    8.8.8.8
Address 1: 8.8.8.8 google-public-dns-a.google.com

Non-authoritative answer:
Name:   example.com
Address: 93.184.216.34
</code></pre>

<p>If I run <code>tcpdump</code> on the guest I can see the requests go out, but I only get a response back when I query against x.x.x.x. I've tried to run <code>telnet 8.8.8.8 53</code> and it connects successfully, so something is explicitly blocking DNS requests. This same issues occurs on all four guests on the machine, so I suspect that ESXi is blocking all requests that aren't against the configured DNS somehow, but I have no idea where or why.</p>

<p>Any ideas what might be preventing the guests from using other DNS servers? Configuring other DNS servers in <code>/etc/resolv.conf</code> doesn't change anything, and neither does changing servers under ""DNS and Routing"" in vSphere (although I didn't try restarting the host afterwards).</p>

<p>Edit: <code>iptables -S</code> on one of the guest machines:</p>

<pre><code>-P INPUT DROP
-P FORWARD DROP
-P OUTPUT ACCEPT
-N DOCKER
-N sshguard
-N ufw-after-forward
-N ufw-after-input
-N ufw-after-logging-forward
-N ufw-after-logging-input
-N ufw-after-logging-output
-N ufw-after-output
-N ufw-before-forward
-N ufw-before-input
-N ufw-before-logging-forward
-N ufw-before-logging-input
-N ufw-before-logging-output
-N ufw-before-output
-N ufw-logging-allow
-N ufw-logging-deny
-N ufw-not-local
-N ufw-reject-forward
-N ufw-reject-input
-N ufw-reject-output
-N ufw-skip-to-policy-forward
-N ufw-skip-to-policy-input
-N ufw-skip-to-policy-output
-N ufw-track-forward
-N ufw-track-input
-N ufw-track-output
-N ufw-user-forward
-N ufw-user-input
-N ufw-user-limit
-N ufw-user-limit-accept
-N ufw-user-logging-forward
-N ufw-user-logging-input
-N ufw-user-logging-output
-N ufw-user-output
-A INPUT -j sshguard
-A INPUT -j ufw-before-logging-input
-A INPUT -j ufw-before-input
-A INPUT -j ufw-after-input
-A INPUT -j ufw-after-logging-input
-A INPUT -j ufw-reject-input
-A INPUT -j ufw-track-input
-A FORWARD -o docker0 -j DOCKER
-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -i docker0 ! -o docker0 -j ACCEPT
-A FORWARD -i docker0 -o docker0 -j ACCEPT
-A FORWARD -j ufw-before-logging-forward
-A FORWARD -j ufw-before-forward
-A FORWARD -j ufw-after-forward
-A FORWARD -j ufw-after-logging-forward
-A FORWARD -j ufw-reject-forward
-A FORWARD -j ufw-track-forward
-A OUTPUT -j ufw-before-logging-output
-A OUTPUT -j ufw-before-output
-A OUTPUT -j ufw-after-output
-A OUTPUT -j ufw-after-logging-output
-A OUTPUT -j ufw-reject-output
-A OUTPUT -j ufw-track-output
-A ufw-after-input -p udp -m udp --dport 137 -j ufw-skip-to-policy-input
-A ufw-after-input -p udp -m udp --dport 138 -j ufw-skip-to-policy-input
-A ufw-after-input -p tcp -m tcp --dport 139 -j ufw-skip-to-policy-input
-A ufw-after-input -p tcp -m tcp --dport 445 -j ufw-skip-to-policy-input
-A ufw-after-input -p udp -m udp --dport 67 -j ufw-skip-to-policy-input
-A ufw-after-input -p udp -m udp --dport 68 -j ufw-skip-to-policy-input
-A ufw-after-input -m addrtype --dst-type BROADCAST -j ufw-skip-to-policy-input
-A ufw-after-logging-forward -m limit --limit 3/min --limit-burst 10 -j LOG --log-prefix ""[UFW BLOCK] ""
-A ufw-after-logging-input -m limit --limit 3/min --limit-burst 10 -j LOG --log-prefix ""[UFW BLOCK] ""
-A ufw-before-forward -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A ufw-before-forward -p icmp -m icmp --icmp-type 3 -j ACCEPT
-A ufw-before-forward -p icmp -m icmp --icmp-type 4 -j ACCEPT
-A ufw-before-forward -p icmp -m icmp --icmp-type 11 -j ACCEPT
-A ufw-before-forward -p icmp -m icmp --icmp-type 12 -j ACCEPT
-A ufw-before-forward -p icmp -m icmp --icmp-type 8 -j ACCEPT
-A ufw-before-forward -j ufw-user-forward
-A ufw-before-input -i lo -j ACCEPT
-A ufw-before-input -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A ufw-before-input -m conntrack --ctstate INVALID -j ufw-logging-deny
-A ufw-before-input -m conntrack --ctstate INVALID -j DROP
-A ufw-before-input -p icmp -m icmp --icmp-type 3 -j ACCEPT
-A ufw-before-input -p icmp -m icmp --icmp-type 4 -j ACCEPT
-A ufw-before-input -p icmp -m icmp --icmp-type 11 -j ACCEPT
-A ufw-before-input -p icmp -m icmp --icmp-type 12 -j ACCEPT
-A ufw-before-input -p icmp -m icmp --icmp-type 8 -j ACCEPT
-A ufw-before-input -p udp -m udp --sport 67 --dport 68 -j ACCEPT
-A ufw-before-input -j ufw-not-local
-A ufw-before-input -d 224.0.0.251/32 -p udp -m udp --dport 5353 -j ACCEPT
-A ufw-before-input -d 239.255.255.250/32 -p udp -m udp --dport 1900 -j ACCEPT
-A ufw-before-input -j ufw-user-input
-A ufw-before-output -o lo -j ACCEPT
-A ufw-before-output -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A ufw-before-output -j ufw-user-output
-A ufw-logging-allow -m limit --limit 3/min --limit-burst 10 -j LOG --log-prefix ""[UFW ALLOW] ""
-A ufw-logging-deny -m conntrack --ctstate INVALID -m limit --limit 3/min --limit-burst 10 -j RETURN
-A ufw-logging-deny -m limit --limit 3/min --limit-burst 10 -j LOG --log-prefix ""[UFW BLOCK] ""
-A ufw-not-local -m addrtype --dst-type LOCAL -j RETURN
-A ufw-not-local -m addrtype --dst-type MULTICAST -j RETURN
-A ufw-not-local -m addrtype --dst-type BROADCAST -j RETURN
-A ufw-not-local -m limit --limit 3/min --limit-burst 10 -j ufw-logging-deny
-A ufw-not-local -j DROP
-A ufw-skip-to-policy-forward -j DROP
-A ufw-skip-to-policy-input -j DROP
-A ufw-skip-to-policy-output -j ACCEPT
-A ufw-track-output -p tcp -m conntrack --ctstate NEW -j ACCEPT
-A ufw-track-output -p udp -m conntrack --ctstate NEW -j ACCEPT
-A ufw-user-input -p tcp -m tcp --dport 2222 -j ACCEPT
-A ufw-user-input -p tcp -m tcp --dport 80 -j ACCEPT
-A ufw-user-input -p tcp -m tcp --dport 443 -j ACCEPT
-A ufw-user-input -p tcp -m tcp --dport 143 -j ACCEPT
-A ufw-user-input -p tcp -m tcp --dport 993 -j ACCEPT
-A ufw-user-input -p tcp -m tcp --dport 110 -j ACCEPT
-A ufw-user-input -p tcp -m tcp --dport 995 -j ACCEPT
-A ufw-user-input -p tcp -m tcp --dport 587 -j ACCEPT
-A ufw-user-input -p tcp -m tcp --dport 25 -j ACCEPT
-A ufw-user-input -p udp -m udp --dport 123 -j ACCEPT
-A ufw-user-input -p tcp -m tcp --dport 53 -j ACCEPT
-A ufw-user-input -p udp -m udp --dport 53 -j ACCEPT
-A ufw-user-limit -m limit --limit 3/min -j LOG --log-prefix ""[UFW LIMIT BLOCK] ""
-A ufw-user-limit -j REJECT --reject-with icmp-port-unreachable
-A ufw-user-limit-accept -j ACCEPT
</code></pre>
","<domain-name-system><vmware-esxi><nslookup>","2016-04-10 18:37:50"
"769343","Firewall rules on a hosting (dedicated server)","<p>I am on Linux Debian Jessie 8.4.</p>

<p>I have set up a web server. It seems to run fine.</p>

<p>I'm just wondering, if I need the following firewall rules are ok for HTTP(S) server to be running fine.</p>

<pre><code>iptables -A INPUT -m conntrack --ctstate INVALID -j DROP
iptables -A INPUT -i lo -j ACCEPT
iptables -A INPUT -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
iptables -A INPUT -p tcp -m tcp --dport 22 -j ACCEPT
iptables -A INPUT -p tcp -m tcp --dport 21 -j ACCEPT
iptables -A INPUT -p tcp -m tcp --dport 10100 -j ACCEPT
iptables -A INPUT -p tcp -m tcp --dport 80 -j ACCEPT
iptables -A INPUT -p tcp -m tcp --dport 443 -j ACCEPT
</code></pre>

<p>The server is behind a router. Port 22 is not forwarded. 10100 is used for passive FTP</p>

<p>Thank you.</p>
","<iptables><web-server>","2016-04-10 19:10:13"
"769349","Port decisions for email setup in 2016 (Ubuntu)","<p>I have a new Ubuntu 14.04 LAMP server with the UFW firewall setup on DigitalOcean.  My goal is to add email for use on my phone(iPhone) and laptop(via Roundcube).</p>

<p>There is a lot of old information floating around about what you need to do to setup email and ports.  It seems my server will be needing Postfix AND Dovecot, correct?<br>
I am trying to open just enough ports on my firewall to get email from everyone.  I will list my port research.  I have starred (*) the ports I think I must open.  Is this enough to get email from everyone and successfully manage it on my devices?</p>

<pre><code>SMTP (for inbound and outbound mail)
 *25  = non secure
  465 = secure, old (smtp over ssl)
 *587 = secure, new (smtp-msa)

POP3 (bad way for managing mail)
  110 = non secure
  995 = secure (pop3 over ssl)

IMAP (good way for managing mail)
  143 = non secure
 *993 = secure (imap over ssl)
</code></pre>
","<postfix><smtp><port><dovecot><imap>","2016-04-10 20:59:30"
"841457","nginx truncates responses to external requests but not local","<p>I've configured nginx to do port forwarding to Ghost blog that's running on the same machine. When I open localhost, nginx returns everything correctly.</p>

<p>However when I hit the server from the outside, my scripts get truncated. Any ideas why that might be? I noticed plenty of questions about truncated responses but not a single one mentioning that happening only on remote requests.</p>

<p>Thank you.</p>
","<nginx><ghost-blog>","2017-03-30 02:33:37"
"985091","Prevent direct access to my server using reverse proxy","<p>I have a web server setup in <em>Microsoft Azure</em> and it has a domain name associated with it like <code>www.example.com</code>. This web server is very critical and i don't want anyone other than 1 or two people to know the exact URL or its IP. I believe direct access to this web server will be a huge security threat and want to prevent the my workers from accessing it directly. </p>

<p><em>Why a restriction in needed ?</em> </p>

<ul>
<li>Prevent direct access </li>
<li>I don't want my workers to access my web server outside my office. </li>
</ul>

<p>Even with these restrictions applied i want one or two people still be able to access this web server directly (in case of any emergency). </p>

<p><em>My thoughts</em> </p>

<p>Setup a reverse proxy at my office. Allow workers to access to my web server like if the web server is located on our local network. </p>

<p>So, when users type </p>

<pre><code>172.16.10.1 &lt;---&gt; Reverse Proxy &lt;---&gt; www.example.com
</code></pre>

<p>I'm sure whether what i plan to do is possible or not. If you have any other ideas please do let me know. </p>

<p><a href=""https://i.sstatic.net/1Zwve.png"" rel=""nofollow noreferrer"">Visual Representation</a></p>

<p>Currently i have not setup to prevent the direct access. I have not tried anything. </p>

<p><strong>EDIT</strong></p>

<p><em>I cannot restrict access to my web server using IP (There are other reasons of this)</em></p>
","<linux><web-server><azure><reverse-proxy><local-area-network>","2019-09-21 05:15:49"
"769663","How to create or change unmanaged computer account to managed computer account in Server 2012R2","<p>My DC Server use Windows Server 2012R2.</p>

<p>I want to use Windows Deployment Service but I don't see any computer account in active directory prestaged devices.</p>

<p>From this <a href=""https://www.windows-noob.com/forums/topic/506-how-can-i-prestage-a-computer-for-wds/"" rel=""nofollow noreferrer"">Ref</a>, I must change computer account to managed computer account.</p>

<p>But I don't see any this setting about that in computer account.</p>

<p>This is my new computer account image.</p>

<p><a href=""https://i.sstatic.net/nGqQE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/nGqQE.png"" alt=""enter image description here""></a></p>

<p>No ""Next"" Button to Managed Window </p>

<p>What should i do?</p>

<p>PS. I want to post more links and image but I'm newer.I can't post more 2 links. Sorry.</p>
","<active-directory><windows-server-2012-r2><wds>","2016-04-12 01:51:37"
"769668","Ubuntu Server IP address not listed in ifconfig","<p>I am not sure why the command ""ifconfig"" did not show my IP address, but I was still able to reach other hosts in the same network. It happens too many times to me and I did try to search but no luck to explain it. It actually does show up in ""ip addr"" command.
These are  my configuration for a static IP address, ""ifconfig"" and ""ip addr"" </p>

<p>This one is a ubuntu version 14.04 installed on physical server.
jessie/sid
(Linux openstack 3.19.0-56-generic #62~14.04.1-Ubuntu SMP Fri Mar 11 11:03:15 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux)</p>

<p><a href=""https://i.sstatic.net/7VNbX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7VNbX.png"" alt=""ifconfig, ip addr output""></a></p>

<p>Thank you so much!</p>

<pre><code>root@openstack:/home/stack# ip address list
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default 
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet 169.254.169.254/32 scope link lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: em1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq master br100 state UP group default qlen 1000
    link/ether 0c:c4:7a:76:1b:08 brd ff:ff:ff:ff:ff:ff
    inet6 fe80::ec4:7aff:fe76:1b08/64 scope link 
       valid_lft forever preferred_lft forever
3: em2: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000
    link/ether 0c:c4:7a:76:1b:09 brd ff:ff:ff:ff:ff:ff
4: virbr0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default 
    link/ether fa:26:3a:76:77:1c brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0
       valid_lft forever preferred_lft forever
5: br100: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default 
    link/ether 0c:c4:7a:76:1b:08 brd ff:ff:ff:ff:ff:ff
    inet 10.11.12.1/24 brd 10.11.12.255 scope global br100
       valid_lft forever preferred_lft forever
    inet 192.168.20.46/24 brd 192.168.20.255 scope global br100
       valid_lft forever preferred_lft forever
    inet 192.168.20.49/32 scope global br100
       valid_lft forever preferred_lft forever
    inet6 fe80::8c2f:b6ff:fe58:4b11/64 scope link 
       valid_lft forever preferred_lft forever
10: vnet0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master br100 state UNKNOWN group default qlen 500
    link/ether fe:16:3e:88:1e:51 brd ff:ff:ff:ff:ff:ff
    inet6 fe80::fc16:3eff:fe88:1e51/64 scope link 
       valid_lft forever preferred_lft forever
13: vnet1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master br100 state UNKNOWN group default qlen 500
    link/ether fe:16:3e:b4:69:7e brd ff:ff:ff:ff:ff:ff
    inet6 fe80::fc16:3eff:feb4:697e/64 scope link 
       valid_lft forever preferred_lft forever
17: vnet2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master br100 state UNKNOWN group default qlen 500
    link/ether fe:16:3e:b9:d7:4a brd ff:ff:ff:ff:ff:ff
    inet6 fe80::fc16:3eff:feb9:d74a/64 scope link 
       valid_lft forever preferred_lft forever




root@openstack:/home/stack# ifconfig
br100     Link encap:Ethernet  HWaddr 0c:c4:7a:76:1b:08  
          inet addr:10.11.12.1  Bcast:10.11.12.255  Mask:255.255.255.0
          inet6 addr: fe80::8c2f:b6ff:fe58:4b11/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:627132 errors:0 dropped:0 overruns:0 frame:0
          TX packets:522836 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:3074313781 (3.0 GB)  TX bytes:308312132 (308.3 MB)

em1       Link encap:Ethernet  HWaddr 0c:c4:7a:76:1b:08  
          inet6 addr: fe80::ec4:7aff:fe76:1b08/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:2884361 errors:0 dropped:24125 overruns:0 frame:0
          TX packets:816202 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:3881966559 (3.8 GB)  TX bytes:181453552 (181.4 MB)
          Memory:fb200000-fb27ffff 

lo        Link encap:Local Loopback  
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:45652477 errors:0 dropped:0 overruns:0 frame:0
          TX packets:45652477 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:30760291923 (30.7 GB)  TX bytes:30760291923 (30.7 GB)

virbr0    Link encap:Ethernet  HWaddr fa:26:3a:76:77:1c  
          inet addr:192.168.122.1  Bcast:192.168.122.255  Mask:255.255.255.0
          UP BROADCAST MULTICAST  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)

vnet0     Link encap:Ethernet  HWaddr fe:16:3e:88:1e:51  
          inet6 addr: fe80::fc16:3eff:fe88:1e51/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:487 errors:0 dropped:0 overruns:0 frame:0
          TX packets:58390 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:500 
          RX bytes:63154 (63.1 KB)  TX bytes:4422625 (4.4 MB)

vnet1     Link encap:Ethernet  HWaddr fe:16:3e:b4:69:7e  
          inet6 addr: fe80::fc16:3eff:feb4:697e/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:13333 errors:0 dropped:0 overruns:0 frame:0
          TX packets:63555 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:500 
          RX bytes:1042620 (1.0 MB)  TX bytes:29097792 (29.0 MB)

vnet2     Link encap:Ethernet  HWaddr fe:16:3e:35:95:a0  
          inet6 addr: fe80::fc16:3eff:fe35:95a0/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:5086 errors:0 dropped:0 overruns:0 frame:0
          TX packets:5567 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:500 
          RX bytes:450420 (450.4 KB)  TX bytes:8912581 (8.9 MB)


root@openstack:/home/stack# cat /etc/network/interfaces
# This file describes the network interfaces available on your system
# and how to activate them. For more information, see interfaces(5).

# The loopback network interface
auto lo
iface lo inet loopback

# The primary network interface
auto em1
iface em1 inet static
    address 192.168.20.46
    netmask 255.255.255.0
    network 192.168.20.0
    broadcast 192.168.20.255
    gateway 192.168.20.1
    # dns-* options are implemented by the resolvconf package, if installed
    dns-nameservers 8.8.8.8
</code></pre>
","<ubuntu><ip><ifconfig>","2016-04-12 02:27:24"
"769898","Running python script on anisble","<p>I have an ansible server that I find works fine, but when I try to run a python script on the deployed servers I get <code>no such file or directory</code>, even though I visually confirm they are there.</p>

<p>Here's my anisble playbook code:</p>

<pre><code>create a user directory and a dev subdirectory
- name: Create directory for python files
  file: path=/home/{{ user_name }}/python_files
    state=directory
    owner={{ user_name }}
    group={{ user_name }}
    mode=755

- name: Copy python file over
  copy: 
    src=example.py 
    dest=/home/{{ user_name }}/python_files/example.py
    owner={{ user_name }}
    group={{ user_name }}
    mode=777 

- name: Execute script
  command: /home/{{ user_name }}/python_files/example.py
</code></pre>

<hr>

<p>This is the error I get:</p>

<pre><code>TASK [python-dev : Execute script] *********************************************
task path: /usr/share/ansible-demo/roles/python-dev/tasks/main.yml:37
&lt;192.168.1.240&gt; ESTABLISH SSH CONNECTION FOR USER: None
&lt;192.168.1.240&gt; SSH: EXEC sshpass -d14 ssh -C -vvv -o ControlMaster=auto -o ControlPersist=60s -o ConnectTimeout=10 -o ControlPath=/home/achilles/.ansible/cp/ansible-ssh-%h-%p-%r -tt 192.168.1.240 '/bin/sh -c '""'""'( umask 22 &amp;&amp; mkdir -p ""` echo $HOME/.ansible/tmp/ansible-tmp-1460496110.48-100414848375832 `"" &amp;&amp; echo ""` echo $HOME/.ansible/tmp/ansible-tmp-1460496110.48-100414848375832 `"" )'""'""''
&lt;192.168.1.240&gt; PUT /tmp/tmpcLPC60 TO /home/achilles/.ansible/tmp/ansible-tmp-1460496110.48-100414848375832/command
&lt;192.168.1.240&gt; SSH: EXEC sshpass -d14 sftp -b - -C -vvv -o ControlMaster=auto -o ControlPersist=60s -o ConnectTimeout=10 -o ControlPath=/home/achilles/.ansible/cp/ansible-ssh-%h-%p-%r '[192.168.1.240]'
&lt;192.168.1.240&gt; ESTABLISH SSH CONNECTION FOR USER: None
&lt;192.168.1.240&gt; SSH: EXEC sshpass -d14 ssh -C -vvv -o ControlMaster=auto -o ControlPersist=60s -o ConnectTimeout=10 -o ControlPath=/home/achilles/.ansible/cp/ansible-ssh-%h-%p-%r -tt 192.168.1.240 '/bin/sh -c '""'""'sudo -H -S  -p ""[sudo via ansible, key=dbpmnfbipbkpualueeaxdbkosbjdwsdk] password: "" -u root /bin/sh -c '""'""'""'""'""'""'""'""'echo BECOME-SUCCESS-dbpmnfbipbkpualueeaxdbkosbjdwsdk; /bin/sh -c '""'""'""'""'""'""'""'""'""'""'""'""'""'""'""'""'""'""'""'""'""'""'""'""'""'""'LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 /usr/bin/python /home/achilles/.ansible/tmp/ansible-tmp-1460496110.48-100414848375832/command; rm -rf ""/home/achilles/.ansible/tmp/ansible-tmp-1460496110.48-100414848375832/"" &gt; /dev/null 2&gt;&amp;1'""'""'""'""'""'""'""'""'""'""'""'""'""'""'""'""'""'""'""'""'""'""'""'""'""'""''""'""'""'""'""'""'""'""''""'""''

fatal: [192.168.1.240]: FAILED! =&gt; {""changed"": false, ""cmd"": ""/home/achilles/python_files/example.py"", ""failed"": true, ""invocation"": {""module_args"": {""_raw_params"": ""/home/achilles/python_files/example.py"", ""_uses_shell"": false, ""chdir"": null, ""creates"": null, ""executable"": null, ""removes"": null, ""warn"": true}, ""module_name"": ""command""}, ""msg"": ""[Errno 2] No such file or directory"", ""rc"": 2}
</code></pre>

<hr>

<p>The directory permissions look ok on the deployed server:</p>

<pre><code>achilles@ansible-test:~/python_files$ ll
total 12
drwxr-xr-x  2 achilles achilles 4096 Apr 12 15:56 ./
drwxr-xr-x 18 achilles achilles 4096 Apr 12 15:56 ../
-rwxrwxrwx  1 achilles achilles  153 Apr 12 15:56 example.py*
</code></pre>

<p>Any suggestions? I don't think I overlooked the obvious but I am fairly new to ansible and this <a href=""https://serverfault.com/questions/536908/ansible-playbook-to-upload-and-execute-a-python-script"">link</a> didn't help.</p>
","<python><ansible>","2016-04-12 21:23:25"
"769998","linux + when change passwd we get permission denied","<p>We have installed Redhat Linux 6.5 from scratch the VIA kick start installation and booted into single user mode because I don't have the root password.</p>

<p>But when I perform the following:</p>

<pre><code>passwd root
changing password for user root
passwd: permission denied 
</code></pre>

<p>Please advice what are the reasons that I get permission denied and how to resolve the problem.</p>
","<linux><permissions><redhat><password><root>","2016-04-13 11:11:55"
"770016","Exim4 and ""550 smtp auth required"" error","<p>I have a server with Exim4
It manage the domain example.com</p>

<p>A desktop user which running Thunderbird, is using the SMTP of his ISP (it don't using the server's smtp for some pertinent reasons).</p>

<p>The SMTP outgoing server is authentified by SSL, on 587 port.</p>

<p>When the user, owner of the mailbox user1@example.com, send a email to anybody, it's working without problem.</p>

<p>But if user1@example.com try to send a message to user2@example.com, it received an error message like this :</p>

<pre><code>    This is the mail system at host slow1-d.mail.gandi.net.
I'm sorry to have to inform you that your message could not
be delivered to one or more recipients.

                  The mail system

&lt;user2@example.com&gt;: host domain.com[213.xxx.xxx.xxx] said: 550
smtp auth requried
</code></pre>

<p>Yes, the writing error is real on ""requried"" ;)</p>

<p>And on exim4 log I have this :</p>

<pre><code>2016-04-13 12:06:42 H=slow1-d.mail.gandi.net [217.xxx.xxx.xxx] F=&lt;user1@domain.com&gt; rejected RCPT &lt;user2@example.com&gt;: smtp auth requried
</code></pre>

<p>What's going wrong ?</p>
","<exim>","2016-04-13 12:10:42"
"908653","ISP gave me 3 ppoe accounts for 3 dedicated IPs, what kind of router do I need?","<p>I made an optic fiber ISP contract that included 1 dedicated IP addresses and 2 additional dedicated IP addresses, now when they gave me the 3 IPs I see I have actually 3 different ppoe accounts.</p>

<p>Is there a reliably gigabit wired router that can have multiple ppoe accounts ? 
Or is there a certain protocol name that I should look for ?</p>

<p>ISP provided this fiber optic router/device ""Fiberhome AN5506-02-FG"" and they said to bridge from that for the additional PPOE accounts. But I want a cleaner solution and I wonder if this single device could work: ""MikroTik RB2011UiAS-IN"", not sure if the fiber connection on the MikroTik does the same thing as my current device, but it should support 3 ppoe accounts.</p>
","<networking><router><isp><fiber><pppoe>","2018-04-20 07:44:46"
"985527","Malware infection possible on Cisco switches?","<p>Are there any malware that target Cisco switches? I recently got one from a friend whose company threw them out after a ransomware attack, and are wondering if there is anything to be concerned about.</p>

<p>Edit: model number is SG200-50.</p>
","<switch><malware>","2019-09-25 00:35:52"
"908786","Finding a vulnerability in PHP website","<p>I'm hosting a Wordpress site that got hacked. Not a big deal, usual malicious <code>favicon_ea9b28.ico</code> file containing PHP code.</p>

<p>The problem is, in this case, that after I remove the bad scripts and clean other modified files, and after I update all the plugins and themes to their latest version, the site gets hacked again. 
It has already happened several times since a few weeks. There must be a vulnerable piece of code that hasn't been addressed yet.</p>

<p>How do I find it? I mean, is there any kind of logging in PHP that logs when something creates a new file on disk? Such as <em>script xyz.php at line 123 called <code>fopen('favicon_ea9b28.ico', 'wb')</code></em>?</p>

<p>EDIT: the server itself is not compromised for sure, because just removing the write permission from the directories, for the user running php-fpm, and changing the owner to <code>root</code>, ""fixes"" the problem, but then the media library becomes unusable, so that's no real fix. However it does show that the server is not compromised.</p>

<p>Given it's not a server compromise, <a href=""https://serverfault.com/questions/218005"">this is not a duplicate</a>, because this question assumes the server is NOT compromised. Regardless of the truth of that assumption in my particular case, I need a way to deal with a PHP web app compromise, not a server compromise.</p>
","<security><php7>","2018-04-20 22:30:11"
"908820","502 Bad gateway nginx ubuntu","<p>i have problem to configure my server with laravel application i get the message 502 bad gateway nginx i tried to configure nginx conf but with no success.</p>

<p>server info:<br>
Distributor ID: Ubuntu.<br>
Description:    Ubuntu 16.04.4 LTS.<br>
Release:        16.04.<br>
Codename:       xenial</p>

<p>nginx site-available default:</p>

<pre><code>server {
    listen 80 default_server;
    listen [::]:80 default_server;

    # SSL configuration
    #
    # listen 443 ssl default_server;
    # listen [::]:443 ssl default_server;
    #
    # Note: You should disable gzip for SSL traffic.
    # See: https://bugs.debian.org/773332
    #
    # Read up on ssl_ciphers to ensure a secure configuration.
    # See: https://bugs.debian.org/765782
    #
    # Self signed certs generated by the ssl-cert package
    # Don't use them in a production server!
    #
    # include snippets/snakeoil.conf;

    root /var/www/horizonemail/public;

    # Add index.php to the list if you are using PHP
   index index.php index.html index.htm index.nginx-debian.html;

    server_name 159.65.91.237;

    location / {
            # First attempt to serve request as file, then
            # as directory, then fall back to displaying a 404.
            try_files $uri $uri/ /index.php?$query_string;
    }

    # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000
    #
    location ~ \.php$ {
            include snippets/fastcgi-php.conf;
    #
    #       # With php7.0-cgi alone:
    #       fastcgi_pass 127.0.0.1:9000;
    #       # With php7.0-fpm:
            fastcgi_pass unix:/run/php/php7.2-fpm.sock;
}
    # deny access to .htaccess files, if Apache's document root
    # concurs with nginx's one
    #
    location ~ /\.ht {
            deny all;
    }
</code></pre>
","<nginx>","2018-04-21 11:22:38"
"908842","I am unable to access my website from the web (curl and wget return html of page)","<p>I am having several issues trying to get my website to the world. I am very new to the notion of running a web server, so I followed many of the tutorials to help me get started. There is a likelihood I did something wrong (where likelihood >= 95%), and I would very much appreciate assistance correcting these errors.</p>

<p>First, attempts to access it via http are stymied by ERR_CONNECTION_TIMED_OUT. For the record, I cannot reach it via ping.</p>

<p>I ran '<code>sudo netstat -plunt</code>' and this was the output:</p>

<pre><code>Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name
tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1429/sshd 
tcp 0 0 0.0.0.0:25 0.0.0.0:* LISTEN 10863/master
tcp 0 0 127.0.0.1:3306 0.0.0.0:* LISTEN 15618/mysqld
tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 23351/apache2
tcp6 0 0 :::22 :::* LISTEN 1429/sshd 
tcp6 0 0 :::25 :::* LISTEN 10863/master
tcp6 0 0 :::443 :::* LISTEN 23351/apache2
</code></pre>

<p>Running '<code>curl http://unaffiliatedstudios.com</code>' returns the HTML for the webpage; '<code>curl https://unaffiliatedstudios.com</code>' returns the following:</p>

<pre><code>curl: (51) SSL: certificate subject name (ubuntu-unaffiliated-com) does not match target host name 'unaffiliatedstudios.com'
</code></pre>

<p>I ran '<code>sudo iptables -L | grep ACCEPT</code>', and received this:</p>

<pre><code>Chain INPUT (policy ACCEPT)
ACCEPT tcp -- anywhere anywhere state NEW tcp dpt:https
ACCEPT tcp -- anywhere anywhere tcp dpt:https
Chain OUTPUT (policy ACCEPT)
ACCEPT all -- anywhere anywhere ctstate RELATED,ESTABLISHED
ACCEPT icmp -- anywhere anywhere icmp destination-unreachable
ACCEPT icmp -- anywhere anywhere icmp source-quench
ACCEPT icmp -- anywhere anywhere icmp time-exceeded
ACCEPT icmp -- anywhere anywhere icmp parameter-problem
ACCEPT icmp -- anywhere anywhere icmp echo-request
ACCEPT all -- anywhere anywhere
ACCEPT all -- anywhere anywhere ctstate RELATED,ESTABLISHED
ACCEPT icmp -- anywhere anywhere icmp destination-unreachable
ACCEPT icmp -- anywhere anywhere icmp source-quench
ACCEPT icmp -- anywhere anywhere icmp time-exceeded
ACCEPT icmp -- anywhere anywhere icmp parameter-problem
ACCEPT icmp -- anywhere anywhere icmp echo-request
ACCEPT udp -- anywhere anywhere udp spt:bootps dpt:bootpc
ACCEPT udp -- anywhere 224.0.0.251 udp dpt:mdns
ACCEPT udp -- anywhere 239.255.255.250 udp dpt:1900
ACCEPT all -- anywhere anywhere
ACCEPT all -- anywhere anywhere ctstate RELATED,ESTABLISHED
ACCEPT all -- anywhere anywhere
ACCEPT tcp -- anywhere anywhere ctstate NEW
ACCEPT udp -- anywhere anywhere ctstate NEW
ACCEPT tcp -- anywhere anywhere tcp dpt:ssh /* 'dapp_OpenSSH' */
ACCEPT tcp -- anywhere anywhere tcp dpt:http
ACCEPT tcp -- anywhere anywhere multiport dports http,https /* 'dapp_Apache%20Full' */
ACCEPT all -- anywhere anywhere
</code></pre>

<p>I've attached <a href=""https://i.sstatic.net/4cObz.png"" rel=""nofollow noreferrer"">an image</a> containing the droplet's DNS settings.</p>

<p>Last, and I'm not sure how much this will help, but I ran nmap and these were the results:</p>

<pre><code>Starting Nmap 7.70 ( https://nmap.org ) at 2018-04-21 08:54 Central Daylight Time

NSE: Loaded 148 scripts for scanning.

NSE: Script Pre-scanning.

Initiating NSE at 08:54

Completed NSE at 08:54, 0.02s elapsed

Initiating NSE at 08:54

Completed NSE at 08:54, 0.00s elapsed

Initiating Ping Scan at 08:54

Scanning unaffiliatedstudios.com (138.197.139.28) [4 ports]

Completed Ping Scan at 08:54, 3.51s elapsed (1 total hosts)

Nmap scan report for unaffiliatedstudios.com (138.197.139.28) [host down]

NSE: Script Post-scanning.

Initiating NSE at 08:54

Completed NSE at 08:54, 0.00s elapsed

Initiating NSE at 08:54

Completed NSE at 08:54, 0.00s elapsed

Read data files from: C:\Program Files (x86)\Nmap

Note: Host seems down. If it is really up, but blocking our ping probes, try -Pn

Nmap done: 1 IP address (0 hosts up) scanned in 31.23 seconds

Raw packets sent: 8 (304B) | Rcvd: 0 (0B)
</code></pre>

<p>Any help is appreciated.</p>

<p>EDIT:  Adding the settings for Cloud Firewall:</p>

<pre><code>Inbound
Type    Protocol    Port Range  Sources 
SSH TCP 22  All IPv4 All IPv6   
Outbound
Type    Protocol    Port Range  Destinations    
ICMP    ICMP        All IPv4 All IPv6   
All TCP TCP All ports   All IPv4 All IPv6   
HTTPS   TCP 443 All IPv4 All IPv6   
All UDP UDP All ports   All IPv4 All IPv6
</code></pre>
","<domain-name-system><ssl-certificate><ubuntu-16.04><digital-ocean><google-domains>","2018-04-21 17:30:19"
"770276","use postfix two mail server with two MX records","<p>Our current setup is a webserver at <code>www.domain.tld</code> (DNS A record) and a mail server (postfix) at <code>mail.domain.tld</code> (DNS MX record) on one dedicated server (IP address <code>1.2.3.4</code>).</p>

<p>Our website sends out newsletters every day to our subscribers so the current machine has to handle both, the webserver and mail server load.</p>

<p>We would like to ""outsource"" the newsletter delivery to a second dedicated mail server with IP address <code>4.3.2.1</code> at <code>mail2.domain.tld</code>.</p>

<p>So in addition to the current MX record <code>mail</code> to <code>1.2.3.4</code>, we are planning to add a second MX record <code>mail2</code> to <code>4.3.2.1</code> with lower priority.</p>

<p>Now we would like to continue to use our current mail server <code>mail</code> as before for our regular mail accounts (postfix, courier, DKIM, DMARC, SPF, virtual users is already set up). Further, our newsletter application hosted on the webserver at <code>1.2.3.4</code> should be using the new mail server at <code>mail2</code> to send out newsletters, and relaying mails to <code>@domain.tld</code> sent to <code>mail2</code> to <code>mail</code>. (It could be that some mails get sent to <code>mail2</code>.)</p>

<p>To my understanding, <code>mail2.example.com</code> should ONLY be allowed to send out mails when they come from <code>1.2.3.4</code>, AND it should relay incoming mails to <code>mail</code> ONLY when they go to <code>@domain.tld</code>.</p>

<p><code>mail</code> should continue to work for our ""regular"" mail accounts.</p>

<p>Thus, the ""hard work"" of sending out several thousand mails per day would be offloaded to the machine at <code>4.3.2.1</code> separating web and ""newsletter"" traffic.</p>

<p>How can I achieve this? We use Debian Jessie and postfix on both machines. I understand I can relay the incoming mail as similarly described here: <a href=""https://serverfault.com/questions/639289/postfix-relaying-all-incoming-mail-to-another-host/639302"">Postfix : Relaying all incoming mail to another host</a></p>

<p>How do I get the ""send newsletter"" part?</p>

<p>If you need more specific information please let me know!</p>
","<postfix><email-server>","2016-04-14 10:31:17"
"908862","How to create local proxy server accessible to machines on LAN","<p>My requirement is ...
Route ""all"" the traffic from locally connected machine A to pass through machine B, as if the traffic originally got generated from machine B.</p>

<p>Reason..</p>

<p>Machine B has access to some remote resources via a private VPN connection e.g. database servers, hosted repositories etc.</p>

<p>The problem..</p>

<p>Only one person can use the remote resources at a time makes it frustrating if 2 or 3 guys need to access the remote resources.</p>

<p>Bottom line...
i need a way so all my users could somehow route their network traffic through that machine B, so all can work at the same time with the remote resources that are only available when the private VPN is connected on machine B.</p>

<p>I have tried ssh tunneling with no significant results as every application needs to be configured to use socks to route the traffic from that application to the proxy server. i need something global!</p>

<p>Any Help?</p>

<p>Update 1:</p>

<p>I have managed to make it work by 
1- creating a sshd server on machine B and did ssh from machine A in to B.
2- set the socks5 proxy on machine A.</p>

<p>but machine A is only routing successfully if machine A is on ubuntu, whereas when using machine A with windows 10, doesn't route the traffic to the machine B.</p>

<p>Any help?</p>

<p>Update 2:</p>

<p>Even on ubuntu it seems that only traffic for port 80 is being forwarded... Nothing else is working..</p>

<p>Need help!</p>
","<ssh><vpn><proxy><ssh-tunnel>","2018-04-21 22:47:04"
"986098","iptables matching pattern followed by 4 random values followed by another pattern","<p>I'm trying to create a iptables rule that matches the following pattern in this UDP packet:</p>

<pre><code>0x0000:  0000 030a 0000 0000 0000 0000 0000 0800  ................
0x0010:  4500 0027 5d30 0000 6c11 232a 5164 585d  E..']0..l.#*QdX]
0x0020:  c0a8 6402 fe25 1e61 0013 b382 5341 4d50  ..d..%.a....SAMP
0x0030:  c063 ba71 e2ea 63                        .c.q..c
</code></pre>

<p>The pattern is <code>|53414d50c063ba71|</code> followed by a random hex, in this case <code>|e2ea|</code>, followed by <code>|63|</code>.</p>

<p>The rule has to assure that there are 4 digits after the <code>|53414d50c063ba71|</code>, and that after those 4 random digits, there is a <code>|63|</code>.</p>

<p>Right now I have this, but I don't know how to modify it accordingly:</p>

<pre><code>iptables -I INPUT -p udp --dport 7777 -m string --algo kmp \
    --hex-string '|53414d50c063ba71????63|' -j DROP
</code></pre>

<p>???? -> How??</p>

<p>Please help me.</p>
","<linux><security><iptables><udp><packets>","2019-09-29 16:26:14"
"909173","apache2.2 + php-fpm (php7.1)","<p>Here is my virtualhost of my apache2.2, and i want to config it to connect php7.1 php-fpm, there is a apache error</p>

<pre><code>SuexecUserGroup ""#504"" ""#504""
ServerName test.example.com
ServerAlias www.test.example.com
ServerAlias webmail.test.example.com
ServerAlias admin.test.example.com
DocumentRoot /home/test/public_html
ErrorLog /var/log/virtualmin/test.example.com_error_log
CustomLog /var/log/virtualmin/test.example.com_access_log combined
ScriptAlias /cgi-bin/ /home/test/cgi-bin/
ScriptAlias /awstats/ /home/test/cgi-bin/
DirectoryIndex index.html index.htm index.php index.php4 index.php5
&lt;Directory /home/test/public_html&gt;
Options -Indexes +IncludesNOEXEC +SymLinksIfOwnerMatch 
allow from all
AllowOverride All Options=ExecCGI,Includes,IncludesNOEXEC,Indexes,MultiViews,SymLinksIfOwnerMatch
&lt;/Directory&gt;
&lt;Directory /home/test/cgi-bin&gt;
allow from all
AllowOverride All Options=ExecCGI,Includes,IncludesNOEXEC,Indexes,MultiViews,SymLinksIfOwnerMatch
&lt;/Directory&gt;
RewriteEngine on
RewriteCond %{HTTP_HOST} =webmail.test.example.com
RewriteRule ^(.*) https://test.example.com:20000/ [R]
RewriteCond %{HTTP_HOST} =admin.test.example.com
RewriteRule ^(.*) https://test.example.com:10000/ [R]
ProxyPassMatch ^/(.*.php(/.*)?)$ fcgi://127.0.0.1:9000/home/test/public_html/$1
#RemoveHandler .php
RemoveHandler .php5
php_admin_value engine Off
Alias /dav /home/test/public_html
&lt;Location /dav&gt;
DAV on
AuthType Basic
AuthName ""test.example.com""
AuthUserFile /home/test/etc/dav.digest.passwd
Require valid-user
ForceType text/plain
Satisfy All
RemoveHandler .php
RemoveHandler .php7.1
RewriteEngine off
&lt;/Location&gt;
&lt;Files awstats.pl&gt;
AuthName ""test.example.com statistics""
AuthType Basic
AuthUserFile /home/test/.awstats-htpasswd
require valid-user
&lt;/Files&gt;
</code></pre>

<p>the error is </p>

<pre><code>[Tue Apr 24 18:45:41 2018] [warn] proxy: No protocol handler was valid for the URL /phpinfo.php. If you are using a DSO version of mod_proxy, make sure the proxy submodules are included in the configuration using LoadModule.
</code></pre>

<p>anyone know what is the problem?</p>

<p>--------------update----------------</p>

<pre><code>LoadModule auth_basic_module modules/mod_auth_basic.so
LoadModule auth_digest_module modules/mod_auth_digest.so
LoadModule authn_file_module modules/mod_authn_file.so
LoadModule authn_alias_module modules/mod_authn_alias.so
LoadModule authn_anon_module modules/mod_authn_anon.so
LoadModule authn_dbm_module modules/mod_authn_dbm.so
LoadModule authn_default_module modules/mod_authn_default.so
LoadModule authz_host_module modules/mod_authz_host.so
LoadModule authz_user_module modules/mod_authz_user.so
LoadModule authz_owner_module modules/mod_authz_owner.so
LoadModule authz_groupfile_module modules/mod_authz_groupfile.so
LoadModule authz_dbm_module modules/mod_authz_dbm.so
LoadModule authz_default_module modules/mod_authz_default.so
LoadModule ldap_module modules/mod_ldap.so
LoadModule authnz_ldap_module modules/mod_authnz_ldap.so
LoadModule include_module modules/mod_include.so
LoadModule log_config_module modules/mod_log_config.so
LoadModule logio_module modules/mod_logio.so
LoadModule env_module modules/mod_env.so
LoadModule ext_filter_module modules/mod_ext_filter.so
LoadModule mime_magic_module modules/mod_mime_magic.so
LoadModule expires_module modules/mod_expires.so
LoadModule deflate_module modules/mod_deflate.so
LoadModule headers_module modules/mod_headers.so
LoadModule usertrack_module modules/mod_usertrack.so
LoadModule setenvif_module modules/mod_setenvif.so
LoadModule mime_module modules/mod_mime.so
LoadModule dav_module modules/mod_dav.so
LoadModule status_module modules/mod_status.so
LoadModule autoindex_module modules/mod_autoindex.so
LoadModule info_module modules/mod_info.so
LoadModule dav_fs_module modules/mod_dav_fs.so
LoadModule vhost_alias_module modules/mod_vhost_alias.so
LoadModule negotiation_module modules/mod_negotiation.so
LoadModule dir_module modules/mod_dir.so
LoadModule actions_module modules/mod_actions.so
LoadModule speling_module modules/mod_speling.so
LoadModule userdir_module modules/mod_userdir.so
LoadModule alias_module modules/mod_alias.so
LoadModule substitute_module modules/mod_substitute.so
LoadModule rewrite_module modules/mod_rewrite.so
LoadModule proxy_module modules/mod_proxy.so
LoadModule proxy_balancer_module modules/mod_proxy_balancer.so
LoadModule proxy_ftp_module modules/mod_proxy_ftp.so
LoadModule proxy_http_module modules/mod_proxy_http.so
LoadModule proxy_ajp_module modules/mod_proxy_ajp.so
LoadModule proxy_connect_module modules/mod_proxy_connect.so
LoadModule cache_module modules/mod_cache.so
LoadModule suexec_module modules/mod_suexec.so
LoadModule disk_cache_module modules/mod_disk_cache.so
LoadModule cgi_module modules/mod_cgi.so
LoadModule version_module modules/mod_version.so
</code></pre>
","<apache-2.2><php><php-fpm>","2018-04-24 11:03:11"
"986382","Configuring an Site to Site VPN ,open VPN client on Pfsense and Azure will host Open VPN Scess Server","<p>Dears, </p>

<p>I am relatively  new  to Open vpn , by reading through forums i have tried may of the suggestions, but still not able to get it work. I am trying to have a Site To Site VPN , with Open VPn Access Server on Azure. Site A and Site B will have open VPN client configured on  pfsense. <strong>Windows machines on the Site A and Site B are able to ping the Open VPN Access Server Internal Ip (10.0.0.4), But  Windows machines on Site A LAN is not able to reach the Site B LAN desktop and vice versa.</strong> 
Please find the below configuration i have added.</p>

<p><strong>Vm on Azure (open VPN Access Server)</strong>
OS: ubuntu 
<strong>Added Routes on virtual network:</strong></p>

<p><strong>Address Prefix:</strong> 192.168.50.0/24 , NEXT Hop: 10.0.0.4
Open VPN Access Server Installed with Below Settings
Internal IP :10.0.0.4
VPN Client Subnet: 192.168.59.0/24
VPN Mode: Layer 3 (routing/NAT)
Should VPN Clients Have Acess to private Subnets:
Yes, using routing rounting (Advanced)
Private subnets to whcihc clients should be given access:
10.0.0.4/24
Server Config Directives
push ""route 10.0.0.0 255.255.255.0""
push ""route 192.168.10.0 255.255.255.0""
push ""route 192.168.40.0 255.255.255.0""
push ""route 192.168.50.0 255.255.255.0""
route 192.168.10.0 255.255.255.0
route 192.168.40.0 255.255.255.0
route 192.168.50.0 255.255.255.0
client-to-client
User Permissions:</p>

<p><strong>Site A user:</strong>
Authentication :Local
Configure VPN Gateway:Yes
Allow Client to act as a VPN gateway for this client side  subnets:
192.168.10.0/24
Site B user:
Authentication :Local
Configure VPN Gateway:Yes
Allow Client to act as a VPN gateway for this client side  subnets:
192.168.50.0/24</p>

<p><strong>Open VPN Client on PFSense</strong>
Router: PFSense
Open VPN Client Config</p>

<p>Site A Internal Ip :192.168.10.0/24
Tunnel Network:192.168.59.0/24
Remote Network:192.168.50.0/24,10.0.0.0/24
Custom Options:
route 192.168.10.0 255.255.255.0</p>

<p>Site B Internal Ip :192.168.50.0/24
Tunnel Network:192.168.59.0/24
Remote Network:192.168.10.0/24,10.0.0.0/24
Custom Options:
route 192.168.50.0 255.255.255.0</p>

<p><strong>Lan Interface Rules on both firewall.</strong>
Default Allow LAN T0 any rules , Gateway is Open VPN Client Interface </p>

<p>Added Outbound rules on both Site A and Site B (SiteB Lan ip will be 192.168.50.0/24)
Interface : OPEN-VPN Interface Address , Source :127.0.0.1/8 ,port:<em>, Destination :</em>,Port: 500(ISAKMP) NAT Address :OPEN-VPN Interface Address</p>

<p>Interface : OPEN-VPN Interface Address , Source :127.0.0.1/8 ,port:<em>, Destination :</em>,Port: * ,NAT Address :OPEN-VPN Interface Address</p>

<p>Interface : OPEN-VPN Interface Address , Source :192.168.10.0/24,port:<em>, Destination :</em>,Port: 500(ISAKMP) NAT Address :OPEN-VPN Interface Address</p>

<p>Interface : OPEN-VPN Interface Address ,  Source :192.168.10.0/24,port:<em>, Destination :</em>,Port: * ,NAT Address :OPEN-VPN Interface Address</p>
","<openvpn><site-to-site-vpn><azure-networking>","2019-10-01 13:34:34"
"909367","Centos running python script every new file added on ftp","<p>We are running on centos 6.9 Final and vsftpd version 2.2.2
Basically we had some folder which is our FTP folder that connected to ANPR/LPR camera. Every camera recognize the plate than the camera will capture image and put it on our ftp which is our folder.</p>

<p>We want to running some python script that every new image added on the directory. How to approach that?</p>

<p>Thanks</p>
","<centos><centos6><vsftpd>","2018-04-25 10:31:14"
"770818","How to correctly create a filesystem on disks that are connected to a RAID disk?","<p>I want to create a ext4 filesystem on sda, sdb, and sdc and <code>mount</code> them to use them but I don't know how to handle all the raid stuff.</p>

<p><code>lsblk</code> output:</p>

<pre><code>NAME    MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINT
sda       8:0    0 111.8G  0 disk
|-sda1    8:1    0     8G  0 part
| `-md0   9:0    0     8G  0 raid1 [SWAP]
|-sda2    8:2    0   512M  0 part
| `-md1   9:1    0 511.4M  0 raid1 /boot
`-sda3    8:3    0 103.3G  0 part
  `-md2   9:2    0 206.5G  0 raid5 /
sdb       8:16   0   2.7T  0 disk
|-sdb1    8:17   0     8G  0 part
| `-md0   9:0    0     8G  0 raid1 [SWAP]
|-sdb2    8:18   0   512M  0 part
| `-md1   9:1    0 511.4M  0 raid1 /boot
`-sdb3    8:19   0 103.3G  0 part
  `-md2   9:2    0 206.5G  0 raid5 /
sdc       8:32   0   2.7T  0 disk
|-sdc1    8:33   0     8G  0 part
| `-md0   9:0    0     8G  0 raid1 [SWAP]
|-sdc2    8:34   0   512M  0 part
| `-md1   9:1    0 511.4M  0 raid1 /boot
`-sdc3    8:35   0 103.3G  0 part
  `-md2   9:2    0 206.5G  0 raid5 /
</code></pre>

<p><code>fdisk -l /dev/sda</code> output:</p>

<pre><code>Disk /dev/sda: 111.8 GiB, 120034123776 bytes, 234441648 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x17dd98e6

Device     Boot    Start       End   Sectors   Size Id Type
/dev/sda1           2048  16779264  16777217     8G fd Linux raid autodetect
/dev/sda2       16781312  17829888   1048577   512M fd Linux raid autodetect
/dev/sda3       17831936 234439600 216607665 103.3G fd Linux raid autodetect
</code></pre>

<p><code>fdisk -l /dev/sdb</code> (<code>sdc</code> is nearly the same) output :</p>

<pre><code>Disk /dev/sdb: 2.7 TiB, 3000592982016 bytes, 5860533168 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x788018fb

Device     Boot    Start       End   Sectors   Size Id Type
/dev/sdb1           2048  16779264  16777217     8G fd Linux raid autodetect
/dev/sdb2       16781312  17829888   1048577   512M fd Linux raid autodetect
/dev/sdb3       17831936 234439600 216607665 103.3G fd Linux raid autodetect
</code></pre>

<p>So how do I create filesystems on sda, sdb, sdc and mount them?</p>
","<linux><raid><filesystems><raid5>","2016-04-16 11:04:46"
"986628","nginx proxy_pass works on http but fails on https","<p>When I pass <code>http:localhost:9000</code> to proxy_pass then it is working. But When I pass <code>https:localhost:9000</code> then it fails</p>

<pre><code>pid /var/run/nginx.pid;
worker_processes  2;

events {
    worker_connections   65536;
    use epoll;
    multi_accept on;
}

http {
    limit_req_zone $binary_remote_addr zone=basic_limit:10m rate=20r/s;
    limit_conn_zone $binary_remote_addr zone=limit_conn:1m;
    keepalive_timeout 65;
    keepalive_requests 100000;
    sendfile         on;
    tcp_nopush       on;
    tcp_nodelay      on;

    client_body_buffer_size    128k;
    client_max_body_size       10m;
    client_header_buffer_size    1k;
    large_client_header_buffers  4 4k;
    output_buffers   1 32k;
    postpone_output  1460;

    client_header_timeout  3m;
    client_body_timeout    3m;
    send_timeout           3m;

    open_file_cache max=1000 inactive=20s;
    open_file_cache_valid 30s;
    open_file_cache_min_uses 5;
    open_file_cache_errors off;

    gzip on;
    gzip_min_length  1000;
    gzip_buffers     4 4k;
    gzip_types       application/x-javascript text/css application/javascript text/javascript text/plain text/xml application/json application/vnd.ms-fontobject application/x-font-opentype application/x-font-truetype application/x-font-ttf application/xml font/eot font/opentype font/otf image/svg+xml image/vnd.microsoft.icon;
    gzip_disable ""MSIE [1-6]\."";

    # [ debug | info | notice | warn | error | crit | alert | emerg ] 
    error_log  /var/log/nginx.error_log  debug;

    log_format main      '$remote_addr - $remote_user [$time_local]  '
      '""$request"" $status $bytes_sent '
      '""$http_referer"" ""$http_user_agent"" '
        '""$gzip_ratio""';

    log_format download  '$remote_addr - $remote_user [$time_local]  '
      '""$request"" $status $bytes_sent '
      '""$http_referer"" ""$http_user_agent"" '
        '""$http_range"" ""$sent_http_content_range""';

    map $status $loggable {
    ~^[23]  0;
    default 1;
    } 

    server {
    listen   8080;
    server_name   _;
        #ssl_certificate /etc/nginx/ssl/nginx.crt;
    #ssl_certificate_key /etc/nginx/ssl/nginx.key;
    access_log   /var/log/nginx/access.log;
    error_log    /var/log/nginx/error.log;

      location / {
        limit_req zone=basic_limit burst=30 nodelay;
        limit_conn limit_conn 20;
        limit_req_status 429;
        proxy_pass         http://127.0.0.1:9000;
        proxy_redirect     off;
        proxy_set_header   Host             $host;
        proxy_set_header Host $http_host;
        proxy_set_header X-Forwarded-Host $http_host;
        proxy_set_header   X-Real-IP        $remote_addr;
        proxy_set_header  X-Forwarded-For  $proxy_add_x_forwarded_for;
        proxy_connect_timeout      90;
        proxy_send_timeout         90;
        proxy_read_timeout         90;
        proxy_buffer_size          4k;
        proxy_buffers              4 32k;
        proxy_busy_buffers_size    64k;
        proxy_temp_file_write_size 64k;
        proxy_temp_path            /etc/nginx/proxy_temp;
    }
       location /nginx_status {
        stub_status on;
        allow 127.0.0.1;
        deny all;
    }
    }
}
</code></pre>
","<nginx><proxypass>","2019-10-03 10:32:03"
"909548","504 DNS Lookup failed with domain-user, working fine with local user","<p>I just set up a DNS Server on my Windows Server 2012 R2 DC and joined my PC into this domain (test.local).</p>

<p>But now I a DNS error when trying to access an Web Application from another domain (eg. <a href=""https://mail.test2.local/owa"" rel=""nofollow noreferrer"">https://mail.test2.local/owa</a>). What am I doing wrong? Everything is working fine when I switch back to the local Administrator</p>

<p>Traceroute and nslookup for mail.test2.local are giving me the same results for both scenarios</p>
","<domain-name-system><active-directory><windows-server-2012-r2><dns-server>","2018-04-26 10:22:48"
"909560","Cannot get new computer to connect to domain","<p>Windows server essential connector, cannot get new computer to connect to domain, states server unavailable.</p>

<p>I have made sure server and client have correct synced time, updates are current on both machines</p>

<p>Note: Task tray has a yellow triangle on network connection indicating no connection, I can connect to all network shares and I can also connect to the internet. I can also RDP into server from another location.</p>
","<windows>","2018-04-26 12:15:24"
"909639","SQL Cluster Between Cloud Providers - Multi Cloud SQL","<p>I want to explore the options available to me in setting up MS SQL replication between Azure and Amazon AWS.</p>

<p>Has anyone got any ideas?  I think Always On Availability Groups is off the table.</p>

<p><strong>-- Edit</strong></p>

<p>Ok so a couple of suggestions asked me to put some constraints/requirements into play - here they are:</p>

<ol>
<li><p>The product has to be Microsoft SQL server.  I do not care whether
it is installed on a VM or part of the managed offerings of each
provider (RDS/Elastic etc etc) it just has to act like a SQL server.</p></li>
<li><p>Master / Slave setup.  Do not worry about routing or anything like that, simply the server in one of the clouds will be the master, the server in the other the slave.</p></li>
<li><p>Data sync - There will be something in place to sync the data between master and slave.  Imagine log shipping from cloud to cloud.</p></li>
</ol>
","<amazon-ec2><amazon-web-services><sql-server><azure>","2018-04-26 21:44:59"
"909693","Powershell remote access denied on workgroup","<p>I've configured a domenstic network with PCs running windows 10 and I'm trying to set up remote access via powershell between these hosts.
I've enabled remote access on all the hosts, authorizing all the hosts to control AND be controlled by all the hosts in the network, I've tested the connection between two PCs and it seems to be working cause the following returns no error:</p>

<pre><code>PS C:\WINDOWS\system32&gt; test-WSman 192.168.1.132
 wsmid           : http://schemas.dmtf.org/wbem/wsman/identity/1/wsmanidentity.xsd 
 ProtocolVersion : http://schemas.dmtf.org/wbem/wsman/1/wsman.xsd 
 ProductVendor   : Microsoft Corporation                        
 ProductVersion  : OS: 0.0.0 SP: 0.0 Stack: 3.0
</code></pre>

<p>but, when I try to establish a full session between the two hosts with the following command, I get error:</p>

<pre><code>PS C:\WINDOWS\system32&gt;Enter-PSSession -ComputerName 192.168.1.132 -Credential User1            

Enter-PSSession : Connecting to remote server 192.168.1.132 failed        
with the following error message : Access is denied.                               
For more information, see the about_Remote_Troubleshooting Help topic.                                                       
At line:1 char:1                                                             
+ Enter-PSSession -ComputerName 192.168.1.132 -Credential User1                        
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ CategoryInfo : InvalidArgument:(192.168.1.132:String) [Enter-PSSession], PSRemotingTransportException
+ FullyQualifiedErrorId : CreateRemoteRunspaceFailed 
</code></pre>

<p>After issuing this command, of course, pops-up the dialog box, asking for password. 
Maybe, the error could be in the credential I'm issuing...where do I get valid and acceptable credentials(user &amp; password) in Windows, for connecting to a remote machine in my home network?   </p>

<p>Do I need to create new ones, with get-credential cmdlet, or maybe credentials are tied to the local account on the machine I'm connecting from, or perhaps on the machine I'm connecting to?<br>
I'm totally inexpert in this field so I don't know how to solve it       </p>

<p>P.S. I'm in a simply windows workgroup(with no home-group setted), not in a domain 
so the machines are in peer-to-peer configuration                                                                                                                  </p>
","<windows><networking><powershell><command-line-interface><remote-access>","2018-04-27 11:01:37"
"909791","Reverse Proxy - ProxyPass to HTTPS site","<p>Im trying to config a reverse proxy server (Centos 7). The virtual host configuration as below:</p>

<pre><code> &lt;VirtualHost *:443&gt;
  ServerName www.example2.com
  ServerAlias example2.com

  ProxyPass / https://keycdn.com/
  ProxyPassReverse / https://keycdn.com/

  SSLEngine on
  SSLProxyEngine on

  SSLCertificateFile /etc/ssl/certs/apache-selfsigned.crt
  SSLCertificateKeyFile /etc/ssl/private/apache-selfsigned.key

  ErrorLog /var/log/httpd/example.com-error.log
  CustomLog /var/log/httpd/example.com-access.log combined
 &lt;/VirtualHost&gt;
</code></pre>

<p>It is always do redirect to the actual URL rather than working as reverse-proxy, but working well for http site :</p>

<pre><code>ProxyPass / http://example.com/
ProxyPassReverse / http://example.com/
</code></pre>

<p>Anybody can enlighten me on what should I do to able doing ProxyPass/ProxyPassReverse for HTTPS. Thanks. </p>

<p><strong>[EDIT]</strong></p>

<p>It always reply 301-Moved Permanently for https proxypass.</p>
","<reverse-proxy><apache2>","2018-04-28 09:02:11"
"987114","Installing EC2 .pem publickey to Debian ssh","<p>I recently generated a publickey (<code>.pem</code>) from an EC2 instance, used <code>ssh-add</code> to add the key to my local machine. The IdentifyFile in the Host entry at <code>/etc/ssh/ssh_config</code> points to the .pem file at <code>~/.ssh</code>.  As noted in the AWS guide, I used <code>chmod</code> to edit permissions on the file before invoking <code>ssh-add</code>.</p>

<p>Unfortunately, I'm still getting:</p>

<pre><code>user@deb $ ssh aws
Permission denied (publickey).
</code></pre>

<p>I don't think it's relevant, but for the record it's the 5th or 6th key I've generated for the EC2 instance. Any suggestions appreciated.  </p>
","<ssh><amazon-ec2><public-key>","2019-10-07 22:47:10"
"841470","Netflix using self signed certificates?","<p>I recently ran into problems running an apple tv with netflix at my work.
 After some digging around our sonicwall firewall with one of their support techs we found the service being blocked by the certificate netflix was passing.  Apparently the certificate Wasn't passing the ""Detect Certificate signed by an Untrusted CA"" so there using self signed or their certificate has the wrong name or whatever.</p>

<p>Has anyone seen netflix signing there own certs? seem odd that a public company would do that?
The tech said the cert was missing the common name on it so maybe its just misconfigured.
Would be interesting to hear your thoughts, I turned off the check and its working fine, but never had a problem with a big company with non compliant certs.
Interestingly the problem doesn't arise on browsers running netflix in our office. </p>

<p>Well I reconstructed the certificate that were sent back and forth and there not self signed... But is there anything wrong with them? so my new question would be how do you check if a certificate is good or bad?</p>

<p><a href=""https://drive.google.com/open?id=0B0whF2PJIIWcSVg4MEJrc3liM1U"" rel=""nofollow noreferrer"">https://drive.google.com/open?id=0B0whF2PJIIWcSVg4MEJrc3liM1U</a>
<a href=""https://drive.google.com/open?id=0B0whF2PJIIWcN2R0RHBYT2FGRm8"" rel=""nofollow noreferrer"">https://drive.google.com/open?id=0B0whF2PJIIWcN2R0RHBYT2FGRm8</a></p>
","<certificate><sonicwall>","2017-03-30 05:09:13"
"841612","Can't telnet a server from vps","<p>i try to understand why i can't telnet this server 213.132.48.107 from any of my VPSs.
Well this ip is for a mail server i don't own but our emails are not received by this server. when i checked the log i saw this error:</p>

<pre><code>554 5.7.1 You are not allowed to connect.
Connection closed by foreign host.
</code></pre>

<p>I thought i'm in black list so i tried to telnet to that server from all my VPSs (7 VPSs) and always get the same error.</p>

<pre><code>telnet 213.132.48.107 25
Trying 213.132.48.107...
Connected to 213.132.48.107.
Escape character is '^]'.
554 5.7.1 You are not allowed to connect.
</code></pre>

<p>But when i try from a dedicate server i can telnet.</p>

<p>I have contacted the server admin but didn't get any response. I still don't see any reason for them to block my IP address. we just installed the server and it's used for regular emails only.</p>

<p>Please help me</p>

<p>My VPSs run debian/ubuntu/centos and all created created by proxmox</p>
","<ubuntu><debian><smtp><kvm-virtualization><proxmox>","2017-03-30 16:43:16"
"910328","Apollo broker run error","<p>I have been trying to create a broker on my activemq apache-apollo 1.7.1 installation. I get the following error:</p>

<pre><code>$ apollo-broker run
Startupfailed:java.lang.NoClassDefFoundError:javax/xml/bind/ValidationEventHandler
</code></pre>
","<java><activemq>","2018-05-02 11:11:55"
"771716","HTML5 Audio not working for Android","<p>I am creating a page with an embedded audio player that works on all systems I have tested, except on Android. The players do appear on the page, but you cannot start them playing.</p>

<p>I've tried both the <code>&lt;audio&gt;</code> tag and <a href=""http://www.schillmania.com/projects/soundmanager2/"" rel=""nofollow noreferrer"">SoundManager 2</a>, checked whether the device does normally play mp3 files with <a href=""http://hpr.dogphilosophy.net/test/"" rel=""nofollow noreferrer"">HTML5 Audio Formats Test</a> (which it does), and downloaded that same test to my server and checked whether it worked there (which it doesn't). And I cannot find anything helpful in the log files.</p>

<p>Any ideas what I could do?</p>

<p>edit: I have since tried the same with Apache on the same server and with Nginx on another server with the same results.</p>
","<android><audio><html5><mp3>","2016-04-20 13:34:21"
"910408","Shell script to enumerate the (already existing) LDAP group called ""VPN Users""","<p>I am new to LDAP and learning about the ldapsearch and ldap in general. Would like to write a shell script that enumerate the (already existing) LDAP group ""VPN Users"", then get all user ""samaccountname"" (e.g.joe.smith). Can someone please help? I've tried to do it but seems like my ldapsearch has errors:</p>

<pre><code>ldapsearch -h 127.0.0.1 -x -b (&amp;(objectCategory=group)(cn=VPN Users))
</code></pre>

<p>Edit - Sorry about that. I made some progress and now at least cat get some data from the LDAP server by running something like this: </p>

<pre><code>echo -n 'password' |
ldapsearch -y pass.txt -h [IP here] -b ""ou=x,dc=x,dc=x,dc=x"" -x `
 -D ""cn=user,ou=test,ou=x,ou=x,dc=x,dc=x,dc=xx"" `
 -W ""(&amp;(objectClass=person)(objectClass=user)(sAMAccountName=*) (memberOf=cn=VPN Users,ou=test,ou=x,ou=x,dc=x,dc=x,dc=xx))"" `
 -y /dev/fd/0
</code></pre>

<p>But I would like it to:</p>

<ul>
<li>enumerate the (already existing) LDAP group ""VPN Users""</li>
<li>get all user ""samaccountname"" (e.g. joe.smith, etc.)</li>
</ul>

<p>Thanks</p>
","<ldap><shell><openldap><shell-scripting>","2018-05-02 17:41:12"
"841845","traceroute to google cloud instance","<p>When I do a trace route to our Google Cloud instance, it goes all the way to United Kingdom and times out.  It then goes to our Google Cloud instance.  It is causing our application to run slow. Any reason the routes to our Google Cloud instance is taking long time?</p>
","<linux>","2017-03-31 17:23:32"
"910696","Check existence of multiple network shared files","<p>Let's say I have a list of files (<code>files.txt</code>), which looks like:</p>

<pre><code>\\myshare\file1
\\myshare\file2
\\myshare\file3
</code></pre>

<p>How is it possible to check via a batch script (windows) to check if those files exist and output this? I tried with <code>if exists \\path\file</code>, but it always tells me that the path can not be used syntactically at that point.</p>
","<windows><scripting><network-share><batch>","2018-05-04 08:18:46"
"910708","How to list all services linked to libwrap.a (tcp wrappers)","<p>I know most Linux network services are linked to libwarp.a (tcp-wrappers) and this can be check via the <em>ldd</em> command. But can't seem to find how to list all the network services being linked to <em>libwarp.a</em>.</p>

<p>After some waiting, I created a dump way myself. Run the script below in <em>/usr/sbin</em></p>

<p><code>find ./ -type f -exec echo {} \; -exec ldd {} \; | egrep ""\\./|libwrap"" | less</code></p>

<p>Found 5 in CentOS7. Is there a correct way to do this?</p>
","<linux><centos><redhat><linux-networking>","2018-05-04 10:02:58"
"841983","How to combine .frm and .ibd file in MySQL","<p>I have a problem with my MySQL Database.
I reinstalled my Operating System, of course, I backed up my .frm and ibdata
But, when i tried to open the database, MySQL won't read any of the data in there,
the files aren't corrupt because I can open them with other programs. 
Can someone tell me how could I repair this, because I tried with several commands, some told me to modify the my.ini, some told me to try with third party programs, but none of them didn't work.</p>

<p>I would really appreciate someone trying to help me</p>

<p>EDIT: Links to my.cnf and error: </p>

<p>pastebin.com/smQz4Pj1 -My.cfg 
 pastebin.com/BQky47dA -Error </p>
","<mysql><database>","2017-04-01 17:09:36"
"842003","How to convert this Apache rewrite rule to Nginx","<p>I've been struggling to convert my past Apache rewrite rule to Nginx (also not sure if I'm placing it in the right place so would appreciate if you can tell me where to place it).</p>

<p>Basically, this was the Apache rewrite rule in my <code>.htaccess</code> file on WordPress:</p>

<pre><code>&lt;IfModule mod_rewrite.c&gt;
  RewriteEngine on
  RewriteRule ^/?hosts/(.*)$ /user/$1 [R,L]
&lt;/IfModule&gt;
</code></pre>

<p>As you can see, I'm using this rule in order redirect from<br>
<code>example.com/hosts/username</code> to <code>example.com/user/username</code>.</p>

<p>I've used this converting tool <a href=""https://labs.gidix.de/nginx/"" rel=""nofollow noreferrer"">https://labs.gidix.de/nginx/</a> and it outputs this conversion:</p>

<pre><code>rewrite ^/?hosts/(.*)$ /user/$1 last;
</code></pre>

<p>However, I tried placing this in Ajenti's (control panel) advanced custom configuration but it's not working.</p>
","<apache-2.2><nginx><mod-rewrite><rewrite><wordpress>","2017-04-01 22:21:46"
"910781","nginx throws many errors","<p>I have a fresh installation of a nginx server running nextcloud with letsencrypt. I thought I configured everything correctly, but now I thought I could check my logs and voila I got a bunch of errors, which I can´t figure out how to fix :(</p>

<p>I already tried to modify my XYZ.com.conf, but it didn´t work.</p>

<p>Any help is appreciated.</p>

<pre><code>2018/05/04 19:22:09 [error] 4243#4243: ocsp.int-x3.letsencrypt.org could not be resolved (110: Operation timed out) while requesting certificate status, responder: ocsp.int-x3.letsencrypt.org, certificate: ""/etc/letsencrypt/live/XYZ.com/fullchain.pem""
2018/05/04 19:22:40 [error] 4244#4244: *87 open() ""/etc/nginx/html/status.php"" failed (2: No such file or directory), client: XXX.XXX.XXX.XXX, server: XYZ.com, request: ""GET /status.php HTTP/1.1"", host: ""XYZ.com""
2018/05/04 19:22:41 [error] 4244#4244: *87 open() ""/etc/nginx/html/owncloud/status.php"" failed (2: No such file or directory), client: XXX.XXX.XXX.XXX, server: XYZ.com, request: ""GET /owncloud/status.php HTTP/1.1"", host: ""XYZ.com""
2018/05/04 19:23:05 [error] 4243#4243: *94 open() ""/etc/nginx/html/status.php"" failed (2: No such file or directory), client: XXX.XXX.XXX.XXX, server: XYZ.com, request: ""GET /status.php HTTP/1.1"", host: ""XYZ.com""
2018/05/04 19:23:05 [error] 4243#4243: *94 open() ""/etc/nginx/html/owncloud/status.php"" failed (2: No such file or directory), client: XXX.XXX.XXX.XXX, server: XYZ.com, request: ""GET /owncloud/status.php HTTP/1.1"", host: ""XYZ.com""
2018/05/04 19:23:36 [error] 4243#4243: *101 open() ""/etc/nginx/html/status.php"" failed (2: No such file or directory), client: XXX.XXX.XXX.XXX, server: XYZ.com, request: ""GET /status.php HTTP/1.1"", host: ""XYZ.com""
2018/05/04 19:23:36 [error] 4243#4243: *101 open() ""/etc/nginx/html/owncloud/status.php"" failed (2: No such file or directory), client: XXX.XXX.XXX.XXX, server: XYZ.com, request: ""GET /owncloud/status.php HTTP/1.1"", host: ""XYZ.com""
2018/05/04 19:24:08 [error] 4243#4243: *108 open() ""/etc/nginx/html/status.php"" failed (2: No such file or directory), client: XXX.XXX.XXX.XXX, server: XYZ.com, request: ""GET /status.php HTTP/1.1"", host: ""XYZ.com""
2018/05/04 19:24:08 [error] 4243#4243: *108 open() ""/etc/nginx/html/owncloud/status.php"" failed (2: No such file or directory), client: XXX.XXX.XXX.XXX, server: XYZ.com, request: ""GET /owncloud/status.php HTTP/1.1"", host: ""XYZ.com""
2018/05/04 19:24:40 [error] 4244#4244: *119 open() ""/etc/nginx/html/status.php"" failed (2: No such file or directory), client: XXX.XXX.XXX.XXX, server: XYZ.com, request: ""GET /status.php HTTP/1.1"", host: ""XYZ.com""
2018/05/04 19:24:40 [error] 4244#4244: *119 open() ""/etc/nginx/html/owncloud/status.php"" failed (2: No such file or directory), client: XXX.XXX.XXX.XXX, server: XYZ.com, request: ""GET /owncloud/status.php HTTP/1.1"", host: ""XYZ.com""
2018/05/04 19:25:12 [error] 4244#4244: *126 open() ""/etc/nginx/html/status.php"" failed (2: No such file or directory), client: XXX.XXX.XXX.XXX, server: XYZ.com, request: ""GET /status.php HTTP/1.1"", host: ""XYZ.com""
2018/05/04 19:25:12 [error] 4244#4244: *126 open() ""/etc/nginx/html/owncloud/status.php"" failed (2: No such file or directory), client: XXX.XXX.XXX.XXX, server: XYZ.com, request: ""GET /owncloud/status.php HTTP/1.1"", host: ""XYZ.com""
2018/05/04 19:25:44 [error] 4243#4243: *136 open() ""/etc/nginx/html/status.php"" failed (2: No such file or directory), client: XXX.XXX.XXX.XXX, server: XYZ.com, request: ""GET /status.php HTTP/1.1"", host: ""XYZ.com""
2018/05/04 19:25:45 [error] 4243#4243: *136 open() ""/etc/nginx/html/owncloud/status.php"" failed (2: No such file or directory), client: XXX.XXX.XXX.XXX, server: XYZ.com, request: ""GET /owncloud/status.php HTTP/1.1"", host: ""XYZ.com""
2018/05/04 19:26:16 [error] 4243#4243: *143 open() ""/etc/nginx/html/status.php"" failed (2: No such file or directory), client: XXX.XXX.XXX.XXX, server: XYZ.com, request: ""GET /status.php HTTP/1.1"", host: ""XYZ.com""
2018/05/04 19:26:16 [error] 4243#4243: *143 open() ""/etc/nginx/html/owncloud/status.php"" failed (2: No such file or directory), client: XXX.XXX.XXX.XXX, server: XYZ.com, request: ""GET /owncloud/status.php HTTP/1.1"", host: ""XYZ.com""
2018/05/04 19:26:48 [error] 4243#4243: *150 open() ""/etc/nginx/html/status.php"" failed (2: No such file or directory), client: XXX.XXX.XXX.XXX, server: XYZ.com, request: ""GET /status.php HTTP/1.1"", host: ""XYZ.com""
2018/05/04 19:26:48 [error] 4243#4243: *150 open() ""/etc/nginx/html/owncloud/status.php"" failed (2: No such file or directory), client: XXX.XXX.XXX.XXX, server: XYZ.com, request: ""GET /owncloud/status.php HTTP/1.1"", host: ""XYZ.com""
2018/05/04 19:27:20 [error] 4243#4243: *154 open() ""/etc/nginx/html/status.php"" failed (2: No such file or directory), client: XXX.XXX.XXX.XXX, server: XYZ.com, request: ""GET /status.php HTTP/1.1"", host: ""XYZ.com""
2018/05/04 19:27:20 [error] 4243#4243: *154 open() ""/etc/nginx/html/owncloud/status.php"" failed (2: No such file or directory), client: XXX.XXX.XXX.XXX, server: XYZ.com, request: ""GET /owncloud/status.php HTTP/1.1"", host: ""XYZ.com""
2018/05/04 19:27:52 [error] 4243#4243: *158 open() ""/etc/nginx/html/status.php"" failed (2: No such file or directory), client: XXX.XXX.XXX.XXX, server: XYZ.com, request: ""GET /status.php HTTP/1.1"", host: ""XYZ.com""
2018/05/04 19:27:52 [error] 4243#4243: *158 open() ""/etc/nginx/html/owncloud/status.php"" failed (2: No such file or directory), client: XXX.XXX.XXX.XXX, server: XYZ.com, request: ""GET /owncloud/status.php HTTP/1.1"", host: ""XYZ.com""
2018/05/04 19:28:24 [error] 4243#4243: *162 open() ""/etc/nginx/html/status.php"" failed (2: No such file or directory), client: XXX.XXX.XXX.XXX, server: XYZ.com, request: ""GET /status.php HTTP/1.1"", host: ""XYZ.com""
2018/05/04 19:28:24 [error] 4243#4243: *162 open() ""/etc/nginx/html/owncloud/status.php"" failed (2: No such file or directory), client: XXX.XXX.XXX.XXX, server: XYZ.com, request: ""GET /owncloud/status.php HTTP/1.1"", host: ""XYZ.com""
</code></pre>

<p>EDIT:</p>

<p>XYZ.com.conf </p>

<pre><code>server {
    listen 80 default_server;
    server_name XYZ.com www.XYZ.com;

    root /var/www;

    location ^~ /.well-known/acme-challenge {
        proxy_pass http://127.0.0.1:81;
        proxy_redirect off;
    }       
    location / {
        # Enforce HTTPS
        # Use this if you always want to redirect to the DynDNS address (no local access).
        return 301 https://$server_name$request_uri;

        # Use this if you also want to access the server by local IP:
        #return 301 https://$server_addr$request_uri;
    }       
}

server {
    listen 443 ssl http2;
    server_name XYZ.com www.XYZ.com;

    #
    # Configure SSL
    #
    ssl on;

    # Certificates used
    ssl_certificate /etc/letsencrypt/live/XYZ.com/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/XYZ.com/privkey.pem;

    # Not using TLSv1 will break:
    #   Android &lt;= 4.4.40
    #   IE &lt;= 10
    #   IE mobile &lt;=10
    # Removing TLSv1.1 breaks nothing else!
    ssl_protocols TLSv1.2;

    # Using the recommended cipher suite from: https://wiki.mozilla.org/Security/Server_Side_TLS
    ssl_ciphers 'ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA256:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA:DHE-RSA-AES256-SHA:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!3DES:!MD5:!PSK';

    # Diffie-Hellman parameter for DHE ciphersuites, recommended 2048 bits
    ssl_dhparam /etc/nginx/ssl/dhparams.pem;

    # Specifies a curve for ECDHE ciphers.
    # High security, but will not work with Chrome:
    #ssl_ecdh_curve secp521r1;  
    # Works with Windows (Mobile), but not with Android (DavDroid):
    #ssl_ecdh_curve secp384r1;
    # Works with Android (DavDroid):
    ssl_ecdh_curve prime256v1; 

    # Server should determine the ciphers, not the client
    ssl_prefer_server_ciphers on;

    # OCSP Stapling
    # fetch OCSP records from URL in ssl_certificate and cache them
    ssl_stapling on;
    ssl_stapling_verify on;
    ssl_trusted_certificate /etc/letsencrypt/live/XYZ.com/fullchain.pem;
    resolver XYZ.com;

    # SSL session handling
    ssl_session_timeout 24h;
    ssl_session_cache shared:SSL:50m;
    ssl_session_tickets off;

    #
    # Add headers to serve security related headers
    #  
    # HSTS (ngx_http_headers_module is required)
    # In order to be recoginzed by SSL test, there must be an index.hmtl in the server's root
    add_header Strict-Transport-Security ""max-age=63072000; includeSubdomains"" always;
    add_header X-Content-Type-Options ""nosniff"" always;
    # Usually this should be ""DENY"", but when hosting sites using frames, it has to be ""SAMEORIGIN""
    add_header Referrer-Policy ""same-origin"" always;
    add_header X-XSS-Protection ""1; mode=block"" always;
    add_header X-Robots-Tag none;
    add_header X-Download-Options noopen;
    add_header X-Permitted-Cross-Domain-Policies none;

    location = / {
        # Disable access to the web root, the Nextcloud subdir should be used instead.
        #deny all;

        # If you want to be able to access the cloud using the webroot only, use the following command instead:
        rewrite ^ /nextcloud;
    }   

    #
    # Nextcloud
    #
    location = /favicon.ico {
        log_not_found off;
    }

    location ^~ /nextcloud {
        # Set max. size of a request (important for uploads to Nextcloud)
        client_max_body_size 10G;
        # Besides the timeout values have to be raised in nginx' Nextcloud config, these values have to be raised for the proxy as well
        proxy_connect_timeout 3600;
        proxy_send_timeout 3600;
        proxy_read_timeout 3600;
        send_timeout 3600;
        proxy_buffering off;
        proxy_request_buffering off;
        proxy_max_temp_file_size 1024m;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_pass http://127.0.0.1:82;
        proxy_redirect off;
    }   
}
</code></pre>

<p>XYZ.com_letsencrypt.conf</p>

<pre><code>server {
    listen 127.0.0.1:81;
    server_name 127.0.0.1;  

    location ^~ /.well-known/acme-challenge {
        default_type text/plain;
        root /var/www/letsencrypt;
    }
}
</code></pre>

<p>XYZ.com_nextcloud.conf</p>

<pre><code>upstream php-handler {
    server unix:/run/php/php7.0-fpm.sock;
}

server {
    listen 82;
    server_name 127.0.0.1;

    # Add headers to serve security related headers
    # Use 'proxy_set_header' (not 'add_header') as the headers have to be passed through a proxy.
    proxy_set_header Strict-Transport-Security ""max-age=15768000; includeSubDomains; always;"";
    proxy_set_header X-Content-Type-Options ""nosniff; always;"";
    proxy_set_header X-XSS-Protection ""1; mode=block; always;"";
    proxy_set_header X-Robots-Tag none;
    proxy_set_header X-Download-Options noopen;
    proxy_set_header X-Permitted-Cross-Domain-Policies none;

    # Path to the root of your installation
    root /var/www/;

    location = /robots.txt {
        allow all;
        log_not_found off;
        access_log off;
    }

    # The following 2 rules are only needed for the user_webfinger app.
    # Uncomment it if you're planning to use this app.
    #rewrite ^/.well-known/host-meta /nextcloud/public.php?service=host-meta last;
    #rewrite ^/.well-known/host-meta.json /nextcloud/public.php?service=host-meta-json last;

    location = /.well-known/carddav { 
        return 301 $scheme://$host/nextcloud/remote.php/dav; 
    }

    location = /.well-known/caldav { 
        return 301 $scheme://$host/nextcloud/remote.php/dav; 
    }

    location /.well-known/acme-challenge { }

    location ^~ /nextcloud {
        # set max upload size
        client_max_body_size 10G;
        fastcgi_buffers 64 4K;

        # Enable gzip but do not remove ETag headers
        gzip on;
        gzip_vary on;
        gzip_comp_level 4;
        gzip_min_length 256;
        gzip_proxied expired no-cache no-store private no_last_modified no_etag auth;
        gzip_types application/atom+xml application/javascript application/json application/ld+json application/manifest+json application/rss+xml application/vnd.geo+json application/vnd.ms-fontobject application/x-font-ttf application/x-web-app-manifest+json application/xhtml+xml application/xml font/opentype image/bmp image/svg+xml image/x-icon text/cache-manifest text/css text/plain text/vcard text/vnd.rim.location.xloc text/vtt text/x-component text/x-cross-domain-policy;

        # Uncomment if your server is build with the ngx_pagespeed module
        # This module is currently not supported.
        #pagespeed off;

        location /nextcloud {
            rewrite ^ /nextcloud/index.php$uri;
        }

        location ~ ^/nextcloud/(?:build|tests|config|lib|3rdparty|templates|data)/ {
            deny all;
        }

        location ~ ^/nextcloud/(?:\.|autotest|occ|issue|indie|db_|console) {
            deny all;
        }

        location ~ ^/nextcloud/(?:index|remote|public|cron|core/ajax/update|status|ocs/v[12]|updater/.+|ocs-provider/.+|core/templates/40[34])\.php(?:$|/) {
            include fastcgi_params;
            fastcgi_split_path_info ^(.+\.php)(/.+)$;
            fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
            fastcgi_param PATH_INFO $fastcgi_path_info;
            #Avoid sending the security headers twice
            fastcgi_param modHeadersAvailable true;
            fastcgi_param front_controller_active true;
            fastcgi_pass php-handler;
            fastcgi_intercept_errors on;

            # Raise timeout values.
            # This is especially important when the Nextcloud setup runs into timeouts (504 gateway errors)
            fastcgi_read_timeout 600;
            fastcgi_send_timeout 600;
            fastcgi_connect_timeout 600;
            fastcgi_request_buffering off;

            # Pass PHP variables directly to PHP.
            # This is usually done in the php.ini. For more flexibility, these variables are configured in the nginx config.
        # All the PHP parameters have to be set in one fastcgi_param. When using more 'fastcgi_param PHP_VALUE' directives, the last one will override all the others.
            fastcgi_param PHP_VALUE ""open_basedir=/var/www:/tmp/:/mnt/raid/data:/dev/urandom:/proc/meminfo
                upload_max_filesize = 10G
                post_max_size = 10G
                max_execution_time = 3600
                output_buffering = off"";

            # Make sure that the real IP of the remote host is passed to PHP.
            fastcgi_param REMOTE_ADDR $http_x_real_ip;
        }

        location ~ ^/nextloud/(?:updater|ocs-provider)(?:$|/) {
            try_files $uri/ =404;
            index index.php;
        }

        # Adding the cache control header for js and css files
        # Make sure it is BELOW the PHP block
        location ~* \.(?:css|js)$ {
            try_files $uri /nextcloud/index.php$uri$is_args$args;
            proxy_set_header Cache-Control ""public, max-age=7200"";
            # Add headers to serve security related headers
            # Again use 'proxy_set_header' (not 'add_header') as the headers have to be passed through a proxy.
            proxy_set_header Strict-Transport-Security ""max-age=15768000; includeSubDomains; preload;"";
            proxy_set_header X-Content-Type-Options nosniff;
            #proxy_set_header X-Frame-Options ""SAMEORIGIN"";
            proxy_set_header X-XSS-Protection ""1; mode=block"";
            proxy_set_header X-Robots-Tag none;
            proxy_set_header X-Download-Options noopen;
            proxy_set_header X-Permitted-Cross-Domain-Policies none;
            # Optional: Don't log access to assets
            access_log off;
        }

        location ~* \.(?:svg|gif|png|html|ttf|woff|ico|jpg|jpeg)$ {
            try_files $uri /nextcloud/index.php$uri$is_args$args;
            # Optional: Don't log access to other assets
            access_log off;
        }
    }
}
</code></pre>
","<nginx><php>","2018-05-04 17:54:48"
"842050","webdav, controlling access via group of users","<p>I'm in the first steps with webdav.</p>

<p>To restrict access to a folder it's possible to use:</p>

<pre><code>Require user myownuser
</code></pre>

<p>Is it possible to allow access to a group of users (like samba does)?</p>
","<linux><ubuntu><amazon-ec2><apache-2.4><webdav>","2017-04-02 11:55:03"
"910820","2 different sites on 2 different ports not working. Apache","<p>I'm trying to host 2 different sites on 2 different ports: 80 and 8080.
I have currently set this up:</p>

<p>ports.conf</p>

<pre><code>listen 80
listen 8080
NameVirtualHost *:80
NameVirtualHost *:8080
</code></pre>

<p>site1.conf</p>

<pre><code>&lt;VirtualHost *:80&gt;
  ServerName my.site
...
</code></pre>

<p>site2.conf</p>

<pre><code>&lt;VirtualHost *:8080&gt;
  ServerName a.my.site
...
</code></pre>

<p>Portscan tells me port 80 an 8080 are open and both have an http service.</p>

<p><code>http://my.site</code> works and I get the desired site, but when I'm trying to connect to <code>http://my.site:8080</code>. I get <code>This site can't be reached. my.site refused to connect</code></p>

<p>What did I do wrong?</p>

<p>(apache 2.2)</p>

<p>EDIT:</p>

<p><code>http://(ip):8080</code> works.</p>

<p>I changed <code>site1</code> <code>site2</code>.
Now I tried <code>http://a.my.site</code> and it didn't work. Nor did <code>... :8080</code>.</p>
","<apache-2.2>","2018-05-04 22:25:53"
"842070","Linux IP forwarding; source IP is changed?","<p>Apologies for the vague title of this post, I wasn't quite sure what to call this behaviour.</p>

<p>I have set up a virtual network topology like so:</p>

<pre><code>+------------+                  +------------+
| host1      |                  | router     |
| 172.41.1.1 | &lt;---- net1 ----&gt; | 172.41.2.1 |                  +------------+
+------------+  (172.41.0.0/16) | 172.42.2.1 | &lt;---- net2 ----&gt; | host2      |
                                +------------+  (172.42.0.0/16) | 172.42.1.1 |
                                                                +------------+
</code></pre>

<p>These ""nodes"" are in fact containers, and the networks are <a href=""https://docs.docker.com/engine/userguide/networking/#user-defined-networks"" rel=""nofollow noreferrer"">Docker networks</a>. (Additionally, <code>net1</code> is a tunnel network, but I have elided this as I think it is irrelevant for the problem I am seeing.)</p>

<p>When I run <code>ping 172.42.1.2</code> on <code>host1</code>, I do not see responses. Running <code>tcpdump -i eth1 -n</code> on <code>router</code> (<code>eth1</code> is the interface for <code>net2</code>), I see what looks to me like correct packets being forwarded:</p>

<pre><code>15:06:09.180072 IP 172.41.1.1 &gt; 172.42.1.1: ICMP echo request, id 1, seq 14, length 64
15:06:10.181545 IP 172.41.1.1 &gt; 172.42.1.1: ICMP echo request, id 1, seq 15, length 64
15:06:11.182521 IP 172.41.1.1 &gt; 172.42.1.1: ICMP echo request, id 1, seq 16, length 64
15:06:12.184381 IP 172.41.1.1 &gt; 172.42.1.1: ICMP echo request, id 1, seq 17, length 64
</code></pre>

<p>However, on <code>host2</code> I see the following:</p>

<pre><code>15:06:58.257789 IP 172.42.0.1 &gt; 172.42.1.1: ICMP echo request, id 1, seq 63, length 64
15:06:58.257811 IP 172.42.1.1 &gt; 172.42.0.1: ICMP echo reply, id 1, seq 63, length 64
15:06:59.259254 IP 172.42.0.1 &gt; 172.42.1.1: ICMP echo request, id 1, seq 64, length 64
15:06:59.259282 IP 172.42.1.1 &gt; 172.42.0.1: ICMP echo reply, id 1, seq 64, length 64
</code></pre>

<p>That is, the source IP of the packets appears to be changed to the network address of <code>net2</code>: <code>172.42.0.1</code>. <strong>Why would this happen?</strong></p>
","<linux><networking><docker>","2017-04-02 15:11:02"
"842078","How to Access a host behind a server over VPN (IP Forwarding)","<p>I have a Ubuntu server (172.10.200.11) and many remote terminal units with simcards and each individual simcard operator using different IP pool such as 10.57.0.0/16 for OP-A, 10.112.0.0/16 for OP-B, etc. </p>

<p>In the server actually i am using a routing table to be able to access the remote terminal units on diffent IP pools. Without this routing table i have no access to the IP pools. 
For instance :
10.57.0.0/16 using gw 172.10.238.1
10.112.0.0/16 using gw 172.10.238.2
10.155.0.0/16 using gw 172.10.238.3</p>

<p>And...</p>

<p>Actually i am using ""Cisco VPN Client"" to access my server IP from outside of datacenter. For this purpose i am making a connection to VPN gateway using ""Cisco VPN CLient"" application then trying to ping my server's IP 172.10.200.11</p>

<p>I can do the following things successfully from my laptop after my VPN Connection established to VPN Gateway:</p>

<ul>
<li>Establish Remote Desktop connection the server's IP:172.10.238.3</li>
<li>Establish SSH connection from my laptop to the server's IP:172.10.238.3</li>
<li>ICMP ping to the server's IP:172.10.238.3</li>
<li>Traceroute to the server's IP:172.10.238.3</li>
</ul>

<p>For checking the connection (up or down) status  of Remote Terminal Units i am connecting to the server via SSH or Remote Desktop then trying to ping to the IP address of Remote Terminal Unit. </p>

<p>Everything is OK until here but this way consumes too much bandwidth especially in case if connect via Remote Desktop connection. </p>

<ol>
<li>Connect to VPN Gateway using ""Cisco VPN Client"" from the laptop</li>
<li>Establish a Remote Desktop connection to server's IP</li>
<li>Open Browser in the server on Remote Desktop Connection.</li>
<li>Enter Remote Terminal Unit (RTU)'s IP (10.155.1.22) on the browser and do what you want!</li>
<li>Successfullu establish an ICMP ping to the RTU's IP address (10.155.1.22) in a terminal screen on Remote Desktop or in a SSH connection session. </li>
</ol>

<p>But i want to do following:</p>

<ul>
<li>Connect to VPN Gateway using Cisco VPN Client from the laptop</li>
<li>Open Browser in the laptop</li>
<li>Enter Remote Terminal Unit's IP (10.155.1.22) on browser and do what you want!</li>
<li>Successfully establish an ICMP ping to the RTU's IP address (10.155.1.22) in a shell/terminal screen on my laptop (NOT in Remote Desktop)</li>
</ul>

<p><strong>Restrictions:</strong> </p>

<ul>
<li>Actually i have no right to change the VPN gateway settings.  But i can
only change the server settings to achieve this.  </li>
</ul>

<p>Is there any way to do this? I know it exist but my mind confused. 
First i have installed Hamachi but this way give me access to server without need of VPN connection. But still i can not directly ping to RTU IP's from my laptop. It did not resolved my problem. </p>

<p>In technically i want to use my server act as a ""router"" to route/forward incoming requests from my laptop (ICMP &amp; IP packets) to the RTU's IP. </p>

<p>I have researched about how to enable IP MASQUERADE or IP FORWARDING on Ubuntu 16.04. If i am right -technically- it needs 2 NICs or 2 Different IPs on the machine. But i have only one IP (Bonded) on my server.</p>

<p>I need to reach  directly to the RTU IP addresses from my laptop and my server should be acting as a router/gateway or etc. to achieve this. </p>

<p>Is there anybody can explain me step-by-step how to do this on Ubuntu?</p>

<p><a href=""https://i.sstatic.net/TmzV6.jpg"" rel=""nofollow noreferrer"">Click to see picture of my system diagram </a></p>
","<vpn><ping><cisco-vpn><ip-forwarding><masquerade>","2017-04-02 16:20:55"
"910840","Windows 10 pro event log file is corrupted","<p>I'm trying to read event log with LogParser on windows 10 pro, and the query <code>select * from security</code> executed well.</p>

<p>But I'm getting a message saying that the file is corrupted After executing the query <code>select count(*) from security</code>. Does anybody like me?</p>

<p>And <code>System event</code> also shows the same result. But <code>Application event</code> executed well all query. For reference, all events read well in Event Viewer.</p>
","<windows-event-log><logparser>","2018-05-05 03:10:29"
"842079","webdav, open just one particular (/webdav/public/) folder to the world","<p>My webdav stuff is on <code>/var/www/webdav/</code>, there I have multiple access roles to enable access to users and groups. And I would like to open one particular folder to the world (/webdav/public/). How do I do that?</p>

<p>I tried:</p>

<pre><code>Alias /webdav /var/www/webdav
&lt;Location /webdav&gt;
    DAV On
    AuthType Digest
    AuthName ""webdav""
    AuthUserFile /etc/apache2/users.password
    Require valid-user
&lt;/Location&gt;

Alias /webdav/public /var/www/webdav/public
&lt;Directory /var/www/webdav/public&gt;
    DAV On
&lt;/Directory&gt;
</code></pre>

<p>But this way, requesting <code>http://mysite.ddns.net/webdav/public</code> asks for credentials.</p>
","<linux><ubuntu><amazon-ec2><apache-2.4><webdav>","2017-04-02 16:26:42"
"910903","AWS S3 bucket items uploaded by JS should only be accessible from Heroku app (edited)","<p>OK, I figured out my original issue of every item in a bucket getting the Public ACL was not because of my policies but because I am using the presigned URL JavaScript upload and had <code>acl:public-read</code> in the upload. But I don't want that. I want the heroku app to be able to access the photos but not the rest of the world. Can this be done in Rails? Can the concept of pre-signed URLs be used for access as well upload? I have CORS setup for the upload as:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;CORSConfiguration xmlns=""http://s3.amazonaws.com/doc/2006-03-01/""&gt;
&lt;CORSRule&gt;
    &lt;AllowedOrigin&gt;http://mylocalhost&lt;/AllowedOrigin&gt;
    &lt;AllowedOrigin&gt;http://www.myherokuapp.com&lt;/AllowedOrigin&gt;
    &lt;AllowedMethod&gt;PUT&lt;/AllowedMethod&gt;
    &lt;AllowedMethod&gt;POST&lt;/AllowedMethod&gt;
    &lt;AllowedMethod&gt;DELETE&lt;/AllowedMethod&gt;
    &lt;MaxAgeSeconds&gt;3000&lt;/MaxAgeSeconds&gt;
    &lt;ExposeHeader&gt;ETag&lt;/ExposeHeader&gt;
    &lt;ExposeHeader&gt;x-amz-server-side-encryption&lt;/ExposeHeader&gt;
    &lt;ExposeHeader&gt;x-amz-request-id&lt;/ExposeHeader&gt;
    &lt;ExposeHeader&gt;x-amz-id-2&lt;/ExposeHeader&gt;
    &lt;AllowedHeader&gt;*&lt;/AllowedHeader&gt;
    &lt;AllowedHeader&gt;x-amz-acl&lt;/AllowedHeader&gt;
&lt;/CORSRule&gt;
&lt;/CORSConfiguration&gt;
</code></pre>

<p>Would adding GET to my CORS be the solution to my problem? Does it consider the original request for the photo when embedded in the web page to be coming from heroku? or from the browser?</p>

<p>------- old question -----</p>

<p>I have an AWS S3 bucket that I am trying to keep private except for access through my Heroku app. The only access should be from an IAM user or a CORS upload using a presigned URL. I already have a bunch of images in the bucket. Those were uploaded when the ACL was set to Public:READ. I would like to undo that but first I need a policy that will allow my Heroku app to access every image in the bucket. So my object is to:</p>

<p>1) Remove the ACL of Public from all sub items in the bucket</p>

<p>2) Set a bucket permission that denies ALL, then allows all access from my root user and my heroku IAM user (already set up). </p>

<p>I have a bucket policy of:</p>

<pre><code>{
  ""Id"": ""Policy1525547050154"",
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Sid"": ""Stmt1525546474878"",   
      ""Action"": ""s3:*"",
      ""Effect"": ""Allow"",
      ""Resource"": ""arn:aws:s3:::my-bucket"",
      ""Principal"": {
        ""AWS"": [
          ""arn:aws:iam::xxxxxxxxxxx:user/heroku-app""
        ]
      }
    },
    {
      ""Sid"": ""Stmt1525547042878"",
      ""Action"": ""s3:*"",
      ""Effect"": ""Allow"",
      ""Resource"": ""arn:aws:s3:::my-bucket"",
      ""Principal"": {
        ""AWS"": [
          ""arn:aws:iam::xxxxxxxxxxx:root""
        ]
      }
    }
  ]
}
</code></pre>

<p>I have applied this and it will apply to every NEW item uploaded into the bucket by the Heroku IAM user. But items that were previously uploaded do not seem to have this applied. The web console is very confusing and IMHO poorly written. I get that most big users will be using a command line console to do all of these things. I am a Ruby/Rails dev with a fair bit of systems management experience (pre-AWS) and I don't mind using the command line if it is in fact more useful than the web console. </p>

<p>EDIT: OK, looking at the doc again, I'm thinking my explicit deny of * at the beginning is never going to be overridden by my allows that follow. So I need to rethink my policy but it still doesn't change how to override the ACLs of previously saved items. I would do them manually except here are hundreds of them and I don't want to have to click through each one. It just doesn't make sense to me that an ACL at the top level wouldn't have an option of ""apply to all child items recursively"". And shouldn't my explicit policy override that?</p>

<p>Latest: I deleted the explicit deny on my policy. I then went to my Heroku app, uploaded a new photo and am able to do all of the actions the app should. BUT even the new upload is publicly available from a private window session in another browser just by copying the web address. I even tried the web address in an anonymous proxy and I can view the photo. I then removed the public ACL from the photo I uploaded and when I try to view it through my Heroku app it gets access denied. </p>
","<amazon-web-services><amazon-s3>","2018-05-05 20:06:55"
"911031","Update cannot find MSA60-cabinet","<p>We have a MSA60 connected to a P812-controller in a DL360 G6-server, and I've been trying to update the MSA60 to version 2.28 for about 2 weeks now using different methods but to no avail.</p>

<p>What has been tested so far:</p>

<ul>
<li>Offline update via HP SUM from Service Pack for ProLiant 2017.04</li>
<li>Offline update via HP SUM from Smart Update Firmware DVD 10.10</li>
<li>Offline update via HP SUM from Smart Update Firmware DVD 9.30</li>
</ul>

<p>The following versions are installed:</p>

<ul>
<li>DL360: CentOS 7</li>
<li>P410i-controller: 6.64</li>
<li>P812-controller: 6.64</li>
<li>MSA60: 2.18</li>
</ul>

<p>What is happening is that I boot with a medium, open a terminal and try to install the update</p>

<pre>
$ cd /mnt/boot/hp/swpackages
$ ./CP013530.scexe -q
</pre>

<p>And I get the following output:</p>

<p><em>Service Pack for ProLiant 2017.04</em></p>

<pre>
HP Enclosure ROM 2.28 Flash

No devices updated
</pre>

<p><em>Smart Update Firmware DVD 10.10</em></p>

<pre>
HP Enclosure ROM 2.28 Flash
    Smart Array controller in slot 0 - something about no update needed

No devices updated
</pre>

<p><em>Smart Update Firmware DVD 9.30</em></p>

<pre>
HP Enclosure ROM 2.28 Flash
    Smart Array controller in slot 0 - something about no update needed

    Smart Array controller in slot 3 - something about no update needed

No devices updated
</pre>

<p>Slot 0 is the P410i and slot 3 is the P812</p>

<p>Other things I've tried</p>

<ul>
<li>Downgrading the P812 to 5.56</li>
<li>Disabling the P410i in BIOS</li>
<li>Disconnecting all drives from the MSA60</li>
<li>HP SUM graphical update (9.30, 10.10 and 2017.04)</li>
<li>4 different updates (from disks and downloaded from hp)</li>
<li>Switched port of MSA60 in P812 from 4 to other ones</li>
<li>Changed SAS-cable</li>
<li>Restarted chassis several times</li>
</ul>

<p>The installation mediums are made with the HP USB Key Utility into a USB-stick.</p>

<p>The backplanes and midplane of the MSA60 are ""new"" enough as mentioned in <a href=""https://support.hpe.com/hpsc/doc/public/display?docId=c01361896"" rel=""nofollow noreferrer"">https://support.hpe.com/hpsc/doc/public/display?docId=c01361896</a></p>

<p>I have sniffed around the HPE-forum which didn't help me enough, and I am now running out of ideas. Any advices?</p>
","<hp-proliant><hp-storageworks><hpe>","2018-05-07 08:47:14"
"911034","i8042 no controller not found Oracle linux","<p>my server installed Oracle-Linux  7.4 . 
it worked till yesterday but today when i restart this server, it has an issue : 
my error code is :</p>

<blockquote>
  <p>i-8042 no controller not found</p>
</blockquote>

<p>i need some helps</p>
","<linux><oracle-linux>","2018-05-07 09:07:21"
"842590","Connecting to headless machine via crossover","<p>I'm trying to connect to a headless machine over a crossover cable. The headless machine gets it's address via dhcp. What is the easiest way to give the headless machine an ip address so I can connect to it?</p>

<pre><code> My Laptop -- Ethernet Cable -- Headless machine
</code></pre>

<p>The headless machine is running ssh, but I don't know the ip address it has. I know it was configured for dhcp to get the address, and it runs Fedora 25.</p>
","<ssh><dhcp><ip-address><headless>","2017-04-04 20:08:22"
"911352","Different website when accessing http and www","<p>I am trying to configure our website and currently, we have two versions of it:</p>

<ol>
<li>Old website (v1) which uses http:// (without www.)</li>
<li>New Website (v2) which uses <a href=""http://www"" rel=""nofollow noreferrer"">http://www</a>.</li>
</ol>

<p>I am a beginner at this and im scared to delete records in my admin profile because we really need the website to work but if by mistake, I delete a vital record, it is hard for me to fix things because I am not knowledgeable with Google Cloud. Here is the record set page:</p>

<p><a href=""https://i.sstatic.net/KAuqF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KAuqF.png"" alt=""enter image description here""></a></p>

<hr>

<p><a href=""https://i.sstatic.net/493VO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/493VO.png"" alt=""enter image description here""></a></p>

<p>What I want to happen is to remove the <a href=""http://pointblue.ph"" rel=""nofollow noreferrer"">http://pointblue.ph</a> or at least redirect it to www.pointblue.ph. Any leads?</p>
","<domain-name-system><google-cloud-platform><g-suite><google-app-engine>","2018-05-08 21:30:49"
"988493","AWS SFTP connection closed error","<p>WHen I try to connect with my server endpoint getting connection closed. It should be connected with server endpoint.</p>

<p><a href=""https://i.sstatic.net/wJUsP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wJUsP.png"" alt=""Sccreeshot""></a></p>

<p><a href=""https://i.sstatic.net/wJUsP.png"" rel=""nofollow noreferrer"">AWS Connection Closed</a></p>
","<amazon-web-services><sftp>","2019-10-17 23:41:23"
"772745","Using SSL certificate issued by Godaddy for localhost website","<p>I have a Godaddy SSL certificate that i want to use on <code>localhost</code>, however the certificate is issued to <code>example.com</code> and if i use that it definitely gives a certificate mismatch error. Is there any workaround to use it for <code>localhost</code> without having to go through the certificate error on any user? and the website is intranet based and would be accessed by other users using this <code>https://&lt;publicip&gt;/appdesk</code> or <code>https://localhost/appdesk</code> &amp; i need it to bypass the certificate warning or error automatically not manually that the user has to click everytime through to allow website access.</p>

<p><strong>Update</strong></p>

<p>I did try by issuing the certificate on the hostname itself but it too displays the certificate mismatch error.There are lot of computers accessing the server though publicip and cannot add the hostname to host file in every computer. Is there any way i can issue certificate directly to sub-directories like <code>appdesk</code> to avoid the mismatch error, so that accessing <code>https://localhost/appdesk</code> or <code>https://192.168.1.0/appdesk</code> doesn't show any error. </p>
","<windows-server-2008><iis><ssl><godaddy>","2016-04-25 13:43:12"
"842687","converting from raid 10 to raid 1 on windows server 2012 r2","<p>My current setup is that my OS and other important installed files are all installed in my raid 10 array, however, I need the 2 drives (ssd)  in the array for something else, is there a way to convert my existing raid 10 to raid 1? or do I need to clean install? Can I just backup my drive C, then reformat my raid to configure it to raid 1 then put my backup back? My OS is windows server 2012 r2. my raid controller is the Avago Megaraid 3108 and the server brand is supermicro. Any suggestions will be nice too, Thanks!  </p>
","<ssd><raid1><raid10><supermicro>","2017-04-05 08:45:03"
"911458","IPTables NAT Ubuntu redirect 2nd IP to another IP","<p>i have got two servers : </p>

<p>Server 1: </p>

<p>IP1= 8.8.8.1</p>

<p>IP2= 8.8.8.2</p>

<p>Server 2:</p>

<p>IP1= 8.8.8.3 </p>

<p>i want to redirect every access to IP 8.8.8.2 to 8.8.8.3. Therefor I activated IP forward</p>

<pre><code>sysctl net.ipv4.ip_forward=1
</code></pre>

<p>and installed the IP Tables:</p>

<pre><code>iptables -t nat -A  PREROUTING -d 8.8.8.2 -j DNAT --to-destination 8.8.8.3
iptables -t nat -A POSTROUTING -s 8.8.8.3 -j SNAT --to-source 8.8.8.2
</code></pre>

<p>My Problem is that this doesnt work. If i ping now my IP address 8.8.8.2 I expect an answer from 8.8.8.3 instead i get an timeout.</p>

<p>What am I doing wrong?
Thanks </p>

<p>EDIT:</p>

<p>The servers are connected via Internet, so all IPs are public IPs. </p>

<p>The IP of Server 2 changes every 2-3 Months due to regulations of the ISP. So i want to use the static IP of Server 1 for Server 2 . </p>

<p>The subnets are /32. So I only own this three specific IP addresses. </p>

<p>Server 1 is a Strato Vserver. Server 2 is a Vserver Host at my home.</p>

<p>EDIT 2:
 Tunnel Would be an good option, but does this work with IP Tables?</p>

<p>I dont exactly know what double nat could help there, since the Second Server is acessible from the internet an there is no need for nat trough a router. ( this is the place where i know you can use double nat)</p>
","<linux><ubuntu><iptables><nat><ipv4>","2018-05-09 13:49:32"
"911512","Delete files after x number of days and ignore one subfolder","<p>While using another powershell script that I found online works it is deleting the directories instead of the files inside of the directories. The whole idea is to look in certain folders while ignoring one folder and delete files older than 365 days while leaving the subfolders. Here is the script that I have tried to modify:</p>

<pre><code># Variables 
$dump_path = ""C:\DoceServe""    # set folder path this would be changed to a network drive
$Ignore_Path = ""C:\DoceServe\System"" # I want to ignore the system folder on the network drive
$max_days = ""-365""    # set min age of files
$curr_date = Get-Date    # get the current date
$del_date = $curr_date.AddDays($max_days)   # determine how far back we go based on current date
$includefiles = ""*.txt, *.bak, *.csv, *.prn, *.PDF, *.S@#""  # determine what to include in the delete
$excludefiles = ""DoceServe Info* , DoceServe Log*""   # Determine what to exclude from the delete  -Exclude $exclude

# delete the files
Get-ChildItem $dump_path -Exclude $excludefiles -Include $includedfiles  | Where-Object { $_.LastWriteTime -lt $del_date } | Remove-Item -Recurse -Force -WhatIf
</code></pre>

<p>Any help would be appreciated as I am new with using powershell and some of it is kind of confusing.</p>
","<powershell>","2018-05-09 17:55:50"
"842823","How to resurrect an AWS cluster using command line tools that I cannot ssh into?","<p>About two weeks ago I was suddenly unable to <code>ssh</code> into my AWS machines, they just time out.  Really, the best information I can get from an <code>ssh -vvv &lt;ip&gt;</code> is <code>ssh: connect to host &lt;ip&gt; port 22: Connection refused</code>.  if the ip address has changed no one told me about it.</p>

<p>Since I still have the information about the machines from when I set them up, I'm wondering if there's a way to resurrect an AWS cluster using command line tools.</p>

<p>Here is all of the information I have about the machines in the cluster:</p>

<pre><code>- instance IDs
- region
- groupName
- VpcId
- SubnetId
- SubnetId
- public IP addresses
</code></pre>

<p>Using the AWS CLI, is there a way to resurrect these instances?  </p>

<p><em>EDIT</em> I'm pretty sure the instances are <code>stopped</code>, judging by this:  When I do <code>aws describe-instances --region &lt;region&gt; --instance-ids &lt;id&gt;</code> I see <code>""State"" { ""Code"": 80, ""Name"": ""stopped""}</code>.</p>
","<ssh><amazon-ec2><amazon-web-services><aws-cli>","2017-04-05 17:28:17"
"843011","adding txt record for domain verification","<p>I would like to verify ownership of my domain by adding TXT record.
For that i have two set of values 
1)TXT Record Name: _acme-challenge.cooloffers.in
Value: M6ozS9PeCBZ-Q1sw4mBuJ3tsbuLQTMAkISHF--noe0k</p>

<p>TXT Record Name: _acme-challenge.www.cooloffers.in
Value: 7jeD7BMCmB5ksXIi7QYmp3gC6lGv-_E1s-ZW2mUNjKQ</p>

<p>I just have to add those values from DNS Zone, then it will return via the
function dns_get_record(""cooloffers.in""); ?</p>

<p>How we can verify after adding those pair of values?</p>
","<dns-zone>","2017-04-06 12:55:52"
"843031","gcc: /usr/lib/libxml2.a: No such file or directory","<p>I have installed the following pre-built rpm on RHEL6 32bit: </p>

<pre><code>libxml2-2.7.6-21.el6_8.1.i686,
libxml2-devel-2.7.6-21.el6_8.1.i686,
libxml2-python-2.7.6-21.el6_8.1.i686
</code></pre>

<p>How can I resolve this issue? </p>

<pre><code>gcc: /usr/lib/libxml2.a: No such file or directory
</code></pre>
","<rhel6><gcc><library>","2017-04-06 13:54:22"
"843032","Unable to delete a folder from server 2012 R2","<p>I had a replication enable a folder which was replicating data to a server 2012 r2 but i have disbale the replication but i'm unable to delete this folder or re-enable the replication.</p>

<p>We had a probelm the dfs replication on a folder which we disble it to remove any data from other site but now when I re-enable to dfs i get error ""server(folder): Security cannot be set on the replicated folder. access is denined""</p>

<p>When I try to delete the folder from server 2012 R2 i get ""You required permission from admin to mkae changes to this folder""</p>

<p>Please find the images</p>

<p><a href=""https://1drv.ms/f/s!At0UGnArT7e5nDVt3MPjYJvv3dZ1"" rel=""nofollow noreferrer"">https://1drv.ms/f/s!At0UGnArT7e5nDVt3MPjYJvv3dZ1</a></p>

<p>UPDATE:
After delete the membership i'm now able to delete the data but i got new problem now. I cant delete the connection. When I delete the connection i get following error ""Server to mainserver: The dfs replication connection object cannot be deleted. the user has insuffuient access reights.""</p>
","<windows-server-2012-r2><dfs>","2017-04-06 13:55:48"
"843066","Apache 2.2 on linux too slow when showing 2,50,000 files from one directory","<p>Using the web browser, I have a requirement to access a directory on a Linux server hosting around 2,00,000 files in it. </p>

<p>I am using the 'Alias' directive in apache to achieve this requirement. Please see below - </p>

<pre><code>Alias /barcodes/ ""/m01/apps/codes/barcodes/""

&lt;Directory ""/m01/apps/codes/barcodes/""&gt;
Options +Indexes
IndexOptions +TrackModified
AllowOverride None
Order allow,deny
Allow from all
#Doing IndexOrderDefault so to see the files in a descending order (by date/timestamp)
IndexOrderDefault Descending Date
&lt;/Directory&gt;
</code></pre>

<p>Issue - The web browser takes a lot of time in displaying up the files from the directory and becomes too slow to access.</p>

<p>Appreciate if someone can help in achieving this requirement.</p>

<p>Thanks.. </p>
","<apache-2.2><mod-cache><mod-alias><directoryindex>","2017-04-06 15:40:27"
"911912","Yum Install Packages Behind Firewall","<p>I have a CentOS 7 server (server X)in a datacenter that has a firewall that I couldn't do yum:</p>

<pre><code># yum search htop
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
Could not retrieve mirrorlist http://mirrorlist.centos.org/?release=7&amp;arch=x86_64&amp;repo=os&amp;infra=stock error was
14: curl#6 - ""Could not resolve host: mirrorlist.centos.org; Unknown error""
Could not retrieve mirrorlist http://mirrorlist.centos.org/?release=7&amp;arch=x86_64&amp;repo=extras&amp;infra=stock error was
14: curl#6 - ""Could not resolve host: mirrorlist.centos.org; Unknown error""
Could not retrieve mirrorlist http://mirrorlist.centos.org/?release=7&amp;arch=x86_64&amp;repo=updates&amp;infra=stock error was
14: curl#6 - ""Could not resolve host: mirrorlist.centos.org; Unknown error""
 * base: mirrors.163.com
 * extras: mirrors.shu.edu.cn
 * updates: ftp.sjtu.edu.cn
Warning: No matches found for: htop
No matches found
</code></pre>

<p>And the datacenter doesn't offer a proxy server for me to yum install anything.</p>

<p>However, I can ssh to server X from my home computer over Internet + VPN.</p>

<p>I just wonder whether there is a way to utilize the ssh connection to yum install packages (or any other ways to yum install packages).</p>

<p><a href=""https://serverfault.com/questions/66077/install-apache-on-redhat-without-internet-connection"">This post</a> suggested that people just install with RPMs or DVD.</p>

<p>I think installing from RPMs are simple if there isn't any dependency but it will be troublesome to deal with dependency.</p>

<p>Any suggestion is welcome.</p>

<p>Thanks in advance!</p>

<p>Update 1:
I couldn't even ping the IP of mirrorlist.centos.org (although it is pingable):</p>

<pre><code># ping 85.236.43.108
PING 85.236.43.108 (85.236.43.108) 56(84) bytes of data.
^C
--- 85.236.43.108 ping statistics ---
3 packets transmitted, 0 received, 100% packet loss, time 1999ms
</code></pre>

<p>So the DNS resolution isn't the root cause.</p>

<p>Traceroute indicates the traffic is blocked:</p>

<pre><code># traceroute 85.236.43.108
traceroute to 85.236.43.108 (85.236.43.108), 30 hops max, 60 byte packets
 1  gateway (172.18.22.254)  3.884 ms  4.100 ms  4.322 ms
 2  gateway (172.18.22.254)  3.200 ms !H * *
</code></pre>

<p>It couldn't go out because the datacenter has a firewall.  Server X itself has no firewall nor iptable to prevent going out.</p>

<p>Update 2: downloading rpms and rsync-ing and then installing failed</p>

<pre><code># yum localinstall /tmp/audit-libs-2.8.1-3.el7.x86_64.rpm --disablerepo=*
Loaded plugins: fastestmirror
Examining /tmp/audit-libs-2.8.1-3.el7.x86_64.rpm: audit-libs-2.8.1-3.el7.x86_64
Marking /tmp/audit-libs-2.8.1-3.el7.x86_64.rpm as an update to audit-libs-2.7.6-3.el7.x86_64
Resolving Dependencies
--&gt; Running transaction check
---&gt; Package audit-libs.x86_64 0:2.7.6-3.el7 will be updated
--&gt; Processing Dependency: audit-libs(x86-64) = 2.7.6-3.el7 for package: audit-2.7.6-3.el7.x86_64
Loading mirror speeds from cached hostfile
---&gt; Package audit-libs.x86_64 0:2.8.1-3.el7 will be an update
--&gt; Finished Dependency Resolution
Error: Package: audit-2.7.6-3.el7.x86_64 (@anaconda)
           Requires: audit-libs(x86-64) = 2.7.6-3.el7
           Removing: audit-libs-2.7.6-3.el7.x86_64 (@anaconda)
               audit-libs(x86-64) = 2.7.6-3.el7
           Updated By: audit-libs-2.8.1-3.el7.x86_64 (/audit-libs-2.8.1-3.el7.x86_64)
               audit-libs(x86-64) = 2.8.1-3.el7
 You could try using --skip-broken to work around the problem
 You could try running: rpm -Va --nofiles --nodigest
</code></pre>
","<firewall><proxy><centos7><yum>","2018-05-12 14:23:43"
"911955","ASA can ping but not computers","<p>I just setup a new ASA 5506-X. From the ASA I can ping outside and inside. However inside computers can't ping outside ip, for example  8.8.8.8. Tracert 8.8.8.8 are all timeout. Here are the configuration:</p>

<pre><code>: Hardware:   ASA5506, 4096 MB RAM, CPU Atom C2000 series 1250 MHz, 1 CPU (4 cores)
:
ASA Version 9.8(2) 
!
hostname ciscoasa
enable password $sha512$5000$CQmyTVA8Hnz5EPKvxkmsTQ==$olS735SaLOSZR/N052FWAQ== pbkdf2
names

!
interface GigabitEthernet1/1
 nameif outside
 security-level 0
 ip address dhcp setroute 
!
interface GigabitEthernet1/2
 nameif inside
 security-level 100
 ip address 192.168.1.1 255.255.255.0 
!
interface GigabitEthernet1/3
 shutdown
&lt;--- More ---&gt;

 no nameif
 no security-level
 no ip address
!
interface GigabitEthernet1/4
 shutdown
 no nameif
 no security-level
 no ip address
!
interface GigabitEthernet1/5
 shutdown
 no nameif
 no security-level
 no ip address
!
interface GigabitEthernet1/6
 shutdown
 no nameif
 no security-level
 no ip address
!
interface GigabitEthernet1/7
 shutdown
&lt;--- More ---&gt;

 no nameif
 no security-level
 no ip address
!
interface GigabitEthernet1/8
 shutdown
 no nameif
 no security-level
 no ip address
!
interface Management1/1
 management-only
 no nameif
 no security-level
 no ip address
!
ftp mode passive
object network obj_any
 subnet 0.0.0.0 0.0.0.0
pager lines 24
logging asdm informational
mtu outside 1500
mtu inside 1500
no failover
&lt;--- More ---&gt;

no monitor-interface service-module 
icmp unreachable rate-limit 1 burst-size 1
no asdm history enable
arp timeout 14400
no arp permit-nonconnected
arp rate-limit 16384
!
object network obj_any
 nat (any,outside) static interface
!
nat (inside,outside) after-auto source dynamic any interface
route outside 0.0.0.0 0.0.0.0 192.168.11.254 1
timeout xlate 3:00:00
timeout pat-xlate 0:00:30
timeout conn 1:00:00 half-closed 0:10:00 udp 0:02:00 sctp 0:02:00 icmp 0:00:02
timeout sunrpc 0:10:00 h323 0:05:00 h225 1:00:00 mgcp 0:05:00 mgcp-pat 0:05:00
timeout sip 0:30:00 sip_media 0:02:00 sip-invite 0:03:00 sip-disconnect 0:02:00
timeout sip-provisional-media 0:02:00 uauth 0:05:00 absolute
timeout tcp-proxy-reassembly 0:01:00
timeout floating-conn 0:00:00
timeout conn-holddown 0:00:15
timeout igp stale-route 0:01:10
user-identity default-domain LOCAL
aaa authentication login-history
&lt;--- More ---&gt;

http server enable
http 192.168.1.0 255.255.255.0 inside
no snmp-server location
no snmp-server contact
service sw-reset-button
crypto ipsec security-association pmtu-aging infinite
crypto ca trustpool policy
telnet timeout 5
ssh stricthostkeycheck
ssh timeout 5
ssh key-exchange group dh-group1-sha1
console timeout 0

dhcpd auto_config outside
!
dhcpd address 192.168.1.5-192.168.1.99 inside
dhcpd dns 8.8.8.8 interface inside
dhcpd enable inside
!
threat-detection basic-threat
threat-detection statistics access-list
no threat-detection statistics tcp-intercept
dynamic-access-policy-record DfltAccessPolicy
!
&lt;--- More ---&gt;

class-map inspection_default
 match default-inspection-traffic
!
!
policy-map type inspect dns preset_dns_map
 parameters
  message-length maximum client auto
  message-length maximum 512
  no tcp-inspection
policy-map global_policy
 class inspection_default
  inspect dns preset_dns_map 
  inspect ftp 
  inspect h323 h225 
  inspect h323 ras 
  inspect rsh 
  inspect rtsp 
  inspect esmtp 
  inspect sqlnet 
  inspect skinny  
  inspect sunrpc 
  inspect xdmcp 
  inspect sip  
  inspect netbios 
&lt;--- More ---&gt;

  inspect tftp 
  inspect ip-options 
!
service-policy global_policy global
prompt hostname context 
no call-home reporting anonymous
Cryptochecksum:9c2cff01a07174783f08cca102d29eab
: end
[OK]
</code></pre>
","<cisco>","2018-05-12 22:41:30"
"773297","(Ubuntu Server) Why does /usr/bin/node --version show v6.0.0 when /usr/bin/nodejs --version show v4.3.3?","<p>I recently had nodejs v6.0.0 installed.  I ran
    <code>sudo apt-get remove nodejs</code>
and reinstalled the LTS version using
    <code>curl -sL https://deb.nodesource.com/setup_4.x | sudo -E bash -
    sudo apt-get install -y nodejs</code></p>

<p>After reinstalling I ran
    <code>sudo rm /usr/bin/node</code></p>

<p>And re-created the symbolic link using
    <code>sudo ln -sv /usr/bin/nodejs /usr/bin/node</code></p>

<p>Now running either with the --version parameter shows different versions...
Thanks ahead of time for any assistance!</p>
","<linux><ubuntu><ubuntu-14.04><node.js>","2016-04-27 14:28:23"
"773316","/30 IP block 4 - 3 useable IP","<p>I have 3 useable IP's?</p>

<p>This is what the ISP supplied. when I requested a IP block 4</p>

<ul>
<li>Gateway .232</li>
<li>Router .233</li>
<li>Spare IP .234</li>
<li>Broadcast .235</li>
</ul>

<p>How it is setup in the router.</p>

<p>Router picks up .233 as main IP. (has DCHP on currently)</p>

<p>.232 &amp; .234 added as wan ip alias </p>

<p>They all can be pinged externally. I even did a test and swapped about port 80 between IP's to check you can access the same webpage from a external source.</p>

<p><img src=""https://i.sstatic.net/m0J8p.png"" alt=""Image to IP setup""></p>

<p>From my understanding.</p>

<p>.233 is the IP the router should pick up from the ISP.
.234 is the spare IP Which I use for a SSL cert for our web application.</p>

<p>Hence as advertised by the ISP you only get 2 useable IP's.</p>

<p>Any advise on this.</p>
","<wide-area-network><draytek>","2016-04-27 15:09:07"
"773330","Win 2012 server will not resolve domains using web browser, PING or NSLOOKUP, yet services on server are working","<p>This is a windows 2012 server running SQL 2014. </p>

<p>The server does not seem to be able to resolve IP address when given a domain name. </p>

<p>The problem first showed itself when I tried to go to a web page to download a client we use to manage our servers. The web browser failed to resolve the address and then tried to go to BING and perform a search. It then failed to resolve the address for BING and returned the following error message: </p>

<blockquote>
  <p>""Unable to open search page"".  </p>
</blockquote>

<p>Inputting any domain name results in the same error message. I realize that it is telling me it can't find BING, but it seems to point to not being able to resolve any DNS queries. I get similar results when trying to PING a URL. </p>

<p>PING always returns a NO RESPONSE FROM SERVER, even using addresses like www.google.com. Where as my laptop computer will at least resolve www.google.com to an IP address. </p>

<p>NSLOOKUP also fails to resolve domains to IP address. I always get the generic message ""No response from server"" All the other servers on our windows domain are resolving domains to IPs just fine. And our back software, MozyPro, is able to backups without any problems. My team and I can connect to the SQL instance running on this server just fine. I've even tried turning off the firewall, but that had no effect.</p>
","<networking><domain-name-system><windows-server-2012-r2><local-area-network>","2016-04-27 15:44:01"
"843269","User sync'd with Mail-User contact not Mailbox using Azure AD Connect","<p>When using AD Connect to sync on-prem AD with Azure AD for Office 365, I've got an issue where when one user syncs they're being setup as a mail-user and thus created as a contact, not a mailbox.</p>

<p>There is a bit of history, as this user left and had their mail forwarded probably as a mail contact when their was a hybrid/transitional setup.  So, I'm wondering if their is something still in Active Directory that thinks this user is still a contact.</p>

<p>Any suggestions would be great.</p>

<p>Thanks</p>
","<active-directory><exchange><azure><microsoft-office-365>","2017-04-07 14:05:26"
"773380","Script to delete older files and copy what is left","<p>We have a server that runs an application that is used internally.  The application has a utility that creates its own backups every 24 hours.</p>

<p>Backups are stored in: /var/application/application-data/exports</p>

<p>We have a NFS mount under /mnt/AppDataBkp</p>

<p>I would like to set up a bash script to do the following:</p>

<ol>
<li><p>On a schedule (cron, every 24hrs), it will look for any .zip files in var/application/application-data/exports older than 7 days and remove them.</p></li>
<li><p>Then, it will copy whatever it is left in that directory to /mnt/AppDataBkp</p></li>
<li><p>Optionally, it would be nice if it could also remove any files/backups from /mnt/AppDataBkp older than 7 days.</p></li>
</ol>

<p>So far, I've come up with this which is supposed to find files older than 7 days and delete them:</p>

<pre><code>#!/bin/bash

find var/application/application-data/exports -name ""*.zip"" -type f -mtime +7 -exec rm {} \;
rsync -rlptgoD /var/application/application-data/exports /mnt/AppDataBkp &amp;&amp;
find /mnt/AppDataBkp -name ""*.zip"" -type f -mtime +7 -exec rm {} \;
</code></pre>

<p>Any help would be much appreciated.</p>
","<linux><centos><bash><scripting><rsync>","2016-04-27 19:00:14"
"773436","How can I add firewall rules remotely without GPO?","<p>I know the best way to do this is through group policy, however that's not an option right now due to our really old server running Win Server SB 2003 (we're planning on decommissioning it asap, but that'll still be several months from now.)</p>

<p>Anyway, I've tried looking for .reg files to script the new firewall rules, but they don't seem to do anything. I found registry entries under HKLM\SYSTEM\CurrentControlSet\services\SharedAccess\Parameters\FirewallPolicy\FirewallRules that seem to be what I'm after, but when I execute that reg file the rules are not actually added anywhere. I've found a way to do this with Powershell as well, but unfortunately, one of the rules I need to enable is Windows Remote Management, which is what allows PS to run on the remote machines...</p>

<p>I just found a psexec command to do this, but that doesn't seem to be working either... I need to do this to 30-40 computers so I really don't want to do each one manually. Is there any other way you guys have used in the past or anything to try at all now?</p>
","<windows-server-2003><powershell><remote>","2016-04-27 22:58:17"
"773510","Suddenly, apache stops workings","<p>my apache server stops working and I don't know how to solve it. I have two wordpress and a redmine running on that server. After few hours it stops. I can ping the server, but when I try to get the web with a browser it doesn't respond. If I restart apache (""service apache2 restart"") it works properly again. </p>

<p>My apache version: </p>

<pre><code>Server version: Apache/2.4.10 (Debian)
Server built:   Aug 31 2015 00:01:48
Server's Module Magic Number: 20120211:37
Server loaded:  APR 1.5.1, APR-UTIL 1.5.4
Compiled using: APR 1.5.1, APR-UTIL 1.5.4
Architecture:   32-bit
</code></pre>

<p>There's no info on /var/log/apache2/acces.log or /var/log/apache2/error.log</p>

<p>Apache is running a lot of process and maybe the server is running out of memory.</p>

<p>When I run 'service apache2 status' I get this:</p>

<pre><code>● apache2.service - LSB: Apache2 web server
   Loaded: loaded (/etc/init.d/apache2)

   Active: active (running) since Thu 2016-04-28 06:05:10 UTC; 1h 51min ago

  Process: 20345 ExecStop=/etc/init.d/apache2 stop (code=exited, status=0/SUCCESS)

  Process: 21763 ExecReload=/etc/init.d/apache2 reload (code=exited, status=0/SUCCESS)

  Process: 20371 ExecStart=/etc/init.d/apache2 start (code=exited, status=0/SUCCESS)

   CGroup: /system.slice/apache2.service

           ├─20406 /usr/sbin/apache2 -k start
           ├─20428 /usr/sbin/apache2 -k start
           ├─20429 /usr/sbin/apache2 -k start
           ├─20430 /usr/sbin/apache2 -k start
           ├─20431 /usr/sbin/apache2 -k start
           ├─20432 /usr/sbin/apache2 -k start
           ├─20436 /usr/sbin/apache2 -k start
           ├─20437 /usr/sbin/apache2 -k start
           ├─20438 /usr/sbin/apache2 -k start
           ├─20439 /usr/sbin/apache2 -k start
           ├─20440 /usr/sbin/apache2 -k start
           ├─20441 /usr/sbin/apache2 -k start
           ├─20442 /usr/sbin/apache2 -k start
           ├─20451 /usr/sbin/apache2 -k start
           ├─20465 /usr/sbin/apache2 -k start
           ├─20466 /usr/sbin/apache2 -k start
           ├─20467 /usr/sbin/apache2 -k start
           ├─20471 /usr/sbin/apache2 -k start
           ├─20474 /usr/sbin/apache2 -k start
           ├─20476 /usr/sbin/apache2 -k start
           ├─20478 /usr/sbin/apache2 -k start
           ├─20482 /usr/sbin/apache2 -k start
           ├─20485 /usr/sbin/apache2 -k start
           ├─20486 /usr/sbin/apache2 -k start
           ├─20490 /usr/sbin/apache2 -k start
           ├─20498 /usr/sbin/apache2 -k start
           ├─20505 /usr/sbin/apache2 -k start
           ├─20506 /usr/sbin/apache2 -k start
           ├─20507 /usr/sbin/apache2 -k start
           ├─20520 /usr/sbin/apache2 -k start
           ├─20521 /usr/sbin/apache2 -k start
           ├─20527 /usr/sbin/apache2 -k start
           ├─20528 /usr/sbin/apache2 -k start
           ├─20530 /usr/sbin/apache2 -k start
           ├─20545 /usr/sbin/apache2 -k start
           ├─20558 /usr/sbin/apache2 -k start
           ├─20561 /usr/sbin/apache2 -k start
           ├─20566 /usr/sbin/apache2 -k start
           ├─20568 /usr/sbin/apache2 -k start
           ├─20569 /usr/sbin/apache2 -k start
           ├─20577 /usr/sbin/apache2 -k start
           ├─20592 /usr/sbin/apache2 -k start
           ├─20598 /usr/sbin/apache2 -k start
           ├─20600 /usr/sbin/apache2 -k start
           ├─20615 /usr/sbin/apache2 -k start
           ├─20618 /usr/sbin/apache2 -k start
           ├─20621 /usr/sbin/apache2 -k start
           ├─20623 /usr/sbin/apache2 -k start
           ├─20636 /usr/sbin/apache2 -k start
           ├─20638 /usr/sbin/apache2 -k start
           ├─20653 /usr/sbin/apache2 -k start
           ├─20655 /usr/sbin/apache2 -k start
           ├─20656 /usr/sbin/apache2 -k start
           ├─20674 /usr/sbin/apache2 -k start
           ├─20675 /usr/sbin/apache2 -k start
           ├─20679 /usr/sbin/apache2 -k start
           ├─20680 /usr/sbin/apache2 -k start
           ├─20725 /usr/sbin/apache2 -k start
           ├─20729 /usr/sbin/apache2 -k start
           ├─20731 /usr/sbin/apache2 -k start
           ├─20732 /usr/sbin/apache2 -k start
           ├─20756 /usr/sbin/apache2 -k start
           ├─20762 /usr/sbin/apache2 -k start
           ├─20763 /usr/sbin/apache2 -k start
           ├─20781 /usr/sbin/apache2 -k start
           ├─20785 /usr/sbin/apache2 -k start
           ├─20790 /usr/sbin/apache2 -k start
           ├─20792 /usr/sbin/apache2 -k start
           ├─20800 /usr/sbin/apache2 -k start
           ├─20809 /usr/sbin/apache2 -k start
           ├─20810 /usr/sbin/apache2 -k start
           ├─20811 /usr/sbin/apache2 -k start
           ├─20823 /usr/sbin/apache2 -k start
           ├─20834 /usr/sbin/apache2 -k start
           ├─20837 /usr/sbin/apache2 -k start
           ├─20838 /usr/sbin/apache2 -k start
           ├─20839 /usr/sbin/apache2 -k start
           ├─20846 /usr/sbin/apache2 -k start
           ├─20864 /usr/sbin/apache2 -k start
           ├─20866 /usr/sbin/apache2 -k start
           ├─20867 /usr/sbin/apache2 -k start
           ├─20872 /usr/sbin/apache2 -k start
           ├─20893 /usr/sbin/apache2 -k start
           ├─20894 /usr/sbin/apache2 -k start
           ├─20900 /usr/sbin/apache2 -k start
           ├─20911 /usr/sbin/apache2 -k start
           ├─20914 /usr/sbin/apache2 -k start
           ├─20915 /usr/sbin/apache2 -k start
           ├─20916 /usr/sbin/apache2 -k start
           ├─20918 /usr/sbin/apache2 -k start
           ├─20936 /usr/sbin/apache2 -k start
           ├─20942 /usr/sbin/apache2 -k start
           ├─20951 /usr/sbin/apache2 -k start
           ├─20954 /usr/sbin/apache2 -k start
           ├─20955 /usr/sbin/apache2 -k start
           ├─20957 /usr/sbin/apache2 -k start
           ├─20958 /usr/sbin/apache2 -k start
           ├─20961 /usr/sbin/apache2 -k start
           ├─20965 /usr/sbin/apache2 -k start
           ├─20968 /usr/sbin/apache2 -k start
           ├─20974 /usr/sbin/apache2 -k start
           ├─20977 /usr/sbin/apache2 -k start
           ├─20978 /usr/sbin/apache2 -k start
           ├─20991 /usr/sbin/apache2 -k start
           ├─20996 /usr/sbin/apache2 -k start
           ├─21002 /usr/sbin/apache2 -k start
           ├─21003 /usr/sbin/apache2 -k start
           ├─21006 /usr/sbin/apache2 -k start
           ├─21015 /usr/sbin/apache2 -k start
           ├─21021 /usr/sbin/apache2 -k start
           ├─21031 /usr/sbin/apache2 -k start
           ├─21045 /usr/sbin/apache2 -k start
           ├─21057 /usr/sbin/apache2 -k start
           ├─21063 /usr/sbin/apache2 -k start
           ├─21066 /usr/sbin/apache2 -k start
           ├─21067 /usr/sbin/apache2 -k start
           ├─21078 /usr/sbin/apache2 -k start
           ├─21079 /usr/sbin/apache2 -k start
           ├─21087 /usr/sbin/apache2 -k start
           ├─21090 /usr/sbin/apache2 -k start
           ├─21091 /usr/sbin/apache2 -k start
           ├─21120 /usr/sbin/apache2 -k start
           ├─21128 /usr/sbin/apache2 -k start
           ├─21129 /usr/sbin/apache2 -k start
           ├─21130 /usr/sbin/apache2 -k start
           ├─21138 /usr/sbin/apache2 -k start
           ├─21145 /usr/sbin/apache2 -k start
           ├─21147 /usr/sbin/apache2 -k start
           ├─21148 /usr/sbin/apache2 -k start
           ├─21149 /usr/sbin/apache2 -k start
           ├─21151 /usr/sbin/apache2 -k start
           ├─21154 /usr/sbin/apache2 -k start
           ├─21164 /usr/sbin/apache2 -k start
           ├─21165 /usr/sbin/apache2 -k start
           ├─21171 /usr/sbin/apache2 -k start
           ├─21176 /usr/sbin/apache2 -k start
           ├─21178 /usr/sbin/apache2 -k start
           ├─21790 PassengerWatchdog
           ├─21794 PassengerHelperAgent
           ├─21801 PassengerLoggingAgent
           ├─21902 /usr/sbin/apache2 -k start
           ├─21932 /usr/sbin/apache2 -k start
           ├─21986 /usr/sbin/apache2 -k start
           ├─22019 /usr/sbin/apache2 -k start
           ├─22030 /usr/sbin/apache2 -k start
           ├─22050 /usr/sbin/apache2 -k start
           ├─22071 /usr/sbin/apache2 -k start
           ├─22072 /usr/sbin/apache2 -k start
           ├─22085 /usr/sbin/apache2 -k start
           ├─22107 /usr/sbin/apache2 -k start
           ├─22126 /usr/sbin/apache2 -k start
           ├─22149 /usr/sbin/apache2 -k start
           ├─22331 /usr/sbin/apache2 -k start
           └─22527 /usr/sbin/apache2 -k start

Apr 28 06:05:09 onyserver1 apache2[20371]: Starting web server: apache2AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 10.1.46.47. Set the 'ServerName' directive gl
obally to suppress this message

Apr 28 06:05:10 onyserver1 apache2[20371]: .

Apr 28 06:05:10 onyserver1 systemd[1]: Started LSB: Apache2 web server.

Apr 28 06:25:07 onyserver1 systemd[1]: Reloading LSB: Apache2 web server.

Apr 28 06:25:07 onyserver1 apache2[21763]: Reloading web server: apache2.

Apr 28 06:25:07 onyserver1 systemd[1]: Reloaded LSB: Apache2 web server.
</code></pre>

<p>My apache configurations:</p>

<p>wordpress:</p>

<pre><code>&lt;VirtualHost *:80&gt;
    ServerAdmin xxx@xxx.com
    ServerName blog.xxx.com

    DocumentRoot /var/www/xxx
    &lt;Directory /&gt;
        Options FollowSymLinks
        AllowOverride All
    &lt;/Directory&gt;
    &lt;Directory /var/www/xxx&gt;
        Options Indexes FollowSymLinks MultiViews
        AllowOverride All
        Order allow,deny
        allow from all
    &lt;/Directory&gt;
&lt;/VirtualHost&gt;
</code></pre>

<p>Redmine:</p>

<pre><code>&lt;VirtualHost *:80&gt;
    ServerName redmine.xxx.com 
    # this is the passenger config
    RailsEnv production
    SetEnv X_DEBIAN_SITEID ""default""
    PassengerDefaultUser www-data
    Alias ""/plugin_assets/"" /var/cache/redmine/default/plugin_assets/
    DocumentRoot /usr/share/redmine/public
    &lt;Directory ""/usr/share/redmine/public""&gt;
    Allow from all
    Options -MultiViews
    Require all granted
    &lt;/Directory&gt;
&lt;/VirtualHost&gt;
</code></pre>

<p>Any suggestion?</p>

<p>Thanks in advance.</p>
","<apache-2.4><wordpress><redmine>","2016-04-28 08:30:38"
"843486","Nginx + PHP7.0-fpm – PHP errors #500 go into the status code header and into the browser console. Error log is not written","<p>I am using Nginx + php7.0-fpm. Some of my errors (500 code) go into the status code header. In the browser console I see, for example, such a message:</p>

<blockquote>
  <p>POST <a href=""https://example.com"" rel=""nofollow noreferrer"">https://example.com</a> 500 (DateTime::__construct(): Failed to parse time string (2017-04-09 :) at position 11 (:): Unexpected character)</p>
</blockquote>

<p>I tried disabling error reporting, <code>display_errors = off</code> does not work as well – errors go to the header, and they are never logged. The nginx error log is clear. But not all errors are risen like that – most of them are handled correctly. I have the <code>fastcgi_intercept_errors</code> directive enabled in the <code>nginx.conf</code>.</p>

<p>When I switch back to php5, I do not see such behaviour. The problem is only when I use php7.0. Can't figure out how to make errors go into the log instead of the browser console.</p>
","<nginx><php-fpm><500-error><php7>","2017-04-09 04:07:46"
"843679","Send email to PostFix on LAN","<p>Hi I'd like to have an Exchange server on our network forward mail to a mail server on a local Linux box which uses Postfix/Dovecot. </p>

<p>To give some context, there is an Exchange server on premise that we would rather avoid interacting with directly as we don't manage it. One email account receives emails, which we would like to download and process into the system we manage. So we thought it would be a low impact solution to set up a Linux box locally and have the mail forwarded to that. We've got a basic thing set up but we're unsure how one forwards locally on a LAN. Plus I thought it would be fun to tinker with Linux as I'm a developer. If this is a terrible idea by all means say!</p>

<p>I'm a newbie to this so I followed this tutorial</p>

<p><a href=""https://www.linux.com/learn/how-build-email-server-ubuntu-linux"" rel=""nofollow noreferrer"">https://www.linux.com/learn/how-build-email-server-ubuntu-linux</a></p>

<p>Which ""works"" - in that I used telnet to send an email to ubuntu_user@serverhostname and I have set up Thunderbird to read the mail.</p>

<p>So next question would be what address would the Exchange server need to use to send the mail? And can anyone recommend a good way to test sending mail locally. I'm a little unsure how a mail client like Outlook (using e.g. Office 365) or Exchange - will interpret a local email address. Cheers, Chris.</p>
","<postfix><exchange><dovecot>","2017-04-10 14:34:38"
"843716","mongod rs.initiate() error msg","<p>I am a newbie so please bear with me.<br>
I have used this command <code>$ sudo mongod --config /etc/mongod2.conf</code>in terminal to start a mongodb service instance.  In another terminal I ran <code>$ mongo --port XXXX</code> where XXXX is the port number I configure in <em><strong>mongod.conf</strong></em> file.  <strong>rs.initiate()</strong> in mongo shell is giving the error  <code>""errmsg"" : ""assertion src/mongo/db/repl/replset_commands.cpp:275"",
    ""code"" : 8</code>
why ?</p>

<p>edit:
<code>**mongodb version v3.2.12**</code><br>
<em>mongod2.conf file consist of:</em> </p>

<p><code># for documentation of all options, see:</code><br>
<code># http://docs.mongodb.org/manual/reference/configuration-options/</code><br>
<code># Where and how to store data.</code><br>
<code>storage:
  dbPath: /var/lib/mongodb2
  journal:
    enabled: true</code><br>
<code>#  engine:</code><br>
<code>#  mmapv1:</code><br>
<code>#  wiredTiger:</code><br>
<code># where to write logging data.</code><br>
<code>systemLog:
  destination: file
  logAppend: true
  path: /var/log/mongodb2/mongod.log</code>
<code># network interfaces</code><br>
<code>net:
  port: 27019
  bindIp: 127.0.0.1
</code><br>
<code>processManagement:
  fork: true</code>
<code>#security:</code><br>
<code>#operationProfiling:</code><br>
<code>replication: 
  replSetName: myDevReplSet</code></p>
","<mongodb><replica-set>","2017-04-10 17:59:13"
"843720","Dual-boot EC2 instance","<p>Can I modify an EC2 instance to support dual-/multi-boot (any 2 or more OSes)? </p>

<p>If yes, how? </p>

<p>If no, why not / what needs to change?</p>

<p>Would this be easier on one of the other large IaaS platforms (Azure /GCP)?</p>

<p>Before this gets shut down, I should mention that this is not something that I plan to do. I just want to understand the technical restrictions / challenges.</p>
","<amazon-ec2><boot><cloud>","2017-04-10 18:06:26"
"773863","How is data centres allocating the correct resources?","<p>I've been using Microsoft Azure for a while and I'm a bit curious how the hardware resources in a data centre are allocated according to the selections that you make.</p>

<p>For example if I choose a hardware setup of ""1 core, 1.75 GB RAM"" on my instance, how is that restricted on the server? I mean there is a lot of apps on every server.</p>

<p>Or is every instance / app allocated as a new virtual machine?</p>
","<cloud><resources>","2016-04-28 12:56:32"
"843800","Remove ""Cc: "" from mailx command","<p>I have recently setup a mail server on my Ubuntu Desktop 16.10, and I made a script that when a specific user logs in, they will execute the script instead of bash. Here is the script:</p>

<pre><code>#!/bin/bash
while [ true ]
do
    echo -n ""To: ""
    read To
    echo -n ""Subject: ""
    read Subject
    echo -n ""Attachments [y/N]: ""
    read AttachYN
    if [[ $AttachYN == """" || $AttachYN == ""n"" || $AttachYN == ""N"" ]]
    then
        mail ""$To"" -s ""$Subject""
        exit
    elif [[ $AttachYN == ""y"" || $AttachYN == ""Y""  ]]
    then
        echo -n ""File path: ""
        read File
        mail ""$To"" -s ""$Subject"" -A ""$file""
        exit
    else
        echo ""Incorrect character, must be y, Y, n, N, or blank for default [n]""
    fi
done
</code></pre>

<p>Upon execution, it asks for <code>Cc:</code> input, but I don't want this, as I don't use <code>Cc</code>.</p>

<p>How do I remove this?</p>

<p>Thanks</p>
","<ubuntu><email-server><sendmail><terminal><command>","2017-04-11 05:55:40"
"773971","Unable to connect to an Oracle XE database from Oracle Sql Devloper","<p>I've installed an Oracle 11g XE on a Linux <code>ubuntu 14.04</code>, successfully created a user, and imported a database into it.
my problem is , if when I want create new connection for connect with DATABASE hr for example Bring me a message :</p>

<blockquote>
  <p>Status : Failure -Test failed: IO Error: The Network Adapter could not
  establish the connection</p>
</blockquote>

<p><a href=""https://i.sstatic.net/OFVE0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/OFVE0.png"" alt=""https://i.sstatic.net/OFVE0.png""></a></p>

<p>the result of <code>lsnrctl status</code> is :</p>

<pre><code>LSNRCTL for Linux: Version 11.2.0.2.0 - Production on 29-APR-2016 14:01:13

Copyright (c) 1991, 2011, Oracle.  All rights reserved.

Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=IPC)(KEY=EXTPROC_FOR_XE)))
TNS-12541: TNS:no listener
 TNS-12560: TNS:protocol adapter error
  TNS-00511: No listener
   Linux Error: 111: Connection refused
Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=DrBrm)(PORT=1521)))
TNS-12541: TNS:no listener
 TNS-12560: TNS:protocol adapter error
  TNS-00511: No listener
   Linux Error: 111: Connection refused
</code></pre>
","<oracle><oracle-11g>","2016-04-30 01:20:40"
"843939","Strange ModSecurity entries in Apache error log","<p>I recently migrated my VPS to Plesk Onyx v17 (running on Ubuntu 14.04) and, when checked the error logs this morning, I noticed multiple records like this one:</p>

<pre><code>[Tue Apr 11 06:26:33.063983 2017] [:error] [pid 3306:tid 140450353870592] [client XXX.XXX.XXX.XXX] ModSecurity: Access denied with code 403 (phase 2). Operator EQ matched 0 at REQUEST_HEADERS. [file ""/etc/apache2/modsecurity.d/rules/comodo/12_HTTP_Protocol.conf""] [line ""139""] [id ""217270""] [rev ""2""] [msg ""COMODO WAF: Request Containing Content, but Missing Content-Type header||www.example.com|F|2""] [data ""REQUEST_HEADERS=0""] [severity ""CRITICAL""] [hostname ""www.example.com""] [uri ""/""] [unique_id ""WOyvWX8AAQEAAAzqQuAAAAAA""]*
</code></pre>

<p>Here XXX.XXX.XXX.XXX is the IP address of my VPS. I decided to investigate this error but, when I opened Comodo WAF rules file 12_HTTP_Protocol.conf, I noticed that there is actually no rule there with id ""217270"": after the rule 217261 the next one listed is 217280...?</p>

<p>Looks like I'm stuck here... 
Any thoughts/suggestions? </p>
","<apache-2.4><ubuntu-14.04><mod-security>","2017-04-11 16:10:27"
"843955","Email server doesn't resolve external DNS","<p>The setup: I have a hosting, linked to example1.com. Inside, I have a domain as example2.com with its website. The mailing of example2.com is handled by mx.example3.com (example3 is outside of my server).</p>

<p>The problem: user@example1.com cannot email user@example2.com, because when trying to resolve the DNS never gets outside the server, and I receive an automated email saying that user@example2.com is not found.</p>

<p>My suggestion: It never gets outside of my server because in my server example2.com it is found, but we don't handle the email services, so nothing is found. Emailing from Gmail to example2.com works as expected.</p>

<p>My solution didn't work: I configured the DNS record on my example2.com domain as:
MX example2.com 10 mx.example3.com
I thought that should work but it didn't.</p>

<p>Any idea in mind?</p>

<p><br/>
<strong>SOLUTION</strong><br/>
Even this is not the best, we just deactivated the e-mail service for that domain and now works properly.</p>

<p>Thank you for your ideas.</p>
","<domain-name-system><email><email-server>","2017-04-11 17:22:58"
"844125","Windows 2012r2 w/ sql server 2012 Broken","<p>Sql server 2012 was working yesterday.</p>

<p>But then windows update happened today at 4AM 4/12/2017 and now it seems like the sql server is timing out and such... </p>

<p>In addition none of the SQL integration services projects are running till competition. may have something to do with timeouts. </p>

<p>Anyone else having this issue after windows update? If so is there any way to resolve what the real issue is if no one else is having the issue?</p>

<p>EDIT: 
Rebooted twice and restarted service twice. </p>

<p>EDIT MORE INFO: (UPDATES LISTED)</p>

<p><a href=""https://i.sstatic.net/0VhVA.png"" rel=""nofollow noreferrer"">Windows updates installed</a></p>

<pre><code>    Date,Source,Severity,Message
04/12/2017 10:42:53,spid55,Unknown,External dump process return code 0x20000001.&lt;nl/&gt;External dump process returned no errors.
04/12/2017 10:42:44,spid55,Unknown,Stack Signature for the dump is 0x0000000121D2A808
04/12/2017 10:42:44,spid55,Unknown,* Short Stack Dump
04/12/2017 10:42:44,spid55,Unknown,* -------------------------------------------------------------------------------
04/12/2017 10:42:44,spid55,Unknown,* *******************************************************************************
04/12/2017 10:42:44,spid55,Unknown,*
04/12/2017 10:42:44,spid55,Unknown,*             truncate table tmpSTPOTrack
04/12/2017 10:42:44,spid55,Unknown,* Input Buffer 76 bytes -
04/12/2017 10:42:44,spid55,Unknown,*
04/12/2017 10:42:44,spid55,Unknown,* Latch timeout
04/12/2017 10:42:44,spid55,Unknown,*
04/12/2017 10:42:44,spid55,Unknown,*   04/12/17 10:42:44 spid 55
04/12/2017 10:42:44,spid55,Unknown,* BEGIN STACK DUMP:
04/12/2017 10:42:44,spid55,Unknown,*
04/12/2017 10:42:44,spid55,Unknown,* *******************************************************************************
04/12/2017 10:42:44,spid55,Unknown,***Stack Dump being sent to C:\Program Files\Microsoft SQL Server\MSSQL12.MSSQLSERVER\MSSQL\LOG\SQLDump0019.txt
04/12/2017 10:42:44,spid55,Unknown,**Dump thread - spid = 0&lt;c/&gt; EC = 0x000000086B61EAC0
04/12/2017 10:42:44,spid55,Unknown,Using 'dbghelp.dll' version '4.0.5'
04/12/2017 10:42:43,spid54,Unknown,Timeout occurred while waiting for latch: class 'LOG_MANAGER'&lt;c/&gt; id 000000086FD80B98&lt;c/&gt; type 4&lt;c/&gt; Task 0x000000086D766CA8 : 0&lt;c/&gt; waittime 600 seconds&lt;c/&gt; flags 0x1a&lt;c/&gt; owning task 0x000000086D9EF088. Continuing to wait.
04/12/2017 10:42:43,spid55,Unknown,Timeout occurred while waiting for latch: class 'LOG_MANAGER'&lt;c/&gt; id 000000086FD80B98&lt;c/&gt; type 4&lt;c/&gt; Task 0x000000086D7664E8 : 0&lt;c/&gt; waittime 600 seconds&lt;c/&gt; flags 0x1a&lt;c/&gt; owning task 0x000000086D9EF088. Continuing to wait.
04/12/2017 10:30:41,spid69,Unknown,Autogrow of file 'POSI_PIM5_log' in database 'POSI_PIM5' was cancelled by user or timed out after 3789 milliseconds.  Use ALTER DATABASE to set a smaller FILEGROWTH value for this file or to explicitly set a new file size.
04/12/2017 10:30:37,spid75,Unknown,Autogrow of file 'POSI_PIM5_log' in database 'POSI_PIM5' was cancelled by user or timed out after 30016 milliseconds.  Use ALTER DATABASE to set a smaller FILEGROWTH value for this file or to explicitly set a new file size.
04/12/2017 10:29:45,spid68,Unknown,Unsafe assembly 'microsoft.sqlserver.integrationservices.server&lt;c/&gt; version=12.0.0.0&lt;c/&gt; culture=neutral&lt;c/&gt; publickeytoken=89845dcd8080cc91&lt;c/&gt; processorarchitecture=msil' loaded into appdomain 2 (SSISDB.dbo[runtime].1).
04/12/2017 10:29:45,spid68,Unknown,Unsafe assembly 'microsoft.sqlserver.integrationservices.server&lt;c/&gt; version=12.0.0.0&lt;c/&gt; culture=neutral&lt;c/&gt; publickeytoken=89845dcd8080cc91&lt;c/&gt; processorarchitecture=msil' loaded into appdomain 2 (SSISDB.dbo[runtime].1).
04/12/2017 10:29:45,spid68,Unknown,AppDomain 2 (SSISDB.dbo[runtime].1) created.
04/12/2017 10:26:35,Server,Unknown,Software Usage Metrics is enabled.
04/12/2017 10:26:15,spid51,Unknown,Using 'xpstar.dll' version '2014.120.4100' to execute extended stored procedure 'xp_sqlagent_notify'. This is an informational message only; no user action is required.
04/12/2017 10:26:15,spid51,Unknown,Attempting to load library 'xpstar.dll' into memory. This is an informational message only. No user action is required.
04/12/2017 10:26:15,spid51,Unknown,Using 'xpsqlbot.dll' version '2014.120.2000' to execute extended stored procedure 'xp_qv'. This is an informational message only; no user action is required.
04/12/2017 10:26:15,spid51,Unknown,Attempting to load library 'xpsqlbot.dll' into memory. This is an informational message only. No user action is required.
04/12/2017 10:26:15,spid51,Unknown,Configuration option 'show advanced options' changed from 1 to 0. Run the RECONFIGURE statement to install.
04/12/2017 10:26:15,spid51,Unknown,Configuration option 'Agent XPs' changed from 0 to 1. Run the RECONFIGURE statement to install.
04/12/2017 10:26:15,spid51,Unknown,Configuration option 'show advanced options' changed from 0 to 1. Run the RECONFIGURE statement to install.
04/12/2017 10:26:14,spid7s,Unknown,Recovery is complete. This is an informational message only. No user action is required.
04/12/2017 10:26:14,spid24s,Unknown,Service Broker manager has started.
04/12/2017 10:26:14,spid24s,Unknown,The Database Mirroring endpoint is in disabled or stopped state.
04/12/2017 10:26:14,spid24s,Unknown,The Service Broker endpoint is in disabled or stopped state.
04/12/2017 10:26:13,spid9s,Unknown,Starting up database 'tempdb'.
04/12/2017 10:26:13,spid21s,Unknown,Recovery completed for database POSI_PIM5 (database ID 7) in 1 second(s) (analysis 237 ms&lt;c/&gt; redo 114 ms&lt;c/&gt; undo 4 ms.) This is an informational message only. No user action is required.
04/12/2017 10:26:13,spid21s,Unknown,Recovery is writing a checkpoint in database 'POSI_PIM5' (7). This is an informational message only. No user action is required.
04/12/2017 10:26:13,spid21s,Unknown,1 transactions rolled back in database 'POSI_PIM5' (7:0). This is an informational message only. No user action is required.
04/12/2017 10:26:12,spid21s,Unknown,6 transactions rolled forward in database 'POSI_PIM5' (7:0). This is an informational message only. No user action is required.
04/12/2017 10:26:11,spid19s,Unknown,Recovery is writing a checkpoint in database 'PIM5_Operational' (8). This is an informational message only. No user action is required.
04/12/2017 10:26:11,spid19s,Unknown,0 transactions rolled back in database 'PIM5_Operational' (8:0). This is an informational message only. No user action is required.
04/12/2017 10:26:11,spid17s,Unknown,Recovery completed for database msdb (database ID 4) in 1 second(s) (analysis 24 ms&lt;c/&gt; redo 85 ms&lt;c/&gt; undo 93 ms.) This is an informational message only. No user action is required.
04/12/2017 10:26:10,spid9s,Unknown,Clearing tempdb database.
04/12/2017 10:26:10,spid17s,Unknown,Recovery is writing a checkpoint in database 'msdb' (4). This is an informational message only. No user action is required.
04/12/2017 10:26:10,spid19s,Unknown,9 transactions rolled forward in database 'PIM5_Operational' (8:0). This is an informational message only. No user action is required.
04/12/2017 10:26:10,spid17s,Unknown,0 transactions rolled back in database 'msdb' (4:0). This is an informational message only. No user action is required.
04/12/2017 10:26:10,spid17s,Unknown,164 transactions rolled forward in database 'msdb' (4:0). This is an informational message only. No user action is required.
04/12/2017 10:26:10,spid18s,Unknown,0 transactions rolled back in database 'ReportServer' (5:0). This is an informational message only. No user action is required.
04/12/2017 10:26:10,spid20s,Unknown,0 transactions rolled back in database 'ReportServerTempDB' (6:0). This is an informational message only. No user action is required.
04/12/2017 10:26:10,spid18s,Unknown,1 transactions rolled forward in database 'ReportServer' (5:0). This is an informational message only. No user action is required.
04/12/2017 10:26:10,spid9s,Unknown,Starting up database 'model'.
04/12/2017 10:26:10,spid20s,Unknown,1 transactions rolled forward in database 'ReportServerTempDB' (6:0). This is an informational message only. No user action is required.
04/12/2017 10:26:09,spid9s,Unknown,The resource database build version is 12.00.4487. This is an informational message only. No user action is required.
04/12/2017 10:26:09,spid23s,Unknown,Starting up database 'PositiveUsers'.
04/12/2017 10:26:09,spid9s,Unknown,Starting up database 'mssqlsystemresource'.
04/12/2017 10:26:09,spid22s,Unknown,Starting up database 'SSISDB'.
04/12/2017 10:26:09,spid18s,Unknown,Starting up database 'ReportServer'.
04/12/2017 10:26:09,spid19s,Unknown,Starting up database 'PIM5_Operational'.
04/12/2017 10:26:09,spid17s,Unknown,Starting up database 'msdb'.
04/12/2017 10:26:09,spid20s,Unknown,Starting up database 'ReportServerTempDB'.
04/12/2017 10:26:09,spid21s,Unknown,Starting up database 'POSI_PIM5'.
04/12/2017 10:26:09,Logon,Unknown,Login failed for user 'POS-PIM5-APP'. Reason: Failed to open the explicitly specified database 'POSI_PIM5'. [CLIENT: 192.168.2.237]
04/12/2017 10:26:09,Logon,Unknown,Error: 18456&lt;c/&gt; Severity: 14&lt;c/&gt; State: 38.
04/12/2017 10:26:08,spid14s,Unknown,A new instance of the full-text filter daemon host process has been successfully started.
04/12/2017 10:26:08,Server,Unknown,The SQL Server Network Interface library successfully registered the Service Principal Name (SPN) [ MSSQLSvc/pos-web-sql2.POSITIVE.LOCAL:1433 ] for the SQL Server service.
04/12/2017 10:26:08,Server,Unknown,The SQL Server Network Interface library successfully registered the Service Principal Name (SPN) [ MSSQLSvc/pos-web-sql2.POSITIVE.LOCAL ] for the SQL Server service.
04/12/2017 10:26:08,Server,Unknown,SQL Server is attempting to register a Service Principal Name (SPN) for the SQL Server service. Kerberos authentication will not be possible until a SPN is registered for the SQL Server service. This is an informational message. No user action is required.
04/12/2017 10:26:08,spid13s,Unknown,SQL Server is now ready for client connections. This is an informational message; no user action is required.
04/12/2017 10:26:08,Server,Unknown,Dedicated admin connection support was established for listening locally on port 1434.
04/12/2017 10:26:08,Server,Unknown,Server is listening on [ 127.0.0.1 &lt;ipv4&gt; 1434].
04/12/2017 10:26:08,Server,Unknown,Server is listening on [ ::1 &lt;ipv6&gt; 1434].
04/12/2017 10:26:08,spid13s,Unknown,Server named pipe provider is ready to accept connection on [ \\.\pipe\sql\query ].
04/12/2017 10:26:08,spid13s,Unknown,Server local connection provider is ready to accept connection on [ \\.\pipe\SQLLocal\MSSQLSERVER ].
04/12/2017 10:26:08,spid13s,Unknown,Server is listening on [ 'any' &lt;ipv4&gt; 1433].
04/12/2017 10:26:08,spid13s,Unknown,Server is listening on [ 'any' &lt;ipv6&gt; 1433].
04/12/2017 10:26:08,spid13s,Unknown,A self-generated certificate was successfully loaded for encryption.
04/12/2017 10:26:08,spid7s,Unknown,Server name is 'POS-WEB-SQL2'. This is an informational message only. No user action is required.
04/12/2017 10:26:07,spid7s,Unknown,SQL Trace ID 1 was started by login ""sa"".
04/12/2017 10:26:06,spid7s,Unknown,SQL Server Audit has started the audits. This is an informational message. No user action is required.
04/12/2017 10:26:06,spid7s,Unknown,SQL Server Audit is starting the audits. This is an informational message. No user action is required.
04/12/2017 10:26:05,Server,Unknown,Common language runtime (CLR) functionality initialized using CLR version v4.0.30319 from C:\Windows\Microsoft.NET\Framework64\v4.0.30319\.
04/12/2017 10:26:04,Server,Unknown,CLR version v4.0.30319 loaded.
04/12/2017 10:26:03,spid7s,Unknown,Recovery is writing a checkpoint in database 'master' (1). This is an informational message only. No user action is required.
04/12/2017 10:26:03,spid7s,Unknown,0 transactions rolled back in database 'master' (1:0). This is an informational message only. No user action is required.
04/12/2017 10:26:03,spid7s,Unknown,28 transactions rolled forward in database 'master' (1:0). This is an informational message only. No user action is required.
04/12/2017 10:26:03,spid7s,Unknown,Starting up database 'master'.
04/12/2017 10:26:03,Server,Unknown,Using dynamic lock allocation.  Initial allocation of 2500 Lock blocks and 5000 Lock Owner blocks per node.  This is an informational message only.  No user action is required.
04/12/2017 10:26:03,Server,Unknown,Node configuration: node 0: CPU mask: 0x00000000000000ff:0 Active CPU mask: 0x00000000000000ff:0. This message provides a description of the NUMA configuration for this computer. This is an informational message only. No user action is required.
04/12/2017 10:26:03,Server,Unknown,The maximum number of dedicated administrator connections for this instance is '1'
04/12/2017 10:26:03,Server,Unknown,This instance of SQL Server last reported using a process ID of 2224 at 4/12/2017 10:06:18 AM (local) 4/12/2017 2:06:18 PM (UTC). This is an informational message only; no user action is required.
04/12/2017 10:26:01,Server,Unknown,Default collation: SQL_Latin1_General_CP1_CI_AS (us_english 1033)
04/12/2017 10:26:00,Server,Unknown,Using conventional memory in the memory manager.
04/12/2017 10:26:00,Server,Unknown,Detected 32668 MB of RAM. This is an informational message; no user action is required.
04/12/2017 10:26:00,Server,Unknown,SQL Server is starting at normal priority base (=7). This is an informational message only. No user action is required.
04/12/2017 10:26:00,Server,Unknown,SQL Server detected 1 sockets with 4 cores per socket and 8 logical processors per socket&lt;c/&gt; 8 total logical processors; using 8 logical processors based on SQL Server licensing. This is an informational message; no user action is required.
04/12/2017 10:26:00,Server,Unknown,Command Line Startup Parameters:&lt;nl/&gt;     -s ""MSSQLSERVER""
04/12/2017 10:26:00,Server,Unknown,Registry startup parameters: &lt;nl/&gt;    -d C:\Program Files\Microsoft SQL Server\MSSQL12.MSSQLSERVER\MSSQL\DATA\master.mdf&lt;nl/&gt;     -e C:\Program Files\Microsoft SQL Server\MSSQL12.MSSQLSERVER\MSSQL\Log\ERRORLOG&lt;nl/&gt;    -l C:\Program Files\Microsoft SQL Server\MSSQL12.MSSQLSERVER\MSSQL\DATA\mastlog.ldf
04/12/2017 10:26:00,Server,Unknown,The service account is 'NT Service\MSSQLSERVER'. This is an informational message; no user action is required.
04/12/2017 10:26:00,Server,Unknown,Logging SQL Server messages in file 'C:\Program Files\Microsoft SQL Server\MSSQL12.MSSQLSERVER\MSSQL\Log\ERRORLOG'.
04/12/2017 10:26:00,Server,Unknown,Authentication mode is MIXED.
04/12/2017 10:26:00,Server,Unknown,System Manufacturer: 'Dell Inc.'&lt;c/&gt; System Model: 'PowerEdge R730'.
04/12/2017 10:26:00,Server,Unknown,Server process ID is 2212.
04/12/2017 10:26:00,Server,Unknown,All rights reserved.
04/12/2017 10:26:00,Server,Unknown,(c) Microsoft Corporation.
04/12/2017 10:26:00,Server,Unknown,UTC adjustment: -4:00
04/12/2017 10:25:59,Server,Unknown,Microsoft SQL Server 2014 (SP1-CU9-GDR) (KB3194722) - 12.0.4487.0 (X64) &lt;nl/&gt;    Oct  5 2016 19:04:21 &lt;nl/&gt;  Copyright (c) Microsoft Corporation&lt;nl/&gt;    Standard Edition (64-bit) on Windows NT 6.3 &lt;X64&gt; (Build 9600: )
</code></pre>

<p><strong>SQL Agent Log</strong></p>

<pre><code>Date,Source,Severity,Message
04/12/2017 10:30:12,,Warning,[000] Request to run job Generate NetAval (from User POSITIVE\ywang) refused because the job is already running from a request by Schedule 1016 (Every 30 Minutes)
04/12/2017 10:26:17,,Warning,[396] An idle CPU condition has not been defined - OnIdle job schedules will have no effect
04/12/2017 10:26:17,,Warning,[475] Database Mail is not enabled for agent notifications.
04/12/2017 10:26:17,,Information,[129] SQLSERVERAGENT starting under Windows NT service control
04/12/2017 10:26:16,,Information,[432] There are 13 subsystems in the subsystems cache
04/12/2017 10:26:15,,Information,[339] Local computer is POS-WEB-SQL2 running Windows NT 6.2 (9200)
04/12/2017 10:26:15,,Information,[310] 8 processor(s) and 32669 MB RAM detected
04/12/2017 10:26:15,,Information,[103] NetLib being used by driver is DBNETLIB; Local host server is
04/12/2017 10:26:15,,Information,[102] SQL Server ODBC driver version 11.00.6518
04/12/2017 10:26:15,,Information,[101] SQL Server POS-WEB-SQL2 version 12.00.4487 (0 connection limit)
04/12/2017 10:26:15,,Information,[000] Configuration option 'show advanced options' changed from 0 to 1. Run the RECONFIGURE statement to install. [SQLSTATE 01000] (Message 15457)  Configuration option 'Agent XPs' changed from 0 to 1. Run the RECONFIGURE statement to install. [SQLSTATE 01000] (Message 15457)  Configuration option 'show advanced options' changed from 1 to 0. Run the RECONFIGURE statement to install. [SQLSTATE 01000] (Message 15457)
04/12/2017 10:26:11,,Information,[393] Waiting for SQL Server to recover database 'msdb'...
04/12/2017 10:26:09,,Information,[495] The SQL Server Agent startup service account is NT Service\SQLSERVERAGENT.
04/12/2017 10:26:09,,Information,[100] Microsoft SQLServerAgent version 12.0.4487.0 (X64 unicode retail build) : Process ID 3564
04/12/2017 09:58:31,,Warning,[098] SQLServerAgent terminated (forcefully)
04/12/2017 09:58:31,,Information,[000] Configuration option 'show advanced options' changed from 0 to 1. Run the RECONFIGURE statement to install. [SQLSTATE 01000] (Message 15457)  Configuration option 'Agent XPs' changed from 1 to 0. Run the RECONFIGURE statement to install. [SQLSTATE 01000] (Message 15457)  Configuration option 'show advanced options' changed from 1 to 0. Run the RECONFIGURE statement to install. [SQLSTATE 01000] (Message 15457)
04/12/2017 09:58:30,,Error,[311] Thread 'JobInvocationEngine' (ID 1468) is still running
04/12/2017 09:58:30,,Error,[240] 1 engine thread(s) failed to stop after 2 seconds of waiting
04/12/2017 09:58:28,,Information,[131] SQLSERVERAGENT service stopping due to a stop request from a user&lt;c/&gt; process&lt;c/&gt; or the OS...
04/12/2017 09:58:12,,Error,[136] Job Generate Location39 reported: Unable to terminate process 19f8 launched by step 1 of job 0xD8D121C1B6A6F8409EF65DC99AFE3D67 (reason: Access is denied)
04/12/2017 09:56:41,,Warning,[396] An idle CPU condition has not been defined - OnIdle job schedules will have no effect
04/12/2017 09:56:41,,Warning,[475] Database Mail is not enabled for agent notifications.
04/12/2017 09:56:41,,Information,[129] SQLSERVERAGENT starting under Windows NT service control
04/12/2017 09:56:40,,Information,[432] There are 13 subsystems in the subsystems cache
04/12/2017 09:56:39,,Information,[339] Local computer is POS-WEB-SQL2 running Windows NT 6.2 (9200)
04/12/2017 09:56:39,,Information,[310] 8 processor(s) and 32669 MB RAM detected
04/12/2017 09:56:39,,Information,[103] NetLib being used by driver is DBNETLIB; Local host server is
04/12/2017 09:56:39,,Information,[102] SQL Server ODBC driver version 11.00.6518
04/12/2017 09:56:39,,Information,[101] SQL Server POS-WEB-SQL2 version 12.00.4487 (0 connection limit)
04/12/2017 09:56:39,,Information,[000] Configuration option 'show advanced options' changed from 0 to 1. Run the RECONFIGURE statement to install. [SQLSTATE 01000] (Message 15457)  Configuration option 'Agent XPs' changed from 0 to 1. Run the RECONFIGURE statement to install. [SQLSTATE 01000] (Message 15457)  Configuration option 'show advanced options' changed from 1 to 0. Run the RECONFIGURE statement to install. [SQLSTATE 01000] (Message 15457)
04/12/2017 09:56:38,,Information,[393] Waiting for SQL Server to recover database 'msdb'...
04/12/2017 09:56:38,,Information,[495] The SQL Server Agent startup service account is NT Service\SQLSERVERAGENT.
04/12/2017 09:56:38,,Information,[100] Microsoft SQLServerAgent version 12.0.4487.0 (X64 unicode retail build) : Process ID 5300
</code></pre>
","<windows><sql-server>","2017-04-12 14:09:33"
"913001","How to find the MySQL database table data directory when it's different from the global one","<p>When creating a new table, I specified the data directory with:</p>

<pre><code>CREATE TABLE t2 (c1 INT PRIMARY KEY) 
TABLESPACE = innodb_file_per_table 
DATA DIRECTORY = '/alternative/directory';
</code></pre>

<p>How can I find the data directory of this table later?
If I do <code>select @@DataDirectory</code> I see the global data directory, not the custom data directory for this table in particular.</p>
","<mysql><mysql5.6>","2018-05-20 14:48:17"
"913086","Docker container port 80 - ""Connection refused""","<p><strong>Docker file content -</strong></p>

<p>[root@ansiblecontrolnode dockerbuild]# cat Dockerfile</p>

<pre><code>FROM centos:latest
MAINTAINER dhirendra120285.rai@gmail.com
RUN yum update -y &amp;&amp; yum install httpd net-tools -y
CMD [""apachectl"",""-D"",""FOREGROUND""]
EXPOSE 80
</code></pre>

<p><strong>Ansible playbook - To create a new docker image -</strong></p>

<pre><code>[root@ansiblecontrolnode dockerbuild]# cat build_docker_image.yml
---
        - name: Build a docker image
          hosts: localhost
          gather_facts: no
          tasks:
                - name: Build a CENTOS (latest) docker image with fully updated and Webservice installed
                  docker_image:
                        path: /root/ANSIBLE/ANSIBLEDOCKER/dockerbuild/
                        name: docker-image-created-by-ansible
                        tag: ansibleexample
...
</code></pre>

<p><strong>Checking syntax</strong></p>

<pre><code>[root@ansiblecontrolnode dockerbuild]# ansible-playbook 
build_docker_image.yml --syntax-check

playbook: build_docker_image.yml
</code></pre>

<p><strong>Building Docker image using ansible with http installed -</strong></p>

<pre><code>[root@ansiblecontrolnode dockerbuild]# ansible-playbook build_docker_image.yml
</code></pre>

<p><strong>Post build - checking images</strong></p>

<pre><code>[root@ansiblecontrolnode dockerbuild]# docker images

REPOSITORY                        TAG                 IMAGE ID            CREATED             SIZE
docker-image-created-by-ansible   ansibleexample      622e72211b67        35 minutes ago      449MB
centos                            latest              e934aafc2206        6 weeks ago         199MB
</code></pre>

<p><strong>Spinning-up 1st container from 622e72211b67 image -</strong></p>

<pre><code>[root@ansiblecontrolnode dockerbuild]# docker run -it --name httpimage 622e72211b67 bash
</code></pre>

<p><strong>Container run status (with port 80 exposed) -</strong></p>

<pre><code>[root@ansiblecontrolnode dockerbuild]# docker ps -a
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
be452b97dda0        622e72211b67        ""bash""              7 minutes ago       Up 7 minutes        80/tcp              httpimage
</code></pre>

<p><strong>Getting container IP address -</strong></p>

<pre><code>[root@ansiblecontrolnode dockerbuild]# docker inspect --format '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'  httpimage
172.17.0.2

[root@ansiblecontrolnode dockerbuild]# ping -c2 172.17.0.2

PING 172.17.0.2 (172.17.0.2) 56(84) bytes of data.
64 bytes from 172.17.0.2: icmp_seq=1 ttl=64 time=0.107 ms
64 bytes from 172.17.0.2: icmp_seq=2 ttl=64 time=0.058 ms

--- 172.17.0.2 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 999ms
rtt min/avg/max/mdev = 0.058/0.082/0.107/0.026 ms

[root@ansiblecontrolnode dockerbuild]# telnet 172.17.0.2 80
Trying 172.17.0.2...

telnet: connect to address 172.17.0.2: Connection refused

[root@ansiblecontrolnode dockerbuild]# curl http://172.17.0.2:80

curl: (7) Failed connect to 172.17.0.2:80; Connection refused
</code></pre>

<p>Please suggest what is wrong here?</p>

<p>Thanks,
Dhirendra</p>
","<docker><curl>","2018-05-21 12:41:04"
"844297","OVH : SSH Connections Timout after multiple connections","<p>So i have making a control panel of sorts with php on my webserver that is able to send commands to my dedicated server (remote server) via ssh2.</p>

<p>This works fine, however i have noticed that after a few times of it reconnecting to ssh2 it stops working and just times out on the connection.
(It also does not work trying to ssh directly from the server to the target)</p>

<p>I have flushed iptables but that seems to have no effect.</p>

<p>I checked auth.log and the system log but neither seem to show up anything about the failed connection.</p>

<p>The target server is running Ubuntu 16.04 and is an OVH server.</p>

<p>Could someone please offer suggestions on things i could try/check to resolve this issue? I thought since it's an OVH server it might be getting picked up by their Anti-DDoS but as far as i am aware, there is no way for me to check that.</p>

<p><strong>Traceroute to port 22 from the webserver - Unsuccessful</strong></p>

<pre><code># traceroute -n -T -p 22 TARGET
traceroute to TARGET (TARGET), 30 hops max, 60 byte packets
 1  * * *
 2  185.145.200.13  0.293 ms  0.277 ms  0.316 ms
 3  185.145.200.11  0.195 ms  0.188 ms  0.208 ms
 4  * * *
 5  91.121.128.92  9.393 ms * *
 6  * * *
</code></pre>

<p>until
    30  * * *</p>

<p><strong>Traceroute to port 22 from the webserver - Successful</strong></p>

<pre><code>traceroute -n -T -p 22 TARGET
traceroute to TARGET (TARGET), 30 hops max, 60 byte packets
 1  * * *
 2  185.145.200.13  0.335 ms  0.352 ms  0.297 ms
 3  185.145.200.11  0.273 ms  0.267 ms *
 4  * * *
until
30  * * *
</code></pre>
","<networking><ssh><security><tcpip><ovh>","2017-04-13 08:43:13"
"844310","Multiple LAN on SonicWall TZ 215","<p>I have the sonicwall configured as this</p>

<ul>
<li><strong>X0</strong>, LAN,  192.168.1.1</li>
<li><strong>X3</strong>, WAN, 1.2.3.4 (sample ip)</li>
<li><strong>X4</strong>, LAN, 192.168.3.1</li>
</ul>

<p>X0 and X4 are natted to go to Internet, but PC with 192.168.1.10 cannot ping PC with 192.168.3.10.  Why?</p>

<p>In network zones LAN group is trusted and ""Auto-generate Access Rules to allow traffic between zones of the same trust level"" is checked.</p>

<p><strong>EDIT:</strong></p>

<p>In packet monitor the error is this:</p>

<pre><code>Drop Code: 40(Enforced firewall rule), Module Id: 25(network), (Ref.Id: _5646_uyHtJcpfngKrRmv) 1:1)
</code></pre>
","<networking><local-area-network><sonicwall>","2017-04-13 09:47:30"
"913271","http to https redirection multiple sites","<p>I use IIS10 and I want to redirect all HTTP to HTTPS for all my sites.</p>

<p>For example:</p>

<ul>
<li><code>http://one.test.example</code> should be redirected to <code>https://one.test.example</code></li>
<li><code>http://two.test.example</code> should be redirected to <code>https://two.test.example</code></li>
<li><code>http://three.test.example</code> should be redirected to <code>https://three.test.example</code></li>
</ul>

<p>All the sites are in one server behind one interface (same IP, same 443 port)</p>

<p>I tried multiple solutions but nothing worked.</p>
","<iis><https><http><redirect>","2018-05-22 15:04:03"
"774678","Building router with one ethernet interface","<p>Good day!</p>

<p>I have 2 servers in datacenter and I want to use one of them as router for another instead of default gateway provided by DC. Both servers have one ethernet interface called 'eth0' with public IPv4 and IPv6 addresses and both servers are running Linux. Distributive isn't important, because I can use any: CentOS, Debian, Arch, OpenSUSE etc. Please help to get routing work. I understand that I can build private network with help of tunneling but it looks like overkill.</p>

<p>Also datacenter provides /56 routed to server 1.</p>

<p>Let's say that addresses are distributed as below:</p>

<p>server 1: 139.100.100.1/24, gw 139.100.100.254
2a01:7e00::f03c:dcff:acd0:742c/64</p>

<p>server 2: 139.100.100.2/24, gw 139.100.100.254
2a01:7e00::f03c:dcff:acd0:6124/64</p>
","<linux><router><ipv6><ipv4>","2016-05-03 23:23:47"
"913530","How can one monitor RDP connections to a workstations","<p>How can one verify if someone is connecting to their workstations (laptops) via RDP ? what policies govern such access? Can such policies be overruled by a local admin of a workstation?? if yes, how can one prevent this?</p>
","<windows><rdp>","2018-05-23 23:24:20"
"844738","Generating SSH keys for multiple users?","<p>Say, I have a server (CentOS 7) where currently I'm the only user, and have SSH key based authentication set up, it works perfectly. But what if I want to add more users (not many, say, 5 more) to the server and want to disable password-based authentication and enable key-based auth. for them, too?</p>

<p>Can I generate the key-pairs for them or they have to do it for themselves? If the former, how? I'm only familiar with generating the keys for myself.</p>

<p>Many thanks for all ideas!</p>
","<centos><ssh><ssh-keys>","2017-04-16 12:35:55"
"844798","Run script on remote server securely","<p>I want to run a single script on a remote server once a user has changed a setting via my web interface. I only want to run 1 specific file, I know I can do a simple bash script to do this. </p>

<p>My issue is security, if somehow my web server was compromised they could do other things aside from running that script on the remote server.</p>

<p>Is there a was to lock down the user for the remote session so it can only run that specific file? Or another way I've not thought of?</p>

<p>Thanks</p>
","<linux><bash>","2017-04-16 21:43:10"
"844862","Can send test mail via Outlook, but cannot connect to Exchange Server with POP3","<p>We are testing Exchange Server 2016 mail server. We can connect to user's mailbox via OWA (Outlook Web Access). When testing POP3 mail from Microsoft Outlook (on a non domain-joined PC) we can send test email but can not login to POP3 server. The error message says: </p>

<blockquote>
  <p>(Log onto incoming mail server (POP3): The connection to the server
  was interrupted. If this problem continues, contact your server
  administrator or Internet service provider (ISP))</p>
  
  <ul>
  <li>Incoming server port: 110 </li>
  <li>Outgoing server port: 25 (no encrypted)</li>
  </ul>
</blockquote>

<p>Even when I turn off all firewalls, it still cannot connect. I have the same problem with IMAP. Do you have any ideas, please?</p>
","<active-directory><outlook><imap><pop3><exchange-2016>","2017-04-17 09:33:25"
"775000","Wireless Client (Laptop) unable to connect to domain (windows server)","<p>I have established a server in the office using Windows Server 2008 R2 and Active Directory. And then created several users in Active Directory Users and Computers.</p>

<p>The PC users can successfully join the domain and log in using their username/passwords.</p>

<p>However, the laptop users with wireless connectivity are not able to join the domain and access the network. I checked laptops with ethernet cable, and they successfully logged in to domain. But when unplugging the ethernet cable and enabling the wireless, they cannot log in to domain any more.</p>

<p><strong><em>How can I enable domain access for wireless clients?</em></strong></p>

<p><strong><em>If the problem is with DNS or DHCP, then, why does a laptop connect to domain with Ethernet cable, but not with wireless connection (with the same exact DNS configurations) !!!? The only difference is being connected wired or wireless !! Am I wrong !!?</em></strong></p>

<hr>

<p><strong><em>Edit 1</em></strong></p>

<p>We are using a <em>D-Link 2750U</em> ADSL Modem which has 4 ethernet ports and wireless enabled. We use this modem both for accessing the internet, and also for establishing the server-client network. The server PC is connected via ethernet cable and has Windows Server 2008 R2 running, while the clients are some PC and some laptop, mostly Windows 7.</p>

<p><strong><em>Edit 2</em></strong></p>

<p>I tried the <code>ping example.local</code> command in laptops. When connected with ethernet cable, it has nice and successful reply; but when connected via wireless (WLAN), the ping does not respond, and gives the following error: <em>""Ping request could not find host example.local. Please check the name and try again.""</em></p>

<p><strong><em>Edit 3</em></strong></p>

<p>It seems that I have to enable something in the server or active directory, so that wireless clients can find, connect, and access the server.</p>

<p><strong><em>Edit 4</em></strong></p>

<p>This is what I've done to the DNS settings of clients (both wired PCs and wireless laptops): In the properties section of IPv4 (in client computers), I choose the <em>Use the following DNS server addresses</em>, and in front of <em>Preferred DNS server</em>, I type the IPv4 of the server PC. This works perfect for wired PCs, but not for wireless laptops.</p>

<p><strong><em>Edit 5</em></strong></p>

<p>Unfortunately, we do not have static IP. So we are using dynamic IP (for both server and clients). Is it possible to configure DNS and DHCP with dynamic IP?</p>

<p><strong><em>Edit 6</em></strong></p>

<p>The following pictures show the <em>Network Connection Details</em> for both the Server and the wireless Client:</p>

<p><a href=""https://i.sstatic.net/2MRaF.jpg"" rel=""nofollow noreferrer""><strong><em>Server</em></strong></a></p>

<p><a href=""https://i.sstatic.net/2MRaF.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2MRaF.jpg"" alt=""enter image description here""></a></p>

<p><a href=""https://i.sstatic.net/ItkVR.jpg"" rel=""nofollow noreferrer""><strong><em>Client</em></strong></a></p>

<p><a href=""https://i.sstatic.net/ItkVR.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ItkVR.jpg"" alt=""enter image description here""></a></p>

<p><strong><em>Edit 7</em></strong></p>

<p>I tried <code>ping 192.168.1.34</code> with wireless laptop. The result was <code>request time out</code>, followed by trying to connect to 192.168.1.<strong>33</strong> (!!), which again resulted in <code>request time out</code>.</p>

<p>It was really strange. So I tried <code>ping 192.168.1.35</code> from wireless laptop. It replied successfully, but with 192.168.1.<strong>33</strong> IP !!!!</p>

<p>The 192.168.1.33 IP belongs to another win-7 PC in the office, which used to host the shared drives and shared printers when we used win-7 homegroup (instead of server) in the past.</p>

<p><strong><em>Edit 8</em></strong></p>

<p>I checked the IP of the computers again. They have changed. The Server PC is now 192.168.1.35 and the wireless laptop is 192.168.1.33</p>

<p><strong><em>Edit 9</em></strong></p>

<p>I'm not able to ping the wireless laptop. Right now, the laptop's IP is 192.168.1.36, and when I do <code>ping 192.168.1.36</code> from server PC, it says <code>Destination Host Unreachable</code> !! However, the wireless laptop is able to connect to internet with wifi.</p>

<p><strong><em>Edit 10</em></strong></p>

<p>I tried <code>192.168.1.1</code> in a web browser with the wireless laptop, and it worked successfully. The browser opened the D-Link Modem page.</p>
","<active-directory><windows-server-2008-r2><domain><dhcp><wifi>","2016-05-05 11:30:37"
"844939","Linux RAID5 array error","<p>I have two RAID5 arrays, 1 array for <code>/boot</code> and for <code>/</code>. The arrays both consist of three partitions:<br>
MD0: /boot<br>
MD1: /  </p>

<p>My problem is, if I detached a disk (disappear) then mdadm prints that a drive went wrong. Okay no problem, but this message only from MD1. And the MD0 doesn't say anything. Look this:</p>

<pre><code>md1 : active raid5 sda2[3](F) sdb2[2] sdc2[1]
      7383040 blocks super 1.2 level 5, 512k chunk, algorithm 2 [3/2] [_UU]

md0 : active raid5 sda1[3] sdb1[2] sdc1[1]
      995328 blocks super 1.2 level 5, 512k chunk, algorithm 2 [3/3] [UUU]
</code></pre>

<p>The sda drive disappeared but only in MD1 array. I can detached the drive from MD1 with the following command: mdadm /dev/md1 -r detached. Because the drive not existing.</p>

<p>But I can't detached from MD0:</p>

<pre><code>mdadm: hot remove failed for 8:1: Device or resource busy
</code></pre>

<p>Because, the mdadm didn't say the sda drive went wrong. But why?</p>
","<linux><raid><mdadm>","2017-04-17 18:42:11"
"845041","Docker container setup on Docker free web server","<p>Simple question, but I haven't found much after Googling.</p>

<p>What I would like to create Linux systems that are based on Docker containers from Docker hub. </p>

<p>If I temporary need a docker-engine this would be OK, once I create a Linux image, but after that docker and docker-engine packages would not be needed.</p>

<p>Any idea would be helpful.</p>
","<docker>","2017-04-18 10:43:21"
"845054","Write performance issues with SSD RAID 10 on P840/4gb fbwc","<p>I have a new system with 4x512GB Samsung SSD 850 in a RAID10 array using P840/4gb fbwc RAID controller. When i download a file from network (10 Gbit/s) write speed is not that good. I tried downloading a single 1000MB file using wget:</p>

<pre><code>Saving to: ‘1000mb.bin’

1000mb.bin                                       100%[=========================================================================================================&gt;]   1000M   383MB/s   in 2.6s
</code></pre>

<p>Write speed 383Mbyte/s</p>

<p>EDIT: When downloading the same file to /dev/null I get full 10Gbit/s speed. The use case is downloading and storing files of that size.</p>

<p>Also when i try to write a file using dd, speed is the same with block size 512byte:</p>

<pre><code>dd if=/dev/zero of=bench.bin bs=512 count=10000K
10240000+0 records in
10240000+0 records out
5242880000 bytes (5.2 GB) copied, 14.6632 s, 358 MB/s
</code></pre>

<p>However block size 4k gives better performance:</p>

<pre><code>dd if=/dev/zero of=bench.bin bs=4k count=1000K
1024000+0 records in
1024000+0 records out
4194304000 bytes (4.2 GB) copied, 3.02447 s, 1.4 GB/s
</code></pre>

<p>So i tried various different settings concerning cache, SSD smart path, etc. for the raid controller. But i didn't see much difference. Any ideas how to increase write speed?</p>

<p>Current settings for the controller:</p>

<pre><code>Smart Array P840 in Slot 1
   Bus Interface: PCI
   Slot: 1
   Serial Number: 
   Cache Serial Number: 
   RAID 6 (ADG) Status: Enabled
   Controller Status: OK
   Hardware Revision: B
   Firmware Version: 4.52
   Rebuild Priority: High
   Expand Priority: Medium
   Surface Scan Delay: 3 secs
   Surface Scan Mode: Idle
   Parallel Surface Scan Supported: Yes
   Current Parallel Surface Scan Count: 1
   Max Parallel Surface Scan Count: 16
   Queue Depth: Automatic
   Monitor and Performance Delay: 60  min
   Elevator Sort: Enabled
   Degraded Performance Optimization: Disabled
   Inconsistency Repair Policy: Disabled
   Wait for Cache Room: Disabled
   Surface Analysis Inconsistency Notification: Disabled
   Post Prompt Timeout: 15 secs
   Cache Board Present: True
   Cache Status: OK
   Cache Ratio: 10% Read / 90% Write
   Drive Write Cache: Disabled
   Total Cache Size: 4.0 GB
   Total Cache Memory Available: 3.8 GB
   No-Battery Write Cache: Disabled
   SSD Caching RAID5 WriteBack Enabled: True
   SSD Caching Version: 2
   Cache Backup Power Source: Batteries
   Battery/Capacitor Count: 1
   Battery/Capacitor Status: OK
   SATA NCQ Supported: True
   Spare Activation Mode: Activate on physical drive failure (default)
   Controller Temperature (C): 44
   Cache Module Temperature (C): 37
   Number of Ports: 2 Internal only
   Encryption: Disabled
   Express Local Encryption: False
   Driver Name: hpsa
   Driver Version: 3.4.4
   Driver Supports HP SSD Smart Path: True
   PCI Address (Domain:Bus:Device.Function): 0000:06:00.0
   Negotiated PCIe Data Rate: PCIe 3.0 x8 (7880 MB/s)
   Controller Mode: RAID
   Controller Mode Reboot: Not Required
   Latency Scheduler Setting: Disabled
   Current Power Mode: MaxPerformance
   Host Serial Number: 
   Sanitize Erase Supported: False
   Primary Boot Volume: logicaldrive 1 
   Secondary Boot Volume: logicaldrive 2
</code></pre>

<hr>

<pre><code>Physical Drives
      physicaldrive 2I:1:1 (port 2I:box 1:bay 1, Solid State SATA, 512.1 GB, OK)
      physicaldrive 2I:1:2 (port 2I:box 1:bay 2, Solid State SATA, 512.1 GB, OK)
      physicaldrive 2I:1:3 (port 2I:box 1:bay 3, Solid State SATA, 512.1 GB, OK)
      physicaldrive 2I:1:4 (port 2I:box 1:bay 4, Solid State SATA, 512.1 GB, OK)
      None attached

   Array: A
      Interface Type: Solid State SATA
      Unused Space: 0  MB (0.0%)
      Used Space: 1.9 TB (100.0%)
      Status: OK
      MultiDomain Status: OK
      Array Type: Data
      HP SSD Smart Path: disable



      Logical Drive: 1
         Size: 953.8 GB
         Fault Tolerance: 1+0
         Heads: 255
         Sectors Per Track: 32
         Cylinders: 65535
         Strip Size: 256 KB
         Full Stripe Size: 512 KB
         Status: OK
         MultiDomain Status: OK
         Caching:  Enabled
         Unique Identifier: 
         Disk Name: /dev/sda
         Mount Points: /boot 487 MB Partition Number 2, / 14.0 GB Partition Number 7
         OS Status: LOCKED
         Logical Drive Label: 
         Mirror Group 1:
            physicaldrive 2I:1:1 (port 2I:box 1:bay 1, Solid State SATA, 512.1 GB, OK)
            physicaldrive 2I:1:2 (port 2I:box 1:bay 2, Solid State SATA, 512.1 GB, OK)
         Mirror Group 2:
            physicaldrive 2I:1:3 (port 2I:box 1:bay 3, Solid State SATA, 512.1 GB, OK)
            physicaldrive 2I:1:4 (port 2I:box 1:bay 4, Solid State SATA, 512.1 GB, OK)
         Drive Type: Data
         LD Acceleration Method: Controller Cache
</code></pre>

<p>Any help appreciated :)</p>

<hr>

<p>UPDATE: We have reached Performance increase by switching from ext4 to xfs. Thanks to everyone who has answered.</p>
","<hp><ssd><raid-controller>","2017-04-18 11:58:38"
"913942","NGINX not accessing folder","<p>I cannot access to directories inside server's document root. It throw 403 Forbidden error!</p>

<p>here is <code>/etc/nginx/nginx.conf</code> file: </p>

<pre><code>user www-data;
worker_processes auto;
pid /run/nginx.pid;

events {
    worker_connections 768;
    # multi_accept on;
}

http {
#################
# Custom config
################
#disable_symlinks off;


##
# Basic Settings
##

sendfile on;
tcp_nopush on;
tcp_nodelay on;
keepalive_timeout 65;
types_hash_max_size 2048;
server_tokens off;

server_names_hash_bucket_size 64;
# server_name_in_redirect off;

include /etc/nginx/mime.types;
default_type application/octet-stream;

##
# SSL Settings
##

ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # Dropping SSLv3, ref: POODLE
ssl_prefer_server_ciphers on;

##
# Logging Settings
##

access_log /var/log/nginx/access.log;
error_log /var/log/nginx/error.log;

##
# Gzip Settings
##

gzip on;
gzip_disable ""msie6"";

# gzip_vary on;
# gzip_proxied any;
# gzip_comp_level 6;
# gzip_buffers 16 8k;
# gzip_http_version 1.1;
# gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;

##
# Virtual Host Configs
##

include /etc/nginx/conf.d/*.conf;
include /etc/nginx/sites-enabled/*;
}
</code></pre>

<p>I cannot access to directories inside server's document root. It throw 403 Forbidden error!</p>

<p>here is <code>/etc/nginx/nginx.conf</code> file:</p>

<pre><code>user www-data;
worker_processes auto;
pid /run/nginx.pid;

events {
    worker_connections 768;
    # multi_accept on;
}

http {
#################
# Custom config
################
#disable_symlinks off;


##
# Basic Settings
##

sendfile on;
tcp_nopush on;
tcp_nodelay on;
keepalive_timeout 65;
types_hash_max_size 2048;
server_tokens off;

server_names_hash_bucket_size 64;
# server_name_in_redirect off;

include /etc/nginx/mime.types;
default_type application/octet-stream;

##
# SSL Settings
##

ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # Dropping SSLv3, ref: POODLE
ssl_prefer_server_ciphers on;

##
# Logging Settings
##

access_log /var/log/nginx/access.log;
error_log /var/log/nginx/error.log;

##
# Gzip Settings
##

gzip on;
gzip_disable ""msie6"";

# gzip_vary on;
# gzip_proxied any;
# gzip_comp_level 6;
# gzip_buffers 16 8k;
# gzip_http_version 1.1;
# gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;

##
# Virtual Host Configs
##

include /etc/nginx/conf.d/*.conf;
include /etc/nginx/sites-enabled/*;
}
</code></pre>

<p><code>/etc/nginx/sites-available/default</code> file:</p>

<pre><code>server {
listen 80 default_server;
listen [::]:80 default_server;

root /var/www/html;

index index.html index.htm index.nginx-debian.html;

server_name _;

location / {
    try_files $uri $uri/ =404;
}

location ~ \.php$ {
    include snippets/fastcgi-php.conf;
    fastcgi_pass unix:/run/php/php7.0-fpm.sock;
}

location ~ /\.ht {
    deny all;
}

}
</code></pre>

<p>I cannot access to directories inside server's document root. It throw 403 Forbidden error!</p>

<p>here is <code>/etc/nginx/nginx.conf</code> file:</p>

<pre><code>user www-data;
worker_processes auto;
pid /run/nginx.pid;

events {
    worker_connections 768;
}

http {

sendfile on;
tcp_nopush on;
tcp_nodelay on;
keepalive_timeout 65;
types_hash_max_size 2048;
server_tokens off;

server_names_hash_bucket_size 64;

include /etc/nginx/mime.types;
default_type application/octet-stream;

ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # Dropping SSLv3, ref: POODLE
ssl_prefer_server_ciphers on;

access_log /var/log/nginx/access.log;
error_log /var/log/nginx/error.log;

gzip on;
gzip_disable ""msie6"";

# gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;

include /etc/nginx/conf.d/*.conf;
include /etc/nginx/sites-enabled/*;
}
</code></pre>

<p><code>/etc/nginx/sites-available/default</code> file:</p>

<pre><code>server {
listen 80 default_server;
listen [::]:80 default_server;

root /var/www/html;

index index.html index.htm index.nginx-debian.html;

server_name _;

location / {
    try_files $uri $uri/ =404;
}

location ~ \.php$ {
    include snippets/fastcgi-php.conf;
    fastcgi_pass unix:/run/php/php7.0-fpm.sock;
}

location ~ /\.ht {
    deny all;
}

}
</code></pre>

<p>server root directory <code>map</code>:</p>

<pre><code>/var/www/html/index.php

&lt;?php include 'folder/file.php';

/var/www/html/folder/file.php

&lt;?php echo 'file included';
</code></pre>

<p>if I request <a href=""http://localhost/index.php"" rel=""nofollow noreferrer"">http://localhost/index.php</a> it respond me 'file included' but if I request directly for <a href=""http://localhost/folder/file.php"" rel=""nofollow noreferrer"">http://localhost/folder/file.php</a> it says me 403 Forbidden!</p>
","<nginx>","2018-05-26 21:04:14"
"845091","Wordpress website showing IP instead the domain?","<p>I have WordPress website on google cloud and it is sometimes showing my Ip Adress instead the domain. Some parts of the website are working fine with the domain but others like search and subdomains are showing the IP instead of my domain.</p>

<p>How can I solve this problem?</p>
","<wordpress>","2017-04-18 14:32:25"
"775302","Built-In PowerShell Module Path","<p>I'm trying to figure out where the built-in modules are hosted in the filesystem. I would've thought it an easy task, but I'm not having much luck. Most/all of the Google results are about getting the path of the current script or adding a search-path. I'm searching the hard drive for the nouns in the names and have not been successful. I've searched for general ""ps1"" files, in general, and haven't been successful. I've searched both ""Program Files"" directories and the ""Windows"" directory as well.</p>

<p>I'm using the ISE to find scripts whose nouns I can search for, but the built-in ones seem uselessly-generically named (too many results) and the proprietary ones are probably located somewhere non-general (as I understand that there's a search-path that might've been updated by the install-process).</p>

<p>Any ideas? Thanks.</p>
","<powershell>","2016-05-06 18:04:12"
"845210","Did someone just run intentionally db.dropDatabase() on my mongo shell? Can you access mongo remotely and bypass ssh password?","<p>To my surprise, my database was empty after not looking at it for a week. There were collections and a few rows of data. It's running on a digital ocean 14.04 Ubuntu droplet. Mongod is running on the default port, no password or bind ip (I have enabled bind ip now though)</p>

<p>Anyways I check the <code>logpath /var/log/mongodb/mongodb.log</code> to see what happened. I see <code>dropDatabase DB_DROPPED starting</code>. and <code>185.129.62.63:41783</code> which originates from Paris France. I'm in Hong Kong! So.... the only way I can access my mongo shell is via ssh with my username and pass. </p>

<p><strong>Did someone figure out my ssh username and pass and then go to <code>usr/bin</code> run <code>mongo</code> then <code>db.dropDatabase()</code>? or is there another way to hack my mongo shell?</strong></p>

<p>Just wondering if this will happen again (some dude deleting my data) or maybe I have some automatic <code>db.dropDatabase()</code> setting running (i doubt the latter).</p>

<p>below are the logs.</p>

<pre><code>Tue Apr 18 05:36:39.251 [conn894] end connection 216.218.206.66:30916 (4 connections now open)
Tue Apr 18 05:36:52.241 [initandlisten] connection accepted from 216.218.206.66:35132 #895 (5 $
Tue Apr 18 05:36:52.430 [conn895] end connection 216.218.206.66:35132 (4 connections now open)
Tue Apr 18 16:44:55.121 [initandlisten] connection accepted from 185.129.62.63:41448 #896 (5 c$
Tue Apr 18 16:44:56.052 [initandlisten] connection accepted from 185.129.62.63:41783 #897 (6 c$
Tue Apr 18 16:44:57.426 [conn896] end connection 185.129.62.63:41448 (5 connections now open)
Tue Apr 18 16:44:57.522 [conn897] end connection 185.129.62.63:41783 (4 connections now open)
Tue Apr 18 16:44:59.409 [initandlisten] connection accepted from 185.129.62.63:42904 #898 (5 c$
Tue Apr 18 16:45:00.580 [conn898] dropDatabase DB_DROPPED starting
Tue Apr 18 16:45:00.580 [conn898] removeJournalFiles
Tue Apr 18 16:45:00.585 [conn898] dropDatabase DB_DROPPED finished
Tue Apr 18 16:45:01.208 [conn898] dropDatabase easysmile starting
Tue Apr 18 16:45:01.208 [conn898] removeJournalFiles
Tue Apr 18 16:45:01.211 [conn898] dropDatabase easysmile finished
Tue Apr 18 16:45:01.756 [conn898] dropDatabase admin starting
Tue Apr 18 16:45:01.756 [conn898] removeJournalFiles
Tue Apr 18 16:45:01.758 [conn898] dropDatabase admin finished
Tue Apr 18 16:45:02.361 [conn898] dropDatabase cool_db starting
Tue Apr 18 16:45:02.361 [conn898] removeJournalFiles
Tue Apr 18 16:45:02.362 [conn898] dropDatabase cool_db finished
Tue Apr 18 16:45:03.579 [FileAllocator] allocating new datafile /data/db/DB_DELETED.ns, fillin$
Tue Apr 18 16:45:03.585 [FileAllocator] done allocating datafile /data/db/DB_DELETED.ns, size:$
Tue Apr 18 16:45:03.585 [FileAllocator] allocating new datafile /data/db/DB_DELETED.0, filling$

Tue Apr 18 16:45:03.587 [FileAllocator] done allocating datafile /data/db/DB_DELETED.0, size: $
Tue Apr 18 16:45:03.588 [FileAllocator] allocating new datafile /data/db/DB_DELETED.1, filling$
Tue Apr 18 16:45:03.589 [FileAllocator] done allocating datafile /data/db/DB_DELETED.1, size: $
Tue Apr 18 16:45:03.596 [conn898] build index DB_DELETED.DB_DELETED { _id: 1 }
Tue Apr 18 16:45:03.601 [conn898] build index done.  scanned 0 total records. 0.005 secs
Tue Apr 18 16:45:04.274 [conn898] end connection 185.129.62.63:42904 (4 connections now open)
Wed Apr 19 02:37:33.739 [FileAllocator] allocating new datafile /data/db/easysmile.ns, filling$
Wed Apr 19 02:37:33.746 [FileAllocator] done allocating datafile /data/db/easysmile.ns, size: $
Wed Apr 19 02:37:33.746 [FileAllocator] allocating new datafile /data/db/easysmile.0, filling $
Wed Apr 19 02:37:33.748 [FileAllocator] done allocating datafile /data/db/easysmile.0, size: 6$
Wed Apr 19 02:37:33.752 [FileAllocator] allocating new datafile /data/db/easysmile.1, filling $
Wed Apr 19 02:37:33.754 [FileAllocator] done allocating datafile /data/db/easysmile.1, size: 1$
Wed Apr 19 02:37:33.755 [conn863] build index easysmile._SCHEMA { _id: 1 }
</code></pre>
","<ssh><database><shell><mongodb>","2017-04-19 06:40:58"
"775364","chcon: failed to change context of ‘archivejson.cgi’ to ‘unconfined_u:object_r:http_sys_content_t:s0’: Invalid argument","<p>When I run:</p>

<pre><code>chcon -R -t http_sys_content_t /usr/local/nagios/sbin/
</code></pre>

<p>I get the following errors:</p>

<pre><code>chcon: failed to change context of ‘archivejson.cgi’ to ‘unconfined_u:object_r:http_sys_content_t:s0’: Invalid argument
chcon: failed to change context of ‘avail.cgi’ to ‘unconfined_u:object_r:http_sys_content_t:s0’: Invalid argument
chcon: failed to change context of ‘cmd.cgi’ to ‘unconfined_u:object_r:http_sys_content_t:s0’: Invalid argument
chcon: failed to change context of ‘config.cgi’ to ‘unconfined_u:object_r:http_sys_content_t:s0’: Invalid argument
chcon: failed to change context of ‘extinfo.cgi’ to ‘unconfined_u:object_r:http_sys_content_t:s0’: Invalid argument
chcon: failed to change context of ‘histogram.cgi’ to ‘unconfined_u:object_r:http_sys_content_t:s0’: Invalid argument
chcon: failed to change context of ‘history.cgi’ to ‘unconfined_u:object_r:http_sys_content_t:s0’: Invalid argument
chcon: failed to change context of ‘notifications.cgi’ to ‘unconfined_u:object_r:http_sys_content_t:s0’: Invalid argument
chcon: failed to change context of ‘objectjson.cgi’ to ‘unconfined_u:object_r:http_sys_content_t:s0’: Invalid argument
chcon: failed to change context of ‘outages.cgi’ to ‘unconfined_u:object_r:http_sys_content_t:s0’: Invalid argument
chcon: failed to change context of ‘showlog.cgi’ to ‘unconfined_u:object_r:http_sys_content_t:s0’: Invalid argument
chcon: failed to change context of ‘status.cgi’ to ‘unconfined_u:object_r:http_sys_content_t:s0’: Invalid argument
chcon: failed to change context of ‘statusjson.cgi’ to ‘unconfined_u:object_r:http_sys_content_t:s0’: Invalid argument
chcon: failed to change context of ‘statusmap.cgi’ to ‘unconfined_u:object_r:http_sys_content_t:s0’: Invalid argument
chcon: failed to change context of ‘statuswml.cgi’ to ‘unconfined_u:object_r:http_sys_content_t:s0’: Invalid argument
chcon: failed to change context of ‘statuswrl.cgi’ to ‘unconfined_u:object_r:http_sys_content_t:s0’: Invalid argument
chcon: failed to change context of ‘summary.cgi’ to ‘unconfined_u:object_r:http_sys_content_t:s0’: Invalid argument
chcon: failed to change context of ‘tac.cgi’ to ‘unconfined_u:object_r:http_sys_content_t:s0’: Invalid argument
chcon: failed to change context of ‘trends.cgi’ to ‘unconfined_u:object_r:http_sys_content_t:s0’: Invalid argument
chcon: failed to change context of ‘/usr/local/nagios/sbin/’ to ‘unconfined_u:object_r:http_sys_content_t:s0’: Invalid argument
</code></pre>

<p>How can I fix it and what causes this problem?</p>
","<linux><centos><centos7>","2016-05-07 02:00:07"
"845258","Is the apache running on Event MPM although Event MPM not mentioned in 'Compiled in modules'?","<p>I used <code>apache2 -l</code> in order to determine which Apache MPM is compiled. The output is:</p>

<pre><code> Compiled in modules:
  core.c
  mod_so.c
  mod_watchdog.c
  http_core.c
  mod_log_config.c
  mod_logio.c
  mod_version.c
  mod_unixd.c
</code></pre>

<p>Then, I run <code>apache2ctl -M</code> wich shows <code>mpm_event_module (shared)</code> and <code>a2query -M</code> which outputs <code>event</code>.</p>

<p>The question, is the apache running on Event MPM although there is no Event MPM in 'Compiled in modules'?</p>
","<debian><apache-2.4><debian-jessie>","2017-04-19 10:32:40"
"775504","Is it possible to access a Telnet server via SRV-Record?","<p>I wanted to make a little telnet server using PHP as an addition to my website, which already looks like a terminal.</p>

<p>I could create the <code>telnet.example.com</code>-subdomain, but that will take away the effect of the website beeing an actual terminal.</p>

<p>The transfer can be telnet, because there is no sensitive data sent between user and server.</p>

<p>I'm using Cloudflare, so I can't just connect to the server, it needs to be SRV.</p>

<p>Is there a standard for SRV-records, that all Telnet-Clients support?</p>

<p>And if yes: What is the correct name for the ""Service""?</p>
","<telnet><srv-record>","2016-05-08 16:23:43"
"775512","DNS Look up does not show if domain starts with www","<p>I am a little confused about DNS look up. If I add 'www' to the domain while looking up I see no records. But if I remove 'www' from the domain and then query, I see the DNS records.</p>

<p><a href=""https://www.whatsmydns.net/#NS/codeproject.com"" rel=""nofollow noreferrer"">DNS Propagation Checker</a> (shows records)<br>
<a href=""https://www.whatsmydns.net/#NS/www.codeproject.com"" rel=""nofollow noreferrer"">DNS Propagation Checker</a> (no records)</p>

<p>Why does this happen?</p>
","<networking><domain-name-system>","2016-05-08 18:00:51"
"845347","Does IBM Softlayer have ""VM Snapshot"" equivalent feature","<p>Coming from Hyper-V and VMWare background (limited), I would like to know if the concept of VM snapshot exists in SoftLayer. What is the direct equivalent or the closest approach to point-in-time restores if their VMs?</p>

<p>I did try to research, but all references I am finding point to disk space management concepts. While main moving part in Hyper-V and VM the restore is ""Freezing"" the vHD states and starting to write into ""incremental"" files, the snapshotting concept there is impelemented at VM level, not strictly at disk management. So in that sense - is there anything I missed in SoftLayer?</p>

<p><em>EDIT</em></p>

<p><a href=""https://knowledgelayer.softlayer.com/procedure/endurance-snapshots"" rel=""nofollow noreferrer"">This</a> is the closest concept that I found (at storage level)</p>

<p>We are using their Citrix XenServer offering.</p>
","<ibm>","2017-04-19 16:44:26"
"845503","Usage of libvirt rpc protocol","<p>Where libvirt rpc protocol is used? Is it used inside remote connection communication?
I have read that it used between libvirt library and libvirt daemon.</p>
","<kvm-virtualization><libvirt><rpc>","2017-04-20 11:58:21"
"845510","zfs backup script cannot receive destination zpool is renamed","<p>Hello I have an issue with  a backup script for ZFS snapshots:</p>

<p>basically the break down of the script is this:</p>

<pre><code>### START OF SCRIPT
# These variables are named first because they are nested in other variables.
snap_prefix=snap
retention=10

# Full paths to these utilities are needed when running the script from cron.
#date=/usr/bin/date
GDATE=""/opt/csw/bin/gdate""
grep=/usr/bin/grep
#mbuffer=/usr/local/bin/mbuffer
sed=/usr/bin/sed
sort=/usr/bin/sort   
xargs=/usr/bin/xargs
zfs=/sbin/zfs
src_0=""ServerStoreR10SSD""

dst_0=""zpoolRZ5SATA3/backuppool4/ServerStoreR10SSD""
host=""root@hostbk""
today=""$snap_prefix-`date +%Y%m%d`""
#yesterday=""$snap_prefix-`date -v -1d +%Y%m%d`""
yesterday=$snap_prefix-`$GDATE -d ""-1 day"" +""%Y%m%d""`
snap_today=""$src_0@$today""
snap_yesterday=""$src_0@$yesterday""
snap_old=`$zfs list -t snapshot -o name | $grep ""$src_0@$snap_prefix*"" | $sort -r | $sed 1,${retention}d | $sort | $xargs -n 1`
log=/root/bin/zfsreplication/cronlog/ServerStoreR10SSD.txt
# Create a blank line between the previous log entry and this one.
echo &gt;&gt; $log

# Print the name of the script.
echo ""zfsrep_ServerStoreR10SSD.sh"" &gt;&gt; $log

# Print the current date/time.
$date &gt;&gt; $log

echo &gt;&gt; $log

# Look for today's snapshot and, if not found, create it.
if $zfs list -H -o name -t snapshot | $sort | $grep ""$snap_today$"" &gt; /dev/null
then
        echo ""Today's snapshot '$snap_today' already exists."" &gt;&gt; $log
        # Uncomment if you want the script to exit when it does not create today's snapshot:
        #exit 1
else
        echo ""Taking today's snapshot: $snap_today"" &gt;&gt; $log
        $zfs snapshot -r $snap_today &gt;&gt; $log 2&gt;&amp;1
fi

echo &gt;&gt; $log

# Look for yesterday snapshot and, if found, perform incremental replication, else print error message.
if $zfs list -H -o name -t snapshot | $sort | $grep ""$snap_yesterday$"" &gt; /dev/null
then
        echo ""Yesterday's snapshot '$snap_yesterday' exists. Proceeding with replication..."" &gt;&gt; $log
        $zfs send -R -i $snap_yesterday $snap_today | ssh $host $zfs receive -vudF $dst_0 &gt;&gt; $log 2&gt;&amp;1
        #For use in local snapshots
        #$zfs send -R -i $snap_yesterday $snap_today | $zfs receive -vudF $dst_0 &gt;&gt; $log 2&gt;&amp;1
        echo &gt;&gt; $log
        echo ""Replication complete."" &gt;&gt; $log
else
        echo ""Error: Replication not completed. Missing yesterday's snapshot."" &gt;&gt; $log
fi

echo &gt;&gt; $log

 # Remove snapshot(s) older than the value assigned to $retention.
 echo ""Attempting to destroy old snapshots..."" &gt;&gt; $log

  if [ -n ""$snap_old"" ]
  then
    echo ""Destroying the following old snapshots:"" &gt;&gt; $log
    echo ""$snap_old"" &gt;&gt; $log
    $zfs list -t snapshot -o name | $grep ""$src_0@$snap_prefix*"" | $sort -r 
| $sed 1,${retention}d | $sort | $xargs -n 1 $zfs destroy -r &gt;&gt; $log 2&gt;&amp;1
else
echo ""Could not find any snapshots to destroy.""     &gt;&gt; $log
fi

# Mark the end of the script with a delimiter.
echo ""**********"" &gt;&gt; $log

# END OF SCRIPT
~
</code></pre>

<p>the log shows the following </p>

<p><strong>Yesterday's snapshot 'ServerStoreR10SSD@snap-20170419' exists. Proceeding with replication...
cannot receive: specified fs (zpoolRZ5SATA3/backuppool4/ServerStoreR10SSD) does not exist
attempting destroy zpoolRZ5SATA3/backuppool4/ServerStoreR10SSD
failed - trying rename zpoolRZ5SATA3/backuppool4/ServerStoreR10SSD to zpoolRZ5SATA3/backuppool4/ServerStoreR10SSDrecv-5424-1
cannot open 'zpoolRZ5SATA3/backuppool4/ServerStoreR10SSD': dataset does not exist</strong></p>

<p>The script was successfully up until one point when i had a power outage. The main issue is that every time it runs the incremental portion the receiving zfs pool gets renamed to something weird like ""..recv-5424-1"" hence it cannot open the destination pool and the backup fails...</p>

<p>any suggestions please? </p>
","<backup><zfs><opensolaris><zpool>","2017-04-20 12:38:12"
"845522","initial configuration of a switch","<p>I have a HPE FF 5940 switch , i have configured an admin user and a management interface (M-GigabitEthernet0/0/0)  and ssh connection to the switch. these are the only configuration i did .</p>

<p>i have connected interface 1/0/1 with a 1GB transceiver 1/0/3 with a 10GB transceiver . this is the configuration :</p>

<p>Interface              Link            Speed          Duplex       Type PVID   Description<br>
XGE1/0/1               UP             1G                 F                  A       1<br>
XGE1/0/2              DOWN       auto             A                  A       1<br>
XGE1/0/3              UP             10G(a)          F(a)              A      1  </p>

<p>the problem is that the traffic doesn't flow through the switch  what i mean is i tried pinging host behind the 1GB interface from the host behind the 10GB port , i also tried pinging the M-GigabitEthernet interface from the host connected in 1/0/3 without success . </p>

<p>what an i missing here ? </p>

<p>Edit: one more thing the led of the 1-GB transceiver is orange not green , plus it is not learning MACs while the 10-GB transceiver led is green and i can see that it is learning MACs.</p>
","<networking><switch><hpe>","2017-04-20 13:30:52"
"775733","Apache Request Stuck","<p>Our company system is having slowness issue for the past week.
After some debugging I was able to reproduce the problem by press and holding the F5 to let the page refresh numerous of time.</p>

<p>The page freeze and become unresponsive.
I realize some request was stucked by looking at the server-status.</p>

<p><a href=""https://i.sstatic.net/cTeee.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cTeee.png"" alt=""server-status screenshot""></a></p>

<p>I did strace to the process and it shows the following message:
""flock(16, LOCK_EX""</p>

<p>Here's the httpd.conf for prework.c andd keepalive was off:</p>

<pre><code>&lt;IfModule prefork.c&gt;
StartServers       4
MinSpareServers    5
MaxSpareServers   20
ServerLimit      256
MaxClients       256
MaxRequestsPerChild  4000

&lt;/IfModule&gt;
</code></pre>

<p>Seems like the request was deadlocked and stucked there forever, could this be the cause of the system slowness?
Is it caused by some kind of mis-configuration?</p>

<p><b>ADD ON</b>:</p>

<p>I also realize MySQL has process sleeping and doing nothing when the page is unresponsive. This will go away after 60 seconds which I believe is the time limit.</p>

<p><a href=""https://i.sstatic.net/TECEk.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TECEk.jpg"" alt=""Mysql Process List""></a></p>

<p><b>UPDATE http.conf:</b></p>

<pre><code>&lt;IfModule prefork.c&gt;
StartServers       2
MinSpareServers    2
MaxSpareServers   10
ServerLimit      256
MaxClients       100
MaxRequestsPerChild  4000

&lt;/IfModule&gt;
</code></pre>
","<apache-2.2>","2016-05-09 19:27:51"
"988584","vm is running,DNS is connected no webpage","<p>Sorry, I'm a Noob. My VM's were stopped on GCP. I managed to get 1 VM up and running no problem by restarting it. I have tried to restart the other 2 VM's and they appear to be running but I've got no webpage. The external IP addresses are correct and the same at Google Domains, but still nothing. </p>

<p>Am I missing something in the backend of GCP? I've tried to SSH to restart Apache using this code:</p>

<pre><code>sudo /opt/bitnami/ctlscript.sh restart apache
</code></pre>

<p>But I am getting this code back from the VM.</p>

<pre><code>  Unmonitored apache
  Syntax OK
  /opt/bitnami/apache2/scripts/ctl.sh : apache not running
  Syntax OK
  (98)Address already in use: AH00073: make_sock: unable to listen for connections on address [::]:80
  (98)Address already in use: AH00073: make_sock: unable to listen for connections on address 0.0.0.0:80
  no listening sockets available, shutting down
  AH00015: Unable to open logs
  /opt/bitnami/apache2/scripts/ctl.sh : httpd could not be started
  Monitored apache
</code></pre>

<p>Is this an easy fix or do I need to start afresh from a snapshot in a new VM instance? Some help would be appreciated, please.</p>
","<virtual-machines>","2019-10-18 17:16:22"
"845692","ansible: using host_vars in template file","<p>Unix: server1:</p>
<p>I have a structure for my var hosts like this:</p>
<h1>&gt;ls hosts/</h1>
<p>test_servers<br />
uat_servers</p>
<h1>&gt;more test_servers</h1>
<p>server1<br />
server2<br />
server3</p>
<h1>&gt;ls host_vars/</h1>
<p>server1.yml<br />
server2.yml<br />
server3.yml</p>
<h1>&gt;more server1.yml</h1>
<p>envs: [<br />
{ type: &quot;dev&quot;,<br />
list_vars: [<br />
{ param: &quot;AMD&quot; },<br />
{ param: &quot;INTEL&quot;, param2: &quot;2&quot;}<br />
]<br />
},<br />
{ type: &quot;uat&quot;,<br />
list_vars: [<br />
{ param: &quot;AMD&quot; },<br />
{ param: &quot;INTEL&quot;, param2: &quot;3&quot;}<br />
]<br />
}<br />
]</p>
<hr />
<p>template:</p>
<p>more test.j2</p>
<p>{% for host in groups['test_servers'] %}</p>
<h1>&quot;{{ host }}  &quot;</h1>
<p>{% for env in envs %}<br />
{% for par in env.list_vars %}<br />
Alias {{ env.type }}/{{ par.type }}/ &quot;www&quot;<br />
{% endfor %}<br />
{% endfor %}<br />
{% endfor %}</p>
<p>Result:</p>
<p>I always have the same variables for different servers and it always server1 but {{ host }} returns server1, server2,server3.</p>
<p>How can I get the parameters for server2 , server3 if I want to use such structure?</p>
","<ansible><ansible-playbook>","2017-04-21 08:57:16"
"914656","how to use Rsync to synchronize data from multiple servers to one server in a folder","<p>I'm using GlusterFS to connect 5 servers to be a big storage, now i want to transfer the data on these 5 servers to other 5 servers and i'm thinking about use Rsync to do this. It's easier if i use only one server to run command Rsync to a new server (connected by glusterfs) but the speed will be really slow so i want to use old 5 servers transfer data to 5 new servers that makes the speed faster x5 times. Is there any way to do that with Rsync or any other ways to do this?</p>
","<linux><rsync>","2018-05-31 18:46:48"
"845934","Is this RAM M393B2G70EB0-CMA compatible with SAMSUNG M393B1G70QH0-CMA already installed on IBM X3650 M4 server?","<p>I have a server (IBM X3650 M4 E5-2650 v2) with 16GB RAM (8GBx2) using this RAM part code: (SAMSUNG M393B1G70QH0-CMA) and i want to extend it.</p>

<p>However, is it possible to add different size of new RAM and that would not affect the server?</p>

<p>the suggested items is this (SAMSUNG M393B2G70EB0-CMA) the main specifications are same (speed,voltage) but i'm afraid that there's anything else i should cover while upgrading my RAM capacity.</p>
","<memory><hardware>","2017-04-22 12:12:48"
"988852","Windows Server 2012 applocker does not work on Windows 10","<p>Configured applocker to prevent opening cmd , go to system services application identity=automatic,preferences services=application identity set to automatic, 
link the gpo(applocker) to the whole domain somehow, it only works on the domain computer itself ,and windows 8 clients, doesnt work on Windows 10 </p>

<p>Am i missing something,please let me know ?</p>
","<windows><group-policy><applocker>","2019-10-21 13:50:22"
"915348","Why don't need any server validation when I register a domain?","<p>I register a domain in freenom today (I had never done this before) and I was surprised that I did not need any validation from my server side I only register my a domain with my bare ip.</p>

<p>I don't understand why DNS doesn't have any registration or authentication mechanism when I want to assign a domain in a server. I think that someone else would be able to assign a domain in my server before. it's possible avoid this?</p>
","<domain-name-system>","2018-06-05 22:26:40"
"846104","How can I access my own IP under StrongSwan? [MacOS]","<p>I've set up SoftEther VPN on a brand-new Ubuntu 16.04 server and connected macOS, Windows and iOS client to it.</p>

<p>I've created Ruby on Rails webserver on my macbook and binded it to my VPN address.</p>

<p>Other clients can access it but not my macbook - I can't even ping my macbook's IP from itself.</p>

<p>I've had an access to macbook's VPN IP yesterday - but it suddenly broke so I can no longer have access to my own IP.</p>

<p>The same thing happened to my Windows PC - but it can ping its own IP and access webserver from curl. Unfortunately, Edge and Internet Explorer are refusing to load the website (timeout).</p>

<p>I am using static IP for my macbook and entered the netmask, gateway and IP in the macOS Network Settings.</p>

<p>How can I fix it so my macbook and Windows PC can access own webservers hosted on VPN IP?</p>

<p>Here is my macbook's ifconfig:</p>

<pre><code>[mwolfram@mwolfram-macbook ~]$ ifconfig
lo0: flags=8049&lt;UP,LOOPBACK,RUNNING,MULTICAST&gt; mtu 16384
        options=1203&lt;RXCSUM,TXCSUM,TXSTATUS,SW_TIMESTAMP&gt;
        inet 127.0.0.1 netmask 0xff000000
        inet6 ::1 prefixlen 128
        inet6 fe80::1%lo0 prefixlen 64 scopeid 0x1
        nd6 options=201&lt;PERFORMNUD,DAD&gt;
        gif0: flags=8010&lt;POINTOPOINT,MULTICAST&gt; mtu 1280
        stf0: flags=0&lt;&gt; mtu 1280
        en1: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500
        ether [mac_address]
        inet6 fe80::2c:78a0:6beb:e171%en1 prefixlen 64 secured scopeid 0x4
        inet 10.13.1.24 netmask 0xffff0000 broadcast 10.13.255.255
        nd6 options=201&lt;PERFORMNUD,DAD&gt;
        media: autoselect
        status: active
en0: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500
        options=10b&lt;RXCSUM,TXCSUM,VLAN_HWTAGGING,AV&gt;
        ether [mac_address]
        nd6 options=201&lt;PERFORMNUD,DAD&gt;
        media: autoselect (none)
        status: inactive
fw0: flags=8822&lt;BROADCAST,SMART,SIMPLEX,MULTICAST&gt; mtu 4078
        lladdr [mac_address]
        media: autoselect &lt;full-duplex&gt;
        status: inactive
en2: flags=963&lt;UP,BROADCAST,SMART,RUNNING,PROMISC,SIMPLEX&gt; mtu 1500
        options=60&lt;TSO4,TSO6&gt;
        ether [mac_address]
        media: autoselect &lt;full-duplex&gt;
        status: inactive
p2p0: flags=8843&lt;UP,BROADCAST,RUNNING,SIMPLEX,MULTICAST&gt; mtu 2304
        ether [mac_address]
        media: autoselect
        status: inactive
bridge0: flags=8822&lt;BROADCAST,SMART,SIMPLEX,MULTICAST&gt; mtu 1500
        options=63&lt;RXCSUM,TXCSUM,TSO4,TSO6&gt;
        ether [mac_address]
        Configuration:
                id 0:0:0:0:0:0 priority 0 hellotime 0 fwddelay 0
                maxage 0 holdcnt 0 proto stp maxaddr 100 timeout 1200
                root id 0:0:0:0:0:0 priority 0 ifcost 0 port 0
                ipfilter disabled flags 0x2
        member: en2 flags=3&lt;LEARNING,DISCOVER&gt;
                ifmaxaddr 0 port 7 priority 0 path cost 0
        media: &lt;unknown type&gt;
        status: inactive
utun0: flags=8051&lt;UP,POINTOPOINT,RUNNING,MULTICAST&gt; mtu 2000
        inet6 fe80::8cc2:57af:1057:c37f%utun0 prefixlen 64 scopeid 0xa
        nd6 options=201&lt;PERFORMNUD,DAD&gt;
ppp0: flags=8051&lt;UP,POINTOPOINT,RUNNING,MULTICAST&gt; mtu 1280
        inet 10.17.0.17 --&gt; 1.0.0.1 netmask 0xffff0000
</code></pre>
","<vpn><mac-osx><ip><l2tp><softether>","2017-04-23 19:02:01"
"846406","Hoping I can see NGINX success screen one day","<p>Total Linux noob here, but lots of experience with PHP, MySQL, OOP, Java, Eclipse, etc. I have previously tried to dive into the Linux pond several times, but always retreated when the pain became too great. And it looks like it has happened again. Any help would sincerely be appreciated.</p>

<p>I booked a small Centos 6.9 VPS and tried to install NGINX, and this simple thing has taken me two days, dozens of browser search tabs open, and still no joy to see the ""NGINX Working"" screen in a browser. I tried several different tutorials, but the one that looked most complete is:</p>

<p>[<a href=""https://www.atlantic.net/community/howto/configure-nginx-on-a-centos-6-server/][1]"" rel=""nofollow noreferrer"">https://www.atlantic.net/community/howto/configure-nginx-on-a-centos-6-server/][1]</a></p>

<p>I ran update as recommended. I installed MariaDB instead of MySQL. I am using puTTY and SSH Explorer.</p>

<p>nginx -t reports successful. </p>

<pre><code>$ nginx -t

nginx: the configuration file /etc/nginx/nginx.conf syntax is ok

nginx: configuration file /etc/nginx/nginx.conf test is successful
</code></pre>

<p>I can ping the server IP. </p>

<p>If I try to restart NGINX, I get a permissions error which I don't understand, because I was doing everything as user root:</p>

<pre><code>$ service nginx restart

nginx: [emerg] open() ""/var/run/nginx.pid"" failed (13: Permission denied)

nginx: configuration file /etc/nginx/nginx.conf test failed [root@centos6:/etc/nginx/conf.d ] 
</code></pre>

<p>Is this a fatal error? </p>

<p>Next question: If I do this step by step, and only try to access the server by IP, please could someone tell me in the simplest possible terms how to change the default config files? I tried a few tutorials, but sadly still no joy. Some tutorials don't make any changes to config files, and it should just work, but nope. And some tutorials make config changes mostly using domain names, I don't have a domain name (that would be next step), and nope not working. Any other commands I could run to shed some light on what is happening? </p>

<p><strong>Adding requested info thank you sir:</strong></p>

<p>My ambitious goal is to try to get WP Multisite working, but I thought I was being reasonable in only expecting to see a success screen. No, sorry, I used Yum install and didn't work out of the box. Just wondering how could it know my IP address out of the box? It doesn't need that for configuration? Sorry for total noob questions.</p>

<p>Contents of nginx.conf are unchanged from install:</p>

<pre><code>        # For more information on configuration, see:
    #   * Official English Documentation: http://nginx.org/en/docs/
    #   * Official Russian Documentation: http://nginx.org/ru/docs/

    user nginx;
    worker_processes auto;
    error_log /var/log/nginx/error.log;
    pid /var/run/nginx.pid;

    # Load dynamic modules. See /usr/share/nginx/README.dynamic.
    include /usr/share/nginx/modules/*.conf;

    events {
        worker_connections  1024;
    }


    http {
        log_format  main  '$remote_addr - $remote_user [$time_local] ""$request"" '
                          '$status $body_bytes_sent ""$http_referer"" '
                          '""$http_user_agent"" ""$http_x_forwarded_for""';

        access_log  /var/log/nginx/access.log  main;

        sendfile            on;
        tcp_nopush          on;
        tcp_nodelay         on;
        keepalive_timeout   65;
        types_hash_max_size 2048;

        include             /etc/nginx/mime.types;
        default_type        application/octet-stream;

        # Load modular configuration files from the /etc/nginx/conf.d directory.
        # See http://nginx.org/en/docs/ngx_core_module.html#include
        # for more information.
        include /etc/nginx/conf.d/*.conf;
    }

Contents of error log as requested:

    2017/04/25 13:35:17 [emerg] 2563#0: bind() to 0.0.0.0:8000 failed (13: Permission denied)
    2017/04/25 13:41:40 [emerg] 2606#0: bind() to 0.0.0.0:8000 failed (13: Permission denied)
    2017/04/25 15:13:41 [emerg] 14498#0: open() ""/var/run/nginx.pid"" failed (13: Permission denied)
    2017/04/25 15:14:41 [emerg] 14513#0: open() ""/var/run/nginx.pid"" failed (13: Permission denied)
    2017/04/25 16:41:46 [emerg] 14591#0: open() ""/var/run/nginx.pid"" failed (13: Permission denied)

Additional info.

Contents of /etc/nginx/conf.d/default.conf (unchanged from installer)

#
# The default server
#

server {
    listen       80 default_server;
    listen       [::]:80 default_server;
    server_name  _;
    root         /usr/share/nginx/html;

    # Load configuration files for the default server block.
    include /etc/nginx/default.d/*.conf;

    location / {
    }

    error_page 404 /404.html;
        location = /40x.html {
    }

    error_page 500 502 503 504 /50x.html;
        location = /50x.html {
    }

}
</code></pre>
","<nginx><centos6>","2017-04-25 10:22:05"
"846469","Apache 2.4 Always Serving Same Virtual","<p>I have no idea how I've misconfigured this. My Apache-fu is weak. Help, please!</p>

<p>I'm trying to serve two different virtual sites. I'm running on Windows.</p>

<p>I've set up c:/webs as my virtual root. Within c:/webs, I have 2 subfolders, c:/webs/site1 and c:/webs/site2.</p>

<p>The relevant entries in my configuration files, I think, are:</p>

<h1>httpd.conf:</h1>

<pre><code>&lt;Directory /&gt;
    Options FollowSymLinks
    AllowOverride All
    Order deny,allow
    Deny from all
&lt;/Directory&gt;

#later
DocumentRoot ""c:/webs""

&lt;Directory ""c:/webs""&gt;
    Options Indexes FollowSymLinks MultiViews Includes ExecCGI
    AllowOverride All
    Order Allow,Deny
    Allow from All
    Require all granted
&lt;/Directory&gt;

# at end of file
Include conf/virtuals.conf
</code></pre>

<h1>virtuals.conf:</h1>

<pre><code>&lt;virtualhost *:80&gt;
  DocumentRoot ""c:/webs/site1/public""
  ServerName site1
&lt;/virtualhost&gt;

&lt;virtualhost *:80&gt;
  DocumentRoot ""c:/webs/site2""
  ServerName site2
&lt;/virtualhost&gt;
</code></pre>

<p>The problem is that no matter which URL I use, I get site1 served as the root. So the following URL's all actually point to site1:</p>

<pre><code>http://myserver
http://myserver/site1
http://myserver/site2
</code></pre>
","<apache-2.4><virtualhost>","2017-04-25 15:17:05"
"846494","Haproxy redirection to Docker container","<p>We have Haproxy in AWS and it will redirect all the traffic to our development environment.Recently i have deployed 2 docker container in one of the Ec2 instance. I would like to access those container via Haproxy.
Guide me how to configure the haproxy redirect to container. Find the sample config below. </p>

<pre><code>global
    log /dev/log    local0
    log /dev/log    local1 notice
    stats timeout 30s
    maxconn 4096
    user haproxy
    group haproxy
    daemon
defaults
    log     global
    mode    tcp
    option  tcplog
    option  dontlognull
    timeout connect 15s
    timeout client  15s
    timeout server  15s

frontend http_80_frontend
    bind *:80
    mode http
    redirect scheme https code 301 if !{ ssl_fc }

frontend https_443_frontend
    bind *:443
    option tcplog
    mode tcp
    acl tls req.ssl_hello_type 1
    tcp-request inspect-delay 5s
    tcp-request content accept if tls
    acl container01 req.ssl_sni -i container01.test.com
    use_backend container01 if container01
    acl container02 req.ssl_sni -i container02.test.com
    use_backend container02 if container02

backend container01
    mode tcp
    option ssl-hello-chk
    server container01 10.10.1.10:9090 check
backend container02
    mode tcp
    option ssl-hello-chk
    server container02 10.10.1.10:9091 check 
</code></pre>
","<haproxy><docker>","2017-04-25 16:24:54"
"846642","HP Proliant ML350 G5 dont runs at 48GB RAM","<p>Before I had 28GB of RAM in this configuration:</p>

<pre><code>DIMM 1A : 2048 MB 667 MHz
DIMM 2B : 4096 MB 667 MHz
DIMM 3C : 4096 MB 667 MHz 
DIMM 4D : 4096 MB 667 MHz 
DIMM 5A : 2048 MB 667 MHz
DIMM 6B : 4096 MB 667 MHz
DIMM 7C : 4096 MB 667 MHz 
DIMM 8D : 4096 MB 667 MHz 
</code></pre>

<p>Everything worked fine.</p>

<p>Now I ordered 4x 8GB Sticks which are PC2-5300F ECC RAMs, same as the others.</p>

<p>I put them in following:</p>

<pre><code>1A: 8GB
2B: 8GB
3C: 4GB
4D: 4GB
5A: 8GB
6B: 8GB
7C: 4GB
8D: 4GB
</code></pre>

<p>But on Startup it only shows 32GB of RAM.
In Bios it says 49XXXMB installed (Dont remeber correct value)
HP ILO says following:</p>

<pre><code>DIMM 1A : 8192 MB 667 MHz
DIMM 2B : 8192 MB 667 MHz 
DIMM 3C : 4096 MB 667 MHz 
DIMM 4D : 4096 MB 667 MHz
DIMM 5A : 8192 MB 667 MHz
DIMM 6B : 8192 MB 667 MHz
DIMM 7C : 4096 MB 667 MHz 
DIMM 8D : 4096 MB 667 MHz
</code></pre>

<p>After that i replaced 2x 4GB RAMs with 2x 2GB RAMs, Startup shows 32GB, in BIOS 44GB.</p>

<p>I switched them much times around and all time only 32GB.</p>

<p>One time i only started with 4x 8GB Sticks, show 32GB Ram and works.</p>

<p>So whats the problem?
Is it because of different Memory Types?
Should I order new 4x 8GB Sticks so I have 64GB RAM with 8x 8GB sticks?</p>

<p>I installed newest BIOS already.</p>

<p>I have 2x Xeon E5420 CPUs.</p>
","<memory>","2017-04-26 12:41:52"
"846677","logstash multiline logging with docker gelf driver","<p>I'm trying to force logstash to not split my mulitiline logs, i'm testing it with such simple config:</p>

<pre><code>input {
  gelf {
    port =&gt; 5055
}
filter {
  multiline {
    pattern =&gt; ""^\s""
    what =&gt; previous
  }
}
output {
  stdout {
  }
}
</code></pre>

<p>But im getting still such error (i was trying to put it to gelf codec options too but result was that same):</p>

<blockquote>
  <p>Sending Logstash's logs to /var/log/logstash which is now configured
  via log4j2.properties [2017-04-26T14:11:24,422][ERROR][logstash.agent 
  ] Cannot load an invalid configuration {:reason=>""Expected one of #,
  => at line 6, column 13 (byte 58) after input {\n  gelf {\n    port => 5055\n}\nfilter {\n  multiline ""}</p>
</blockquote>
","<logstash>","2017-04-26 14:31:40"
"916039","In AWS VPC subnet id changed","<p>I deleted the default subnet in my VPC and tried to create new. </p>

<p>After creating the new subnet it showed a different subnet id, as you would expect. </p>

<p>When I try to create subnet group in RDS the same subnet shows the Default subnet's id that was deleted.</p>
","<amazon-web-services><subnet><amazon-vpc><groups><rds>","2018-06-11 05:20:06"
"846706","IIS has arithmetic overflow when viewing worker processes","<p>Using IIS v7.5.7600 and clicking on 'Worker Processes' I get this error pop up:</p>
<blockquote>
<p>There was an error while performing this operation.</p>
<p>Arithmetic operation resulted in an overflow.</p>
</blockquote>
<p>This is on Windows Server 2008 R2 Enterprise SP1 run by a well-known hosting company.</p>
<p>If I run <code>appcmd list config</code> on the command line there are no complaints, suggesting the applicationHost.config file is valid.</p>
<p>Could the 'arithmetic' perhaps refer to calculating values to display in these columns:
<a href=""https://i.sstatic.net/I40ZP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/I40ZP.png"" alt=""Screenshot of IIS Worker Processes display, taken from a different machine"" /></a></p>
<p>I'd rather not have to reinstall IIS as we have several live ASP.NET and ColdFusion websites, all working correctly, and we'd like to avoid downtime. But equally, I'd like to use the Worker Processes feature to investigate a problem (possibly hackers).</p>
<p><strong>Perhaps there is a way to fix it without switching it off for too long?</strong></p>
","<iis><iis-7.5><coldfusion>","2017-04-26 16:06:54"
"916102","reading Linux /proc directory with curl","<p>I have a server with trasversal directory allowed where I'm able to use curl to read system operating files.</p>

<p>I'd like to read the /proc directory as well but for some reason, I receive this:</p>

<pre><code>curl -v http://myserver:9999///proc/cpuinfo
*   Trying myserver...
* TCP_NODELAY set
* Connected to myserver (myserver) port 9999 (#0)
&gt; GET ///proc/cpuinfo HTTP/1.1
&gt; Host: myserver:9999
&gt; User-Agent: curl/7.60.0
&gt; Accept: */*
&gt; 
&lt; HTTP/1.1 200 OK
&lt; Accept-Ranges: bytes
&lt; Cache-Control: no-cache
&lt; Content-length: 0
&lt; Content-type: text/plain
&lt; 
* Connection #0 to host myserver left intact
</code></pre>

<p>Maybe the reason is the Content-lenght equal to zero but even though I pass the parameter --ignore-content-lenght the result is the same.</p>

<p>if I use cat:</p>

<pre><code>cat /proc/cpuinfo
processor   : 0
vendor_id   : GenuineIntel
cpu family  : 6
model       : 78
model name  : Intel(R) Core(TM) i5-6200U CPU @ 2.30GHz
stepping    : 3
cpu MHz     : 2400.032
cache size  : 3072 KB
physical id : 0
siblings    : 1
core id     : 0
cpu cores   : 1
apicid      : 0
initial apicid  : 0
fpu     : yes
fpu_exception   : yes
cpuid level : 22
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 syscall nx rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc eagerfpu pni pclmulqdq monitor ssse3 cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx rdrand hypervisor lahf_lm abm 3dnowprefetch rdseed clflushopt
bugs        :
bogomips    : 4800.06
clflush size    : 64
cache_alignment : 64
address sizes   : 39 bits physical, 48 bits virtual
power management:
</code></pre>

<p>Any help?
Thank you</p>
","<linux><security><web-server><curl>","2018-06-11 13:16:41"
"916124","No static file is cached","<p>I have set up CloudFlare for my website <a href=""http://www.funfun.org.cn"" rel=""nofollow noreferrer"">http://www.funfun.org.cn</a>, the caching level is <code>Standard</code> and The development mode is <code>Disabled</code>.  </p>

<p>By loading the home page <a href=""http://www.funfun.org.cn/1/#/home"" rel=""nofollow noreferrer"">http://www.funfun.org.cn/1/#/home</a>, I can see <code>Server: cloudflare</code> in the response header. But it seems that all the static JS files have <code>CF-Cache-Status:MISS</code> in their response header.</p>

<p>Does anyone know why?</p>

<p><strong>Edit 1:</strong> I also see <code>Cache-Control:no-cache</code> in the request header. How could we modify this?</p>

<p><a href=""https://i.sstatic.net/C7UlV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/C7UlV.png"" alt=""enter image description here""></a></p>
","<nginx><cache><cloudflare><cdn>","2018-06-11 15:44:51"
"989751","OpenVPN Nameserver for Mapped Network Drives","<p>Good Day,</p>

<p>I have setup a raspberry pi on my internal network as a OpenVPN server. I am able to connect to it on the public IP using the OpenVPN Windows client. basic VPN seems to be working (when i check my public IP, once connected, it is as per the internal networks). </p>

<p>My issue is that mapped network drives are not accessible, for example drives mapped using \\SOME_INTERNAL_SERVER name. I am able to remap these drives using its IP address however I have tools and systems that need it to be mapped on NAME. How can I fix this?</p>

<p>My client.ovpn looks as follows:</p>

<pre><code>client
proto udp
remote vpn.somenetwork.com 1194
dev tun
resolv-retry infinite
nobind
persist-key
persist-tun
remote-cert-tls server
verify-x509-name server_L4CReiBJ79AjZ4aN name
auth SHA256
auth-nocache
cipher AES-128-GCM
tls-client
tls-version-min 1.2
tls-cipher TLS-ECDHE-ECDSA-WITH-AES-128-GCM-SHA256
setenv opt block-outside-dns # Prevent Windows 10 DNS leak
verb 3
script-security 2                                                                                                       
dhcp-option DNS 10.0.8.1                                                                                           
dhcp-option DOMAIN company.local 
</code></pre>

<p>The last 3 lines were added after some stack overflow searching but does not seem to help.</p>

<p>If I could get a better understanding of the issue and what steps I can take to identify the cause that would be helpful.</p>

<p>Thanks.</p>
","<windows><vpn><openvpn><remote-access><vpn-client>","2019-10-29 09:52:03"
"846907","No package matching 'vim' on Centos7 (via EPEL) using ansible","<p>Installing vim8</p>

<p>epel role task:</p>

<pre><code>---
- name: Install epel-release
  yum: name=epel-release state=latest
  become: yes
</code></pre>

<p>vim role task:</p>

<pre><code>---
- name: Install vim
  yum: disablerepo=* enablerepo=epel update_cache=yes name=vim state=latest
  become: yes
</code></pre>

<p>error:</p>

<pre><code>fatal: [DevBox]: FAILED! =&gt; {""changed"": false, ""failed"": true, ""msg"": ""No package matching 'vim' found available, installed or updated"", ""rc"": 126, ""results"": [""No package matching 'vim' found available, installed or updated""]}
</code></pre>

<p>further research indicates epel doesn't contain vim8:</p>

<pre><code>yum list | grep vim
vim-minimal.x86_64                      2:7.4.160-1.el7                @anaconda
beakerlib-vim-syntax.noarch             1.15-1.el7                     epel
fluxbox-vim-syntax.noarch               1.3.7-1.el7                    epel
golang-vim.noarch                       1.3.3-2.el7_0                  extras
protobuf-vim.x86_64                     2.5.0-8.el7                    base
vim-X11.x86_64                          2:7.4.160-1.el7_3.1            updates
vim-clustershell.noarch                 1.7.3-1.el7                    epel
vim-common.x86_64                       2:7.4.160-1.el7_3.1            updates
vim-enhanced.x86_64                     2:7.4.160-1.el7_3.1            updates
vim-filesystem.x86_64                   2:7.4.160-1.el7_3.1            updates
vim-go.x86_64                           1.8-3.el7                      epel
vim-gtk-syntax.noarch                   20130716-1.el7                 epel
vim-minimal.x86_64                      2:7.4.160-1.el7_3.1            updates
vim-vimoutliner.noarch                  0.3.7-5.el7                    epel
</code></pre>
","<ansible><ansible-playbook>","2017-04-27 13:50:47"
"989823","Traffic on port 80 although Http-Server is not running","<p>Good'day</p>

<p>Just out of curiosity I did a tcpdump port 80 on one of my remote virtual machines. I see a huge amount of requests to this http-port, 95% going to some AWS EC2 instance. Strange thing: The Web-Server was not on high load at all. The packets from tcpdump look like this:</p>

<pre><code>20:57:16.860028 IP user123.example.net.http &gt; ec2-55-155-123-123.ap-northeast-2.compute.amazonaws.com.12345: Flags [S.], seq 1234567, ack 12345567, win 12300, options [mss 1460], length 0
</code></pre>

<p><em>(user123.example.net.http is the address of my remote machine)</em></p>

<p>All of them looks basically the same, only thing that changes, is the actual EC2-adress (aka IP-adress) and the destination port. Sometimes it's an incoming packag (to port 80, too), but most of them are outgoing. </p>

<p>I run this tcpdump on port 80 today, from the office now. Now the traffic is between the remote virtual machine and another IP-adress, not the AWS-instance anymore:</p>

<pre><code>08:34:46.369161 IP user123.example.net.http  &gt; 123.12.123.132.37176: Flags [R.], seq 0, ack 2821777444, win 0, length 0
08:34:46.388933 IP 123.12.123.132.55539 &gt; user123.example.net.http : Flags [S], seq 2934790784, win 29200, length 0
</code></pre>

<p>In this case, the IP-Adress does not change. </p>

<p>Only thing that changed: I remotely logged in from my working place now, using another IP-Adress (not the one you can see in tcpdump) and another SSH-client. </p>

<h2>Investigation</h2>

<p>What I tried so far, without success: </p>

<ul>
<li>I checked the logfile of nginx, they don't show any traffic (I am not running huges sites, just some  little private projects)</li>
<li>I stopped nginx, the only apparent port80-daemon in this setup</li>
<li>I stopped all other non-system services: no changes</li>
</ul>

<h2>Current setup</h2>

<p>I am running <em>nginx</em> on this server on port 80 and 443, but traffic is generally routed to HTTPS. Also I am running <em>PHP</em> 7.3 with FPM, but on sockets, not via TCP.</p>

<p>Besides that, I am running some e-mail services (<em>dovecot,</em> <em>postfix,</em> ...), <em>Unbound,</em> a *MySQL-*Server and an *ELK-*stack. This is what <strong>netstat -tulpn</strong> confirms:
(I x'ed my public IP-Adress and my SSHD-port, which is not the default one)</p>

<pre><code>Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 127.0.0.1:53            0.0.0.0:*               LISTEN      858/unbound
tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      2806/master
tcp        0      0 x.x.x.x:25              0.0.0.0:*               LISTEN      2806/master
tcp        0      0 127.0.0.1:8953          0.0.0.0:*               LISTEN      858/unbound
tcp        0      0 0.0.0.0:443             0.0.0.0:*               LISTEN      7942/nginx -g daemo
tcp        0      0 0.0.0.0:4190            0.0.0.0:*               LISTEN      628/dovecot
tcp        0      0 0.0.0.0:993             0.0.0.0:*               LISTEN      628/dovecot
tcp        0      0 127.0.0.1:3306          0.0.0.0:*               LISTEN      491/mysqld
tcp        0      0 127.0.0.1:587           0.0.0.0:*               LISTEN      2806/master
tcp        0      0 x.x.x.x:587             0.0.0.0:*               LISTEN      2806/master
tcp        0      0 0.0.0.0:143             0.0.0.0:*               LISTEN      628/dovecot
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      7942/nginx -g daemo
tcp        0      0 0.0.0.0:???             0.0.0.0:*               LISTEN      27952/sshd
tcp6       0      0 ::1:53                  :::*                    LISTEN      858/unbound
tcp6       0      0 ::1:25                  :::*                    LISTEN      2806/master
tcp6       0      0 ::1:8953                :::*                    LISTEN      858/unbound
tcp6       0      0 :::443                  :::*                    LISTEN      7942/nginx -g daemo
tcp6       0      0 :::4190                 :::*                    LISTEN      628/dovecot
tcp6       0      0 :::993                  :::*                    LISTEN      628/dovecot
tcp6       0      0 ::1:587                 :::*                    LISTEN      2806/master
tcp6       0      0 :::143                  :::*                    LISTEN      628/dovecot
tcp6       0      0 :::80                   :::*                    LISTEN      7942/nginx -g daemo
tcp6       0      0 :::???                  :::*                    LISTEN      27952/sshd
udp        0      0 127.0.0.1:53            0.0.0.0:*                           858/unbound
udp6       0      0 ::1:53                  :::*                                858/unbound
</code></pre>

<p>After stopping all of the above mentioned servies, this is the output - Packets on port 80 still remains: </p>

<pre><code>Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:???             0.0.0.0:*               LISTEN      27952/sshd
tcp6       0      0 :::???                  :::*                    LISTEN      27952/sshd
</code></pre>

<p>That's the output of <em>service status-all|grep +</em> (marked some with an * - those didn't stop when stopping the parent process, still trying to figure that out)</p>

<pre><code> [ + ]  amavis-mc *
 [ + ]  amavisd-snmp-subagent *
 [ + ]  apache-htcacheclean *
 [ + ]  apparmor
 [ + ]  cron
 [ + ]  fail2ban
 [ + ]  gdomap
 [ + ]  ip6tables
 [ + ]  iptables
 [ + ]  lm-sensors
 [ ? ]  modules_dep.sh
 [ + ]  nscd
 [ ? ]  php-fpm-chroot-setup.sh (make PHP available after reboot)
 [ + ]  procps
 [ + ]  quota
 [ + ]  rc.local
 [ + ]  resolvconf
 [ + ]  rsyslog
 [ + ]  ssh
 [ + ]  sysstat
 [ + ]  udev
 [ + ]  unattended-upgrades
 [ + ]  urandom
 [ + ]  uwsgi
</code></pre>

<h3>Question is..</h3>

<p>Why is there traffic on port 80, even if nginx is not running? 
Where does this traffic comes from (what process)?</p>
","<nginx><http><tcpdump>","2019-10-29 20:21:46"
"846930","Wildcard SSL on domain with nginx redirects to www. regardless","<p>We have a server setup for SSL. Below are the 80 and 443 configs. Regardless of how we set up nginx the system automatically redirects the m.domain.com to www.m.domain.com.</p>

<p>Is anyone able to assist? I've tried numerous configurations and suggestions, but to no avail. nginx v1.6.2</p>

<p><strong>SSL</strong></p>

<pre><code>server {
    listen 443 ssl;

    ssl_certificate /etc/ssl/certs/en/&lt;domain&gt;.com.cer;
    ssl_certificate_key /etc/ssl/certs/en/&lt;domain&gt;.com.key;
    ssl_protocols TLSv1 TLSv1.1 TLSv1.2;

    root /var/www/vhosts/m.&lt;domain&gt;.com/public;
    server_name m.&lt;domain&gt;.com api.&lt;domain&gt;.com;
    access_log /var/log/nginx/m.&lt;domain&gt;.com-access.log;
    error_log /var/log/nginx/m.&lt;domain&gt;.com-error.log;

    location / {
            index index.php;
            try_files $uri $uri/ /index.php?q=$uri&amp;$args;
    }

    # Pass PHP scripts to PHP-FPM
    location ~* \.php$ {
            fastcgi_index   index.php;
            fastcgi_pass    127.0.0.1:9000;
            #fastcgi_pass   unix:/var/run/php-fpm/php-fpm.sock;
            include         fastcgi_params;
            fastcgi_param   SCRIPT_FILENAME    $document_root$fastcgi_script_name;
            fastcgi_param   SCRIPT_NAME        $fastcgi_script_name;
            fastcgi_param   LARAVEL_ENV             production;
    }
}
</code></pre>

<p><strong>NON SSL</strong></p>

<pre><code>server {
    listen 80;

    server_name m.&lt;domain&gt;.com api.&lt;domain&gt;.com;

    root /var/www/vhosts/m.&lt;domain&gt;.com/public;
    access_log /var/log/nginx/m.&lt;domain&gt;.com-access.log;
    error_log /var/log/nginx/m.&lt;domain&gt;.com-error.log;

    location / {
            index index.php;
            try_files $uri $uri/ /index.php?q=$uri&amp;$args;
    }

    # Pass PHP scripts to PHP-FPM
    location ~* \.php$ {
            fastcgi_index   index.php;
            fastcgi_pass    127.0.0.1:9000;
            #fastcgi_pass   unix:/var/run/php-fpm/php-fpm.sock;
            include         fastcgi_params;
            fastcgi_param   SCRIPT_FILENAME    $document_root$fastcgi_script_name;
            fastcgi_param   SCRIPT_NAME        $fastcgi_script_name;
            fastcgi_param   LARAVEL_ENV             production;
    }
}
</code></pre>
","<nginx><ssl><ssl-certificate>","2017-04-27 15:27:17"
"846960","Ip4 and Ipv6 Default Gateway","<p>We occasionally restrict internet access on some of our test servers by just quickly removing the default gateway. With the introduction of Windows Server 2016 we decided to test a few of its features and again remove the Default Gateway; although 90% of the internet didn't work, any web site that did use ipv6 worked (google.com, bbc.com) etc. </p>

<p>We are assuming this is because the test servers have some sort of route via ipv6 as when we ping these sites we get replies? Aside from sending a changelog to the network team to restrict access to the internet for these servers, is there a way we can just block them from using the internet completely? </p>
","<ipv6><internet><ipv4>","2017-04-27 17:27:27"
"989874","ZABBIX : Should I need to add the ListenIP in all my host machines zabbix-agentd.conf file?","<p>zabbix-agentd.conf file in HOST:</p>

<pre><code>### Option: ListenIP
#       List of comma delimited IP addresses that the agent should listen on.
#       First IP address is sent to Zabbix server if connecting to it to retrieve list of active checks.
#
# Mandatory: no
# Default:
# ListenIP=0.0.0.0
</code></pre>
","<zabbix><zabbix-agent>","2019-10-30 06:53:31"
"847147","Can I create IPv6 /48 addresses from IPv6 /64 block on my server? Or I need to request /48 block?","<p>On my dedicated server and I have request from support IPv6 /48. For some reason they deliver to me IPv6 /64 like this <code>IPv6 /64 2001.9d2.190.846v::2</code> Can I build /48 addresses from the IPv6 /64? Or I need to request IPv6 /48 block from support?</p>
","<linux><networking><ip><linux-networking><ipv6>","2017-04-28 13:35:03"
"916514","multi homed Ubuntu sever with site to site vpn","<p>I have a site to site vpn between the two network 192. 168.XXX.XXX (local) and 192.168.YYY.YYY (remote). I have Ubuntu box that is multi homed that is configured with 192.168.ZZZ.Z , gateway of 192.168.Z.1 and second address with 192.168.XXX.X. I am not able to get to 192.168.YYY.YYY network. Can this be accomplished? Do I need to setup static route on the ubutu box, if so I don’t know how? Any help would be appreciated. Note, I have no issues with vpn tunnel with single adapter just struggling with the multi-homed ubuntu box.</p>
","<ubuntu><vpn><linux-networking>","2018-06-13 18:30:12"
"916537","Batch File if process is running kill task","<p>I need to run this batch file remotely through PDQ deploy</p>

<p>here is the batch file:</p>

<pre><code>@echo off
taskkill /f /im wccad.exe &gt;nul
taskkill /f /im ACSR.exe &gt;nul
cd C:\csg\wccad\
wccad.exe -run ACSR WIN_32
timeout 600 &gt;nul
taskkill /f /im ACSR.exe &gt;nul
</code></pre>

<p>Here is the ouput log after the job failed </p>

<blockquote>
  <p>ERROR: The process ""wccad.exe"" not found. ERROR: Input redirection is
  not supported, exiting the process immediately. ERROR: The process
  ""ACSR.exe"" not found.</p>
</blockquote>

<p>I think i need an if statement but i can't find any solutions on google people 
Please help </p>
","<batch>","2018-06-13 20:39:41"
"847284","Executing external file as www-data (Ubuntu 16.04)","<p>I want to start a virtual machine through PHP, but i've had no luck.  Here's what i've got.</p>

<pre><code>&lt;?php
if(isset($_POST['btn_start'])){
  echo shell_exec('whoami');
  echo exec('virsh start winagain');
}
?&gt;

&lt;!DOCTYPE HTML&gt;
&lt;html&gt;
&lt;head&gt;
  &lt;title&gt;Manage VPS&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;form method=""POST"" action=""vps.php""&gt;
  &lt;input type=""submit"" name=""btn_start"" value=""Start""&gt;
&lt;/form&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>I get the error:</p>

<pre><code>error: failed to connect to the hypervisor
error: no valid connection
error: Failed to connect socket to '/var/run/libvirt/libvirt-sock': Permission denied
</code></pre>

<p>So i tried putting this line into my /etc/sudoers file</p>

<pre><code>www-data ALL = NOPASSWD: /var/run/libvirt/libvirt-sock
</code></pre>

<p>But it has done nothing.</p>

<p>I'm running out of ideas now, please help.</p>
","<linux><ubuntu><php><permissions><www-data>","2017-04-29 00:28:19"
"990303","www.googleapis.com geolocation failure","<p>I'm using various things on GCP APIs and the gelocation for www.googleapis.com appears to be wrong:</p>

<pre><code>PING www.googleapis.com (172.217.169.138) 56(84) bytes of data.
64 bytes from sof02s32-in-f10.1e100.net (172.217.169.138): icmp_seq=1 ttl=55 time=112 ms
^C
--- www.googleapis.com ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 112.008/112.008/112.008/0.000 ms
</code></pre>

<pre><code>138.169.217.172.in-addr.arpa domain name pointer sof02s32-in-f10.1e100.net.
</code></pre>

<p>Which is Sofia, Bulgaria and I'm in New York.</p>

<p>Any idea how to reach someone inside the google cloud team who can assist?</p>
","<google-cloud-platform><geolocation>","2019-11-01 19:54:22"
"916896","Add hop in tracert","<p>Basically, I'm interested how to make one of my servers hop in tracert.</p>

<p>My first server is located within DC in France</p>

<p>My second server if located within DC in Russia</p>

<p>I want my 1st server to be in traceroute behind my 2nd server</p>

<p>Current path to my server (let's imagine its like that):</p>

<pre><code>1. Hop #1 (your IP)
2. Hop #2 (my 2nd server)
</code></pre>

<p>How I want it to be:</p>

<pre><code>1. Hop #1 (your IP)
1. Hop #2 (my 1st server)
2. Hop #3 (my 2nd server)
</code></pre>

<p>Reason: I want all of the traffic to filter through my firewall within 1st server, and only afterwards go to the 2nd server. Both of the servers are not in local network, they're based within two different hosting providers and its impossible to make them in local network. Is it possible, to start routing all of the traffic through the 1st server before it reaches 2nd server without making 1st server a UDP/TCP proxy and giving users its IP to connect and than route traffic to my back-end basically (which is being my 2nd server) and drop all traffic coming except from 1st server?</p>

<p>If I need network access: I do understand that I may need network access directly, but what access I really need? Is it router access within the DC where the 2nd server is located? Or I need access to routers in both DC's? How can I really configure it? </p>

<p>Why I'm asking here?: I tried to google this issue and find explanation for more than a week now, and I still had no clear explanation except people saying ""Just do TCP/UDP proxy from your 1st server"".</p>

<p>Why I don't want to do proxy?: Because I think ping is going to increase too much to the second server if using first server as proxy, however if I do put in traceroute I think ping is not going to be ""seriously increased"" since this server can sometimes act just as a switch and route traffic, if I'm wrong here - please, say I'm wrong and ping is going to be the same as traffic still will need to go through this server anyway and TCP/UDP proxy is my easiest solution.</p>
","<networking><iptables><routing><traceroute>","2018-06-15 22:08:08"
"990573","Google Cloud - Sys Err 28 - No space left after rebooting","<p>I'm not an expert on servers, but I had to reboot an instance on Google Cloud Platform, and after that I got an Error n 28. I was able to connect through serial port, tried to create a new instance with the snapshot I took of that disk, whatsoever the error persists. As not-a-expert I have no clue on how to solve that problem. If you had any tips I'll be very grateful.</p>

<pre><code>Welcome to [1mUbuntu 18.04.3 LTS[0m!
[    7.197152] cloud-init[733]: tee: /var/log/cloud-init-output.log: No space left on device
[    7.198408] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,138 - util.py[DEBUG]: Writing to /var/log/cloud-init.log - ab: [644] 0 bytes
[    7.199329] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,139 - util.py[DEBUG]: Changing the ownership of /var/log/cloud-init.log to 102:4
[    7.199740] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,139 - util.py[DEBUG]: Attempting to remove /var/lib/cloud/instance/boot-finished
[    7.199960] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,139 - util.py[DEBUG]: Attempting to remove /var/lib/cloud/data/no-net
[    7.200188] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,140 - handlers.py[DEBUG]: start: init-local/check-cache: attempting to read from cache [check]
[    7.200392] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,140 - util.py[DEBUG]: Reading from /var/lib/cloud/instance/obj.pkl (quiet=False)
[    7.200651] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,140 - stages.py[DEBUG]: no cache found
[    7.200890] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,140 - handlers.py[DEBUG]: finish: init-local/check-cache: SUCCESS: no cache found
[    7.201099] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,140 - util.py[DEBUG]: Attempting to remove /var/lib/cloud/instance
[    7.204812] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,144 - stages.py[DEBUG]: Using distro class &lt;class 'cloudinit.distros.ubuntu.Distro'&gt;
[    7.205187] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,145 - __init__.py[DEBUG]: Looking for data source in: ['GCE', 'None'], via packages ['', 'cloudinit.sources'] that matches dependencies ['FILESYSTEM']
[    7.210174] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,150 - __init__.py[DEBUG]: Searching for local data source in: []
[    7.210438] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,150 - main.py[DEBUG]: No local datasource found
[    7.211235] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,151 - util.py[DEBUG]: Reading from /sys/class/net/ens4/name_assign_type (quiet=False)
[    7.211457] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,151 - util.py[DEBUG]: Read 2 bytes from /sys/class/net/ens4/name_assign_type
[    7.211700] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,151 - util.py[DEBUG]: Reading from /sys/class/net/ens4/carrier (quiet=False)
[    7.211919] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,152 - util.py[DEBUG]: Reading from /sys/class/net/ens4/dormant (quiet=False)
[    7.212122] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,152 - util.py[DEBUG]: Reading from /sys/class/net/ens4/operstate (quiet=False)
[    7.212337] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,152 - util.py[DEBUG]: Read 5 bytes from /sys/class/net/ens4/operstate
[    7.212595] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,152 - util.py[DEBUG]: Reading from /sys/class/net/ens4/address (quiet=False)
[    7.212807] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,152 - util.py[DEBUG]: Read 18 bytes from /sys/class/net/ens4/address
[    7.213032] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,152 - util.py[DEBUG]: Reading from /sys/class/net/ens4/address (quiet=False)
[    7.213239] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,152 - util.py[DEBUG]: Read 18 bytes from /sys/class/net/ens4/address
[    7.213440] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,152 - stages.py[DEBUG]: applying net config names for {'config': [{'type': 'physical', 'name': 'ens4', 'mac_address': '42:01:0a:9e:00:11', 'subnets': [{'type': 'dhcp'}]}], 'version': 1}
[    7.213647] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,153 - util.py[DEBUG]: Reading from /sys/class/net/ens4/device/device (quiet=False)
[    7.213857] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,153 - util.py[DEBUG]: Read 7 bytes from /sys/class/net/ens4/device/device
[    7.214072] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,153 - util.py[DEBUG]: Reading from /sys/class/net/ens4/addr_assign_type (quiet=False)
[    7.214280] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,153 - util.py[DEBUG]: Read 2 bytes from /sys/class/net/ens4/addr_assign_type
[    7.214484] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,153 - util.py[DEBUG]: Reading from /sys/class/net/ens4/uevent (quiet=False)
[    7.214688] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,153 - util.py[DEBUG]: Read 25 bytes from /sys/class/net/ens4/uevent
[    7.214889] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,153 - util.py[DEBUG]: Reading from /sys/class/net/ens4/address (quiet=False)
[    7.215092] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,153 - util.py[DEBUG]: Read 18 bytes from /sys/class/net/ens4/address
[    7.215300] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,154 - util.py[DEBUG]: Reading from /sys/class/net/ens4/device/device (quiet=False)
[    7.269476] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,154 - util.py[DEBUG]: Read 7 bytes from /sys/class/net/ens4/device/device
[    7.269647] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,154 - util.py[DEBUG]: Reading from /sys/class/net/lo/addr_assign_type (quiet=False)
[    7.269831] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,154 - util.py[DEBUG]: Read 2 bytes from /sys/class/net/lo/addr_assign_type
[    7.269938] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,154 - util.py[DEBUG]: Reading from /sys/class/net/lo/uevent (quiet=False)
[    7.270078] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,154 - util.py[DEBUG]: Read 23 bytes from /sys/class/net/lo/uevent
[    7.270176] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,154 - util.py[DEBUG]: Reading from /sys/class/net/lo/address (quiet=False)
[    7.270284] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,154 - util.py[DEBUG]: Read 18 bytes from /sys/class/net/lo/address
[    7.270379] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,154 - util.py[DEBUG]: Reading from /sys/class/net/lo/device/device (quiet=False)
[    7.270534] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,155 - util.py[DEBUG]: Reading from /sys/class/net/ens4/operstate (quiet=False)
[    7.270629] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,155 - util.py[DEBUG]: Read 5 bytes from /sys/class/net/ens4/operstate
[    7.270761] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,155 - util.py[DEBUG]: Reading from /sys/class/net/lo/operstate (quiet=False)
[    7.270857] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,155 - util.py[DEBUG]: Read 8 bytes from /sys/class/net/lo/operstate
[    7.270965] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,155 - util.py[DEBUG]: Running command ['ip', '-6', 'addr', 'show', 'permanent', 'scope', 'global'] with allowed return codes [0] (shell=False, capture=True)
[    7.271061] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,171 - util.py[DEBUG]: Running command ['ip', '-4', 'addr', 'show'] with allowed return codes [0] (shell=False, capture=True)
[    7.271232] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,175 - __init__.py[DEBUG]: no work necessary for renaming of [['42:01:0a:9e:00:11', 'ens4', 'virtio_net', '0x0001']]
[    7.271326] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,176 - stages.py[INFO]: Applying network configuration from fallback bringup=False: {'config': [{'type': 'physical', 'name': 'ens4', 'mac_address': '42:01:0a:9e:00:11', 'subnets': [{'type': 'dhcp'}]}], 'version': 1}
[    7.271423] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,178 - util.py[DEBUG]: Reading from /etc/os-release (quiet=False)
[    7.271521] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,178 - util.py[DEBUG]: Read 386 bytes from /etc/os-release
[    7.271643] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,180 - __init__.py[DEBUG]: Selected renderer 'netplan' from priority list: None
[    7.271749] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,183 - util.py[DEBUG]: Writing to /etc/netplan/50-cloud-init.yaml - wb: [644] 473 bytes
[    7.271861] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,198 - util.py[DEBUG]: Running command ['netplan', 'generate'] with allowed return codes [0] (shell=False, capture=True)
[    7.412225] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,351 - util.py[DEBUG]: Running command ['udevadm', 'test-builtin', 'net_setup_link', '/sys/class/net/ens4'] with allowed return codes [0] (shell=False, capture=True)
[    7.418983] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,358 - util.py[DEBUG]: Running command ['udevadm', 'test-builtin', 'net_setup_link', '/sys/class/net/lo'] with allowed return codes [0] (shell=False, capture=True)
[    7.424030] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,363 - main.py[DEBUG]: [local] Exiting without datasource
[    7.424945] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,365 - util.py[DEBUG]: Reading from /proc/uptime (quiet=False)
[    7.425214] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,365 - util.py[DEBUG]: Read 11 bytes from /proc/uptime
[    7.425425] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,365 - util.py[DEBUG]: cloud-init mode 'init' took 0.272 seconds (0.27)
[    7.425634] cloud-init[733]: FALLBACK: 2019-11-04 21:29:53,365 - handlers.py[DEBUG]: finish: init-local: FAIL: searching for local datasources
[    7.551545] cloud-init[733]: OSError: [Errno 28] No space left on device
[    7.552317] cloud-init[733]: During handling of the above exception, another exception occurred:
[    7.552755] cloud-init[733]: Traceback (most recent call last):
[    7.553087] cloud-init[733]:   File ""/usr/bin/cloud-init"", line 11, in &lt;module&gt;
[    7.553196] cloud-init[733]:     load_entry_point('cloud-init==19.1', 'console_scripts', 'cloud-init')()
[    7.553404] cloud-init[733]:   File ""/usr/lib/python3/dist-packages/cloudinit/cmd/main.py"", line 893, in main
[    7.553611] cloud-init[733]:     get_uptime=True, func=functor, args=(name, args))
[    7.553820] cloud-init[733]:   File ""/usr/lib/python3/dist-packages/cloudinit/util.py"", line 2521, in log_time
[    7.554026] cloud-init[733]:     ret = func(*args, **kwargs)
[    7.554322] cloud-init[733]:   File ""/usr/lib/python3/dist-packages/cloudinit/cmd/main.py"", line 671, in status_wrapper
[    7.554661] cloud-init[733]:     atomic_helper.write_json(status_path, status)
[    7.554863] cloud-init[733]:   File ""/usr/lib/python3/dist-packages/cloudinit/atomic_helper.py"", line 40, in write_json
[    7.554963] cloud-init[733]:     omode=""w"", mode=mode)
[    7.555166] cloud-init[733]:   File ""/usr/lib/python3/dist-packages/cloudinit/atomic_helper.py"", line 33, in write_file
[    7.555375] cloud-init[733]:     raise e
[    7.555595] cloud-init[733]:   File ""/usr/lib/python3/dist-packages/cloudinit/atomic_helper.py"", line 27, in write_file
[    7.555800] cloud-init[733]:     tf.close()
[    7.555999] cloud-init[733]:   File ""/usr/lib/python3.6/tempfile.py"", line 650, in close
[    7.556204] cloud-init[733]:     self._closer.close()
[    7.556527] cloud-init[733]:   File ""/usr/lib/python3.6/tempfile.py"", line 584, in close
[    7.556746] cloud-init[733]:     self.file.close()
[    7.556947] cloud-init[733]: OSError: [Errno 28] No space left on device
[    7.557156] cloud-init[733]: Error in sys.excepthook:
[    7.557255] cloud-init[733]: Traceback (most recent call last):
[    7.557452] cloud-init[733]:   File ""/usr/lib/python3/dist-packages/apport_python_hook.py"", line 109, in apport_excepthook
[    7.557661] cloud-init[733]:     pr.add_proc_info(extraenv=['PYTHONPATH', 'PYTHONHOME'])
[    7.557763] cloud-init[733]:   File ""/usr/lib/python3/dist-packages/apport/report.py"", line 543, in add_proc_info
[    7.558175] cloud-init[733]:     self.add_proc_environ(pid, extraenv)
[    7.558410] cloud-init[733]:   File ""/usr/lib/python3/dist-packages/apport/report.py"", line 610, in add_proc_environ
[    7.558646] cloud-init[733]:     env = _read_file('environ', dir_fd=proc_pid_fd).replace('\n', '\\n')
[    7.558848] cloud-init[733]:   File ""/usr/lib/python3/dist-packages/apport/report.py"", line 73, in _read_file
[    7.559049] cloud-init[733]:     with open(path, 'rb', opener=lambda path, mode: os.open(path, mode, dir_fd=dir_fd)) as fd:
[    7.559287] cloud-init[733]:   File ""/usr/lib/python3/dist-packages/apport/report.py"", line 73, in &lt;lambda&gt;
[    7.559541] cloud-init[733]:     with open(path, 'rb', opener=lambda path, mode: os.open(path, mode, dir_fd=dir_fd)) as fd:
[    7.559752] cloud-init[733]: TypeError: argument should be integer or None, not list
[    7.559963] cloud-init[733]: Original exception was:
[    7.560163] cloud-init[733]: OSError: [Errno 28] No space left on device
[    7.560262] cloud-init[733]: During handling of the above exception, another exception occurred:
[    7.560487] cloud-init[733]: Traceback (most recent call last):
[    7.560827] cloud-init[733]:   File ""/usr/bin/cloud-init"", line 11, in &lt;module&gt;
[    7.560928] cloud-init[733]:     load_entry_point('cloud-init==19.1', 'console_scripts', 'cloud-init')()
[    7.561131] cloud-init[733]:   File ""/usr/lib/python3/dist-packages/cloudinit/cmd/main.py"", line 893, in main
[    7.561334] cloud-init[733]:     get_uptime=True, func=functor, args=(name, args))
[    7.561543] cloud-init[733]:   File ""/usr/lib/python3/dist-packages/cloudinit/util.py"", line 2521, in log_time
[    7.561755] cloud-init[733]:     ret = func(*args, **kwargs)
[    7.561954] cloud-init[733]:   File ""/usr/lib/python3/dist-packages/cloudinit/cmd/main.py"", line 671, in status_wrapper
[    7.562176] cloud-init[733]:     atomic_helper.write_json(status_path, status)
[    7.562380] cloud-init[733]:   File ""/usr/lib/python3/dist-packages/cloudinit/atomic_helper.py"", line 40, in write_json
[    7.562479] cloud-init[733]:     omode=""w"", mode=mode)
[    7.562692] cloud-init[733]:   File ""/usr/lib/python3/dist-packages/cloudinit/atomic_helper.py"", line 33, in write_file
[    7.562892] cloud-init[733]:     raise e
[    7.563101] cloud-init[733]:   File ""/usr/lib/python3/dist-packages/cloudinit/atomic_helper.py"", line 27, in write_file
[    7.563333] cloud-init[733]:     tf.close()
[    7.563433] cloud-init[733]:   File ""/usr/lib/python3.6/tempfile.py"", line 650, in close
[    7.619468] cloud-init[733]:     self._closer.close()
[    7.619989] cloud-init[733]:   File ""/usr/lib/python3.6/tempfile.py"", line 584, in close
[[0;1;31mFAILED[0m] Failed to start Initial cloud-init job (pre-networking).
See 'systemctl status cloud-init-local.service' for details.
[    7.620281] cloud-init[733]:     self.file.close()
[[0;32m  OK  [0m] Reached target Network (Pre).
[    7.620913] cloud-init[733]: OSError: [Errno 28] No space left on device
         Starting Network Service...
[[0;32m  OK  [0m] Started Network Service.
[[0;32m  OK  [0m] Reached target Network.
         Starting Wait for Network to be Configured...
[[0;32m  OK  [0m] Started Wait for Network to be Configured.
         Starting Initial cloud-init job (metadata service crawler)...
[   10.049472] cloud-init[958]: OSError: [Errno 28] No space left on device
[   10.049843] cloud-init[958]: During handling of the above exception, another exception occurred:
[   10.050122] cloud-init[958]: Traceback (most recent call last):
[   10.050339] cloud-init[958]:   File ""/usr/bin/cloud-init"", line 11, in &lt;module&gt;
[   10.050495] cloud-init[958]:     load_entry_point('cloud-init==19.1', 'console_scripts', 'cloud-init')()
[   10.050628] cloud-init[958]:   File ""/usr/lib/python3/dist-packages/cloudinit/cmd/main.py"", line 893, in main
[   10.050792] cloud-init[958]:     get_uptime=True, func=functor, args=(name, args))
[   10.050927] cloud-init[958]:   File ""/usr/lib/python3/dist-packages/cloudinit/util.py"", line 2521, in log_time
[   10.051058] cloud-init[958]:     ret = func(*args, **kwargs)
[   10.051257] cloud-init[958]:   File ""/usr/lib/python3/dist-packages/cloudinit/cmd/main.py"", line 648, in status_wrapper
[   10.051362] cloud-init[958]:     atomic_helper.write_json(status_path, status)
[   10.051466] cloud-init[958]:   File ""/usr/lib/python3/dist-packages/cloudinit/atomic_helper.py"", line 40, in write_json
[   10.051600] cloud-init[958]:     omode=""w"", mode=mode)
[   10.051714] cloud-init[958]:   File ""/usr/lib/python3/dist-packages/cloudinit/atomic_helper.py"", line 33, in write_file
[   10.051854] cloud-init[958]:     raise e
[   10.051967] cloud-init[958]:   File ""/usr/lib/python3/dist-packages/cloudinit/atomic_helper.py"", line 27, in write_file
[   10.052102] cloud-init[958]:     tf.close()
[   10.052217] cloud-init[958]:   File ""/usr/lib/python3.6/tempfile.py"", line 650, in close
[   10.052364] cloud-init[958]:     self._closer.close()
[   10.052521] cloud-init[958]:   File ""/usr/lib/python3.6/tempfile.py"", line 584, in close
[   10.052719] cloud-init[958]:     self.file.close()
[   10.052928] cloud-init[958]: OSError: [Errno 28] No space left on device
[   10.053169] cloud-init[958]: Error in sys.excepthook:
[   10.053406] cloud-init[958]: Traceback (most recent call last):
[   10.053614] cloud-init[958]:   File ""/usr/lib/python3/dist-packages/apport_python_hook.py"", line 109, in apport_excepthook
[   10.053835] cloud-init[958]:     pr.add_proc_info(extraenv=['PYTHONPATH', 'PYTHONHOME'])
[   10.054069] cloud-init[958]:   File ""/usr/lib/python3/dist-packages/apport/report.py"", line 543, in add_proc_info
[   10.054285] cloud-init[958]:     self.add_proc_environ(pid, extraenv)
[   10.054507] cloud-init[958]:   File ""/usr/lib/python3/dist-packages/apport/report.py"", line 610, in add_proc_environ
[   10.054718] cloud-init[958]:     env = _read_file('environ', dir_fd=proc_pid_fd).replace('\n', '\\n')
[   10.054942] cloud-init[958]:   File ""/usr/lib/python3/dist-packages/apport/report.py"", line 73, in _read_file
[   10.055173] cloud-init[958]:     with open(path, 'rb', opener=lambda path, mode: os.open(path, mode, dir_fd=dir_fd)) as fd:
[   10.055407] cloud-init[958]:   File ""/usr/lib/python3/dist-packages/apport/report.py"", line 73, in &lt;lambda&gt;
[   10.055760] cloud-init[958]:     with open(path, 'rb', opener=lambda path, mode: os.open(path, mode, dir_fd=dir_fd)) as fd:
[   10.055928] cloud-init[958]: TypeError: argument should be integer or None, not list
[   10.056126] cloud-init[958]: Original exception was:
[   10.056371] cloud-init[958]: OSError: [Errno 28] No space left on device
[   10.056617] cloud-init[958]: During handling of the above exception, another exception occurred:
[   10.056799] cloud-init[958]: Traceback (most recent call last):
[   10.057007] cloud-init[958]:   File ""/usr/bin/cloud-init"", line 11, in &lt;module&gt;
[   10.057142] cloud-init[958]:     load_entry_point('cloud-init==19.1', 'console_scripts', 'cloud-init')()
[   10.057278] cloud-init[958]:   File ""/usr/lib/python3/dist-packages/cloudinit/cmd/main.py"", line 893, in main
[   10.057422] cloud-init[958]:     get_uptime=True, func=functor, args=(name, args))
[   10.057542] cloud-init[958]:   File ""/usr/lib/python3/dist-packages/cloudinit/util.py"", line 2521, in log_time
[   10.057732] cloud-init[958]:     ret = func(*args, **kwargs)
[   10.057867] cloud-init[958]:   File ""/usr/lib/python3/dist-packages/cloudinit/cmd/main.py"", line 648, in status_wrapper
[   10.058000] cloud-init[958]:     atomic_helper.write_json(status_path, status)
[   10.058201] cloud-init[958]:   File ""/usr/lib/python3/dist-packages/cloudinit/atomic_helper.py"", line 40, in write_json
[   10.058338] cloud-init[958]:     omode=""w"", mode=mode)
[   10.058460] cloud-init[958]:   File ""/usr/lib/python3/dist-packages/cloudinit/atomic_helper.py"", line 33, in write_file
[   10.058564] cloud-init[958]:     raise e
[   10.115887] cloud-init[958]:   File ""/usr/lib/python3/dist-packages/cloudinit/atomic_helper.py"", line 27, in write_file
[   10.116050] cloud-init[958]:     tf.close()
[   10.116152] cloud-init[958]:   File ""/usr/lib/python3.6/tempfile.py"", line 650, in close
[   10.116334] cloud-init[958]:     self._closer.close()
[   10.116453] cloud-init[958]:   File ""/usr/lib/python3.6/tempfile.py"", line 584, in close
[   10.116627] cloud-init[958]:     self.file.close()
[   10.116813] cloud-init[958]: OSError: [Errno 28] No space left on device
[[0;1;31mFAILED[0m] Failed to start Initial cloud-init job (metadata service crawler).
See 'systemctl status cloud-init.service' for details.
[[0;32m  OK  [0m] Reached target Cloud-config availability.
[[0;32m  OK  [0m] Reached target Network is Online.
[[0;32m  OK  [0m] Reached target Remote File Systems (Pre).
[[0;32m  OK  [0m] Reached target Remote File Systems.
         Starting Availability of block devices...
[[0;32m  OK  [0m] Started Availability of block devices.
You are in emergency mode. After logging in, type ""journalctl -xb"" to view
system logs, ""systemctl reboot"" to reboot, ""systemctl default"" or ""exit""
to boot into default mode.
Press Enter for maintenance
(or press Control-D to continue):
</code></pre>
","<google-cloud-platform>","2019-11-04 22:04:50"
"990681","How do I host a subdomain?","<p>I'm not sure why I've had so much difficulty finding an easy answer to a fairly common task. I'm sure there are easy answers, and I probably haven't been asking the right questions. I'm obviously not a server admin by any means, but I can usually figure out basic tasks when needed (except for this).</p>

<p>I have a website - example.com - which I am using on shopify. </p>

<p>I want to develop my landing pages on try.example.com which I would like to be hosted on Digital Ocean.</p>

<ol>
<li>How do I point the cname to the digital ocean server? Obviously I need to set a host name of some kind on the digital ocean server, then point to that?</li>
<li>After the domain is pointed, how do I handle its files? Is it the same thing as simply creating a virtual host to handle them?</li>
</ol>

<p>I apologize if this question seems basic, but I'm trying to learn and I don't know what to search. I know this is possible because I've pointed other cnames from my domain to other services.</p>

<p>Any help is greatly appreciated. Any sort of starting off point should get me started.</p>
","<linux><ubuntu><web-hosting>","2019-11-05 16:38:02"
"847896","SSL certificates are automatically redirecting subdomains to the top level","<p>I need a certificate for each one of my subdomains on my server. I have two subdomains and a top level (I don't need one for the top level). The subdomains I need an SSL certificate are</p>

<ul>
<li>pma (remote administration)</li>
<li>beta (private beta access to the website)</li>
</ul>

<p>I'm using <a href=""https://certbot.eff.org/"" rel=""nofollow noreferrer"">Certbot</a> with <a href=""https://letsencrypt.org/"" rel=""nofollow noreferrer"">Let's Encrypt</a>. I'm running Apache on Fedora 25. I've already got the certificates (by running <code>certbot --apache -d pma.mydomain.com -d beta.mydomain.com</code>) however now whenever I go to either https:/pma.mydomain.com or https:/beta.mydomain.com it redirects me to mydomain.com. Any suggestions why this might be happening?</p>

<p><em>(I know the above URLs only use one slash, I did that so it didn't parse into a clickable link).</em></p>

<p><strong>EDIT</strong> Apache config files:</p>

<p><strong>ssl.conf</strong></p>

<pre><code>#
# When we also provide SSL we have to listen to the 
# the HTTPS port in addition.
#
Listen 443 https

##
##  SSL Global Context
##
##  All SSL configuration in this context applies both to
##  the main server and all SSL-enabled virtual hosts.
##

#   Pass Phrase Dialog:
#   Configure the pass phrase gathering process.
#   The filtering dialog program (`builtin' is a internal
#   terminal dialog) has to provide the pass phrase on stdout.
SSLPassPhraseDialog exec:/usr/libexec/httpd-ssl-pass-dialog

#   Inter-Process Session Cache:
#   Configure the SSL Session Cache: First the mechanism 
#   to use and second the expiring timeout (in seconds).
SSLSessionCache         shmcb:/run/httpd/sslcache(512000)
SSLSessionCacheTimeout  300

#   Pseudo Random Number Generator (PRNG):
#   Configure one or more sources to seed the PRNG of the 
#   SSL library. The seed data should be of good random quality.
#   WARNING! On some platforms /dev/random blocks if not enough entropy
#   is available. This means you then cannot use the /dev/random device
#   because it would lead to very long connection times (as long as
#   it requires to make more entropy available). But usually those
#   platforms additionally provide a /dev/urandom device which doesn't
#   block. So, if available, use this one instead. Read the mod_ssl User
#   Manual for more details.
SSLRandomSeed startup file:/dev/urandom  256
SSLRandomSeed connect builtin
#SSLRandomSeed startup file:/dev/random  512
#SSLRandomSeed connect file:/dev/random  512
#SSLRandomSeed connect file:/dev/urandom 512

#
# Use ""SSLCryptoDevice"" to enable any supported hardware
# accelerators. Use ""openssl engine -v"" to list supported
# engine names.  NOTE: If you enable an accelerator and the
# server does not start, consult the error logs and ensure
# your accelerator is functioning properly. 
#
SSLCryptoDevice builtin
#SSLCryptoDevice ubsec

##
## SSL Virtual Host Context
##

&lt;VirtualHost _default_:443&gt;

# General setup for the virtual host, inherited from global configuration
#DocumentRoot ""/var/www/html""
#ServerName www.example.com:443

# Use separate log files for the SSL virtual host; note that LogLevel
# is not inherited from httpd.conf.
ErrorLog logs/ssl_error_log
TransferLog logs/ssl_access_log
LogLevel warn

#   SSL Engine Switch:
#   Enable/Disable SSL for this virtual host.
SSLEngine on

#   List the protocol versions which clients are allowed to connect with.
#   Disable SSLv3 by default (cf. RFC 7525 3.1.1).  TLSv1 (1.0) should be
#   disabled as quickly as practical.  By the end of 2016, only the TLSv1.2
#   protocol or later should remain in use.
SSLProtocol all -SSLv3
SSLProxyProtocol all -SSLv3

#   User agents such as web browsers are not configured for the user's
#   own preference of either security or performance, therefore this
#   must be the prerogative of the web server administrator who manages
#   cpu load versus confidentiality, so enforce the server's cipher order.
SSLHonorCipherOrder on

#   SSL Cipher Suite:
# List the ciphers that the client is permitted to negotiate.
# See the mod_ssl documentation for a complete list.
# The OpenSSL system profile is configured by default.  See
# update-crypto-policies(8) for more details.
SSLCipherSuite PROFILE=SYSTEM
SSLProxyCipherSuite PROFILE=SYSTEM

#   Server Certificate:
# Point SSLCertificateFile at a PEM encoded certificate.  If
# the certificate is encrypted, then you will be prompted for a
# pass phrase.  Note that a kill -HUP will prompt again.  A new
# certificate can be generated using the genkey(1) command.
SSLCertificateFile /etc/letsencrypt/live/pma.tfconnections.com/fullchain.pem

#   Server Private Key:
#   If the key is not combined with the certificate, use this
#   directive to point at the key file.  Keep in mind that if
#   you've both a RSA and a DSA private key you can configure
#   both in parallel (to also allow the use of DSA ciphers, etc.)
SSLCertificateKeyFile /etc/letsencrypt/live/pma.tfconnections.com/privkey.pem

#   Server Certificate Chain:
#   Point SSLCertificateChainFile at a file containing the
#   concatenation of PEM encoded CA certificates which form the
#   certificate chain for the server certificate. Alternatively
#   the referenced file can be the same as SSLCertificateFile
#   when the CA certificates are directly appended to the server
#   certificate for convinience.
#SSLCertificateChainFile /etc/pki/tls/certs/server-chain.crt

#   Certificate Authority (CA):
#   Set the CA certificate verification path where to find CA
#   certificates for client authentication or alternatively one
#   huge file containing all of them (file must be PEM encoded)
#SSLCACertificateFile /etc/pki/tls/certs/ca-bundle.crt

#   Client Authentication (Type):
#   Client certificate verification type and depth.  Types are
#   none, optional, require and optional_no_ca.  Depth is a
#   number which specifies how deeply to verify the certificate
#   issuer chain before deciding the certificate is not valid.
#SSLVerifyClient require
#SSLVerifyDepth  10

#   Access Control:
#   With SSLRequire you can do per-directory access control based
#   on arbitrary complex boolean expressions containing server
#   variable checks and other lookup directives.  The syntax is a
#   mixture between C and Perl.  See the mod_ssl documentation
#   for more details.
#&lt;Location /&gt;
#SSLRequire (    %{SSL_CIPHER} !~ m/^(EXP|NULL)/ \
#            and %{SSL_CLIENT_S_DN_O} eq ""Snake Oil, Ltd."" \
#            and %{SSL_CLIENT_S_DN_OU} in {""Staff"", ""CA"", ""Dev""} \
#            and %{TIME_WDAY} &gt;= 1 and %{TIME_WDAY} &lt;= 5 \
#            and %{TIME_HOUR} &gt;= 8 and %{TIME_HOUR} &lt;= 20       ) \
#           or %{REMOTE_ADDR} =~ m/^192\.76\.162\.[0-9]+$/
#&lt;/Location&gt;

#   SSL Engine Options:
#   Set various options for the SSL engine.
#   o FakeBasicAuth:
#     Translate the client X.509 into a Basic Authorisation.  This means that
#     the standard Auth/DBMAuth methods can be used for access control.  The
#     user name is the `one line' version of the client's X.509 certificate.
#     Note that no password is obtained from the user. Every entry in the user
#     file needs this password: `xxj31ZMTZzkVA'.
#   o ExportCertData:
#     This exports two additional environment variables: SSL_CLIENT_CERT and
#     SSL_SERVER_CERT. These contain the PEM-encoded certificates of the
#     server (always existing) and the client (only existing when client
#     authentication is used). This can be used to import the certificates
#     into CGI scripts.
#   o StdEnvVars:
#     This exports the standard SSL/TLS related `SSL_*' environment variables.
#     Per default this exportation is switched off for performance reasons,
#     because the extraction step is an expensive operation and is usually
#     useless for serving static content. So one usually enables the
#     exportation for CGI and SSI requests only.
#   o StrictRequire:
#     This denies access when ""SSLRequireSSL"" or ""SSLRequire"" applied even
#     under a ""Satisfy any"" situation, i.e. when it applies access is denied
#     and no other module can change it.
#   o OptRenegotiate:
#     This enables optimized SSL connection renegotiation handling when SSL
#     directives are used in per-directory context. 
#SSLOptions +FakeBasicAuth +ExportCertData +StrictRequire
&lt;Files ~ ""\.(cgi|shtml|phtml|php3?)$""&gt;
    SSLOptions +StdEnvVars
&lt;/Files&gt;
&lt;Directory ""/var/www/cgi-bin""&gt;
    SSLOptions +StdEnvVars
&lt;/Directory&gt;

#   SSL Protocol Adjustments:
#   The safe and default but still SSL/TLS standard compliant shutdown
#   approach is that mod_ssl sends the close notify alert but doesn't wait for
#   the close notify alert from client. When you need a different shutdown
#   approach you can use one of the following variables:
#   o ssl-unclean-shutdown:
#     This forces an unclean shutdown when the connection is closed, i.e. no
#     SSL close notify alert is send or allowed to received.  This violates
#     the SSL/TLS standard but is needed for some brain-dead browsers. Use
#     this when you receive I/O errors because of the standard approach where
#     mod_ssl sends the close notify alert.
#   o ssl-accurate-shutdown:
#     This forces an accurate shutdown when the connection is closed, i.e. a
#     SSL close notify alert is send and mod_ssl waits for the close notify
#     alert of the client. This is 100% SSL/TLS standard compliant, but in
#     practice often causes hanging connections with brain-dead browsers. Use
#     this only for browsers where you know that their SSL implementation
#     works correctly. 
#   Notice: Most problems of broken clients are also related to the HTTP
#   keep-alive facility, so you usually additionally want to disable
#   keep-alive for those clients, too. Use variable ""nokeepalive"" for this.
#   Similarly, one has to force some clients to use HTTP/1.0 to workaround
#   their broken HTTP/1.1 implementation. Use variables ""downgrade-1.0"" and
#   ""force-response-1.0"" for this.
BrowserMatch ""MSIE [2-5]"" \
         nokeepalive ssl-unclean-shutdown \
         downgrade-1.0 force-response-1.0

#   Per-Server Logging:
#   The home of a custom SSL log file. Use this when you want a
#   compact non-error SSL logfile on a virtual host basis.
CustomLog logs/ssl_request_log \
          ""%t %h %{SSL_PROTOCOL}x %{SSL_CIPHER}x \""%r\"" %b""

ServerName pma.tfconnections.com
ServerAlias beta.tfconnections.com
&lt;/VirtualHost&gt;
</code></pre>

<p><strong>httpd.conf</strong></p>

<pre><code>#
# This is the main Apache HTTP server configuration file.  It contains the
# configuration directives that give the server its instructions.
# See &lt;URL:http://httpd.apache.org/docs/2.4/&gt; for detailed information.
# In particular, see 
# &lt;URL:http://httpd.apache.org/docs/2.4/mod/directives.html&gt;
# for a discussion of each configuration directive.
#
# Do NOT simply read the instructions in here without understanding
# what they do.  They're here only as hints or reminders.  If you are unsure
# consult the online docs. You have been warned.  
#
# Configuration and logfile names: If the filenames you specify for many
# of the server's control files begin with ""/"" (or ""drive:/"" for Win32), the
# server will use that explicit path.  If the filenames do *not* begin
# with ""/"", the value of ServerRoot is prepended -- so 'log/access_log'
# with ServerRoot set to '/www' will be interpreted by the
# server as '/www/log/access_log', where as '/log/access_log' will be
# interpreted as '/log/access_log'.

#
# ServerRoot: The top of the directory tree under which the server's
# configuration, error, and log files are kept.
#
# Do not add a slash at the end of the directory path.  If you point
# ServerRoot at a non-local disk, be sure to specify a local disk on the
# Mutex directive, if file-based mutexes are used.  If you wish to share the
# same ServerRoot for multiple httpd daemons, you will need to change at
# least PidFile.
#
ServerRoot ""/etc/httpd""

#
# Listen: Allows you to bind Apache to specific IP addresses and/or
# ports, instead of the default. See also the &lt;VirtualHost&gt;
# directive.
#
# Change this to Listen on specific IP addresses as shown below to 
# prevent Apache from glomming onto all bound IP addresses.
#
#Listen 12.34.56.78:80
Listen 80

#
# Dynamic Shared Object (DSO) Support
#
# To be able to use the functionality of a module which was built as a DSO you
# have to place corresponding `LoadModule' lines at this location so the
# directives contained in it are actually available _before_ they are used.
# Statically compiled modules (those listed by `httpd -l') do not need
# to be loaded here.
#
# Example:
# LoadModule foo_module modules/mod_foo.so
#
Include conf.modules.d/*.conf

#
# If you wish httpd to run as a different user or group, you must run
# httpd as root initially and it will switch.  
#
# User/Group: The name (or #number) of the user/group to run httpd as.
# It is usually good practice to create a dedicated user and group for
# running httpd, as with most system services.
#
User apache
Group apache

# 'Main' server configuration
#
# The directives in this section set up the values used by the 'main'
# server, which responds to any requests that aren't handled by a
# &lt;VirtualHost&gt; definition.  These values also provide defaults for
# any &lt;VirtualHost&gt; containers you may define later in the file.
#
# All of these directives may appear inside &lt;VirtualHost&gt; containers,
# in which case these default settings will be overridden for the
# virtual host being defined.
#

&lt;VirtualHost *:80&gt;
    ServerName tfconnections.com
    DocumentRoot /var/www/html/
&lt;/VirtualHost&gt;

&lt;VirtualHost *:80&gt;
    ServerName beta.tfconnections.com
    DocumentRoot /var/www/html/tfconnections/
&lt;/VirtualHost&gt;

&lt;VirtualHost *:80&gt;
    ServerName pma.tfconnections.com
    DocumentRoot /var/www/html/phpmyadmin
&lt;/VirtualHost&gt;

#
# ServerAdmin: Your address, where problems with the server should be
# e-mailed.  This address appears on some server-generated pages, such
# as error documents.  e.g. admin@your-domain.com
#
ServerAdmin oliver.w.dixon@gmail.com

#
# ServerName gives the name and port that the server uses to identify itself.
# This can often be determined automatically, but we recommend you specify
# it explicitly to prevent problems during startup.
#
# If your host doesn't have a registered DNS name, enter its IP address here.
#
ServerName tfconnections.com:80

#
# Deny access to the entirety of your server's filesystem. You must
# explicitly permit access to web content directories in other 
# &lt;Directory&gt; blocks below.
#
&lt;Directory /&gt;
    AllowOverride All
    Require all denied
&lt;/Directory&gt;

#
# Note that from this point forward you must specifically allow
# particular features to be enabled - so if something's not working as
# you might expect, make sure that you have specifically enabled it
# below.
#

#
# DocumentRoot: The directory out of which you will serve your
# documents. By default, all requests are taken from this directory, but
# symbolic links and aliases may be used to point to other locations.
#
DocumentRoot ""/var/www/html""

#
# Relax access to content within /var/www.
#
&lt;Directory ""/var/www""&gt;
    AllowOverride All
    # Allow open access:
    Require all granted
&lt;/Directory&gt;

# Further relax access to the default document root:
&lt;Directory ""/var/www/html""&gt;
    #
    # Possible values for the Options directive are ""None"", ""All"",
    # or any combination of:
    #   Indexes Includes FollowSymLinks SymLinksifOwnerMatch ExecCGI MultiViews
    #
    # Note that ""MultiViews"" must be named *explicitly* --- ""Options All""
    # doesn't give it to you.
    #
    # The Options directive is both complicated and important.  Please see
    # http://httpd.apache.org/docs/2.4/mod/core.html#options
    # for more information.
    #
    Options Indexes FollowSymLinks

    #
    # AllowOverride controls what directives may be placed in .htaccess files.
    # It can be ""All"", ""None"", or any combination of the keywords:
    #   Options FileInfo AuthConfig Limit
    #
    AllowOverride All

    #
    # Controls who can get stuff from this server.
    #
    Require all granted
&lt;/Directory&gt;

&lt;Directory ""/var/www/html/phpmyadmin""&gt;

    # Allow pma access from only the local network

    Order Deny,Allow
    Deny from All
    Allow from 127.0.0.1
    Allow from ::1
    Allow from localhost
    Allow from 192.168.0
    Options Indexes FollowSymLinks
    DirectoryIndex index.php
&lt;/Directory&gt;

#
# DirectoryIndex: sets the file that Apache will serve if a directory
# is requested.
#
&lt;IfModule dir_module&gt;
    DirectoryIndex index.html
&lt;/IfModule&gt;

#
# The following lines prevent .htaccess and .htpasswd files from being 
# viewed by Web clients. 
#
&lt;Files "".ht*""&gt;
    Require all denied
&lt;/Files&gt;

#
# ErrorLog: The location of the error log file.
# If you do not specify an ErrorLog directive within a &lt;VirtualHost&gt;
# container, error messages relating to that virtual host will be
# logged here.  If you *do* define an error logfile for a &lt;VirtualHost&gt;
# container, that host's errors will be logged there and not here.
#
ErrorLog ""logs/error_log""

#
# LogLevel: Control the number of messages logged to the error_log.
# Possible values include: debug, info, notice, warn, error, crit,
# alert, emerg.
#
LogLevel warn

&lt;IfModule log_config_module&gt;
    #
    # The following directives define some format nicknames for use with
    # a CustomLog directive (see below).
    #
    LogFormat ""%h %l %u %t \""%r\"" %&gt;s %b \""%{Referer}i\"" \""%{User-Agent}i\"""" combined
    LogFormat ""%h %l %u %t \""%r\"" %&gt;s %b"" common

    &lt;IfModule logio_module&gt;
      # You need to enable mod_logio.c to use %I and %O
      LogFormat ""%h %l %u %t \""%r\"" %&gt;s %b \""%{Referer}i\"" \""%{User-Agent}i\"" %I %O"" combinedio
    &lt;/IfModule&gt;

    #
    # The location and format of the access logfile (Common Logfile Format).
    # If you do not define any access logfiles within a &lt;VirtualHost&gt;
    # container, they will be logged here.  Contrariwise, if you *do*
    # define per-&lt;VirtualHost&gt; access logfiles, transactions will be
    # logged therein and *not* in this file.
    #
    #CustomLog ""logs/access_log"" common

    #
    # If you prefer a logfile with access, agent, and referer information
    # (Combined Logfile Format) you can use the following directive.
    #
    CustomLog ""logs/access_log"" combined
&lt;/IfModule&gt;

&lt;IfModule alias_module&gt;
    #
    # Redirect: Allows you to tell clients about documents that used to 
    # exist in your server's namespace, but do not anymore. The client 
    # will make a new request for the document at its new location.
    # Example:
    # Redirect permanent /foo http://www.example.com/bar

    #
    # Alias: Maps web paths into filesystem paths and is used to
    # access content that does not live under the DocumentRoot.
    # Example:
    # Alias /webpath /full/filesystem/path
    #
    # If you include a trailing / on /webpath then the server will
    # require it to be present in the URL.  You will also likely
    # need to provide a &lt;Directory&gt; section to allow access to
    # the filesystem path.

    #
    # ScriptAlias: This controls which directories contain server scripts. 
    # ScriptAliases are essentially the same as Aliases, except that
    # documents in the target directory are treated as applications and
    # run by the server when requested rather than as documents sent to the
    # client.  The same rules about trailing ""/"" apply to ScriptAlias
    # directives as to Alias.
    #
    ScriptAlias /cgi-bin/ ""/var/www/cgi-bin/""

&lt;/IfModule&gt;

#
# ""/var/www/cgi-bin"" should be changed to whatever your ScriptAliased
# CGI directory exists, if you have that configured.
#
&lt;Directory ""/var/www/cgi-bin""&gt;
    AllowOverride None
    Options None
    Require all granted
&lt;/Directory&gt;

&lt;IfModule mime_module&gt;
    #
    # TypesConfig points to the file containing the list of mappings from
    # filename extension to MIME-type.
    #
    TypesConfig /etc/mime.types

    #
    # AddType allows you to add to or override the MIME configuration
    # file specified in TypesConfig for specific file types.
    #
    #AddType application/x-gzip .tgz
    #
    # AddEncoding allows you to have certain browsers uncompress
    # information on the fly. Note: Not all browsers support this.
    #
    #AddEncoding x-compress .Z
    #AddEncoding x-gzip .gz .tgz
    #
    # If the AddEncoding directives above are commented-out, then you
    # probably should define those extensions to indicate media types:
    #
    AddType application/x-compress .Z
    AddType application/x-gzip .gz .tgz

    #
    # AddHandler allows you to map certain file extensions to ""handlers"":
    # actions unrelated to filetype. These can be either built into the server
    # or added with the Action directive (see below)
    #
    # To use CGI scripts outside of ScriptAliased directories:
    # (You will also need to add ""ExecCGI"" to the ""Options"" directive.)
    #
    #AddHandler cgi-script .cgi

    # For type maps (negotiated resources):
    #AddHandler type-map var

    #
    # Filters allow you to process content before it is sent to the client.
    #
    # To parse .shtml files for server-side includes (SSI):
    # (You will also need to add ""Includes"" to the ""Options"" directive.)
    #
    AddType text/html .shtml
    AddOutputFilter INCLUDES .shtml
&lt;/IfModule&gt;

#
# Specify a default charset for all content served; this enables
# interpretation of all content as UTF-8 by default.  To use the 
# default browser choice (ISO-8859-1), or to allow the META tags
# in HTML content to override this choice, comment out this
# directive:
#
AddDefaultCharset UTF-8

&lt;IfModule mime_magic_module&gt;
    #
    # The mod_mime_magic module allows the server to use various hints from the
    # contents of the file itself to determine its type.  The MIMEMagicFile
    # directive tells the module where the hint definitions are located.
    #
    MIMEMagicFile conf/magic
&lt;/IfModule&gt;

#
# Customizable error responses come in three flavors:
# 1) plain text 2) local redirects 3) external redirects
#
# Some examples:
#ErrorDocument 500 ""The server made a boo boo.""
#ErrorDocument 404 /missing.html
#ErrorDocument 404 ""/cgi-bin/missing_handler.pl""
#ErrorDocument 402 http://www.example.com/subscription_info.html
#

#
# EnableMMAP and EnableSendfile: On systems that support it, 
# memory-mapping or the sendfile syscall may be used to deliver
# files.  This usually improves server performance, but must
# be turned off when serving from networked-mounted 
# filesystems or if support for these functions is otherwise
# broken on your system.
# Defaults if commented: EnableMMAP On, EnableSendfile Off
#
#EnableMMAP off
EnableSendfile on

# Supplemental configuration
#
# Load config files in the ""/etc/httpd/conf.d"" directory, if any.
IncludeOptional conf.d/*.conf
</code></pre>

<p><strong>result of a <code>curl -i https://pma.tfconnections.com</code></strong>
This is the source for the top level, not what pma should be returning.</p>

<pre><code>HTTP/1.1 200 OK
Date: Wed, 03 May 2017 00:21:29 GMT
Server: Apache/2.4.25 (Fedora) OpenSSL/1.0.2k-fips PHP/7.0.18 mod_perl/2.0.10 Perl/v5.24.1
Last-Modified: Wed, 03 May 2017 00:21:17 GMT
ETag: ""1d4-54e93a2878b94""
Accept-Ranges: bytes
Content-Length: 468
Content-Type: text/html; charset=UTF-8

&lt;html&gt;

    &lt;head&gt;
        &lt;title&gt;Coming Soon&lt;/title&gt;

        &lt;style&gt;

            body {
                background-color:#666;
                font-family:Arial;
                color:#fff;
                text-align:center;
                margin-top:100px;
            }

            #subtitle  {font-size:30px;padding-top:30px;}
                  img  {width:300px;height:150px;}

        &lt;/style&gt;
    &lt;/head&gt;

    &lt;body&gt;
        &lt;img src=""logo.svg"" /&gt;
        &lt;div id=""subtitle""&gt;Coming soon in late 2017.&lt;br /&gt;
            Contact oliver.w.dixon@gmail.com for more information.&lt;/div&gt;
    &lt;/body&gt;

&lt;/html&gt;
</code></pre>

<p><strong>Result of <code>cat ssl.conf | grep pma.tfconnections.com</code></strong></p>

<pre><code>SSLCertificateFile /etc/letsencrypt/live/pma.tfconnections.com/fullchain.pem
SSLCertificateKeyFile /etc/letsencrypt/live/pma.tfconnections.com/privkey.pem
ServerName pma.tfconnections.com
</code></pre>
","<ssl-certificate><subdomain><apache2><lets-encrypt><certbot>","2017-05-03 00:06:15"
"1021071","Set up a sub-domain for multiple Public IPs","<p>I hope you can help me.</p>
<p>I have 5 Public IP's in an subnet <code>186.121.200.X/29</code></p>
<p>I have <code>example.com</code> addressed to one of those public IPs, and some subdomains that point to the rest of them.</p>
<p>Now, I have other 5 Public IP's in another subnet <code>190.181.15.Y/29</code></p>
<p><strong>My question is:</strong></p>
<p><em>Can I configure <code>example.com</code> to also point to an IP of the subnet <code>190.181.15.Y/29</code> ?</em></p>
<p><em>How can I do it?</em></p>
<p>I have these configuration files:</p>
<pre><code>(named.conf.options)
options {
        directory &quot;/var/cache/bind&quot;;
        forwarders {
                // Google Public DNS (IPv4)
                8.8.8.8;
                8.8.4.4;
                // Google Public DNS (IPv6)
                2001:4860:4860::8888;
                2001:4860:4860::8844;
                // ADSL router
                186.121.200.X;
        };
        dnssec-validation auto;
        auth-nxdomain no;    # conform to RFC1035
        listen-on-v6 { any; };
};
(named.conf.local)
zone &quot;example.com&quot; {
        type master;
        file &quot;/etc/bind/db.direct&quot;;
        allow-query { any; };
};
zone &quot;206.121.186.in-addr.arpa&quot; {
        type master;
        file &quot;/etc/bind/db.reverse&quot;;
        allow-query { any; };
};
(db.direct)
$TTL    604800
@       IN      SOA     example.com. root.example.com. (
                             11         ; Serial
                         604800         ; Refresh
                          86400         ; Retry
                        2419200         ; Expire
                         604800 )       ; Negative Cache TTL
;
@               IN       NS     example.com.
@               IN       MX     10    mail
@               IN       A      186.121.206.X1
www             IN       A      186.121.206.X1
mail            IN       A      186.121.206.X2
subdomain1      IN       A      186.121.206.X3
subdomain2      IN       A      186.121.206.X4
subdomain3      IN       A      186.121.206.X5

(db.reverse)
$TTL    604800
@       IN      SOA     example.com. root.example.com. (
                              8         ; Serial
                         604800         ; Refresh
                          86400         ; Retry
                        2419200         ; Expire
                         604800 )       ; Negative Cache TTL
;
@       IN      NS      example.com.
X1     IN      PTR     ns.example.com.
X1     IN      PTR     www.example.com.
X2     IN      PTR     mail.example.com.
X3     IN      PTR     subdomain1.example.com.
X4     IN      PTR     subdomain2.example.com.
X5     IN      PTR     subdomain3.example.com.

</code></pre>
<p>EDIT:
I have tried doing the following:</p>
<pre><code>(db.direct)
...
subdomain4      IN       A      190.181.15.Y1
</code></pre>
<pre><code>(db.reverse)
...
Y1     IN      PTR     subdomain4.example.com.
</code></pre>
<p>When saving files and restarting bind, the subdomain responds to a ping.</p>
<p><code>ping subdomain4.example.com</code></p>
<p><code>PING subdomain4.example.com (190.181.15.Y1) 56(84) bytes of data.</code></p>
<p>But when accessing subdomain by browser, it redirects to Public IP. It is not kept with the name of the subdomain.</p>
<p>What is the problem?</p>
","<ip><bind>","2020-06-11 17:18:18"
"917412","Script to enable bitlocker in All Drive","<p>I am trying to enable bitlocker in all domain joined user machines in my office.</p>

<p>I have used a Widows task scheduler script to enable bitlocker in all machines.</p>

<p>But the below code is enabling bitlocker in C drive alone. </p>

<p>I need to enable this in all drive in the laptop. How do i proceed. I have attached the script below</p>

<pre><code>$TPM = Get-WmiObject win32_tpm -Namespace root\cimv2\security\microsofttpm | where {$_.IsEnabled().Isenabled -eq 'True'} -ErrorAction SilentlyContinue
$WindowsVer = Get-WmiObject -Query 'select * from Win32_OperatingSystem where (Version like ""6.2%"" or Version like ""6.3%"" or Version like ""10.0%"") and ProductType = ""1""' -ErrorAction SilentlyContinue
$BitLockerReadyDrive = Get-BitLockerVolume -MountPoint $env:SystemDrive -ErrorAction SilentlyContinue


#If all of the above prequisites are met, then create the key protectors, then enable BitLocker and backup the Recovery key to AD.
if ($WindowsVer -and $TPM -and $BitLockerReadyDrive) {

#Creating the recovery key
Start-Process 'manage-bde.exe' -ArgumentList "" -protectors -add $env:SystemDrive -recoverypassword"" -Verb runas -Wait

#Adding TPM key
Start-Process 'manage-bde.exe' -ArgumentList "" -protectors -add $env:SystemDrive  -tpm"" -Verb runas -Wait
sleep -Seconds 15 #This is to give sufficient time for the protectors to fully take effect.

#Enabling Encryption
Start-Process 'manage-bde.exe' -ArgumentList "" -on $env:SystemDrive -em aes256"" -Verb runas -Wait

#Getting Recovery Key GUID
$RecoveryKeyGUID = (Get-BitLockerVolume -MountPoint $env:SystemDrive).keyprotector | where {$_.Keyprotectortype -eq 'RecoveryPassword'} | Select-Object -ExpandProperty KeyProtectorID

#Backing up the Recovery to AD.
manage-bde.exe  -protectors $env:SystemDrive -adbackup -id $RecoveryKeyGUID

#Restarting the computer, to begin the encryption process
Restart-Computer}
</code></pre>
","<powershell><vbscript><bitlocker>","2018-06-20 05:34:21"
"917474","Apache URL Rewrite like IIS How To?","<p>Sorry if this is answered better somewhere but I have not got to grips with the answers I have seen via search (probably down to my search terms more than anything else)</p>

<p>I am new to trying anything to do with web tech and previously found happiness in figuring out how to host multiple sites on one IIS server so the technical bar is low with me.</p>

<p>A colleague has created an internal site with a bunch of buttoned links to various web tools we have on site, this led to a page that displayed awfully and we found an answer that was only workable if I switched the site to an Apache server. I have cobbled through and got that up and running once I got the basic hang of Virtual hosts and a range of other things.</p>

<p>Now I am trying to move the other sites over and have run into a snag with some of the sites that had URL Re-Writes on IIS. Our helpdesk site for instance, is hosted on a separate server and is run from a different port, so DNS points helpdesk to the webserver and IIS had URL re write that sent requests to the correct location. Users only ever saw Helpdesk in the URL followed by which ever page they were visiting, /tickets, /portal and so on.</p>

<p>I have managed to create a virtual host for the helpdesk with a re-direct but the end user then has the full local name and port number for the URL rather than ""helpdesk"". Also, any links to <a href=""http://helpdesk"" rel=""nofollow noreferrer"">http://helpdesk</a> get re-directed but if I add a particular page like /tickets, the link doesn't work directly. Obviously I am not doing something right but my searches are driving me nuts.</p>

<p>What do I need to do in apache to get the same results as I was getting via IIS with sites hosted on other internal servers? A step by step for simpletons would be most appreciated.</p>

<p>This is what I had in web.config which was created by the URL Re-write rules in IIS: (I have removed the preceding &lt; from each line so the data shows up in this post)</p>

<pre><code>&lt;xml version=""1.0"" encoding=""UTF-8""?&gt;

configuration&gt;

    system.webServer&gt;

        rewrite&gt;

            rules&gt;

                rule name=""Proxy"" enabled=""true"" patternSyntax=""Wildcard"" stopProcessing=""true""&gt;

                    match url=""*"" /&gt;

                    action type=""Rewrite"" url=""http://servername:9675/{R:1}"" /&gt;

                /rule&gt;

            /rules&gt;

        /rewrite&gt;

    /system.webServer&gt;

&lt;/configuration&gt;
</code></pre>

<p>And this is what I have created in the http-vhosts.conf for the same site:</p>

<pre><code>&lt;VirtualHost *:80&gt;

    ServerName helpdesk
    ServerAlias helpdesk

    RedirectPermanent / http://servername:9675

(commented out)    ServerAdmin admin@fakedomain.com

    ErrorLog ""logs/helpdesk-access.log""
    CustomLog ""logs/helpdesk-access.log"" common

&lt;/VirtualHost&gt;
</code></pre>
","<apache-2.4>","2018-06-20 13:41:25"
"848133","Is there a way to ssh using hostname alias without modifying the etc/hosts file?","<p>I spent hours researching how I can automate 100s of hosts that are all under one local domain. (i'm on a ubuntu linux 17.04 machine)</p>

<p>For example, I have node1.domain.org, node2.domain.org, and the list goes to node100.domain.org </p>

<p>it is practically impossible to edit the /etc/hosts file adding all the alias manually.</p>

<p>For example, </p>

<blockquote>
  <p>127.0.0.1       localhost</p>
  
  <p>127.0.1.1       harish</p>
  
  <h1>manually adding hostname aliases</h1>
  
  <p>10.2.1.3        node1.domain.org   node1</p>
  
  <h1>The following lines are desirable for IPv6 capable hosts</h1>
  
  <p>::1     ip6-localhost ip6-loopback</p>
  
  <p>fe00::0 ip6-localnet</p>
  
  <p>ff00::0 ip6-mcastprefix</p>
  
  <p>ff02::1 ip6-allnodes</p>
  
  <p>ff02::2 ip6-allrouters</p>
</blockquote>

<p>I tried the <code>ip route | grep default</code> and found that the gateway to be 10.1.10.1 but not clue if I should be on this gateway or of any further steps from here.</p>

<p>I tried <code>cat /etc/resolv.conf</code> and found the DNS Server I'm using</p>

<blockquote>
  <p>cat /etc/resolv.conf</p>
  
  <h1>Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8)</h1>
  
  DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN
  
  127.0.0.53 is the systemd-resolved stub resolver.
  
  run ""systemd-resolve --status"" to see details about the actual nameservers.
  
  <p>nameserver 10.2.254.254 nameserver 8.8.8.8 nameserver 127.0.0.53</p>
  
  <p>search domain.com</p>
</blockquote>

<p>I found the DNS that all my nodes are using</p>

<blockquote>
  <p>xcat node1 dns</p>
  
  <p>SERVER: 10.2.254.254#53(10.2.254.254)</p>
</blockquote>

<p>Both remote node dns and my localmachine dns matched. </p>

<p>However, No clue why it is domain.com in resolve.conf file and not domain.org as all the nodes are under domain.org and not domain.com (but it told it will overwrite when I try to edit it and it did.  I tried to run 'sudo /etc/init.d/networking restart' or 'sudo service network-manager restart' after updating it to domain.org and the file is overwritten). I observed that can SSH using all aliases such as <code>node1</code> without using <code>node1.domain.org</code> when I did this change but the file is always overwritten on system restart or when i go on to wifi connection instead of Ethernet connection.</p>

<p>I also tried verifying the <code>route</code> command and everything looks normal, still no clue</p>

<blockquote>
  <p>route </p>
  
  <p>Kernel IP routing table </p>
  
  <p>Destination     Gateway         Genmask Flags Metric Ref    Use Iface </p>
  
  <p>default         gateway         0.0.0.0 UG    100    0        0 enp2s0f1</p>
  
  <p>10.1.10.0       0.0.0.0         255.255.255.0   U     100    0        0 enp2s0f1 </p>
  
  <p>link-local      0.0.0.0         255.255.0.0     U     1000 0        0 enp2s0f1</p>
</blockquote>

<p>All I want is to 
ssh node1 instead of ssh node1.domain.org
or
ssh node50 instead of ssh node50.domain.org
or
do ssh into any node using its alias without adding it manually to /etc/hosts. I just want to find out a robust way to do this.</p>
","<ssh><hostname><route><hosts><hosts-file>","2017-05-03 23:48:40"
"917637","Can't log in MySQL with newly created user, but any username without password is accepted","<p>I installed MySQL 5.6 on Windows, imported a database (as root), and created a new user with a password to access this database.</p>

<p>Connexion with the username and password does not work (tested with Workbench).</p>

<p>However, if I connect without password, login is accepted, but I don't see the database. Actually, this works with <em>any</em> username.</p>

<p>How do I connect with the newly created user? And how do I prevent connections with any username and no password, which look like a security issue?</p>
","<mysql><authentication><mysql5.6>","2018-06-21 13:14:41"
"917639","Setting up a Windows domain CA with a wildcard certificate","<p>As the title suggests, I'm trying to set up a new Windows domain. Seemingly a simple thing, and one I've already done before, but this time around, management wants this to be synced up with Office 365 and generally be publicly accessible.</p>

<p>OK, well, we have the domain name registered, and we bought a wildcard SSL certificate to go along with this. I've already got the basic DC server ready.</p>

<p>Now, I'd like to set up a proper CA for this domain. I was hoping a wildcard certificate would be enough to let such a CA create valid certificates for whatever else I may need that will be in this domain.</p>

<p>Unfortunately, I'm having some issue, and I can't really find a guide on how to do this. Specifically, when configuring the CA I can't actually use the certificate (""imported certificate does not match chosen CA type""). So, is what I'm planning to do even possible? It seems like it should, but the error suggests otherwise.</p>

<p>Can someone point me in the right direction, please?</p>

<p>EDIT:<br>
I've just found <a href=""https://serverfault.com/questions/650119/wildcard-cert-for-local-ssl-certificate-authority"">this ServerFault question</a>, which appears to answer the basic question of ""can I use the wildcard to sign new certificates?"". The answer, sadly, appears to be ""no"". That said, I'd still like to know what my options are to make things nice and smooth on the new domain.</p>
","<ssl-certificate><windows-server-2016><certificate-authority>","2018-06-21 13:18:11"
"917644","AWS VPC: Reserve an external IP for VPN","<p>I need to fill in a document from a customer company. They need to know an external IP address for the VPN. 
We are using AWS VPCs but I can't find the option to reserve IP addresses for later use or even to change the IP addresses on an existing VPN so I can create a dummy VPN for later usage.</p>

<p>Can anyone help me with that?</p>

<blockquote>
  <h3>Update 1</h3>
  
  <p>Elastic IPs are no help here, they can only be attached to EC2 instances but creating an VPG or an VPN doesn't let you choose an EIP</p>
</blockquote>

<hr>

<blockquote>
  <h3>Update 2</h3>
  
  <p>It seems that there are no options for doing this easily. A Solution I could think of are creating an ec2 instance as a router (with an eip accessed over internet gateway). But this seems pretty complicated. I've decided to ask the customer to change the form blueprint and allow DNS names for remote VPN endpoints. </p>
</blockquote>

<p>BTW: I really don't know why this question was downvoted 2 times. It seems that people are full of troll-like mindsets instead of trying to help</p>
","<amazon-web-services><vpn><ip>","2018-06-21 13:37:55"
"1021484","How do I ensure that a CRM On-Prem server is running TLS 1.2","<p>We have a few servers here running CRM 2016 On-Premise, and we need to make sure that they are running TLS 1.2 as TLS 1.0 and TLS 1.1 is about to stop working as far as i understand it.</p>
<p>How can I make sure that this is the case? The CRM solutions are running custom workflows, but I don't believe any of them is coded to specifically run TLS 1.0 / 1.1. So is there any global setting I can set somewhere that I can use, and is there a way to test it out before and after I change the setting to make sure it running TLS 1.2 after the change?</p>
","<ssl><microsoft-dynamics-crm>","2020-06-15 10:25:56"
"848376","Cannot ping any Windows device on local network, but can ping *nix","<p>Strange issue started yesterday: I cannot ping any Windows PC on my local network.</p>

<p>I have (2) Windows 7 Pro devices, one of which was reimaged just before this issue started; (1) Windows 10 Education device; (1) Linux Mint 17.3 device; and an Android 7.0 device.  No matter where I initiate a ping from, including my gateway, I cannot hit any of my Windows devices.  I can hit my Linux and Android devices fine from any of the others though, including the Windows ones.</p>

<p>I have tried turning off A/V and Firewalls; rebooting all devices involved, including the router; flushing DNS from the Windows PC's; I have looked for rules in the router and on the devices that might be blocking ICMP; and I have found nothing.</p>

<p>Unfortunately, I rebooted the router before pulling logs from it... so those are gone.  I have not made any changes to these devices (other than a clean image on one of the Windows 7 PC's), nor have I made changes to my router.</p>

<p>Everything worked fine until yesterday evening.  I RDP'd into the freshly imaged PC to start reinstalling software and configuring the OS, etc. AVG on that device froze while I was configuring it and I had to hard boot the PC.  When it came back up I was unable to RDP it again or ping it.  Today I realized that I couldn't ping any of the Windows PC on my network.  Also, strange: I use xrdp on the Linux box and I can connect but not login to the box.  SSH on that box works just fine though (I even tried to tunnel RDP through SSH and still no go).  All of these devices connect to the internet just fine.  They can ping any non-Windows device on the network.</p>

<p>Any suggestions or guidance would be greatly appreciated.  Thanks in advance!</p>

<p>Update: I connected my work laptop that has not been on my network since the issue first arose.  It did not respond to a ping, nor did it receive a response from any other Windows devices, but it could ping my Linux box.  So, I performed a factory reset on my router... did not make a difference.</p>
","<windows><networking><troubleshooting>","2017-05-05 02:44:15"
"848423","Virtualize SCO 5v6.0.0 P2V","<p>I try to virtualize old SCO server and i Have a little problem with that.</p>

<p>When i try to startup VM i see somethink like that:
<a href=""https://i.sstatic.net/uvJZb.jpg"" rel=""nofollow noreferrer"">SCO error</a></p>

<p>Physical server: HP ProLiant DL380 G5 with six harddisk's ( Combined into raid ) - On this server we have SCO 5v6.0.0 </p>

<p>And i Try to virtualize this into Esxi. I do a backup by aomei backupper professional, next i create a virtual machine and recovery sco from backup. I see welcome screen from SCO and after 5 minutes i see error</p>

<p>Anyone do some think like that or have some experience and can help me ?</p>

<hr>

<p>SCO 5v6.0.0 - i found that is SCO openserver 6 propably without any Maintenance Pack's ( I don't know how to check the version correctly i only found command that show this version - and yes i search on google )</p>

<blockquote>
  <p>To be honest I'd suggest just installing a modern version of ESXi on a
  modern server then try to convert the functionality of this existing
  box over to a VM based on a modern OS</p>
</blockquote>

<p>My first thing is to do that :) but unfortunetly I can't. We have a ERP ( 3rd party company implemented it for us ) on this server with very important data.</p>

<blockquote>
  <p>The HP DL380 G5 is only supported on ESXi 5.0u3and lower and although
  I'm not exactly sure what you mean by 'SCO 5v6.0.0' the only SCO
  support with ESXi 5.0u3 is for OpenServer 5 and UnixWare 7.</p>
</blockquote>

<p>Yes, but you can choose a compatibility when you create a VM, or I'm wrong ? And by the way I have Esxi 6.5</p>

<blockquote>
  <p>That said it appears your problem relates to the VM not providing a HP
  CCISS compatible virtual disk controller - which it's trying to boot
  from - you'd need to install vmtools</p>
</blockquote>

<p>Can I install vmtools on that VM even system don't boot ?</p>

<p>So many questions :) I just asked if anyone has already virtualized SCO and has some experience in this topic :)</p>
","<linux><virtualization><vmware-esxi><physical-to-virtual><convert>","2017-05-05 10:01:25"
"1021555","how to filter dns requests with iptables","<p>I am trying to filter the dns requests from my local network. Only authorize requests to specific dns and deny the rest, but it has not worked for me. This is my rule (with dns google example):</p>
<pre><code>internal=enp2s1
external=enp2s0

iptables -P INPUT ACCEPT
iptables -P FORWARD ACCEPT
iptables -P OUTPUT ACCEPT

echo 1 &gt; /proc/sys/net/ipv4/ip_forward # default 0
echo 1 &gt; /proc/sys/net/ipv6/conf/all/disable_ipv6 # default 0
echo 1 &gt; /proc/sys/net/ipv6/conf/default/disable_ipv6 # default 0
echo 1 &gt; /proc/sys/net/ipv6/conf/lo/disable_ipv6 # default 0

iptables -A INPUT -p all -i lo -j ACCEPT
iptables -A INPUT -s 192.168.0.10 -j ACCEPT
iptables -A OUTPUT -p all -o lo -j ACCEPT
iptables -A OUTPUT -p all -s 127.0.0.1 -j ACCEPT
iptables -t mangle -A PREROUTING -p all -i lo -j ACCEPT
iptables -t mangle -A PREROUTING -p all -s 127.0.0.1 -j ACCEPT
iptables -t nat -A PREROUTING -p all -i lo -j ACCEPT
iptables -t mangle -A PREROUTING -i lo -s 127.0.0.0/8 -j ACCEPT
iptables -t mangle -A PREROUTING -i lo -d 127.0.0.0/8 -j ACCEPT

iptables -t mangle -A PREROUTING -i $internal -s 255.255.255.0/32 -j ACCEPT
iptables -t mangle -A PREROUTING -i $internal -d 255.255.255.0/32 -j ACCEPT

iptables -t mangle -A PREROUTING -i $internal -s 192.168.0.0/24 -j ACCEPT
iptables -t mangle -A PREROUTING -i $internal -d 192.168.0.0/24 -j ACCEPT

iptables -t mangle -A PREROUTING -p udp --dport 853 -j DROP
iptables -t mangle -A PREROUTING -p tcp --dport 853 -j DROP

dns=&quot;8.8.8.8 8.8.4.4&quot;
for ip in $dns; do
   iptables -A INPUT -s $ip -p udp --sport 53 -m state --state RELATED,ESTABLISHED -j ACCEPT
   iptables -A OUTPUT -d $ip -p udp --dport 53 -m state --state RELATED,ESTABLISHED -j ACCEPT
   iptables -A FORWARD -d $ip -p udp --dport 53 -m state --state RELATED,ESTABLISHED -j ACCEPT
done
iptables -A FORWARD -p udp --dport 53 -j REJECT

</code></pre>
<p>For example, if I put manually cloudflare dns on a PC on my local network (1.1.1.1 1.0.0.1) the PC has internet access</p>
<p>PD:</p>
<ol>
<li><strike>The rule &quot;-m state --state RELATED,ESTABLISHED&quot; is in <a href=""https://serverfault.com/a/209019/480349"">this question selected as correct</a></strike></li>
<li><strike>The blocking rule is in <a href=""https://superuser.com/a/1429139/886205"">this question selected as correct</a></strike></li>
<li>I have tried the same blocking rule on all chains (INPUT, Mangle, OUTPUT, FORWARD) and change REJECT with DROP and it does not block</li>
<li>I added additional blocking rules for src and it also does not block. Example:</li>
</ol>
<pre><code>iptables -A FORWARD -s $ip -p udp --sport 53 -m state --state RELATED,ESTABLISHED -j ACCEPT
# and block
iptables -A FORWARD -p udp --sport 53 -j DROP
</code></pre>
<p><em><strong>Note</strong>: These rules are also for TCP (but not to repeat them I do not put them)</em></p>
<p><strong>Update:</strong></p>
<p>I changed dns rule to:</p>
<pre><code>dns=&quot;8.8.8.8 8.8.4.4&quot;
for ip in $dns; do
   iptables -A INPUT -s $ip -p udp --sport 53 -j ACCEPT
   iptables -A OUTPUT -d $ip -p udp --dport 53 -j ACCEPT
   iptables -A FORWARD -d $ip -p udp --dport 53 -j ACCEPT
done
iptables -A INPUT -p udp --sport 53 -j DROP
iptables -A OUTPUT -p udp --dport 53 -j DROP
iptables -A FORWARD -p udp --dport 53 -j DROP

</code></pre>
<p>But it doesn't do the blocking correctly either</p>
<p><em><strong>Note</strong>: INPUT rule with or without &quot;-m state --state RELATED,ESTABLISHED -j ACCEPT&quot; it is irrelevant because what interests me is to block the connection and it is what does not happen</em></p>
<p>thanks</p>
","<iptables><ubuntu-20.04>","2020-06-15 18:37:57"
"1021556","what means service VIP?","<p><a href=""https://i.sstatic.net/82HFG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/82HFG.png"" alt=""enter image description here"" /></a></p>
<p>Hi,
I try to understand the image above but I can't understand what VIP service is.</p>
<p>Is this the virtual IP address of the battery?</p>
<p><a href=""https://i.sstatic.net/3s58K.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3s58K.png"" alt=""enter image description here"" /></a></p>
<p>And there on the image above, I see IP LB. I imagine this is the real address of the LB.</p>
","<haproxy>","2020-06-15 18:52:45"
"848461","IIS recovery script on ping failure","<p>we have running webapp on IIS. Sometimes IIS does not respond to ping and we have to restart it manually.</p>

<p>Can you help me with recovery script doing these steps after ping related hangs occurs:</p>

<ol>
<li>create full dump of IIS and WAS process</li>
<li>restart IIS</li>
<li>send email or write event to windows log</li>
</ol>

<p>Thank you</p>
","<iis>","2017-05-05 13:27:48"
"848528","Office 2016 fails installation on Win 10","<p>I'm loosing my cool over this. I'm installing Office 2016 Standard (VLSP) license on Windows 10 Pro (Lenovo stock installed). Installation keeps failing. Same thing happened when I tried to install Office 2013 Standard on the same machine. Only Office 2010 would install properly. System had preinstalled Office 2016 Click-to-install which I since removed. I tried everything I could possibly find on the web including manual uninstall of Click-to-Install, removing folders from Task Scheduler and hot-fixing Windows Update, still to no success. Relevant setup log shown below. Please help!</p>

<pre><code>MSI(INFO): 'CAQuietExec:  ERROR: The system cannot find the path specified.' 
MSI(INFO): 'CAQuietExec:  ' 
MSI(INFO): 'CAQuietExec:  Error 0x80070001: Command line returned an error.' 
MSI(INFO): 'CAQuietExec:  Error 0x80070001: CAQuietExec Failed' 
MSI(INFO): 'CustomAction CAOSMRegisterLogonTask.x64.1033 returned actual error code 1603 
           (note this may not be 100% accurate if translation happened inside sandbox)' 
MSI(INFO): 'Action ended 11:39:23: InstallExecute. Return value 3.' 
MSI(COMMONDATA): 'Message type:  2, Argument:  0' 
MSI(COMMONDATA): 'Message type:  2, Argument:  0' 
MSI(COMMONDATA): 'Message type:  0, Argument:  1033' 
MSI(COMMONDATA): 'Message type:  1, Argument:  Microsoft Office OSM MUI (English) 2016'
</code></pre>
","<windows-10><microsoft-office-2016>","2017-05-05 18:21:39"
"775807","Problems with routing","<p>I have issue achieving this:   A-->B-->C-->internet (doing A--C-->internet is working)</p>

<ul>
<li>A: client lan: 192.168.57.0/24 (example 192.168.57.50)</li>
<li>B: ubuntu GW eth0: 172.10.0.1 (/24, default gw:172.10.0.254) eth1:
192.168.57.1 (/24) eth3 10.152.152.0 (/18)</li>
<li>C: whonix-gw eth0: 10.0.0.1 eth1: 10.152.152.10</li>
</ul>

<p>At the beginning I tried using ipbales and tried <code>DNAT</code>, <code>MASQUERADE</code>, etc. but was not able to achieve the goal.
Then I tried with PBR (policy based routing), so created a new table and added the default gw and the policy with no luck.</p>

<pre><code>echo 200 John &gt;&gt; /etc/iproute2/rt_tables
ip rule add from 192.168.57.1 table John  #tried also adding the subnet here
ip route add default via 10.152.152.10 dev eth3 table John
ip route flush cache
</code></pre>
","<ubuntu><policy-routing>","2016-05-10 07:56:35"
"1021886","ERROR: Unrecognized TLD for nameserver ns1.111.222.333.444","<p>I have a virtual private server(VPS) with some IP address like <code>http://111.222.333.444/</code>, and wanted to publish my Angular-NodeJS project on it, so I bought a domain with default nameservers like <code>ns1.example.com , ns2.example.com</code>. Then I tried to change the default nameservers to the followings:</p>
<pre><code>ns1.111.222.333.444
ns2.111.222.333.444
</code></pre>
<p>But I get this error message:</p>
<pre><code>ERROR: Unrecognized TLD for nameserver ns1.111.222.333.444
</code></pre>
<p>What is the problem and how can I connect my VPS to the domain I have bought?</p>
","<domain><vps><nameserver>","2020-06-17 14:54:52"
"848541","FTP connection is succssful via finder in mac, but not through filezilla and terminal","<p>Modifying my question.</p>

<p>There are 2 issues :</p>

<p>1) FTP server in windows server 2012 R2 does not work.</p>

<p>I went through most of the articles and forums and discussion regarding 1st error. But when i try to connect to FTP to the server via safari browser, FTP is successful and get access through finder. But not via terminal or filezilla. </p>

<p>In command line, login is successful, but not able to enter passive mode. Mac used advanced passive mode. But not successful in windows PC</p>

<p>2) FTP server created using Filezilla uses advanced passive mode and does not use the port numbers specified (55000-65000).It is good to know reason for this. But my primary concern is to make default windows FTP to work.</p>
","<windows-server-2012-r2><ftp><terminal>","2017-05-05 19:38:03"
"775885","Binding domain and SMTP server on a foreign server","<p>I have a trouble with my server setup, I'm trying to bind both IP domain + his  SMTP server on the same external server.
However mxtoolbox still give me the same error : ""Reverse DNS is not a valid Hostname""</p>

<p>I uploaded some pictures, so it is easier to understand my problem.</p>

<p>1) The main domain name belongs to OVH (hosting service)</p>

<p>2) External server configuration:</p>

<p><a href=""https://i.sstatic.net/t4Lvf.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/t4Lvf.jpg"" alt=""Here""></a></p>

<p>3) I added DNS settings on plesk.</p>

<p>4) Here the result on mxtoolbox:</p>

<p><a href=""https://i.sstatic.net/EsIim.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EsIim.jpg"" alt=""Here""></a></p>

<p>In this way I cannot send some email via SMTP without be considered as spam by  gmail or hotmail. I could be more specific.</p>
","<domain-name-system><windows-server-2008-r2><smtp><plesk>","2016-05-10 13:38:57"
"918104","How can I know how much Google Cloud Platform had been charged or need from me in the billing aspects?","<p>I have an account inside GCP, and need to know exactly what GCP need from me as a charge or from my MasterCard balance so far? How can I do that as the amount it said 1.x K USD which is too high I guess.</p>

<p>If someone can help, this will be appreciated.</p>
","<security><google-cloud-platform>","2018-06-25 15:05:13"
"848806","How secure is a headless linux server with passwordless sudoers accounts","<p>The only means of accessing the server console is via ssh, using keys. Also via exploits of various services the server is running.</p>

<p>My administrative account (belongs to sudo group) has no password, but root does.</p>

<p>Is that considered unsecure?</p>
","<linux><security>","2017-05-08 10:35:19"
"849007","failed to start LSB :Bring Up down Networking","<p>I am new to centos 7 and I am configuring a static IP on centos 7, so I have edited the file <code>/etc/sysconfig/network-scipts/ifcfg-eth0</code> as following</p>

<pre><code>TYPE=Ethernet
BOOTPROTO=none
Device=eth0
ONBBOOT=yes
IPADDR=192.168.4.196
NETMASK=255.255.255.0
GATEWAY=192.168.88.254
DNS1=8.8.8.8
USERCTL=no
</code></pre>

<p>but when I issue the command</p>

<pre><code>systemctl restart network 
</code></pre>

<p>I am getting the error</p>

<pre><code>failed to start LSB :/Bring Up down Networking
</code></pre>

<p><code>ip route</code> show gives me no output</p>

<p>and I have applied the solution that stops networkmanager with the same existing error.</p>

<p>I am able to configure a dynamic dhcp and get a dynamic IP address but not static one what can be possible solutions ?</p>
","<centos><centos7>","2017-05-09 10:07:05"
"776361","Redirecting with a reverse proxy nginx","<p>I have a reverse proxy nginx that listens on port 8888. The frontend is listening to 8000. The problem is the following: the browser is redirected to <code>http://frontend:8000/user/username</code>. At this stage, nginx is supposed to edit the replied <code>Location</code> HTTP header to redirect towards the same URL, but with an ending <code>/</code>.</p>

<p>A trivial rewrite redirect directive unfortunately changes the port to the nginx port, so I end up loading <code>http://frontend:8888/user/username/</code>. Adding <code>port_in_redirect off</code> does not solve, because the port is stripped and I end up to <code>http://frontend/user/username</code></p>

<p>Currently I have (the nginx server is on a localhost docker container)</p>

<pre><code>location = /user/username {
    proxy_redirect http://127.0.0.1:8888/user/username /user/username/;
}
</code></pre>

<p>but this one gives me a 404, because apparently it is not doing a redirection at all, and it's trying to serve me a file</p>

<pre><code>2016/05/12 12:12:16 [error] 186#0: *1 open()     ""/usr/share/nginx/html/user/username"" failed (2: No such file or directory), client: 10.0.42.1, server: , request: ""GET /user/username HTTP/1.1"", host: ""172.17.5.168:8000""
</code></pre>
","<nginx>","2016-05-12 12:34:07"
"849029","Postgres DB HDD needs expanding","<p>I have an Azure VM running Postgres on Unix, supplied by Bitnami: PostgreSQL 9.6.2-0 (Ubuntu 14.04)</p>
<p>The DB works fine and I am populating it, but after some time, the DB reaches about 25GB in size and then my code (populating the DB) crashes...</p>
<pre><code>psycopg2.OperationalError: could not extend file &quot;base/16450/79079&quot;: No space left on device
HINT:  Check free disk space.
</code></pre>
<p>I thought it would be a simple resize of my VM. Nope. The VM now has a HDD size of 400GB. Plenty of space.</p>
<p>I then tried my code and again, at 25GB, boom. No space.</p>
<p>The next thing I did was log into the VM via SSH and check the disks with <code>df -h</code>:</p>
<p><a href=""https://i.sstatic.net/1wtNn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1wtNn.png"" alt=""enter image description here"" /></a></p>
<p>As can be seen above, <code>/dev/sda1</code> is 30GB in size, of which 29GB is used. I suspect my Postgres is using this HDD... set automatically when I used the Bitnami Azure template.</p>
<p>You can see the 400GB HDD that I have set via the Azure portal: <code>/dev/sdb1</code>.</p>
<p>What can I do? I need a lot more data to go into my Postgres DB... and there is no visible way to change the size of <code>/dev/sda1</code> via the portal.</p>
<p><strong>Update</strong></p>
<p>As requested, here is the contents of fstab before modification (default):
<a href=""https://i.sstatic.net/OXV5G.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/OXV5G.png"" alt=""enter image description here"" /></a></p>
<p>I changed <code>/mnt</code> to <code>/opt/bitnami/postgresql/data</code> (according to the path: <a href=""https://stackoverflow.com/a/8237512/596841"">https://stackoverflow.com/a/8237512/596841</a>). When I updated, saved, quit and rebooted the server, it went back to the default (as seen above).</p>
","<azure><postgresql><ubuntu-14.04><bitnami>","2017-05-09 12:08:04"
"918474","Squid 3.5.20 doesn't authentificate via Active Directory and Kerberos","<p>I make transparent proxy via AD and Kerberos V5. CentOS joined to Windows domain with realm:</p>

<pre><code>[root@vs-otr-squid02 ~]# realm list
domain.ru
  type: kerberos
  realm-name: DOMAIN.RU
  domain-name: domain.ru
  configured: kerberos-member
  server-software: active-directory
  client-software: sssd
  required-package: oddjob
  required-package: oddjob-mkhomedir
  required-package: sssd
  required-package: adcli
  required-package: samba-common-tools
  login-formats: %U@domain.ru
  login-policy: allow-realm-logins
</code></pre>

<p>Squid info:</p>

<pre><code>Squid Cache: Version 3.5.20
Service Name: squid
configure options:  '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--disable-strict-error-checking' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-logdir=$(localstatedir)/log/squid' '--with-pidfile=$(localstatedir)/run/squid.pid' '--disable-dependency-tracking' '--enable-eui' '--enable-follow-x-forwarded-for' '--enable-auth' '--enable-auth-basic=DB,LDAP,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,SMB_LM,getpwnam' '--enable-auth-ntlm=smb_lm,fake' '--enable-auth-digest=file,LDAP,eDirectory' '--enable-auth-negotiate=kerberos' '--enable-external-acl-helpers=file_userip,LDAP_group,time_quota,session,unix_group,wbinfo_group,kerberos_ldap_group' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' '--enable-delay-pools' '--enable-epoll' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl-crtd' '--enable-storeio=aufs,diskd,rock,ufs' '--enable-wccpv2' '--enable-esi' '--enable-ecap' '--with-aio' '--with-default-user=squid' '--with-dl' '--with-openssl' '--with-pthreads' '--disable-arch-native' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic -fpie' 'LDFLAGS=-Wl,-z,relro  -pie -Wl,-z,relro -Wl,-z,now' 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic -fpie' 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig'
</code></pre>

<p>Keytab file content:</p>

<pre><code>slot KVNO Principal
---- ---- ---------------------------------------------------------------------
   1    3               HTTP/vs-otr-squid02@DOMAIN.RU
</code></pre>

<p>AD user - squid2018, for it this keytab file made. Make sure:</p>

<pre><code>[root@vs-otr-squid02 ~]# kinit HTTP/vs-otr-squid02@DOMAIN.RU
Password for HTTP/vs-otr-squid02@DOMAIN.RU:
</code></pre>

<p>The passwrod for squid2018 accepted, recieved ticket:</p>

<pre><code>[root@vs-otr-squid02 ~]# klist
Ticket cache: KEYRING:persistent:0:0
Default principal: HTTP/vs-otr-squid02@DOMAIN.RU

Valid starting       Expires              Service principal
06/27/2018 12:05:09  06/27/2018 22:05:09  krbtgt/DOMAIN.RU@DOMAIN.RU
        renew until 07/04/2018 12:04:52
</code></pre>

<p>Delete it.</p>

<p>Settings /etc/squid/squid.conf</p>

<pre><code># Active Directory
auth_param negotiate program /usr/lib64/squid/negotiate_kerberos_auth -s HTTP/vs-otr-squid02@DOMAIN.RU
auth_param negotiate children 600
auth_param negotiate keep_alive off

external_acl_type ad_group_member_check ttl=120 %LOGIN /usr/lib64/squid/ext_ldap_group_acl -d -v3 -P -R -K -b ""DC=domain,DC=ru"" -D ""vs-otr-squid02@domain.ru"" -w VerySecretPassword -f ""(&amp;(objectclass=person)(sAMAccountName=%v)(memberOf=cn=%g,CN=Domain Users,CN=Users,DC=domain,DC=ru))"" -h hs-dc-1.domain.ru

# Authenticate 
acl auth proxy_auth REQUIRED

acl ad_users external ad_group_member_check full_access

# Standart ports
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

# Standart permisson
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localhost
http_access allow auth

# Permission for AD users
http_access allow ad_users

# Deny everything else
http_access deny all

# Proxy ports
http_port 172.31.4.64:3128
http_port 172.31.4.64:3127 transparent

# Cache settings
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

# DNS
dns_nameservers 172.31.2.113
dns_v4_first on
</code></pre>

<p>Squid work status: </p>

<pre><code>[root@vs-otr-squid02 ~]# systemctl status squid
● squid.service - Squid caching proxy
   Loaded: loaded (/usr/lib/systemd/system/squid.service; enabled; vendor preset: disabled)
   Active: active (running) since Wed 2018-06-27 11:31:08 MSK; 42min ago
  Process: 2753 ExecStop=/usr/sbin/squid -k shutdown -f $SQUID_CONF (code=exited, status=0/SUCCESS)
  Process: 2762 ExecStart=/usr/sbin/squid $SQUID_OPTS -f $SQUID_CONF (code=exited, status=0/SUCCESS)
  Process: 2756 ExecStartPre=/usr/libexec/squid/cache_swap.sh (code=exited, status=0/SUCCESS)
 Main PID: 2765 (squid)
   CGroup: /system.slice/squid.service
           ├─2765 /usr/sbin/squid -f /etc/squid/squid.conf
           ├─2767 (squid-1) -f /etc/squid/squid.conf
           ├─2768 (ext_ldap_group_acl) -d -v3 -P -R -K -b DC=domain,DC=ru -D vs-otr-squid02@domain.ru -w VerySecretPassword -f (&amp;(objectclass=person)(sAMAccountName=%v)(memberOf=cn=%g,CN=Domain Users,CN=Users,DC=domain,DC=ru)) -h hs-dc-1.domain.ru
           ├─2769 (ext_ldap_group_acl) -d -v3 -P -R -K -b DC=domain,DC=ru -D vs-otr-squid02@domain.ru -w VerySecretPassword -f (&amp;(objectclass=person)(sAMAccountName=%v)(memberOf=cn=%g,CN=Domain Users,CN=Users,DC=domain,DC=ru)) -h hs-dc-1.domain.ru
           ├─2770 (ext_ldap_group_acl) -d -v3 -P -R -K -b DC=domain,DC=ru -D vs-otr-squid02@domain.ru -w VerySecretPassword -f (&amp;(objectclass=person)(sAMAccountName=%v)(memberOf=cn=%g,CN=Domain Users,CN=Users,DC=domain,DC=ru)) -h hs-dc-1.domain.ru
           ├─2771 (ext_ldap_group_acl) -d -v3 -P -R -K -b DC=domain,DC=ru -D vs-otr-squid02@domain.ru -w VerySecretPassword -f (&amp;(objectclass=person)(sAMAccountName=%v)(memberOf=cn=%g,CN=Domain Users,CN=Users,DC=domain,DC=ru)) -h hs-dc-1.domain.ru
           ├─2772 (ext_ldap_group_acl) -d -v3 -P -R -K -b DC=domain,DC=ru -D vs-otr-squid02@domain.ru -w VerySecretPassword -f (&amp;(objectclass=person)(sAMAccountName=%v)(memberOf=cn=%g,CN=Domain Users,CN=Users,DC=domain,DC=ru)) -h hs-dc-1.domain.ru
           └─2773 (logfile-daemon) /var/log/squid/access.log

Jun 27 11:31:08 vs-otr-squid02 systemd[1]: Starting Squid caching proxy...
Jun 27 11:31:08 vs-otr-squid02 squid[2765]: Squid Parent: will start 1 kids
Jun 27 11:31:08 vs-otr-squid02 squid[2765]: Squid Parent: (squid-1) process 2767 started
Jun 27 11:31:08 vs-otr-squid02 systemd[1]: Started Squid caching proxy.
</code></pre>

<p>But the browser shows a window for entering the name and password, which should not be. Okay, I am input, but it is not accepted, and in /var/log/squid/cache.log:</p>

<pre><code>2018/06/27 12:19:16 kid1| Accepting NAT intercepted HTTP Socket connections at local=172.31.4.64:3127 remote=[::] FD 24 flags=41
2018/06/27 12:19:29| Current Directory is /
2018/06/27 12:19:29 kid1| Preparing for shutdown after 0 requests
2018/06/27 12:19:29 kid1| Waiting 30 seconds for active connections to finish
2018/06/27 12:19:29 kid1| Closing HTTP port 172.31.4.64:3128
2018/06/27 12:19:29 kid1| Closing HTTP port 172.31.4.64:3127
2018/06/27 12:19:29 kid1| Current Directory is /
2018/06/27 12:19:29 kid1| Starting Squid Cache version 3.5.20 for x86_64-redhat-linux-gnu...
2018/06/27 12:19:29 kid1| Service Name: squid
2018/06/27 12:19:29 kid1| Process ID 2858
2018/06/27 12:19:29 kid1| Process Roles: worker
2018/06/27 12:19:29 kid1| With 16384 file descriptors available
2018/06/27 12:19:29 kid1| Initializing IP Cache...
2018/06/27 12:19:29 kid1| DNS Socket created at [::], FD 8
2018/06/27 12:19:29 kid1| DNS Socket created at 0.0.0.0, FD 10
2018/06/27 12:19:29 kid1| Adding nameserver 172.31.2.113 from squid.conf
2018/06/27 12:19:29 kid1| helperOpenServers: Starting 0/600 'negotiate_kerberos_auth' processes
2018/06/27 12:19:29 kid1| helperStatefulOpenServers: No 'negotiate_kerberos_auth' processes needed.
2018/06/27 12:19:29 kid1| helperOpenServers: Starting 5/5 'ext_ldap_group_acl' processes
2018/06/27 12:19:29 kid1| Logfile: opening log daemon:/var/log/squid/access.log
2018/06/27 12:19:29 kid1| Logfile Daemon: opening log /var/log/squid/access.log
2018/06/27 12:19:29 kid1| Local cache digest enabled; rebuild/rewrite every 3600/3600 sec
2018/06/27 12:19:29 kid1| Store logging disabled
2018/06/27 12:19:29 kid1| Swap maxSize 0 + 262144 KB, estimated 20164 objects
2018/06/27 12:19:29 kid1| Target number of buckets: 1008
2018/06/27 12:19:29 kid1| Using 8192 Store buckets
2018/06/27 12:19:29 kid1| Max Mem  size: 262144 KB
2018/06/27 12:19:29 kid1| Max Swap size: 0 KB
2018/06/27 12:19:29 kid1| Using Least Load store dir selection
2018/06/27 12:19:29 kid1| Current Directory is /
2018/06/27 12:19:29 kid1| Finished loading MIME types and icons.
2018/06/27 12:19:29 kid1| HTCP Disabled.
2018/06/27 12:19:29 kid1| Squid plugin modules loaded: 0
2018/06/27 12:19:29 kid1| Adaptation support is off.
2018/06/27 12:19:29 kid1| Accepting HTTP Socket connections at local=172.31.4.64:3128 remote=[::] FD 23 flags=9
2018/06/27 12:19:29 kid1| Accepting NAT intercepted HTTP Socket connections at local=172.31.4.64:3127 remote=[::] FD 24 flags=41
2018/06/27 12:19:30 kid1| storeLateRelease: released 0 objects
2018/06/27 12:19:46 kid1| Starting new negotiateauthenticator helpers...
2018/06/27 12:19:46 kid1| helperOpenServers: Starting 1/600 'negotiate_kerberos_auth' processes
2018/06/27 12:19:46 kid1| ERROR: Negotiate Authentication validating user. Result: {result=BH, notes={message: received type 1 NTLM token; }}
2018/06/27 12:19:50 kid1| ERROR: Negotiate Authentication validating user. Result: {result=BH, notes={message: received type 1 NTLM token; }}
2018/06/27 12:19:51 kid1| ERROR: Negotiate Authentication validating user. Result: {result=BH, notes={message: received type 1 NTLM token; }}
2018/06/27 12:19:51 kid1| ERROR: Negotiate Authentication validating user. Result: {result=BH, notes={message: received type 1 NTLM token; }}
2018/06/27 12:19:55 kid1| ERROR: Negotiate Authentication validating user. Result: {result=BH, notes={message: received type 1 NTLM token; }}
2018/06/27 12:19:55 kid1| ERROR: Negotiate Authentication validating user. Result: {result=BH, notes={message: received type 1 NTLM token; }}
2018/06/27 12:19:57 kid1| ERROR: Negotiate Authentication validating user. Result: {result=BH, notes={message: received type 1 NTLM token; }}
2018/06/27 12:19:59 kid1| ERROR: Negotiate Authentication validating user. Result: {result=BH, notes={message: received type 1 NTLM token; }}
2018/06/27 12:20:00 kid1| ERROR: Negotiate Authentication validating user. Result: {result=BH, notes={message: received type 1 NTLM token; }}
2018/06/27 12:20:03 kid1| ERROR: Negotiate Authentication validating user. Result: {result=BH, notes={message: received type 1 NTLM token; }}
2018/06/27 12:20:06 kid1| ERROR: Negotiate Authentication validating user. Result: {result=BH, notes={message: received type 1 NTLM token; }}
</code></pre>

<p>/var/log/squid/access.log</p>

<pre><code>1530088115.039      4 172.31.10.71 TCP_DENIED/407 4164 CONNECT yandex.ru:443 - HIER_NONE/- text/html
1530088115.039      4 172.31.10.71 TCP_DENIED/407 4164 CONNECT yandex.ru:443 - HIER_NONE/- text/html
1530088115.039      4 172.31.10.71 TCP_DENIED/407 4176 CONNECT yastatic.net:443 - HIER_NONE/- text/html
1530088115.039      4 172.31.10.71 TCP_DENIED/407 4164 CONNECT yandex.ru:443 - HIER_NONE/- text/html
1530088115.039      4 172.31.10.71 TCP_DENIED/407 4164 CONNECT yandex.ru:443 - HIER_NONE/- text/html
1530088115.039      4 172.31.10.71 TCP_DENIED/407 4176 CONNECT yastatic.net:443 - HIER_NONE/- text/html
1530088115.039      4 172.31.10.71 TCP_DENIED/407 4176 CONNECT yastatic.net:443 - HIER_NONE/- text/html
1530088115.039      4 172.31.10.71 TCP_DENIED/407 4176 CONNECT yastatic.net:443 - HIER_NONE/- text/html
1530088115.039      4 172.31.10.71 TCP_DENIED/407 4176 CONNECT yastatic.net:443 - HIER_NONE/- text/html
1530088115.039      4 172.31.10.71 TCP_DENIED/407 4176 CONNECT yastatic.net:443 - HIER_NONE/- text/html
1530088115.047      0 172.31.10.71 TCP_DENIED/407 4269 CONNECT yandex.ru:443 - HIER_NONE/- text/html
1530088116.578      1 172.31.10.71 TCP_DENIED/407 4333 CONNECT cdn.syndication.twimg.com:443 - HIER_NONE/- text/html
1530088116.585      7 172.31.10.71 TCP_DENIED/407 4313 CONNECT gekko.spiceworks.com:443 - HIER_NONE/- text/html
1530088116.585      6 172.31.10.71 TCP_DENIED/407 4269 CONNECT 3dnews.ru:443 - HIER_NONE/- text/html
1530088116.585      1 172.31.10.71 TCP_DENIED/407 5648 POST http://top-fwz1.mail.ru/tracker? - HIER_NONE/- text/html
1530088116.596     18 172.31.10.71 TCP_DENIED/407 4269 CONNECT yandex.ru:443 - HIER_NONE/- text/html
1530088118.941      0 172.31.10.71 TCP_DENIED/407 4236 CONNECT googleads.g.doubleclick.net:443 - HIER_NONE/- text/html
1530088118.946      0 172.31.10.71 TCP_DENIED/407 4341 CONNECT googleads.g.doubleclick.net:443 - HIER_NONE/- text/html
1530088121.934      0 172.31.10.71 TCP_DENIED/407 4228 CONNECT sec.api.browser.yandex.ru:443 - HIER_NONE/- text/html
1530088121.938      0 172.31.10.71 TCP_DENIED/407 4333 CONNECT sec.api.browser.yandex.ru:443 - HIER_NONE/- text/html
1530088125.390      0 172.31.10.71 TCP_DENIED/407 4228 CONNECT sec.api.browser.yandex.ru:443 - HIER_NONE/- text/html
1530088125.393      0 172.31.10.71 TCP_DENIED/407 4333 CONNECT sec.api.browser.yandex.ru:443 - HIER_NONE/- text/html
1530091186.082      0 172.31.10.71 TCP_DENIED/407 4164 CONNECT yandex.ru:443 - HIER_NONE/- text/html
1530091186.103     17 172.31.10.71 TCP_DENIED/407 4269 CONNECT yandex.ru:443 - HIER_NONE/- text/html
1530091190.539      1 172.31.10.71 TCP_DENIED/407 4269 CONNECT yandex.ru:443 - HIER_NONE/- text/html
1530091191.307      1 172.31.10.71 TCP_DENIED/407 4269 CONNECT yandex.ru:443 - HIER_NONE/- text/html
1530091191.770      0 172.31.10.71 TCP_DENIED/407 4269 CONNECT yandex.ru:443 - HIER_NONE/- text/html
1530091195.390      0 172.31.10.71 TCP_DENIED/407 4164 CONNECT yandex.ru:443 - HIER_NONE/- text/html
1530091195.395      0 172.31.10.71 TCP_DENIED/407 4228 CONNECT sec.api.browser.yandex.ru:443 - HIER_NONE/- text/html
1530091195.396      1 172.31.10.71 TCP_DENIED/407 4269 CONNECT yandex.ru:443 - HIER_NONE/- text/html
1530091195.398      0 172.31.10.71 TCP_DENIED/407 4333 CONNECT sec.api.browser.yandex.ru:443 - HIER_NONE/- text/html
1530091197.499      0 172.31.10.71 TCP_DENIED/407 4269 CONNECT yandex.ru:443 - HIER_NONE/- text/html
1530091199.183      0 172.31.10.71 TCP_DENIED/407 4348 GET http://forundex.ru/unix%20linux/favicon.ico - HIER_NONE/- text/html
1530091199.185      0 172.31.10.71 TCP_DENIED/407 4453 GET http://forundex.ru/unix%20linux/favicon.ico - HIER_NONE/- text/html
1530091200.420      0 172.31.10.71 TCP_DENIED/407 4228 CONNECT sec.api.browser.yandex.ru:443 - HIER_NONE/- text/html
1530091200.423      0 172.31.10.71 TCP_DENIED/407 4333 CONNECT sec.api.browser.yandex.ru:443 - HIER_NONE/- text/html
1530091203.169      0 172.31.10.71 TCP_DENIED/407 4228 CONNECT cdn.syndication.twimg.com:443 - HIER_NONE/- text/html
1530091203.172      0 172.31.10.71 TCP_DENIED/407 4333 CONNECT cdn.syndication.twimg.com:443 - HIER_NONE/- text/html
1530091206.171      0 172.31.10.71 TCP_DENIED/407 4208 CONNECT gekko.spiceworks.com:443 - HIER_NONE/- text/html
1530091206.174      0 172.31.10.71 TCP_DENIED/407 4313 CONNECT gekko.spiceworks.com:443 - HIER_NONE/- text/html
</code></pre>

<p>I need help. Thanks for advanced.</p>
","<active-directory><squid><kerberos>","2018-06-27 12:09:05"
"849131","How to differentiate GE, Optic and FC adapter/SFP?","<p>I'm going through old switches laying around and find myself troubled by all the different connection types the PCI adapters and their SFP have.</p>

<p>I found a 10GE (gigabit ethernet?) SFP that says it's a Fiber channel (Brocade 57-0000075-01) and then I get one that says Fiber Optic (Intel   TXN310110000000) or even yet, a Cisco adapter that says optic but means both (CC2-N320E-SR (B) 110-1088-30 B0) but I cannot seem to figure out (easily) which one is which even by searching their model numbers (I have many, many more laying around, these are exemples)</p>

<p>Wherever I go, if I ask what is the difference between Fiber Channel and Optic says it's the same but if I ask what is Gibabit Ethernet says it's both copper or Fiber optic. I'm getting frustrated, I just want to not put a FC gizmo with the GE ones and vice-versa...</p>
","<fibre-channel><fiber><gigabit-ethernet><pci-express><sfp>","2017-05-09 19:00:35"
"776491","How to host multiple SSL websites with a single IP address using a wildcard certificate in an IBM Domino server","<p>I am using Domino 9.0.1 FP5. I am planning on hosting multiple websites, all of which will be accessed using SSL. I want to use one IP address for all the sites. I will be obtaining a wildcard certificate for the domain so each website will have the same domain but a different sub domain.</p>

<p>Example:</p>

<pre><code>site1.example.com
site2.example.com
</code></pre>

<p>In Domino Administrator I am creating Internet Site documents for each site. The documentation states that you must specify the IP address in the ""Host names or addresses mapped to this site"" field when using SSL.</p>

<p>I understand the reasoning behind this - the server does not know the host domain until it can decrypt the http header but in my case since I am planning on using a wildcard certificate I want to be able to tell the server to use this certificate regardless of the host name. Is there any way to do this?</p>
","<ibm-domino>","2016-05-12 23:37:12"
"849269","How do I run specific queries in SCCM using Powershell","<p>Could you advise how could I run query ID using command prompt? For example, I do not want to go to Monitoring-Queries-expand tree and search for the specific querry. I Just want to have script file, when I launch it the command show the result of my query or export it to csv file.
What is the syntaxis if I should go to File - connect via Windows Poweshell? </p>

<p>MS SCCM 2012 console version 5 site version 5</p>
","<powershell><sccm><query>","2017-05-10 11:20:22"
"1022739","Microsoft Azure Boot Diagnostics Stuck Screen","<p>When I start my VM my Azure VM boot diagnostics shows up this screen.
How do I interact with this screen?
Need Help
<a href=""https://i.sstatic.net/zMLVf.png"" rel=""nofollow noreferrer"">Azure Boot Diagnostics Error Image</a></p>
","<azure>","2020-06-24 11:21:05"
"992933","make a proxy with two servers","<p>I want to implement a proxy which has two (A and B) servers in the middle. traffic goes from my computer to server B and then to server A and then gets the data.
I found that with this configuration in ~/.ssh/config:</p>

<pre><code>HOST B
    HostName B
    user debian
    DynamicForward 0.0.0.0:1081
</code></pre>

<p>I can use my 1081 port as a proxy. But now I don't know how to proxy all B's traffic through A.</p>
","<linux><ssh><proxy><linux-networking>","2019-11-22 18:08:52"
"776682","SPF, CloudFlare, Google Apps - Email sent through Contact Form marked as spam","<p>One of the main reasons of putting your server behind CloudFlare is to hide your server's IP address so an attacker can't DDoS your IP and render CloudFlare obsolete. By putting your server behind CloudFlare their IPs are exposed to the outside world so your server's IP address is only known to you and nobody else.</p>

<p>The problem here is when you want to use a contact form on you website to receive mail from your visitors. By using the contact form, basically your original server's IP is sending emails to you or whichever address you want.</p>

<p>But now since the server is behind CloudFlare the following SPF record becomes a hindrance</p>

<pre><code>v=spf1 a include:_spf.google.com ~all
</code></pre>

<p>because that letter <code>a</code> between <code>v=spf1</code> and <code>include:_spf.google.com</code> tells that all IPs that have A record in DNS are permitted to send email. However, by using CloudFlare, the original server's IP is no longer visible, and CloudFlare's IP becomes identified as the one which is permitted to send emails, not your real server's IP. And as result, every email sent through PHP, from you server, is marked as spam because that IP is now basically not allowed to send email.</p>

<p>Of course you can add your real server's IP in your SPF record, as shown below, to allow it to send emails and solve the problem right there</p>

<pre><code>v=spf1 a ip4:xxx.xx.xxx.xx include:_spf.google.com ~all
</code></pre>

<p>but that would defeat the purpose of using CloudFlare in the first place. They too explicitly tell you not to put your server's IP anywhere that can be publicly revealed including SPF and TXT entries.</p>

<p>When you want to send/receive emails directly by using Google Apps, there is of course no problem because you're using Google Servers to accomplish that (your SPF record says they're allowed to do so) but in the case above you're unable to use a simple contact form on your website if you want to use CloudFlare.</p>

<p>You can also change the last bit in the SPF record from <code>~all</code> to <code>?all</code> so all other IPs other than those specified in the SPF record can be treated as neutral, but that would not stop marking the emails from your contact form as spam.</p>

<p>Am I missing here something very obvious or if you want to use CloudFlare you should forget about using you server's IP at all?</p>
","<email><spam><spf><g-suite><cloudflare>","2016-05-13 20:55:57"
"776779","Creating & mapping user home folders on a network share using GPO","<p>I have deployed a fileserver which has had its harddrive partioned to include a shared partition along with a shared folder within it. The fileserver has joined my domain, hosted on a Windows Server 2012 R2 server, and I can access any resource through my network.<br />
The problems begin when I try to create and apply my GPO. I created a new GPO that uses a drive map to the path <code>\\fileserver\Homes\%username%</code>, which is the location that I want to store my user folders in.<br />
The GPO is set to Action: <strong>Create</strong>, <strong>Run in logged-on user's security context</strong>, as well as having the previously mentioned path set. These are the only things that I edited in my GPO.</p>
<p>I would like to note that I can create maps to each user successfully, using the <strong>profile</strong> tab in each user object, but I want to be able to solve this task using a GPO.</p>
<p>To put it simply; my GPO does not seem to do anything. Is there anything obvious that I am missing in my configuration?</p>
","<active-directory><group-policy>","2016-05-14 15:16:34"
"849431","Verify that Logging is up and enabled","<pre><code>COMMAND   PID   USER   FD   TYPE DEVICE SIZE/OFF      NODE NAME
httpd    1744 apache   16w   REG  202,1        0 167906281 phpapi_access_log
httpd    2334 apache   16w   REG  202,1        0 167906281 phpapi_access_log
</code></pre>

<p>Above is the output of lsof for my phpapi_access_log file. Logging frequency is very low so I cannot determine if logs are being written to the file or not. I enabled logrotate for the file, till now the file is empty. How can I verify logs will be written to the file when required? <strong>What is SIZE/OFF and why is it ""0"" ?</strong></p>
","<logrotate><lsof>","2017-05-11 05:55:26"
"919026","bind9 on ubuntu not pingible from bind9 server (but pingible from other machines) (noob)","<p>As in question. Is it possible to ping on bind9 server machine to dns its serving? </p>

<p>I want to do:</p>

<h1>ping hpc.lan</h1>

<pre><code>PING hpc.lan (12.1.1.1) 56(84) bytes of data.
64 bytes from M.hpc.lan (12.1.1.1): icmp_seq=1 ttl=64 time=0.146 ms
64 bytes from M.hpc.lan (12.1.1.1): icmp_seq=2 ttl=64 time=0.171 ms
64 bytes from M.hpc.lan (12.1.1.1): icmp_seq=3 ttl=64 time=0.175 ms
</code></pre>

<p>And its working besides server machine with bind9.</p>

<p>What could be the reason. I mess up config of bind, or should state it in hosts? I can ping to hostnames in my lan... Im noob in networking, sit like one day this lan + dhcp + dns, so please show some compassion :D</p>

<p>When I do dig from other machines i have:</p>

<pre><code>dig hpc.lan

; &lt;&lt;&gt;&gt; DiG 9.10.3-P4-Ubuntu &lt;&lt;&gt;&gt; hpc.lan
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 50994
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 1, ADDITIONAL: 2

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;hpc.lan.           IN  A

;; ANSWER SECTION:
hpc.lan.        604800  IN  A   12.1.1.1

;; AUTHORITY SECTION:
hpc.lan.        604800  IN  NS  M.hpc.lan.

;; ADDITIONAL SECTION:
M.hpc.lan.      604800  IN  A   12.1.1.1

;; Query time: 1 msec
;; SERVER: 127.0.1.1#53(127.0.1.1)
;; WHEN: Sun Jul 01 10:49:52 CEST 2018
;; MSG SIZE  rcvd: 84
</code></pre>

<p>Also from windows machine:</p>

<pre><code>nslookup hpc.lan

Server: gateway.hpc.lan
Address: 12.1.1.1

Name: hpc.lan
Address: 12.1.1.1
</code></pre>

<p>or reverse</p>

<pre><code>nslookup 12.1.1.1
Server:     127.0.1.1
Address:    127.0.1.1#53

1.1.1.12.in-addr.arpa   name = M.hpc.lan.
1.1.1.12.in-addr.arpa   name = gateway.hpc.lan.
</code></pre>

<p>But from bind9 server machine dig gives:</p>

<pre><code>dig hpc.lan


; &lt;&lt;&gt;&gt; DiG 9.10.3-P4-Ubuntu &lt;&lt;&gt;&gt; hpc.lan
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NXDOMAIN, id: 6784
;; flags: qr rd ra ad; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 512
;; QUESTION SECTION:
;hpc.lan.           IN  A

;; AUTHORITY SECTION:
.           86398   IN  SOA a.root-servers.net. nstld.verisign-grs.com. 2018063002 1800 900 604800 86400

;; Query time: 26 msec
;; SERVER: 8.8.8.8#53(8.8.8.8)
;; WHEN: Sun Jul 01 10:58:31 CEST 2018
;; MSG SIZE  rcvd: 111
</code></pre>

<p>and for nslookup</p>

<pre><code>nslookup hpc.lan
Server:     8.8.8.8
Address:    8.8.8.8#53

** server can't find hpc.lan: NXDOMAIN

a@M:~$ nslookup 12.1.1.1
Server:     8.8.8.8
Address:    8.8.8.8#53

** server can't find 1.1.1.12.in-addr.arpa: NXDOMAIN

a@M:~$ nslookup 12.1.1.50
Server:     8.8.8.8
Address:    8.8.8.8#53

** server can't find 50.1.1.12.in-addr.arpa: NXDOMAIN

a@M:~$ nslookup 12.1.1.90
Server:     8.8.8.8
Address:    8.8.8.8#53

** server can't find 90.1.1.12.in-addr.arpa: NXDOMAIN
</code></pre>

<p>or reverse one</p>

<pre><code>nslookup 12.1.1.1
Server:     8.8.8.8
Address:    8.8.8.8#53

** server can't find 1.1.1.12.in-addr.arpa: NXDOMAIN
</code></pre>

<p>Clearly it dont see dns of my lan, then go in sky...</p>

<p>My config on bind9 server is as follows:</p>

<h1>sudo nano /etc/bind/named.conf.options</h1>

<pre><code>acl ""trusted"" {
    12.1.1.0/24; # ns1
    //192.168.1.0/24;
    localhost;
    localnets;
};

options {
    directory ""/var/cache/bind"";

    recursion yes;
    allow-recursion { trusted; };
    listen-on { trusted; };
    //allow-transfer { none; };

    //allow-recursion { any; };
    allow-query { trusted; };
    allow-query-cache { any; };

    forwarders {
        12.1.1.1;
        8.8.8.8;
        8.8.4.4;
    };

    dnssec-validation auto;

    auth-nxdomain no;    # conform to RFC1035
    listen-on-v6 { any; };
};
</code></pre>

<h1>sudo nano /etc/bind/named.conf.local</h1>

<pre><code>include ""/etc/bind/zones.rfc1918"";

// send ads to black hole
include ""/etc/bind/ad-blacklist"";


// our local zone
zone ""hpc.lan"" {
    type master;
    file ""/etc/bind/db.hpc.lan"";
};


// reverse for .hpc domain
zone ""1.1.12.in-addr.arpa"" {
    type master;
    notify yes;
    file ""/etc/bind/db.hpc.lan.r"";
};
</code></pre>

<h1>sudo nano /etc/bind/db.hpc.lan</h1>

<pre><code>;
; BIND data file for local loopback interface
;
$TTL    604800
@   IN  SOA M.hpc.lan. root.hpc.lan. (
                  2     ; Serial
             604800     ; Refresh
              86400     ; Retry
            2419200     ; Expire
             604800 )   ; Negative Cache TTL
;
hpc.lan.    IN  NS  M.hpc.lan.
hpc.lan.    IN  A   12.1.1.1
;@      IN  NS  localhost.
;@      IN  A   127.0.0.1
;@      IN  AAAA    ::1
M       IN  A   12.1.1.1
N0      IN  A   12.1.1.55
L0      IN  A   12.1.1.90
www     IN  CNAME   hpc.lan.
gateway     IN  A   192.168.1.1
</code></pre>

<h1>sudo nano /etc/hpc/db.hpc.lan.r</h1>

<pre><code>;
; BIND reverse data file for local loopback interface
;
$TTL    604800
@   IN  SOA M.hpc.lan. root.hpc.lan. (
                  2     ; Serial
             604800     ; Refresh
              86400     ; Retry
            2419200     ; Expire
             604800 )   ; Negative Cache TTL
;
@   IN  NS  M.
1   IN  PTR gateway.hpc.lan.
1   IN  PTR M.hpc.lan.
55  IN  PTR N0.hpc.lan.
90  IN  PTR L0.hpc.lan.
</code></pre>

<h1>sudo nano /etc/hosts</h1>

<pre><code>127.0.0.1   localhost
127.0.1.1   M #VN278AA-UUW-m9860sc
12.1.1.1    M #VN278AA-UUW-m9860sc-enp1s10
192.168.1.106   MS #VN278AA-UUW-m9860sc-enp0s10
</code></pre>

<h1>sudo nano /etc/hostname</h1>

<pre><code>M
</code></pre>

<h1>sudo nano /etc/network/interfaces</h1>

<pre><code># interfaces(5) file used by ifup(8) and ifdown(8)
auto lo
iface lo inet loopback


# external
auto enp0s10
iface enp0s10 inet dhcp
#iface enp0s10 inet static
#   address 192.168.1.106
#   gateway 192.168.1.1
#   mtu 1500
#   metric 1000
#   dns-nameservers 8.8.8.8 4.4.4.4

# internal 
auto enp1s10
iface enp1s10 inet static
    address 12.1.1.1
    network 12.1.1.0
    netmask 255.255.255.0
    gateway 192.168.1.106
    broadcast 12.1.0.255
    mtu 7152
    dns-nameservers 12.1.1.1
    metric 100
</code></pre>

<h1>sudo nano  /etc/apparmor.d/usr.sbin.named</h1>

<pre><code>... 
  /var/log/bind/** rw,
  /var/log/bind/ rw,

}
</code></pre>

<h1>sudo nano /etc/dhcp/dhcpd.conf</h1>

<pre><code>ddns-update-style none;

option domain-name ""hpc.lan"";
option domain-name-servers 12.1.1.1;

default-lease-time 86400;
max-lease-time 172800;

authoritative;

log-facility local7;

# wan network, we dont provide service here
subnet 192.168.1.0 netmask 255.255.255.0 {
}

# lan network we provide service for
subnet 12.1.1.0 netmask 255.255.255.0 {
    range 12.1.1.50 12.1.1.99;
    option routers 12.1.1.1;
    option subnet-mask 255.255.255.0;
    option broadcast-address 12.1.0.255;
}

host N0 {
    hardware ethernet 00:25:11:4f:9d:92;
    fixed-address 12.1.1.55;
}

host L0 {
    hardware ethernet f0:de:f1:5b:d3:da;
    fixed-address 12.1.1.90;
}
</code></pre>

<h1>Edit</h1>

<p>by they way nslookup works dns and reverse dns works, but problem is that on server machine, lookup is skipping 12.1.1.1 and go directly to 8.8.8.8</p>

<p>may there be some problem with forwarding here? </p>

<p>my iptables are</p>

<h1>cat /etc/iptables.conf</h1>

<pre><code># Generated by iptables-save v1.6.0 on Mon Jul  2 01:22:43 2018
*nat
:PREROUTING ACCEPT [406:28839]
:INPUT ACCEPT [180:15615]
:OUTPUT ACCEPT [2024:169350]
:POSTROUTING ACCEPT [66:3986]
-A POSTROUTING -o enp0s10 -j MASQUERADE
-A POSTROUTING -o enp1s10 -j MASQUERADE
COMMIT
# Completed on Mon Jul  2 01:22:43 2018
# Generated by iptables-save v1.6.0 on Mon Jul  2 01:22:43 2018
*filter
:INPUT ACCEPT [1211066:4289490990]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [750206:1045506458]
-A FORWARD -i enp1s10 -o enp0s10 -j ACCEPT
-A FORWARD -i enp0s10 -o enp1s10 -m state --state RELATED,ESTABLISHED -j ACCEPT
COMMIT
# Completed on Mon Jul  2 01:22:43 2018
</code></pre>
","<ubuntu><bind><local-area-network>","2018-07-01 09:18:30"
"1023038","Count requests from blocked IPs","<p>Site on my server is being attacked using SQL injections. I block IP from which was attacked with command:</p>
<pre><code># iptables -A INPUT -s ATTACK-IP-ADDRESS -j DROP
</code></pre>
<p>Can I review activity of <code>ATTACK-IP-ADDRESS</code> now? I mean when and what HTTP request was from this IP.</p>
","<ubuntu-16.04><ip-blocking>","2020-06-26 13:52:41"
"849585","Changed nameservers lost email","<p>My client had a website on Wix and his domain name was registered at Aruba.it. I set up a new website for him with my host (1&amp;1 Internet) and I have today changed the nameservers with his registrar (Aruba) to point to my host. All went fine and I can see his new website. However his email has broken in the process and he cannot receive any new email. I discovered in my 1&amp;1 Account that the email now appears to be setup there (I assume this is what happens when I change the nameservers) as i have created a new mailbox and can see his mail arriving in webmail for 1&amp;1 internet.</p>

<p>His setup at home is quite complicated and he uses Exchange, which he thinks is setup to work with Aruba - I have no experience of Exchange - I was hoping the easy way around this would be to update the incoming and outgoin mail server in his email client but I don't know how to do this with Exchange (which is also connected to Office365 somehow).</p>

<p>I thought the next best thing to do was to send the mail server back to Aruba from my 1&amp;1 Internet account, but then I am not sure what I need to do, I assume it's related to the MX Record and possibly A record, but I do not know what i need to enter into them - I have emailed Aruba to see if they can tell me what to enter, but I am concerned of the delay in a response.</p>
","<email><exchange><domain><mx-record>","2017-05-11 19:25:37"
"919031","sendmail send Authentication-Warning","<p>Dears,<br>
I receive successful message from postfix/pip in Centos7 (delivered via spamassassin service) while it doesn't deliver email to user with sendmail Authentication warning log. you can find log in the bellow.</p>

<pre><code>mail postfix/pipe[33055]: 586D85F1EE: to=&lt;root@servername.com&gt;, relay=spamassassin, delay=103, delays=0.06/0.02/0/103, dsn=2.0.0, status=sent (delivered via spamassassin service)
mail postfix/qmgr[1778]: 586D85F1EE: removed
mail postfix/smtpd[46730]: disconnect from unknown[127.0.0.1]
mail sendmail[46763]: w617bxpb046763: Authentication-Warning: servername.com: spamfilter set sender to MAILER-DAEMON using -f
mail sendmail[46763]: w617bxpb046763: from=MAILER-DAEMON, size=15920, class=0, nrcpts=1, msgid=&lt;201807010715.w617Fcg4089763@servername.com&gt;, relay=spamfilter@localhost
mail sendmail[46763]: w617bxpb046763: SYSERR(spamfilter): Too many hops 27 (25 max): from MAILER-DAEMON via localhost, to root@servername.com
mail sendmail[46763]: w617bxpb046763: w617bxpc046763: DSN: Too many hops 27 (25 max): from MAILER-DAEMON via localhost, to root@servername.com
mail postfix/smtpd[53780]: connect from unknown[127.0.0.1]
mail postfix/smtpd[53780]: Anonymous TLS connection established from unknown[127.0.0.1]: TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits)
mail sendmail[46763]: STARTTLS=client, relay=[127.0.0.1], version=TLSv1/SSLv3, verify=FAIL, cipher=ECDHE-RSA-AES256-GCM-SHA384, bits=256/256
mail postfix/smtpd[53780]: NOQUEUE: reject: RCPT from unknown[127.0.0.1]: 550 5.1.1 &lt;MAILER-DAEMON@servername.com&gt;: Recipient address rejected: User unknown in virtual mailbox table; from=&lt;&gt; to=&lt;MAILER-DAEMON@servername.com&gt; proto=ESMTP helo=&lt;servername.com&gt;
mail sendmail[46763]: w617bxpc046763: to=MAILER-DAEMON, delay=00:00:01, xdelay=00:00:01, mailer=relay, pri=46944, relay=[127.0.0.1] [127.0.0.1], dsn=5.1.1, stat=User unknown
mail sendmail[46763]: w617bxpc046763: w617bxpd046763: return to sender: User unknown
mail postfix/smtpd[53780]: NOQUEUE: reject: RCPT from unknown[127.0.0.1]: 550 5.1.1 &lt;postmaster@servername.com&gt;: Recipient address rejected: User unknown in virtual mailbox table; from=&lt;&gt; to=&lt;postmaster@servername.com&gt; proto=ESMTP helo=&lt;servername.com&gt;
mail sendmail[46763]: w617bxpd046763: to=postmaster, delay=00:00:00, xdelay=00:00:00, mailer=relay, pri=47968, relay=[127.0.0.1] [127.0.0.1], dsn=5.1.1, stat=User unknown
mail sendmail[46763]: w617bxpc046763: Losing ./qfw617bxpc046763: save    mail panic
mail sendmail[46763]: w617bxpc046763: SYSERR(spamfilter): savemail: cannot save rejected e    mail anywhere
mail postfix/smtpd[53780]: disconnect from unknown[127.0.0.1]
mail postfix/pipe[33055]: 8C1105F1EF: to=&lt;root@servername.com&gt;, relay=spamassassin, delay=104, delays=0.3/0.03/0/103, dsn=5.3.0, status=bounced (service unavailable)
mail postfix/qmgr[1778]: 8C1105F1EF: removed
</code></pre>

<p>What is the Authentication-Warning? and why it said <code>Recipient address rejected: User unknown in virtual mailbox table</code>?</p>

<p><strong>Update</strong><br><br>
I add users to sendmail.mc file but it doesn't fix the issue. there is also <code>SASL LOGIN authentication failed: Invalid authentication mechanism</code> in log? what does it mean and where can I fix authentication mechanism.</p>

<pre><code>mail postfix/smtpd[4579]: connect from unknown[178.141.251.45]
mail postfix/smtpd[4579]: warning: unknown[178.141.251.45]: SASL LOGIN authentication failed: Invalid authentication mechanism
mail postfix/smtpd[4579]: disconnect from unknown[178.141.251.45]
mail sendmail[4593]: w635Z73D004593: Authentication-Warning: servername.com: spamfilter set sender to MAILER-DAEMON using -f
mail sendmail[4593]: w635Z73D004593: from=MAILER-DAEMON, size=15920, class=0, nrcpts=1, msgid=&lt;201807030512.w635CkWe044938@servername.com&gt;, relay=spamfilter@localhost
mail sendmail[4593]: w635Z73D004593: SYSERR(spamfilter): Too many hops 27 (25 max): from MAILER-DAEMON via localhost, to root@servername.com
mail sendmail[4593]: w635Z73D004593: w635Z73E004593: DSN: Too many hops 27 (25 max): from MAILER-DAEMON via localhost, to root@servername.com
mail postfix/smtpd[4579]: connect from unknown[127.0.0.1]
mail postfix/smtpd[4579]: Anonymous TLS connection established from unknown[127.0.0.1]: TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits)
mail sendmail[4593]: STARTTLS=client, relay=[127.0.0.1], version=TLSv1/SSLv3, verify=FAIL, cipher=ECDHE-RSA-AES256-GCM-SHA384, bits=256/256
mail postfix/smtpd[4579]: NOQUEUE: reject: RCPT from unknown[127.0.0.1]: 550 5.1.1 &lt;MAILER-DAEMON@servername.com&gt;: Recipient address rejected: User unknown in virtual mailbox table; from=&lt;&gt; to=&lt;MAILER-DAEMON@winwinmarke$    mail sendmail[4593]: w635Z73E004593: to=MAILER-DAEMON, delay=00:00:00, xdelay=00:00:00, mailer=relay, pri=46944, relay=[127.0.0.1] [127.0.0.1], dsn=5.1.1, stat=User unknown
mail sendmail[4593]: w635Z73E004593: w635Z73F004593: return to sender: User unknown
mail postfix/smtpd[4579]: NOQUEUE: reject: RCPT from unknown[127.0.0.1]: 550 5.1.1 &lt;postmaster@servername.com&gt;: Recipient address rejected: User unknown in virtual mailbox table; from=&lt;&gt; to=&lt;postmaster@servername.com&gt; $    mail sendmail[4593]: w635Z73F004593: to=postmaster, delay=00:00:00, xdelay=00:00:00, mailer=relay, pri=47968, relay=[127.0.0.1] [127.0.0.1], dsn=5.1.1, stat=User unknown
mail sendmail[4593]: w635Z73E004593: Losing ./qfw635Z73E004593: save    mail panic
mail sendmail[4593]: w635Z73E004593: SYSERR(spamfilter): savemail: cannot save rejected e    mail anywhere
mail postfix/smtpd[4579]: disconnect from unknown[127.0.0.1]
mail postfix/pipe[4591]: BBCB623331: to=&lt;root@servername.com&gt;, relay=spamassassin, delay=104, delays=0.05/0.02/0/103, dsn=5.3.0, status=bounced (service unavailable)
mail postfix/qmgr[69009]: BBCB623331: removed
mail postfix/smtpd[18405]: connect from unknown[161.132.201.90]
mail postfix/smtpd[18405]: warning: unknown[161.132.201.90]: SASL LOGIN authentication failed: Invalid authentication mechanism
mail postfix/smtpd[18405]: disconnect from unknown[161.132.201.90]
mail postfix/smtpd[29270]: connect from unknown[94.16.117.217]
mail postfix/smtpd[29270]: warning: unknown[94.16.117.217]: SASL LOGIN authentication failed: Invalid authentication mechanism
mail postfix/smtpd[29270]: disconnect from unknown[94.16.117.217]
mail postfix/anvil[39360]: statistics: max connection rate 1/60s for (smtp:178.141.251.45) at Jul  3 05:33:37
mail postfix/anvil[39360]: statistics: max connection count 1 for (smtp:178.141.251.45) at Jul  3 05:33:37
mail postfix/anvil[39360]: statistics: max cache size 1 at Jul  3 05:33:37
mail postfix/smtpd[43131]: connect from unknown[46.244.212.248]
mail postfix/smtpd[43131]: warning: unknown[46.244.212.248]: SASL LOGIN authentication failed: Invalid authentication mechanism
mail postfix/smtpd[43131]: disconnect from unknown[46.244.212.248]
</code></pre>
","<postfix><centos7><sendmail>","2018-07-01 11:15:29"
"777011","Why does my HP Smart Array show two logical drives as failed?","<p>I have HP Proliant DL 380 G8 with raid 0+1 ..still its not booting in smart array controller showing 2 logical drive is failed how can we fix it</p>

<p><a href=""https://i.sstatic.net/qt5Z4.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qt5Z4.jpg"" alt=""enter image description here""></a></p>
","<raid>","2016-05-16 09:23:08"
"1023115","Is it possible to give windows 10 access to a sql server hosted on windows server 2016?","<p>Is it possible to give windows 10 access to a SQL server hosted on windows server 2016? I have found resources for setting up a domain for my computers using a windows server.
The windows server will host an instance of SQL Server Express. I have Windows computers that will need access to the SQL server. Each Windows 10 machines will have Visual Studios and thus will need the access to the data from the SQL Servers installed on the Windows Server 2016.</p>
<p>Is it possible to do this without giving each user access to the domain on the Windows Server over a Network connection or do I need to give each user access to the domain and thus allowing for individual users to be given access to using the SQL Server from the domain?</p>
<p>I read literature on <a href=""https://docs.microsoft.com/en-us/sql/database-engine/configure-windows/configure-windows-service-accounts-and-permissions?view=sql-server-ver15"" rel=""nofollow noreferrer"">Microsoft's Documentation</a> that states nothing about Active Directory and giving access through a domain.</p>
<p>currently I am confused. I keep finding a lot of AZURE Active Directory articles and none for Microsoft Docs explaining Active Directory Domains for this setup I am going to setup.</p>
","<active-directory><windows-server-2016><domain-controller><windows-10><sql-server-express>","2020-06-27 07:42:03"
"1023237","Iptables is slowing down website","<p><code>IPtables</code> is slowing down my gitlab instances. Clicking on Project or refreshing the page is taking so long (almost 30 seconds to 60 seconds). If I flush the <code>IPtables</code> list then the page refreshes within seconds.</p>
<p>The list is very small but I'm not sure why its slowing down.</p>
<p>Is there anything that I can do to improve the speed?</p>
<p>Here are my rules:</p>
<pre><code>root@hostname ~]# iptables -L --line-numbers -n
Chain INPUT (policy ACCEPT)
num  target     prot opt source               destination
1    ACCEPT     all  --  10.10.20.0/24        0.0.0.0/0
2    ACCEPT     tcp  --  x.x.x.x              0.0.0.0/0            tcp dpt:8080
3    ACCEPT     tcp  --  x.x.x.x              0.0.0.0/0            tcp dpt:8080
4    DROP       tcp  --  0.0.0.0/0            0.0.0.0/0            tcp dpt:8080
5    DROP       tcp  --  0.0.0.0/0            0.0.0.0/0            tcp dpt:22
6    DROP       tcp  --  0.0.0.0/0            0.0.0.0/0            tcp dpt:25
7    DROP       tcp  --  0.0.0.0/0            0.0.0.0/0            tcp dpt:1234
8    DROP       tcp  --  0.0.0.0/0            0.0.0.0/0            tcp dpt:1235
9    DROP       tcp  --  0.0.0.0/0            0.0.0.0/0            tcp dpt:1236

Chain FORWARD (policy ACCEPT)
num  target     prot opt source               destination

Chain OUTPUT (policy ACCEPT)
num  target     prot opt source               destination
</code></pre>
<p>iptables-save output:</p>
<pre><code># Generated by iptables-save v1.4.21 on Sun Jun 28 14:37:15 2020
*filter
:INPUT ACCEPT [10621:23037348]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [12051:23280396]
-A INPUT -s 10.10.20.0/24 -j ACCEPT
-A INPUT -s x.x.x.x/32 -p tcp -m tcp --dport 8080 -j ACCEPT
-A INPUT -s x.x.x.x/32 -p tcp -m tcp --dport 8080 -j ACCEPT
-A INPUT -p tcp -m tcp --dport 8080 -j DROP
-A INPUT -p tcp -m tcp --dport 22 -j DROP
-A INPUT -p tcp -m tcp --dport 25 -j DROP
-A INPUT -p tcp -m tcp --dport 1234 -j DROP
-A INPUT -p tcp -m tcp --dport 1235 -j DROP
-A INPUT -p tcp -m tcp --dport 1236 -j DROP
COMMIT
# Completed on Sun Jun 28 14:37:15 2020
</code></pre>
","<linux><centos><iptables><centos7>","2020-06-28 14:15:29"
"777198","Amazon Linux - expand root partition","<p>I need to expand the root partition '/' on my linux machine to occupy entire available space. I launched a machine with 60 GB storage space. 'lsblk' for my machine is as follows.</p>

<pre><code>$ lsblk
NAME    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
xvda    202:0    0   60G  0 disk 
├─xvda1 202:1    0  6.4G  0 part /
├─xvda2 202:2    0  290M  0 part /home
├─xvda3 202:3    0  2.5G  0 part /var
├─xvda4 202:4    0  690M  0 part /var/log
├─xvda5 202:5    0  290M  0 part /var/log/audit
└─xvda6 202:6    0  1.5G  0 part /tmp
</code></pre>

<p>I need to expand '/' to occupy the remaining space. I tried this -</p>

<pre><code>sudo resize2fs /dev/xvda1
resize2fs 1.42.12 (29-Aug-2014)
The filesystem is already 1674240 (4k) blocks long.  Nothing to do!
</code></pre>
","<linux><amazon-linux>","2016-05-17 05:41:08"
"849790","My disk free space in ubuntu has been stolen","<p>This is output after i run <code>df -h</code></p>

<pre><code>Filesystem      Size  Used Avail Use% Mounted on
/dev/vda1        30G   19G  9.1G  68% /
none            4.0K     0  4.0K   0% /sys/fs/cgroup
udev            487M  4.0K  487M   1% /dev
tmpfs           100M  348K  100M   1% /run
none            5.0M     0  5.0M   0% /run/lock
none            498M     0  498M   0% /run/shm
none            100M     0  100M   0% /run/user
/dev/sda         25G  6.4G   17G  28% /mnt/volume-sgp1-01
</code></pre>

<p>I want to identify what are <code>19G</code> being used.</p>

<p>So i run below command</p>

<p><code>du -h --max-depth=10 --one-file-system / | sort -rh | head -25</code></p>

<p>to get following output</p>

<pre><code>14G /
12G /var
8.8G    /var/lib
8.6G    /var/lib/mysql
8.1G    /var/lib/mysql/getapple_phpbb
1.6G    /var/www
916M    /usr
546M    /var/www/tskthai.com/public_html
546M    /var/www/tskthai.com
517M    /var/www/tskthai.com/public_html/wp-content
464M    /lib
431M    /var/www/ereaderok.com/public_html
431M    /var/www/ereaderok.com
396M    /home
381M    /var/www/himaparn.com/public_html
381M    /var/www/himaparn.com
378M    /var/lib/mysql/getapple_paysmile
368M    /lib/modules
306M    /var/www/tskthai.com/public_html/wp-content/uploads
295M    /usr/lib
255M    /var/www/getapple.net/public_html
255M    /var/www/getapple.net
250M    /opt
244M    /usr/share
242M    /opt/datadog-agent
</code></pre>

<p>you can see that the main directory being used is <code>/</code> which is only <code>14gb</code></p>

<p>So where are the remaining 5GB are being used?</p>

<p>Please advise.</p>
","<ubuntu><disk-space-utilization>","2017-05-12 16:45:24"
"993490","Velostrata Migration from On-premises to GCP. Error log: echo boot failed and waiting for disks online. Migrated VM Rolled-back to on-prem from GCP","<p>I am seeing the errors in the log: echo boot failed and waiting for disks online in the console log of Velostrata Manager while I am doing the migration of a windows 2016 VM (No Symantec End Point Protection) from on-premises vcenter to Google Cloud Platform. Below is the error log for a quick reference. Answers are welcome and much appreciated. I am trying for the resolution from one day, but no luck. Thank you.</p>

<pre><code>2019-11-27 07:28:51.623 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} 
2019/11/27 07:24:36.874Z Waiting for disks to be available... 
2019-11-27 07:28:51.623 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} 
2019/11/27 07:24:38.937Z Waiting for disks to be available... 
2019-11-27 07:28:51.623 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} 
2019/11/27 07:24:40.968Z Waiting for disks to be available... 
2019-11-27 07:28:51.623 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} 
2019/11/27 07:24:43.015Z Waiting for disks to be available... 
2019-11-27 07:28:51.623 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} 
2019/11/27 07:24:45.062Z Waiting for disks to be available... 
2019-11-27 07:28:51.623 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} 
2019/11/27 07:24:47.093Z Waiting for disks to be available... 
2019-11-27 07:28:51.623 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} 
2019/11/27 07:24:49.140Z Waiting for disks to be available... 
2019-11-27 07:28:51.623 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} 
2019/11/27 07:24:51.187Z Waiting for disks to be available... 
2019-11-27 07:28:51.623 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} 
2019/11/27 07:24:53.218Z Waiting for disks to be available... 
2019-11-27 07:28:51.623 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6}
2019/11/27 07:24:55.265Z Waiting for disks to be available... 
2019-11-27 07:28:51.623 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} 
2019/11/27 07:24:57.312Z Waiting for disks to be available... 
2019-11-27 07:28:51.623 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} 
2019/11/27 07:24:59.343Z Waiting for disks to be available... 
2019-11-27 07:28:51.623 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} 
2019/11/27 07:25:01.390Z Waiting for disks to be available... 
2019-11-27 07:28:51.623 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} 
2019/11/27 07:25:03.437Z Waiting for disks to be available... 
2019-11-27 07:28:51.623 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} ============================ port: 2 ============================ 2019-11-27 07:28:51.623 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} ============================ port: 3 ============================ 2019-11-27 07:28:51.623 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} 
============================ port: 4 ============================ 
2019-11-27 07:28:52.405 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} #!ipxe 
2019-11-27 07:28:52.405 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} # iqn:iqn.2014-05.com.velostrata.iscsi.base:vlst-6 
2019-11-27 07:28:52.405 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} # ip1:10.120.22.11 
2019-11-27 07:28:52.405 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} # ip2:10.120.22.12 
2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} # token:haeth2vp 
2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} # mode:normal 
2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} # boot:bios 
2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} :setenv 
2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} set syslog 10.120.22.11 
2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} route 
2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} set net0/hostname vlsboot 
2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} set initiator-iqn iqn.2014-05.com.velostrata.int.haeth2vp:vlst-6 2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} set tgt1 iscsi:10.120.22.11 
2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} set tgt2 iscsi:10.120.22.12 
2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} set iqn iqn.2014-05.com.velostrata.iscsi.base:vlst-6 2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} echo Initiator IQN ${initiator-iqn} 
2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} echo Target root path ${tgt1}::::${iqn} 
2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} echo Target replica path ${tgt2}::::${iqn} 
2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} :hookretry 
2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} inc hook-attempt 
2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} echo Hook attempt ${hook-attempt} at ${unixtimestr} 
2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} clear hook-error 
2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} isset ${drive1} || sanhook --drive 0x81 --no-describe ${tgt1}:::1:${iqn} ${tgt2}:::1:${iqn} &amp;&amp; inc drive1 || inc hook-error 
2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53,step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} isset ${hook-error} || goto bootretry 
2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} iseq ${hook-attempt} 25 &amp;&amp; goto end || sleep 10 
2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} goto hookretry 
2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} :bootretry 
2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} inc boot-attempt 
2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} echo Boot attempt ${boot-attempt} at ${unixtimestr} 
2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} sanboot ${tgt1}::::${iqn} ${tgt2}::::${iqn} || 
2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} iseq ${boot-attempt} 25 &amp;&amp; goto end || sleep 10 
2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} goto bootretry 
2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} :end 
2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} echo Boot failed 
2019-11-27 07:28:52.406 [tasks-39] DEBUG consoleLogsLogger - {instanceId=projects/gcp-shared-host-nonprod/zones/asia-southeast1-c/instances/vmtestgcp02, originVmName=VMTESTGCP02, parentTaskId=t-53, step=stateMonitorInstanceSuccessfulBoot, task=RunVmInCloud, taskId=t-54, vmId=vlst-6} poweroff
</code></pre>
","<google-cloud-platform><migration>","2019-11-27 15:42:33"
"777256","How to eject sas hdd win server 2012 proliant dl380 g9","<p>We got Proliant DL380 G9, when I try to eject or remove SAS HDD by taking it offline, the Red LED (Do not remove) is still on. I'm not really familiar with Proliant, on other servers I just take the drive offline and I can safely remove it. This one is not. If I take it out while Red Light is on Logical Drive with be corrupted. Below is screenshot of my drives, I wanted to remove Array B and C after it's full for backup purposes.</p>

<p>In addition, I am intending to use the server as file server. So if I am doing it wrong, please do kindly correct me and direct me to which is the right thing to do. The fplan is put 1tb as a file storage and we will be putting another 1tb to backup the files every week and when it's full it will be removed and replaced by another 1tb drive.</p>

<p><a href=""https://i.sstatic.net/FZfPl.png"" rel=""nofollow noreferrer"">This is the screenshot of HPSSA</a></p>
","<backup><storage><hp-proliant><hp-smart-array><hotswap>","2016-05-17 12:53:20"
"777438","postfix mail server not configured in outlook","<p>I have configured postfix mail server with squirrel mail web client. I'm able to success send out mail through squirrel mail/web mail but when I tried to configure it in outlook then smtp is not test success. </p>

<pre><code>error: Snd test e-mail message: None of the authentication methods supported by this client are supported by your server.
</code></pre>

<p>Please let me know where I'm wrong below is the configuration detail from my server:</p>

<pre><code>[root@mailserver ~]# postconf -n
alias_database = hash:/etc/aliases
alias_maps = hash:/etc/aliases
command_directory = /usr/sbin
config_directory = /etc/postfix
daemon_directory = /usr/libexec/postfix
data_directory = /var/lib/postfix
debug_peer_level = 2
home_mailbox = Maildir/
html_directory = no
inet_interfaces = all
inet_protocols = all
mail_owner = postfix
mailq_path = /usr/bin/mailq.postfix
manpage_directory = /usr/share/man
mydestination = $myhostname, localhost.$mydomain, localhost, $mydomain
mydomain = agilis.com
myhostname = mailserver.agilis.com
mynetworks = 192.168.6.0/24, 58.68.50.51, 209.44.115.63, 127.0.0.0/8, [::1]/128
myorigin = $mydomain
newaliases_path = /usr/bin/newaliases.postfix
queue_directory = /var/spool/postfix
readme_directory = /usr/share/doc/postfix-2.6.6/README_FILES
relayhost = mail.agilisinternational.com
sample_directory = /usr/share/doc/postfix-2.6.6/samples
sendmail_path = /usr/sbin/sendmail.postfix
setgid_group = postdrop
smtp_always_send_ehlo = yes
smtp_sasl_auth_enable = yes
smtp_sasl_password_maps = hash:/etc/postfix/sasl_passwd
smtp_sasl_security_options = noanonymous
unknown_local_recipient_reject_code = 550
[root@mailserver ~]#
[root@mailserver ~]# netstat -tanp|grep 25
tcp        0      0 0.0.0.0:25                  0.0.0.0:*                   LISTEN      23184/master
tcp        0      0 :::25                       :::*                        LISTEN      23184/master
tcp        0      0 :::36125                    :::*                        LISTEN      1270/rpc.statd
</code></pre>
","<postfix>","2016-05-18 05:52:08"
"777465","create folder from a list","<p>i need to create a personal folder for each user of my company.</p>

<p>I've tried to create a batch script using <strong>ND</strong> command and it works just with name folder without . (dot) </p>

<p>The name of each folder should be <strong>name.surname</strong></p>

<pre><code>   ND jonny.green,
   micheal.fox,
   laura.young
</code></pre>

<p>Do you have some suggestion ?</p>
","<windows><scripting><batch>","2016-05-18 08:15:44"
"993689","Why does it not work when I set CNAME to let my domain point to another domain?","<p>I have registered a domain, say myDomain.com, and I want to try out how CNAME works. I changed the CNAME of www.mydomain.com in GoDaddy and let it point to another domain, say Amazon.com, but when I try to visit www.mydomain.com, it displayed ""this site is not currently available"". </p>

<p>From what I've read here about <a href=""https://ns1.com/resources/cname"" rel=""nofollow noreferrer"">CNAME</a>, it seems such a setting should be valid. I wonder what is the reason and is it really possible if I want to let such a setting work. </p>

<p>I am also curious, if it is really possible to let www.mydomain.com point to www.amazon.com, when the request is sent by the browser, what value will be in the <code>Host</code> header, will it be <code>mydomain.com</code> or will it be <code>amazon.com</code>? </p>
","<domain-name-system><cname-record>","2019-11-28 21:07:22"
"777475","Iptables on a Debian machine, the delete command needs a networking restart to work?","<p>I have a big problem with iptables, this the thing I try to make :</p>

<p>PC windows (192.168.1.1) -> (192.168.1.2) PC Linux (192.168.2.1) -> (192.168.2.2) card </p>

<p>I want to forward the messages from Windows to card. I wrote these rules :</p>

<pre><code>iptables -t nat -A PREROUTING -d 192.168.1.2 -p udp --dport 49000 -j DNAT --to-destination 192.168.2.2:49000
iptables -t nat -A PREROUTING -d 192.168.2.1 -p udp --dport 49001 -j DNAT --to-destination 192.168.1.1:49001
iptables -t nat -A POSTROUTING -o eth1 -j MASQUERADE
iptables -t nat -A POSTROUTING -o eth2 -j MASQUERADE
</code></pre>

<p>It works if I restart the card after created the rules, this is not a really big problem.</p>

<p>The bigger one is if I delete a rule, the rule doesn't appear with :</p>

<pre><code>iptables -t nat --list
</code></pre>

<p>but it still work, the messages continue to be forward...</p>

<p>If I want to really delete a rule I need to flush the table (<code>iptables -t nat -F</code>), after that to restart the network (<code>/etc/init.d/networking restart</code>) and finnaly to re-create the rules I don't want to delete...</p>

<p>I found this in the forum but in my case it's false :
<a href=""https://serverfault.com/questions/304216/is-a-reboot-required-after-edit-saving-linux-iptables/304217"">Page I found here</a></p>

<p>Do you have some ideas? :)</p>

<p>edit 1: </p>

<pre><code>net.ipv4.ip_forward=1 OK
</code></pre>

<p>no more iptables rules than the 4 I wrote.</p>

<p>edit 2 :</p>

<p>When I delete the rule in iptables, I can see it after in the conntrack, so I try to remove the rule, I flushed the conntrack, it was empty, but it still doesnt work : the rule comeback in the conntrack when a package arrive.</p>

<p>It's seems that the rules was saved somewhere and the conntrack can find it, but not me... The only way to delete it is to flush itables, and restart the network...</p>
","<debian><iptables>","2016-05-18 09:02:24"
"919836","how to get file download connections on linux","<p>I have a site with video content (mp4 files).</p>

<p>When people are watching videos, they are downloading video files from my site. I want to know the number of the active connections via the linux terminal. They are probably TCP packets on port 80. </p>

<p>However:</p>

<pre><code>$ netstat -an |grep :80 | wc -l
</code></pre>

<p>gives a huge number, ~6-7k. I don't think that this is correct?</p>
","<linux><connection><terminal><netstat>","2018-07-06 19:24:09"
"777649","Point domain name registered with Godaddy and administered on XYZ server to AWS","<p>There is <strong>domain</strong> e.g example.com registered with Godaddy, <strong>hosted</strong> on <code>SomeHosting</code> service (The server which serves the contents is here). This same hosting company provides the DNS Zone editor (cPanel) with following (sample) entries</p>

<pre><code>ns1.example.com.            A   IP_1
ns2.example.com.            A   IP_2
example.com.                A   IP_3
mail.example.com.           A   IP_2
www.example.com.            CNAME   example.com
ftp.example.com.            A   IP_2
cpanel.example.com.         A   IP_2
east.example.com.           A   IP_1
west.example.com.           A   IP_1
test.example.com.           A   IP_2
qa.example.com.             A   x.x.x.x
special.example.com.          CNAME some-load-balancer.us-east-1.elb.amazonaws.com
</code></pre>

<p>I want to get rid of this SomeHosting company (<strong>not Godady</strong>) and want to move to AWS. So I created instances and beanstalks etc on AWS and they are working fine.
I want that now when some one type example.com, It should point to my load balancer(ELB) on AWS.</p>

<p>So after reading some tutorail, In AWS's Route 53 console, I created a hosted zone with <strong>example.com.</strong>, it automatically added 2 records</p>

<pre><code>example.com.    NS  ns-xxxx.awsdns-37.co.uk.
                    ns-xxx.awsdns-29.net.
                    ns-xxxx.awsdns-14.org.
                    ns-xx.awsdns-03.com.

example.com.        SOA ns-xxxx.awsdns-37.co.uk. awsdns-hostmaster.amazon.com. 1 7200 900 1209600 86400
</code></pre>

<p><strong>Please confirm following questions with Yes/No following details if any</strong></p>

<ol>
<li>My first confusion is, should I copy only <strong>4</strong> entries from <strong>NS</strong> record to Godaddy configuration?</li>
<li>Where and how I will map example.com to take user to Elastic Load balancer (my-loadbalancer.us-east-1.elb.amazonaws.com) By creating a new Recordset in same hosted zone?</li>
<li>I need to move email to Google Apps, I think, it will require MX record (in same hosted zone)?</li>
<li>Is that every thing I require to leave my old hosting company and switch to AWS (though not leaving Godaddy) ?</li>
</ol>
","<amazon-web-services><domain-name><dns-hosting><dns-zone>","2016-05-18 23:05:39"
"919928","Can't login to IMAP/POP3 server external","<p>I have installed Postfix, Dovecot and MySQL via this tutorial (<a href=""https://www.linode.com/docs/email/postfix/troubleshooting-problems-with-postfix-dovecot-and-mysql/"" rel=""nofollow noreferrer"">https://www.linode.com/docs/email/postfix/troubleshooting-problems-with-postfix-dovecot-and-mysql/</a>).</p>

<p>When I do the following command, I get the following response:</p>

<pre><code>$ openssl s_client -connect mail.domain.com:993

* OK [CAPABILITY IMAP4rev1 LITERAL+ SASL-IR LOGIN-REFERRALS ID ENABLE IDLE AUTH=PLAIN] Dovecot ready.
</code></pre>

<p>I can login to my virtual accounts via the console, but when I'm using this PHP-script, it fails (the page keeps loading).</p>

<pre><code>&lt;?php 
    $mbox = imap_open(""{mail.domain.com:993}"", ""user@domain.com"", ""password"");
?&gt;
</code></pre>

<p>When I run the dovecot -n command, I get the following output:</p>

<pre><code>$ dovecot -n

# 2.2.22 (fe789d2): /etc/dovecot/dovecot.conf
# Pigeonhole version 0.4.13 (7b14904)
# OS: Linux 4.4.0-130-generic x86_64 Ubuntu 16.04.3 LTS
auth_debug = yes
auth_verbose = yes
log_path = /var/log/dovecot.log
mail_location = mbox:~/mail:INBOX=/var/mail/%u
mail_privileged_group = mail
namespace inbox {
  inbox = yes
  location =
  mailbox Drafts {
    special_use = \Drafts
  }
  mailbox Junk {
    special_use = \Junk
  }
  mailbox Sent {
    special_use = \Sent
  }
  mailbox ""Sent Messages"" {
    special_use = \Sent
  }
  mailbox Trash {
    special_use = \Trash
  }
  prefix =
}
passdb {
  args = /etc/dovecot/dovecot-sql.conf.ext
  driver = sql
}
protocols = imap pop3 lmtp
service auth-worker {
  user = vmail
}
service auth {
  unix_listener /var/spool/postfix/private/auth {
    group = postfix
    mode = 0666
    user = postfix
  }
  unix_listener auth-userdb {
    mode = 0600
    user = vmail
  }
  user = dovecot
}
service imap-login {
  inet_listener imap {
    port = 0
  }
}
service lmtp {
  unix_listener /var/spool/postfix/private/dovecot-lmtp {
    group = postfix
    mode = 0600
    user = postfix
  }
}
service pop3-login {
  inet_listener pop3 {
    port = 0
  }
}
ssl = required
ssl_cert = &lt;/etc/dovecot/private/dovecot.crt
ssl_key = &lt;/etc/dovecot/private/dovecot.key
userdb {
  args = uid=vmail gid=vmail home=/var/mail/vhosts/%d/%n
  driver = static
}
</code></pre>

<p>I use the firewall 'ufw'.</p>

<pre><code>$ ufw status

Status: active

To                         Action      From
--                         ------      ----
Apache Full                ALLOW       Anywhere
993                        ALLOW       Anywhere
995                        ALLOW       Anywhere
587                        ALLOW       Anywhere
OpenSSH                    ALLOW       Anywhere
Dovecot POP3               ALLOW       Anywhere
Dovecot Secure IMAP        ALLOW       Anywhere
Dovecot Secure POP3        ALLOW       Anywhere
Postfix                    ALLOW       Anywhere
Postfix SMTPS              ALLOW       Anywhere
Postfix Submission         ALLOW       Anywhere
Apache Full (v6)           ALLOW       Anywhere (v6)
993 (v6)                   ALLOW       Anywhere (v6)
995 (v6)                   ALLOW       Anywhere (v6)
587 (v6)                   ALLOW       Anywhere (v6)
OpenSSH (v6)               ALLOW       Anywhere (v6)
Dovecot POP3 (v6)          ALLOW       Anywhere (v6)
Dovecot Secure IMAP (v6)   ALLOW       Anywhere (v6)
Dovecot Secure POP3 (v6)   ALLOW       Anywhere (v6)
Postfix (v6)               ALLOW       Anywhere (v6)
Postfix SMTPS (v6)         ALLOW       Anywhere (v6)
Postfix Submission (v6)    ALLOW       Anywhere (v6)
</code></pre>

<p>Is this a firewall problem? It seems like I only can connect from localhost, not from remote.</p>

<p>Thanks in advance.</p>

<p>PS: if you need more information, I'll be happy to share. But I don't know exactly the information you all need for this problem.</p>

<p>Edit: my SMTP server also does not work. I tested it with this (<a href=""https://www.wormly.com/test-smtp-server"" rel=""nofollow noreferrer"">https://www.wormly.com/test-smtp-server</a>) tool.</p>

<p>Output:</p>

<pre><code>Resolving hostname...
Connecting...
Connection: opening to mail.domain.com:25, timeout=300, options=array (
                 )
Connection: opened
SERVER -&gt; CLIENT: 220 mail.domain.com ESMTP Postfix (Ubuntu)
CLIENT -&gt; SERVER: EHLO tools.wormly.com
SERVER -&gt; CLIENT: 250-mail.domain.com
                 250-PIPELINING
                 250-SIZE 10240000
                 250-VRFY
                 250-ETRN
                 250-STARTTLS
                 250-ENHANCEDSTATUSCODES
                 250-8BITMIME
                 250 DSN
CLIENT -&gt; SERVER: STARTTLS
SERVER -&gt; CLIENT: 454 4.7.0 TLS not available due to local problem
SMTP ERROR: STARTTLS command failed: 454 4.7.0 TLS not available due to local problem
2018-07-07 17:06:08 SMTP Error: Could not connect to SMTP host.
CLIENT -&gt; SERVER: QUIT
SERVER -&gt; CLIENT: 221 2.0.0 Bye
Connection: closed
2018-07-07 17:06:08 SMTP connect() failed. https://github.com/PHPMailer/PHPMailer/wiki/Troubleshooting
Message sending failed.
</code></pre>
","<postfix><dovecot><imap><pop3>","2018-07-07 17:02:10"
"919970","Postfix as smart host for Exchange 2016","<p>I am trying to set up <code>Postfix</code> as a smart host for an Exchange 2016 server. Right now I have it successfully relaying mail in to Exchange from the internet but outbound relay is not working.</p>

<p>I'm looking to get the simplest setup to essentially make this box be a mail proxy for Exchange 2016. </p>

<ul>
<li><code>mx1.example.com</code> == Exchange server (A Record)</li>
<li><code>mailfw01.example.com</code> == this postfix server (A record)</li>
<li><code>example.com. MX mailfw01.example.com.</code> (MX Record)</li>
</ul>

<p>Inbound Mail Flow (works in the current config):</p>

<ul>
<li>AnyDomain -> <code>mailfw01.example.com:25</code> -> <code>mx1.example.com:50510</code></li>
<li>(Sender)  -> (In to postfix on port 25) -> (In to Exchange on port 50510)</li>
</ul>

<p>Outbound Mail (does not work):</p>

<ul>
<li>Exchange -> Postfix -> Any Remote Domain</li>
<li>Exchange outbound connector as smarthost to Postfix -> Postfix(?) -> recipient</li>
</ul>

<p>Below is the postfix configuration I have:</p>

<pre><code># See /usr/share/postfix/main.cf.dist for a commented, more complete version

smtpd_banner = $myhostname ESMTP $mail_name (Ubuntu)
biff = no

# appending .domain is the MUA's job.
append_dot_mydomain = no   
readme_directory = no

# TLS parameters
smtpd_tls_cert_file=/etc/ssl/certs/ssl-cert-snakeoil.pem
smtpd_tls_key_file=/etc/ssl/private/ssl-cert-snakeoil.key
smtpd_use_tls=yes
smtpd_tls_session_cache_database = btree:${data_directory}/smtpd_scache
smtp_tls_session_cache_database = btree:${data_directory}/smtp_scache

smtpd_relay_restrictions = permit_mynetworks, reject_unauth_destination
myhostname = mailfw01.example.com
alias_maps = hash:/etc/aliases
alias_database = hash:/etc/aliases
myorigin = /etc/mailname
mydestination = mailfw01.example.com, mailfw1.example.com, example.com, localhost.localdomain, localhost
relayhost = mx1.example.com:50510
mynetworks = 0.0.0.0/0
mailbox_size_limit = 0
recipient_delimiter = +
inet_interfaces = all
inet_protocols = all

#added lines
mydomain = example.com
relay_domains = $mydestination
smtpd_sender_restrictions = reject_unknown_sender_domain
smtpd_recipient_restrictions = permit_mynetworks,\
        reject_unauth_destination,reject_invalid_hostname,\
        reject_unauth_pipelining,reject_non_fqdn_sender, \
        reject_unknown_recipient_domain,reject_unknown_sender_domain
transport_maps = hash:/etc/postfix/transport
relay_recipient_maps =
local_recipient_maps =
compatibility_level = 2
</code></pre>
","<postfix><exchange>","2018-07-08 02:05:20"
"777998","Removing 1 line of code from all php files","<p>I can see same code under all php files.</p>

<p>Starting from </p>

<pre><code>&lt;?php $rfghoh = '*f%)sfxpmpusut)tpqssu6
</code></pre>

<p>And end </p>

<pre><code>$rfghoh=$yuoopi-1; ?&gt;
</code></pre>

<p><strong>I am just looking for script using which only first line of php code get remove not the entire php file.</strong></p>

<p>I tried <code>find -name '*.php' -exec sed -i '/&lt;?php $rfghoh/,/?&gt;/d' '{}' \;</code></p>

<p>But its removing all content under files though I need to remove just under those tags.</p>

<p>Thanks</p>
","<linux><php><ssh><unix>","2016-05-20 12:44:08"
"851040","Monitoring a Servers Connections to a default gateway via SNMP","<p>I wanted to know how to configure SNMP to monitor a servers connection to a gateway:</p>

<p>I have a server at location A running an SNMP monitoring software
I have another server at Location B running my companies services</p>

<p>Is it possible to have an SNMP command that is run on Server A to check Server Bs connection to a gateway, like a ping command to 8.8.8.8</p>

<p>For example, running a command like:</p>

<pre><code>[Server A%] snmpwalk -c community -v 1 Server.B.IP.Address  1.3.6.1.4.etc.etc.ping.8.8.8.8
</code></pre>

<p>Both Servers are running CentOS 6.</p>
","<linux><centos6><ping><snmp>","2017-05-19 06:16:05"
"778390","How can I connect a group of vms in Azure?","<p>I have a number of Azure Linux VMs. I am able to ssh into all of them using the public azure connection info, however I am not able to ssh from one vm into another vm using the private info. </p>

<p>I have found that there are different gateways on the vms, and I am thinking that might be the issue.</p>

<p>VM 1</p>

<pre><code>$ route -n

Destination  Gateway       Genmask         Flags Metric Ref    Use Iface
xx.xx.xx.16  xx.xx.xx.25   255.255.255.255 UGH   0      0        0 eth0
xx.xx.xx.24  0.0.0.0       255.255.255.248 U     0      0        0 eth0
yy.yy.0.0    0.0.0.0       255.255.0.0     U     1002   0        0 eth0
yy.yy.0.0    0.0.0.0       255.240.0.0     U     0      0        0 eth1
0.0.0.0      xx.xx.xx.25   0.0.0.0         UG    0      0        0 eth0
0.0.0.0      xx.xx.xx.25   0.0.0.0         UG    0      0        0 eth0
</code></pre>

<p>VM 2</p>

<pre><code>$ route -n

Destination  Gateway       Genmask         Flags Metric Ref    Use Iface
xx.xx.xx.16  xx.xx.xx.33   255.255.255.255 UGH   0      0        0 eth0
xx.xx.xx.32  0.0.0.0       255.255.255.248 U     0      0        0 eth0
yy.yy.0.0    0.0.0.0       255.255.0.0     U     1002   0        0 eth0
yy.yy.0.0    0.0.0.0       255.240.0.0     U     0      0        0 eth1
0.0.0.0      xx.xx.xx.33   0.0.0.0         UG    0      0        0 eth0
0.0.0.0      xx.xx.xx.33   0.0.0.0         UG    0      0        0 eth0
</code></pre>

<p>I've tried setting up routes on one vm to match the route on the other vm. </p>

<p>How can I get these vms to communicate to each other?</p>

<p>Edit:</p>

<p>Error message is:</p>

<pre><code>ssh: connect to host xxxxxxx port 22: No route to host
</code></pre>

<p>Edit:</p>

<p>Turns out my virtual network was configured with an address space of 172.21.0.0/25, but there are two subnets 172.21.0.0/28 and 172.21.0.16/29.</p>

<p>I'm pretty sure the subnets are the issue, and I'll update once I make the subnet space match the address space.</p>

<p>Edit:</p>

<p>I created a new subnet on 172.21.0.64/26 which allows me to let all of my VMs join a single subnet.</p>

<p>For some reason the Azure web portal was not allowing me to attach a VM to a subnet.</p>
","<linux><networking><centos><azure>","2016-05-23 02:15:34"
"778476","Windows Server connection using putty","<p>Iam having hard time connecting to an Azure Windows Server 2012 from my institution (University). The goal is to connect to the SQL Server that runs on this Windows server</p>

<p>This is the strange situation:</p>

<p>From my house, i connect with RDP, ssh and sql management studio fine, with no problem.</p>

<p>From the institution, i can connect only with RDP!! The other to ways are not working. </p>

<p>I asked the network administrator and told me that they are not using any filtering at all. I wonder then...whose problem is this?</p>

<p>To give an example, when I try to connect with ssh (putty), I get the following screen.</p>

<p><a href=""https://i.sstatic.net/LBzyt.png"" rel=""nofollow noreferrer"">Putty login process error</a></p>

<p>Just after i type the password, the screen pauses and nothing happens. </p>

<p>The client runs Windows 10.
The server runs windows Server 2012 R2</p>
","<windows-server-2012><ssh-tunnel><putty>","2016-05-23 12:46:40"
"778500","trouble deploying django under apache on centos with mod_wsgi","<p>EDIT 2:
I got this running mostly!  It was indeed a SELinux issue.   So most things work, I get a cannot write to a read only database error.  And some of my things that seem like static are not found, primarily the /admin isn't seen on anything. I am not sure if it is mysite.settings issue or what.</p>

<p>EDIT:
Thanks to the commenter below I double checked everything and got a new error.    This is progress perhaps, but looks like indeed i have some SELinux policy enabled, and not being a sys-admin by trade, I am not sure where to start looking to figure out what this is telling me.  Here is the apache error_log</p>

<pre><code>[Wed May 25 10:08:16.399785 2016] [core:notice] [pid 14935] SELinux policy enabled; httpd running as context system_u:system_r:httpd_t:s0
[Wed May 25 10:08:16.401177 2016] [suexec:notice] [pid 14935] AH01232: suEXEC mechanism enabled (wrapper: /usr/sbin/suexec)
[Wed May 25 10:08:16.432549 2016] [so:warn] [pid 14935] AH01574: module wsgi_module is already loaded, skipping
AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.20.0.33. Set the 'ServerName' directive globally to suppress this message
[Wed May 25 10:08:16.438588 2016] [auth_digest:notice] [pid 14935] AH01757: generating secret for digest authentication ...
[Wed May 25 10:08:16.439945 2016] [lbmethod_heartbeat:notice] [pid 14935] AH02282: No slotmem from mod_heartmonitor
[Wed May 25 10:08:16.448393 2016] [mpm_prefork:notice] [pid 14935] AH00163: Apache/2.4.6 (CentOS) mod_wsgi/3.4 Python/2.7.5 OpenSSL/1.0.1e-fips mod_fcgid/2.3.9 configured -- resuming normal operations
[Wed May 25 10:08:16.448434 2016] [core:notice] [pid 14935] AH00094: Command line: '/usr/sbin/httpd -D FOREGROUND'
[Wed May 25 10:08:20.102783 2016] [mime_magic:error] [pid 14937] [client 172.20.0.33:59454] AH01512: mod_mime_magic: can't read `/home/sthomas/django_nga_site/mysite/wsgi.py'
[Wed May 25 10:08:20.117380 2016] [:error] [pid 14937] (13)Permission denied: [client 172.20.0.33:59454] mod_wsgi (pid=14937, process='', application='172.20.0.33|/nga_sw'): Call to fopen() failed for '/home/sthomas/django_nga_site/mysite/wsgi.py'.
</code></pre>

<p>It gives me a 500 error I think if i try to hit the url.</p>

<p>I should add the permissions on the wsgi.py file is:</p>

<p><code>-rw-r--r-x</code>
and the permissions on the folders up through home that the wsgi.py file is in is:</p>

<p><code>drwxr-xr-x</code></p>

<p>Reading through the questions that may already have my answer, it didn't look like those were the ones so here I am posting in hopes of a lead.</p>

<p>I was trying to use this url:</p>

<p><a href=""https://docs.djangoproject.com/en/1.9/howto/deployment/wsgi/modwsgi/"" rel=""nofollow noreferrer"">https://docs.djangoproject.com/en/1.9/howto/deployment/wsgi/modwsgi/</a></p>

<p>my versions:</p>

<p>Server version: Apache/2.4.6 (CentOS)
Server built:   Nov 19 2015 21:43:13</p>

<p>CentOs version:
CentOS Linux release 7.2.1511 (Core) </p>

<p>Python Version:
Python 2.7.5</p>

<p>Django version:
(1, 9, 6, 'final', 0)</p>

<p>On a centos box that I do not manage, but I have sudo access to so I was able to install everything needed to get my django app running using the <code>python manage.py runserver</code>.  Looked good, so I wanted to tackle the next thing which was getting it to run under apache.  </p>

<p>Apache was already serving a static version of my website under /var/www/html   and it was running fine, though I added my own magic for the WSGI stuff to httpd.conf file and loaded the mod_wsgi too I thought.  Though all I get now when hitting the ip address of the url is:</p>

<p><code>You don't have permission to access / on this server.</code></p>

<p>So I looked back through the httpd.conf and don't try to set (alias?) anything to just / so I am not sure what it is trying to do.</p>

<p>my app is in the folder /home/sthomas under the values there and I thought I had the right chwon and chmod on set on it. </p>

<p>To get this far it took me a while to realize I had to move/del/rename the welcome.conf file that apache had setup to not see the default welcome screen every time I hit my URL (my linux kung-fu is basic)</p>

<pre><code>#
# This is the main Apache HTTP server configuration file.  It contains the
# configuration directives that give the server its instructions.
# See &lt;URL:http://httpd.apache.org/docs/2.4/&gt; for detailed information.
# In particular, see 
# &lt;URL:http://httpd.apache.org/docs/2.4/mod/directives.html&gt;
# for a discussion of each configuration directive.
#
# Do NOT simply read the instructions in here without understanding
# what they do.  They're here only as hints or reminders.  If you are unsure
# consult the online docs. You have been warned.  
#
# Configuration and logfile names: If the filenames you specify for many
# of the server's control files begin with ""/"" (or ""drive:/"" for Win32), the
# server will use that explicit path.  If the filenames do *not* begin
# with ""/"", the value of ServerRoot is prepended -- so 'log/access_log'
# with ServerRoot set to '/www' will be interpreted by the
# server as '/www/log/access_log', where as '/log/access_log' will be
# interpreted as '/log/access_log'.

#
# ServerRoot: The top of the directory tree under which the server's
# configuration, error, and log files are kept.
#
# Do not add a slash at the end of the directory path.  If you point
# ServerRoot at a non-local disk, be sure to specify a local disk on the
# Mutex directive, if file-based mutexes are used.  If you wish to share the
# same ServerRoot for multiple httpd daemons, you will need to change at
# least PidFile.
#
ServerRoot ""/etc/httpd""

#
# Listen: Allows you to bind Apache to specific IP addresses and/or
# ports, instead of the default. See also the &lt;VirtualHost&gt;
# directive.
#
# Change this to Listen on specific IP addresses as shown below to 
# prevent Apache from glomming onto all bound IP addresses.
#
#Listen 12.34.56.78:80
Listen 80

#
# Dynamic Shared Object (DSO) Support
#
# To be able to use the functionality of a module which was built as a DSO you
# have to place corresponding `LoadModule' lines at this location so the
# directives contained in it are actually available _before_ they are used.
# Statically compiled modules (those listed by `httpd -l') do not need
# to be loaded here.
#
# Example:
# LoadModule foo_module modules/mod_foo.so
LoadModule wsgi_module modules/mod_wsgi.so
#
Include conf.modules.d/*.conf

#
# If you wish httpd to run as a different user or group, you must run
# httpd as root initially and it will switch.  
#
# User/Group: The name (or #number) of the user/group to run httpd as.
# It is usually good practice to create a dedicated user and group for
# running httpd, as with most system services.
#
User apache
Group apache

# 'Main' server configuration
#
# The directives in this section set up the values used by the 'main'
# server, which responds to any requests that aren't handled by a
# &lt;VirtualHost&gt; definition.  These values also provide defaults for
# any &lt;VirtualHost&gt; containers you may define later in the file.
#
# All of these directives may appear inside &lt;VirtualHost&gt; containers,
# in which case these default settings will be overridden for the
# virtual host being defined.
#

#
# ServerAdmin: Your address, where problems with the server should be
# e-mailed.  This address appears on some server-generated pages, such
# as error documents.  e.g. admin@your-domain.com
#
ServerAdmin root@localhost

#
# ServerName gives the name and port that the server uses to identify itself.
# This can often be determined automatically, but we recommend you specify
# it explicitly to prevent problems during startup.
#
# If your host doesn't have a registered DNS name, enter its IP address here.
#
#ServerName www.example.com:80

#
# Deny access to the entirety of your server's filesystem. You must
# explicitly permit access to web content directories in other 
# &lt;Directory&gt; blocks below.
#
&lt;Directory /&gt;
    AllowOverride none
    Require all denied
&lt;/Directory&gt;

#
# Note that from this point forward you must specifically allow
# particular features to be enabled - so if something's not working as
# you might expect, make sure that you have specifically enabled it
# below.
#

#
# DocumentRoot: The directory out of which you will serve your
# documents. By default, all requests are taken from this directory, but
# symbolic links and aliases may be used to point to other locations.
#
#DocumentRoot ""/var/www/html""
DocumentRoot ""/var/www""

#
# Relax access to content within /var/www.
#
&lt;Directory ""/var/www""&gt;
    AllowOverride None
    # Allow open access:
    Require all granted
&lt;/Directory&gt;

# Further relax access to the default document root:
&lt;Directory ""/var/www/html""&gt;
    #
    # Possible values for the Options directive are ""None"", ""All"",
    # or any combination of:
    #   Indexes Includes FollowSymLinks SymLinksifOwnerMatch ExecCGI MultiViews
    #
    # Note that ""MultiViews"" must be named *explicitly* --- ""Options All""
    # doesn't give it to you.
    #
    # The Options directive is both complicated and important.  Please see
    # http://httpd.apache.org/docs/2.4/mod/core.html#options
    # for more information.
    #
    Options Indexes FollowSymLinks

    #
    # AllowOverride controls what directives may be placed in .htaccess files.
    # It can be ""All"", ""None"", or any combination of the keywords:
    #   Options FileInfo AuthConfig Limit
    #
    AllowOverride None

    #
    # Controls who can get stuff from this server.
    #
    Require all granted
&lt;/Directory&gt;

#
# DirectoryIndex: sets the file that Apache will serve if a directory
# is requested.
#
&lt;IfModule dir_module&gt;
    DirectoryIndex index.html
&lt;/IfModule&gt;

#
# The following lines prevent .htaccess and .htpasswd files from being 
# viewed by Web clients. 
#
&lt;Files "".ht*""&gt;
    Require all denied
&lt;/Files&gt;

#
# ErrorLog: The location of the error log file.
# If you do not specify an ErrorLog directive within a &lt;VirtualHost&gt;
# container, error messages relating to that virtual host will be
# logged here.  If you *do* define an error logfile for a &lt;VirtualHost&gt;
# container, that host's errors will be logged there and not here.
#
ErrorLog ""logs/error_log""

#
# LogLevel: Control the number of messages logged to the error_log.
# Possible values include: debug, info, notice, warn, error, crit,
# alert, emerg.
#
LogLevel warn

&lt;IfModule log_config_module&gt;
    #
    # The following directives define some format nicknames for use with
    # a CustomLog directive (see below).
    #
    LogFormat ""%h %l %u %t \""%r\"" %&gt;s %b \""%{Referer}i\"" \""%{User-Agent}i\"""" combined
    LogFormat ""%h %l %u %t \""%r\"" %&gt;s %b"" common

    &lt;IfModule logio_module&gt;
      # You need to enable mod_logio.c to use %I and %O
      LogFormat ""%h %l %u %t \""%r\"" %&gt;s %b \""%{Referer}i\"" \""%{User-Agent}i\"" %I %O"" combinedio
    &lt;/IfModule&gt;

    #
    # The location and format of the access logfile (Common Logfile Format).
    # If you do not define any access logfiles within a &lt;VirtualHost&gt;
    # container, they will be logged here.  Contrariwise, if you *do*
    # define per-&lt;VirtualHost&gt; access logfiles, transactions will be
    # logged therein and *not* in this file.
    #
    #CustomLog ""logs/access_log"" common

    #
    # If you prefer a logfile with access, agent, and referer information
    # (Combined Logfile Format) you can use the following directive.
    #
    CustomLog ""logs/access_log"" combined
&lt;/IfModule&gt;

&lt;IfModule alias_module&gt;
    #
    # Redirect: Allows you to tell clients about documents that used to 
    # exist in your server's namespace, but do not anymore. The client 
    # will make a new request for the document at its new location.
    # Example:
    # Redirect permanent /foo http://www.example.com/bar

    #
    # Alias: Maps web paths into filesystem paths and is used to
    # access content that does not live under the DocumentRoot.
    # Example:
    # Alias /webpath /full/filesystem/path
    #
    # If you include a trailing / on /webpath then the server will
    # require it to be present in the URL.  You will also likely
    # need to provide a &lt;Directory&gt; section to allow access to
    # the filesystem path.

    #
    # ScriptAlias: This controls which directories contain server scripts. 
    # ScriptAliases are essentially the same as Aliases, except that
    # documents in the target directory are treated as applications and
    # run by the server when requested rather than as documents sent to the
    # client.  The same rules about trailing ""/"" apply to ScriptAlias
    # directives as to Alias.
    #
    ScriptAlias /cgi-bin/ ""/var/www/cgi-bin/""

&lt;/IfModule&gt;

#
# ""/var/www/cgi-bin"" should be changed to whatever your ScriptAliased
# CGI directory exists, if you have that configured.
#
&lt;Directory ""/var/www/cgi-bin""&gt;
    AllowOverride None
    Options None
    Require all granted
&lt;/Directory&gt;

&lt;IfModule mime_module&gt;
    #
    # TypesConfig points to the file containing the list of mappings from
    # filename extension to MIME-type.
    #
    TypesConfig /etc/mime.types

    #
    # AddType allows you to add to or override the MIME configuration
    # file specified in TypesConfig for specific file types.
    #
    #AddType application/x-gzip .tgz
    #
    # AddEncoding allows you to have certain browsers uncompress
    # information on the fly. Note: Not all browsers support this.
    #
    #AddEncoding x-compress .Z
    #AddEncoding x-gzip .gz .tgz
    #
    # If the AddEncoding directives above are commented-out, then you
    # probably should define those extensions to indicate media types:
    #
    AddType application/x-compress .Z
    AddType application/x-gzip .gz .tgz

    #
    # AddHandler allows you to map certain file extensions to ""handlers"":
    # actions unrelated to filetype. These can be either built into the server
    # or added with the Action directive (see below)
    #
    # To use CGI scripts outside of ScriptAliased directories:
    # (You will also need to add ""ExecCGI"" to the ""Options"" directive.)
    #
    #AddHandler cgi-script .cgi

    # For type maps (negotiated resources):
    #AddHandler type-map var

    #
    # Filters allow you to process content before it is sent to the client.
    #
    # To parse .shtml files for server-side includes (SSI):
    # (You will also need to add ""Includes"" to the ""Options"" directive.)
    #
    AddType text/html .shtml
    AddOutputFilter INCLUDES .shtml
&lt;/IfModule&gt;

#
# Specify a default charset for all content served; this enables
# interpretation of all content as UTF-8 by default.  To use the 
# default browser choice (ISO-8859-1), or to allow the META tags
# in HTML content to override this choice, comment out this
# directive:
#
AddDefaultCharset UTF-8

&lt;IfModule mime_magic_module&gt;
    #
    # The mod_mime_magic module allows the server to use various hints from the
    # contents of the file itself to determine its type.  The MIMEMagicFile
    # directive tells the module where the hint definitions are located.
    #
    MIMEMagicFile conf/magic
&lt;/IfModule&gt;

#
# Customizable error responses come in three flavors:
# 1) plain text 2) local redirects 3) external redirects
#
# Some examples:
#ErrorDocument 500 ""The server made a boo boo.""
#ErrorDocument 404 /missing.html
#ErrorDocument 404 ""/cgi-bin/missing_handler.pl""
#ErrorDocument 402 http://www.example.com/subscription_info.html
#

#
# EnableMMAP and EnableSendfile: On systems that support it, 
# memory-mapping or the sendfile syscall may be used to deliver
# files.  This usually improves server performance, but must
# be turned off when serving from networked-mounted 
# filesystems or if support for these functions is otherwise
# broken on your system.
# Defaults if commented: EnableMMAP On, EnableSendfile Off
#
#EnableMMAP off
EnableSendfile on

# Supplemental configuration
#
# Load config files in the ""/etc/httpd/conf.d"" directory, if any.
IncludeOptional conf.d/*.conf

WSGIScriptAlias /nga_sw /home/sthomas/django_nga_site/mysite/wsgi.py
WSGIPythonPath /home/sthomas/django_nga_site

&lt;Directory /home/sthomas/django_nga_site/mysite&gt;
&lt;Files wsgi.py&gt;
Require all granted
&lt;/Files&gt;
&lt;/Directory&gt;
</code></pre>
","<centos><apache-2.4><python><django><mod-wsgi>","2016-05-23 15:17:20"
"778527","Assigning user a virtual machine","<p>Is this possible?  I started a new position, and in one of the environments, logging in through horizon view, a user randomly logs on to one of four machines when they login.  Many of the users get frustrated because all of their data and documents are on a different machine than they logged into.  </p>

<p>What is the best solution to this problem?  </p>

<p>My goal is that when a user logs on, their desktop, documents and everything goes with them.</p>

<p>The only option in Horizon View is which one of the two domains I can log onto, not which machine.  </p>
","<windows><vmware-vsphere><vdi><vmware-horizon-view>","2016-05-23 17:35:22"
"778612","autofs mounting without passwords","<p>I have an existing <code>autofs</code> Centos-Centos mount. I forgot how I set those up without having to require a password.</p>

<pre><code># /etc/auto.folder
folder -fstype=nfs,rw 10.10.10.1:/folder
</code></pre>

<p>Now, I'm trying to setup another mount but I get access denied.</p>

<pre><code># /etc/auto.folder
folder        -fstype=nfs,rw 10.10.10.1:/folder
anotherfolder -fstype=nfs,rw 10.10.10.2:/anotherfolder
</code></pre>

<p>Manual <code>mount</code> of <code>anotherfolder</code> expects a password while <code>folder</code> doesn't. Is whitelisting a server/folder possible?</p>

<p>Please help. </p>
","<mount><autofs>","2016-05-24 03:28:01"
"778616","Web server files being publicly served despite permissions set to user/group only","<p>I'm running an Apache2 server on Raspbian that serves an instance of DokuWiki, however I seem to be running into permissions issues.</p>

<p>For some reason all my directories and the files in them are accessible publicly on the server. All the directories are recursively set to chmod <em>770</em>, I even tried <em>700</em>, yet I can still access them in the browser. I previously had it set up where everything was owned by my user, <em>pi</em>, but even when putting everything under the <em>www-data</em> user, I can't get it to work.</p>

<p>There are .htaccess files that come with DokuWiki that are also supposed to prevent access but those are not functioning either. I've enabled <em>mod-rewrite</em> and <em>AllowOverride All</em>, still no luck...</p>

<p><strong>Example of my permissions setup:</strong></p>

<pre><code>pi@piserver:/var/www/html $ ls -al
total 96
drwxr-xr-x  9 www-data www-data  4096 May 24 03:15 .
drwxr-xr-x  3 root     root      4096 May 24 02:21 ..
drwxrwx---  2 www-data www-data  4096 May 24 02:39 bin
drwxrwx---  2 www-data www-data  4096 May 24 03:02 conf
-rw-r--r--  1 www-data www-data 18092 May 24 02:39 COPYING
drwxrwx--- 12 www-data www-data  4096 May 24 02:39 data
-rw-r--r--  1 www-data www-data  3674 May 24 02:39 doku.php
-rw-r--r--  1 www-data www-data 19372 May 24 02:39 feed.php
-rwxr-x---  1 www-data www-data    66 May 24 03:15 .htaccess
drwxrwxr-x  6 www-data www-data  4096 May 24 02:39 inc
-rw-r--r--  1 www-data www-data   182 May 24 02:39 index.php
drwxrwxr-x  8 www-data www-data  4096 May 24 02:39 lib
drwxrwx---  2 www-data www-data  4096 May 24 03:03 old
-rw-r--r--  1 www-data www-data   306 May 24 02:39 README
drwxrwx---  5 www-data www-data  4096 May 24 02:40 vendor
-rw-r--r--  1 www-data www-data    23 May 24 02:39 VERSION

pi@piserver:/var/www/html/data $ ls -al
total 100
drwxrwx--- 12 www-data www-data  4096 May 24 02:39 .
drwxr-xr-x  9 www-data www-data  4096 May 24 03:15 ..
drwxrwx---  2 www-data www-data  4096 May 24 03:08 attic
drwxrwx--- 16 www-data www-data  4096 May 24 03:08 cache
-rwxrwx---  1 www-data www-data 20836 May 24 02:39 deleted.files
-rwxrwx---  1 www-data www-data    32 May 24 02:39 _dummy
-rwxrwx---  1 www-data www-data    31 May 24 02:39 .htaccess
drwxrwx---  2 www-data www-data  4096 May 24 03:08 index
drwxrwx---  2 www-data www-data  4096 May 24 03:08 locks
drwxrwx---  3 www-data www-data  4096 May 24 02:39 media
drwxrwx---  2 www-data www-data  4096 May 24 02:39 media_attic
drwxrwx---  2 www-data www-data  4096 May 24 02:39 media_meta
drwxrwx---  3 www-data www-data  4096 May 24 03:08 meta
drwxrwx---  4 www-data www-data  4096 May 24 03:08 pages
-rwxrwx---  1 www-data www-data  6516 May 24 02:39 security.png
-rwxrwx---  1 www-data www-data 12093 May 24 02:39 security.xcf
drwxrwx---  2 www-data www-data  4096 May 24 02:39 tmp
</code></pre>

<p><strong>My .htaccess files look like this:</strong></p>

<pre><code>order allow,deny
deny from all
</code></pre>

<p><strong><em>What exactly am I doing wrong here?</em></strong></p>
","<linux><debian><permissions><web-server><apache-2.4>","2016-05-24 04:10:34"
"920631","How to copy user permissions in Windows server 2012 r2?","<p>I have windows server 2012 r2 and two storage drive. lest say A and B. I have created users and given files and folder access in drive A but now it is crashed and now I want to give all the same user permission in Drive B. I have same folders in both drives. How can I achieve it?  Is there any way to copy the access rights and permissions? </p>
","<windows-server-2012-r2><windows-server-2012><filesystems><user-permissions><robocopy>","2018-07-12 10:53:01"
"920671","Windows Update KB4338819 ""Can't create object"" COM component","<p>We are using on a C# COM component to link our asp classic app to SQL Server.</p>

<p>Since the windows update <code>KB4338819</code>, when I run the app we get the ASP exception: </p>

<pre><code>Microsoft JScript runtime error '800a01ad'
Automation server can't create object
</code></pre>

<p>executing ASP line:</p>

<pre><code>var foo = Server.CreateObject(""MyComComponent"");
</code></pre>

<p>If I uninstall the update it works.</p>

<p>The application pool is set to allow 32 bit applications (value: <code>true</code>) (even without the update, if this is set to <code>false</code> we get the same error)</p>

<p>Ideas? 
Is there a better place to ask such question?</p>

<p>Thanks!</p>

<hr>

<p>UPDATE:</p>

<p>Microsoft acknowledges the issue but they are still working on a solution.
Meanwhile they suggest some workarounds here:</p>

<ul>
<li><a href=""https://support.microsoft.com/en-us/help/4345913/access-denied-errors-after-installing-july-2018-security-rollup-update"" rel=""nofollow noreferrer"">https://support.microsoft.com/en-us/help/4345913/access-denied-errors-after-installing-july-2018-security-rollup-update</a></li>
</ul>
","<windows-update><com>","2018-07-12 14:28:52"
"920687","How to diagnose/fix slow file management on Windows Server 2012?","<p>The setup is:  </p>

<ul>
<li>FUJITSU PRIMERGY TX300 S7  </li>
<li>RAID Ctrl SAS 6g 5/6 512mb (d2616)  </li>
<li>Windows Server 2012 64 bit  </li>
<li>Volume C: RAID 1 (2 HDDs)  </li>
<li>Volume D: RAID 5 (8 HDDs)  </li>
</ul>

<p>The problem is:<br>
When we do something with a large amount of files on volume <code>D:</code>, at first everything is OK, but after several minutes speed goes drastically down (if it's deleting - it goes from 100 files/sec to 1 file/sec; if it's copying - from 100 MB/sec to 15 MB/sec).<br>
Sometimes volume <code>D:</code> becomes inaccessible (it is still visible in Explorer, but the used space bar disappears).<br>
And sometimes the system freezes so hard - it even stops repying to pings.</p>

<p>We thought that it might be something to do with caching, but we can't disable it, we get the ""<code>windows could not change the write-caching setting for the device</code>"" error.</p>

<p>How do we diagnose/fix the problem? Please help</p>
","<windows-server-2012><filesystems><raid5><lsi>","2018-07-12 15:45:59"
"851504","vsftp not able to login from localhost","<p>I have setup <code>vsftpd</code> on a centos-7. There are simple and straight forward tutorials for that. However, not only from outside of the server, even from the <code>localhost</code>, I am not able to login via ftp.</p>

<pre><code>[root@localhost ~]# cat /etc/vsftpd/ftpusers
# Users that are not allowed to login via ftp
root
mahmood
bin
daemon
adm
lp
sync
shutdown
halt
mail
news
uucp
operator
games
nobody
[root@localhost ~]# ftp mahmood@localhost
ftp: mahmood@localhost: Name or service not known
ftp&gt; quit

[root@localhost ~]# ftp localhost
Trying ::1...
Connected to localhost (::1).
220 (vsFTPd 3.0.2)
Name (localhost:root): root
530 Permission denied.
Login failed.
ftp&gt; quit
221 Goodbye.
</code></pre>

<p>Any idea about that?</p>
","<centos7><vsftpd>","2017-05-22 08:54:30"
"851511","My server virtual memory full","<p>occasionally my server hits 100% usage on virtual memory and I don't know if that is something alarming or not? ( this happends gradualy to fill to that percent over 1-2 weeks from clean swap)</p>

<p>I usually resolve this issue like this:</p>

<pre><code>swapoff -a &amp;&amp; swapon -a
</code></pre>

<p>do you suggest any other method of resolving this?</p>

<p>I tried to see what is in swap and clearly it's <code>php-cgi</code> or each sites processes that runs on my server..</p>

<p>after I run this:</p>

<pre><code>for file in /proc/*/status ; do awk '/VmSwap|Name/{printf $2 "" "" $3}END{ print """"}' $file; done | sort -k 2 -n -r | less
</code></pre>

<p>I get this result: </p>

<pre><code>miniserv.pl 17096 kB
php-cgi7.0 15732 kB
php-cgi7.0 15532 kB
php-cgi7.0 13768 kB
php-cgi7.0 13532 kB
php-cgi7.0 13300 kB
php-cgi7.0 12056 kB
php-cgi7.0 11004 kB
php-cgi7.0 9636 kB
php-cgi7.0 4712 kB
php-cgi7.0 3700 kB
php-cgi7.0 3600 kB
php-cgi7.0 3288 kB
php-cgi7.0 3100 kB
php-cgi7.0 3084 kB
php-cgi7.0 3040 kB
php-cgi7.0 2824 kB
php-cgi7.0 2756 kB
(sd-pam) 2580 kB
php-cgi7.0 2524 kB
php-cgi7.0 2524 kB
(sd-pam) 2280 kB
(sd-pam) 2192 kB
php-loop.pl 2076 kB
php-loop.pl 2076 kB
php-loop.pl 2072 kB
php-loop.pl 2072 kB
php-loop.pl 2072 kB
php-loop.pl 2072 kB
php-loop.pl 2072 kB
php-loop.pl 2072 kB
(sd-pam) 2036 kB
php-loop.pl 1976 kB
php-loop.pl 1964 kB
php-loop.pl 1964 kB
php-loop.pl 1964 kB
php-loop.pl 1960 kB
php-loop.pl 1956 kB
php-loop.pl 1956 kB
php-loop.pl 1948 kB
php-loop.pl 1940 kB
php-loop.pl 1920 kB
php-loop.pl 1908 kB
php-loop.pl 1908 kB
php-loop.pl 1904 kB
php-loop.pl 1872 kB
</code></pre>
","<php><ssd><swap><webmin><virtualmin>","2017-05-22 09:26:36"
"778716","Compare 20 files with diff, not 2","<h1>Background</h1>

<p>We are moving from managing hosts by hand to configuration management.</p>

<h1>20 files</h1>

<p>I want to compare 20 times a config file from 20 hosts. For example <code>/etc/crontab</code></p>

<h1>Use case</h1>

<p>I guess about 15 of 20 files are identical. I want to see the five files which where modified with ""vi"" by hand.</p>

<p>I want an overview, no automated action like patching ...</p>

<h1>How to compare them ...?</h1>

<p>I tried my favorit diff tool (meld), but it does not allow more than three files :-(</p>
","<configuration-management><diff>","2016-05-24 13:49:46"
"778739","Meaning of root zone","<p>In a <code>db.root</code> file, what are these lines supposed to mean?</p>

<pre><code>$TTL 86400
. IN SOA A.ROOT-SERVERS.EDU. root.A.ROOT-SERVERS.EDU. (
        1       ; serial number (update count)
        28800   ; refresh (8 h)
        7200    ; retry (2 h)
        604800  ; expire
        10800   ; negative caching
        )
</code></pre>
","<domain-name-system><root><dns-zone>","2016-05-24 15:34:32"
"920884","Forwarding HTTP to HTTPS on Elastic Beanstalk load balancer","<p>I'm using Elastic Beanstalk, and I installed my SSL certificate in the load balancer of an EC2 instance belonging to the EB. 
Every time the server is not healthy, the load balancer delete the instance and creates a new one, which means I'm gonna lose my redirection codes and SSL certificate that are set up inside the instance level.</p>

<p>So, I got to install my SSL certificate on the load balancer through AWS ACM. However, how can I redirect HTTP to HTTPS on the load balancer? If I still leave the redirection codes in the server, the load balancer is gonna remove the redirection codes when the instance is not healthy.</p>

<p>Is setting up redirection inside the instance the only way possible here?</p>

<p><strong>UPDATE</strong></p>

<p>I found some solution that is done under <code>.ebextensions</code>. I'm currently working on Apache server. What I found is the following:</p>

<p>I made a file <code>httpd_redirect.conf</code> under <code>.ebextensions</code> with the below contents.</p>

<pre><code>files:
  ""/etc/httpd/conf.d/httpd_redirect.conf"" :
    mode: ""000644""
    owner: root
    group: root
    content: |
      RewriteEngine On
      RewriteCond %{HTTP_HOST} !^www\. [NC]
      RewriteRule ^(.*) https://www.%{SERVER_NAME}%{REQUEST_URI} [L,R=301]
</code></pre>
","<load-balancing><elastic-beanstalk>","2018-07-13 21:04:05"
"851784","IP route of many subnets - one rule","<p>I want to simply modify routing rule in Linux.
Now I have to add:</p>
<blockquote>
<p>ip route add 5.3.4.3 via 10.4.4.4</p>
<p>ip route add 3.4.5.3 via 10.4.4.4</p>
<p>ip route add 200.45.32.3 via 10.4.4.4</p>
<p>ip route add 9.33.4.3 via 10.4.4.4</p>
</blockquote>
<p>Does exist someting like ipset for ip route? I want to have this IP addresses in one list 5.3.4.3, 3.4.5.3, 200.45.32.3 and 9.33.4.3. I want to have only one route insted of 4(or more).</p>
<p>Thank you.</p>
","<linux><routing><ip><routes>","2017-05-23 12:24:18"
"922148","After Windows 10 update 1803, network fails randomly","<p>We have a network with 4 servers and several clients with W10.</p>

<p>The server's OS versions are: </p>

<ul>
<li>W10 Enterprise: 10.0.17134.137</li>
<li>W10 Pro: 10.0.17134.165</li>
<li>W10 Enterprise: 10.0.17134.165</li>
<li>W10 Pro: 10.0.17134.165</li>
</ul>

<p>After the update, wich took place by the end of May, we are experiencing several network issues: Many Client computers are unable to connect with the servers or connect only sometimes. Also, it affects any resource available on the network. May be a printer, another computer with shared folders...etc...</p>

<p>After looking for solutions over the internet, we have not found any way to solve this situation.</p>

<p>An important clue is that IS POSSIBLE to connect using the IP but not the name of the computer in the network. Maybe there is a problem with the DNS?</p>
","<windows><networking><connection><update>","2018-07-16 16:32:43"
"922182","Pointing sub-domain on Google domains to Wordpress running on Digital Ocean?","<p>I want a sub-domain on my google domains to point to <code>blog.mydomain.example</code>. Currently, it points to DNS at the root domain to Github pages. I want that to remain the same, but the blog subdomain to point to my wordpress running on digital ocean. </p>

<p>How can I do this?</p>
","<domain-name-system><dns-zone><a-record><digital-ocean><google-domains>","2018-07-16 21:27:15"
"779338","Monit not able to start program in screen","<p>I have monit able to watch the program but the start script is not working. The stop script works just fine.</p>

<p>The when monit tries to start it, it just says execution failed.</p>

<p>Monit part</p>

<pre><code>check process tsdnsserver
  matching ""tsdnsserver""
  start program = ""/bin/bash -c '/root/ts3/tsdns/stop.sh'""
  stop program = ""/bin/bash -c '/root/ts3/tsdns/stop.sh'""
  if failed port 41144 type tcp then restart
  if 2 restarts within 3 cycles then timeout
</code></pre>

<p>Start Script</p>

<pre><code>#!/bin/bash
screen -dmS tsdns ./tsdnsserver 41144
</code></pre>

<p>Stop Script</p>

<pre><code>#!/bin/bash
screen -X -S tsdns kill
</code></pre>
","<bash><monit>","2016-05-27 05:16:40"
"852320","Letsencrypt + Cloudflare setup","<p>I work on many side projects and try to set up https for each one. Since I can't afford to pay for certs, I use Letsencrypt. I also use Cloudflare in case I get traffic spikes. I start with getting an SSL cert with Letsencrypt, then put Cloudflare in front of it.</p>

<p>Problem is, when time comes to renew the Letsencrypt cert, I have to turn off Cloudflare in order to do so every time because the IP it sees is now different (Cloudflare's IP instead of my server's IP).</p>

<p>I know Cloudflare provides SSL for free and as long as it has a https connection to my server, it's fine, so technically I don't even have to update my SSL cert at all. But I'd like to keep my server's SSL cert updated so I have the flexibility to turn off Cloudflare at any time and still be up and have a valid cert.</p>

<p>Am I doing something wrong in my process? Is there an option I can use in letsencrypt renew to handle this better, instead of having to log in to Cloudflare to turn it off, wait a few mins, run renew, then turn Cloudflare back on again, every 2 months?</p>

<p>[edit] Getting downvoted for some reasons, maybe because I didn't include my error msg with my setup. Here it is.</p>

<pre><code>-------------------------------------------------------------------------------
Processing /etc/letsencrypt/renewal/[mydomain].com.conf
-------------------------------------------------------------------------------
Cert is due for renewal, auto-renewing...
Renewing an existing certificate
Performing the following challenges:
tls-sni-01 challenge for [mydomain].com
Waiting for verification...
Cleaning up challenges
Attempting to renew cert from /etc/letsencrypt/renewal/[mydomain].com.conf produced an unexpected error: Failed authorization procedure. [mydomain].com (tls-sni-01): urn:acme:error:tls :: The server experienced a TLS error during domain verification :: Failed to connect to 104.18.55.209:443 for tls-sni-01 challenge. Skipping.

IMPORTANT NOTES:
 - The following errors were reported by the server:

   Domain: [mydomain].com
   Type:   tls
   Detail: Failed to connect to 104.18.55.209:443 for tls-sni-01
   challenge
</code></pre>

<p>Obviously, 104.18.55.209 is a Cloudflare IP. It's not able to connect to it for the challenge.</p>

<p>[edit 2]
Tim said to give the CloudFlare Page rules and my domain. My domain is memechicken.com, and my CF Page Rules is just empty. Everything is default setup pretty much. And finally here are my Crypto tab settings that should be relevant:</p>

<p><a href=""https://i.sstatic.net/Xm1zv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Xm1zv.png"" alt=""Crypto screen 1""></a></p>

<p><a href=""https://i.sstatic.net/QxXdQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QxXdQ.png"" alt=""Crypto screen 2""></a></p>

<p>Everything is default. I didn't change anything.</p>
","<cloudflare><lets-encrypt>","2017-05-25 23:44:29"
"922553","How to increase limit of concurrent EC2 instances running in a region?","<p>I am trying to add three t2.small instances to single region, in this case us-east-2, but couldn't do so.</p>

<p>I could edit this question to tell you the precise error message if requested.</p>

<p>How do I increase this limit?</p>
","<amazon-ec2><amazon-web-services><amazon-elb><amazon-cloudformation>","2018-07-18 20:00:01"
"1026004","Best way to update from Windows 10 Enterprise 1607 to 1903 offline","<p>We have a customer that our manufacture shipped some (4) PCs to a customer with a Windows Enterprise 1607 edition on them. The customer found a virus and want to run Windows update on these systems.</p>
<p>As I test, I ran Windows updates on the same OS. After about 3 hours it, was stuck on this download:</p>
<pre><code>2019-04 Cumulative Update for Windows 10 version 1607 for x64-based Systems (KB4493470) at 95%.  
</code></pre>
<p>I was accidentally cut the power to it, and after several reboots, it now seems to be successfully updated to Windows 10 1903.</p>
<p>Question: Is there a way I can do all the downloads offline, then just apply them? How would I identify exactly which files I need?</p>
<p>WSUS is not an option as this is a customer site and we have no control over how they do Windows updates.</p>
<p>I also that you can do it with an iso. I this possible, or would that wipe out the existing software?</p>
<p>Any help or suggestions are much appreciated. The hardest part is knowing if it's downloading or just stuck (as it apparently was this morning).</p>
","<windows-10><windows-update><automatic-updates>","2020-07-20 18:24:52"
"779713","Two domains on one IP, added a subdomain and different domain's SSL gets protocol error","<p>I've been running two domains on one IP for years using SSL. One is example.com and the other is other.com. example.com has three names with its SSL cert; example.com, www.example.com and dev.example.com. other.com has other.com and www.other.com. </p>

<p>For the first time, I started up dev.example.com by just copying the nginx config for example.com like so:</p>

<pre><code>server {
    listen 80;
    server_name example.com www.example.com dev.example.com;
    root /var/empty;
    return 301 https://example.com$request_uri;
}

server {
    listen 443 ssl http2;
    server_name  dev.example.com;
    root /home/dev;
    index index.html;
    charset utf-8;

    add_header X-Content-Type-Options nosniff;
    add_header X-Frame-Options DENY;
    add_header X-Frame-Options SAMEORIGIN;
    add_header X-Xss-Protection ""1; mode=block"" always;

    ssl on;
    ssl_stapling on;
    ssl_stapling_verify on;

    ssl_prefer_server_ciphers on;
    ssl_ciphers ...
...
}
server {
    listen 443 ssl http2;
    server_name  example.com www.example.com;
    root /home/example;
    index index.html;
    charset utf-8;

    add_header X-Content-Type-Options nosniff;
    add_header X-Frame-Options DENY;
    add_header X-Frame-Options SAMEORIGIN;
    add_header X-Xss-Protection ""1; mode=block"" always;

    ssl on;
    ssl_stapling on;
    ssl_stapling_verify on;

    ssl_prefer_server_ciphers on;
    ssl_ciphers ...
...
}
</code></pre>

<p>Having done that, I can now access all three variations of example.com. However, none of other.com are accessible in that I get a (paraphrased): </p>

<blockquote>
  <p>SSL_protocol_error</p>
</blockquote>

<p>and </p>

<blockquote>
  <p>This site is not serving securely</p>
</blockquote>

<p>in Chrome while Firefox redirects to Google (my default home page).</p>

<p>The config for other.com is identical to example.com except for the dev subdomain. I did not set any location blocks for dev.example.com. The rest of the config file only contains ssl cert pointers and location blocks.</p>

<p>So I'm a bit confused as to why the dev subdomain took down other.com.</p>
","<nginx><ssl><subdomain>","2016-05-29 17:26:11"
"852549","Linux centos 6.5 Disk performance monitoring using smartctl","<p>i installed smartctl utility in centos 6.7 but cant get output of smartctl ,                                                  i fire </p>

<pre><code># smartctl -i /dev/sda
smartctl 5.43 2016-09-28 r4347 [x86_64-linux-2.6.32-573.el6.x86_64] (local build)
Copyright (C) 2002-12 by Bruce Allen, http://smartmontools.sourceforge.net

Vendor:               VMware 
Product:              Virtual disk
Revision:             1.0
User Capacity:        858,993,459,200 bytes [858 GB]
Logical block size:   512 bytes
Device type:          disk
Local Time is:        Sat May 27 15:16:54 2017 IST
</code></pre>

<p>Device does not support SMART? does anyone having idea to resolve this ?</p>
","<linux><centos6><smartctl>","2017-05-27 10:01:51"
"922806","Configure Apache2 to run a cherrypy app with other apps running","<p>I have a Cherrypy app that I want to deploy on my Apache2 server. However, the same server serves another app. What are the options to configure Apache to serve both?</p>
","<ubuntu><apache2><cherrypy>","2018-07-20 08:40:34"
"852629","GUI Package Management on Remote Server","<p>Usually I manage the packages on remote debian servers via SSH using aptitude or apt-get in CLI mode. Aptitude also does have an ugly interactive mode. However, on desktop systems I love the comfort and overall view of GUI tools like Synaptic.</p>

<p>Is it somehow possible to configure a local desktop system to manage an SSH remote server? I could imagine some chroot environment with mounted sshfs FUSE filesystems, however, I have no clue how to setup such an environment to get it work with Synaptic connected to the local display managing apt tools on remote side. There is no xserver running on the remote server.</p>

<p>Any alternative approach is welcome as well.</p>
","<ssh><apt><remote><sshfs><synaptic>","2017-05-28 01:18:18"
"922884","Event code 3005 ID 1309 - IIS","<p>I have problems with my application after last Windows Updates and ASP.NET  4.0.303.<br>
My application has crashed after some minutes, looking in Windows Event Viewer, I found this error log, is the same who appears in my application after ASP.NET error.</p>

<p>Somenone, have any idea how i can resolve this?</p>

<pre><code>(Version: Microsoft .NET Framework Versão:4.0.30319; Version do ASP.NET:4.7.3062.0)

Event code: 3005 
Event message: An unhandled exception has occurred. 
Event time: 7/20/2018 9:49:32 AM 
Event time (UTC): 7/20/2018 12:49:32 PM 
Event ID: 3289371ae4054ebf8cf135d6d4e0689e 
Event sequence: 2 
Event occurrence: 1 
Event detail code: 0 

Application information: 
Application domain: /LM/W3SVC/2/ROOT-10-131765645707725685 
Trust level: Full 
Application Virtual Path: / 
Application Path: C:\inetpub\wwwroot\Aplication\ 
Machine name: SRV02

Process information: 
Process ID: 4560 
Process name: w3wp.exe 
Account name: NT AUTHORITY\NETWORK SERVICE 

Exception information: 
Exception type: MissingMethodException 
Exception message: Method not found: 'Void Microsoft.Owin.Logging.AppBuilderLoggerExtensions.SetLoggerFactory(Owin.IAppBuilder, Microsoft.Owin.Logging.ILoggerFactory)'.
at Microsoft.Owin.Host.SystemWeb.OwinAppContext.Initialize(Action`1 startup)
at Microsoft.Owin.Host.SystemWeb.OwinBuilder.Build(Action`1 startup)
at Microsoft.Owin.Host.SystemWeb.OwinHttpModule.InitializeBlueprint()
at System.Threading.LazyInitializer.EnsureInitializedCore[T](T&amp; target, Boolean&amp; initialized, Object&amp; syncLock, Func`1 valueFactory)
at Microsoft.Owin.Host.SystemWeb.OwinHttpModule.Init(HttpApplication context)
at System.Web.HttpApplication.RegisterEventSubscriptionsWithIIS(IntPtr appContext, HttpContext context, MethodInfo[] handlers)
at System.Web.HttpApplication.InitSpecial(HttpApplicationState state, MethodInfo[] handlers, IntPtr appContext, HttpContext context)
at System.Web.HttpApplicationFactory.GetSpecialApplicationInstance(IntPtr appContext, HttpContext context)
at System.Web.Hosting.PipelineRuntime.InitializeApplication(IntPtr appContext)

User: 
Is authenticated: False 
Authentication Type: 
Thread account name: NT AUTHORITY\NETWORK SERVICE 

Thread information: 
Thread ID: 71 
Thread account name: NT AUTHORITY\NETWORK SERVICE 
Is impersonating: False 
Stack trace: at Microsoft.Owin.Host.SystemWeb.OwinAppContext.Initialize(Action`1 startup)
at Microsoft.Owin.Host.SystemWeb.OwinBuilder.Build(Action`1 startup)
at Microsoft.Owin.Host.SystemWeb.OwinHttpModule.InitializeBlueprint()
at System.Threading.LazyInitializer.EnsureInitializedCore[T](T&amp; target, Boolean&amp; initialized, Object&amp; syncLock, Func`1 valueFactory)
at Microsoft.Owin.Host.SystemWeb.OwinHttpModule.Init(HttpApplication context)
at System.Web.HttpApplication.RegisterEventSubscriptionsWithIIS(IntPtr appContext, HttpContext context, MethodInfo[] handlers)
at System.Web.HttpApplication.InitSpecial(HttpApplicationState state, MethodInfo[] handlers, IntPtr appContext, HttpContext context)
at System.Web.HttpApplicationFactory.GetSpecialApplicationInstance(IntPtr appContext, HttpContext context)
at System.Web.Hosting.PipelineRuntime.InitializeApplication(IntPtr appContext)
</code></pre>
","<windows><iis><asp.net><.net-4.0>","2018-07-20 18:08:00"
"1026309","Send mail via port 465 or 587 on Google Cloud VM / Postfix","<p>I am trying to send mail from a VM instance on Google Cloud which I have recently set up and migrated everything over from my old server.</p>
<p>However, I have found out Google Cloud does not allow sending mail via port 25, so I am trying to send via ports 465 or 587, but mail is not getting through, it is ending up as deferred in the mail queue.</p>
<p>How do I send mail from Google cloud?</p>
<p>According to this:
<a href=""https://cloud.google.com/compute/docs/tutorials/sending-mail#choosing_an_email_service_to_use"" rel=""nofollow noreferrer"">https://cloud.google.com/compute/docs/tutorials/sending-mail#choosing_an_email_service_to_use</a></p>
<p>...ports 465 and 587 are open.</p>
<p>Just a little more information, I have tried sending via the webmail on Plesk, and via Thunderbird. Both ways are accepting the username and password, and shows the email as sent. But is then stuck in the queue.</p>
<p>This is in: /var/log/maillog</p>
<pre><code>Jul 22 16:11:19 104-155-103-102 postfix/cleanup[2128]: 60E50220AB66: message-id=&lt;0044ead7-f1c2-9665-8fca-ea50c7673c06@email_address.com&gt;
Jul 22 16:11:19 104-155-103-102 spf[2131]: Starting the spf filter...
Jul 22 16:11:19 104-155-103-102 spf[2131]: SPF status: PASS
Jul 22 16:11:19 104-155-103-102 psa-pc-remote[31599]: PASS during call 'spf' handler
Jul 22 16:11:19 104-155-103-102 check-quota[2132]: Starting the check-quota filter...
Jul 22 16:11:19 104-155-103-102 psa-pc-remote[31599]: SKIP during call 'check-quota' handler
Jul 22 16:11:19 104-155-103-102 postfix/qmgr[723]: 60E50220AB66: from=&lt;enquiries@email_address.com&gt;, size=4190, nrcpt=1 (queue active)
Jul 22 16:11:19 104-155-103-102 postfix/error[2134]: 60E50220AB66: to=&lt;to_email_address@live.co.uk&gt;, relay=none, delay=0.58, delays=0.57/0.01/0/0, dsn=4.4.1, status=deferred (delivery temporarily suspended: connect to eur.olc.protection.outlook.com[104.47.22.161]:25: Connection timed out)
Jul 22 16:11:19 104-155-103-102 postfix/smtpd[2123]: disconnect from unknown[112.30.25.225] ehlo=1 auth=1 mail=1 rcpt=1 data=1 quit=1 commands=6
</code></pre>
<p>Thank you</p>
<p><strong>UPDATE</strong></p>
<p>I still have access to my old server which allowed mail sending on all ports inc port 25. This server was still set up with Plesk and the domain, along with all my  enquiries@mydomain.com</p>
<p>Prior to posting the original post, I <em>had</em> already tried the following:</p>
<p>DNS Records</p>
<pre><code>mydomain.com -- MX (10) -- mail.mydomain.com
mail.mydomain.com -- A -- old.server.ip.address
</code></pre>
<p>This obviously worded to be able to use the old server as the email server and bypassed the new server when sending emails using a desktop app like Thunderbird. However, I also have PHP scripts to send emails to members, so I needed to get the relayhost working.</p>
<p>I tried this below, along with many other variants over many days, however it would not work, hence why I made this post. I am still not sure why it was showing &quot;relay=none&quot; and was showing port 25 in the maillog.</p>
<pre><code>smtp_sasl_auth_enable = yes
smtp_sasl_security_options = noanonymous
smtp_sasl_password_maps = static:enquiries@mydomain.com:mypassword
relayhost = [old.server.ip.address]:465
#also tried port 587

</code></pre>
<p>This kept timing out as shown above in the maillog.
I then decided to try using gmail as an smtp server, which didnt work at first:</p>
<pre><code>smtp_sasl_password_maps = static:mygmail@gmail.com:mypassword
smtp_sasl_password_maps = hash:/etc/postfix/saslpasswd
relayhost = [smtp.gmail.com]:465
</code></pre>
<p>However, last night I added the first 3 lines shown, then it worked great (except the &quot;from&quot; email address was showing as my personal email address, but at least the emails were sending):</p>
<pre><code>smtp_always_send_ehlo = yes
smtp_tls_security_level = encrypt
smtp_tls_wrappermode = yes

smtp_sasl_password_maps = static:mygmail@gmail.com:mypassword
smtp_sasl_password_maps = hash:/etc/postfix/saslpasswd
relayhost = [smtp.gmail.com]:465
</code></pre>
<p>I then decided to try my old server again, and replaced the ip address for the server with &quot;mail.mydomain.com&quot;.</p>
<pre><code>smtp_sasl_password_maps = static:enquiries@mydomain.com:mypassword
relayhost = [mail.mydomain.com]:465

</code></pre>
<p>I have since replaced the static password to:</p>
<pre><code>#smtp_sasl_password_maps = hash:/etc/postfix/saslpasswd
</code></pre>
<p>And it is now working fine. I will reflect this in an answer below.</p>
<p>As someone who has never had to set up a SMTP server, ports etc, I was unaware that mail boxes only usually listen for new mail on port 25, and was unsure what this meant, hence my confusion.</p>
","<centos><postfix><google-cloud-platform>","2020-07-22 16:20:35"
"922917","Official BIND9 Windows binaries - host.exe, dig.exe - don't do anything","<p>I have downloaded the <a href=""https://www.isc.org/downloads/"" rel=""nofollow noreferrer"">official BIND 9 Windows binaries from ISC</a>. They don't do anything:</p>

<pre><code>$ .\host.exe google.com
$ (no output)
</code></pre>

<p>Same with <code>dig</code>. How do I make <code>host</code>, <code>dig</code> etc work on Windows?</p>
","<windows><domain-name-system><bind><host><dig>","2018-07-20 20:51:48"
"922921","Force a specific user or application to user a specific java version","<p>Bear with me, I'm new to sysadmin stuff. We have a server running RHEL 6 (2.6.32). We have a Pentaho application that interacts with the server as a <code>pentaho</code> user, and we have a <code>team</code> user under whose home directory most of our work is stored (we have reasons and are working on separate logins for this).</p>

<p>The problem is that the <code>pentaho</code> user can no longer save files to the <code>team</code> user's directory, despite never having an issue with this before. Nothing has changed as far as permissions are concerned.</p>

<p>It appears to be an issue with the version of Java (looks like JDK 1.8.0 got installed) that the user/application is using, so my question is this: how can I control the specific version of Java used by or seen by a specific user or application? How can I tell what versions of JDK are available?</p>
","<redhat><java><rhel6><versioning>","2018-07-20 21:29:39"
"780021","Restrict a program to only access files in a specific folder not containing it","<p>I have a folder <code>F</code> and a program <code>P</code> <em>outside</em> of the folder <code>F</code>. How to make <code>P</code> only able to access files in <code>F</code>? Answers for any OS are accepted but preferably something that work in Debian.</p>

<p>E.g
<code>F</code> is <code>~/playarea</code> and <code>P</code> is <code>~/prog</code>. I want <code>~/prog</code> to able to read, modify, and execute <code>~/playarea/foo</code> but not <code>~/others</code> or <strong>itself</strong>.</p>
","<security>","2016-05-31 13:34:50"
"922944","after Disabled Bitnami banner, Apache not running, Httpd could not be started","<p>I went to restart Apache after disabling the banner with</p>

<pre><code>sudo /opt/bitnami/apps/wordpress/bnconfig --disable_banner 1
</code></pre>

<p>now Apache won't restart</p>

<pre><code>~$ sudo /opt/bitnami/ctlscript.sh status
php-fpm already running
apache not running
mysql already running
</code></pre>

<p>I tried</p>

<pre><code>~$ sudo /opt/bitnami/ctlscript.sh start apache
Syntax OK
/opt/bitnami/apache2/scripts/ctl.sh : httpd could not be started
</code></pre>

<p>Monitored apache</p>

<p>for error log -  <code>cat /opt/bitnami/apache2/logs/error_log</code><br>
you can find it here in this link<br>
<a href=""https://drive.google.com/file/d/1f2Xt0bwcMEhKcXkILLMf_kdBl0JoSZUr/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1f2Xt0bwcMEhKcXkILLMf_kdBl0JoSZUr/view?usp=sharing</a></p>

<p>for <em>httpd-app.conf</em> file here copy of it in below link<br>
<a href=""https://drive.google.com/file/d/1l2zEPzIU0uBHdEyFQEZ22eEPS9fMnBTO/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1l2zEPzIU0uBHdEyFQEZ22eEPS9fMnBTO/view?usp=sharing</a></p>
","<ubuntu><ssh><amazon-web-services><bitnami>","2018-07-21 04:25:07"
"922952","Connection between windows 2012r2 DC and ADC","<p>I have a head office and a number of suboffices. Both head office and suboffices need to access some corporate server, which is physically in the head office. But suboffices should not be able to access head office or each other. Suboffices are at the substantial distance of each other and from the head office (many kms).</p>

<p>I have knowledge of servers and I have created a site on my domain controller also which is head office but the problem is the additional domain controller we have is on another site which is very far from head office,I know how to creates site in server but dont have any idea of how can I connect that site with another site using public IP.</p>
","<networking><site-to-site-vpn>","2018-07-21 08:20:46"
"852771","Zabbix monitoring event log","<p>i want monitoring event log event id:5002,5004,5007. i have create item: <a href=""https://i.sstatic.net/shBRO.png"" rel=""nofollow noreferrer"">Item</a>
when i create trigger is error:Incorrect trigger expression. Check expression part starting from ""eventlog[Applications,,,,&lt;5002>,,].last(,86400)}=0"". 
<a href=""https://i.sstatic.net/wHuoy.png"" rel=""nofollow noreferrer"">trigger</a></p>

<p>every day should be check event id: 5002,5004,5007. if this event id dont have is ok</p>
","<windows-event-log><zabbix>","2017-05-29 09:11:07"
"1026470","What is causing bounceback “550 Blocked” from Mimecast?","<p>Reading Mimecast's FAQ: <a href=""https://community.mimecast.com/s/article/Mimecast-SMTP-5xx-Error-Codes-573763479"" rel=""nofollow noreferrer"">https://community.mimecast.com/s/article/Mimecast-SMTP-5xx-Error-Codes-573763479</a></p>
<p>Error 550 with reason &quot;Blocked&quot; is not listed in their FAQ.  I have called Mimecast and they are not returning my call.  We have a client who is getting this bounceback when sending emails through a mass email service.  I have verified their SPF record is correct.  They do not have a DKIM record set up.  Their mass email service says to contact Mimecast.  So I am stuck at the moment and hope someone has an answer for this.</p>
<pre><code>X-SendGrid-QueueID: 644483390
X-SendGrid-Sender: &lt;bounces+28313-3d0d-hill=####.com@email.bullhornmail.com&gt;
Arrival-Date: 2020-07-21 19-51-57

Final-Recipient: rfc822; ####@####.com
Original-Recipient: rfc822; ####@####.com
Action: failed
Status: 
Diagnostic-Code: 550 spamcop.mimecast.org Blocked - see https://www.spamcop.net/bl.shtml?167.89.63.60. - https://community.mimecast.com/docs/DOC-1369#550 [bZ52-d7nMLmippSyFi6_bA.us116] 
</code></pre>
","<email-bounces>","2020-07-23 18:14:52"
"1026492","Proving DNS/Network issues on client network","<h3>Problem</h3>
<p>1 of ~150 server clients (in different locations and with different network setups) is not redirected through my apache service. I need to know where the problem is but can't figure it out.</p>
<p>All clients access a virtual host and send the same requests against the proxy:</p>
<pre><code>  &lt;VirtualHost *:80&gt;
  ServerName update.***.tld
  ServerAdmin mail@company.tld

  CustomLog /var/log/apache2/update.***.tld_access.log combined
  ErrorLog  /var/log/apache2/update.***.tld_error.log

  # redirect all http request to https
  RewriteEngine on
  Options +FollowSymLinks
  RewriteCond %{SERVER_PORT} !^443$
  RewriteRule ^.*$ https://%{SERVER_NAME}%{REQUEST_URI} [L,R]
  &lt;/VirtualHost&gt;

  &lt;VirtualHost *:443&gt;
  ServerName update.***.tld

  LogLevel warn
  SSLEngine on
  SSLCertificateFile /etc/ssl/certs/wildcard.***.tld.cert
  SSLCertificateKeyFile /etc/ssl/private/wildcard.***.tld.key
  SSLCertificateChainFile /etc/ssl/certs/wildcard.***.tld.combined.cert

  CustomLog /var/log/apache2/update.***.tld_access.log combined
  ErrorLog  /var/log/apache2/update.***.tld_error.log

  TimeOut 3600
  KeepAlive On

  AddDefaultCharset UTF-8

  SSLProxyEngine on
  ProxyPreserveHost Off
  SetEnv force-proxy-request-1.0 1
  SetEnv proxy-nokeepalive 1
  ProxyTimeout 15

  ProxyRequests Off
  ProxyPass /         https://***-***-prod.aws.tld/
  ProxyPassReverse /  https://***-***-prod-prod.aws.tld/

  &lt;Proxy *&gt;
     AddDefaultCharset UTF-8
     Require all granted
  &lt;/Proxy&gt;
  &lt;/VirtualHost&gt;
</code></pre>
<h3>Debug</h3>
<ul>
<li>Changed the client to a static IP setup with Google DNS server</li>
<li>Checked the log files
<ul>
<li>It seems that all requests from that special client are not redirected correctly</li>
<li>The requests from that client are logged in default_access.log and dont reach my custom log so I guess the forwarding is not working but why it is for 150 other clients ..</li>
<li>Default access log: clientipaddress - - [23/Jul/2020:20:23:17 +0200] &quot;GET /api/agent/ping HTTP/1.1&quot; 400 301 &quot;-&quot; &quot;-&quot;</li>
<li>When I send a wget from the client to my proxy on Port 443 it is correctly forwarded to the virtual host and logged in my custom log</li>
<li>I have checked the tcpdump and noticed that the client tries to send to the correct server with the correct port (443)</li>
</ul>
</li>
</ul>
<p>tcpdump on corrupt client:</p>
<pre><code>tcpdump -i eth0 -vvv host update.***.tld  &gt; dump
</code></pre>
<p><a href=""https://gist.github.com/herz0g/e02ef883688c904667164a175955ecc0"" rel=""nofollow noreferrer"">https://gist.github.com/herz0g/e02ef883688c904667164a175955ecc0</a></p>
<h3>Conclusion</h3>
<p>I guess it is a problem on the customer side network otherwise it would not work for 150 other clients but I am not sure how to prove that or what could be debugged further.</p>
","<networking><domain-name-system><proxy><400>","2020-07-23 20:39:04"
"923110","Let's Encrypt Expire Bot showing different date then command line","<p>Today I got an email notification from Let's Encrypt Expire Bot that the SSL certificate for my domain will be expired on the 31, july, 2018.</p>

<p>but when I log in to the server, and check the certificate with this command,</p>

<pre><code>echo | openssl s_client -connect mydomaintobeexpired.com:443 2&gt; /dev/null | openssl x509 -noout -dates
</code></pre>

<p>I got the following out,</p>

<pre><code>notBefore=Jul  1 21:03:23 2018 GMT
notAfter=Sep 29 21:03:23 2018 GMT
</code></pre>

<p>So it looks like everything is OK?</p>

<p>Why does Let's Encrypt Expire Bot give me a different expiring date then what the command line shows?</p>

<hr>

<p><em>Any reason why downvoting for this question?</em></p>
","<ssl-certificate><lets-encrypt>","2018-07-23 08:02:42"
"852895","postfix NOQUEUE: reject: RCPT from Recipient address rejected","<p>I have setup Email filter appliance which utilises postfix.
postfix gives the following error in the logs</p>

<pre><code>NOQUEUE: reject: RCPT from mail-oln040092070044.outbound.protection.outlook.com[40.92.70.44]: 451 4.7.1 &lt;info@infasys.co.uk&gt;: Recipient address rejected: Greylisted for 5 minutes; from=&lt;rehan_miah@hotmail.com&gt; to=&lt;info@infasys.co.uk&gt; proto=ESMTP helo=&lt;EUR03-AM5-obe.outbound.protection.outlook.com&gt;
</code></pre>

<p>below is my main.cf file contents</p>

<pre><code>    alias_maps = hash:/etc/aliases
alias_database = hash:/etc/aliases
debug_peer_level = 2
debugger_command =
     PATH=/bin:/usr/bin:/usr/local/bin:/usr/X11R6/bin
     ddd $daemon_directory/$process_name $process_id &amp; sleep 5
sendmail_path = /usr/sbin/sendmail.postfix
newaliases_path = /usr/bin/newaliases.postfix
setgid_group = postdrop
html_directory = no
manpage_directory = /usr/share/man
sample_directory = /usr/share/doc/postfix-3.1.3/samples
readme_directory = /usr/share/doc/postfix-3.1.3/README_FILES
inet_protocols = ipv4

smtputf8_enable = no

meta_directory = /etc/postfix
shlib_directory = no
mynetworks = 127.0.0.0/8
header_checks = regexp:/etc/postfix/header_checks
myorigin = $mydomain
relay_domains = hash:/etc/postfix/transport
transport_maps = hash:/etc/postfix/transport
local_recipient_maps = 
smtpd_helo_required = yes
smtpd_delay_reject = yes
disable_vrfy_command = yes
virtual_alias_maps = hash:/etc/postfix/virtual
default_destination_recipient_limit = 1
broken_sasl_auth_clients = yes
smtpd_sasl_auth_enable = yes
smtpd_sasl_local_domain = 
smtpd_sasl_path = smtpd
smtpd_sasl_security_options = noanonymous
smtp_sasl_password_maps = hash:/etc/postfix/sasl_passwd
smtp_sasl_type = cyrus
smtp_use_tls = yes
smtpd_use_tls = yes
smtp_tls_CAfile = /etc/postfix/ssl/smtpd.pem
smtp_tls_session_cache_database = btree:/var/lib/postfix/smtp_tls_session_cache
smtp_tls_note_starttls_offer = yes
smtpd_tls_key_file = /etc/postfix/ssl/smtpd.pem
smtpd_tls_cert_file = /etc/postfix/ssl/smtpd.pem
smtpd_tls_CAfile = /etc/postfix/ssl/smtpd.pem
smtpd_tls_loglevel = 1
smtpd_tls_received_header = yes
smtpd_tls_session_cache_timeout = 3600s
tls_random_source = dev:/dev/urandom
smtpd_tls_session_cache_database = btree:/var/lib/postfix/smtpd_tls_session_cache
smtpd_tls_security_level = may
smtpd_tls_mandatory_protocols = !SSLv2,!SSLv3
smtp_tls_mandatory_protocols = !SSLv2,!SSLv3
smtpd_tls_protocols = !SSLv2,!SSLv3
smtp_tls_protocols = !SSLv2,!SSLv3
smtpd_helo_restrictions = check_helo_access hash:/etc/postfix/helo_access, reject_invalid_hostname
smtpd_sender_restrictions = permit_sasl_authenticated, check_sender_access hash:/etc/postfix/sender_access, reject_non_fqdn_sender, reject_unknown_sender_domain
smtpd_data_restrictions = reject_unauth_pipelining
smtpd_relay_restrictions = 
    permit_mynetworks, 
    permit_sasl_authenticated, 
    defer_unauth_destination hash:/etc/postfix/recipient_access, check_policy_service inet:127.0.0.1:2501
masquerade_domains = $mydomain
smtpd_tls_dh1024_param_file = /etc/postfix/ssl/dhparam.pem
smtpd_tls_ciphers = low
</code></pre>
","<postfix>","2017-05-29 21:15:33"
"780254","How vCPUs are calculated?","<p>Yesterday I contacted my vendor to buy a server, and I asked them how I should calculate the CPU and RAM requirements for my server, given that I want to create multiple virtual servers on it</p>

<p>He said 12 core (6 core x 2 sockets) = 24 Logical Processor (with hyper threading technology), and if you are planning to use VMware Virtualization technology, it doubles the Logical Processor which means 48 vCPUs</p>

<p>He said you can create 6 virtual servers with 8 vCPUs each, which is more than enough.</p>

<p>Is this statement correct? Can I count on this and buy it?</p>

<p>Does 12 core equals to 48 vCPU? And can i create 6 virtual servers dividing by 8 vCPU for each?</p>
","<vmware-workstation><hyperthreading><vcpu>","2016-06-01 11:21:51"
"923214","How to redirect from port 80 to port 8443 using Tomcat and iptables","<p>I have the following iptable entry:</p>

<pre><code>REDIRECT   tcp  --  anywhere             anywhere             tcp dpt:http redir ports 8443
</code></pre>

<p>In my server.xml, I have <strong>only</strong> the following connector:</p>

<pre><code>&lt;Connector port=""8443"" maxHttpHeaderSize=""8192""
                   maxThreads=""150"" minSpareThreads=""25"" maxSpareThreads=""75""
                   enableLookups=""false"" disableUploadTimeout=""true""
                   acceptCount=""100"" scheme=""https"" secure=""true""
                   clientAuth=""false"" sslEnabledProtocols=""TLSv1,TLSv1.1,TLSv1.2"" SSLEnabled=""true""
                   URIEncoding=""UTF-8"" keyAlias=""tomcat"" keystorePass=""pass"" keystoreFile=""/home/myhome/.keystore"" /&gt;
</code></pre>

<p>When I use http say from Firefox, I see a GET request for a small (7 bytes) file with type application/octet-stream instead of the page that I would get if using https.  I'm using Ubuntu 18 and Tomcat 8.</p>

<p>How can I configure to redirect http to https?</p>
","<linux><tomcat><redirect>","2018-07-23 22:40:26"
"780294","How does nginx rewrite break flag work?","<p>I'm trying to set up a ""coming soon"" temporary page on a website:</p>

<pre><code>rewrite ^/(css|img|js)/ -                break;
rewrite ^/$             /comingsoon.html last;
rewrite ^               /?               redirect;
</code></pre>

<p>But I'm having trouble with the <code>break</code> flag: it's supposed to stop processing rewrite rules, but it doesn't seem to work.</p>

<p>My intention here is to:</p>

<ol>
<li>serve all css, img, and js files just as they are;</li>
<li>serve the comingsoon.html page in place of the homepage;</li>
<li>(temporary) redirect all other pages to the homepage.</li>
</ol>

<p>The problem is that directive 1. is not working: its URLs fall through to the 3rd rule and are redirected to the homepage, as if the first rule did not have a <code>break</code> flag. </p>

<p>The regexp in the first directive does match, because if I replace <code>break</code> with <code>redirect</code>, I get a 302 to <code>-</code>. So this leaves the <code>break</code> flag as the culprit.</p>

<p>Am I using it incorrectly? </p>

<hr>

<p>Edit: I solved it with a negative lookahead:</p>

<pre><code>rewrite ^/$                 /comingsoon.html last;
rewrite ^/(?!css/|img/|js/) /?               redirect;
</code></pre>

<p>But I'm still interested in understanding why <code>break</code> does not seem to work.</p>
","<nginx><rewrite>","2016-06-01 14:15:45"
"853024","Which protocol to use for a barebones LAN file upload with mixed Unix and Windows clients","<p>I have a set of about 20 embedded Unix clients who physically move from time to time. When they move into the range of a specific wireless AP, they will push a certain set of files to a Windows file server in the LAN. Every client will send a 250Mib file. There will never be any remote access to this LAN.</p>

<p>It basically looks like a train (embedded Raspbian clients) on a traintrack collecting passengers (the files) and then dropping them off at a specific point (The windows server).</p>

<p>I <strong>only</strong> want to push files to this Windows server and get an acknowledgement that it completed or not. I don't want or need anything else. I'm not going to share files between the clients or the server, files won't be modified or anything else than just a barebones bulk upload of files. The clients will never interact in any other way with the server or files other than uploading them. The server will process these files further.</p>

<p>I have read the following questions and answers: <a href=""https://serverfault.com/questions/582153/which-file-sharing-protocol-smb-afs-nfs-smb2-smb3-is-best-for-an-mixed-client"">1</a>, <a href=""https://serverfault.com/questions/14577/what-network-file-sharing-protocol-has-the-best-performance-and-reliability?rq=1"">2</a>, <a href=""https://serverfault.com/questions/310387/best-file-sharing-protocol-for-windows-clients"">3</a> but found that the use cases presented there did not match mine as these all request and consider features I don't need.</p>

<p>To summarize: Which protocol is most suitable for solely uploading files to a server?</p>
","<windows><unix><files><upload>","2017-05-30 13:30:30"
"993711","Extended Support for Windows Server 2008 on Azure","<p>We have a SharePoint 2010 Farm on premise.
The Extended support for Windows server 2008 ends in January 2020.
The Microsoft documentation here(<a href=""https://support.microsoft.com/en-in/help/4456235/end-of-support-for-windows-server-2008-and-windows-server-2008-r2"" rel=""nofollow noreferrer"">https://support.microsoft.com/en-in/help/4456235/end-of-support-for-windows-server-2008-and-windows-server-2008-r2</a>) mentions that if the Windows 2008 servers are migrated to Azure, the customers would get 3 additional years of Critical and Important security updates at no additional charge.
We would like to know if the support for SharePoint 2010 and the SQL Server 2008 R2 support would also be extended? 
What are the Microsoft guidelines for SharePoint 2010 and SQL Server 2008?</p>
","<windows-server-2008>","2019-11-29 04:04:56"
"1026938","Automatically connect duplicate Windows RDP users to an existing, shared session?","<p>When a new Remote Desktop client connects to an RD server with a username that's already connected, Remote Desktop Services normally does one of two things:</p>
<ul>
<li>Disconnect the previous remote, and connect the new remote to the old session (<code>fSingleSessionPerUser</code> = 1), or</li>
<li>Create a new session for that user (<code>fSingleSessionPerUser</code> = 0)</li>
</ul>
<p>How can Windows be configured/forced/hacked to automatically connect new RDP clients having the same username to the existing session, as <a href=""https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/shadow"" rel=""nofollow noreferrer"">shadow</a>s?</p>
<p><code>Shadow -&gt; Full Control without user's permission</code> is necessary but not sufficient.  Would <code>Termsrv.dll</code> have to be patched?</p>
","<windows><remote-desktop><remote-desktop-services><windows-terminal-services>","2020-07-27 18:33:44"
"780380","Server suddenly became too slow with timeout error","<p>My server suddenly became slow, all the websites in it at the http level
i have checked all the logs but there is nothing that indicate why it's happening if anyone have any suggestion please</p>

<p>My setup : </p>
Centos 6.8 </p>
Nginx 1.10 </p>
php-fpm 7.0 </p>
Mariadb 10.1 </p>
Memory 32gb with 13 gb free</p></p>

<p>1 year later still the same problem </p>
Note: The only way i was able to reduce load to 10 seconds was by moving all cache to memory tmpfs </p>
 - Enabling Nginx cache jump the load back to 100s either adding cache folder to memory or not (Which i couldn't understand as it was working fine before) </p>

<p>Caching system:
Opcache
Filecache (Codeigniter)
Memcached
Redis</p>

<pre><code>netstat -an | wc -l
22445

netstat -an | grep :80 | wc -l
51196

netstat -ant | awk '{print $6}' | sort | uniq -c | sort -n
  1 established)
  1 FIN_WAIT1
  1 Foreign
  2 SYN_RECV
  3 LAST_ACK
  6 LISTEN
21144 ESTABLISHED
30908 TIME_WAIT
</code></pre>

<p><strong>netstat -i 15</strong></p>

<pre><code>Kernel Interface table
Iface       MTU Met    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg
eth0       1500   0 218927712      0      0      0 257329662      0      0      0 BMRU
lo        65536   0 310876055      0      0      0 310876055      0      0      0 LRU
eth0       1500   0 218994531      0      0      0 257405202      0      0      0 BMRU
lo        65536   0 311006988      0      0      0 311006988      0      0      0 LRU
eth0       1500   0 219060371      0      0      0 257479672      0      0      0 BMRU
lo        65536   0 311139650      0      0      0 311139650      0      0      0 LRU
eth0       1500   0 219131683      0      0      0 257561956      0      0      0 BMRU
lo        65536   0 311270631      0      0      0 311270631      0      0      0 LRU
eth0       1500   0 219201250      0      0      0 257640969      0      0      0 BMRU
lo        65536   0 311407443      0      0      0 311407443      0      0      0 LRU
eth0       1500   0 219268062      0      0      0 257717388      0      0      0 BMRU
lo        65536   0 311534133      0      0      0 311534133      0      0      0 LRU
eth0       1500   0 219335153      0      0      0 257792649      0      0      0 BMRU
lo        65536   0 311666747      0      0      0 311666747      0      0      0 LRU
</code></pre>

<p><strong>iostat 15</strong></p>

<pre><code>avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          10.98    0.52    3.15    0.30    0.00   85.05

Device:            tps   Blk_read/s   Blk_wrtn/s   Blk_read   Blk_wrtn
sda               20.79 833.21       490.35   76292548   44898504

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          28.67    0.00    9.33    0.83    0.00   61.17

Device:            tps   Blk_read/s   Blk_wrtn/s   Blk_read   Blk_wrtn
sda              46.67        32.00       873.60        480      13104

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          27.82    0.00   10.18    0.15    0.00   61.85

Device:            tps   Blk_read/s   Blk_wrtn/s   Blk_read   Blk_wrtn
sda              25.47        33.60      1692.80        504      25392

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          28.84    0.00   10.21    0.06    0.00   60.89

Device:            tps   Blk_read/s   Blk_wrtn/s   Blk_read   Blk_wrtn
sda              11.27        32.53       712.00        488      10680

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          27.83    0.00   10.45    0.95    0.00   60.76

Device:            tps   Blk_read/s   Blk_wrtn/s   Blk_read   Blk_wrtn
sda              58.67        18.67      1861.33        280      27920
</code></pre>

<p><strong>vmstat 15</strong></p>

<pre><code>procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu-----
r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
12  0  0 11291076 1118568 12297544    0    0    17    10    8    4 11  3 
85  0  0
3  0   0 11248336 1118568 12299980    0    0     9   399 29547 52458 27  8 65  0  0
11  0  0 11224000 1118568 12303336    0    0    26   683 29830 52331 27  
9 63  1  0
21  0  0 11193280 1118568 12306744    0    0    39   347 31722 67277 31 10 59  0  0
16  0  0 11158436 1118568 12310428    0    0    17   809 30677 62257 27 
10 62  1  0
10  0  0 11129768 1118568 12314088    0    0    32   472 32385 61133 30 10 60  0  0
7  2   0 11120616 1118572 12317448    0    0    18   761 31739 66259 29 10 60  0  0
</code></pre>

<p><strong>top -b -n 1 | head</strong></p>

<pre><code>top - 20:17:07 up 1 day,  1:12,  1 user,  load average: 3.95, 4.11, 4.29
Tasks: 641 total,  15 running, 626 sleeping,   0 stopped,   0 zombie
Cpu(s): 10.8%us,  2.9%sy,  0.5%ni, 85.3%id,  0.3%wa,  0.0%hi,  0.2%si,  0.0%st
Mem:  32865012k total, 21206392k used, 11658620k free,  1118552k buffers
Swap:  8388604k total,        0k used,  8388604k free, 12165180k cached

PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
27709 mysql     20   0 5189m 1.0g  13m S 559.9  3.1   1800:22 mysqld
2750 nginx     10 -10  437m  42m 1488 R 19.2  0.1  46:05.13 nginx
3135 nginx     10 -10  433m  41m 1496 S 15.4  0.1  46:45.90 nginx
</code></pre>

<p><strong>iotp</strong></p>

<pre><code>Total DISK READ: 64.98 K/s | Total DISK WRITE: 667.90 K/s
TID  PRIO  USER     DISK READ  DISK WRITE  SWAPIN     IO&gt;    COMMAND
884 be/3 root        0.00 B/s  491.00 K/s  0.00 %  4.80 % [jbd2/sda5-8]
6091 be/4 nginx      64.98 K/s   64.98 K/s  0.00 %  0.16 % php-fpm: pool www
16943 be/4 mysql       0.00 B/s    3.61 K/s  0.00 %  0.00 % mysqld
27829 be/4 mysql       0.00 B/s    7.22 K/s  0.00 %  0.00 % mysqld
27862 be/4 mysql       0.00 B/s    0.00 B/s  0.00 %  0.00 % mysqld 
27887 be/4 mysql       0.00 B/s  238.28 K/s  0.00 %  0.00 % mysqld 
2745 be/2 nginx       0.00 B/s    3.61 K/s  0.00 %  0.00 % nginx: worker process
2747 be/2 nginx       0.00 B/s    7.22 K/s  0.00 %  0.00 % nginx: worker process
2748 be/2 nginx       0.00 B/s   10.83 K/s  0.00 %  0.00 % nginx: worker process
2750 be/2 nginx       0.00 B/s    3.61 K/s  0.00 %  0.00 % nginx: worker process
2751 be/2 nginx       0.00 B/s    7.22 K/s  0.00 %  0.00 % nginx: worker process
2788 be/2 nginx       0.00 B/s   14.44 K/s  0.00 %  0.00 % nginx: worker process
2872 be/2 nginx       0.00 B/s   10.83 K/s  0.00 %  0.00 % nginx: worker process
2919 be/2 nginx       0.00 B/s    7.22 K/s  0.00 %  0.00 % nginx: worker process
2920 be/2 nginx       0.00 B/s   10.83 K/s  0.00 %  0.00 % nginx: worker process
2991 be/2 nginx       0.00 B/s    3.61 K/s  0.00 %  0.00 % nginx: worker process
3038 be/2 nginx       0.00 B/s    3.61 K/s  0.00 %  0.00 % nginx: worker process
3121 be/2 nginx       0.00 B/s    7.22 K/s  0.00 %  0.00 % nginx: worker process
3135 be/2 nginx       0.00 B/s   14.44 K/s  0.00 %  0.00 % nginx: worker process
3218 be/2 nginx       0.00 B/s    3.61 K/s  0.00 %  0.00 % nginx: worker process
3256 be/2 nginx       0.00 B/s    3.61 K/s  0.00 %  0.00 % nginx: worker process
3258 be/2 nginx       0.00 B/s    3.61 K/s  0.00 %  0.00 % nginx: worker process
3316 be/2 nginx       0.00 B/s   10.83 K/s  0.00 %  0.00 % nginx: worker process
3325 be/2 nginx       0.00 B/s   14.44 K/s  0.00 %  0.00 % nginx: worker process
3390 be/2 nginx       0.00 B/s    7.22 K/s  0.00 %  0.00 % nginx: worker process
28264 be/4 mysql       0.00 B/s   57.76 K/s  0.00 %  0.00 % mysqld 
27959 be/4 mysql       0.00 B/s    0.00 B/s  0.00 %  0.00 % mysqld 
28000 be/4 mysql       0.00 B/s    0.00 B/s  0.00 %  0.00 % mysqld
5892 be/4 nginx       0.00 B/s   25.27 K/s  0.00 %  0.00 % php-fpm: pool www
5918 be/4 nginx       0.00 B/s    3.61 K/s  0.00 %  0.00 % php-fpm: pool www
5938 be/4 nginx       0.00 B/s    3.61 K/s  0.00 %  0.00 % php-fpm: pool www
5958 be/4 nginx       0.00 B/s   28.88 K/s  0.00 %  0.00 % php-fpm: pool www
5981 be/4 nginx       0.00 B/s   43.32 K/s  0.00 %  0.00 % php-fpm: pool www
5990 be/4 nginx       0.00 B/s   57.76 K/s  0.00 %  0.00 % php-fpm: pool www
6037 be/4 nginx       0.00 B/s   28.88 K/s  0.00 %  0.00 % php-fpm: pool www
6062 be/4 nginx       0.00 B/s   21.66 K/s  0.00 %  0.00 % php-fpm: pool www
6114 be/4 nginx       0.00 B/s    3.61 K/s  0.00 %  0.00 % php-fpm: pool www
1 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % init
2 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [kthreadd]
3 rt/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [migration/0]
4 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [ksoftirqd/0]
5 rt/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [stopper/0]
6 rt/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [watchdog/0]
7 rt/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [migration/1]
8 rt/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [stopper/1]
9 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [ksoftirqd/1]
10 rt/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [watchdog/1]
11 rt/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [migration/2]
12 rt/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [stopper/2]
13 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [ksoftirqd/2]
14 rt/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [watchdog/2]
15 rt/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [migration/3]
16 rt/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [stopper/3]
17 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [ksoftirqd/3]
18 rt/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [watchdog/3]
19 rt/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [migration/4]
20 rt/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [stopper/4]
21 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [ksoftirqd/4]
22 rt/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [watchdog/4]
23 rt/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [migration/5]
24 rt/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [stopper/5]
</code></pre>

<p><strong>Nginx configuration</strong></p>

<blockquote>
<pre><code>user  nginx nginx; 
worker_processes  24; 
worker_rlimit_nofile 300000;


pid  /var/run/nginx.pid;

events {
use                 epoll;
worker_connections  20192;
multi_accept        on;
   }

http {
server_tokens off;
include       /etc/nginx/mime.types;
default_type  application/octet-stream;
set_real_ip_from 127.0.0.1;
real_ip_header X-Forwarded-For;
sendfile        on;
tcp_nopush      on;
charset UTF-8;


fastcgi_buffers 8 16k;
fastcgi_buffer_size 32k;

proxy_http_version 1.1;
proxy_set_header Connection """";
proxy_connect_timeout  600s;
proxy_send_timeout  600s;
proxy_read_timeout  600s;
fastcgi_send_timeout 600s;
fastcgi_read_timeout 600s;

keepalive_timeout  60;
keepalive_disable   none;
max_ranges                1;
reset_timedout_connection on;
tcp_nodelay               on;

open_file_cache max=10000 inactive=5m;
open_file_cache_valid 10m;
open_file_cache_min_uses 5;
open_file_cache_errors off;

client_body_buffer_size  1m;   
client_header_buffer_size 1m;   
client_max_body_size 1m;  
large_client_header_buffers 2 1m;  

gzip  on;
gzip_comp_level 5;
gzip_min_length 1000;
gzip_proxied any;
gzip_vary on;
gzip_types
  application/atom+xml
  application/javascript
  application/json
  application/rss+xml
  application/vnd.ms-fontobject
  application/x-font-ttf
  application/x-web-app-manifest+json
  application/xhtml+xml
  application/xml
  font/opentype
  image/svg+xml
  image/x-icon
  text/css
  text/plain
  text/x-component;

 #limit_req_zone  $binary_remote_addr  zone=gulag:1m   rate=50r/m;

# Upstream to abstract backend connection(s) for PHP.
upstream php {
server unix:/tmp/php-fpm.sock;
server 127.0.0.1:9000;
}

include /etc/nginx/conf.d/*;
 }
</code></pre>
</blockquote>

<hr>

<p><strong>php-fpm config</strong>
<a href=""http://pasted.co/d6a2fa9c"" rel=""nofollow noreferrer"">http://pasted.co/d6a2fa9c</a></p>

<h2><strong>mysql Configuration</strong></h2>

<blockquote>
<pre><code>port = 3306
#bind-address    =  
default-storage-engine = ARIA 
aria-pagecache-buffer-size= 2048M 
key_buffer = 2024M 
query_cache_size = 64M 
query_cache_limit = 512M 
max_connections = 3000 
thread_cache_size = 512 
query_cache_min_res_unit = 0 
tmp_table_size = 32M 
max_heap_table_size = 32M 
table_cache=1024 
concurrent_insert=2
max_allowed_packet = 64M 
sort_buffer_size = 128K 
read_buffer_size = 512K 
read_rnd_buffer_size = 512K 
net_buffer_length = 2K 
thread_stack = 512K 
wait_timeout = 300 
table_definition_cache = 4000 
thread_handling = pool-of-threads 
host_cache_size = 2000 
skip_name_resolve 
thread-pool-max-threads= 1000
</code></pre>
</blockquote>

<hr>

<p><strong>php.in</strong>
<a href=""http://pasted.co/523a81db"" rel=""nofollow noreferrer"">http://pasted.co/523a81db</a></p>

<p><strong>nginx error log</strong>
nothing beside</p>

<blockquote>
  <p>[warn] 10063#10063: conflicting server name</p>
</blockquote>

<p><strong>mysql error log</strong></p>

<blockquote>
  <p>also empty beside restarting log</p>
</blockquote>

<p><strong>php-fpm error log</strong></p>

<blockquote>
<pre><code>[01-Jun-2016 21:54:00] WARNING: [pool www] child 9596, script '/var/www/index.php' (request: ""POST /index.php"") execution timed out (302.124548 sec), terminating 
[01-Jun-2016 21:54:00] WARNING: [pool www] child 9596 exited on signal 15 (SIGTERM) after 1203.573297 seconds from start 
[01-Jun-2016 21:54:00] NOTICE: [pool www] child 10116 started 
[01-Jun-2016 22:07:20] WARNING: [pool www] child 9450, script '/var/www/index.php' (request: ""GET /index.php"") execution timed out (331.919262 sec), terminating 
[01-Jun-2016 22:07:25] WARNING: [pool www] child 9450 exited on signal 15 (SIGTERM) after 2009.277917 seconds from start 
[01-Jun-2016 22:07:25] NOTICE: [pool www] &gt;     &gt;     child 10271 started
</code></pre>
</blockquote>

<p><strong>Top</strong></p>

<blockquote>
<pre><code>top - 22:46:35 up  1:20,  1 user,  load average: 306.50, 283.23,
290.78 Tasks: 801 total,  89 running, 712 sleeping,   0 stopped,   0 zombie Cpu(s): 85.3%us, 14.2%sy,  0.0%ni,  0.0%id,  0.0%wa,  0.0%hi, 
0.5%si,  0.0%st Mem:  32865348k total, 13868888k used, 18996460k free,    28104k buffers Swap:  8388604k total,   727164k used,  7661440k free, 
394328k cached

PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND 
7962 mysql     20   0 3082m 321m 4072 S 114.1  1.0  82:07.36 mysqld 
9405 php       20   0 1387m 924m 8156 R  4.9  2.9   1:33.70 php-fpm 
9505 php       20   0  714m 252m 8116 R  4.9  0.8   1:29.31 php-fpm 
9448 php       20   0  475m  21m  14m S  4.6  0.1   1:34.66 php-fpm 
9533 php       20   0  472m  19m  12m D  4.6  0.1   1:34.74 php-fpm 
9628 php       20   0  516m  62m  14m R  4.6  0.2   1:40.33 php-fpm 
9685 php       20   0  670m 210m 8156 R  4.6  0.7   1:31.48 php-fpm 
9380 php       20   0  712m 253m 7896 R  4.3  0.8   1:28.64 php-fpm 
9387 php       20   0  507m  48m 7564 R  4.3  0.2   1:31.58 php-fpm 
9395 php       20   0  473m  20m  12m D  4.3  0.1   1:36.94 php-fpm 
9435 php       20   0 1597m 1.1g  12m R  4.3  3.6   1:34.38 php-fpm 
9461 php       20   0  500m  47m  13m R  4.3  0.1   1:36.33 php-fpm 
9493 php       20   0  491m  32m 7560 R  4.3  0.1   1:41.10 php-fpm 
9514 php       20   0  477m  25m  14m R  4.3  0.1   1:32.29 php-fpm 
9554 php       20   0  513m  59m  13m R  4.3  0.2   1:33.10 php-fpm 
9556 php       20   0  512m  57m  12m R  4.3  0.2   1:24.28 php-fpm 
9566 php       20   0  473m  15m 7524 D  4.3  0.0   1:33.42 php-fpm 
9598 php       20   0  531m  71m 7764 R  4.3  0.2   1:32.87 php-fpm 
9620 php       20   0  473m  21m  14m S  4.3  0.1   1:20.62 php-fpm 
9653 php       20   0  509m  54m  11m R  4.3  0.2   1:36.90 php-fpm 
9677 php       20   0  513m  23m 7392 R  4.3  0.1   1:34.66 php-fpm 
9689 php       20   0  714m 258m  14m R  4.3  0.8   1:30.03 php-fpm 
9695 php       20   0 1431m 973m  12m R  4.3  3.0   1:31.20 php-fpm 
9408 php       20   0  472m  13m 7456 S  4.1  0.0   1:35.02 php-fpm 
9463 php       20   0  514m  60m  12m R  4.1  0.2   1:33.01 php-fpm 
9504 php       20   0  505m  48m  21m R  4.1  0.1   1:36.00 php-fpm 
9507 php       20   0  491m  38m  12m R  4.1  0.1   1:37.42 php-fpm 
9516 php       20   0  493m  32m 8108 R  4.1  0.1   1:35.84 php-fpm 
9522 php       20   0  627m 166m 7816 R  4.1  0.5   1:30.21 php-fpm 
9524 php       20   0  936m 480m  13m R  4.1  1.5   1:33.89 php-fpm 
9534 php       20   0  475m  15m 8060 D  4.1  0.0   1:38.95 php-fpm 
9638 php       20   0  504m  45m 7652 S  4.1  0.1   1:35.04 php-fpm 
9641 php       20   0  777m 314m 8508 R  4.1  1.0   1:37.97 php-fpm 
9655 php       20   0  507m  56m  14m R  4.1  0.2   1:28.89 php-fpm 
9671 php       20   0  513m  54m 7544 R  4.1  0.2   1:34.73 php-fpm 
9684 php       20   0  514m  61m  14m R  4.1  0.2   1:36.00 php-fpm 
9793 php       20   0  472m  19m  13m R  4.1  0.1   1:28.19 php-fpm 
9806 php       20   0  515m  55m 8172 R  4.1  0.2   1:35.00 php-fpm 
9818 php       20   0  771m 308m 8068 R  4.1  1.0   1:31.61 php-fpm 
9825 php       20   0  473m  13m 8740 S  4.1  0.0   1:29.41 php-fpm 
9850 php       20   0  475m  18m  11m S  4.1  0.1   1:35.34 php-fpm 
9861 php       20   0  473m  14m 7556 S  4.1  0.0   1:32.32 php-fpm 
9425 php       20   0  475m  21m  13m S  3.8  0.1   1:34.69 php-fpm 
9446 php       20   0  471m  12m 7640 S  3.8  0.0   1:36.17 php-fpm 
9579 php       20   0  471m  12m 7592 D  3.8  0.0   1:37.13 php-fpm 
9588 php       20   0  491m  32m 7468 R  3.8  0.1   1:36.44 php-fpm
</code></pre>
  
  <p>[root@davidsm01 ~]# vmstat</p>

<pre><code>procs -----------memory---------- ---swap-- -----io---- --system-------cpu-----  
r  b   swpd   free   buff  cache   si   so    bi    bo   in  cs us sy id wa st 
273  0 669112 13713228  76480 541424   21   30  69    50  567  165 83 15  1  1  0
</code></pre>
</blockquote>

<p><strong>tcpdump -i eth0</strong></p>

<blockquote>
<pre><code>22:55:11.703006 IP 141.101.70.49.36344 &gt;
expressvpn16.clients.netelligent.ca.http: Flags [.], ack 2921, win 77,
length 0 22:55:11.703017 IP 141.101.70.49.36344 &gt;
expressvpn16.clients.netelligent.ca.http: Flags [.], ack 3718, win 80,
length 0 22:55:11.717115 IP 108.162.214.215.23885 &gt;
expressvpn16.clients.netelligent.ca.http: Flags [.], ack 3878216429,
win 29, length 0 22:55:11.717173 IP 108.162.214.215.23885 &gt;
expressvpn16.clients.netelligent.ca.http: Flags [P.], seq 0:485, ack
1, win 29, length 485 22:55:11.717192 IP
expressvpn16.clients.netelligent.ca.http &gt; 108.162.214.215.23885:
Flags [.], ack 485, win 123, length 0

15493 packets captured 
43347 packets received by filter 
27820 packets dropped by kernel
</code></pre>
</blockquote>

<p><strong>mpstat -P ALL</strong></p>

<blockquote>
<pre><code>Linux 2.6.32-642.1.1.el6.x86_64 (davidsm01.localdomain)         06/01/2016      _x86_64_        (12 CPU)

11:14:56 PM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest   %idle
11:14:56 PM  all   83.76    0.00   14.16    1.08    0.00    0.56    0.00    0.00    0.43
11:14:56 PM    0   84.53    0.00   13.89    1.18    0.00    0.12    0.00    0.00    0.28
11:14:56 PM    1   83.44    0.00   14.19    1.14    0.00    0.85    0.00    0.00    0.38
11:14:56 PM    2   83.38    0.00   14.23    1.16    0.00    0.85    0.00    0.00    0.38
11:14:56 PM    3   84.59    0.00   13.76    1.15    0.00    0.10    0.00    0.00    0.39
11:14:56 PM    4   84.30    0.00   14.04    1.14    0.00    0.13    0.00    0.00    0.39
11:14:56 PM    5   84.66    0.00   13.73    1.10    0.00    0.11    0.00    0.00    0.40
11:14:56 PM    6   83.24    0.00   14.39    1.16    0.00    0.84    0.00    0.00    0.37
11:14:56 PM    7   84.44    0.00   13.94    0.55    0.00    0.15    0.00    0.00    0.92
11:14:56 PM    8   83.07    0.00   14.54    1.17    0.00    0.87    0.00    0.00    0.35
11:14:56 PM    9   83.07    0.00   14.43    1.09    0.00    0.97    0.00    0.00    0.45
11:14:56 PM   10   83.11    0.00   14.50    1.05    0.00    0.84    0.00    0.00    0.50
11:14:56 PM   11   83.31    0.00   14.33    1.13    0.00    0.84    0.00    0.00    0.39
</code></pre>
</blockquote>

<p><strong>ps -eo pcpu,pid,user,args | sort -k 1 -r | head -10</strong></p>

<blockquote>
<pre><code>%CPU   PID USER     COMMAND
 2.3 10772 php      php-fpm: pool www
 2.3 10490 php      php-fpm: pool www
 2.2  9869 php      php-fpm: pool www
 2.2  9868 php      php-fpm: pool www
 2.2  9864 php      php-fpm: pool www
 2.2  9854 php      php-fpm: pool www
 2.2  9850 php      php-fpm: pool www
 2.2  9827 php      php-fpm: pool www
 2.2  9824 php      php-fpm: pool www
</code></pre>
</blockquote>

<p><strong>ss -s</strong></p>

<blockquote>
<pre><code> Total: 14878 (kernel 15172)
 TCP:   30203 (estab 11194, closed 16290, orphaned 89, synrecv 0, timewait &gt;      16281/0), ports 4092

 Transport Total     IP        IPv6
 *         15172     -         -
 RAW       0         0         0
 UDP       0         0         0
 TCP       13913     13908     5
 INET      13913     13908     5
 FRAG      0         0         0
</code></pre>
</blockquote>

<p><strong>Device eth0 [] (1/5):</strong></p>

<blockquote>
<pre><code> Incoming:

 Curr: 1.61 MBit/s
 Avg: 1.78 MBit/s
 Min: 886.95 kBit/s
 Max: 5.11 MBit/s
 Ttl: 2.06 GByte

 Outgoing

 Curr: 4.82 MBit/s
 Avg: 5.08 MBit/s
 Min: 2.10 MBit/s
 Max: 14.50 MBit/s
 Ttl: 5.59 GByte
</code></pre>
</blockquote>
","<linux><nginx><php-fpm>","2016-06-01 19:41:02"
"853347","CentOS6 --skip-broken","<p>my linux skills are not so good and i cant fix my problem..</p>

<p>Im trying to update (yum update) my 2 linux virtual machines. i get allways following error:</p>

<pre><code>Error: Package: nss-3.28.4-3.el6_9.x86_64 (updates)
Requires: nspr &gt;= 4.13.0
Installed: nspr-4.11.0-1.el6.x86_64 (@base)
nspr = 4.11.0-1.el6
Error: Package: nss-util-3.28.4-1.el6_9.x86_64 (updates)
Requires: nspr &gt;= 4.13.0-1
Installed: nspr-4.11.0-1.el6.x86_64 (@base)
nspr = 4.11.0-1.el6
You can try to use --skip-broken to get around the problem.
You could try running: rpm -Va --nofiles --nodigest
</code></pre>

<p>(Note: the text above is a translation from German; it may not be the exact same text as you'd get if you run the commands on a system with English locale.)</p>

<p>How do I fix this? </p>
","<centos6>","2017-06-01 07:18:48"
"853379","CRC error in several compressed files","<p>I have a server which backs up itself on another server with <code>duplicity</code> (actually <code>duply</code>). The full backup is about 330 1-GB files. The full backup finished without problems, but the next day the incremental terminated with ""CRC check failed"". On the backup server several files appear to have a problem:</p>

<pre><code># gzip *20170530* --test

gzip: duplicity-full-signatures.20170530T032515Z.sigtar.gz: invalid compressed data--crc error

gzip: duplicity-full.20170530T032515Z.vol139.difftar.gz: invalid compressed data--crc error

gzip: duplicity-full.20170530T032515Z.vol139.difftar.gz: invalid compressed data--length error

gzip: duplicity-full.20170530T032515Z.vol146.difftar.gz: invalid compressed data--crc error

gzip: duplicity-full.20170530T032515Z.vol169.difftar.gz: invalid compressed data--crc error

gzip: duplicity-full.20170530T032515Z.vol171.difftar.gz: invalid compressed data--crc error

gzip: duplicity-full.20170530T032515Z.vol171.difftar.gz: invalid compressed data--length error

gzip: duplicity-full.20170530T032515Z.vol193.difftar.gz: invalid compressed data--crc error

gzip: duplicity-full.20170530T032515Z.vol223.difftar.gz: invalid compressed data--crc error

gzip: duplicity-full.20170530T032515Z.vol224.difftar.gz: invalid compressed data--crc error

gzip: duplicity-full.20170530T032515Z.vol233.difftar.gz: invalid compressed data--crc error

gzip: duplicity-full.20170530T032515Z.vol301.difftar.gz: invalid compressed data--crc error

gzip: duplicity-full.20170530T032515Z.vol310.difftar.gz: invalid compressed data--crc error

gzip: duplicity-full.20170530T032515Z.vol310.difftar.gz: invalid compressed data--length error

gzip: duplicity-full.20170530T032515Z.vol53.difftar.gz: invalid compressed data--crc error

gzip: duplicity-full.20170530T032515Z.vol53.difftar.gz: invalid compressed data--length error

gzip: duplicity-full.20170530T032515Z.vol63.difftar.gz: invalid compressed data--crc error

gzip: duplicity-full.20170530T032515Z.vol63.difftar.gz: invalid compressed data--length error
</code></pre>

<p>If only one file had an error I'd just retry hoping it would be a random error. But... 13 files with error? How should I debug this?</p>

<p>Both servers are Debian 8. Duplicity is 0.6.24, installed with apt; the same thing with all dependencies, with the exception of paramiko, for which 1.16.0 has been installed.</p>

<p>The main server's logs do show some SATA stuff, but would this result in silently corrupting files? Wouldn't the full backup have stopped with an I/O error or something? Here's an example of stuff written in the log:</p>

<pre><code>May 31 06:49:11 acheloos kernel: [1887359.720042] ata3.00: exception Emask 0x50 SAct 0x40000 SErr 0x280900 action 0x6 frozen
May 31 06:49:11 acheloos kernel: [1887359.720472] ata3.00: irq_stat 0x08000000, interface fatal error
May 31 06:49:11 acheloos kernel: [1887359.720870] ata3: SError: { UnrecovData HostInt 10B8B BadCRC }
May 31 06:49:11 acheloos kernel: [1887359.721255] ata3.00: failed command: READ FPDMA QUEUED
May 31 06:49:11 acheloos kernel: [1887359.721639] ata3.00: cmd 60/40:90:ac:3b:d8/00:00:2e:00:00/40 tag 18 ncq 32768 in
May 31 06:49:11 acheloos kernel: [1887359.721639]          res 40/00:94:ac:3b:d8/00:00:2e:00:00/40 Emask 0x50 (ATA bus error)
May 31 06:49:11 acheloos kernel: [1887359.722430] ata3.00: status: { DRDY }
May 31 06:49:11 acheloos kernel: [1887359.722927] ata3: hard resetting link
May 31 06:49:11 acheloos kernel: [1887360.040025] ata3: SATA link up 3.0 Gbps (SStatus 123 SControl 300)
May 31 06:49:11 acheloos kernel: [1887360.041846] ata3.00: configured for UDMA/133
May 31 06:49:11 acheloos kernel: [1887360.041859] ata3: EH complete
</code></pre>
","<linux><sata><corruption><duplicity>","2017-06-01 10:02:31"
"780524","CNAME record for CDN not working","<p>I am trying to point cdn.mydomain.in to xxxxxx.rsc.cdn77.org. using DIGITAL OCEAN. What is wrong in my CNAME records? What am i missing?</p>

<pre><code>$ORIGIN magiccrate.in.  
$TTL 1800  
magiccrate.in. IN SOA ns1.digitalocean.com. hostmaster.magiccrate.in. 146489033438 10800 3600 604800 1800  
magiccrate.in. 1800 IN NS ns1.digitalocean.com.  
magiccrate.in. 1800 IN NS ns2.digitalocean.com.  
magiccrate.in. 1800 IN NS ns3.digitalocean.com.  
www.magiccrate.in. 1800 IN CNAME mydomain.in.  
magiccrate.in. 1800 IN MX 10 aspmx1.googlemail.com.  
magiccrate.in. 1800 IN MX 10 aspmx9.googlemail.com.   
magiccrate.in. 1800 IN A xx.xx.xx.xx  
cdn.magiccrate.in. 1800 IN CNAME xxxxxx.rsc.cdn77.org.  
</code></pre>

<p>When I do a DIG lookup it shows an A record to mydomain's IP. I am doing this essentially to set up a cdn for my domain. </p>
","<domain-name-system>","2016-06-02 12:22:18"
"853441","sending HTTPS traffic after Squid+SSL_BUMP to a second proxy","<p>After configuring Squid to perform SSL Bump on HTTPS SSL requests from clients..i want to send this to another proxy that will  perform its own MITM and connect to the 'target Server' and return back the information to the client........  What is needed for squid to pass the requests(after ssl bump is performed) to the second proxy?</p>

<p>client(box1)->iptables(box1)->squiq+ssl_bump(box2)->anotherproxy(box3)->targetServer</p>

<p>box1 was updated with iptable rules </p>

<pre><code>iptables -t nat -A OUTPUT -p tcp --dport 443 -j DNAT --to 10.1.1.1.1:8444
</code></pre>

<p>box2 has SSL_BUMP configured to listen and decrypt on 8444.. however i am unsure how to configure squid how to pass the descrypted ssl redirect from the ssl_bump...I've tried cache_peer(parent) and squid does not successfully connect to the 'cache_peer(parent)..</p>
","<ssl><iptables><squid>","2017-06-01 14:54:23"
"853462","Cannot get vCenter installed in VMware Workstation using OVA file","<p>I am testing in a lab environment with the following setup:</p>

<p>I have a Windows 10 laptop with VMware Workstation 12 Pro installed. I have create a virtual machine in Workstation running Windows Server 2012 R2. I have set up AD and DNS on it. This server has an IP address of 192.168.59.129 and its FQDN is win2012.ad.example.com. I can ping the machine and DNS is working correctly.</p>

<p>I am now trying to install vCenter Server Appliance 6.5 , so far with no success. I am trying to use the OVA file: <code>VMware-vCenter-Server-Appliance-6.5.0.5200-4944578_OVF10.ova</code> located in the ISO. I understand after reading several articles that I first need to configure the .vmx file before booting my machine. I think the main reason is that I am not fully understanding the settings that should be placed in the file, specifically the vmdir settings. I have tried a number of different variations on the settings and still nothing seems to be working. My most recent configuration looks like this:</p>

<pre><code>guestinfo.cis.deployment.node.type = ""embedded""
guestinfo.cis.appliance.net.addr.family = ""ipv4""
guestinfo.cis.appliance.net.mode = ""static""
guestinfo.cis.appliance.net.pnid = ""vc.ad.example.com""
guestinfo.cis.appliance.net.addr = ""192.168.59.194""
guestinfo.cis.appliance.net.prefix = ""24""
guestinfo.cis.appliance.net.gateway = ""192.168.59.129""
guestinfo.cis.appliance.net.dns.servers = ""192.168.59.129""
guestinfo.cis.appliance.root.passwd = ""Password#1""
guestinfo.cis.appliance.ssh.enabled = ""True""
guestinfo.cis.deployment.autoconfig = ""True""
guestinfo.cis.vmdir.password = ""Password#1""
guestinfo.cis.vmdir.site-name = ""vc""
guestinfo.cis.vmdir.domain-name = ""ad.example.com""
guestinfo.cis.ceip_enabled = ""False""
</code></pre>

<p>I have also made sure that vc.ad.example.com is mapped correctly to 192.168.59.194 on the DNS server, including reverse DNS (PTR) entries.</p>

<p>After the server begins its initialization routine, I eventually get this error on the screen:</p>

<pre><code>Failed to start services. Firstboot Error.
</code></pre>

<p>What am I doing wrong here?</p>

<p><strong>Update</strong></p>

<p>As per the answer below, I have changed the config settings to what you see below, but it is still giving me the same error:</p>

<pre><code>guestinfo.cis.deployment.node.type = ""embedded""
guestinfo.cis.appliance.net.addr.family = ""ipv4""
guestinfo.cis.appliance.net.mode = ""static""
guestinfo.cis.appliance.net.pnid = ""vc.ad.example.com""
guestinfo.cis.appliance.net.addr = ""192.168.59.194""
guestinfo.cis.appliance.net.prefix = ""24""
guestinfo.cis.appliance.net.gateway = ""192.168.59.129""
guestinfo.cis.appliance.net.dns.servers = ""192.168.59.129""
guestinfo.cis.appliance.root.passwd = ""Password#1""
guestinfo.cis.appliance.ssh.enabled = ""True""
guestinfo.cis.deployment.autoconfig = ""True""
guestinfo.cis.appliance.ntp.servers = ""pool.ntp.org""
guestinfo.cis.vmdir.password = ""Password#1""
guestinfo.cis.vmdir.site-name = ""mysite""
guestinfo.cis.vmdir.domain-name = ""vsphere.local""
guestinfo.cis.ceip_enabled = ""False""
</code></pre>

<p>Any other ideas?</p>
","<vmware-vsphere><vmware-vcenter>","2017-06-01 16:23:57"
"853481","Can't add DNS CNAME record","<p>I have internal AD domain: ad.domain.com
external public: domain.com</p>

<p>on DNS I have two forward lookup zone:</p>

<p>ad.domain.com for my active directory, and
mail.domain.com - with black A record pointing to my internal IP Mail server.</p>

<p>On public DNS my domain register pointing mail.domain.com to my public IP on router which is next dst-nat to internal mail server - do you think that this configuration is OK?</p>

<p>Now I need that local users typing in web browser address: mail.domain.com automatically go to mail.ad.domain.com, so on mail.domain.com I'm trying to create CNAME pointing it to A record that resides in ad.domain.com with name ""mail"" and ip address of mail server, and get this error:</p>

<p>""A new record cannot be created. An alias (CNAME) record cannot be added to this DNS name. The DNS name contains records that are incompatible with the CNAME record.""</p>
","<domain-name-system><internal-dns>","2017-06-01 18:50:37"
"853651","How do I delete all DNS configuration and records?","<p>I was wondering how I would be able to delete my entire DNS server configuration, including all DNS records on Windows Server 2012 r2. Is there a built in command or an available script to accomplish this? </p>
","<domain-name-system><windows-server-2012-r2>","2017-06-02 13:41:10"
"923356","weird windows 7 update Intel Display","<p>I have this weird update in windows update on windows 7.</p>

<p>Full details from windows update: </p>

<blockquote>
  <p>Intel Corporation - Display - 12/14/2017 12:00:00 AM - 10.18.14.4889</p>
  
  <p>Download size: 86.7 MB</p>
  
  <p>You may need to restart your computer for this update to take effect.</p>
  
  <p>Update type: Important</p>
  
  <p>Intel Corporation Display  driver update released in  December 2017</p>
  
  <p>More information: 
  <a href=""http://sysdev.microsoft.com/support/default.aspx"" rel=""nofollow noreferrer"">http://sysdev.microsoft.com/support/default.aspx</a></p>
  
  <p>Help and Support: 
  <a href=""http://support.microsoft.com/select/?target=hub"" rel=""nofollow noreferrer"">http://support.microsoft.com/select/?target=hub</a></p>
</blockquote>

<p>You can see it's ""Important"". I don't have ""Optional"" turned on.</p>

<p>I do have ""Get updates for other Microsoft Products"" aka Microsoft Update turned on, but I doubt that's relevant.</p>

<p>I have Device Installation settings set to ""Never"".</p>

<p>There's no KB number.</p>

<p>The more information link brings up a blank page and blank address bar in firefox for me. In IE it redirected to <code>E:\WESPortal_down\default.htm</code> no joke.</p>

<p>DuckDuckGo'ing the title brings this page: <a href=""http://www.catalog.update.microsoft.com/ScopedViewInline.aspx?updateid=db4db1a5-4140-4d80-a63e-c0447c791353"" rel=""nofollow noreferrer"">http://www.catalog.update.microsoft.com/ScopedViewInline.aspx?updateid=db4db1a5-4140-4d80-a63e-c0447c791353</a></p>

<p>Which provides almost no information, certainly no commentary, save this: "" Driver Model: Intel(R) Iris(TM) Pro Graphics 5200"".</p>

<p>And you can't dl the update manually from that page.</p>

<p>The Intel Iris Pro Graphics 5200 driver download page doesnt have any drivers that match in version number or date: <a href=""https://downloadcenter.intel.com/product/81493/Intel-Iris-Pro-Graphics-5200"" rel=""nofollow noreferrer"">https://downloadcenter.intel.com/product/81493/Intel-Iris-Pro-Graphics-5200</a></p>

<p>It appears from the net that things like this have happened before but I can't find any discussion on this particular one.</p>

<p>In response to comment, to further clarify what the the actual question is: WU is presenting me with an update about which there is literally NO information. I do not trust updates from WU unconditionally, because many have gone wrong in the past. A small percent, to be sure, but dozens maybe hundreds to date. So I ask myself, is this a good or a bad update? Literally the only piece of evidence suggesting that it's good is that it is suggested by WU. That's not enough. So I'm looking for some corroborating evidence that this is actually a good update, and not a hack, and not a corrupted or accidental update.</p>

<p>Do you have any info about what this update or somewhere I can learn about this update?</p>
","<windows-7><windows-update><drivers><intel>","2018-07-24 22:04:24"
"853729","How Do I Configure a Static Route for a Gateway on the Same Network?","<p>I'm trying to configure a router to forward traffic to another router on the same network (so that I don't have to configure static routes on each other machine on the network).</p>

<p>Here's a simplistic diagram of the network.</p>

<p><img src=""https://i.sstatic.net/oIViu.png"" width=""400"" /></p>

<p><strong>Goal</strong>: I want to be able to contact station_d from station_a (without configuring a static route on station_a).</p>

<p>I attempted to create a static route on inet_gateway, like <code>192.168.3.0/24 10.0.0.5</code>. When that is in place, I can contact station_d from inet_gateway. However, I cannot contact station_d from station_a. Attempts to contact station_d get routed out to the Internet.</p>

<p>It appears that inet_gateway doesn't know that it's supposed to route the traffic. The ping I send from station_a has the MAC address of inet_gateway in the ethernet header, but inet_gateway doesn't (appear to) forward the traffic to router_b.</p>

<p>If I configure a static route on station_a (exactly like the one I made on inet_gateway), station_a can contact station_d. I don't want to have to maintain static routes like this on all the stations on the network, though.</p>

<p>Is this kind of thing possible: to have a router ""route"" traffic through another router on the same network as the originator?</p>

<p><strong>Additional Information</strong>
Here are the routing tables (IP addresses changed to match diagram and protect the innocent). The Internet traffic goes through 192.168.0.1/24.</p>

<p>(inet_gateway)</p>

<pre><code>Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
default         192.168.0.1     0.0.0.0         UG    0      0        0 eth0
loopback        *               255.0.0.0       U     0      0        0 lo
10.0.0.0        *               255.255.0.0     U     0      0        0 eth2
192.168.0.0     *               255.255.255.0   U     0      0        0 eth0
192.168.3.0     10.0.0.5        255.255.255.0   UG    0      0        0 eth2
</code></pre>

<p>(router_b)</p>

<pre><code>Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
192.168.3.0     *               255.255.255.0   U     0      0        0 ath0
10.0.0.0        *               255.255.0.0     U     0      0        0 br0
default         10.0.0.1        0.0.0.0         UG    0      0        0 br0
</code></pre>

<p>//Bridged Adapters//: This kind of bridging is the default for the Ubiquiti device I'm using. I didn't attempt to unbridge some of the adapters. I put the device into ""router"" mode, but it still has the adapters bridged. I'm not certain if I can change that. <strong>I think this is unrelated to the fundamental problem.</strong></p>

<h2>ICMP Redirection</h2>

<p>This seems like what I need to get working. Thank you very much for all the advice.</p>

<p>I cannot get inet_gateway to send the ICMP redirects. I believe the device is configured to send them, but its behavior suggests that it believes that it doesn't know it should be sending the redirects.</p>

<p>Attempting a traceroute to station_d from station_a results in this:</p>

<pre><code>&gt; tracert -d 192.168.3.10
Tracing route to 192.168.3.10 over a maximum of 30 hops

  1    &lt;1 ms    &lt;1 ms    &lt;1 ms  10.0.0.1
  2     1 ms     7 ms     2 ms  192.168.0.1
  3    15 ms    15 ms    16 ms  10.30.0.1
  4    34 ms    37 ms    22 ms  216.229.64.249
  5    35 ms    32 ms    27 ms  216.229.80.185
  6    25 ms    50 ms    35 ms  10.92.1.38
  7    32 ms    30 ms    28 ms  10.92.1.86
</code></pre>

<p>I would conclude that inet_gateway (Ubiquiti EdgeMax) does not know if should be sending ICMP redirects.</p>
","<networking><routing>","2017-06-02 20:20:26"
"780855","Installing IDE and GUI inside Elastic Beanstalk","<p>I am attempting to make a development environment in elastic beanstalk, which will be similar to a production environment. </p>

<p>I would like to have an IDE installed in the environment, and have graphical desktop to work with. I can create all this in a new instance, and create an AMI out of it, but that will not work with Elastic beanstalk.</p>

<p>Amazon Linux doesn't allow you to install a GUI, and i have been unable to find a Red Hat AMI that works with Elastic Beanstalk, or create one. </p>

<p>Please let me know your thoughts.</p>
","<amazon-web-services><redhat><elastic-beanstalk>","2016-06-03 17:18:32"
"780924","ESXi Host Supported Maximum Physical Hard Drive","<p>Does anyone know what is the maximum physical hard drive space a single host of ESXi 6.0 can support? And no, this is not that dumb because Microsoft only supports a maximum storage of 64 TB.</p>

<p>I am planning to purchase 24 QTY 4 TB Seagate SAS drives from Amazon for my NORCO RFC-4224U case, with 24 hard drive bays, and an ARECA 1883IX-24 RAID card. If ESXi does not support 96 TB of total physical hard drives directly hosted in this box, what other operating systems or software (Openfiler, FREENAS?) can support this file size?</p>

<p>Thanks!</p>
","<vmware-esxi><storage>","2016-06-04 08:04:06"
"1028666","(active directory)How to authorize an user to use a permission required software/application?","<p>I'm trying to  authorize a user to use a permission required software/application.</p>
<p>I've tried two ways:</p>
<p><strong>1.</strong> I built up a OU under current OU and add the user into it.</p>
<p>Then, set up a GPO : Computer Configuration -&gt; Policies -&gt; Windows settings-&gt; Software Restriction Policies -&gt; Others</p>
<p>Right click-&gt; New Path Policy</p>
<p>Paste the path of the application and set the security level to &quot;unrestricted&quot;</p>
<p><strong>2</strong> <a href=""https://thesysadminchannel.com/add-local-administrators-via-gpo-group-policy/"" rel=""nofollow noreferrer"">https://thesysadminchannel.com/add-local-administrators-via-gpo-group-policy/</a></p>
<p>Neither worked for me.</p>
","<active-directory>","2020-08-03 09:38:20"
"1028736","Dell R510 can't access Lifecycle controller","<p>NB: I've checked <a href=""https://serverfault.com/questions/942416/how-to-upgrade-idrac-firmware-and-bios-of-dell-poweredge-r510"">How to upgrade iDRAC firmware and BIOS of Dell PowerEdge R510?</a> -- they are much further along in that they have LifeCycle  controller working and just had an issue with the filetype.</p>
<p>I'm having a tough time trying to update BIOS firmware and other ancient drivers/software on my Dell PowerEdge R510. It seems like the most straightforward way to do it would be to use the LifeCycle controller, point it to ftp.dell.com and handle it from there. Problem is, I don't seem to have LCC installed (even though on SUU it says it <em>is</em> installed). When I boot into system services via F10, I am launched into UEFI Unified Server Configurator. In there, my only options are Home, OS Deployment, Hardware Diagnostics, USC settings (set ip address), and About. The OS deployment is the only options that says anything about Drivers, but when I try to load up from ftp.dell.com, I hit next, and it just refreshes back to the same page with no further progress. If I opt to save locally, I always get an error that USB is too small (even with empty 16GB drive).</p>
<p><a href=""https://i.sstatic.net/S0IGy.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/S0IGy.jpg"" alt=""Unified Server configurator doesn't have platform options or LifeCycle Controller"" /></a></p>
<p><a href=""https://i.sstatic.net/9mG1m.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9mG1m.jpg"" alt=""Failing at ftp.dell.com"" /></a></p>
<p>So I downloaded and install Dell Repository Manager thinking I might be able to use that with SUU; but after installing, I don't have an option to add bundles/create repos for R510 (it jumps from R440 to R520). Even so, I tried adding my own BIOS exe bundles, and always get the error <code>0 of n DUP's installed</code>).
<a href=""https://i.sstatic.net/4vhZP.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4vhZP.jpg"" alt=""enter image description here"" /></a>
The server is running esxi 5.1 (yes it's old, this is why I'm trying to update firmware so I can update OS). I also tried scp'ing the LifeCycle controller BIN to the host and running there, but was met with:</p>
<pre><code> /tmp # ./Lifecycle-Controller_Legacy_Application_0WFGM_LN_1.7.5.4_A00.BIN
Dell Update Package 14.11.201 (BLD_248)press RETURN)
Copyright (c) 2015 Dell, Inc. All Rights Reserved.
Release Title:
Dell LifeCycle Controller v1.7.5, 1.7.5.4, A00

Release Date:
April 24, 2015

Default Log File Name:
0WFGMA00

Reboot Required:
No

Supported System(s):
Equallogic DX6012S
Equallogic DX6000
Equallogic DX6004S
Equallogic DX6000G
Equallogic FS7500
PowerVault DL2100
PowerVault NX3500
PowerVault DL2200
PowerVault NX300'less', press 'q' to continue DUP exec; lines 1-23/104 11%
PowerVault NX3100less', press 'q' to continue DUP exec; lines 2-24/104 11%
Collecting inventory.................................................................................................................................................................................................................................................................................................................
Inventory Failure: Unified Server Configurator Failure - Detach USC failure - Detaching a partition has failed
</code></pre>
<p>It also seems to say in various places that iDrac is running, but I can't access it from any web browser or shell. I ran nmap on the ip address and it never finds any services listening (this is the ip address set in BMIC).</p>
<p>Any ideas on what I might be doing wrong? Or is there an easier way to go about updating this firmware? Since it's running ESXI, can I just scp over BIN files and run those?</p>
","<vmware-esxi><dell-poweredge><firmware>","2020-08-03 21:01:15"
"1028750","dig shows SERVFAIL , while pointing at nameserver works fine","<p>somewhat similar to <a href=""https://stackoverflow.com/questions/12025173/dig-returns-servfail-but-trace-works"">this thread</a>, I faced with a problem where <code>dig blah.net</code> (for the sake of simplicity let's call it <code>blah.net</code>) returns <code>status: SERVFAIL</code></p>
<p>This DNS zone is hosted on route53, and I use GoDaddy as registrar - which points to NS records of the hosted zone on route53</p>
<p>this is a pretty simple zone which consists of a CNAME and an A Record, and the default values for SOA and NS records for route53 hosted zone(only TTL has been updated/reduced)</p>
<p>The problem is <code>dig blah.net</code> causes <code>status: SERVFAIL</code> while pointing at a nameserver resoloves fine (e.g. <code>dig @ns-#.awsdns-##.net. blah.net</code> works fine with <code>status: NOERROR</code> )</p>
<p>It also shows the records/resolves fine (no errors) if I use <code>dig blah.net +trace</code></p>
<p>I've waited for more than 48hrs to make sure GoDaddy propagated the changes, and double/triple checked GoDaddy points to <code>blah.net</code> NS records</p>
<p>I don't see anything weird on <code>/etc/resolve.conf</code> on my machine (but not an expert!), and fwiw it also fails on google's nameserver (<code>@8.8.8.8</code>)</p>
<p>Is it possible that something went wrong from GoDaddy's side?
Any suggestion/comment on how to debug this further is really appreciated</p>
<hr />
<p>[Edit] fix: typo in dns name</p>
","<domain-name-system><dns-zone><amazon-route53><godaddy><dig>","2020-08-03 23:09:01"
"782201","Cassandra node networking accross multiple datacenter","<p>For Multiple datacenter Cassandra cluster deployment Do i need public ip for every nodes? Suppose i've architecture as follows: </p>

<pre><code>Datacenter US
Node1: 
eth0 =162.63.93.45 (public ip)
eth1= 192.168.56.101(private ip)

Node2: 
eth0= 192.168.56.102(private ip)

Node3: 
eth0= 192.168.56.103(private ip)

Node4: 
eth0= 192.168.56.104(private ip)

Datacenter EU
Node1: 
eth0 =180.98.100.87 (public ip)
eth1= 192.168.56.101(private ip)

Node2: 
eth0= 192.168.56.102(private ip)

Node3: 
eth0= 192.168.56.103(private ip)

Node4: 
eth0= 192.168.56.104(private ip)
</code></pre>

<p>Now my question is, is it possible to setup cassandra cluster on this type of architecture? or i need public ip interface for every node??</p>
","<linux><networking><cassandra>","2016-06-06 12:03:18"
"923648","Nginx runs on .com/port but not on .com/","<p>I may need a little help here. I feel I am almost there but missed something in my setup. </p>

<p>I can connect to my <code>.com/3000</code> to see my local node app running on server.
To connect to .com directly I just need to use proxy_pass so I did </p>

<p><code>proxy_pass http://127.0.0.1:3000</code>;</p>

<p>But it won't run.</p>

<p>I have a nodejs app running on localhost:3000 on server.</p>

<p>Below is my default file in <code>/etc/nginx/sites-available</code></p>

<pre><code>server {
    listen 80 default_server;
    listen [::]:80 default_server;


    index index.html index.htm index.nginx-debian.html;

    server_name _;

    location / {
            proxy_pass http://127.0.0.1:3000;
    }

 }
</code></pre>

<p>And here's my <strong>nginx.config</strong> file</p>

<pre><code>user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log;
pid /var/run/nginx.pid;

# Load dynamic modules. See /usr/share/doc/nginx/README.dynamic.
include /usr/share/nginx/modules/*.conf;
include /etc/nginx/sites-enabled/*;

events {
    worker_connections 1024;
}

http {
    log_format  main  '$remote_addr - $remote_user [$time_local] ""$request"" '


                  '$status $body_bytes_sent ""$http_referer"" '
                  '""$http_user_agent"" ""$http_x_forwarded_for""';

access_log  /var/log/nginx/access.log  main;

sendfile            on;
tcp_nopush          on;
tcp_nodelay         on;
keepalive_timeout   65;
types_hash_max_size 2048;

include             /etc/nginx/mime.types;
default_type        application/octet-stream;

# Load modular configuration files from the /etc/nginx/conf.d directory.
# See http://nginx.org/en/docs/ngx_core_module.html#include
# for more information.
include /etc/nginx/conf.d/*.conf;
index   index.html index.htm;

server {
    listen       80 default_server;
    listen       [::]:80 default_server;
    server_name  localhost;
    root         /usr/share/nginx/html;

    # Load configuration files for the default server block.
    include /etc/nginx/default.d/*.conf;

    location / {
    }

     # redirect server error pages to the static page /40x.html
    #
    error_page 404 /404.html;
        location = /40x.html {
    }

    # redirect server error pages to the static page /50x.html
    #
    error_page 500 502 503 504 /50x.html;
        location = /50x.html {
    }

}
</code></pre>

<p><strong>Update -</strong> </p>

<p>I am on Amazon Ec2 instance. The page at .com displays as</p>

<p><a href=""https://i.sstatic.net/SclzN.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/SclzN.png"" alt=""enter image description here""></a></p>

<p><strong>For root directive I have tried</strong></p>

<ol>
<li><code>root /var/www/app/index.html</code> and in this case it will just throw me 404 not found nginx page**</li>
<li><code>root /var/www/app</code> and in this case it gives 403 forbidden error**</li>
</ol>
","<nginx><amazon-ec2>","2018-07-26 12:59:07"
"1028941","DNS subdomain delegation for different servers, but same domain","<p>could someone explain to me what i am doing wrong, or it is just impossible to do. Just for learning, and understading how subdomain delegation works.</p>
<p>I have 1 domain eg.: <strong>server.com</strong> and <strong>3 VPS servers</strong>, what i am trying to do is set custom name servers with subdomain.server.com to VPS servers like eg.: <strong>ns1.vps2.server.com ns2.vps2.server.com</strong> that users on hosting panel could manage DNS settings.</p>
<p>Domain server.com works on ns1.server.com ns2.server.com on 1vps vps1.server.com, some domains changed to ns1.server.com and ns2.server.com NS is changed and works, all DNS can be changed for them from vps1 hosting DNS zone.</p>
<p>For VPS2 and VPS3 i can't make it work.</p>
<p><strong>What i have made on VPS1 where server.com domain is hosted</strong></p>
<pre><code>www.server.com. CNAME  server.com.
vps1.server.com. A  1.1.1.1
vps2.server.com. A  2.2.2.2

ns1.server.com. A  1.1.1.1
ns2.server.com. A  1.1.1.2

server.com. NS  ns1.server.com.
server.com. NS  ns2.server.com.

ns1.vps2.server.com. A  2.2.2.2
ns2.vps2.server.com. A  2.2.2.3

vps2.server.com. NS  ns1.vps2.server.com.
vps2.server.com. NS  ns2.vps2.server.com.

SUCCESS: Getting information for Domain 'server.com' complete.
</code></pre>
<p>^ But this configuration doesn't work ^</p>
<p><strong>When using nslookup on ns1.vps2.server.com</strong></p>
<pre><code>router.asus.com can't find ns1.vps2.server.com: Server failed
</code></pre>
<p><strong>When using nslookup on server.com</strong></p>
<pre><code>server.com nameserver=ns1.server.com
server.com nameserver=ns2.server.com
</code></pre>
<p>Could someone help me with this situation? Maybe someone will find out what i am doing wrong with this.</p>
<p><strong>VPS2 named.conf</strong></p>
<pre><code>// $Id: named.conf,v 1.1.1.1 2001/10/15 07:44:36 kap Exp $

// -- THE FOLLOWING LINES WERE GENERATED BY PLESK. IF YOU MODIFY THEM, THEY WILL BE OVERWRITTEN WHEN THESE SETTINGS ARE MANAGED IN PLESK UI. --
options {
    allow-recursion {
        any;
    };
        listen-on-v6 { any; };
    version &quot;none&quot;;
    directory &quot;/var&quot;;
    auth-nxdomain no;
    pid-file &quot;/var/run/named/named.pid&quot;;
};

key &quot;rndc-key&quot; {
    algorithm hmac-md5;
    secret &quot;20nyv==&quot;;
};

controls {
    inet 127.0.0.1 port 953
    allow { 127.0.0.1; } keys { &quot;rndc-key&quot;; };
};

zone &quot;.&quot; {
    type hint;
    file &quot;named.root&quot;;
};

zone &quot;0.0.127.IN-ADDR.ARPA&quot; {
    type master;
    file &quot;localhost.rev&quot;;
};
// -- END OF LINES GENERATED BY PLESK. --


// -- PLEASE ADD YOUR CUSTOM DIRECTIVES BELOW THIS LINE. --
// ...
// -- END OF YOUR CUSTOM DIRECTIVES. --


// -- ALL LINES BELOW WERE GENERATED BY PLESK. IF YOU MODIFY THEM, THEY WILL BE OVERWRITTEN WHEN THESE SETTINGS ARE MANAGED IN PLESK UI. --

zone &quot;vps2.server.com&quot; {
    type master;
    file &quot;vps2.server.com&quot;;
    allow-transfer {
        common-allow-transfer;
    };
};
acl common-allow-transfer {
    none;
};
</code></pre>
<p><strong>VPS2 named zone /var/named/...vps2.server.com</strong></p>
<pre><code>; *** This file is automatically generated by Plesk ***
$TTL    300

@       IN      SOA     ns1.vps2.server.com. server.server.com. (
                        2020080553      ; Serial
                        18000   ; Refresh
                        3600    ; Retry
                        604800  ; Expire
                        10800 ) ; Minimum

vps2.server.com.            IN NS   ns2.vps2.server.com.
vps2.server.com.            IN NS   ns1.vps2.server.com.
ns2.vps2.server.com.                IN A    1.1.1.2
ipv4.vps2.server.com.               IN A    1.1.1.1
mail.vps2.server.com.               IN A    1.1.1.1
webmail.vps2.server.com.            IN A    1.1.1.1
ns1.vps2.server.com.                IN A    1.1.1.1
www.vps2.server.com.                IN CNAME        vps2.server.com.
ftp.vps2.server.com.                IN CNAME        vps2.server.com.
vps2.server.com.            IN MX  10 mail.vps2.server.com.
vps2.server.com.            IN TXT  &quot;v=spf1 +a +mx +a:vps2.server.com -all&quot;
_dmarc.vps2.server.com.             IN TXT  &quot;v=DMARC1; p=none&quot;
_imaps._tcp.vps2.server.com.                IN SRV 0 0 993 vps2.server.com.
_pop3s._tcp.vps2.server.com.                IN SRV 0 0 995 vps2.server.com.
_smtps._tcp.vps2.server.com.                IN SRV 0 0 465 vps2.server.com.
</code></pre>
<p><strong>VPS1 named zone /var/named/...vps2.server.com</strong></p>
<pre><code>; *** This file is automatically generated by Plesk ***
$TTL    300

@       IN      SOA     ns1.vps2.server.com. server.server.com. (
                        2020080576      ; Serial
                        60      ; Refresh
                        300     ; Retry
                        604800  ; Expire
                        1 )     ; Minimum

vps2.server.com.            IN NS   ns2.vps2.server.com.
vps2.server.com.            IN NS   ns1.vps2.server.com.
ns1.vps2.server.com.                IN A    1.1.1.1
ns2.vps2.server.com.                IN A    1.1.1.2
vps2.server.com.            IN A    1.1.1.1
www.vps2.server.com.                IN CNAME        vps2.server.com.
vps2.server.com.            IN MX  10 vps2.server.com.
</code></pre>
<p><strong>From VPS2 $dig vps2.server.com</strong></p>
<pre><code>root@vps2:~# dig vps2.server.com

; &lt;&lt;&gt;&gt; DiG 9.10.3-P4-Debian &lt;&lt;&gt;&gt; vps2.server.com
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 29912
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 512
;; QUESTION SECTION:
;vps2.server.com.                  IN      A

;; ANSWER SECTION:
vps2.server.com.           299     IN      A       1.1.1.1

;; Query time: 28 msec
;; SERVER: 8.8.8.8#53(8.8.8.8)
;; WHEN: Thu Aug 06 07:57:31 EEST 2020
;; MSG SIZE  rcvd: 57
</code></pre>
<p><strong>From VPS1 $dig vps2.server.com</strong></p>
<pre><code>root@vps2:~# dig vps2.server.com

; &lt;&lt;&gt;&gt; DiG 9.10.3-P4-Debian &lt;&lt;&gt;&gt; vps2.server.com
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 45514
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 2, ADDITIONAL: 3

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;vps2.server.com.                  IN      A

;; ANSWER SECTION:
vps2.server.com.           68      IN      A       1.1.1.1

;; AUTHORITY SECTION:
vps2.server.com.           68      IN      NS      ns1.vps2.server.com.
vps2.server.com.           68      IN      NS      ns2.vps2.server.com.

;; ADDITIONAL SECTION:
ns1.vps2.server.com.       68      IN      A       1.1.1.1
ns2.vps2.server.com.       68      IN      A       1.1.1.2

;; Query time: 0 msec
;; SERVER: 80.208.229.143#53(80.208.229.143)
;; WHEN: Thu Aug 06 07:58:59 EEST 2020
;; MSG SIZE  rcvd: 125
</code></pre>
","<domain-name-system><nameserver><delegation><ns-record>","2020-08-05 09:24:29"
"854371","set up root password aws instance","<p>I need to set the password for root user when sudo su -, ubuntu user should login via ssh key only.</p>

<pre><code>#ssh -i server.pem ubuntu@example.com

ubuntu@example.com:~$ sudo su - --&gt; should prompt the password of root

root@example.com:~#   --&gt; and also I have setup the password for root, but it does not prompt the password.


root@example.com:/etc/sudoers.d# cat 90-cloud-init-users 
# Created by cloud-init v. 0.7.5 on Mon, 12 Sep 2016 10:23:06 +0000

# User rules for ubuntu
ubuntu ALL=(ALL) NOPASSWD:ALL

# User rules for ubuntu
ubuntu ALL=(ALL) NOPASSWD:ALL

# User rules for ubuntu
ubuntu ALL=(ALL) NOPASSWD:ALL
</code></pre>
","<amazon-web-services><password><ssh-keys>","2017-06-07 06:52:02"
"923980","Network hierarchy with subnets","<p>Is it possible to divide networks with subnets so that they intersect?
My example is:  </p>

<p>Network A: only visible inside network A
Network B: only visible inside network B
Network C: visible from both networks?</p>

<p>Can this be achieved with subnets or do I take a different approach?</p>

<p>The idea is to have some devices (printers) to be visible from every subnet, while the some devices are private to their own network (e g. Chromecast, so that you can cast only inside your own network).  </p>

<p>Is there a simple solution? If it's not trivial: what would you recommend instead?</p>

<p>Thanks for your advice!</p>
","<networking><subnet><hierarchy>","2018-07-29 10:28:09"
"1029100","GPO Editor - Password complexity number of days grayed out","<p>Firstly thank you for all your help! I recommend this site to every sysadmin I meet but I rarely post.</p>
<p>I've currently moved from an MSP into internal IT and I've came across and Using MS Sever 2016 trying to altar the password complexity to 12 from 8 is this possible?</p>
","<windows><active-directory><windows-server-2016>","2020-08-06 12:10:58"
"854559","Apache always fall back to built-in page","<p>I made a chess web game in Dart, unsuccessfully trying to deploy to a web host. When I accessed this web app with a public IP, it ran smoothly. However, when accessing with a domain name, it always fall back to default page. Here is my Apache setting:</p>

<pre><code>&lt;IfModule mod_ssl.c&gt;
    &lt;VirtualHost _default_:443&gt;
        ServerAdmin webmaster@localhost

        DocumentRoot /home/user/my_web_app

        ErrorLog ${APACHE_LOG_DIR}/error.log
        CustomLog ${APACHE_LOG_DIR}/access.log combined

        SSLEngine on

        SSLCertificateFile  /etc/ssl/certs/ssl-cert-snakeoil.pem
        SSLCertificateKeyFile /etc/ssl/private/ssl-cert-snakeoil.key

        SSLVerifyClient none

        #SSLOptions +FakeBasicAuth +ExportCertData +StrictRequire
        &lt;FilesMatch ""\.(cgi|shtml|phtml|php)$""&gt;
            SSLOptions +StdEnvVars
        &lt;/FilesMatch&gt;

        ProxyPreserveHost On

        ProxyPass / http://127.0.0.1:8080/
        ProxyPassReverse / http://127.0.0.1:8080/
    &lt;/VirtualHost&gt;
&lt;/IfModule&gt;
</code></pre>

<p>I tried both Apache and Nginx; the same condition happened. However, I could not figure out what was wrong? Any suggestion?</p>

<p>[Update on Jun 8, 2017]
I modified the setting in my Apache, finally finding that it was due to Cloud Flare <a href=""https://support.cloudflare.com/hc/en-us/articles/203487280-How-do-I-fix-mixed-content-issues-or-the-infinite-redirect-loop-error-after-enabling-Flexible-SSL-with-WordPress-"" rel=""nofollow noreferrer"">like this</a>. I changed my DNS provider. Most issues was solved.</p>
","<ssl><reverse-proxy><apache2>","2017-06-08 04:00:56"
"854595","Computers unjoin domain after each reboot","<p>This is my config:</p>

<pre><code>controller.mydomain.local (Domain Controller)
     IP Address   : 192.168.1.1/24

computer1.mydomain.local
     IP Address   : 192.168.1.2/24 
     DNS Server 1 : 192.168.1.1 
     DNS Server 2 : 8.8.8.8 

computer2.mydomain.local
     IP Address   : 192.168.1.3/24 
     DNS Server 1 : 192.168.1.1 
     DNS Server 2 : 8.8.8.8
</code></pre>

<p>All these computers have the same username and password. After each reboot I can't ping <code>computer2.mydomain.local</code> or <code>computer1.mydomain.local</code> or <code>controller.mydomain.local</code> from <code>computer1</code> and <code>computer2</code>. I need to disable and enable network adapter 2 or 3 times, sometimes I need to reboot another time to be able to ping domain name or computer names.</p>
","<windows><networking><active-directory><windows-server-2012-r2><domain-name>","2017-06-08 08:41:40"
"924255","Data transfer between two linux servers","<p>I need to transfer data between two linux servers using the ssh connection to the servers, using specific scripts, and using direct cross-over cable. The two servers have multiple network interfaces; the interface 1 (eth0) of both servers are connected to a switch and go online (with IP 192.168.xx.1 (server 1) and 192.168.xx.2 (server2)).</p>

<p>I connected a crossover cable directly between the two servers, on the eth1 interfaces of both, with IP address of the type 192.168.yy.1 (server 1) and 192.168.yy.2 (server 2). When I start the data transfer script I have to enter the target IP address (server 2), and insert 192.168.xx.2.</p>

<p>But will the actual data transfer be via the crossover cable? Will the fastest route in the routing table be used automatically?</p>
","<linux><ssh>","2018-07-31 11:45:57"
"782865","How to manage hosts with multiple sites using Ansible?","<p>I have this sample inventory file:</p>
<pre><code>[web1] // Web Server 1 contains the following sites:
example1.com
example2.com

[web2] // Web Server 2 contains the following sites:
example3.com
example4.com
</code></pre>
<p>But whenever I run a playbook for example with <code>apt-get update</code> command it tries to iterate through all the sites in the given web servers above.</p>
<p>How do I run the playbook in the host itself and not the example*.com sites because it's redundant.</p>
<h2>EDIT</h2>
<p>As requested here's the yaml file: (nothing fancy here.)</p>
<pre><code># Using all but --limit the execution to webservers
# I know I can use `webservers` but this was deliberate
- hosts: all 
  tasks:
    - // sudo apt-get update
</code></pre>
","<ansible><ansible-playbook>","2016-06-09 05:20:13"
"1029441","dns configuration with multiple subdomains and servers?","<p>Will below configuration allow me to use one domain name (example.com) to access two VPSes hosted with the same provider (prod and dev)?</p>
<pre><code>*.example.com        IN A    &lt;vps prod IP&gt;
*.dev.example.com    IN A    &lt;vps dev IP&gt;
</code></pre>
<p>Will this configuration allow me to point another domain (example.org) to a subdomain (api.example.com) without any A record?</p>
<pre><code>*.example.org        IN CNAME    *.api.example.com
</code></pre>
<p>How should the mail (MX) and the apex domain (@) records be configured?</p>
","<cname-record><dns-zone><dns-hosting><a-record>","2020-08-09 20:29:58"
"854913","Queries execute faster when hosted on laptop than on server","<p>I am experiencing a strange issue when publishing an ASP MVC application to Windows Server 2012.</p>

<p>The app is fairly simple, and on most of the pages visited it will query an Access database which is hosted on the server.</p>

<p>I noticed that the queries are quite slow (often taking between 7-10 seconds to complete), as shown when running the app with MiniProfiler.</p>

<p>As a simple test, I decided to host the app on my Windows 10 laptop to see if there was a difference, and it is much faster (Most queries are under 200ms).</p>

<p>This is what I tried so far:</p>

<ul>
<li>Match advanced Application Pool settings</li>
<li>Match Firewall settings</li>
<li>Enabled link local multicast name resolution (LLMNR)</li>
<li>Confirmed that ODBC tracing is disabled (32 and 64)</li>
<li>Moved the Access Backoffice.mdb file to the server's C drive (same as on the laptop)</li>
<li>Made sure web.config match (apart from connectionString Data Source)</li>
<li>Added <code>C:\inetpub\wwwroot\website</code> and <code>C:\Test\Database</code> to antivirus scan exclusions</li>
<li>Increased the <code>MaxBufferSize</code> key value from 0 in <code>Access Connectivity Engine</code> in the registry</li>
</ul>

<p>Does anyone know of a way that I can try debugging this issue, or any possible solutions?</p>

<p><strong>notes</strong></p>

<p>Both my laptop and the server are on the same network while testing this, and I am accessing the app from an ipad.</p>

<p>I am also experiencing the same slowness when using the application from a browser on the server itself (url = 10.0.0.1 or localhost)</p>

<p>When I execute the exact same query code from a console app on the server, it runs a lot faster (on par with the laptop). I am only experiencing this issue via the asp app.</p>

<h3>Laptop Details:</h3>

<ul>
<li>Processor : Intel Core i7-6700HQ CPU @ 2.60GHz</li>
<li>RAM       : 16.0 GB</li>
<li>OS        : Windows 10 x64</li>
</ul>

<h3>Server Details</h3>

<ul>
<li>Processor : Intel Xeon CPU E5-2603 v4 @ 1.70GHz (2 processors)</li>
<li>RAM       : 32.0 GB</li>
<li>OS        : Windows Server 2012 x64</li>
</ul>

<p><strong>Connection String</strong></p>

<p>I have tried the following:</p>

<pre><code>Provider=Microsoft.ACE.OLEDB.12.0;Data Source=W:\My Backoffice.mdb
Provider=Microsoft.ACE.OLEDB.12.0;Data Source=C:\Test\My Backoffice.mdb
Provider=Microsoft.ACE.OLEDB.12.0;Data Source=\\ServerName\W\My Backoffice.mdb
Provider=Microsoft.ACE.OLEDB.12.0;Data Source=\\ServerName\C\Test\My Backoffice.mdb
</code></pre>

<p>With both <code>16.0</code> and <code>12.0</code> as the <code>ACE.OLEDB</code> version, where W is a Local server drive which is used as a network share acrcoss the domain. All of these gave the same result.</p>
","<iis><windows-server-2012><asp.net-mvc><microsoft-access>","2017-06-09 17:38:55"
"782959","What's is the best way to migrate dovecot imap accounts to office365","<p>i need migrate some accounts from dovecot imap service (how cpanel setup) to office365.
This accounts should be of diferent licences  businnes and kiosk.
Is this possible?</p>

<p>Any suggestes?</p>

<p>Somebody has experience in this migration type?</p>
","<dovecot><imap><microsoft-office-365><exchange-migration>","2016-06-09 12:27:53"
"782982","Nmap how to split and filter this(regex)?","<p>I think i need a regex code to filter nmap output.txt
I use a tool called: Word List Updater 2.7
so this is the output: <a href=""http://pastebin.com/HwgiVHDA"" rel=""nofollow noreferrer"">http://pastebin.com/HwgiVHDA</a></p>

<p>I want to filter,remove duplicates and look like this, uff sorry for my bad english
5.2.128.130:3391
5.2.132.8:3389</p>

<p>and then without port:</p>

<p>1.179.133.46
1.186.40.82</p>

<p>This should work on Windows. </p>
","<regex><nmap>","2016-06-09 14:24:48"
"924638","When someone gains remote access to a server are there other ways to execute system commands other than the bash shell?","<p>I am hardening a server and attempting to build a restrictive layer to a potential hacker even with root access to the server to do harm.  </p>

<p>If a user gains root or user access to the shell via say ssh, is there any other way for a user to access system commands other than via the shell commands available to them?  Although <code>cd</code> is a built-in command and could not be removed easily since the bash shell does that command itself see:(<a href=""https://unix.stackexchange.com/questions/11454/what-is-the-difference-between-a-builtin-command-and-one-that-is-not"">https://unix.stackexchange.com/questions/11454/what-is-the-difference-between-a-builtin-command-and-one-that-is-not</a>) see:(<a href=""https://unix.stackexchange.com/questions/38808/why-is-cd-not-a-program"">https://unix.stackexchange.com/questions/38808/why-is-cd-not-a-program</a>),  If <code>ls</code> and <code>ps</code> were disabled on the server, would a intruder have another way to issue system commands?</p>

<p>Assuming that secure copy (scp) was uninstalled on the system and they could not directly upload a payload to the server via scp, and they only had shell access (not physical access). </p>

<p>EDIT: Another element to this question is do arbitrary code execution vulnerabilities typically use bash commands. So, is would this system hardening procedure do anything to prevent say an Apache exploit that gained full root access.</p>
","<bash><ssh-tunnel><linux-kernel><hardening>","2018-08-02 14:39:08"
"855077","Sudo not asking for password","<p>I have a problem with my Antergos installation, when I try to <code>sudo</code> a command it won't ask root password:</p>

<p><img src=""https://i.goopics.net/ggWm.jpg"" alt=""Screenshot""></p>

<p>Also Ctrl + C won't stop the command. Can't connect to WiFi (Network manager is not running and unpossible to start it).</p>

<p>I don't know what to do. Thank you in advance!</p>
","<networking><sudo><arch-linux>","2017-06-11 00:50:53"
"1029785","Docker installation on RHEL 7.2 and file system requirement","<p>We have old <code>RHEL</code> machine <code>version 7.2</code></p>
<p>And we prepare to install docker service on this server</p>
<p>The problem is that server's filesystem was created with <code>ftype=0</code> parametre and docker requires <code>ftype=1</code></p>
<p>One option of course is to format the disk with mkfs and create new file system with <code>ftype=1</code></p>
<p>But we want to stay with the original OS and not format the disk</p>
<p>Second option is to perform the step as defined in the post - <a href=""https://superuser.com/questions/1321926/recreating-an-xfs-file-system-with-ftype-1/1321963#1321963"">https://superuser.com/questions/1321926/recreating-an-xfs-file-system-with-ftype-1/1321963#1321963</a></p>
<p><strong>But this steps are risky and required server to be down</strong></p>
<p>We are searching for third alternative in order to implement the docker installation on rhel version 7.2 in spite <code>ftype=0</code></p>
<p>Interesting direction could be for example by adding a new disk to the server and create <code>XFS</code> file system, with <em>ftype=1</em> ,  but we are not sure how to defined the docker installation on the additional disk (because this disk is actually without OS).</p>
<p>We  will happy to get any other ideas out of the box.</p>
","<docker><redhat><filesystems><xfs><docker-machine>","2020-08-12 09:02:50"
"995895","Finding example SMTP responses","<p>I am writing a script to extract stats from mail logs. With some SMTP servers, extracting the SMTP error code can be difficult, and even with the error code, the associated text can contain useful data. To that end I am looking for examples of SMTP responses, but everything I find in google is just a regurgitation of the standard <em>meaning</em> of the 3 digit SMTP code.</p>

<p>Are there any lists of real SMTP responses (attributed to specific MTAs) published on the internet?</p>
","<smtp><email-server>","2019-12-17 15:55:23"
"1029835","Nginx directing to wrong server name","<p>I am trying a bunch of things on my Node webapp hosted on DigitalOcean and this is how the NGINX's configuration file looks like.</p>
<pre><code>server {
 server_name maindomain.xyz www.maindomain.xyz;

        location / {
                proxy_pass http://localhost:5000;
                proxy_http_version 1.1;
                proxy_set_header Upgrade $http_upgrade;
                proxy_set_header Connection 'upgrade';
                proxy_set_header Host $host;
                proxy_cache_bypass $http_upgrade;
        }

        #Another location here
}

server
{
        server_name *.maindomain.xyz; #server name for wildcard subdomains
        location /
        {
                proxy_pass http://localhost:3000;
                proxy_http_version 1.1;
                proxy_set_header Upgrade $http_upgrade;
                proxy_set_header Connection 'upgrade';
                proxy_set_header Host $host;
                proxy_cache_bypass $http_upgrade;
        }
}
</code></pre>
<p>Now I had bought a new domain and as I did with the main one I have created an A record on DigitalOcean that lets newdomain.xyz to be directed to the webapp server.
But, weirdly enough if I try to browse the new domain on a browser, instead of getting a 502 error I get the main page of my web app which is linked from maindomain.xyz.</p>
<p>As you can see on NGINX I've never specified any directive for newdomain.xyz but for some reason it gets directed to the 5000 port where my node server runs which provides the built React web app through this block of code:</p>
<pre><code>if(process.env.NODE_ENV === 'production')
{
    //Set static folder
    app.use(express.static('client/build'));

    app.get('*', (req,res) =&gt;
    {
        res.sendFile(path.resolve(__dirname, 'client', 'build', 'index.html')); //Path of the webapp's root
    });
}

const PORT = process.env.PORT || 5000;

app.listen(PORT, () =&gt; console.log(`server ${PORT}`));
</code></pre>
<p>Please be aware the I am steel a newbie and I couldn't find any solution to prevent this behaviour.
Thanks to you all.</p>
<p>EDIT for further clarification.
What is happening right now is this:
if I go to maindomain.xyz I get to the home page (that is the desired behaviour)</p>
<p>if I go to newdomain.xyz I STILL get to the home page even if I did not specify a server name for newdomain.xyz. That is the issue I want to resolve.</p>
","<nginx><nameserver>","2020-08-12 15:27:38"
"1029877","Limit client apps used with ejabberd","<p>We have configured Ejabberd.</p>
<p>I need to find a way to restrict end-users so they can only use the Gaijim XMPP client.</p>
<p>I have researched how to do this on various forums, however I have had no luck.</p>
<p>Does Ejabberd allow restricting client apps used?</p>
","<ejabberd>","2020-08-12 19:47:21"
"1029909","How to set smtp_password in GCP Cloud Composer for configuring third-party SMTP services","<p>I can use only G Suit SMTP mail server due to company security policy.</p>
<p>I want to configuring 3rd-party SMTP service for using G Suit mail service but I can't set &quot;smtp smtp_password&quot; config in AIRFLOW CONFIGURATION OVERRIDES tap of Cloud Composer.</p>
<p>GCP Guides says <em>&quot;smtp  smtp_password   The default SMTP password for Airflow. You cannot configure a new password.&quot;</em></p>
<p><a href=""https://i.sstatic.net/uS1F2.png"" rel=""nofollow noreferrer"">GCP Cloud Composer 3rd-party Email Setting guide</a></p>
<p>Is there any method to set smtp_password in Cloud Composer config? Or any other method of setting G suite mail server for using email sending on DAG?</p>
","<smtp><email-server><google-cloud-platform><composer><apache-airflow>","2020-08-13 03:57:05"
"855397","Why is apt-get ""not found"" on a fresh install of google cloud version of Ubuntu?","<p>Just installed ubuntu-1404-trusty-v20170517 using google cloud.  Why do I get the referenced error? Is there some reason Google doesn't include it? (They include docker and python, fer crying out loud)  Here is some relevant output:</p>

<pre><code> cat /etc/lsb-release 
CHROMEOS_AUSERVER=https://tools.google.com/service/update2
CHROMEOS_BOARD_APPID={76E245CF-C0D0-444D-BA50-36739C18EB00}
CHROMEOS_CANARY_APPID={90F229CE-83E2-4FAF-8479-E368A34938B1}
CHROMEOS_DEVSERVER=
CHROMEOS_RELEASE_APPID={76E245CF-C0D0-444D-BA50-36739C18EB00}
CHROMEOS_RELEASE_BOARD=lakitu-signed-mpkeys
CHROMEOS_RELEASE_BRANCH_NUMBER=60
CHROMEOS_RELEASE_BUILDER_PATH=lakitu-release/R59-9460.60.0
CHROMEOS_RELEASE_BUILD_NUMBER=9460
CHROMEOS_RELEASE_BUILD_TYPE=Official Build
CHROMEOS_RELEASE_CHROME_MILESTONE=59
CHROMEOS_RELEASE_DESCRIPTION=9460.60.0 (Official Build) stable-channel lakitu 
CHROMEOS_RELEASE_NAME=Chrome OS
CHROMEOS_RELEASE_PATCH_NUMBER=0
CHROMEOS_RELEASE_TRACK=stable-channel
CHROMEOS_RELEASE_VERSION=9460.60.0
DEVICETYPE=OTHER
GOOGLE_RELEASE=9460.60.0
HWID_OVERRIDE=LAKITU DEFAULT



$ echo $PATH
/usr/local/bin:/usr/bin:/bin:/opt/bin
</code></pre>
","<google-cloud-platform><google-compute-engine>","2017-06-12 23:32:00"
"925032","Do all Dell r520s allow for 8 drives?","<p>I noticed there are some models that only have 4 bays available and have the other 4 protected by plastic... </p>

<p><a href=""https://i.sstatic.net/PI27Y.jpg"" rel=""nofollow noreferrer"">Picture of Dell r520</a></p>

<p>Can you still transform these into 8 bay servers? Why do some of the r520s have the remaining 4 blocked, while others do not?</p>
","<dell><dell-poweredge>","2018-08-05 23:11:07"
"925041","Failed to set Up SSL on Tomcat 9 on AWS EC2","<p>I am following <a href=""https://dzone.com/articles/setting-ssl-tomcat-5-minutes"" rel=""nofollow noreferrer"">this guide</a> to setup ssl on my Tomcat 9 hosted on AWS EC2 (Ubuntu 16.04.5 LTS). </p>

<p>Version: java-8-oracle , apache-tomcat-9.0.10</p>

<p>Key created at <code>/home/ubuntu</code> :</p>

<pre><code>ubuntu@ip-x-x-x-x:~$ sudo keytool -genkey -alias tomcat -keyalg RSA -keystore ./test
Enter keystore password:  
Re-enter new password: 
What is your first and last name?
  [Unknown]:  Tester
What is the name of your organizational unit?
  [Unknown]:  Test  
What is the name of your organization?
  [Unknown]:  Tester ltd
What is the name of your City or Locality?
  [Unknown]:  City
What is the name of your State or Province?
  [Unknown]:  State
What is the two-letter country code for this unit?
  [Unknown]:  US
Is CN=Tester, OU=Test, O=Tester ltd, L=City, ST=State, C=US correct?
  [no]:  yes

Enter key password for &lt;tomcat&gt;
    (RETURN if same as keystore password):  
Re-enter new password: 

Warning:
The JKS keystore uses a proprietary format. It is recommended to migrate to PKCS12 which is an industry standard format using ""keytool -importkeystore -srckeystore ./test -destkeystore ./test -deststoretype pkcs12"".
</code></pre>

<p>Convert key to PKCS12 :</p>

<pre><code>ubuntu@ip-x-x-x-x:~$ sudo keytool -importkeystore -srckeystore ./test -destkeystore ./test -deststoretype pkcs12
Enter source keystore password:  
Entry for alias tomcat successfully imported.
Import command completed:  1 entries successfully imported, 0 entries failed or cancelled

Warning:
Migrated ""./test"" to Non JKS/JCEKS. The JKS keystore is backed up as ""./test.old"".

ubuntu@ip-x-x-x-x:~$ keytool -list -keystore ./test
Enter keystore password:  
Keystore type: JKS
Keystore provider: SUN

Your keystore contains 1 entry

tomcat, Aug 5, 2018, PrivateKeyEntry, 
Certificate fingerprint (SHA1): E7:F9:46:D4:F8:91:E6:A9:68:54:98:6C:22:CF:EE:6D:C5:6A:FF:17
</code></pre>

<p>Modify <code>/opt/tomcat/conf/server.xml</code> :</p>

<pre><code>&lt;Connector port=""80"" protocol=""HTTP/1.1""
           connectionTimeout=""20000""
           redirectPort=""443"" /&gt;

&lt;Connector SSLEnabled=""true"" acceptCount=""100"" clientAuth=""false""
    disableUploadTimeout=""true"" enableLookups=""false"" maxThreads=""25""
    port=""443"" keystoreFile=""/home/ubuntu/test"" keystorePass=""password""
    protocol=""org.apache.coyote.http11.Http11NioProtocol"" scheme=""https""
    secure=""true"" sslProtocol=""TLS"" /&gt;
</code></pre>

<p>Enable firewall :</p>

<pre><code>ubuntu@ip-x-x-x-x:~$ sudo ufw status
Status: active

To                         Action      From
--                         ------      ----
OpenSSH                    ALLOW       Anywhere                  
80                         ALLOW       Anywhere                  
8443                       ALLOW       Anywhere                  
443                        ALLOW       Anywhere                  
8080                       ALLOW       Anywhere                  
OpenSSH (v6)               ALLOW       Anywhere (v6)             
80 (v6)                    ALLOW       Anywhere (v6)             
8443 (v6)                  ALLOW       Anywhere (v6)             
443 (v6)                   ALLOW       Anywhere (v6)             
8080 (v6)                  ALLOW       Anywhere (v6)         
</code></pre>

<p>Result:</p>

<p>Using Chrome I have no problem connecting to <code>http://x-x-x-x</code> (i'll see the 
Apache Tomcat/9.0.10 homepage). But when I try <code>https://x-x-x-x</code> , i'll get <code>This site connot be reached</code> , <code>ERR_CONNECTION_TIMED_OUT</code></p>
","<linux><ubuntu><ssl><tomcat><keytool>","2018-08-06 03:11:24"
"783555","Unable to access SMTP port 25","<p>After restarting my VPS, the email server I recently setup is not working. In particular, it is not accepting connections on port 25. Before the restart it was. I've tried several things, but I'm not sure what is wrong. I'm hoping it is something simple, but I'm concerned that it might be an incompatibility between SystemD, FirewallD, and OpenVZ, the virtualization type used*. </p>

<p>To clarify, before the restart I had been able to us telnet to send mail to my server. I had also been able to send/receive mail to/from the server from/to my gmail account. </p>

<p>I'm hoping someone can help me diagnose the issue. </p>

<p>I think the problem is with the firewall. Here is a successful telnet connection from the VPS to itself:</p>

<pre><code>[root@VPS ~]# telnet mydomain.com 25
Trying &lt;Server IP&gt;...
Connected to mydomain.com.
Escape character is '^]'.
220 mail.mydomain.com ESMTP Exim 4.84_2 Mon, 13 Jun 2016 00:38:39 -0400
&gt; HELO test
250 mail.mydomain.com Hello mydomain.com [Server IP]
&gt; QUIT
221 mail.mydomain.com closing connection
Connection closed by foreign host.
</code></pre>

<p>The connection fails when trying from a different machine. Note that the domain does resolve and this is the Windows machine that I am using to <code>ssh</code> to the VPS using <code>ssh root@mydomain.com</code>. </p>

<pre><code>C:\Users\Liam&gt;telnet mydomain.com 25
Connecting To mydomain.com...Could not open connection to the host, on port 25: Connect failed
</code></pre>

<p>Here is the firewall zone information:</p>

<pre><code>[root@VPS ~]# firewall-cmd --zone=public --list-all
public (default)
  interfaces:
  sources:
  services: dhcpv6-client imaps pop3s smtp ssh
  ports: 25/udp 587/udp 80/tcp 465/udp 465/tcp 25/tcp 587/tcp 9418/tcp 53/tcp 53/udp
  masquerade: no
  forward-ports:
  icmp-blocks:
  rich rules:
</code></pre>

<p>Here is the current status of FirewallD</p>

<pre><code>[root@VPS~]# systemctl status firewalld
* firewalld.service - firewalld - dynamic firewall daemon
   Loaded: loaded (/usr/lib/systemd/system/firewalld.service; enabled; vendor preset: enabled)
   Active: active (running) since Sun 2016-06-12 23:14:35 EDT; 1h 30min ago
 Main PID: 941 (firewalld)
   CGroup: /system.slice/firewalld.service
           `-941 /usr/bin/python -Es /usr/sbin/firewalld --nofork --nopid

Jun 12 23:14:34 hostname systemd[1]: Starting firewalld - dynamic firewall daemon...
Jun 12 23:14:35 hostname systemd[1]: Started firewalld - dynamic firewall daemon.
Jun 12 23:14:35 hostname firewalld[941]: 2016-06-12 23:14:35 ERROR: ebtables not usable, disabling ethernet bridge firewall.
</code></pre>

<p>I'm not sure if that error was there before the restart, but *<a href=""https://ask.fedoraproject.org/en/question/78952/why-do-i-see-this-error-about-ebtables-when-starting-firewalld-in-fedora-22/"" rel=""nofollow noreferrer"">this post</a> has some relevant information about the error as well as FirewallD + OpenVZ. </p>

<p>Here are the statuses of exim and dovecot, as well as named. </p>

<pre><code>[root@VPS~]# systemctl status {exim,dovecot,named}
 * exim.service - Exim Mail Transport Agent
   Loaded: loaded (/usr
 * List item

/lib/systemd/system/exim.service; enabled; vendor preset: disabled)
   Active: active (running) since Sun 2016-06-12 22:08:27 EDT; 2h 38min ago
 Main PID: 172 (exim)
   CGroup: /system.slice/exim.service
           `-172 /usr/sbin/exim -bd -q1h

Jun 12 22:08:27 hostname systemd[1]: Starting Exim Mail Transport Agent...
Jun 12 22:08:27 hostname systemd[1]: Started Exim Mail Transport Agent.

* dovecot.service - Dovecot IMAP/POP3 email server
   Loaded: loaded (/usr/lib/systemd/system/dovecot.service; enabled; vendor preset: disabled)
   Active: active (running) since Sun 2016-06-12 22:08:27 EDT; 2h 38min ago
 Main PID: 182 (dovecot)
   CGroup: /system.slice/dovecot.service
           |-182 /usr/sbin/dovecot -F
           |-205 dovecot/anvil
           |-206 dovecot/log
           |-866 dovecot/auth
           `-869 dovecot/ssl-params

Jun 12 22:08:27 hostname systemd[1]: Starting Dovecot IMAP/POP3 email server...
Jun 12 22:08:27 hostname systemd[1]: Started Dovecot IMAP/POP3 email server.

* named.service - Berkeley Internet Name Domain (DNS)
   Loaded: loaded (/usr/lib/systemd/system/named.service; enabled; vendor preset: disabled)
   Active: active (running) since Sun 2016-06-12 22:08:27 EDT; 2h 38min ago
 Main PID: 191 (named)
   CGroup: /system.slice/named.service
           `-191 /usr/sbin/named -u named

Jun 12 22:08:27 hostname named[191]: zone 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa/IN: loaded serial 0
Jun 12 22:08:27 hostname named[191]: zone localhost.localdomain/IN: loaded serial 0
Jun 12 22:08:27 hostname 967277 named[191]: zone localhost/IN: loaded serial 0
Jun 12 22:08:27 hostname named[191]: zone mydomain.com/IN: loaded serial 0
Jun 12 22:08:27 hostname named[191]: all zones loaded
Jun 12 22:08:27 hostname systemd[1]: Started Berkeley Internet Name Domain (DNS).
Jun 12 22:08:27 hostname named[191]: running
Jun 12 22:08:27 hostname named[191]: zone domain.com/IN: sending notifies (serial 0)
Jun 12 22:15:21 hostname systemd[1]: Started Berkeley Internet Name Domain (DNS).
Jun 12 23:08:27 hostname named[191]: listening on IPv4 interface venet0:0, &lt;VPS IP&gt;#53
</code></pre>

<p>An excerpt from my <code>exim.conf</code> file:</p>

<pre><code>[root@VPS~]# cat /etc/exim/exim.conf | grep daemon_smtp
daemon_smtp_ports = 25 : 465 : 587
</code></pre>

<p>And, I'm not sure if this is relevant, but here is some <code>iptables</code> information.</p>

<pre><code>[root@VPS~]# iptables -L | grep smtp
ACCEPT     tcp  --  anywhere             anywhere             tcp dpt:smtp ctstate NEW
ACCEPT     udp  --  anywhere             anywhere             udp dpt:smtp ctstate NEW
ACCEPT     tcp  --  anywhere             anywhere             tcp dpt:smtp ctstate NEW
[root@VPS~]# iptables -L -n | grep 25
ACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            tcp dpt:25 ctstate NEW
ACCEPT     udp  --  0.0.0.0/0            0.0.0.0/0            udp dpt:25 ctstate NEW
ACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            tcp dpt:25 ctstate NEW 
</code></pre>

<p>I'm happy to provide any more information that may be relevant. Also, this is my first post to the site. I think this is on-topic, but I briefly debated Unix or Superuser as well. </p>

<p><strong>UPDATE</strong></p>

<p>I turned off the firewall:</p>

<pre><code>[root@domain~]# systemctl stop firewalld
[root@domain~]# systemctl status firewalld
* firewalld.service - firewalld - dynamic firewall daemon
   Loaded: loaded (/usr/lib/systemd/system/firewalld.service; enabled; vendor preset: enabled)
   Active: inactive (dead) since Mon 2016-06-13 01:52:47 EDT; 4s ago
  Process: 941 ExecStart=/usr/sbin/firewalld --nofork --nopid $FIREWALLD_ARGS (code=exited, status=0/SUCCESS)
 Main PID: 941 (code=exited, status=0/SUCCESS)

Jun 12 23:14:34 hostname systemd[1]: Starting firewalld - dynamic fire....
Jun 12 23:14:35 hostname systemd[1]: Started firewalld - dynamic firew....
Jun 12 23:14:35 hostname firewalld[941]: 2016-06-12 23:14:35 ERROR: eb....
Jun 13 01:52:47 hostname systemd[1]: Stopping firewalld - dynamic fire....
Jun 13 01:52:47 hostname systemd[1]: Stopped firewalld - dynamic firew....
Hint: Some lines were ellipsized, use -l to show in full.
</code></pre>

<p>I then checked for an <code>iptables</code> service</p>

<pre><code>[root@domain~]# systemctl status iptables
* iptables.service
   Loaded: not-found (Reason: No such file or directory)
   Active: inactive (dead)
[root@domain~]# service iptables status
Redirecting to /bin/systemctl status  iptables.service
* iptables.service
   Loaded: not-found (Reason: No such file or directory)
   Active: inactive (dead)
</code></pre>

<p>I still got the same error when trying to telnet to the VPS on port 25. </p>
","<email-server><exim><firewalld>","2016-06-13 05:05:16"
"783642","NetApp .Snapshots wont delete","<p>I am working in VMware Vspere client, I am an sys administrator taking over for an Admin who has left.  Many users were complaining about space in our Mounted Linux shared drive.  So I did some digging and found many .snapshot files which are rather large.  If I look in the VMware Vsphere snapshot manager, it shows zero snapshots. </p>

<p>Update, these .snapshots are on the Netapp.  How do I know which ones are safe to delete and which ones should I hold onto?  Is there a configuration on how to only hold one or two at a time?  I would not have this concern but my space is incredibility limited. </p>
","<redhat><vmware-vsphere><netapp><vmware-vma>","2016-06-13 13:36:22"
"925230","amanda backup and take","<p>I'm very new to amanda-backup and also linux <br/>
I'm looking for the amanda-backup setting </p>

<p><a href=""https://www.howtoforge.com/centralized-backup-server-with-amanda-on-centos"" rel=""nofollow noreferrer"">https://www.howtoforge.com/centralized-backup-server-with-amanda-on-centos</a>
<br/>
I saw the ""tape(?)"" setting in this tutorial <br/></p>

<blockquote>
  <ol start=""8"">
  <li>Create tape list:</li>
  </ol>
  
  <p>touch /etc/amanda/intra/tapelist  chown –R amanda:disk
  /etc/amanda/intra</p>
  
  <ol start=""9"">
  <li>Create slots (virtual tapes):</li>
  </ol>
  
  <p>su - amanda  cd /backup/intra/slots for ((i=1; $i&lt;=15; i++)); do mkdir
  slot$i; done  ln -s slot1 data</p>
  
  <ol start=""10"">
  <li>Test virtual tapes:</li>
  </ol>
  
  <p>/usr/sbin/ammt -f file:/backup/intra/slots status</p>
</blockquote>

<p>I guess my linux server does not have Tape thing</p>

<p>is take can be replace with file-server(NAS)??</p>

<p>if it is do I have to change the configuration of tape?</p>
","<backup><amanda>","2018-08-07 06:57:48"
"783702","forwarding emails via postfix, smart routine to actively check that it works","<p>I have a domain registered at <code>namecheap</code> and run my website on a <code>digitalocean</code> droplet.</p>

<p>I have successfully set up email forwarding on the server with <code>postfix</code>, such that emails sent to <code>name@example.com</code> are now being forwarded to another email account I own.</p>

<p>I followed <a href=""https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-postfix-as-a-send-only-smtp-server-on-ubuntu-14-04"" rel=""nofollow noreferrer"">this guide</a> for the initial <code>postfix</code> setup and then adjusted the configuration according to <a href=""https://www.andreagrandi.it/2014/08/31/getting-started-with-digital-ocean-vps-configuring-dns-and-postfix-for-email-forwarding/"" rel=""nofollow noreferrer"">this blog post</a> in order to activate the forwarding. Everything works fine as of now.</p>

<p>My question is: How can I automatically and actively check that everything will keep on working in the future, despite me maybe fiddling with the server or something like that. Any smart suggestions for setting up some automatic check (without having to send an email to myself every day)?</p>

<p>I want to be able to rule out that I can unknowingly make any changes to the server in such a way that I break the forwarding pipeline through the server.</p>

<p>Thanks for any help.</p>
","<linux><ubuntu><postfix><forwarding>","2016-06-13 18:30:04"
"1030263","How to achieve high availability for Active Directory LDAPS (Secure LDAP)","<p>We have around 50 applications currently configured with LDAP and we have around 20 Domain Controllers. As per the security best practice we have to migrate all these applications from LDAP to LDAPS.
Currently, all applications are connected using Domain's &quot;NETBIOS&quot; name so there no need to worry about high availability.</p>
<p>What is the best design approach to achieve high availability for LDAPS?</p>
<p>Prefer not to configure individual DC servers as LDAPS servers in the application.
Note: all the servers (DC and application servers) are enrolled in on-prem PKI.</p>
","<active-directory>","2020-08-15 18:57:09"
"783706","Configure RAID 1 on ThinkServer TS140","<p>I am setting up a server for a business, and one of the specifications that they have set is a RAID 1 configuration for the two drives in the machine (the OS will be installed on this mirror).  One of the mirrored drives will be in a hot swap bay, so that at the end of the day, he can take the bay drive out, put it in a safe, and put another drive in.  The two hot swappable drives will rotate each day.</p>

<p>I am trying to set this up on a Lenovo ThinkServer TS140, but have a two issues:</p>

<ol>
<li><p>When I start the server up, I do not get a prompt to configure a RAID array.  I had worked with another server of the exact same model, and would get a prompt to hit CTRL+I to enter the RAID configuration utility.  Do I need to install something first?</p></li>
<li><p>I was wondering if this would be possible using the mainboard RAID (or given the fact that the mainboard never prompts for such), if I need a controller card for this?</p></li>
</ol>

<p>The owner of the business says that his current server does this, so I'm going to head over there to examine the old one, but that won't be until 5PM.  I tried contacting Lenovo technical support, but 1) he insisted the option was there when it never prompted for such, and 2) also insisted that I needed Lenovo hard drives to make this work (with or without a controller card).</p>

<p>Thanks in advance.</p>

<p>2016.06.14 Edit:</p>

<p>It is exactly as he described it.  He has a FastTrak TX2300 and a RAID controller.  The drive bay and the internal drive both plug into the controller card, and he just yanks the drive at the end of the day.  I also asked how long he has been doing this, and he said ""Just about 10 years.""</p>

<p>As far as ""Taking out a disk out of a RAID array is no replacement for a proper backup"", I didn't say it was.  This is just what he wants to do.  I'm going to recommend he also use NovaBackup to an external hard drive, but this is one of his specifications.</p>
","<raid1><mirror><hotswap>","2016-06-13 18:41:07"
"1030320","Installation of apache on aws throwing an error","<p>When i try and run the command</p>
<pre><code>yum install -y httpd.x86_64
</code></pre>
<p>it throws the following error</p>
<pre><code>Loaded plugins: extras_suggestions, langpacks, priorities, update-motd
No package httpd.86_64 available.
Error: Nothing to do
</code></pre>
<p>What should i do to troubleshoot?</p>
","<amazon-web-services>","2020-08-16 13:53:59"
"996457","Apache web server won't display a webpage","<p>I've literally tried everything to get my web server broadcasted to the public but it's just not working. Every time I type my server IP in a browser like firefox it just gets stuck on ""Waiting for ipaddress..."" forever. </p>

<p>So here's what I did: </p>

<ul>
<li><p>I completely reinstalled the operating system. I am using Centos 7 &amp; Google Cloud</p></li>
<li><p>I installed httpd using the simple step by step guide as seen here:
<a href=""https://phoenixnap.com/kb/install-apache-on-centos-7"" rel=""nofollow noreferrer"">https://phoenixnap.com/kb/install-apache-on-centos-7</a></p></li>
<li>I verified that the server is listening on port 80 with the
command: </li>
</ul>

<p>netstat -anp | grep httpd.</p>

<pre><code>sh-4.2# netstat -anp | grep httpd
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      18025/httpd
unix  3      [ ]         STREAM     CONNECTED     23993    18025/httpd
</code></pre>

<p>At first it was listening on tcp6 so
I typed in the command nano /etc/httpd/conf/httpd.conf and changed ""Listen 80"" to ""Listen 0.0.0.0:80"" 
I still can't connect to my web server after restarting it. </p>

<p>I tried configuring virtual hosts by following the guide here: <a href=""https://support.rackspace.com/how-to/set-up-virtual-hosts-on-centos/"" rel=""nofollow noreferrer"">https://support.rackspace.com/how-to/set-up-virtual-hosts-on-centos/</a></p>

<p>I am not running IPtables.. I had no problems opening up port 80 in firewalld with the commands </p>

<pre><code>sudo firewall-cmd --add-service=http --permanent
sudo firewall-cmd --add-service=https --permanent
sudo firewall-cmd ––reload
</code></pre>

<p>I also tried</p>

<pre><code>sudo firewall-cmd ––permanent ––add-port=80/tcp
sudo firewall-cmd ––permanent ––add-port=443/tcp
sudo firewall-cmd ––reload
</code></pre>

<p>I then typed 
sudo firewall-cmd --list-all and saw that services http and https were listed</p>

<p>I still couldn't get a web page when I type the server ip in my browser so I completely <strong>disabled selinux and firewalld</strong>. It's still not working.</p>

<p>I installed IP tables and opened the necessary ports:</p>

<pre><code>-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT
-A INPUT -p icmp -j ACCEPT
-A INPUT -i lo -j ACCEPT
-A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT
-A INPUT -i eth0 -p tcp -m tcp --dport 80 -m state --state NEW,ESTABLISHED -j ACCEPT
-A INPUT -j REJECT --reject-with icmp-host-prohibited
-A FORWARD -j REJECT --reject-with icmp-host-prohibited
</code></pre>

<p>In the VPC Network -> Firewall Rules tab in Google Cloud you can clearly see that http is open for all ip ranges</p>

<pre><code>default-allow-http
Ingress
http-server
IP ranges: 0.0.0.0/0
tcp:80
Allow
1000
default
</code></pre>

<p>I used a curl command in the SSH console to test the website
curl -I <a href=""http://localhost"" rel=""nofollow noreferrer"">http://localhost</a></p>

<pre><code>HTTP/1.1 200 OK
Date: Sun, 22 Dec 2019 15:19:18 GMT
Server: Apache/2.4.6 (CentOS)
Last-Modified: Sun, 22 Dec 2019 14:54:44 GMT
ETag: ""5-59a4c176b0e2b""
Accept-Ranges: bytes
Content-Length: 5
Content-Type: text/html; charset=UTF-8
</code></pre>

<p>And it responded with 200 OK.. Meaning everything was configured correctly. I opened all the ports and still cannot get a page to display when I type the server IP in my firefox browser. I don't understand.... What am I doing wrong?</p>

<p>EDIT: 
Here is the error log</p>

<pre><code>[Sun Dec 22 16:11:14.246082 2019] [suexec:notice] [pid 1244] AH01232: suEXEC mechanism enabled (wrapper: /usr/sbin/suexec)
[Sun Dec 22 16:11:14.262897 2019] [lbmethod_heartbeat:notice] [pid 1244] AH02282: No slotmem from mod_heartmonitor
[Sun Dec 22 16:11:14.262979 2019] [ssl:warn] [pid 1244] AH01873: Init: Session Cache is not configured [hint: SSLSessionCache]
[Sun Dec 22 16:11:14.270177 2019] [mpm_prefork:notice] [pid 1244] AH00163: Apache/2.4.6 (CentOS) OpenSSL/1.0.2k-fips configured -- resuming normal operations
[Sun Dec 22 16:11:14.270214 2019] [core:notice] [pid 1244] AH00094: Command line: '/usr/sbin/httpd -D FOREGROUND'
</code></pre>
","<centos><httpd>","2019-12-22 15:39:11"
"783794","Power consumption PDU","<p>As you can see in the image below, I can't get the consumption.
Even if I send the log via FTP there's no info about the consumption.</p>

<p><a href=""https://i.sstatic.net/cETWT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cETWT.png"" alt=""PDU interface""></a></p>

<p>I already checked on APC PDU website and co and i didn't found any info.</p>

<pre><code>Model Number: AP7821 

Application Module 
Version: v3.7.0 
Date: 01/13/2009 


APC OS (AOS) 
Version: v3.7.0 
Date: 01/13/2009
</code></pre>

<hr>

<p><strong>Update</strong></p>

<p>This is an exemple of another PDU i have. In red you can see the Kwh consumption. This is what i need.</p>

<p><a href=""https://i.sstatic.net/NHUWw.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/NHUWw.png"" alt=""enter image description here""></a></p>
","<electrical-power><ups><apc>","2016-06-14 08:14:30"
"925342","Can not NAT from IP routed via VPN","<p>I have a linux server running Centos7 that uses a VPN to route to a public IP homed in a remote network. I want to NAT from an IP address on that remote network to a local subnet over a VPN connection using PPP devices. When my tunnels come up, I can ping the 203.0.113.5 IP address from the public internet, but I can not seem to get the NAT statements to work correctly.</p>

<p>Map:</p>

<pre><code>203.0.113.1              ROUTER TO INTERNET
  |
  |eth0                  VPN Server Public Interface
-----------------------
|VPN Server           |
-----------------------
203.0.113.2              VPN Public IP
  ||ppp0                 Tunnel Virtual Device
  ||10.0.0.1/24          Tunnel Gateway IP
  ||
  ||
  ||10.0.0.10/24         Tunnel Client IP
  ||ppp0                 Tunnel Client Device
198.51.100.129                  Broadband Public IP
   |eth0                 Public Interface
-----------------------
|THIS SERVER          |  
-----------------------
eth1|         |eth2
203.0.113.5/32   192.168.1.1/24
</code></pre>

<p>203.0.113.5 is a public IP. 192.168.1.1/24 is the private network. The VPN comes up with 10.0.0.0/24 network, and routing 203.0.113.5 via the Tunnel Client IP (and 203.0.113.0/30 via 10.0.0.1, default via 203.0.113.1) lets me ping 10.0.1.5 from the public internet. That is working, and I have confirmed it is THIS SERVER that is responding. I want to nat out the VPN,  using the public IP 203.0.113.5. This doesn't seem, to work. I have tried the following NAT statements:</p>

<pre><code>  iptables -t nat -A POSTROUTING -s 192.168.1.1/24 -o ppp0 -j MASQUERADE --to-source 203.0.113.5
  iptables -t nat -A POSTROUTING -s 192.168.1.1/24 -o ppp0 -j SNAT --to-source 203.0.113.5
</code></pre>

<p>I do not see and communication, I can not perform a name lookup, or a ping from machines in the 192.168.1.1 network.</p>

<p>Any advice would be appreciated</p>
","<vpn><routing><linux-networking><centos7><nat>","2018-08-07 18:33:28"
"855759","How to execute shell commands as a different user with only sudo password?","<p>I'd like to test a Webhook and is there a way how one can execute a shell command as a different user with sudo?</p>

<p>As root I could do something like:</p>

<pre><code>su - www-data -c 'cd /var/www/xml/tei_staged &amp;&amp; git pull origin master 2&gt;&amp;1'
</code></pre>

<p>With </p>

<pre><code>sudo -u www-data 'cd /var/www/xml/tei_staged &amp;&amp; git pull origin master 2&gt;&amp;1'
</code></pre>

<p>I get an error: </p>

<blockquote>
  <p>sudo: cd /var/www/xml/tei_staged &amp;&amp; git pull origin master 2>&amp;1:
  command not found</p>
</blockquote>
","<bash>","2017-06-14 13:35:30"
"1030666","How to obtain or accept certificates from a site","<p>I have installed Firefox and Chrome on an Ubuntu 18 machine, I'm not a regular 'nix user so I'm a bit lost as to why neither browser is accepting the SSL certificate offered at <a href=""http://www.teamviewer.com"" rel=""nofollow noreferrer"">www.teamviewer.com</a>. The site redirects me to an SSL site, of which I'm not terribly sure at any rate is honest, but I have to install their remoting host application to verify something. I found this old thread <a href=""https://serverfault.com/questions/139728/how-to-download-the-ssl-certificate-from-a-website"">how to download the ssl certificate from a website?</a> which no longer works for me, the line
<code>echo -n | openssl s_client -connect www.google.com | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p'</code> times out, or in the teamviewer site case the site refuses connection. That post seems like a wrong route?</p>
<p>This is probably a stupid question, but I have to start somewhere, I really know very little about how to frame the question, would appreciate alternate ways to access the/any website safely.</p>
<p>/edit
<code>echo -n | openssl s_client -connect www.google.com | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p'</code>
output is:</p>
<pre><code>140014374306240:error:2008A067:BIO routines:BIO_connect:connect error:../crypto/bio/b_sock2.c:111:
140014374306240:error:0200206F:system library:connect:Connection refused:../crypto/bio/b_sock2.c:110:
140014374306240:error:2008A067:BIO routines:BIO_connect:connect error:../crypto/bio/b_sock2.c:111:
connect:errno=111
</code></pre>
<p>Now since my usual computer is on a VPN, I was not eliminating the one big possibility and that is that my ISP is blocking the site, because if I type <a href=""http://www.teamviewer.com"" rel=""nofollow noreferrer"">www.teamviewer.com</a>, my ISP displays a warning redirect, which was not happening when I followed a link directly. So the issue is that my ISP does not trust the site, something I can actually tweak myself. And both web browsers were showing a correct although cryptic error message which I was missinterpreting:</p>
<pre><code>
An error occurred during a connection to www.teamviewer.com. PR_END_OF_FILE_ERROR

    The page you are trying to view cannot be shown because the authenticity of the received data could not be verified.
    Please contact the web site owners to inform them of this problem.
</code></pre>
","<ubuntu><ssl-certificate>","2020-08-18 18:50:37"
"784113","MongoDB, How can I migrate database to new cluster with higher version without an outage","<p>We are having our mongodb cluster at AWS and our production DB mongo is very old - we are still using version 2.6 then latest version at the moment is 3.2.7. We need to upgrade mongodb binaries to the latest version.
We can't upgrade existing DB cluster right in place because of versions incompatibility, new storage engine and ITSec requirements to encrypt EBS volumes. 
That means we need to setup new DB cluster v.3.2 and somehow migrate existing data on it. We have to do this without an outage because DB outage means that whole production delivery stack will be down.</p>

<p>Any idea how I can do it without an outage ?</p>

<p>Thanks</p>
","<amazon-web-services><cluster><migration><mongodb>","2016-06-15 13:04:00"
"1030800","Deploying new version of an application to IIS Application without downtime","<p>I have a small AngularJS web application that runs in kiosks in an intranet network. I've been tasked to make deployments without downtime and in an automated way that can be controlled remotely with a web interface. Since it's a small application, we do not have load balancers or similar setups. I've looked at two options,</p>
<p><strong>Option 1:</strong>
Download versions of the application in a folder and change the Virtual Directory's Physical Path to the folder path of a new version, this is achievable using Microsoft.Web.Administration library with the following code,</p>
<pre><code>            ServerManager sm = new ServerManager();
            Site site = sm.Sites[&quot;Default Web Site&quot;];            
            site.Applications[virtualDirectory].VirtualDirectories[&quot;/&quot;].PhysicalPath = newPath;
            sm.CommitChanges();
</code></pre>
<p>In this approach,</p>
<ol>
<li>How quickly will the switch be reflected?</li>
<li>What are the chances that some of the kiosks still working with the older version of the application?</li>
<li>Is there any chance for a downtime in between the short period of time the switch is made?</li>
<li>Will any requests made to IIS in this period time return errors?</li>
</ol>
<p><strong>Option 2:</strong> Using IIS' URL Rewrite feature. Basically I'll be programmatically adding new rewrite rules to the application's web.config to redirect to a different subfolder which will be the newer version of the application.</p>
<p>In this approach,</p>
<ol>
<li>How quickly will the URL rewrite rule reflect and the rewrite begin?</li>
<li>What are the chances that some of the kiosks still working with the older version of the application</li>
<li>Is there any chance for a downtime in between the period of time when the rewrite rule is added to the web.config?</li>
<li>Will any requests made to IIS in this period time return errors?</li>
<li>Will there be any performance cost for the URL rewrite?</li>
</ol>
<p>Kindly help me out. Are both of these approaches not suited for my task? Are there any other alternatives to this requirement?</p>
","<iis><web-server><rewrite><deployment><web.config>","2020-08-19 19:25:51"
"926141","Firewall refuse my connection to DB","<p>We are working on software that require to connect to database on the server (Win2012)(LAN), after activating windows firewall, we couldn't connect to DB.</p>

<p>I've set a rule for inbound and I Excluded our ports (default 1443 and Software port) and it doesn't work. then I set a new rule just to make sure that Excluding ports is not our problem so I excluded All the ports (Allow for all ports) and still can't connect to DB.</p>

<p>Only when deactivate firewall, everything works fine!! so where is the problem?</p>

<p><a href=""https://i.sstatic.net/IUd7f.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IUd7f.png"" alt=""enter image description here""></a></p>
","<firewall><windows-server-2012-r2><sql-server-2008><connection-refused>","2018-08-13 12:04:39"
"856602","How to change Tcp timestamp value?","<p>The question is straightforward and simple.</p>

<p>I want to know how to change the value of tcp timestamps manually in windows. I know how to disable them but i want to know the process of changing tcp timestamps value field to values that i want them to show. How can i achieve this.</p>

<p>Can we alter these timestamp values? I have read somehwere that it can be done and requires a detailed knowledge of how the kernel interprets system time and how this system time is reflected in the default OS generated fields in packet headers. </p>
","<tcp><timestamp>","2017-06-19 11:17:29"
"997307","Postfixadmin can't connect to mysql database","<p>postfixadmin can't connect to mysql database.</p>

<p>Error: Can't connect to database
Please edit the $CONF['database_*'] parameters in config.local.php.</p>

<p>DEBUG INFORMATION:
Connect: Access denied for user 'postfixadmin'@'localhost' (using password: YES)</p>

<p>in config.local.php everything seems to be fine and using</p>

<pre><code>mysql -u postfixadmin -p
</code></pre>

<p>i can easly connect to this database using the same password.</p>

<p>this is my config.local.php</p>

<pre><code>
$CONF['database_type'] = 'mysqli';
$CONF['database_host'] = 'localhost';
$CONF['database_user'] = 'postfixadmin';
$CONF['database_password'] = 'XXXXXXXX';
$CONF['database_name'] = 'postfixadmin';

$CONF['database_use_ssl'] = false;
$CONF['database_ssl_key'] = NULL;
$CONF['database_ssl_cert'] = NULL;
$CONF['database_ssl_ca'] = NULL;
$CONF['database_ssl_ca_path'] = NULL;
$CONF['database_ssl_cipher'] = NULL;
$CONF['database_prefix'] = '';
</code></pre>

<p>im using raspban 10 buster on raspberrypi 4B and i installed everything directly under the host</p>

<p>in /var/log/nginx/postfixadmin_error.log i have this</p>

<p>2019/12/29 23:04:05 [error] 12968#12968: *1 FastCGI sent in stderr: ""PHP message: ERROR: directory /usr/share/postfixadmin/lib/../templates_c doesn't exist or isn't writeable for the webserver"" while reading response header from upstream, client: 8.8.8.8, server: mail.example.com, request: ""GET /login.php HTTP/2.0"", upstream: ""fastcgi://unix:/run/php/php7.3-fpm.sock:"", host: ""mail.example.com"", referrer: ""<a href=""https://mail.example.com/setup.php"" rel=""nofollow noreferrer"">https://mail.example.com/setup.php</a></p>

<p>i replaced my ip with 8.8.8.8 and my domain name with example.com</p>

<p>i created folder /usr/share/postfixadmin/lib/templates_c</p>

<p>this is permissions
drwxr-xr-x  2 www-data www-data  4096 Jan  2 10:34 templates_c. then i restarted nginx and postfix services, but nothing worked.</p>

<p>in /var/log/mail.err i also get this error</p>

<p>Jan  2 10:38:46 kontakt postfix/proxymap[17706]: error: open /etc/postfix/sql/mysql_virtual_domains_maps.cf: No such file or directory</p>

<p>i would be very grateful for any help.</p>
","<mysql><postfix>","2020-01-01 21:20:12"
"1031213","Mastercard not being accepted on GCP","<p>I just started to use Google Cloud platform for Kubernetes and I am trying to confirm my MasterCard details in order to get started with the free-trial but it gets rejected every time.</p>
<p>I tried contacting google support and searching online for similar issues and solutions but all to no avail.</p>
<p>Please how do I go around this?</p>
","<kubernetes>","2020-08-24 03:54:21"
"997361","Bitnami Weblate on AWS Lightsail","<p>Is it possible to use this Bitnami image in a Lightsail instance instead of of a ec2?
<a href=""https://aws.amazon.com/marketplace/pp/Bitnami-Weblate-Certified-by-Bitnami/B00NN8X6U2"" rel=""nofollow noreferrer"">https://aws.amazon.com/marketplace/pp/Bitnami-Weblate-Certified-by-Bitnami/B00NN8X6U2</a></p>

<p>We will use this not often and want to save money.</p>

<p>Weblate webiste: <a href=""https://weblate.org/de/"" rel=""nofollow noreferrer"">https://weblate.org/de/</a></p>
","<amazon-web-services><amazon-ec2><bitnami><amazon-lightsail>","2020-01-02 13:03:09"
"926391","Unknown A Record in dig results","<p>I'm very new to running my own webserver and have got a long way just reading, but i'm stuck now. I'm trying to point a subdomain to my own server and am coming up stuck.</p>

<p>I am hosting a static front end with netlify and have the followi<a href=""https://i.sstatic.net/5LPwp.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5LPwp.png"" alt=""enter image description here""></a></p>

<p>when i run <code>dig api.candmelectricalltd.co.uk</code></p>

<p>i am seeing </p>

<pre><code>;; QUESTION SECTION:
;api.candmelectricalltd.co.uk.  IN  A

;; ANSWER SECTION:
api.candmelectricalltd.co.uk. 2631 IN   CNAME   18.216.122.25.
18.216.122.25.      0   IN  A   92.242.132.24
</code></pre>

<p>It looks to me like there is something pointing traffic away from my server but i have no idea what.</p>

<p>The registrar is reg-123 but i am using netlifiy's nameservers</p>

<p>If anyone has any idea where to look that would be great as i'm stumped</p>

<p>it looks like 92.242.132.24 is some sort of service that changes dns errors into advertising, so i'm not really sure what i do from here</p>

<p>thanks</p>

<p>--------edit----------</p>

<p>Okay so i removed the cname record as that was incorrect, now the output of dig is:</p>

<pre><code>; &lt;&lt;&gt;&gt; DiG 9.8.3-P1 &lt;&lt;&gt;&gt; api.candmelectricalltd.co.uk
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NXDOMAIN, id: 64766
;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 0

;; QUESTION SECTION:
;api.candmelectricalltd.co.uk.  IN  A

;; AUTHORITY SECTION:
candmelectricalltd.co.uk. 2987  IN  SOA dns1.p01.nsone.net. hostmaster.nsone.net. 1534273483 43200 7200 1209600 3600

;; Query time: 13 msec
;; SERVER: 194.168.4.100#53(194.168.4.100)
;; WHEN: Tue Aug 14 21:34:27 2018
;; MSG SIZE  rcvd: 111
</code></pre>
","<domain-name-system>","2018-08-14 19:30:23"
"926409","Are there any VMware ESXi virtual memory limits?","<p>When using vSphere 6.0 Enterprise Plus Edition, are there any memory limits (virtual RAM per virtual machine)? I am refering to the vRAM entitlements.</p>
","<vmware-esxi><vmware-vsphere>","2018-08-14 22:44:02"
"926436","How is HTTPS secure if Private Key is able to be intercepted?","<p>I'm not a networking guy, so I have little knowledge about internet protocols and SSL. My question is coming from a lack of understanding of how https operates.</p>

<p>Basically in https there is a phase where the Encryption algorithm and Private Key to decrypt any encrypted information that may be coming over the network is agreed upon the client and server, however this is all to create a safe and secure connection. But if the Private Key is send before there is a secure connection so what will prevent hackers intercepting this Private Key and thereby also intercepting any https requests that are encrypted but being that this hacker has the private key the data can easily be decrypted, so where is the security?</p>

<p>This always bothered me, thank you for all your answers!</p>
","<ssl><https><http>","2018-08-15 04:56:58"
"856784","Google Compute Engine >>unexpected (unknown) user making continues request to our website (home page)","<p>we have hosted our website (server, Linux Debian ) on Google Compute Engine instance, the problem is that, from last 5 days, someone (unknown ) unexpected user making continually request to our website,(home page)  that's why, our back-end server load is increasing, and unexpected data download is increasing. we saw our server log, continually request is coming to our server. </p>

<p>for that we tried to reach request point from where these continues request is making, but we are not able to find. Kindly suggest us, how to stop that continuous requests. </p>

<p>Is there any service to protect our compute engine instance from unknown users?</p>

<p>Please find below snippet is out server log.same request making continually</p>

<pre><code>GET /api/beta/home/new/image?&amp;key=7xOyNH554tY83cBN7Ktpw3s1y68ql6Eg 200 150.102 ms - 69746
sjhfjhf
GET /api/beta/article/new/home/assets?key=7xOyNH554tY83cBN7Ktpw3s1y68ql6Eg 200 217.073 ms - 343825
GET /api/beta/article?key=7xOyNH554tY83cBN7Ktpw3s1y68ql6Eg&amp;top_category=true 200 25.765 ms - 11966
GET /api/beta/home/new/image?&amp;key=7xOyNH554tY83cBN7Ktpw3s1y68ql6Eg 200 60.548 ms - 69746
sjhfjhf
GET /api/beta/article/new/home/assets?key=7xOyNH554tY83cBN7Ktpw3s1y68ql6Eg 200 106.435 ms - 343827
GET /api/beta/article?key=7xOyNH554tY83cBN7Ktpw3s1y68ql6Eg&amp;top_category=true 200 27.907 ms - 11965
GET /api/beta/home/new/image?&amp;key=7xOyNH554tY83cBN7Ktpw3s1y68ql6Eg 200 54.128 ms - 69746
sjhfjhf
GET /api/beta/article/new/home/assets?key=7xOyNH554tY83cBN7Ktpw3s1y68ql6Eg 200 86.679 ms - 343824
GET /api/beta/article?key=7xOyNH554tY83cBN7Ktpw3s1y68ql6Eg&amp;top_category=true 200 33.595 ms - 11966
GET /api/beta/home/new/image?&amp;key=7xOyNH554tY83cBN7Ktpw3s1y68ql6Eg 200 87.592 ms - 69746
sjhfjhf
GET /api/beta/article/new/home/assets?key=7xOyNH554tY83cBN7Ktpw3s1y68ql6Eg 200 127.591 ms - 343824
GET /api/beta/article?key=7xOyNH554tY83cBN7Ktpw3s1y68ql6Eg&amp;top_category=true 200 29.016 ms - 11966
GET /api/beta/home/new/image?&amp;key=7xOyNH554tY83cBN7Ktpw3s1y68ql6Eg 200 69.063 ms - 69746
sjhfjhf
GET /api/beta/article/new/home/assets?key=7xOyNH554tY83cBN7Ktpw3s1y68ql6Eg 200 113.770 ms - 343826
GET /api/beta/article?key=7xOyNH554tY83cBN7Ktpw3s1y68ql6Eg&amp;top_category=true 200 20.781 ms - 11965
GET /api/beta/home/new/image?&amp;key=7xOyNH554tY83cBN7Ktpw3s1y68ql6Eg 200 53.646 ms - 69746
sjhfjhf
GET /api/beta/article/new/home/assets?key=7xOyNH554tY83cBN7Ktpw3s1y68ql6Eg 200 82.988 ms - 343825
GET /api/beta/article/dr-n-n-khanna-senior-interventional-cardiologist-apollo-hospital-new-delhi-coronary-artery-disease-compounded-by-peripheral-vascular-disease?limit=1&amp;skip=0&amp;compact=&amp;category=&amp;event=&amp;key=6ZzQ52peX5XqUx3t824670wv8jIaf1B4 200 5.652 ms - 2967
GET /api/beta/article/dr-n-n-khanna-senior-interventional-cardiologist-apollo-hospital-new-delhi-coronary-artery-disease-compounded-by-peripheral-vascular-disease/related?limit=4&amp;skip=0&amp;compact=1&amp;category=&amp;event=&amp;key=6ZzQ52peX5XqUx3t824670wv8jIaf1B4 200 14.006 ms - 6601
GET /api/beta/article?key=7xOyNH554tY83cBN7Ktpw3s1y68ql6Eg&amp;top_category=true 200 28.000 ms - 11966
GET /api/beta/home/new/image?&amp;key=7xOyNH554tY83cBN7Ktpw3s1y68ql6Eg 200 76.620 ms - 69746
sjhfjhf
</code></pre>
","<linux><security><google-compute-engine>","2017-06-20 10:40:44"
"997492","Backup of multiple sites on Ubuntu","<p>I have multiple sites in my /var/www/html/ folder like following:-</p>

<pre><code>/var/www/html/site-1 
/var/www/html/site-2 
/var/www/html/site-3
/var/www/html/site-4
</code></pre>

<p>i want to zip each site on my server and to sync on s3 bucket.</p>
","<ubuntu><backup><amazon-s3><shell-scripting><zip>","2020-01-03 12:55:24"
"856803","How could i get a one line output into a mutliple line one in powershell?","<p>This is my command which i am using :</p>

<pre><code>PS U:\&gt; Get-ADUser paul -properties memberof | fl memberof
</code></pre>

<p>and my output looks like this: </p>

<pre><code>memberof : {CN=WLAN-Allow,OU=Groups,OU=Users,OU=at,....}
</code></pre>

<p>So my question is how do i format my single line output into something which should like like a list:</p>

<pre><code>memeberof :

 - CN=WLAN-Allow
 - OU=Groups
 - OU=Users
 - OU=at*
</code></pre>
","<powershell>","2017-06-20 12:07:04"
"1031418","Lost all login details after the death of administrator of a website hosted in GCP","<p>My customer's administrator recently died and now my customer has no login data, neither for the backend of his content-management-system nor the administration system of his provider. He only has an FTP-Login left.</p>
<p>Via hostadvice.com I found out that his website is hosted on Google Cloud Platform.</p>
<p>Is there a way to retrieve the login data when both the login and the password are missing?</p>
<p>What can I do or what must my customer do to get the control of his webspace back?</p>
","<google-cloud-platform><password><password-recovery>","2020-08-25 13:23:45"
"856837","Two static IPs resolving to the same Apache directory","<p>I currently have a dedicated server that has two static IPs pointing to it, and two domain names pointing to their respective static IPs. Both of those domains have virtual hosts, and both of those domains resolve to the correct Apache directory. Let's call them domain1 (Associated to IP1 through DNS) and domain2(Associated to IP2 through DNS). The hostname is set to the fully qualified domain name (www.domain1.com)</p>

<p>The problem is, if I attempt to navigate through either IPs on a browser, it always refers me to domain1. I would like this to not be the case. I would like direct IPs to either resolve to an error, or to resolve to their specific domains. What am I doing wrong?</p>

<p>EDIT: As asked by a commenter, here are both of the virtual host files. 000-default.conf is empty.</p>

<p>domain1.com.conf, note that this one is a Symfony installation and runs without www:</p>

<pre><code>&lt;VirtualHost *:80&gt;
    ServerName domain1.com
    ServerAlias domain1.com

    DocumentRoot /var/www/html/domain1/web
    &lt;Directory /var/www/html/domain1/web&gt;
        AllowOverride All
        Order Allow,Deny
        Allow from All
    &lt;/Directory&gt;

    # uncomment the following lines if you install assets as symlinks
    # or run into problems when compiling LESS/Sass/CoffeeScript assets
    # &lt;Directory /var/www/html/domain1&gt;
    #     Options FollowSymlinks
    # &lt;/Directory&gt;


    ErrorLog /var/log/apache2/project_error.log
    CustomLog /var/log/apache2/project_access.log combined
&lt;/VirtualHost&gt;

&lt;VirtualHost *:443&gt;
    ServerName domain1.com
    ServerAlias domain1.com

    SSLEngine on

    SSLCertificateFile ""/etc/ssl/certs/domain1_com.pem""

    SSLCertificateKeyFile ""/etc/ssl/private/domain1.key""

    SSLCACertificateFile ""/etc/ssl/certs/domain1_cert.pem""

    DocumentRoot /var/www/html/domain1/web
    &lt;Directory /var/www/html/domain1/web&gt;
        AllowOverride All
        Order Allow,Deny
        Allow from All
    &lt;/Directory&gt;

    # uncomment the following lines if you install assets as symlinks
    # or run into problems when compiling LESS/Sass/CoffeeScript assets
    # &lt;Directory /var/www/html/domain1&gt;
    #     Options FollowSymlinks
    # &lt;/Directory&gt;


    ErrorLog /var/log/apache2/project_error.log
    CustomLog /var/log/apache2/project_access.log combined
&lt;/VirtualHost&gt;
</code></pre>

<p>www.domain2.conf, this one doesn't have a ssl certificate yet, so I only put in the virtual host for port 80;</p>

<pre><code>&lt;VirtualHost *:80&gt;
    ServerName domain2.com
    ServerAlias www.domain2.com

    DocumentRoot /var/www/html/domain2
    &lt;Directory /var/www/html/domain2&gt;
        AllowOverride All
        Order Allow,Deny
        Allow from All
    &lt;/Directory&gt;

    # uncomment the following lines if you install assets as symlinks
    # or run into problems when compiling LESS/Sass/CoffeeScript assets
    # &lt;Directory /var/www/html/domain2&gt;
    #     Options FollowSymlinks
    # &lt;/Directory&gt;


    ErrorLog /var/log/apache2/domain2.log
    CustomLog /var/log/apache2/domain2.log combined
&lt;/VirtualHost&gt;
</code></pre>
","<ubuntu><apache-2.4><domain><ubuntu-16.04>","2017-06-20 13:54:58"
"1031446","Forward SSH from Reverse Proxy Server through VPN to Home Server","<p>I'm trying to set up GitLab on my home server. HTTPS is working and I can get to GitLab's interface, but SSH is not and thus I can't push code to the server.</p>
<p>Here is the setup:</p>
<pre><code>Cloudflare &lt;--&gt; Reverse Proxy (nginx, hosted on Digital Ocean) &lt;--- VPN ---&gt; Untangle Firewall &lt;--&gt; GitLab Server (on ESXi)
</code></pre>
<p>If I try to SSH directly from the Reverse Proxy to the GitLab server (over VPN connection), it works perfect.</p>
<p>If I try to SSH from my laptop using the domain name, I get:</p>
<pre><code>kex_exchange_identification: Connection closed by remote host
Connection closed by 104.31.73.156 port 2095
</code></pre>
<p>If I try to SSH from my laptop using the Reverse Proxy's IP (thus cutting out Cloudflare), I get:</p>
<pre><code>Bad packet length 1231976033.
ssh_dispatch_run_fatal: Connection to {{ IP }} port 2095: message authentication code incorrect
</code></pre>
<p>I'm currently trying to use the nginx stream module to do so, and this is the stream setup:</p>
<pre><code>stream {
        upstream git-ssh {
                server {{INTERNAL GITLAB IP}}:22;
        }
        server {
                listen 2095;
                proxy_pass {{INTERNAL GITLAB IP}}:22;
                proxy_protocol on;
        }
}
</code></pre>
<p>The reason I have upstream git-ssh and then don't use it was because I was wondering if that was the problem, but it makes no difference if I use it or not.</p>
<p>I'm not familiar with iptables, but I tried the following commands:</p>
<pre><code>sudo iptables -t nat -A PREROUTING -i eth0 -p tcp --dport 2095 -j DNAT --to-destination {{GITLAB IP}}:22
sudo iptables -t nat -A POSTROUTING -o eth0 -p tcp --dport 2095 -j SNAT --to-source {{PROXY IP}}
</code></pre>
<p>But it didn't seem to work. ssh just sits there returning nothing and eventually times out.</p>
<p>UFW is currently set to disabled on the proxy for testing, so nothing is going on there. 2095 is an approved port for Cloudflare (<a href=""https://support.cloudflare.com/hc/en-us/articles/200169156-Identifying-network-ports-compatible-with-Cloudflare-s-proxy"" rel=""nofollow noreferrer"">https://support.cloudflare.com/hc/en-us/articles/200169156-Identifying-network-ports-compatible-with-Cloudflare-s-proxy</a>), so that shouldn't be it. SSH is allowed through the GitLab firewall, so that shouldn't be it. I don't have the Firewall set up in DigitalOcean yet, so that shouldn't be it.</p>
<p>I am lost now, and was hoping someone could give me pointers?</p>
","<nginx><ssh><vpn><reverse-proxy><cloudflare>","2020-08-25 15:32:28"
"784871","No space left on device but enough space and enough inodes","<p>My system (openSUSE 42.1 64bit btrfs) says that there would be no space left on device, after doing a reboot. Some days ago I did system updates with yast. 3 of them (multimedia stuff) could not be installed so I chanceld and started teh update again, all did go well. On the next day I did reboot the system. Today I tried to do updates via ssh and discovered this error. </p>

<p>The system can do ssh to other systems but it is not pingable. Even doing a snapper rollback is impossible because there is no space left on device. I checked df -h and df -i but there are enough space and inodes left. What could be the problem? What could I check?</p>

<p><a href=""https://i.sstatic.net/gpi8b.jpg"" rel=""nofollow noreferrer"">screenshot of system error and df -h and df -i</a> (Sorry for the poor quality)</p>
","<linux>","2016-06-19 11:44:05"
"784928","linux + timeout from command","<p>in my bash script I want to add timeout for the lvresize command
so if the command is waiting , </p>

<p>I will exit after 5 second , as the follwing:</p>

<pre><code> timeout 5  lvresize -L 1M /dev/mapper/rootvg-home

 Rounding size to boundary between physical extents: 32.00 MiB
 WARNING: Reducing active and open logical volume to 32.00 MiB
 THIS MAY DESTROY YOUR DATA (filesystem etc.)
 Do you really want to reduce home? [y/n]:
</code></pre>

<p>but from some unclear reason timeout not works on lvresize?</p>

<p>why? timeout not exit the command after 5 sec?</p>
","<linux><redhat><lvm><timeout>","2016-06-19 22:06:51"
"856890","FTP sites can't be accessed from GUI programs, only command-line","<p>We have a Windows Server 2012 R2 box, running IIS 6.2 (Build 9200). 
This is a Rackspace server. </p>

<p>The server was running one FTP site, with no issues. 
We had worked with Rackspace originally to get the right ports opened in their networking environment, and get the server configured to match, and all was good. We were able to connect to the FTP server via command-line, or using GUI tools such as FireFTP, WinSCP, etc. </p>

<p>Now we needed to add a second FTP site to the server for another client.
I've set up the second site, got users created, authorization rules set, the whole 9. </p>

<p>Since adding the second site though, graphical tools no longer connect. I can connect to both sites fine via the Windows command-line FTP. It can log in, connect, list directories, put files and get files, all without issue.</p>

<p>Graphical tools all can't get past connecting though. They authenticate fine, connect, and then time out on trying to list directory contents (messages vary depending on client, FireFTP, Windows Explorer, et al).</p>

<p>Most posts say that this is one of two things, 1) the firewall settings, or 2) the Active/Passive mode. </p>

<p>The timeout issue in graphical tools is occurring when operating in passive mode. If I switch them to active mode they have different issues. FireFTP keeps getting kicked off after issuing the command <code>PORT |2|fe80::cc7e:88c2:277f:f3f6|1038|</code>, getting stuck in an ""attempting to reconnect"" loop. Windows Explorer just hangs itself into oblivion.</p>

<p>The firewall settings should be correct, since as previously stated, we'd already worked on this and got it working when it was just one site being hosted. In the IIS server-level <strong>FTP Firewall Support</strong> section, the <code>Data Control Port Range</code> is set to 2000-2100 and the <code>External IP Address of Firewall</code> is filled in.</p>

<p>The sites are just plain normal FTP, not SFTP at all.<br>
They're both using IIS Manager accounts for login with user-isolation.<br>
There is no request filtering set up, other than the default for <code>_vti_bin</code>.</p>

<p>Both sites are running on the same IP address using virtual hosts.</p>

<p>Any ideas or directions to look? </p>
","<ftp><graphical-user-interface><connections>","2017-06-20 17:52:36"
"784935","How do I enable certain Availability Zones in AWS EC2?","<p>I am trying to launch a t2.nano instance running Debian in the <code>us-east-1a</code> region, but Amazon is only allowing me to choose from between <code>us-west-2a</code>, <code>us-west-2b</code> and <code>us-west-2c</code> which are all in Oregon and too far away for my particular project.</p>

<p>Why can't I choose a <code>us-east-1a</code>, or even a <code>ap-northeast-2</code> availability for that matter?</p>

<p>Why are there only 3 options available to me?</p>

<p>I just launched a t2.nano instance on us-east-1a last week, so I know it's possible.</p>
","<amazon-ec2><amazon-web-services>","2016-06-19 23:49:35"
"784952","Pointing namecheap.com domain name to www.ovh.co.uk server","<p>I have bought vps hosting plan-3 from ovh.com. My ip is 158.69.219.147 or server vps85119.vps.ovh.ca
I have installed my site  on ovh, but my real domain name is www.free-sample-loop.com(typo corrected) from name cheap. I have added A recod on name cheap with IP 158.69.219.147 and vps85119.vps.ovh.ca but when I visit www.free-sample-loop.com (typo corrected) it takes me to 158.69.219.147 or server vps85119.vps.ovh.ca. How can I configure to get website on real URl(www.free-sample-loop.com [typo corrected]). </p>
","<ip><domain><hosting><nameserver><ovh>","2016-06-20 06:05:57"
"784962","I want to reboot the system using puppet by using reboot resource but it is throwing error like ""Invalid resource type: Reboot""?.""","<p>I want to reboot the system using puppet by using reboot resource but it is throwing error like ""Invalid resource type: Reboot""?.</p>

<p>Is there any need modules that I need to install??. please help me.</p>
","<puppet><puppet-agent>","2016-06-20 07:37:06"
"926686","Centos 7 Move storage space from one filesystem to another","<p>I have a server using Centos 7, and with the storage capacity allocated as such:</p>

<p><a href=""https://i.sstatic.net/Jz8KK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Jz8KK.png"" alt=""My free space and mounts""></a></p>

<p>What I would like to do is take some of the space from the <strong>/dev/mapper/centos-home</strong> file system and give it to <strong>/dev/mapper/centos-root</strong>.</p>

<p>Now, I am aware I have to unmount the /home and remount it somehow at a smaller size. Then reallocate the free space. But, will this cause issues, as in being able to login and such? </p>

<p>I know you won't be able to unmount /home on el7 without turning off some services, like cups. </p>

<p>Also, if it is a XFS filesystem I know you cannot shrink it. So, how would I go about it for a XFS system?</p>

<p>As per Sven's request the command <code>blkid /dev/mapper/centos-home</code> returned nothing.</p>
","<centos><filesystems><xfs>","2018-08-16 12:21:59"
"857041","How do I make FilesMatch NOT blocking certain things?","<p>FilesMatch is blocking certain things it should not. I have the following config:</p>

<pre><code>&lt;FilesMatch ""^.*$""&gt;
Deny from all
&lt;/FilesMatch&gt;

&lt;FilesMatch "".+\.(html?|cgi|shtml|phtml|js|php|css|[Jj][Pp][Ee]?[Gg]|[Gg][Ii][Ff]|[Pp][Nn][Gg]|ico|xml|wsdl)$""&gt;
Order deny,allow
Allow from all
Satisfy Any
&lt;/FilesMatch&gt;
</code></pre>

<p>But when I use this, I cannot access:</p>

<pre><code>www.example.com/example/
www.example.com/example/?hello
</code></pre>

<p>I thought FilesMatch only operated on files. What am I doing wrong?</p>

<p>EDIT:</p>

<p>What I'm trying to achieve is to fulfill requirement 5.11 in the Cisecurity benchmark for Apache http server 2.2. It can be found here: <a href=""https://www.cisecurity.org/cis-benchmarks/"" rel=""nofollow noreferrer"">https://www.cisecurity.org/cis-benchmarks/</a></p>

<p>From the document: </p>

<blockquote>
  <p><strong>Description:</strong></p>
  
  <p>Restrict access to inappropriate file extensions that are not expected to be a legitimate part
  of web sites using the FilesMatch directive.</p>
  
  <p><strong>Rationale:</strong></p>
  
  <p>...</p>
  
  <p><strong>Audit:</strong></p>
  
  <p>Perform the following steps to determine if the recommended state is implemented:</p>
  
  <ol>
  <li><p>Verify that the FilesMatch directive that denies access to all files is present as
  shown in step 3 of the remediation with the Order of Deny, Allow.</p></li>
  <li><p>Verify that there is another FilesMatch directive similar to the one in step 4 of the
  remediation, with an expression that matches the approved file extensions.</p></li>
  </ol>
  
  <p><strong>Remediation:</strong></p>
  
  <p>Perform the following to implement the recommended state:</p>
  
  <ol>
  <li><p>Compile a list of existing file extension on the web server. The following find/awk
  command may be useful, but is likely to need some customization according to the
  appropriate webroot directories for your web server. Please note that the find
  command skips over any files without a dot (.) in the file name, as these are not
  expected to be appropriate web content.</p>
  
  <p><code>find */htdocs -type f -name '*.*' | awk -F. '{print $NF }' | sort -u</code></p></li>
  <li><p>Review the list of existing file extensions, for appropriate content for the web
  server, remove those that are inappropriate and add any additional file extensions
  expected to be added to the web server in the near future.</p></li>
  <li><p>Add the FilesMatch directive below which denies access to all files by default. </p></li>
  </ol>
  
  <p><code># Block all files by default, unless specifically allowed.</code></p>
  
  <p><code>&lt;FilesMatch ""^.*$""&gt;</code></p>
  
  <p><code>Require all denied</code></p>
  
  <p><code>&lt;/FilesMatch&gt;</code></p>
  
  <ol start=""4"">
  <li>Add another a FilesMatch directive that allows access to those file extensions
  specifically allowed from the review process in step 2. An example FilesMatch
  directive is below. The file extensions in the regular expression should match your approved list, and not necessarily the expression below.</li>
  </ol>
  
  <p><code># Allow files with specifically approved file extensions</code></p>
  
  <p><code># Such as (css, htm; html; js; pdf; txt; xml; xsl; ...),</code></p>
  
  <p><code># images (gif; ico; jpeg; jpg; png; ...), multimedia</code></p>
  
  <p><code>&lt;FilesMatch ""^.*\.(css|html?|js|pdf|txt|xml|xsl|gif|ico|jpe?g|png)$""&gt;</code></p>
  
  <p><code>Require all granted</code></p>
  
  <p><code>&lt;/FilesMatch&gt;</code></p>
</blockquote>

<p>Of course, I had to modify ""Require all granted"" because I'm using Apache 2.2</p>
","<apache-2.2><apache2>","2017-06-21 13:00:06"
"785250","DKIM to gmail failing","<p>I'm trying to get DKIM setup on a CentOS 7 server running exim but gmail is returning </p>

<pre><code>dkim=neutral (bad format) header.i=@ellie-oli.com;
DKIM-Signature: v=1; a=rsa-sha256; q=dns/txt; c=relaxed/relaxed; d=ellie-oli.com; s=mail;
h=; bh=VAgC5MVP54VmvlcGQaMT2ZdmokXkhMKi/RNSpcUu2qw=;
b=agz8IPjjK9+CerCMv5EDEl3DCVuakvU6StLQUgLTrnmVPPyazb0/Moi5pAopJdGJEUaHNhf9V2dFQNrcDUDw7AxqCUKT+pXwHDPq1tGIhtyntRy4LcoIBaEAf6eieVNScPQHX/hj2AUdMEVk1DaNwnh5rZbNGCydaMbVQFwbLLEXJbaY0sDt8Zpi/BF1JArxUMUeuJZlFGgU8LBpuQ671xuNVxMoK0Bfak3YEJjCx6LWhZBQLRKs2scZ/BHuDKLziY6n7GML8dPcgCpwo/wODJYXnDHxDY7MNCnEF6b110uQKje4kkQG32gVjJHr/gpeinQOWJ/oZAicJcnIp7kH8g==;
</code></pre>

<p>my exim config looks like </p>

<pre><code>remote_smtp:
  debug_print = ""T: remote_smtp_dkim for $local_part@$domain""
  driver = smtp
.ifdef REMOTE_SMTP_HOSTS_AVOID_TLS
  hosts_avoid_tls = REMOTE_SMTP_HOSTS_AVOID_TLS
.endif
.ifdef REMOTE_SMTP_HEADERS_REWRITE
  headers_rewrite = REMOTE_SMTP_HEADERS_REWRITE
.endif
.ifdef REMOTE_SMTP_RETURN_PATH
  return_path = REMOTE_SMTP_RETURN_PATH
.endif
.ifdef REMOTE_SMTP_HELO_DATA
  helo_data=REMOTE_SMTP_HELO_DATA
.endif
dkim_domain = ${lookup{$sender_address}lsearch*@{/etc/exim/dkim_senders}}
dkim_selector = mail
dkim_private_key = /etc/exim/dkim/mail.pem
dkim_canon = relaxed
dkim_strict = false
dkim_sign_headers = DKIM_SIGN_HEADERS
</code></pre>

<p>The DNS record appears to work as far as I can tell and returns as</p>

<pre><code>mail._domainkey.ellie-oli.com   text = ""v=DKIM1\; k=rsa\; p=MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAzo9rv2QgABArrAwPgeR++q3Y/0HGWqoETE3N7/o3hwYBMpujcPM22Lp0PoMYStu/VyyZikM23nEnmlpeOiS8GdGL0ZbP5HatDqvZKoiu5mx5PODtea8XWoKsH1BV2ngOWt0d43SRMSCBT5vJ9tJpjYe20B3lE2XEXHbrxZ5vWajvAi3vFFJ4mQSUKisQ+KV+NEt"" ""pqR9bm9KTk0HbeykdSwjvsz78eHCbJQUI+C9sn5MrKmdatqHOHA1fjf6iqbc7kdA08MGr3KoiySAFrPqRLR/Pw1oRueU8ImPIzY3n2ZvZqMl2zTDhe/luxf6ecCEj0AbfwbGghRxMq4QIZDvzFwIDAQAB""
</code></pre>

<p>Does anyone have any idea what might be causing the problems?</p>
","<linux><email><exim><dkim>","2016-06-21 10:09:30"
"1031835","Executing sudo commands without sudo keyword","<p>How can I enable that each user on a server will be able to execute a sudo command without specifying the sudo keyword - but not across all server, just a few chosen directories?</p>
<p>EDIT:
Just to make sure I understood things correctly, if I want to grant user <code>eliran</code> permissions to read, write and execute for some <code>DIR</code> i should execute <code>setfacl -m u:eliran:rwx DIR</code> ?</p>
","<linux>","2020-08-28 08:07:16"
"785280","How to get the correct MAC address of Windows Server 2008 out of multiple MAC addresses?","<p>I want to know the unique MAC address of my Windows Server 2008 R2. When I used <code>getmac</code> command in command prompt I got a list of MAC addresses(6 count) under Primary Address header. Could you please help me get the correct one out of this 6?</p>

<p><a href=""https://i.sstatic.net/q8cIl.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/q8cIl.png"" alt=""enter image description here""></a></p>

<p>Edited:</p>

<p>If I put <code>ipconfig /all</code> it's showing more information under Tunnel adapter. I know I need to select Physical Address under that Tunnel adapter for getting MAC address, but my doubt is there are 6 Tunnel adapter information available.</p>

<p>Screenshot:</p>

<p><a href=""https://i.sstatic.net/ylCbp.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ylCbp.png"" alt=""enter image description here""></a></p>

<p>My software teams wants to know the MAC address of the Server where they are going to install that particular software . So that I tried those commands (getmac, ipconfig /all) and saw lot of MAC addresses.</p>
","<mac-address>","2016-06-21 10:37:27"
"785285","Delete subdomain from AWS Route 53 using Bash script","<p>I want to write a bash script to delete the subdomain name in Route 53.</p>

<p>I have given access to my ec2 server by using </p>

<pre><code>$ echo ""[Credentials]"" &gt; ~/.boto
$ echo ""AWS_ACCESS_KEY_ID=key"" &gt;&gt; ~/.boto
$ echo ""AWS_SECRET_ACCESS_KEY=secret"" &gt;&gt; ~/.boto
$ chmod 640 ~/.boto
</code></pre>

<p>Now,I am using <code>cli53 rrdelete subdomain_name CNAME</code> command on my ubuntu server but got </p>

<blockquote>
<pre><code>""usage: cli53 rrdelete [-h] [-i IDENTIFIER] [--wait]
                      zone rr [{A,AAAA,CNAME,SOA,NS,MX,PTR,SPF,SRV,TXT,ALIAS}]
cli53 rrdelete: error: argument zone: Zone 'stagingmunnar1.bizom.in' not found""
</code></pre>
</blockquote>

<p>error.
So,Kindly let me know how to use this command to delete my subdomain in AWS Route 53.</p>
","<amazon-web-services><amazon-route53>","2016-06-21 11:52:24"
"857251","NotImplementedError at /","<p>my local.conf file is:</p>

<pre><code>[[local|localrc]]

HOST_IP=&lt;MY IP&gt;
SERVICE_HOST=$HOST_IP
MYSQL_HOST=$HOST_IP
RABBIT_HOST=$HOST_IP
GLANCE_HOSTPORT=$HOST_IP:9292

ADMIN_PASSWORD=secret
DATABASE_PASSWORD=$ADMIN_PASSWORD
RABBIT_PASSWORD=$ADMIN_PASSWORD
SERVICE_PASSWORD=$ADMIN_PASSWORD

KEYSTONE_USE_MOD_WSGI=""True""
NOVA_USE_MOD_WSGI=""True""
SWIFT_USE_MOD_WSGI=""True""
HEAT_USE_MOD_WSGI=""True""
CINDER_USE_MOD_WSGI=""True""



# ceilometer
enable_plugin ceilometer https://git.openstack.org/openstack/ceilometer.git master

# horizon
enable_service horizon

# cloudkitty
enable_plugin cloudkitty https://git.openstack.org/openstack/cloudkitty.git master
enable_service ck-api ck-proc
CLOUDKITTY_COLLECTOR=ceilometer

disable_service tempest
</code></pre>

<p>after successfully completing <code>./stack.sh</code> I got this page instead of dashboard/login page
<a href=""https://i.sstatic.net/fmmDD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fmmDD.png"" alt=""enter image description here""></a></p>

<p>where is the issue?</p>

<p>****y downvoting men?? is this question not appropriate to ask in serverFault????????? or the error is not elobrated??
i have not straight away paster it here...i did tried different methods
if have have an intention to downvote please let me also know what is the reason to downvote so that I can also know what to ask and how its done**</p>
","<openstack>","2017-06-22 10:52:16"
"1031972","From ZFS Array CLI how to get the latest snapshot for share","<p>from ZFS Array cli how to get the latest snapshot for share.</p>
<p>my share name is &quot;test_share&quot; for this share want to check the latest snapshot.</p>
<p>Test-ae0101zfs201:shares test_proj_test/test_share_test snapshots&gt;</p>
","<storage><zfs><oracle>","2020-08-29 15:16:56"
"927135","AWS cli: set permission to public only to files inside specific ""folders""","<p>So we have 300GB+ of files, in folders like the example above:</p>

<p>1000</p>

<pre><code>file.jpg

big/file.jpg

medium/file.jpg
</code></pre>

<p>2000</p>

<pre><code>file.jpg

big/file.jpg

medium/file.jpg
</code></pre>

<p>How to I set permission to allow anyone to read all objects inside ""big"" and ""medium"" in entirely bucket?</p>
","<amazon-web-services><aws-cli>","2018-08-20 05:40:24"
"785462","IPTables chains that just RETURN, as set via iRedMail [SOLVED]","<p>I hate to ask this, but I'm pretty sure I have everything configured correctly with iRedMail as the host for my email server/VPS re dovecot, postfix etc (even though everything is pretty much done automatically done via their install scripts).</p>

<p>While I'm no expert on firewalls at all, I'm not totally dumb, and as such I've been trying to figure out why I can't connect to the mail server via pop, imap, smtp etc over the various ports from an external email client. </p>

<p>I can't telnet to any of the ports remotely, but can get to them locally, so I'm thinking it has to be a firewall issue, which leads me to my current conclusion: the firewall process could be wrong?</p>

<p>These are the results of some commands:</p>

<pre><code>[root@server user]#  telnet localhost 110
Trying ::1...
Connected to localhost.
Escape character is '^]'.
+OK Dovecot ready.
</code></pre>

<p>And just to confirm it's working:</p>

<pre><code>[root@server user]# dovecot -n | grep protocols
protocols = pop3 imap sieve lmtp
ssl_protocols = !SSLv2 !SSLv3
</code></pre>

<p>And then the IP Tables output:</p>

<pre><code>[root@server user]# iptables -S
-P INPUT DROP
-P FORWARD ACCEPT
-P OUTPUT ACCEPT
-N FORWARD_IN_ZONES
-N FORWARD_IN_ZONES_SOURCE
-N FORWARD_OUT_ZONES
-N FORWARD_OUT_ZONES_SOURCE
-N FORWARD_direct
-N FWDI_public
-N FWDI_public_allow
-N FWDI_public_deny
-N FWDI_public_log
-N FWDO_public
-N FWDO_public_allow
-N FWDO_public_deny
-N FWDO_public_log
-N INPUT_ZONES
-N INPUT_ZONES_SOURCE
-N INPUT_direct
-N IN_public
-N IN_public_allow
-N IN_public_deny
-N IN_public_log
-N OUTPUT_direct
-N f2b-default
-N f2b-dovecot
-N f2b-postfix
-N f2b-roundcube
-A INPUT -p tcp -m multiport --dports 80,443,25,587,110,995,143,993,4190 -j f2b-postfix
-A INPUT -p tcp -m multiport --dports 80,443,25,587,110,995,143,993,4190 -j f2b-dovecot
-A INPUT -p tcp -m multiport --dports 80,443,25,587,110,995,143,993,4190 -j f2b-roundcube
-A INPUT -p tcp -j f2b-default
-A INPUT -p tcp -j f2b-default
-A INPUT -p tcp -m multiport --dports 80,443,25,587,110,995,143,993,4190 -j f2b-postfix
-A INPUT -p tcp -m multiport --dports 80,443,25,587,110,995,143,993,4190 -j f2b-dovecot
-A INPUT -p tcp -m multiport --dports 80,443,25,587,110,995,143,993,4190 -j f2b-roundcube
-A INPUT -p tcp -j f2b-default
-A INPUT -p tcp -j f2b-default
-A INPUT -i lo -j ACCEPT 
-A INPUT -s 127.0.0.1/32 -p tcp -m tcp --dport 7822 -j ACCEPT
-A INPUT -s 127.0.0.1/32 -p tcp -m tcp --dport 7822 -j ACCEPT
-A INPUT -s 127.0.0.1/32 -p tcp -m tcp --dport 7822 -j ACCEPT
-A INPUT -s 127.0.0.1/32 -p tcp -m tcp --dport 22 -j ACCEPT
-A INPUT -p tcp -m tcp --dport 80 -j ACCEPT
-A INPUT -p tcp -m tcp --dport 443 -j ACCEPT
-A INPUT -p tcp -m tcp --dport 21 -j ACCEPT
-A INPUT -p tcp -m tcp --dport 20 -j ACCEPT
-A INPUT -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A INPUT -p tcp -m tcp --dport 53 -j ACCEPT
-A INPUT -p udp -m udp --dport 53 -j ACCEPT
-A INPUT -p tcp -m tcp --dport 10000 -j ACCEPT
-A INPUT -p tcp -m tcp --dport 8000 -j ACCEPT
-A INPUT -p tcp -m tcp --dport 8000 -j ACCEPT
-A INPUT -p tcp -m tcp --dport 8000 -j ACCEPT
-A INPUT -p tcp -m tcp --dport 8001 -j ACCEPT 
-A INPUT -p tcp -m state --state NEW -m tcp --dport 25 -j ACCEPT
-A f2b-default -j RETURN
-A f2b-default -j RETURN
-A f2b-default -j RETURN
-A f2b-default -j RETURN
-A f2b-dovecot -j RETURN
-A f2b-dovecot -j RETURN
-A f2b-postfix -j RETURN
-A f2b-postfix -j RETURN
-A f2b-roundcube -j RETURN
-A f2b-roundcube -j RETURN
</code></pre>

<p>Or a different view.</p>

<pre><code>[root@server log]# iptables -nvL
Chain INPUT (policy DROP 8296 packets, 397K bytes)
 pkts bytes target     prot opt in     out     source               destination         
19567 2390K f2b-postfix  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            multiport dports 80,443,25,587,110,995,143,993,4190
19567 2390K f2b-dovecot  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            multiport dports 80,443,25,587,110,995,143,993,4190
19567 2390K f2b-roundcube  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            multiport dports 80,443,25,587,110,995,143,993,4190
 106K   13M f2b-default  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0           
 106K   13M f2b-default  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0           
19567 2390K f2b-postfix  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            multiport dports 80,443,25,587,110,995,143,993,4190
19567 2390K f2b-dovecot  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            multiport dports 80,443,25,587,110,995,143,993,4190
19567 2390K f2b-roundcube  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            multiport dports 80,443,25,587,110,995,143,993,4190
 106K   13M f2b-default  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0           
 106K   13M f2b-default  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0           
46957 7004K ACCEPT     all  --  lo     *       0.0.0.0/0            0.0.0.0/0           
    0     0 ACCEPT     tcp  --  *      *       127.0.0.1            0.0.0.0/0            tcp dpt:7822
    0     0 ACCEPT     tcp  --  *      *       127.0.0.1            0.0.0.0/0            tcp dpt:7822
    0     0 ACCEPT     tcp  --  *      *       127.0.0.1            0.0.0.0/0            tcp dpt:7822
    0     0 ACCEPT     tcp  --  *      *       127.0.0.1            0.0.0.0/0            tcp dpt:22
  396 25848 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:80
 8718 1575K ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:443
    0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:21
    0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:20
43508 3858K ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate RELATED,ESTABLISHED
    0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:53
   21  1248 ACCEPT     udp  --  *      *       0.0.0.0/0            0.0.0.0/0            udp dpt:53
    0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:10000
    0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:8000
    0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:8000
    0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:8000
    0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:8001
  748 43552 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:996
    7   444 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            state NEW tcp dpt:25

Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination         

Chain OUTPUT (policy ACCEPT 112K packets, 80M bytes)
 pkts bytes target     prot opt in     out     source               destination         

Chain FORWARD_IN_ZONES (0 references)
 pkts bytes target     prot opt in     out     source               destination         

Chain FORWARD_IN_ZONES_SOURCE (0 references)
 pkts bytes target     prot opt in     out     source               destination         

Chain FORWARD_OUT_ZONES (0 references)
 pkts bytes target     prot opt in     out     source               destination         

Chain FORWARD_OUT_ZONES_SOURCE (0 references)
 pkts bytes target     prot opt in     out     source               destination         

Chain FORWARD_direct (0 references)
 pkts bytes target     prot opt in     out     source               destination         

Chain FWDI_public (0 references)
 pkts bytes target     prot opt in     out     source               destination         

Chain FWDI_public_allow (0 references)
 pkts bytes target     prot opt in     out     source               destination         

Chain FWDI_public_deny (0 references)
 pkts bytes target     prot opt in     out     source               destination         

Chain FWDI_public_log (0 references)
 pkts bytes target     prot opt in     out     source               destination         

Chain FWDO_public (0 references)
 pkts bytes target     prot opt in     out     source               destination         

Chain FWDO_public_allow (0 references)
 pkts bytes target     prot opt in     out     source               destination         

Chain FWDO_public_deny (0 references)
 pkts bytes target     prot opt in     out     source               destination         

Chain FWDO_public_log (0 references)
 pkts bytes target     prot opt in     out     source               destination         

Chain INPUT_ZONES (0 references)
 pkts bytes target     prot opt in     out     source               destination         

Chain INPUT_ZONES_SOURCE (0 references)
 pkts bytes target     prot opt in     out     source               destination         

Chain INPUT_direct (0 references)
 pkts bytes target     prot opt in     out     source               destination         

Chain IN_public (0 references)
 pkts bytes target     prot opt in     out     source               destination         

Chain IN_public_allow (0 references)
 pkts bytes target     prot opt in     out     source               destination         

Chain IN_public_deny (0 references)
 pkts bytes target     prot opt in     out     source               destination         

Chain IN_public_log (0 references)
 pkts bytes target     prot opt in     out     source               destination         

Chain OUTPUT_direct (0 references)
 pkts bytes target     prot opt in     out     source               destination         

Chain f2b-default (4 references)
 pkts bytes target     prot opt in     out     source               destination         
 422K   50M RETURN     all  --  *      *       0.0.0.0/0            0.0.0.0/0           
    0     0 RETURN     all  --  *      *       0.0.0.0/0            0.0.0.0/0           
    0     0 RETURN     all  --  *      *       0.0.0.0/0            0.0.0.0/0           
    0     0 RETURN     all  --  *      *       0.0.0.0/0            0.0.0.0/0           

Chain f2b-dovecot (2 references)
 pkts bytes target     prot opt in     out     source               destination         
39134 4779K RETURN     all  --  *      *       0.0.0.0/0            0.0.0.0/0           
    0     0 RETURN     all  --  *      *       0.0.0.0/0            0.0.0.0/0           

Chain f2b-postfix (2 references)
 pkts bytes target     prot opt in     out     source               destination         
39134 4779K RETURN     all  --  *      *       0.0.0.0/0            0.0.0.0/0           
    0     0 RETURN     all  --  *      *       0.0.0.0/0            0.0.0.0/0           

Chain f2b-roundcube (2 references)
 pkts bytes target     prot opt in     out     source               destination         
39134 4779K RETURN     all  --  *      *       0.0.0.0/0            0.0.0.0/0           
    0     0 RETURN     all  --  *      *       0.0.0.0/0            0.0.0.0/0  
</code></pre>

<p>As I mentioned, I'm no expert, but to me, it looks like it's capturing all traffic in the ""email"" port realm, and then sending it to the f2b* rules, but then simply giving a RETURN. At no point does it ACCEPT anything.</p>

<p>Or, does the traffic get sent to the actual fail2ban program for analysis, where it is then decided upon, as to what has to happen with it?</p>

<p>Sorry for the simpleton question.</p>

<p>Cheers,
Steve</p>
","<linux><centos><iptables><firewall><iredmail>","2016-06-22 04:54:24"
"785488","How do I install nginx on Ubuntu 16.04 so that I can have upstream config files","<p>I use ubuntu 16.04. I have removed nginx config</p>

<p>$ sudo rm -rvf /etc/nginx/</p>

<p>purged nginx and installed it again</p>

<pre><code># apt-get remove --purge nginx
# apt install nginx
</code></pre>

<p>aptitude shows nginx installed</p>

<h1>aptitude search nginx</h1>

<pre><code>p   lua-nginx-websocket                                - Lua websocket client driver for the nginx embedded Lua langu
i   nginx                                              - small, powerful, scalable web/proxy server                  
v   nginx:i386                                         -                                                             
i A nginx-common                                       - small, powerful, scalable web/proxy server - common files   
i A nginx-core                                         - nginx web/proxy server (core version)                       
p   nginx-core:i386                                    - nginx web/proxy server (core version)                       
</code></pre>

<p>but command # ls /etc/n* shows no nginx config files installed?</p>

<p>what is going on?</p>

<hr>

<p>following this page: <a href=""https://askubuntu.com/questions/361902/how-to-install-nginx-after-removed-it-manually"">https://askubuntu.com/questions/361902/how-to-install-nginx-after-removed-it-manually</a></p>

<p>I have removed nginx and installed again. while installing i got following warnings.</p>

<pre><code>Selecting previously unselected package nginx.
dpkg: warning: files list file for package 'nginx-common' missing; assuming package has no files currently installed
dpkg: warning: files list file for package 'nginx-core' missing; assuming package has no files currently installed
</code></pre>
","<ubuntu><nginx>","2016-06-22 08:38:32"
"927353","Google cloud Linux instance is unable to login suddenly","<p>This is a free 3cx google linux instance we have been using for past 3 weeks. All of sudden i am unable to login VM using Putty/SSH or Google cloud console and getting ran out of storage errors. I have resized the disk but still keeps getting errors. Below is few lines from serial log - </p>

<pre><code>localhost google_accounts_daemon[760]: Activating Google Compute Engine OS Login.

Aug 21 09:47:55 localhost google_accounts_daemon[760]: sed: couldn't open temporary file /etc/ssh/sed5ok0QR: No space left on device
Aug 21 09:47:55 localhost google_accounts_daemon[760]: sed: couldn't open temporary file /etc/ssh/sedsnBxDR: No space left on device
Aug 21 09:47:55 localhost google_accounts_daemon[760]: sed: couldn't open temporary file /etc/ssh/sedC8TiHR: No space left on device
Aug 21 09:47:55 localhost google_accounts_daemon[760]: sed: couldn't open temporary file /etc/sedwyzwwR: No space left on device
Aug 21 09:47:55 localhost google_accounts_daemon[760]: sed: couldn't open temporary file /etc/sedfzoX9P: No space left on device
Aug 21 09:47:55 localhost google_accounts_daemon[760]: sed: couldn't open temporary file /etc/pam.d/sedk4JABH: No space left on device
Aug 21 09:47:55 localhost google_accounts_daemon[760]: sed: couldn't open temporary file /etc/pam.d/sed0i1OtH: No space left on device
Aug 21 09:47:55 localhost google_accounts_daemon[760]: sed: couldn't open temporary file /etc/pam.d/sedyhvdhH: No space left on device
Aug 21 09:47:55 localhost google_accounts_daemon[760]: sed: couldn't open temporary file /etc/pam.d/sedTaGMlH: No space left on device
Aug 21 09:47:55 localhost google_accounts_daemon[760]: Restarting sshd.
Aug 21 09:47:55 localhost systemd[1]: Stopping OpenBSD Secure Shell server...
Aug 21 09:47:55 localhost systemd[1]: Stopped OpenBSD Secure Shell server.
Aug 21 09:47:55 localhost systemd[1]: Starting OpenBSD Secure Shell server...
Aug 21 09:47:55 localhost systemd[1]: Started OpenBSD Secure Shell server.
Aug 21 09:47:55 localhost google-accounts: ERROR Exception calling the response handler. [Errno 2] No usable temporary directory found in ['/tmp', '/var/tmp', '/usr/tmp', '/'].#012Traceback (most recent call last):#012  File ""/usr/lib/python2.7/dist-packages/google_compute_engine/metadata_watcher.py"", line 196, in WatchMetadata#012    handler(response)#012  File ""/usr/lib/python2.7/dist-packages/google_compute_engine/accounts/accounts_daemon.py"", line 263, in HandleAccounts#012    self.utils.SetConfiguredUsers(desired_users.keys())#012  File ""/usr/lib/python2.7/dist-packages/google_compute_engine/accounts/accounts_utils.py"", line 276, in SetConfiguredUsers#012    mode='w', prefix=prefix, delete=True) as updated_users:#012  File ""/usr/lib/python2.7/tempfile.py"", line 463, in NamedTemporaryFile#012    dir = gettempdir()#012  File ""/usr/lib/python2.7/tempfile.py"", line 275, in gettempdir#012    tempdir = _get_default_tempdir()#012  File ""/usr/lib/python2.7/tempfile.py"", line 217, in _get_default_tempdir#012    (""No usable temporary directory found in %s"" % dirlist))#012IOError: [Errno 2] No usable temporary directory found in ['/tmp', '/var/tmp', '/usr/tmp', '/']
</code></pre>

<p>Equivalent command line</p>

<p>Did anyone encounter this error if so can help resolve this?</p>
","<linux><google-cloud-platform><google-compute-engine>","2018-08-21 13:55:17"
"998359","block non full requests with nginx","<p>I have a tube website like YouTube and some android applications keep steeling the videos temporary links like :</p>

<blockquote>
  <p><a href=""https://www.sample-videos.com/video123/mp4/720/big_buck_bunny_720p_30mb.mp4?Key=HUi_kuAjHyUHVGccqoYCAA&amp;Expires=1578620165"" rel=""nofollow noreferrer"">https://www.sample-videos.com/video123/mp4/720/big_buck_bunny_720p_30mb.mp4?Key=HUi_kuAjHyUHVGccqoYCAA&amp;Expires=1578620165</a></p>
</blockquote>

<p>and they using it with their own player which causing me a high bandwidth usage.</p>

<p>The IP address cannot be blocked because its the user IP not the application itself and there is no way to tell if its my user or the application user , also I've tried to block based on user agent + referrer but they faked it ,</p>

<p>due to nature of the application it only need to get the video direct link from the page itself and it doesn't get all the page resources so i want to try blocking non full requests of that page. If someone requests only a piece of that page then it must be blocked, but if the request is for the full page then it should be allowed.</p>
","<nginx>","2020-01-09 21:22:10"
"785716","Username and password are incorrect","<p>I have a domain organization with windows 7 computers.
From time to time
users can not seem to log on, the error message is
<strong>""Username and password are incorrect""</strong></p>

<p>The password used <em>is</em> the correct one.  Only after the computer is restarted does the login work.</p>
","<active-directory><password>","2016-06-23 05:14:19"
"857633","Disable Puppet automatic scheduled runs","<p>Is it possible to disable puppet runs (every 30 minutes) but it should be still listening? </p>

<p>I'm setting up puppetkick or mcollective. So the agents should be still running and listening so I can push updates but should not automatically run to look for updates, not even once (runinterval).</p>

<p>EDIT:</p>

<p>The answers in the topic (link below) doesn't address or resolve the problem. They're onto disabling agent, disabling start-up, or change run interval. Also the daemonize=false, just hangs puppet init start, nothing happens.</p>

<p>I want the agent to be in listening mode, if its possible</p>

<p><a href=""https://serverfault.com/questions/657911/how-to-disable-automatic-scheduled-puppet-runs-that-occur-every-30-minutes"">How to disable automatic scheduled Puppet runs that occur every 30 minutes?</a></p>
","<puppet><puppetmaster><puppet-agent>","2017-06-24 03:51:34"
"927505","Windows Firewall notifies network uses after updating exe","<p>We have a software made in NWjs with auto-update. Recently we publish an update and some  clients report firewall notification after download this update.</p>

<p><img src=""https://i.sstatic.net/TACgL.png"" alt=""Windows Firewall Notification""></p>

<p>Now a need to know if is there a way to disable or avoid this notification show without user interaction.</p>
","<windows><firewall>","2018-08-22 13:40:42"
"857730","Removing IP Address From Files On Site (IIS)","<p>I have an IIS server set up hosting my website (only 2 files now: <code>index.htm</code> and <code>banner.png</code>). I have a domain set up with GoDaddy and have forwarding set up in GoDaddy as well.</p>

<p>So, when I type <code>http://example.com</code>, it takes me straight to the index.htm page with the IP masked in the address bar (shows <code>http://example.com</code>).</p>

<p>If I right-click on the banner at the top of my homepage, the URL from that banner is <code>http://ip/banner.png</code>. I want this to be <code>http://example.com/banner.png</code>. How do I do this?</p>

<p>Is this something I set up through GoDaddy or IIS?</p>
","<domain><files><mask>","2017-06-25 05:02:43"
"927555","Properly implement REMOTE_USER-like thing for ssh","<p>I'd like to know how to make the remote user name available in an ssh session on my server. So when <code>joe@host1</code> logs in as <code>commonuser@server1</code>, the scripts running on <code>server1</code> running as <code>commonuser</code> will know that the person running them is actually <code>joe</code>.</p>

<ol>
<li><p>I tried <code>SendEnv</code> / <code>AcceptEnv</code> without luck. I even looked at SSH debug output and it looked like the environment variable was sent, but when I do</p>

<p><code>echo $REMOTE_USER
</code></p>

<p>I see empty string.</p></li>
<li><p>I would also like this to be working even if <code>joe</code> is less cooperative, for example when he refuses to set the <code>REMOTE_USER</code> variable.</p></li>
</ol>
","<ssh><authentication><remote><users>","2018-08-22 19:10:17"
"1032387","get SSL certificate for mail server, while already having it for domain","<p>I have already achieved an SSL certificate for my domain by <code>certbot</code>. the configuration is:</p>
<pre><code>&lt;VirtualHost *:80&gt;
ServerAdmin admin@xxx.com
DocumentRoot &quot;/root&quot;
ServerName xxx.com
ServerAlias www.xxx.com
ErrorLog &quot;/var/log/httpd/xxx.error_log&quot;
CustomLog &quot;/var/log/httpd/xxx.access_log&quot; common
RewriteEngine on
RewriteCond %{SERVER_NAME} =www.xxx.com [OR]
RewriteCond %{SERVER_NAME} =xxx.com
RewriteRule ^ https://%{SERVER_NAME}%{REQUEST_URI} [END,NE,R=permanent]
&lt;/VirtualHost&gt;
</code></pre>
<p>now I have installed Postfix SMTP Server and like to get a certificate for <code>mail.xxx.com</code> as well. how to get ssl certificate for my mail server now?</p>
<p>I have already tried the following command:</p>
<pre><code>sudo certbot certonly --standalone -d mail.xxx.com
</code></pre>
<p>which outputs:</p>
<pre><code>Problem binding to port 80: Could not bind to IPv4 or IPv6.
</code></pre>
","<centos><postfix><apache-2.4><lets-encrypt><certbot>","2020-09-02 09:20:41"
"927563","Why can't I have more than two SSH connections to a server?","<p>I am using Puppet PE 2017.3.5.0-1 on a CentOS7 AWS instance. When I SSH into this machine, to a default installation, I am getting limited to two connections only.  When I try to connect again (from a 3rd terminal) the  connection is closed automatically.  </p>

<p>I'm not able to connect more than twice at the same time. I am using multiplexing.  </p>

<p>I see a setting in <code>/etc/ssh/sshd_config</code> that says <code>MaxSessions 2</code>, which I highly suspect has something to do with it, but adjustments are not helping.</p>
","<ssh><puppet><multiplexing>","2018-08-22 19:47:31"
"998521","VPN via middle/jump server","<p>Here's the rough picture of what I'm trying to do:</p>
<p>client -&gt; |VPN| -&gt; server A -&gt; |VPN| -&gt; server B -&gt; Internet.</p>
<p>Server A and Server B are both on public internet with public IPs, also I have root access to both servers. Client is behind firewall but can reach server A. Client CANNOT reach server B directly. Server A can reach server B. Server B is where I want packets to exit to internet from. I need to encrypt both communications from client to A, and from A to B, since A and B aren't in a common LAN so traffic between A and B travel via public infrastructure.</p>
<p>I have been able to setup a IPSec VPN server on A and are able to connect to it. But I can't figure out the correct way to establish another VPN from A to B and reroute traffic coming to A from client to B.</p>
<p>Tips?</p>
<p>p.s. originally I thought of using SSH tunnels to connect A and B then route relevant packet from client arriving at A to B via such tunnel. But as people pointed out in comments that this isn't such a good idea. So I'm open to any suggestions. Thanks!</p>
<p><strong>EDIT #1</strong></p>
<p>VPN service on server A is setup with strongswan, which handles any connection between clients and server A. I was done via <code>ipsec.conf</code>:</p>
<pre><code># ipsec.conf - strongSwan IPsec configuration file

# basic configuration
config setup
        charondebug=&quot;ike 2, knl 2, cfg 2, net 2, esp 2, dmn 2, mgr 2&quot;
        strictcrlpolicy=no
        uniqueids=yes
        cachecrls=no
conn ipsec-ikev2-vpn
      auto=add
      compress=no
      type=tunnel  # defines the type of connection, tunnel.
      keyexchange=ikev2
      fragmentation=yes
      forceencaps=yes
      dpdaction=clear
      dpddelay=300s
      rekey=no
      left=%any
      leftid=47.112.200.xxx    # if using IP, define it without the @ sign
      leftcert=vpn-server.cert.pem  # reads the VPN server cert in /etc/ipsec.d/certs
      leftsendcert=always
      leftsubnet=0.0.0.0/0
      right=%any
      rightid=%any
      rightauth=eap-mschapv2
      rightsourceip=10.10.10.0/24  # IP address Pool to be assigned to the clients
      rightdns=1.1.1.1,8.8.8.8  # DNS to be assigned to clients
      rightsendcert=never
      eap_identity=%identity  # defines the identity the client uses to reply to an EAP Identity request.
</code></pre>
<p>With <code>ufw</code> (the tutorial I followed used ufw but I can work with basic <code>iptables</code> rules as well) settings in <code>/etc/ufw/before.rules</code>; I omitted portion auto-generated by ufw, also I've manually allowed udp on port 500/4500 and tcp on port 22:</p>
<pre><code>#
# rules.before
#
# Rules that should be run before the ufw command line added rules. Custom
# rules should be added to one of these chains:
#   ufw-before-input
#   ufw-before-output
#   ufw-before-forward
#

# Don't delete these required lines, otherwise there will be errors
*nat
-A POSTROUTING -s 10.10.10.0/24 -o eth0 -m policy --pol ipsec --dir out -j ACCEPT
-A POSTROUTING -s 10.10.10.0/24 -o eth0 -j MASQUERADE
COMMIT

*mangle
-A FORWARD --match policy --pol ipsec --dir in -s 10.10.10.0/24 -o eth0 -p tcp -m tcp --tcp-flags SYN,RST SYN -m tcpmss --mss 1361:1536 -j TCPMSS --set-mss 1360
COMMIT

*filter
:ufw-before-input - [0:0]
:ufw-before-output - [0:0]
:ufw-before-forward - [0:0]
:ufw-not-local - [0:0]
# End required lines


-A ufw-before-forward --match policy --pol ipsec --dir in --proto esp -s 10.10.10.0/24 -j ACCEPT
-A ufw-before-forward --match policy --pol ipsec --dir out --proto esp -d 10.10.10.0/24 -j ACCEPT
</code></pre>
<p>This is how far I've got. Separate attempt to setup site-to-site VPN via strongswan IPSec between A and B has failed due to port 500 and 4500 being blocked/tempered with: client can reach A via 500/4500, A cannot reach B via 500/4500, and I've exposed all ports of B to open internet for testing.</p>
<h2>EDIT 2</h2>
<p>Currently the only tunnel I've managed to make between A and B is a SSH socks (<code>ssh -N -f -4 -D 1080 [server B ip]</code> from server A). I'm thinking about just redirecting traffic exiting A, whose destination isn't my client device, to B via that SSH tunnel.</p>
<p>Something like:<code>-A OUTPUT -p tcp --dport 443 -j REDIRECT --to-ports 1080</code> in ufw/iptables.(this doesn't actually work, server A's ip still shows up in ip checker and blocked websites are still not accessible)</p>
","<iptables><vpn><firewall><forwarding><tunnel>","2020-01-10 22:04:27"
"857836","Spam rejecting configuration (Postfix)","<p>I want to configure Postfix to reject spam from the Internet. I found the following example <a href=""http://www.postfix.org/SMTPD_ACCESS_README.html"" rel=""nofollow noreferrer"">here</a>:</p>

<pre><code>smtpd_client_restrictions = permit_mynetworks, reject

smtpd_helo_restrictions = reject_unknown_helo_hostname

smtpd_sender_restrictions = reject_unknown_sender_domain

smtpd_relay_restrictions = permit_mynetworks, 
permit_sasl_authenticated,
reject_unauth_destination

smtpd_recipient_restrictions = permit_mynetworks, 
permit_sasl_authenticated,
reject_unauth_destination
reject_rbl_client zen.spamhaus.org,
reject_rhsbl_reverse_client dbl.spamhaus.org,
reject_rhsbl_helo dbl.spamhaus.org,
reject_rhsbl_sender dbl.spamhaus.org

smtpd_data_restrictions = reject_unauth_pipelining
</code></pre>

<p>But when I sent an e-mail to my domain from a Gmail account it got blocked. I then changed:</p>

<pre><code>smtpd_client_restrictions = permit_mynetworks, reject
</code></pre>

<p>to:</p>

<pre><code>smtpd_client_restrictions = permit_mynetworks, reject_unknown_client_hostname
</code></pre>

<p>and now my mail server seems to work.</p>

<p>Is this example provided by postfix.org a best practise or should I strengthen/weaken it?</p>
","<email><postfix><spam>","2017-06-26 04:21:13"
"998583","ubuntu 18.04 vps: auth.log, postfix and webmin users","<p>I have a VPS with ubuntu.</p>

<p>1.I don't need postfix so I shut it down. Today I saw it is up again. how can I stop it completely? does a <code>sudo service postfix stop</code> enough?
What about dovecot? Do I need it or can I stop it as well?</p>

<p>2.I have 40+ users in usermin. I only use root but maybe other processes use other users? How can I know which user I can remove? Or if it's not me who created it, I can delete it?</p>

<p>3.auth.log and mail.log were too big so I deleted them and they were not recreated. I created auth.log and <code>sudo chown syslog:adm /var/log/auth.log</code> but it is still empty. What should I do so those files will be created with logs?</p>

<p>Thanks</p>
","<ubuntu><postfix><vps>","2020-01-11 21:18:41"
"998626","AWS SNS Failed Notifications logging","<p>I had some sns topics which is used for push notification.I want to enable sns topic logging for failed deliveries. Anyone have idea how can i achieve this ??
Thanks in advance </p>
","<linux><amazon-web-services><amazon-cloudwatch><failed><amazon-sns>","2020-01-12 13:40:59"
"927778","Browser does not have rewritten URL with query string","<p>When I rewrite the URL by adding a query string, the URL in the browser does not have the query string.  Here's what I have:</p>

<pre><code>location / {
    rewrite ^/$ /?page=test break;
}
</code></pre>

<p>Instead, I see the original URL in the browser when I visit the root directory.  I am able to rewrite to another path.</p>
","<nginx><rewrite>","2018-08-24 05:43:18"
"786136","How to view dnsmasq client MAC addresses dynamically?","<p>If I have already known the client IP, I know there's <code>/var/log/dnsmasq.log</code> and it contains logs such as</p>

<blockquote>
  <p>Jun 13 12:22:42 dnsmasq-dhcp[499]: DHCPACK(wlan0) 172.24.1.110 34:12:98:11:80:bd ones-iPad<br>
  Jun 13 13:19:44 dnsmasq-dhcp[499]: DHCPDISCOVER(wlan0) d4:97:32:61:4f:73<br>
  Jun 13 13:19:44 dnsmasq-dhcp[499]: DHCPOFFER(wlan0) 172.24.1.82 d4:97:0b:61:4f:23<br>
  Jun 13 13:19:44 dnsmasq-dhcp[499]: DHCPREQUEST(wlan0) 172.24.1.82 d4:97:9f:61:4f:73<br>
  Jun 13 13:19:44 dnsmasq-dhcp[499]: DHCPACK(wlan0) 172.24.1.82 d4:97:0b:23:4f:73 android-ef9f423f7ecaca3c                                                     </p>
</blockquote>

<p>In router</p>

<p>In this way, we can parse the log every time to see the latest MAC address.</p>

<p>But can we know what the client mac without parsing this long big file every time. It drags down CPU.</p>

<p>Thank you!</p>

<hr>

<p>updated</p>

<p>I found another place containing it</p>

<p><code>cat /var/lib/misc/dnsmasq.leases</code></p>

<p>Still it's a file. Or I have to parse the file every time?</p>
","<linux><logging><mac-address><dnsmasq><hostapd>","2016-06-24 18:58:03"
"1032725","How to check Analytics write API calls in cloud console?","<p>I am using Node.JS to call the Analytics API to write some custom dimensions. But, i don't see in the google cloud console any write calls for my request. However, I am seeing error in my console .... Quota Error: Rate limit for writes exceeded. But, when i go to google cloud console i don't see any Write API calls.</p>
<p>Here are my functions which I am using in my Node.JS.</p>
<pre><code>google.analytics('v3').management.customDimensions.list({
      &quot;auth&quot;: jwt,
      &quot;accountId&quot;: accountId,
      &quot;webPropertyId&quot;: webPropertyId
    }
</code></pre>
<p>And:</p>
<pre><code>google.analytics('v3').management.customDimensions.insert({
      auth: jwt,
      &quot;accountId&quot;: req.accountId,
      &quot;webPropertyId&quot;: req.webPropertyId,
      &quot;resource&quot;: {
        &quot;name&quot;: req.offsetName,
        &quot;scope&quot;: req.scope,
        &quot;active&quot;: true
      }
    }
</code></pre>
","<google-cloud-platform>","2020-09-04 21:41:07"
"927934","SSL/TLS support in AWS ELB/NLB with multiple instance ports assigned to Load balancer","<p>AWS NLB does not allow SSL but ELB does. <br>
However NLB supports adding multiple instance ports to LB where as ELB does not. </p>

<p>Is there way to support multiple ports for LB with SSL transport?</p>

<p>For instance I have 4 services running on 2 nodes.</p>

<ul>
<li>Node1 hosts service1_master (port 1111) and service2_slave (port 1112)</li>
<li>Node1 hosts service2_master (port 1111) and service1_slave (port 1112)</li>
</ul>

<p>Service1 and Service2 are running in Active/Passive mode. Meaning only one port, either 1111 or 1112 can be active for each service.</p>

<p>I would like to add port 1111 and 1112 to the listener over SSL/TLS.
Is that possible with AWS load balancers.</p>
","<amazon-web-services><aws-cli>","2018-08-25 00:08:21"
"998790","HP Proliant DL180 G6 HHD failure and replacement","<p>One of the HP HDD in our HP Proliant DL180 G6 had failed, I've replaced the failed HDD with a standard 1 TB Seagate HDD, the Array controller won't start rebuilding on a Seagate, I had the new Seagate HDD in for about a week and the controller status reads interim rebuild. </p>

<p>I've checked the health of the failed HP HHD and it reads as OK, I've put it back into HP Proliant and array controller status reads as rebuilding, I left it overnight and its status still reads 0%. </p>

<p>Should the Controller be taking this long to rebuild the HHD?</p>

<p>If the drive doesn't rebuild do I need HP HHD or can use anyone that I pick up at my lock Computer supply store? </p>
","<hard-drive><hp-proliant><hp-smart-array><drive-failure>","2020-01-13 22:41:01"
"786178","Cannot chroot mounted volume to reset root password ( hangs )","<p>I am trying to reset the root password on my instance. I boot it in rescue mode and mount the primary disk.</p>

<pre><code>fdisk -l
sudo mount /dev/sdb1 /mnt
sudo chroot /mnt
</code></pre>

<p>The prompt just hangs here and it never gives me a new prompt so I can passwd and reset it.</p>
","<linux><ubuntu><centos>","2016-06-25 00:26:32"
"998867","Windows 2016 Password Hash Table Location","<p>Sadly, it is not self evident just how important it is to get a penetration test done at this company. I am rather rusty to this. Where is the hash table stored on a Win2016 DC these days? </p>

<p>I want to run a simple cracker against it, in order to probe for weak passwords and prove that people are not adhering to our policies and guidelines. This will then be the foundation for my argument of how vulnerable we are and how we need to get penetration testing done then improve our security policies.</p>

<p>Thanks in advance.</p>
","<security><password><testing>","2020-01-14 10:32:07"
"928030","Azure Subscription","<p>Currently i am using Free Trial Subscription of Azure.</p>

<p>I am trying to use <code>Get-AzureVnetConfig -ExportToFile D:\NetworkConfig.Xml</code>
but it's asking for the Subscription, when i am putting the Subscription As Free Trial, its not working.</p>

<p><img src=""https://i.sstatic.net/gqccd.png"" alt=""enter image description here""></p>

<p>How do I get this command to work with a trial subscription?</p>
","<azure>","2018-08-26 07:03:58"
"786283","My computer is resolving the domain I own to a wrong one","<p>I updated the DNS from the Digitalocean DNS service and the DNS is updated. When my friends ping the address www.esports.mv its going to the correct server but when I ping from my computer its going to a wrong sevrer 202.71.99.194 &lt; that server is wrong</p>
","<windows><domain-name-system>","2016-06-25 20:30:30"
"858575","ansible tower causing nginx problems","<p>The ansible tower uses  port 40 and 443.</p>
<p>Nginx also listens on port 80.</p>
<p>Now, the servers which I have hosted behind nginx, like netdata.
I accessed them earlier by <a href=""http://xx.xx.xx.xx/netdata"" rel=""nofollow noreferrer"">http://xx.xx.xx.xx/netdata</a>.</p>
<p>Now that ansible tower is running on port 80. Whenever I try to access xx.xx.xx.xx/netdata, it shows me this,</p>
<p><a href=""https://i.sstatic.net/7z4ay.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7z4ay.jpg"" alt=""enter image description here"" /></a></p>
<p>meaning that request never reaches nginx.</p>
<p>I tried running nginx on port other than 80, like 81, but it keeps on loading and request is never completed.</p>
<p>How can I access my servers hosted behind nginx ?</p>
","<nginx><ansible><ansible-tower>","2017-06-29 15:04:04"
"998915","Netmask for point to point ip address?","<p>In a point to point tunnel, I was able to have the same IP twice. It looks like a bug, I think that might be related to some internal representation of the netmask. It can be reproduced with the steps below.</p>

<p>I created a tun tunnel like this:</p>

<pre><code>ip tuntap add dev tun3 mode tun user alice group alice
ip address add 10.8.0.1 peer 10.8.0.2/32 dev tun3
</code></pre>

<p>I listed the address:</p>

<pre><code>$ ip address list dev tun3
…
inet 10.8.0.1 peer 10.8.0.2/32 scope global tun3
…
</code></pre>

<p>As you see, there seems to be no netmask associated with 10.8.0.1. But ifconfig reports /32:</p>

<pre><code>$ ifconfig tun3
…
inet 10.8.0.1  netmask 255.255.255.255  destination 10.8.0.2
…
</code></pre>

<p>If I use <code>iproute2</code> to add the same address 10.8.0.1/32, there is no error, the address is added and then listed.</p>

<pre><code># ip address add 10.8.0.1/32 dev tun3
$ ip address list dev tun3
…
inet 10.8.0.1 peer 10.8.0.2/32 scope global tun3
…
inet 10.8.0.1/32 scope global tun3
…
</code></pre>

<p>The question is, why? I was expecting a:</p>

<pre><code>    RTNETLINK answers: File exists
</code></pre>

<p>In case you wonder why I would try this, I was trying to help with <a href=""https://serverfault.com/questions/998907/same-ip-twice-in-the-same-interface"">this question</a>.</p>
","<ip><linux-networking><tunneling><ifconfig><tunnel>","2020-01-14 14:38:08"
"858595","Install php 5.6.5 in ubuntu 16.04","<p>I need help in installing php 5.6.5 in my ubuntu 16.04 for my Magento 2.1 website.</p>

<p>I tried installing php through these commands:</p>

<pre><code>sudo apt-get -y update
sudo add-apt-repository ppa:ondrej/php
sudo apt-get -y update
apt-get -y install php5.6 php5.6-mcrypt php5.6-mbstring php5.6-curl php5.6-cli php5.6-mysql php5.6-gd php5.6-intl php5.6-xsl php5.6-zip
</code></pre>

<p>But when i do <code>php -v</code> it gives me this version:</p>

<pre><code>PHP 5.6.30-12~ubuntu16.04.1+deb.sury.org+1 (cli)
Copyright (c) 1997-2016 The PHP Group
Zend Engine v2.6.0, Copyright (c) 1998-2016 Zend Technologies
</code></pre>

<p>Is there a way to upgrade 5.6.30 to 5.6.5?</p>

<p>Thanks!</p>
","<php><ubuntu-16.04><magento>","2017-06-29 16:14:59"
"1032912","How can I Change Password of Cisco (C3560-IPBASEK9-M) Switch with Command Line?","<p>I can make remote (ssh) connection to my Cisco (C3560-IPBASEK9-M) switch on putty and I can log in with username and password.
How can I change password -that I use when I log in- on this command line? ( I'm very beginner, I made some search but couldn't find it)</p>
","<cisco><switch><password><putty>","2020-09-07 12:27:41"
"999135","Connect two local networks","<p>I'm trying to connect two local networks together.</p>
<p><strong>Network 1</strong> (192.168.1.1) is connected to 10GBe Internet via router with dhcp. I've added switch Netgear XS708T for more 10G ports (Clients 1 and 2).</p>
<p>Via this switch I've connected a wifi router with DHCP (network 2).</p>
<p><strong>Network 2</strong> (192.168.2.1) is for wifi devices only (Client A, B etc.)</p>
<p><strong>Question 1:</strong> Is it possible / How to configure XS708T to connect both networks so Client 1 or Client 2 would see Client A or any client from network 2?</p>
<p><strong>Question 2:</strong> Do I need any other device to make it happen?</p>
<p>Thanks
Tom from <a href=""https://serwer.io/en/"" rel=""nofollow noreferrer"">Serwer.io</a></p>
","<router><wifi><local-area-network><netgear><10gbethernet>","2020-01-15 21:53:28"
"928182","Will apt-get upgrade modify symlinks? For example libphp7 symlinked instead of libphp5 under /etc/alternatives","<p>I performed sudo apt-get upgrade. 
$ sudo apt-get upgrade</p>

<p>After the upgrade, I noticed some php 5.7 files and folders in the etc directory 
For example: I saw /usr/bin/php7.1 and /etc/php/7.0, 7.1, 7.2, 7.3</p>

<p>I'm not sure why 7.1 files are installed as I didn't intend to do that and the php app doesn't work with PHP7.</p>

<p><strong>I'm not sure after the upgrade if /etc/alternatives/php got symlinked to /usr/bin/php7.1 how do I confirm if upgrade modifies symlinks?</strong></p>

<p>So, I performed to be sure 5.6 modules/extensions is loading and not 5.7<br>
$ sudo update-alternatives --set php /usr/bin/php5.6</p>

<p>I then checked the symlinks:
I checked for symlinks</p>

<pre><code>ubuntu@ip:/etc/alternatives$ ls -alth | grep php
lrwxrwxrwx   1 root root  15 Aug 27 14:05 php -&gt; /usr/bin/php5.6
lrwxrwxrwx   1 root root  31 Aug 27 14:05 php.1.gz -&gt; /usr/share/man/man1/php5.6.1.gz
lrwxrwxrwx   1 root root  16 Aug 25 02:21 phpize -&gt; /usr/bin/phpize5
lrwxrwxrwx   1 root root  32 Aug 25 02:21 phpize.1.gz -&gt; /usr/share/man/man1/phpize5.1.gz
lrwxrwxrwx   1 root root  20 Aug 25 02:21 php-config -&gt; /usr/bin/php-config5
lrwxrwxrwx   1 root root  36 Aug 25 02:21 php-config.1.gz -&gt; /usr/share/man/man1/php-config5.1.gz
lrwxrwxrwx   1 root root  21 Mar 16  2017 libphp7 -&gt; /usr/lib/libphp7.1.so
</code></pre>

<p>and found libphp7 is symlinked to 7.1.
Should it matter?
<strong>We don't want 7.1 modules or extensions to load as we are on php5.6 but will symlink libphp7.1 create any problems in the future? if so how to fix this and get back 5.6.37?</strong></p>

<p>Thanks!</p>
","<linux><php><php-fpm><php5><php7>","2018-08-27 15:49:57"
"928191","Automated server side rebuild of Domino applications","<p>We have a strange problem where XPages stop working (like they would not exist) almost every day. The solution is the rebuild the app with Domino Designer. I'm quite tired of doing that many times a week.</p>

<p>I have not found a solution and I'm thinking about automaticaly rebuilding the applications in server side every day. So far I haven't figured out how to do that. Is it possible and how?</p>

<p><em>Domino 9.0.1FP6 on Windows 64bit</em></p>

<p><strong>Update:</strong></p>

<p>Found this ""command line building"":</p>

<p><a href=""https://www.ibm.com/support/knowledgecenter/SSVRGU_9.0.1/user/wpd_srcctrl_headless_command_line.html"" rel=""nofollow noreferrer"">https://www.ibm.com/support/knowledgecenter/SSVRGU_9.0.1/user/wpd_srcctrl_headless_command_line.html</a></p>

<p>It's close but looks like it will create a new NSF from source files:</p>

<blockquote>
  <p>file name - A string value containing the name of the NSF/NTF to be
  created.</p>
</blockquote>

<p>I need to rebuild an existing NSF file.</p>
","<ibm-domino>","2018-08-27 17:27:15"
"928215","Active Directory Migration from 2003 to 2012: authentications from 2012, 02 works","<p>We have a SBS with one 2003 server for AD. Migration to 2012 went fine. Followed the instructions <a href=""https://blogs.technet.microsoft.com/canitpro/2014/04/01/step-by-step-active-directory-migration-from-windows-server-2003-to-windows-server-2012-r2/"" rel=""nofollow noreferrer"">here</a>,
(just for info purposes)
but did not yet demote <code>dcpromo</code> on the 2003 server. We are still running fileshare on it as well so need to keep it running for now.  And after doing all these steps, when the 2003 server rebooted, I noticed 2012 ins't allowing anyone to log in. Any client machine enters credentials and the login window just spins without error. As long as DC2003 is up, everything is good, but dns is configured right on both machines, and i am getting ips from dc2003, just nothing is authenticating with 2012. I looked further,
I checked dhcp and it was not running on 2003 server, it was on a separate 2008 server, and that was still going. So i removed DHCP role from 2012.
but that shouldn't be stopping anyone from logging in when dc2003 is down right? I cant test until much later next week, so im looking for any insight/possibilities anyone can advise on.</p>
<h1>only dc2003 providing authentication after migration to 2012</h1>
","<windows><active-directory><windows-server-2012><windows-server-2003>","2018-08-27 14:33:40"
"928279","How to set the Ipv6 vor a domain without subdomain","<p>In my DNS setup, I set a record for </p>

<pre><code>* 138.my.ip.here
</code></pre>

<p>this works for <code>spacetrace.org</code> as for <code>www.spacetrace.org</code></p>

<p>Now I tried the same with an AAAA record for IPv6:</p>

<pre><code>* 2a01:4f8:171:27a2:0:77:77:101
</code></pre>

<p>but this seems only work for all subdomains, including <code>www</code>, but not the main domain.</p>

<p>I used this check: <a href=""https://ip6.nl/#!spacetrace.org"" rel=""nofollow noreferrer"">https://ip6.nl/#!spacetrace.org</a></p>

<p>How do I set this right for the main domain?</p>

<p>This is my entry so far:</p>

<p><a href=""https://i.sstatic.net/7jbii.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7jbii.png"" alt=""enter image description here""></a></p>
","<domain-name-system><ipv6>","2018-08-28 07:16:09"
"928330","Changes in fs.* after rebooting?","<p>After rebooting my machine, <code>sysctl -a</code> gave me different outputs (some of changes like <code>kernel.sched_domain.cpu0.domain0.max_newidle_lb_cost</code> is expected):</p>

<pre><code>16c16
&lt; fs.dentry-state = 37641       15280   45      0       0       0
---
&gt; fs.dentry-state = 407249      384656  45      0       0       0
19,22c19,22
&lt; fs.file-max = 19473815
&lt; fs.file-nr = 624      0       19473815
&lt; fs.inode-nr = 36402   297
&lt; fs.inode-state = 36402        297     0       0       0       0       0
---
&gt; fs.file-max = 19473810
&gt; fs.file-nr = 864      0       19473810
&gt; fs.inode-nr = 285420  308
&gt; fs.inode-state = 285420       308     0       0       0       0       0
</code></pre>

<p>But I don't quite understand why fs related settings have changed given I didn't change <code>/etc/sysctl.conf</code> (nor changing <code>sysctl</code> indirectly).</p>

<p><em>update</em>
Especially the dramatic increases in <code>nr_inodes</code> and  the total number of directory cache entries (first value in <code>fs.dentry-state</code>).  It is not like I have created 390K+ directories after rebooting.</p>

<p>Any idea or suggestion is highly appreciated.</p>
","<centos7><filesystems><sysctl>","2018-08-28 13:04:11"
"786956","All domain admin passwords do not work on orphaned DC","<p>I have a remote DC that has been unable to completely replicate AD for at least 90 days so has been tombstoned, had their site-to-site link severed, and has been purged from the organization's main AD infrastructure. This is fine, we do not wish to fix this or bring their now-local AD back into the fold; the office will move forward as its own separate AD office. We were in the middle of preparing for an AD migration when their link to the rest of AD failed, however no changes were made yet. </p>

<p>Currently, none of the domain admin credentials that we have work on that DC. I have 5 different domain admins who swear they know their password, and know what their password was the last time AD replicated successfully. None of us are able to log in, nor can we use the master Enterprise Admin account, which is a password that has definitely not changed between when everything was working correctly and now. </p>

<p>It is not possible that all our domain admins do not remember their password. The passwords do not have an expiry on them, and it is not that we're being forced to change our password, the local DC just thinks they're wrong. The local admin says he hasn't done anything to their DC while we were working on preparing for the AD migration, and his account only had administrative rights over their users and their OU so he shouldn't have been able to change any of our passwords or anything similar. </p>

<p>So how could this have happened? Does booting into DSRM on a DC do something to all domain admin accounts if you bung it up? AKA, did our local guy try to take matters into his own hands and break something? Is it possible that the server was compromised in some way that would cause this?</p>

<p>The only other server on-site is a domain joined file server which local administrator credentials still work to access, so it isn't a huge problem. </p>

<p>The existing DC is Server 2008 R2 as is the existing file server, and all computers in the office are on fully updated Windows 7 machines. </p>

<p><strong>Further relevant details</strong>: We have one global domain in our forest that has remote domain controllers in multiple offices, each office is an AD site. One of those offices is separating from our global infrastructure. Before we could properly migrate them out of our domain, their ISP collapsed and their internet connection was lost. Since that site is not coming back to our control, it was removed from the domain. To assist with them migrating to their own domain we were going to run ADMT, but since their local DC cannot be authenticated against with admin rights, we cannot run ADMT. </p>

<p>User profiles are workstation local with a home drive stored on the file server which we have administrator access to, so manually moving data is an option. </p>

<p>I have several thoughts/questions about how to proceed:</p>

<ol>
<li>I can restore their site and DC and go through the whole un-tombstoning process, but this is a significant amount of work just to get them to a point where we can remove them from AD again. </li>
<li>We can scrap their entire local AD and get them to deploy a new one, and migrate all the computers to a new environment, but this is also a significant amount of work.</li>
<li>If we decide to go with option 2 our biggest concern will be getting user profiles migrated from the current AD to the new one. Is there a tool that can assist with this given that administrative access to the current domain is non-existent? Or is this basically going to just be, create the new AD, join the PC, initialize the new user profile and copy data from the old profile to the new one and reconfigure accounts?</li>
<li>Is there a better way of doing this that I'm not thinking of? All the tools I know of to assist with something <em>like</em> this require administrative access to the source AD. </li>
</ol>
","<active-directory><windows-server-2008-r2><windows-7>","2016-06-29 16:24:57"
"859242","SMTP Error 451 4.1.0","<p>I'm adressing to you guys because I'm having a problem when sending an e-mail to a specific adress, I'm getting this message : </p>

<blockquote>
  <p>Diagnostic-Code: smtp; 451 4.1.0 Recipient unknown</p>
</blockquote>

<p>I looked for what it could be but couldn't find what the 4.1.0 code stands for . When I browse about the ""Recipient unknown"" the answers I find are about the SMTP error 500 which looks pretty confusing to me. At this point i'm trying to understand what the 4.1.0 code means for SMTP error 451 , and try to guess why I'm having this error.</p>

<p>Any help here would be higly appreciated !</p>
","<smtp>","2017-07-04 09:12:00"
"787020","Run shell script on the event of ""possible SYN flooding""","<p>I'd like to write a script that gets all the stats I need (top IPs, used memory, netstat, etc) at the time I got an SYN flooding, and write to a report file.</p>

<p>So, is it possible to trigger a script/command when the kernel alerts for ""possible SYN flooding on port XXX"" ?</p>

<blockquote>
  <p>Jun 27 22:12:21 xxxx kernel: [xxxx.xxxx] possible SYN flooding on port
  443. Sending cookies. Jun 27 22:13:22 xxxx kernel: [xxxx.xxxx] possible SYN flooding on port 443. Sending cookies. Jun 27 22:14:25
  xxxx kernel: [xxxx.xxxx] possible SYN flooding on port 443. Sending
  cookies.</p>
</blockquote>
","<linux><tcp><ddos><scalability><syn>","2016-06-29 21:14:33"
"787041","sendmail : closing port 25","<p>Nmaping my server I got theses results :</p>

<pre><code>PORT      STATE    SERVICE
25/tcp    filtered smtp
80/tcp    open     http
</code></pre>

<p>On the server-side :</p>

<pre><code>netstat -lpn
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      2301/sendmail-mta     
tcp        0      0 127.0.0.1:587           0.0.0.0:*               LISTEN      2301/sendmail-mta
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      1803/apache2 
</code></pre>

<p>I would like to close sendmail from listening on the internet connected interface, to be protected if a security issue is discovered later for example.</p>
","<security><port><sendmail>","2016-06-30 01:04:38"
"999498","Does record domain ""include:"" also refers to subdomains?","<p>I have a doubt setting up my SPF record. I would like to know if I set up an include record in the SPF record will also be ""including"" the subdomains of that principal domain that I have included?.</p>

<p>Let's take a look of an example in my doubt:</p>

<p>Example:</p>

<pre><code>v=spf1 +include:example.com -all
</code></pre>

<p>Does this ""+include:example.com"" will be covering up also if my mail is sent from subdomain1.example.com or subdomain2.example.com ???</p>

<p>I really appreciate your comments.</p>
","<domain-name-system><spf><dns-zone><dns-hosting><dmarc>","2020-01-18 21:46:17"
"999514","Can one have 2 Domains pointing to the same IP and / or 2 IP's resolving to the same Domain?","<p>I am going to attempt to rewrite the initial question so that it makes more sense. </p>

<p>The issue I was being faced with is that I have my Company's website hosted on Site Ground but unfortunately because of the site builder the use, they are unable to transfer is under a different Addon Domain in that domains Document Root. The reason I wanted to have it moved, was because I finally registered the  Domain Name that matches my Company name. Currently that website was being accessed from a temporary Domain Name which I never planned to keep as the Domain name for my Company..</p>

<p>To avoid confusion, my Company's Name is Systopian Solutions. My Public Company Branded home page was being temporarily (or so I thought) run under the Document Root for the Domain Name ""systopian.com"". I had fully planned that once I got my proper Company Domain Name ""systopiansolutions.com"" that I could add that Domain as another Addon Domain in Site Ground, and then have the actual contents of the site transferred from one Addon Domain to another, both on Site Grounds Hosting. </p>

<p>However, in my initial thinking that the site could be transferred, and before I opened the original question that was here, even if that transferred it, I was left with a slight problem. </p>

<p>I also have e VPS Server Hosted by Vultr. This Server runs all of my Company's Public Facing Invoicing, Billing, and Payment Processing Web App Site (Invoice Ninja) as well as a couple Internal Web App Sites which allow me to provide different services to my Client, </p>

<p>SIDE-NOTE:(and while I am aware, you should never run Public Sites, and Internal Sites on the save server, I have implemented a number of steps to insure that no one will be able to go snooping around. For 1. I have set up Aliases for all of my Internal Sites URLs, so that not one of them uses the commonly known URLs, should they know those Sites are there. 2, Invoice Ninja has a very robust security implementation, that will not allow anyone to choose any fqdn/dir unless it is allowed by Invoice Ninja for the functionality of that product, however, by using the /etc/phpmyadmin/apache2.conf, which is formatted completely different from the way a /etc/apache2/sites-available/site-name.conf file is configured. The syntax is completely different, but for reasons I can not explain, if I need to install a new site, rather then using the site-name.conf which will cause Invoice Ninja to reject access to any of the DocumentRoot paths, I use the  /etc/phpmyadmin/apache2.conf and goto the bottom under the phpMyAdmins Config where I then create a new tag section using the markers and place the appropriate paths, mods, paths, and all other config required by the new site, save, then restart apache2. I do however take the security one step further since we are mixing Public and Internal sites on the same server, by adding the use of the .htpasswd and forcing who may try to access some internal site they may find if they poke hard enough, they are Required to the credentials for 1 of the only 2 users which are configured in the .htpasswd. Should they satisfy those credentials some how, the Web Apps Sites themselves each have their own user management / Login credentials required to access those sites. While I am of no illusion that what I have done is impermeable, I can assure you that it would take one VERY determined hacker, which years of experience to hack into the internal sites. There are enough layers, and the passwords chosen, are VERY Strong with a minimum Char length and the right combo of #'s, Special Chars, Upper/Lower Chars)</p>

<p>The slight problem I had was that I wanted my VPS Server which was hosting some of my Company's Public Sites, such as invoice ninja, to use the new Company Domain of systopiansoltions.com.  In the case where the transfer of my Company's Home Page had been transferred to the new Addon Domain on my Site Ground Host, that would mena that I would have to different Servers, that would somehow need to use the systopiansoltions.com domain. And this is what I was asking, is there a way where I can have my site hosted by Site Ground, which the DNS A Record would have a different IP Address for the A Record to point the systopiansoltions.com domain to the Site Ground IP of 37.60.253.172 while somehow also pointing the Same DNS Servers A Record for my VPS Servers Sites to the IP Addr of my VPS sites which is 207.246.87.29. </p>

<p>The initial questions was more or less, without having any kind of a load balancer or router setup, is there anyway to have the same FQDN pointing in some ways to 2 different IP addresses. </p>

<p>Some thoughts I had that could possibly help me do this in the cleanest way possible, would be somehow to setup redirects, though I am not sure how that would work. </p>

<p>So this was the detailed explanation of my problem and question.</p>

<p>There was a second part to this question, which I mistakenly asked with the incorrect information, so I will ask that as well...</p>

<p>Since the VPS Host contains both Internal Sites and Public Customer Sites, I was wondering if there is a way to configure 2 different Domain Names each to a site based it's function...</p>

<p>Example: On the VPS server, the /etc/hosts had the following entries, (I am not including the ""localhost"" entries). And this is how it was configured up until today. The problem is that now that I registered the proper Company Domain Name, I would love to be able to have the systopian-web1.com Domain Name used for all of the Internal Sites, while somehow configuring the Public Customer Facing Invoice Ninja Site to use the systopiansoltions.com domain, however, I don't have a 3rd IP to usem and frankly, I am short of any kind of any Operational Budget for a while, so I can not afford to add a 3rd IP which would make this problem disappear all together. </p>

<blockquote>
  <p>207.246.87.29 systopian-web1.com  &lt;--Primary IP</p>
  
  <p>45.76.165.28 unms.systopian-web1.com &lt;-- Secondary IP Added specifically for this app install as it required a dedicated IP Address.</p>
</blockquote>

<p>So, my question is, is there anyway to use 1 IP with 2 different Domain Names? Of does anyone know of some other way to accomplish this? </p>

<p>I do know that in my apache2/sites-available/invoice-ninja.conf I can change the following entries:
ServerName systopiansolutions.com 
ServerAlias www.systopiansolutions.com 
However, my initial thought was that in order to specify either of these two, the Domain Name needed to be defined in the /etc/hosts. Is that not the case? I am not sure how else these Domains would get resolved, since I know that I can not have two A Records where 2 Domain Names would point to the same IP address
ie have stopiansolutions.com resolve to 207.246.87.29 and then have my other domain name on the same VPS Server systopian-web1.com point to the Same IP 207.246.87.29, unless I was able to update the /etc/hosts contain the following</p>

<blockquote>
  <p>207.246.87.29 systopian-web1.com  &lt;-- This Domain would be set for the internal websites that are used by me to provided functionality and services for my clients, but the Public does not have access to these sites</p>
  
  <p>207.246.87.29 stopiansolutions.com  &lt;-- This Domain would be set for Invoice Ninja, since this is a Public Customer Facing Site, so I want all Public Customer facing sites, to use this domain.</p>
  
  <p>45.76.165.28 unms.systopian-web1.com</p>
</blockquote>

<p>And just to remind you, that what I had asked earlier is still an open question, and that is, since my Public Company's Home Page is currently hosted on Site Ground, would there be any way to use the same Domain Name on two completely different servers each of which has a very different IP?** I may have resolved this another way, but the question still stands.</p>

<p>Short of that, I only have one other option, but I will explain that in a rewrite of my own reply, if I can modify that, if not, I will add another Reply.</p>

<p>I have one friendly request, especially when you guys reply to someone who you can clearly see if very green on using this site. Rather then respond the ways some of you did, which honestly had pushed me to the point that I was going to just leave the site, as I felt that the responses showed complete lack of tolerance towards those who are new, and are not yet all to familiar with the way things are here, and I was fairly ignorant to the expectations that some of you seemed to have regarding to how questions are posted, etc, but instead, try to be welcoming, and either send them a private message, or reply to their post and share with them the link that was later shared with me, after it was made clear to me that Green members, and their not well written questions, are anything but tolerated. This would have given me quite the incentive to have taken the time that I am now to rewrite the question. Sometimes people get so wrapped up in themselves, that they forget how far a small gesture of kindness goes. </p>

<p>If not for one of the other members taking the time to try and make sense of my question, and doing his best to provide me with a different way to consider my issue, I don't know that I would have ever return back again, which would have been a shame because, like many of you, I have 2 decades of Tech Support, Application Support Engineering, Systems Administration, Web Services, and Foreign Systems Integration &amp; Migration Engineering Architect . I am quite sure at some point, I would have been able to share some of my many years of experience to assist others that do not have the Skill Levels that I do in many areas, and I love to help people.  So while I am not trying to beat a dead horse, I would just ask that you approach new members with a little bit of forgiveness and acceptance, and rather then drop the reputation of the first post they make, try to point the in the right direction (as was done again, well after the fact) so they can learn how to write the most effective post, rather then making them feel unwelcome, and alienated from the membership. Hopefully, both my questions and what I just explained makes a difference and hopefully, the next person will not feel as rejected as I. </p>

<p>If anyone has any questions regarding the questions I posed as rewritten, please feel free to ask.</p>

<p>Cheers,</p>

<p>John</p>
","<apache-2.4><ip><vps><hosts><fqdn>","2020-01-19 08:47:31"
"787187","Unexpected Blue Screen on Windows Server 2012","<p>We are starting to have random reboots of our Windows Server 2012 machine. Looking through the information we have, we found a BlueScreen event that correlates with the time of the reboot. Searching around for the BCCode 3b mentions driver problems, but our drivers seem to be working correctly. Does anyone have experience with this code and found a solution to it? Below are the details from the BlueScreen event. </p>

<blockquote>
<pre><code>Problem Event Name: BlueScreen
OS Version: 6.2.9200.2.0.0.272.7
BCCode: 3b
BCP1: 00000000C0000005
BCP2: FFFFF96000220373
BCP3: FFFFF8804EC84BA0
BCP4: 0000000000000000
OS Version: 6_2_9200
Service Pack: 0_0
Product: 272_3
</code></pre>
</blockquote>
","<windows-server-2012>","2016-06-30 15:41:01"
"1033371","Dumpcap - Ram still reserved after stop","<p>System: Win 10 Pro and latest updates (Sept. 2020) 64Bit.</p>
<p>Problem: Using dumpcap (Dumpcap (Wireshark) 3.2.5 (v3.2.5-0-ged20ddea8138)) with a ringbuffer reserves RAM even after closing the dumpcap.exe. The PC needs to be restarted to free the mem. Login off doesn't do it.</p>
<p>Dumpcap command:</p>
<p>Exe: C:\Program Files\Wireshark\dumpcap.exe</p>
<p>Arguments:  -i &quot;Interface&quot; -w &quot;ValidLogFolder&quot; -b files:20 -b filesize:600</p>
<p>Filesize does not really mather, even with a 2GByte Filesize i get the same behaviour. I chekced if i have files * filesize * [KByte] ram free. The data is saved on a ssd.</p>
<p>Before starting the dumpcap.exe the RAM usage is around 4GByte. When i let it run (10Gbit Interface with 8Gbit/s traffic) the usage crawls up. In the following picture is the RAM usage after i stopped the dumpcap.exe. The usage stays where i stop the dumpcap.</p>
<p><a href=""https://i.sstatic.net/EMXpt.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EMXpt.png"" alt=""enter image description here"" /></a></p>
<p>Also, the ringbuffer is not overwriting existing files, its adding new files. I Tried canging the order of the commands.</p>
<p>What is going on here?</p>
<p>I want to make a ringbuffer to make a long term monitoring of the massive traffic.</p>
<p>Edit:
I opened an issue on Gitlab:
<a href=""https://gitlab.com/wireshark/wireshark/-/issues/16846"" rel=""nofollow noreferrer"">https://gitlab.com/wireshark/wireshark/-/issues/16846</a></p>
","<windows><wireshark>","2020-09-10 14:23:04"
"999657","Windows updates stay as ""pending restart"" and eventually fail with error code 80246013","<p><a href=""https://i.sstatic.net/dpreo.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dpreo.png"" alt=""screenshot of the update history""></a></p>

<p>It's a Windows Server 2012 (Build 9200). The machine restarts daily at 03:00 trying to install the pending updates, but the updates stay in the ""Pending restart"" state (see background window in the screenshot). Eventually, they fail with error code 80246013 (see foreground window in the screenshot).</p>

<p>Things I have tried (without success):</p>

<ul>
<li><p>Windows Update Troubleshooter (<a href=""http://go.microsoft.com/?linkid=9830262"" rel=""nofollow noreferrer"">WindowsUpdateDiagnostic.diagcab</a>). Says that it fixed issues, but the problem persists.</p></li>
<li><p><code>DISM.exe /Online /Cleanup-image /Restorehealth</code> followed by <code>sfc /scannow</code>.</p></li>
<li><p>Made sure the BITS, Cryptographic Service and Windows Update Service are running.</p></li>
<li><p>Manually reset the update store by renaming <code>C:\Windows\SoftwareDistribution</code> while wuauserv is stopped.</p></li>
<li><p>No, I don't have (and never had) any third-party anti-virus or security solution installed on the server.</p></li>
</ul>

<p>I also tried manually downloading and installing the Servicing Stack update. It ends with the <em>""Installation complete. You must restart your computer for the updates to take effect.""</em> window, but after the restart - you guessed it - it's still not installed.</p>

<p>I checked C:\Windows\Logs\CBS\CBS.log, but it contains ~30,000 entries for the last 10 minutes, those relating to the service stack update look OK (as far as I can tell), and I don't really know what to look for in there.</p>

<p>Is there anything else I can do?</p>
","<windows-server-2012><windows-update>","2020-01-20 14:42:22"
"1033491","/etc/ folder missing on VM","<p>Apologies if Iam at wrong place to post my issue.</p>
<p>I've spin a CentOS VM on GCP. Some how I</p>
<pre><code>root@mygit090820 ~]# cd ~/etc/
-bash: cd: /root/etc/: No such file or directory
</code></pre>
","<google-cloud-platform>","2020-09-11 10:27:41"
"928785","Nginx configuration for multiple domains and same application","<p>I have the following scenario:</p>

<p>A single application that will serve several different domains, on a single server, and a single context of a single Tomcat instance. (The DNS of all domains are configured and functional)</p>

<p>That is, I need it when the browser points to the address:</p>

<p><code>domain1.example</code> >> redirect to >> <a href=""http://127.0.0.1/websiteapp"" rel=""nofollow noreferrer"">http://127.0.0.1/websiteapp</a></p>

<p><code>domain2.example</code> >> redirect to >> <a href=""http://127.0.0.1/websiteapp"" rel=""nofollow noreferrer"">http://127.0.0.1/websiteapp</a></p>

<p><code>domainn.com.br</code> >> redirect to >> <a href=""http://127.0.0.1/websiteapp"" rel=""nofollow noreferrer"">http://127.0.0.1/websiteapp</a></p>

<p>In the application it is retrieved (via request URL) the calling domain, that is, it is my client identifier and through the domain I get the information in the database and return the configured views to that client.</p>

<p>I configured the NGINX (minimum configuration) like this:</p>

<pre><code>server {
      listen          80;
      server_name     domain1.example domain2.example domainn.com.br;
      root /opt/tomcat2/webapps/websites;

      location / {
            proxy_pass http://127.0.0.1:8080/websitesapp;
            proxy_set_header  Host $http_host;
            proxy_set_header X-Forwarded-Host $host;
            proxy_set_header X-Forwarded-Server $host;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;


      }
}
</code></pre>

<p>Tomcat server.xml</p>

<pre><code>&lt;Context docBase=""websitesapp"" path=""/"" reloadable=""true"" source=""org.eclipse.jst.jee.server:websitesapp""/&gt;
</code></pre>

<p>In the application I retrieve the domain, and ""mount"" the a view with the information loaded from the database.</p>

<p>Is this setting correct?</p>

<p>Is there any way to pass the domain to tomcat more securely, ie without retrieving it by the browser's URL?</p>

<p>Is there any contraindication to this type of approach?</p>

<p>Any other approach that would be more interesting?</p>
","<nginx><tomcat><tomcat8>","2018-08-30 21:34:33"
"787443","Diagnosing Linux OOM Logs","<p>Repeatedly facing mysqld out of memory. Lots of PHP processes spawned, taking up all the swap. </p>

<pre>
[562074.326710] mysqld invoked oom-killer: gfp_mask=0x201da, order=0, oom_adj=0, oom_score_adj=0
[562074.329646] mysqld cpuset=/ mems_allowed=0
[562074.330882] Pid: 21351, comm: mysqld Not tainted 2.6.32-642.1.1.el6.x86_64 #1
[562074.333171] Call Trace:
[562074.334870]  [] ? dump_header+0x90/0x1b0
[562074.336812]  [] ? security_real_capable_noaudit+0x3c/0x70
[562074.338692]  [] ? oom_kill_process+0x82/0x2a0
[562074.340270]  [] ? select_bad_process+0xe1/0x120
[562074.341582]  [] ? out_of_memory+0x220/0x3c0
[562074.343229]  [] ? __alloc_pages_nodemask+0x93c/0x950
[562074.344930]  [] ? ext4_get_block+0x0/0x120 [ext4]
[562074.346436]  [] ? alloc_pages_current+0xaa/0x110
[562074.347604]  [] ? __page_cache_alloc+0x87/0x90
[562074.348889]  [] ? find_get_page+0x1e/0xa0
[562074.350660]  [] ? filemap_fault+0x1a7/0x500
[562074.352207]  [] ? __do_fault+0x54/0x530
[562074.353872]  [] ? handle_pte_fault+0xf7/0xb20
[562074.355966]  [] ? rwsem_down_failed_common+0x95/0x1d0
[562074.357531]  [] ? rwsem_down_read_failed+0x26/0x30
[562074.359503]  [] ? handle_mm_fault+0x299/0x3d0
[562074.361105]  [] ? call_rwsem_down_read_failed+0x14/0x30
[562074.362522]  [] ? __do_page_fault+0x146/0x500
[562074.364590]  [] ? lock_sock_nested+0xac/0xc0
[562074.387996]  [] ? _spin_unlock_bh+0x1b/0x20
[562074.389187]  [] ? pvclock_clocksource_read+0x58/0xd0
[562074.390457]  [] ? kvm_clock_read+0x1c/0x20
[562074.391586]  [] ? kvm_clock_get_cycles+0x9/0x10
[562074.392673]  [] ? getnstimeofday+0x58/0xf0
[562074.394697]  [] ? do_page_fault+0x3e/0xa0
[562074.395974]  [] ? page_fault+0x25/0x30
[562074.397218] Mem-Info:
[562074.399103] Node 0 DMA per-cpu:
[562074.400605] CPU    0: hi:    0, btch:   1 usd:   0
[562074.402129] CPU    1: hi:    0, btch:   1 usd:   0
[562074.403506] Node 0 DMA32 per-cpu:
[562074.405629] CPU    0: hi:  186, btch:  31 usd:   0
[562074.406749] CPU    1: hi:  186, btch:  31 usd:  30
[562074.407914] Node 0 Normal per-cpu:
[562074.409325] CPU    0: hi:  186, btch:  31 usd:   0
[562074.410787] CPU    1: hi:  186, btch:  31 usd:  30
[562074.411941] active_anon:717855 inactive_anon:182000 isolated_anon:5083
[562074.411943]  active_file:32 inactive_file:192 isolated_file:64
[562074.411944]  unevictable:0 dirty:0 writeback:259 unstable:0
[562074.411944]  free:21353 slab_reclaimable:5762 slab_unreclaimable:10906
[562074.411945]  mapped:63 shmem:43 pagetables:21560 bounce:0
[562074.418218] Node 0 DMA free:15724kB min:248kB low:308kB high:372kB active_anon:0kB inactive_anon:0kB active_file:0kB inactive_file:0kB unevictable:0kB isolated(anon):0kB isolated(file):0kB present:15312kB mlocked:0kB dirty:0kB writeback:0kB mapped:0kB shmem:0kB slab_reclaimable:0kB slab_unreclaimable:0kB kernel_stack:0kB pagetables:0kB unstable:0kB bounce:0kB writeback_tmp:0kB pages_scanned:0 all_unreclaimable? yes
[562074.424382] lowmem_reserve[]: 0 3512 4017 4017
[562074.426602] Node 0 DMA32 free:61060kB min:58868kB low:73584kB high:88300kB active_anon:2667432kB inactive_anon:524160kB active_file:4kB inactive_file:396kB unevictable:0kB isolated(anon):15980kB isolated(file):384kB present:3596500kB mlocked:0kB dirty:0kB writeback:560kB mapped:232kB shmem:172kB slab_reclaimable:15184kB slab_unreclaimable:17264kB kernel_stack:6736kB pagetables:66680kB unstable:0kB bounce:0kB writeback_tmp:0kB pages_scanned:64 all_unreclaimable? no
[562074.432920] lowmem_reserve[]: 0 0 505 505
[562074.434773] Node 0 Normal free:8392kB min:8464kB low:10580kB high:12696kB active_anon:203604kB inactive_anon:203616kB active_file:124kB inactive_file:320kB unevictable:0kB isolated(anon):4992kB isolated(file):128kB present:517120kB mlocked:0kB dirty:0kB writeback:476kB mapped:20kB shmem:0kB slab_reclaimable:7864kB slab_unreclaimable:26360kB kernel_stack:6832kB pagetables:19560kB unstable:0kB bounce:0kB writeback_tmp:0kB pages_scanned:32 all_unreclaimable? no
[562074.441562] lowmem_reserve[]: 0 0 0 0
[562074.443251] Node 0 DMA: 3*4kB 0*8kB 0*16kB 1*32kB 1*64kB 0*128kB 1*256kB 0*512kB 1*1024kB 1*2048kB 3*4096kB = 15724kB
[562074.449972] Node 0 DMA32: 492*4kB 4530*8kB 511*16kB 140*32kB 89*64kB 3*128kB 1*256kB 1*512kB 1*1024kB 1*2048kB 0*4096kB = 60784kB
[562074.455851] Node 0 Normal: 1455*4kB 68*8kB 3*16kB 2*32kB 0*64kB 1*128kB 1*256kB 1*512kB 1*1024kB 0*2048kB 0*4096kB = 8396kB
[562074.461844] 22462 total pagecache pages
[562074.463320] 21989 pages in swap cache
[562074.464571] Swap cache stats: add 34726956, delete 34704967, find 25405257/28809084
[562074.466666] Free swap  = 0kB
[562074.467793] Total swap = 4194252kB
[562074.508867] 1048575 pages RAM
[562074.510066] 67960 pages reserved
[562074.511918] 7658 pages shared
[562074.513082] 947230 pages non-shared
[562074.514178] [ pid ]   uid  tgid total_vm      rss cpu oom_adj oom_score_adj name
[562074.516236] [  490]     0   490     2680        0   1     -17         -1000 udevd
[562074.518561] [  790]     0   790     2662        0   1     -17         -1000 udevd
[562074.520752] [ 1342]     0  1342    62368       50   1       0             0 rsyslogd
[562074.522842] [ 1357]     0  1357     4563       23   0       0             0 irqbalance
[562074.525130] [ 1376]    25  1376    59678      129   0       0             0 named
[562074.527174] [ 1394]    81  1394     5359        1   0       0             0 dbus-daemon
[562074.529698] [ 1432]     0  1432     1020        0   0       0             0 acpid
[562074.532021] [ 1451]    28  1451   336791      177   0       0             0 nscd
[562074.534053] [ 1569]     0  1569    16086       24   1     -17         -1000 sshd
[562074.536171] [ 1580]    38  1580     6262       27   1       0             0 ntpd
[562074.539282] [ 2053]     0  2053     4662        1   0       0             0 dovecot
[562074.542072] [ 2065]   401  2065    10639        1   0       0             0 pop3-login
[562074.545036] [ 2066]   401  2066    10637        1   0       0             0 imap-login
[562074.547647] [ 2067]    97  2067     3368        1   0       0             0 anvil
[562074.550029] [ 2068]     0  2068     3401        1   1       0             0 log
[562074.552190] [ 2070]   401  2070    10655        1   0       0             0 pop3-login
[562074.554224] [ 2071]   401  2071    10670        1   0       0             0 imap-login
[562074.556886] [ 2072]     0  2072     3659        1   0       0             0 config
[562074.560112] [ 2082]    47  2082    18238        1   1       0             0 exim
[562074.562160] [ 2100]     0  2100    49742      335   1       0             0 /usr/local/cpan
[562074.564197] [ 2102]     0  2102    53119      223   0       0             0 spamd child
[562074.566209] [ 2129]     0  2129    45352        1   0       0             0 abrtd
[562074.568273] [ 2139]     0  2139    45197        1   0       0             0 abrt-dump-oops
[562074.570489] [ 2150]     0  2150    19189      124   0       0             0 httpd
[562074.572607] [ 2165]     0  2165    34191        0   0       0             0 pure-ftpd
[562074.574719] [ 2167]     0  2167    34036        1   0       0             0 pure-authd
[562074.577609] [ 2260]     0  2260    29271       27   1       0             0 crond
[562074.580212] [ 2275]     0  2275     4857        5   1       0             0 atd
[562074.582301] [ 2375]     0  2375    26952      125   0       0             0 cpsrvd (SSL) - 
[562074.584483] [ 2381]     0  2381     8172       69   0       0             0 queueprocd - wa
[562074.586500] [ 2391]     0  2391    27506       53   0       0             0 dnsadmin - serv
[562074.589509] [ 2419]     0  2419    20822      258   0       0             0 tailwatchd
[562074.592045] [ 2433]     0  2433    19586        1   0       0             0 cPhulkd - proce
[562074.594295] [ 2476]     0  2476    33028       60   0       0             0 cpdavd - accept
[562074.597210] [ 2512]     0  2512     5915      102   1       0             0 cpanellogd - sl
[562074.600503] [ 2540]     0  2540     1016        1   1       0             0 mingetty
[562074.603689] [ 2542]     0  2542     1016        1   0       0             0 mingetty
[562074.610302] [ 2545]     0  2545     1016        1   1       0             0 mingetty
[562074.615154] [ 2547]     0  2547     2679        0   0     -17         -1000 udevd
[562074.617989] [ 2548]     0  2548     1016        1   1       0             0 mingetty
[562074.622920] [ 2550]     0  2550     1016        1   1       0             0 mingetty
[562074.625853] [ 2552]     0  2552     1016        1   1       0             0 mingetty
[562074.628530] [30499]     0 30499   147473       23   0       0             0 clamd
[562074.631934] [ 8712]     0  8712     2394        1   1       0             0 mysqld_safe
[562074.634865] [ 9408]   402  9408   564397    42943   0       0             0 mysqld
[562074.637620] [24553]     0 24553    27283        1   1       0             0 sshd
[562074.641062] [24562]     0 24562    13893        1   0       0             0 sftp-server
[562074.643785] [25296]     0 25296    27283        1   1       0             0 sshd
[562074.646513] [25312]     0 25312    26649        1   0       0             0 bash
[562074.649571] [25954]    99 25954    19222       60   0       0             0 httpd
[562074.652579] [25955]     0 25955    19061        1   0       0             0 leechprotect
[562074.655407] [25958]    99 25958   334134      795   0       0             0 httpd
[562074.657912] [25959]    99 25959   334193      806   1       0             0 httpd
[562074.660551] [25960]    99 25960   334438      802   0       0             0 httpd
[562074.663110] [26044]    99 26044   334439      803   0       0             0 httpd
[562074.665618] [26206]    99 26206   334191      844   1       0             0 httpd
[562074.668698] [26361]    99 26361   334134      886   0       0             0 httpd
[562074.671241] [28324]     0 28324    27283        1   1       0             0 sshd
[562074.673759] [28333]     0 28333    26649        1   0       0             0 bash
[562074.676257] [28546]    99 28546   334167      821   0       0             0 httpd
[562074.702428] [ 1091]     0  1091    27283        1   1       0             0 sshd
[562074.704538] [ 1130]     0  1130    26649        1   0       0             0 bash
[562074.706652] [ 5276]   500  5276    77474     7801   1       0             0 php
[562074.708866] [ 5278]   500  5278    77582     7649   0       0             0 php
[562074.711662] [ 5281]   500  5281    77580     7259   1       0             0 php
[562074.714235] [ 5290]   500  5290    77601     7474   1       0             0 php
[562074.716490] [ 5295]   500  5295    77538     7540   1       0             0 php
[562074.718824] [ 5299]   500  5299    77414     7551   1       0             0 php
[562074.721384] [ 5302]   500  5302    77518     8104   1       0             0 php
[562074.724114] [ 5304]   500  5304    77414     7522   1       0             0 php
[562074.726653] [ 5308]   500  5308    77582     7241   0       0             0 php
[562074.729482] [ 5309]   500  5309    77542     7294   0       0             0 php
[562074.731758] [ 5367]   500  5367    79215     8223   0       0             0 php
[562074.734948] [ 5369]   500  5369    79230     7073   1       0             0 php
[562074.738000] [ 5389]   500  5389    74670     6863   1       0             0 php
[562074.740609] [ 5407]   500  5407    74996     5832   0       0             0 php
[562074.742809] [ 5415]   500  5415    75850     6233   1       0             0 php
[562074.745813] [ 5416]   500  5416    75061     5948   1       0             0 php
[562074.748265] [ 5418]   500  5418    74992     5796   1       0             0 php
[562074.750694] [ 5420]   500  5420    76190     7332   0       0             0 php
[562074.753004] [ 5430]   500  5430    74932     6113   1       0             0 php
[562074.755384] [ 5434]   500  5434    75460     6480   1       0             0 php
[562074.757639] [ 5439]   500  5439    74928     5879   1       0             0 php
[562074.761562] [ 5440]   500  5440    75125     5701   1       0             0 php
[562074.764306] [ 5442]   500  5442    75914     6693   1       0             0 php
[562074.766600] [ 5446]   500  5446    75214     5475   1       0             0 php
[562074.768955] [ 5448]   500  5448    74705     6309   1       0             0 php
[562074.771372] [ 5450]   500  5450    73976     6564   0       0             0 php
[562074.773778] [ 5451]   500  5451    73831     6459   1       0             0 php
[562074.776178] [ 5458]   500  5458    74611     7603   0       0             0 php
[562074.778666] [ 5459]   500  5459    75781     8240   1       0             0 php
[562074.781649] [ 5465]   500  5465    73772     7310   1       0             0 php
[562074.784412] [ 5470]   500  5470    74414     7797   1       0             0 php
[562074.788349] [ 5476]   500  5476    74864     8214   1       0             0 php
[562074.790625] [ 5480]   500  5480    73510     8366   0       0             0 php
[562074.793613] [ 5482]   500  5482    73708     8698   0       0             0 php
[562074.796689] [ 5492]   500  5492    72444    11027   1       0             0 php
[562074.799184] [ 5496]   500  5496    72784    12415   1       0             0 php
[562074.802301] [ 5499]   500  5499    73768    11388   0       0             0 php
[562074.804993] [ 5500]   500  5500    72582    11302   0       0             0 php
[562074.808597] [ 5503]   500  5503    70752     9687   0       0             0 php
[562074.810941] [ 5513]   500  5513    73447    10543   0       0             0 php
[562074.813474] [ 5519]   500  5519    72728    12005   1       0             0 php
[562074.815596] [ 5520]   500  5520    72710    11134   1       0             0 php
[562074.818435] [ 5523]   500  5523    72246    11313   0       0             0 php
[562074.821278] [ 5533]   500  5533    73576    12705   0       0             0 php
[562074.824227] [ 5536]   500  5536    70305    11674   1       0             0 php
[562074.826292] [ 5541]   500  5541    73378    13119   1       0             0 php
[562074.828638] [ 5545]   500  5545    69769    10932   1       0             0 php
[562074.831461] [ 5548]   500  5548    69382    10296   0       0             0 php
[562074.833803] [ 5549]   500  5549    73458    13978   1       0             0 php
[562074.836589] [ 5564]   500  5564    69708    11383   1       0             0 php
[562074.839962] [ 5568]   500  5568    69380    12267   0       0             0 php
[562074.842304] [ 5570]   500  5570    69580    13324   1       0             0 php
[562074.844533] [ 5574]   500  5574    69057    13993   0       0             0 php
[562074.847082] [ 5577]   500  5577    69450    13875   1       0             0 php
[562074.849309] [ 5578]   500  5578    70048    14829   1       0             0 php
[562074.851653] [ 5587]   500  5587    69514    15274   1       0             0 php
[562074.854140] [ 5589]   500  5589    69382    16196   1       0             0 php
[562074.856537] [ 5593]   500  5593    68012    14763   0       0             0 php
[562074.858820] [ 5597]   500  5597    71343    18344   0       0             0 php
[562074.861408] [ 5598]   500  5598    66115    12838   1       0             0 php
[562074.864285] [ 5603]   500  5603    69319    16732   1       0             0 php
[562074.866510] [ 5612]   500  5612    63573    11969   0       0             0 php
[562074.868753] [ 5613]   500  5613    61574     9964   1       0             0 php
[562074.871009] [ 5616]   500  5616    63637    12150   0       0             0 php
[562074.874032] [ 5624]   500  5624    63726    12707   1       0             0 php
[562074.876563] [ 5625]   500  5625    55276     4013   1       0             0 php
[562074.879156] [ 5630]   500  5630    52951     2211   0       0             0 php
[562074.881476] [ 5631]   500  5631    53736     3070   1       0             0 php
[562074.883968] [ 5633]   500  5633    53245     2474   0       0             0 php
[562074.886305] [ 5634]   500  5634    54533     3868   1       0             0 php
[562074.888585] [ 5635]   500  5635    54650     3839   0       0             0 php
[562074.890872] [ 5636]   500  5636    56802     6104   0       0             0 php
[562074.893228] [ 5637]   500  5637    54189     3375   1       0             0 php
[562074.895530] [ 5638]   500  5638    54563     3901   0       0             0 php
[562074.897784] [ 5639]   500  5639    52667     1958   1       0             0 php
[562074.900122] [ 5640]   500  5640    54533     3711   0       0             0 php
[562074.902475] [ 5641]   500  5641    52922     2197   0       0             0 php
[562074.904697] [ 5642]   500  5642    52667     1954   1       0             0 php
[562074.907013] [ 5643]   500  5643    52958     2226   0       0             0 php
[562074.909381] [ 5644]   500  5644    52667     1927   0       0             0 php
[562074.911628] [ 5645]   500  5645    54015     3402   1       0             0 php
[562074.913914] [ 5646]   500  5646    52951     2165   0       0             0 php
[562074.916614] [ 5647]   500  5647    54635     3863   1       0             0 php
[562074.919109] [ 5648]    99  5648   219408      400   1       0             0 httpd
[562074.921510] [ 5649]   500  5649    53928     3331   1       0             0 php
[562074.924011] [ 5650]   500  5650    56653     5958   0       0             0 php
[562074.926651] [ 5651]   500  5651    52951     2223   0       0             0 php
[562074.929157] [ 5652]   500  5652    52735     1997   1       0             0 php
[562074.954567] [ 5653]   500  5653    52457     1707   1       0             0 php
[562074.957251] [ 5654]    99  5654   235792      481   1       0             0 httpd
[562074.959480] [ 5660]   500  5660    53928     3321   0       0             0 php
[562074.961682] [ 5686]   500  5686    54417     3563   0       0             0 php
[562074.964573] [ 5712]   500  5712    52958     2216   0       0             0 php
[562074.966804] [ 5713]   500  5713    53887     3247   0       0             0 php
[562074.969044] [ 5714]   500  5714    52951     2206   0       0             0 php
[562074.971840] [ 5715]   500  5715    53736     3103   1       0             0 php
[562074.974827] [ 5716]   500  5716    52663     1938   1       0             0 php
[562074.977527] [ 5717]   500  5717    56118     5385   0       0             0 php
[562074.980867] [ 5726]   500  5726    57172     6396   1       0             0 php
[562074.984658] [ 5727]   500  5727    52523     1760   0       0             0 php
[562074.987267] [ 5728]   500  5728    52587     1869   1       0             0 php
[562074.989641] [ 5730]   500  5730    52430     1711   1       0             0 php
[562074.992423] [ 5731]   500  5731    53672     3061   0       0             0 php
[562074.995140] [ 5732]   500  5732    52587     1860   1       0             0 php
[562074.998136] [ 5733]   500  5733    52770     2016   0       0             0 php
[562075.002104] [ 5734]   500  5734    53216     2537   1       0             0 php
[562075.004987] [ 5735]   500  5735    54351     3615   1       0             0 php
[562075.007912] [ 5737]   500  5737    54329     3665   0       0             0 php
[562075.010646] [ 5738]   500  5738    53864     3231   1       0             0 php
[562075.013388] [ 5750]   500  5750    53992     3332   1       0             0 php
[562075.016259] [ 5751]   500  5751    53992     3356   1       0             0 php
[562075.018239] [ 5754]   500  5754    52457     1707   0       0             0 php
[562075.020361] [ 5755]   500  5755    53672     3008   0       0             0 php
[562075.022769] [ 5758]   500  5758    53928     3334   1       0             0 php
[562075.024717] [ 5759]   500  5759    53800     3153   0       0             0 php
[562075.026820] [ 5760]   500  5760    52457     1715   1       0             0 php
[562075.029743] [ 5761]   500  5761    53672     2999   1       0             0 php
[562075.031868] [ 5767]   500  5767    52430     1709   0       0             0 php
[562075.033922] [ 5768]   500  5768    53672     3015   1       0             0 php
[562075.035960] [ 5772]   500  5772    52457     1708   1       0             0 php
[562075.038017] [ 5774]   500  5774    53928     3307   0       0             0 php
[562075.040342] [ 5777]   500  5777    52430     1709   0       0             0 php
[562075.042585] [ 5780]   500  5780    52922     2200   1       0             0 php
[562075.044900] [ 5781]   500  5781    29874      419   0       0             0 php
[562075.046949] [ 5782]   500  5782    52430     1710   0       0             0 php
[562075.049163] [ 5787]   500  5787    53736     3111   0       0             0 php
[562075.051348] [ 5788]   500  5788    52428     1681   0       0             0 php
[562075.053411] [ 5789]   500  5789    16226       45   1       0             0 php
[562075.055455] [ 5790]   500  5790    53992     3361   0       0             0 php
[562075.057385] [ 5796]   500  5796    53800     3145   0       0             0 php
[562075.059354] [ 5802]   500  5802    51933     1164   0       0             0 php
[562075.061403] [ 5803]   500  5803    53736     3088   1       0             0 php
[562075.063319] [ 5828]   500  5828    52457     1712   1       0             0 php
[562075.065328] [ 5830]   500  5830    53992     3362   1       0             0 php
[562075.067304] [ 5833]   500  5833    52457     1711   0       0             0 php
[562075.069272] [ 5834]   500  5834    52457     1710   0       0             0 php
[562075.071342] [ 5838]   500  5838    25576       65   0       0             0 php
[562075.073811] [ 5839]    99  5839   121104      248   1       0             0 httpd
[562075.075876] [ 5848]    99  5848   121104      279   1       0             0 httpd
[562075.078224] [ 5849]   500  5849    29230      282   1       0             0 php
[562075.080532] [ 5850]   500  5850    29118      217   1       0             0 php
[562075.082861] [ 5851]   500  5851    53352     2643   0       0             0 php
[562075.084933] [ 5852]    99  5852   121104      281   1       0             0 httpd
[562075.087091] [ 5854]   500  5854    52430     1708   0       0             0 php
[562075.089180] [ 5856]   500  5856    51703      882   0       0             0 php
[562075.091154] [ 5863]    99  5863   121104      286   1       0             0 httpd
[562075.093112] [ 5970]   500  5970    28797      186   1       0             0 php
[562075.095140] [ 5986]     0  5986     5651       60   0       0             0 upcp-running
[562075.097396] [ 5988]   500  5988    29118      223   0       0             0 php
[562075.099621] [ 5991]    99  5991   104720      241   0       0             0 httpd
[562075.101929] [ 5993]   500  5993     3116       43   0       0             0 suphp
[562075.104055] [ 5994]   500  5994    29329      408   0       0             0 php
[562075.106361] [ 6026]   500  6026    29118      223   0       0             0 php
[562075.108660] [ 6027]   500  6027    29118      231   1       0             0 php
[562075.111327] [ 6028]   500  6028     7365       23   1       0             0 php
[562075.113563] [ 6032]    99  6032   121104      279   1       0             0 httpd
[562075.115682] Out of memory: Kill process 9408 (mysqld) score 44 or sacrifice child
[562075.117786] Killed process 9408, UID 402, (mysqld) total-vm:2257588kB, anon-rss:171860kB, file-rss:544kB
</pre>

<p>This occurs only once or twice in the day. Otherwise the load is well within limits (&lt;2.00). The load reaches >100 when this error occurs. As per the access logs I wasn't getting more traffic when this happened. What should I do to fix this?</p>
","<linux><php><lamp>","2016-07-01 16:58:38"
"999797","aws life cycle rules to transition files from s3 bucket to Glacier storage class","<p>I am creating life cycle rules to move files from bucket to the glacier storage class.</p>

<p>I was wondering if there is an option to copy files from S3 bucket to Glacier using  either CLI or console?</p>

<p>I need to maintain a copy of files in S3 bucket and also in Glacier. Currently I have duplicated the folder in S3. Created life cycle rules to transition files in one folder to Glacier. </p>
","<amazon-web-services><amazon-s3><aws-cli><amazon-glacier><lifecycle>","2020-01-21 16:06:28"
"1033703","How to add credentials to bitlocker script","<p>I'm trying to create a script to see if the bitlocker works or not.
If it works, then a file is posted in the c:\ folder We will call &quot;yes&quot; if the bitlocker does not work then add password recovery and activate the bitlocker.
The script runs properly on computers but I need to run it from GPO as login, and not all users of mine are admin so I want to add to the script the fixed credentials so that the user doesn't have to do anything.</p>
<p>Until now I've tried to run the script in startup but it doesn't succeed because it's not admin (I know in startup he runs as a system and yet it doesn't work)
I also tried to run the script in Task Scheduler
But he's having trouble pulling the file from a shared folder.</p>
<p>I'm trying to run this script.</p>
<pre><code>PS Microsoft.PowerShell.Core\FileSystem::\\domin.com\SysVol\domin.com\Policies\{873EBCF2-C88A-4557-AAAB-F01EA2574A5E}\Machine\Scripts\Startup&gt;  $userName = &quot;domin\adminbitlocker&quot;
$password = ConvertTo-SecureString &quot;+Ab0p9o8i!&quot; -AsPlainText -Force
$credentials = New-Object Management.Automation.PSCredential $username, $password


$BLinfo =  Get-Bitlockervolume | Get-Credential -Credential $credentials
if ($BLinfo.mountpoint -eq 'c:' -and $BLinfo.ProtectionStatus -eq 'on' ) {
    Out-File  c:\yes.log
}

if ($BLinfo.mountpoint -eq 'c:' -and $BLinfo.ProtectionStatus -eq 'off' ) { 

    manage-bde.exe -protectors -add c: -rp 

    manage-bde.exe -on c: 

    Out-File  c:\no.log
}
</code></pre>
<p>And i get</p>
<pre><code>Get-CimInstance : Access denied 
At C:\windows\system32\WindowsPowerShell\v1.0\Modules\BitLocker\BitLocker.psm1:144 char:13
+             Get-CimInstance `
+             ~~~~~~~~~~~~~~~~~
    + CategoryInfo          : PermissionDenied: (root\cimv2\Secu...cryptableVolume:String) [Get-CimInstance], CimException
    + FullyQualifiedErrorId : HRESULT 0x80041003,Microsoft.Management.Infrastructure.CimCmdlets.GetCimInstanceCommand

Get-Win32EncryptableVolumeInternal :  does not have an associated BitLocker volume.
At C:\windows\system32\WindowsPowerShell\v1.0\Modules\BitLocker\BitLocker.psm1:696 char:42
+ ...       $AllWin32EncryptableVolume = Get-Win32EncryptableVolumeInternal
+                                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : NotSpecified: (:) [Write-Error], COMException
    + FullyQualifiedErrorId : System.Runtime.InteropServices.COMException,Get-Win32EncryptableVolumeInternal.
</code></pre>
<p>Thank you so much, everyone.
And please forgive my English, it's not the best.</p>
","<powershell><scripting><bitlocker>","2020-09-13 08:52:33"
"928969","pfsense dns doesn't work anymore for a specific domain","<p>I had a local webserver that was working fine and on which I had a website on domain example.com. I was also using a static public IP for the WAN with ISP #1. Now I decided to move the website to a remote hosting company so I changed the A records of my domain to point to that new server. The replication of the A record has been done about 2 weeks ago now.</p>

<p>My main local router is a pfSense v2.4.3 and I removed all the configurations I did for the old local server, but still, when I am inside this pfsense network, I can't access the site at the new location. I get DNS_PROBE_FINISHED_NXDOMAIN error. If I try outside of the network, it works fine. 
Recently I changed to ISP company #2 and went to a dynamic public IP. The ISP company came here, installed a new modem/router and put it in front of the pfSense router. If I try the website when connected to this very first router, everything work fine. So I know it's not a problem of the ISP DNS. So there is something in the pfSense that keeps something somewhere but can't find what it is.</p>

<p>What I tried :</p>

<p>I have disabled DNS Forwarder and DNS Resolver to force queries to bypass the pfSense DNS; </p>

<p>When enabling DNS Resolver, it shows ""unbound"" in the Status>Services page. Restarting the service doesn't change;</p>

<p>I tried putting Google DNS in the DNS Servers; </p>

<p>I did a reset of the connections states of the pfSense;</p>

<p>None of this worked.</p>

<p>On the pfSense, the DNS Lookup returns:  Host ""example.com"" could not be resolved. Also it seems (unconfirmed) that my domain cannot resolve for everyone. Someone outside of the local network told me that he could not access my domain, but at this time, I was also outside the pfSense network and could access it successfully. </p>
","<domain-name-system><pfsense><website>","2018-09-01 11:55:06"
"999890","how to make DNS zone file","<p>my domain is easydata.ir and i create a zone file for my domain but when i use command ""dig"" it return:</p>

<pre><code>; &lt;&lt;&gt;&gt; DiG 9.11.4-P2-RedHat-9.11.4-9.P2.el7 &lt;&lt;&gt;&gt; ns1.easydata.ir
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: SERVFAIL, id: 41395
;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 512
;; QUESTION SECTION:
;ns1.easydata.ir.               IN      A

;; Query time: 136 msec
;; SERVER: 8.8.8.8#53(8.8.8.8)
;; WHEN: Wed Jan 22 00:17:20 EST 2020
;; MSG SIZE  rcvd: 44
</code></pre>

<p>so I put my config file please tell me how to set it correctly.</p>

<pre><code>$TTL 86400;
@ IN SOA ns1.easydata.ir. root.easydata.ir. (
  2013042201 ;Serial
  3600 ;Refresh
  1800 ;Retry
  604800 ;Expire
  86400 ;Minimum TTL ) ; add nameservers
@ IN NS ns1.easydata.ir. 
@ IN NS ns2.easydata.ir. ;IN MX 10 mail.easydata.ir.
ns1 IN A 198.143.181.237
ns2 IN A 198.143.181.237
www IN A 198.143.181.237
ftp IN A 198.143.181.237
</code></pre>

<p>Hera is my /etc/named.conf file. please tell me if it is correct.</p>

<pre><code>options {
        listen-on port 53 { 127.0.0.1; 198.143.181.237;};
        #listen-on-v6 port 53 { ::1; };
        directory       ""/var/named"";
        dump-file       ""/var/named/data/cache_dump.db"";
        statistics-file ""/var/named/data/named_stats.txt"";
        memstatistics-file ""/var/named/data/named_mem_stats.txt"";
        recursing-file  ""/var/named/data/named.recursing"";
        secroots-file   ""/var/named/data/named.secroots"";
        allow-query     { localhost; 87.107.219.167; };

        /*
         - If you are building an AUTHORITATIVE DNS server, do NOT enable recursion.
         - If you are building a RECURSIVE (caching) DNS server, you need to enable
           recursion.
         - If your recursive DNS server has a public IP address, you MUST enable access
           control to limit queries to your legitimate users. Failing to do so will
           cause your server to become part of large scale DNS amplification
           attacks. Implementing BCP38 within your network would greatly
           reduce such attack surface
        */
        recursion no;

        dnssec-enable yes;
        dnssec-validation yes;

        /* Path to ISC DLV key */
        bindkeys-file ""/etc/named.root.key"";

        managed-keys-directory ""/var/named/dynamic"";

        pid-file ""/run/named/named.pid"";
        session-keyfile ""/run/named/session.key"";
};

logging {
        channel default_debug {
                file ""data/named.run"";
                severity dynamic;
        };
};

zone ""."" IN {
        type hint;
        file ""named.ca"";
};

include ""/etc/named.rfc1912.zones"";
include ""/etc/named.root.key"";

zone ""easydata.ir"" IN {
                type master;
                file ""easydata.ir.zone"";
                allow-update { none; };
        };
</code></pre>
","<domain-name-system><bind>","2020-01-22 08:17:00"
"787614","Apache doesn't start after virtualhost setup (CentOS7)","<p>I setup virtualhost file in <code>/etc/httpd/conf.d/vhost.com.conf</code> like the example 
and check my file and addresses many times</p>

<pre><code>&lt;VirtualHost *:80&gt;
  ServerName www.vhost.com
  ServerAlias vhost.com
  DocumentRoot /var/www/html/vhost.com/public_html/
  ErrorLog /var/www/html/vhost.com/logs/error.log
  CustomLog /var/www/html/vhost.com/logs/access.log combined
&lt;/VirtualHost&gt;
</code></pre>

<p>but my Apache  doesn't start it show the following error:</p>

<pre><code>[seconduser@VPS- ~]$ sudo systemctl status httpd.service -l
● httpd.service - The Apache HTTP Server
   Loaded: loaded (/usr/lib/systemd/system/httpd.service; enabled; vendor preset: disabled)
   Active: failed (Result: exit-code) since Sun 2016-07-03 11:44:58 IRDT; 54s ago
     Docs: man:httpd(8)
           man:apachectl(8)
  Process: 7214 ExecStop=/bin/kill -WINCH ${MAINPID} (code=exited, status=1/FAILURE)
  Process: 7212 ExecStart=/usr/sbin/httpd $OPTIONS -DFOREGROUND (code=exited, status=1/FAILURE)
 Main PID: 7212 (code=exited, status=1/FAILURE)

Jul 03 11:44:58 VPS- systemd[1]: Starting The Apache HTTP Server...
Jul 03 11:44:58 VPS- httpd[7212]: AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using fe80::250:56ff:fea3:6805. Set the 'ServerName' directive globally to suppress this message
Jul 03 11:44:58 VPS- systemd[1]: httpd.service: main process exited, code=exited, status=1/FAILURE
Jul 03 11:44:58 VPS- kill[7214]: kill: cannot find process """"
Jul 03 11:44:58 VPS- systemd[1]: httpd.service: control process exited, code=exited status=1
Jul 03 11:44:58 VPS- systemd[1]: Failed to start The Apache HTTP Server.
Jul 03 11:44:58 VPS- systemd[1]: Unit httpd.service entered failed state.
Jul 03 11:44:58 VPS- systemd[1]: httpd.service failed.
</code></pre>

<p>as soon as i remove the <code>vhost.com.conf</code> everything go back to normal and server run again in another configure i add this code to my <code>vhost.com.conf</code> :</p>

<pre><code>&lt;VirtualHost *:80&gt;
  ServerName www.vhost.com
  ServerAlias vhost.com
  DocumentRoot /var/www/vhost.com/public_html/
  ErrorLog /var/www/vhost.com/logs/error.log
  CustomLog /var/www/vhost.com/logs/access.log combined
&lt;/VirtualHost&gt;
</code></pre>

<p>and send this command <code>sudo chmod -R 755 /var/www</code> and this <code>sudo chown -R apache:apache /var/www/html/www.vhost.com</code> I don't know what do they I just saw them on internet.</p>

<p><strong>EDIT</strong>- I checked my error log <code>/var/log/httpd/error_log</code> and get this error : <code>(13)Permission denied: AH00091: httpd: could not open error log file /var/www/vhost.com/logs/error.log.
AH00015: Unable to open logs</code> what can I do?</p>

<p>I do all of these because I need to seprate my subdomain.</p>
","<apache-2.4><virtualhost><subdomain><centos7>","2016-07-03 07:25:43"
"1000031","Adding a private nameserver to public DNS zone for a subdomain","<p>We have a DNS zone setup on Azure which resolves all of our records on our base domain (eg: <code>example.com</code>). We also have an internal DNS server using BIND and are currently duplicating the records on the public and private DNS.</p>

<p>I am trying to set up the DNS to use the private DNS for subdomain <code>*.pvt.example.com</code> and use the public DNS for <code>*.example.com</code></p>

<p>To test this I have created an entry in the main zone file on the private DNS server</p>

<pre><code>testhost.pvt IN      A       172.16.17.18
</code></pre>

<p>On Azure, I have added an NS record for <code>pvt.example.com</code> pointing to my private DNS server IP.</p>

<p>I am able to resolve <code>testhost.pvt.example.com</code> when using the private DNS, but this does not work from the public DNS.</p>

<p>Is there any other network setting that needs to be done? (The private DNS is not accessible from the internet)</p>
","<domain-name-system><bind><internal-dns><dns-zone><azure-dns>","2020-01-23 00:49:07"
"929195","Is ssl CName important for search engine?","<p>My site is under cloudflare and of course I am using free ssl from their side.</p>

<p>So, what what can I have sanctions on the site from the search engines while using free ssl? </p>

<p>Or what benefits will I have after dedicated ssl?</p>

<p><img src=""https://i.sstatic.net/Hkzru.jpg"" alt=""image""></p>
","<ssl><ssl-certificate><cloudflare><seo>","2018-09-03 12:25:05"
"1034029",".sh working on terminal but not on cronjob","<p>I have created a shell file to extract an archive from .7z. The shell works fine in the terminal but in the cronjob I get an error.</p>
<p>The command is:</p>
<pre><code>7z x /path/to/file.7z -aoa
</code></pre>
<p>The error I receive is:</p>
<pre><code>/path/to/script.sh: line 2: 7z: command not found
</code></pre>
<p>I am using the full path to the .7z file and I checked my permissions so I am breaking my head to find out what I am doing wrong.</p>
<p>I changed my script to</p>
<pre><code>#!/bin/bash
/usr/local/bin/7z x /path/to/file.7z -aoa
</code></pre>
<p>I now get</p>
<pre><code>/bin/sh: /path/to/script.sh: Permission denied
</code></pre>
<p>I have checked the script, file and 7z files and all permissions are set to read/write</p>
<p>OK.. So I fixed the permission issue by using chmod +x on the script.</p>
","<mac-osx><cron><archive>","2020-09-15 19:44:11"
"929368","GPO-Restrict Internet Access","<p>I would like to restrict internet access for domain users for the company that I'm working for.</p>

<p>I'm used at doing that with Internet Explorer using Group Policy, but am unable to do so because my server does not have available to me the <strong>Windows Component</strong> node with the relevant policies to configure the lockdown:</p>

<p><img src=""https://i.sstatic.net/ZrSfk.jpg"" alt=""server admin template""></p>

<p>How can I restrict internet access via Internet Explorer on client PCs using Group Policy?</p>
","<windows>","2018-09-04 11:51:08"
"929388","Resolv to CNAME not to A record","<p>At the data center of the university I work, we have a DNS system, that we use for registering the names of our servers and the addresses of the services that are meant for internal use only.</p>

<p>Usually, we use the following pattern:</p>

<pre><code>server     A        xxx.xxx.xxx.xxx
service    CNAME    server
</code></pre>

<p>Now we have a service that we want to resolv to the CNAME record only, when accessing via HTTP, and not to the A record.</p>

<p>Is that possible, using the DNS configuration only?</p>
","<domain-name-system><internal-dns>","2018-09-04 13:24:57"
"861314","Restoring to a blank Cisco switch","<p>If you need to restore over a blank Cisco switch with a config backup taken using TFTP. I hear you can do this using copy and paste from the output given from <strong>show running-configuration</strong> on the live switch but that requires console access in order not to get cut off half way through the restore. On the old switches that console access requires a serial to RJ45 console cable, which some people don't have (or their PC has no serial ports).</p>
","<backup><cisco><switch><restore>","2017-07-10 15:27:28"
"1000911","Problems resolving hostnames on LAN in CentOS","<p>I have a CentOS 7.7 server up and running, which I am trying to access from other CentOS machines on LAN. Unfortunately, I am not able to reach the hostname from other machines. How can I solve this problem?</p>

<p><strong>Server (CentOS 7.7):</strong></p>

<pre><code>$ hostname
centos77-server-1.localdomain
</code></pre>

<p><strong>Client (CentOS 7.7):</strong></p>

<pre><code># ping centos77-server-1.localdomain
PING localhost (127.0.0.1) 56(84) bytes of data.
64 bytes from localhost (127.0.0.1): icmp_seq=1 ttl=64 time=0.089 ms
64 bytes from localhost (127.0.0.1): icmp_seq=2 ttl=64 time=0.104 ms
64 bytes from localhost (127.0.0.1): icmp_seq=3 ttl=64 time=0.115 ms
64 bytes from localhost (127.0.0.1): icmp_seq=4 ttl=64 time=0.053 ms
</code></pre>

<p>As you can see my pinging is resolving as <code>127.0.0.1</code> which does not look right, as the server is on 192.168.3.59.</p>

<p>Is there a network configuration gone wrong on my router not allowing hostnames to be reached internally?</p>

<p><a href=""https://i.sstatic.net/8AwbF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8AwbF.png"" alt=""enter image description here""></a></p>
","<centos><centos7><ping><hostname>","2020-01-29 14:36:36"
"1034582","Windows 10 computer won't update","<p>My PC (Windows 10 2004) will happily install all updates provided via the company WSUS.
But (in contrast to all other company client PCs) it fails to instal updates from Microsoft directly.</p>
<p>It keeps saying that &quot;important security and quality fixes are missing&quot; (which is the reason why I wanted to check directly with MS in the first place), but when I click &quot;Search for updates online with Microsoft&quot;, it fails with a message that no connection with the update service could be made. I should check my internet connectivity (as if ...)</p>
<p>Windows troubleshooting for updates says that it cannot find any problems.</p>
<p>A (tiny) bit more helpful is the accompanying event: Id 36871, source SChannel, with internal error status 10013. Google results for this suggest various things such as</p>
<ul>
<li>enable SSLv3</li>
<li>Make .NET 3.5 use TLS1.2</li>
<li>clear system certificate list</li>
<li>check clock synchronization</li>
<li>check permissions to c:\ProgramData\Microsoft\Crypto\RSA\MachineKeys</li>
</ul>
<p>None of these helped in my case.
What else can I do? Is there a less generic interpretation of the event?</p>
<p>I also tried to use Wireshark and see what happens during the failed attempts. It seems as if a few https (TLS1.2) connections with slscr.update.microsoft.com are being established successfully, but with merely about 64 bytes of payload. By their nature, I can't look deeper into the conversations.</p>
","<ssl><windows-10><windows-update>","2020-09-19 20:14:20"
"788097","Can't telnet to memcache server","<p>I'm trying to connect the application server to the memcache server. </p>

<pre><code>App Server 12.12.12.12
Memcache Server 13.13.13.13
Memcache Port 11211
</code></pre>

<p>I've tried the following. </p>

<pre><code>Ping from 12.12.12.12 to 13.13.13.13 works
telnet 13.13.13.13 80 works
telnet 13.13.13.13 11211 doesn't work: No route to host found. 
</code></pre>

<p>Memcache is listening to 127.0.0.1 and therefore the UFW on 13.13.13.13 is set to: </p>

<pre><code>To                         Action      From
--                         ------      ----
11211/tcp                  ALLOW       Anywhere
22                         ALLOW       Anywhere
Anywhere                   ALLOW       13.13.13.13
127.0.0.1 11211/tcp        ALLOW       13.13.13.13
</code></pre>

<p>Still the server is not responding and result is No route or host found. </p>

<p>UFW logging is enabled but the ufw log can't be found at /var/log/ufw.log</p>

<p>No I'm at the point of being clueless... </p>

<p>[UPDATE]
Output of netstat -tunlp | grep 11211</p>

<pre><code>tcp        0      0 13.13.13.13:11211       0.0.0.0:*               LISTEN      14378/memcached
udp        0      0 13.13.13.13:11211       0.0.0.0:*                           14378/memcached
</code></pre>

<p>ps aux | grep memcache</p>

<pre><code>memcache 14378  0.0  0.0 335580  1440 ?        Sl   15:56   0:00 /usr/bin/memcached -m 3584 -p 11211 -u memcache -l 13.13.13.13
</code></pre>
","<telnet><memcache>","2016-07-05 19:30:02"
"1034761","How to route traffic between L2TP/IPSec and WireGuard tunnels?","<p>I have a KVM VPS running Ubuntu 18.04 which is simultaneously:</p>
<ol>
<li>L2TP server (xl2tpd + strongswan) with IP <code>192.168.42.1/24</code></li>
<li>Wireguard client with IP <code>192.168.73.3/24</code> (server's IP is <code>192.168.73.1/24</code>)</li>
</ol>
<p>Both L2TP and Wireguard connections work pretty well separately from each other.</p>
<p>I want to allow to redirect the traffic from L2TP clients to Wireguard server, i.e. <code>192.168.42.x &lt;===&gt; 192.168.73.1</code></p>
<p>Wireguard config on VPS:</p>
<pre><code>~# cat /etc/wireguard/wg0.conf 
[Interface]
PrivateKey = &lt;....&gt;
Address = 192.168.73.3/24

[Peer]
PublicKey = &lt;...&gt;
Endpoint = &lt;....&gt;
AllowedIPs = 192.168.73.0/24
PersistentKeepalive = 15
</code></pre>
<p>L2TP server has been set up using <a href=""https://github.com/hwdsl2/setup-ipsec-vpn"" rel=""nofollow noreferrer"">this awesome script</a>. It creates following iptables rules:</p>
<pre><code>~# iptables --list-rules
-P INPUT ACCEPT
-P FORWARD ACCEPT
-P OUTPUT ACCEPT
-A INPUT -p udp -m udp --dport 1701 -m policy --dir in --pol none -j DROP
-A INPUT -m conntrack --ctstate INVALID -j DROP
-A INPUT -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A INPUT -p udp -m multiport --dports 500,4500 -j ACCEPT
-A INPUT -p udp -m udp --dport 1701 -m policy --dir in --pol ipsec -j ACCEPT
-A INPUT -p udp -m udp --dport 1701 -j DROP
-A FORWARD -m conntrack --ctstate INVALID -j DROP
-A FORWARD -i ens3 -o ppp+ -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -i ppp+ -o ens3 -j ACCEPT
-A FORWARD -s 192.168.42.0/24 -d 192.168.42.0/24 -i ppp+ -o ppp+ -j ACCEPT
-A FORWARD -j DROP
</code></pre>
<p>IP forwarding is obviously enabled</p>
<pre><code>~# sysctl net.ipv4.ip_forward
net.ipv4.ip_forward = 1
</code></pre>
<p>Routing table (with 1 L2TP client connected) is:</p>
<pre><code>~# netstat -rn
Kernel IP routing table
Destination     Gateway         Genmask         Flags   MSS Window  irtt Iface
0.0.0.0         X.X.X.X         0.0.0.0         UG        0 0          0 ens3
XXX.XXX.XXX.XXX 0.0.0.0         255.255.255.0   U         0 0          0 ens3
X.X.X.X         0.0.0.0         255.255.255.255 UH        0 0          0 ens3
192.168.42.10   0.0.0.0         255.255.255.255 UH        0 0          0 ppp0
192.168.73.0    0.0.0.0         255.255.255.0   U         0 0          0 wg0
</code></pre>
<p><em>XXX - confidential gateway and external IPs.</em></p>
<p>I've tried to add following rules:</p>
<pre><code>~# iptables -D FORWARD -j DROP
~# iptables -A FORWARD -i ppp+ -o wg0 -j ACCEPT    
~# iptables -A FORWARD -i ppp+ -o wg0 -j ACCEPT
~# iptables -A FORWARD -j DROP
</code></pre>
<p>But forwarding <code>ppp0 &lt;===&gt; wg0</code> still does not work.</p>
<p>Which iptables rules should I add to allow such kind of forwarding?</p>
","<ubuntu><iptables><l2tp><wireguard>","2020-09-21 16:15:29"
"788258","NRPE error when using check_nrpe","<p>I have 2 centos server</p>

<p>Server 1 - centos 6.8</p>

<p>Server 2 - centos 6.7</p>

<p>Originaly I had the issue on Server 1 but was able to fix the issue I had with the fix explained here </p>

<p><a href=""https://serverfault.com/questions/323233/nrpe-unable-to-read-output-but-why"">NRPE unable to read output, but why?</a></p>

<p>When I try to the same fix on Server 2 i get the following error as explained in 
the above link.</p>

<pre><code>root@server2 [/usr/local/nagios/libexec]# ./check_nrpe -H 127.0.0.1 -c check_exim
NRPE: Unable to read output
</code></pre>

<p>But if I run the command locally it works</p>

<pre><code>root@server2 [/usr/local/nagios/libexec]#
/usr/local/nagios/libexec/check_exim_queue -c 20 -w 40 Mailqueue OK -
0 messages on queue
</code></pre>

<p>So to test I ran it as the user itself and it works</p>

<pre><code>nagios@cloud-03 [/usr/local/nagios/libexec]# sudo /usr/local/nagios/libexec/check_exim_queue -c 20 -w 40
Mailqueue OK - 0 messages on queue
</code></pre>

<p>Any ideas will help</p>
","<nagios><nrpe>","2016-07-06 14:20:34"
"929574","How configure Bind (DNS) server to respond internet requests","<p>I'm new to Bind so please forgive any ""silly"" questions.
I've configured a Bind Server on CentOS-7, Server name CentOS-DC (10.64.33.115).
I've also configured 3 CentOS-7 Clients</p>

<ul>
<li>CentOS-App-01 10.60.161.99</li>
<li>CentOS-DB-01 10.60.161.169</li>
<li>CentOS-Web-01 10.60.161.229</li>
</ul>

<p>Everything resolves correctly internally.</p>

<p>The issue arises when I try browsing the Web.
I've added in forwarder addresses to /etc/named.conf but am still unable to browse the web.</p>

<p>Below is my /etc/named.conf file</p>

<pre><code>//
// named.conf
//
// Provided by Red Hat bind package to configure the ISC BIND named(8) DNS
// server as a caching only nameserver (as a localhost DNS resolver only).
//
// See /usr/share/doc/bind*/sample/ for example named configuration files.
//
// See the BIND Administrator's Reference Manual (ARM) for details about the
// configuration located in /usr/share/doc/bind-{version}/Bv9ARM.html

options {
    listen-on port 53 { 127.0.0.1; 10.64.33.115;}; ### Master DNS IP ###
        listen-on-v6 port 53 { ::1; };
        directory   ""/var/named"";
        dump-file   ""/var/named/data/cache_dump.db"";
        statistics-file ""/var/named/data/named_stats.txt"";
        memstatistics-file ""/var/named/data/named_mem_stats.txt"";
        allow-query     { localhost; 10.60.161.0/24; };
        allow-transfer  { localhost; 10.64.33.115; };

        /*
         - If you are building an AUTHORITATIVE DNS server, do NOT enable recursion.
         - If you are building a RECURSIVE (caching) DNS server, you need to enable
           recursion.
         - If your recursive DNS server has a public IP address, you MUST enable access
           control to limit queries to your legitimate users. Failing to do so will
           cause your server to become part of large scale DNS amplification
           attacks. Implementing BCP38 within your network would greatly
           reduce such attack surface
        */
    recursion yes;
        allow-recursion {localhost; 10.60.161.0/24; };

        dnssec-enable yes;
        dnssec-validation yes;

        forwarders {
                10.73.240.235;
                152.62.196.10;
        };

    /* Path to ISC DLV key */
        bindkeys-file ""/etc/named.iscdlv.key"";

        managed-keys-directory ""/var/named/dynamic"";

        pid-file ""/run/named/named.pid"";
        session-keyfile ""/run/named/session.key"";
};

logging {
    channel default_debug {
                file ""data/named.run"";
                severity dynamic;
        };
};

zone ""CorkCompute.local"" IN {
        type master;
        file ""forward.CorkCompute"";
        allow-update { none; } ;
};

zone ""161.60.10"" IN {
        type master;
        file ""reverse.CorkCompute"";
        allow-update { none; } ;
};


zone ""."" IN {
    type hint;
        file ""named.ca"";
};

include ""/etc/named.rfc1912.zones"";
include ""/etc/named.root.key"";
</code></pre>

<p>thanks</p>
","<domain-name-system><centos7><bind>","2018-09-05 14:08:37"
"1034806","How to know the file or the directory which consumes all the space?","<p>I have a server with Ubuntu 20.04 the problem is that <code>disk/dev/vda1</code> partitiom is full. However, I have almost nothing to install.</p>
<p>How do you know which file or directory is consuming all the space ?</p>
<pre><code>ubuntu@pv-hdh87 ~ $ df -h
Filesystem      Size  Used Avail Use% Mounted on
udev            2.9G     0  2.9G   0% /dev
tmpfs           595M  1.1M  594M   1% /run
/dev/vda1        20G   18G  1.3G  94% /
tmpfs           3.0G     0  3.0G   0% /dev/shm
tmpfs           5.0M     0  5.0M   0% /run/lock
tmpfs           3.0G     0  3.0G   0% /sys/fs/cgroup
/dev/vda15      105M  9.1M   96M   9% /boot/efi
/dev/loop0       72M   72M     0 100% /snap/lxd/16099
/dev/loop1       55M   55M     0 100% /snap/core18/1880
/dev/vdb         98G  4.7G   89G   6% /home
tmpfs           595M     0  595M   0% /run/user/1000
tmpfs           595M     0  595M   0% /run/user/114
/dev/loop3       30M   30M     0 100% /snap/snapd/8790
/dev/loop4       56M   56M     0 100% /snap/core18/1885
/dev/loop5       71M   71M     0 100% /snap/lxd/16922
/dev/loop6       31M   31M     0 100% /snap/snapd/9279
</code></pre>
","<ubuntu><hard-drive><command-line-interface><ubuntu-18.04>","2020-09-22 00:23:57"
"1034810","Windows Server 2016 multi-factor authentication for RDP with Azure AD","<p>We want to require Multi-factor Authentication for RDP login (and local login) going forward on our Windows Server systems.  Currently all of our Windows Server systems are Windows Server 2016.  We are using Azure Active Directory free tier (but are open to upgrading if that is required). We do not want to use third-party products in the mix.</p>
<p>So ideally we would join the Windows Server 2016 systems to the Azure AD.  Then we would require Multi-factor authentication somehow.</p>
<p>We have not been able to find a simple guide on how to do this.  There are some questions on StackExchange about this topic, but there either are no answers, or the questions are multiple years old and say it cannot be done.</p>
<p>Since it is 2020, and there is also the new Windows Server 2019 and 2016 OSes available, and since Windows 10 desktop supports MFA I wanted to ask this question:</p>
<p>How to achieve Azure AD + MFA + Windows Server 2016 RDP login?</p>
","<windows-server-2016><rdp><entra-id><azure-mfa>","2020-09-22 01:10:34"
"1001819","Email Disappears between IMAP and POP","<p>I have a client who has an online store, and receives order notifications by email.</p>

<p>Their order emails are flowing into their mailbox on the server, but never show up in Outlook.</p>

<p>Their Outlook is configured as POP, I have reset the mailbox password to ensure no other connections, there are no rules created.
If I turn off POP auto-delete, I see the order notification email in the IMAP inbox, but (even with search) I cannot locate it in Outlook.</p>

<p>What else could it be/what else can I check?</p>
","<imap><pop3><outlook-2016>","2020-02-05 01:43:57"
"788279","Daily logrotate doesn't keep the old logs","<p>On a CentOS 6.x I created a simple service called <code>logtraffic</code> which, when started, appends the output of a <code>tcpdump</code> command to a log file at <code>/var/log/logtraffic/logtraffic.log</code>.</p>
<p>I want to achieve the following regarding that log:</p>
<ul>
<li><p>have that log rotated daily at midnight</p>
</li>
<li><p>keep the latest 7 logs and delete the older ones</p>
</li>
</ul>
<p>For this, I've done the following:</p>
<ul>
<li>commented the following line from /etc/anacrontab</li>
</ul>
<p><code>1      5       cron.daily              nice run-parts /etc/cron.daily</code></p>
<ul>
<li><p>added the following cron job:
<code>@midnight /etc/cron.daily/logrotate</code></p>
</li>
<li><p>created the file <code>/etc/logrotate.d/logtraffic</code> with the content:</p>
</li>
</ul>
<pre><code>/var/log/logtraffic/*log {
    daily
    rotate 7
    create
    dateext
    missingok
    notifempty
    sharedscripts
    postrotate
        /sbin/service logtraffic restart &gt; /dev/null 2&gt;/dev/null || true
    endscript
}
</code></pre>
<p>Looks like the log is properly truncated at midnight as the timestamps displayed in logtraffic.log prove that.</p>
<p>The problem is that no other older log file is present in that folder. I was expecting to have older logs with a suffix like YYYYMMDD but there are none.</p>
<p>What am I doing wrong?</p>
","<centos6><logrotate>","2016-07-06 15:24:08"
"1001901","Transitivity of trust when adding new domain","<p>Suppose I have a domain trust between domain A(forest A), and domain B(forest B) (bi-directional, FOREST TRANSITIVE).</p>

<p>Assuming I add domain C to forest A. At this point A trusts C, and B should trust C. But how does domain B knows about domain C?
What are the transactions? How does the configuration change ? Is new domain trust added by B? </p>

<p>Alternatively, if domain B doesn't know about C, it simply trusts any domain information given by A?
(for example ValidationInformation structure that is part of PAC) </p>
","<active-directory><trust-relationship>","2020-02-05 14:54:54"
"1034869","Windows Server 2003R2: automatically attaching an iSCSI disk at each startup and setting network shares?","<p>I have a Windows 2003R2 Server soon to be replaced. The server was just connected via iSCSI to a WD MyCloud 4100 NAS, using the Microsoft iSCSI initiator.</p>
<p>A large number of directories exists on this disk. And most of the root-level directories are network shared over Active Directory.</p>
<p>The deal is that due to power outages the server automatically reboots occasionally. The server makes clean shutdowns in this case, using the UPS provided software. From the looks of it though, even though the iSCSI connection is re-established, the network share information gets lost.</p>
<p>Q1) Is there some sort of registry hack or policy that would allow the server to &quot;remember&quot; share-state?</p>
<p>In case the answer to Q1 is &quot;no&quot;, I'm prepping a script that I could run, containing commands to set the new shares, ie:</p>
<pre><code>net share &quot;sharename1&quot;=&quot;f:\share_dir_1&quot; /grant:&lt;my_ad&gt;\administrator,FULL &quot;f:\share_dir_1&quot; /grant:administrator,FULL /grant:&quot;Domain users&quot;,FULL 
net share &quot;sharename2&quot;=&quot;f:\share_dir_2&quot; /grant:&lt;my_ad&gt;\administrator,FULL /grant:administrator,FULL /grant:&quot;Domain users&quot;,FULL
...
</code></pre>
<p>&quot;F&quot; is the iSCSI volume. The script above should contain minimally some checks whether drive F is mounted (ideas?) and start at system startup (again ideas?)</p>
<p>Q2) Is the above a decent way to accomplish that? If not what would you propose as alternatives?</p>
","<windows-server-2003><group-policy><iscsi><startup>","2020-09-22 09:50:35"
"1034874","Updating windows server 2016 from eval version to standard retail version","<p>We are running an evaluation copy of windows server 2016. We have rearmed the license and extended the time for the trial version for several times. Now the rearm count has reached the limit we want to update our current version to a fully licensed version. We have purchased the Windows Server 2016 Standard - Retail (16-Core) and received a message with the product key and a download link. the message looks like the following.</p>
<p><a href=""https://i.sstatic.net/4jjkD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4jjkD.png"" alt=""enter image description here"" /></a>
I have read the documentation on how to update in here <a href=""https://docs.microsoft.com/en-us/windows-server/get-started/supported-upgrade-paths#converting-a-current-evaluation-version-to-a-current-retail-version"" rel=""nofollow noreferrer"">Converting a current evaluation version to a current retail version</a></p>
<p>I have found that i need to run the following command to update</p>
<pre><code>DISM /online /Set-Edition:&lt;edition ID&gt; /ProductKey:XXXXX
</code></pre>
<p>My confusion and question are
<strong>- I have received a product key and a download link from the seller.</strong>
<strong>- Can I run update with the product key with the above command , if the download link is not active?</strong></p>
<p>The download link is now expired. I need to infor the seller to send a new download link. But I just want to know - <strong>When I update my server with the command do I need the download link to be active? Or do I just need the product key</strong>?</p>
<p>FYI: The post in this link ( <a href=""https://serverfault.com/questions/215405/can-you-help-me-with-my-software-licensing-issue"">Can you help me with my software licensing issue?</a> ) does not answer my question. I have received the answer from someone on this post</p>
","<windows><windows-server-2016><evaluation>","2020-09-22 10:31:44"
"862290","Apache2.conf getting subdomain wildcards to work correctly","<p>What I want to do seems easy enough to me in concept, but I've spent a little over 12 hours stuck on this with no luck. I think I'm close, but not sure. This is what I need:</p>

<ul>
<li><p>example.com redirects to www.example.com (301)</p></li>
<li><p>www.example.com loads /home/example/www/example.com/public_html/</p></li>
<li><p>*.example.com loads /home/example/www/*example.com/public_html/ (THE WILDCARD)</p></li>
</ul>

<p>This where am at... the wildcard subdomains aren't working, but the rest seems to, Any advice? I've seen tons of related posts, but nothing that has been able to get me up and running:</p>

<pre><code>&lt;VirtualHost 1.2.3.4:80&gt;
   ServerName example.com
   Redirect 301 / http://www.example.com/
&lt;/VirtualHost&gt;

&lt;VirtualHost 1.2.3.4:80&gt;   
   DocumentRoot /home/example/www/example.com/public_html/
   ServerName www.example.com

   &lt;Directory /home/example/www/example.com/public_html/&gt;
      AllowOverride all
   &lt;/Directory&gt;

&lt;/VirtualHost&gt;

&lt;VirtualHost 1.2.3.4:80&gt;
   DocumentRoot /home/example/www/%1.example.com/public_html/
   ServerName subs.example.com
   ServerAlias *.example.com

   &lt;Directory /home/example/www/%1.example.com/public_html/&gt;
      AllowOverride all
   &lt;/Directory&gt;
&lt;/VirtualHost&gt;
</code></pre>
","<linux><debian><virtualhost><subdomain><apache2>","2017-07-15 06:50:59"
"788383","How to create many cloud servers on behalf of many customers","<p>I am trying to figure out a way to create cloud servers for customers that I wouldn't have access to.</p>

<p>For example, if I have 100 customers, I need some type of sign up process for the user to create a digital ocean, linode, etc, virtual instance that my company wouldn't have control over.  </p>

<p>Once the instance is created, the customer would just have to run a simple install script to start using our product. </p>

<p>I am familiar with the Linode CLI but I don't think that would solve this problem.</p>

<p>The reason I need this is because my product has sensivtive log files and I don't want me or my employees to be able to see the log files of the customers.  Also, the customers are VERY non-technical.</p>

<p>We are just a small company and cannot expect people to ""just trust us"".</p>

<p>UPDATE
<a href=""https://www.digitalocean.com/company/blog/easily-transfer-snapshots-between-accounts/"" rel=""nofollow noreferrer"">https://www.digitalocean.com/company/blog/easily-transfer-snapshots-between-accounts/</a></p>
","<virtual-machines><cloud><linode><cloud-hosting>","2016-07-07 02:08:31"
"929709","Public subnet routing","<p>I have one public IP from my ISP 198.51.100.123/21 and a public 203.0.113.1/29 subnet routed by my ISP through the first IP. 
OS: CentOS 7. </p>

<p>I want to assign each IP from the /29 subnet to a virtual NIC. How can I do that?</p>

<p>Thank you.</p>
","<routing><centos7>","2018-09-06 10:39:38"
"862424","Windows Server 2008 R2 Standard issued when appliying user permisions","<p>I am new here and I am not very knowledgeable, so here is the reason of my question.
For example, I have noticed, if I need to add to a user a specific permission to a shared folder, then of course if this shared folder has a lot of subfolders and files, once we applied for the specific permission, let's say ""write"", then it will apply and process all this permission to all the subfolders and files under that shared folder, then, this is what happened a couple times to the other users connected to the server, see below.</p>

<p>While applying security permission to all subfolders and files for that specific user, then other users, start losing connection to the server and also, others that are running from their workstation an application that is in the server, they start getting kicked off the server.</p>

<p>The server I have, it is a Dual Xeon processor don't know right now what type it is but is one of the latest(2 processor in it), with 64 GB of ram, SAS HDD in raid 5, 8tb total space and used space 4tb, connected to gigabit switches, not all cables are 5e or 6 cat but most of them are, actually the people near the server, that even use cat 6 cables get dropped out the LAN while applying a permission for a specific user.
can anyone have a general idea, suggestions, what could be or what can I do to discover what is going on? </p>
","<windows>","2017-07-16 15:41:39"
"929828","How to copy a 1 GB file using ansible to remote machine as different remote user and password","<p>I have tried to copy a file output of the ansible-playbook looks ok , but the file was copied to local machine not to remote machine in the location dest mentioned in yaml file.</p>

<pre><code>bash-4.2# ansible --version
ansible 2.6.3
  config file = /home/build/git/playbook-iam/ansible.cfg

  configured module search path = [u'/root/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules']

  ansible python module location = /usr/lib/python2.7/site-packages/ansible

  executable location = /bin/ansible

  python version = 2.7.5 (default, Feb 20 2018, 09:19:12) [GCC 4.8.5 20150623 (Red Hat 4.8.5-28)]
</code></pre>

<p>Playbook</p>

<pre><code>---
- hosts: fim-server
  tasks:
  - name: Ansible copy file to remote server
    copy:
      src: test.conf
      dest: /tmp
</code></pre>

<p>playbooks/WAS_CODE_COPY.yml</p>

<pre><code>---
 - hosts: fim-server
   tasks:
   - name: unpack IIM code on FIM server
     unarchive:
       src: /tmp/iim_1.8.5_for_op_7.4_linux_ml.zip
       dest: /tmp
       remote_src: yes
</code></pre>

<p>Output of run</p>

<pre><code>[build@kvmsbasat01m playbook-iam]$ ansible-playbook playbooks/WAS_CODE_COPY.yml

PLAY [fim-server] *************************************************************************************************************************************************************************************************

TASK [Gathering Facts] ********************************************************************************************************************************************************************************************
ok: [KVMSBAFIM01M]

TASK [unpack IIM code on FIM server] ******************************************************************************************************************************************************************************
fatal: [KVMSBAFIM01M]: FAILED! =&gt; {""changed"": false, ""msg"": ""Source '/tmp/iim_1.8.5_for_op_7.4_linux_ml.zip' does not exist""}
    to retry, use: --limit @/home/build/git/playbook-iam/playbooks/WAS_CODE_COPY.retry

PLAY RECAP ********************************************************************************************************************************************************************************************************
KVMSBAFIM01M               : ok=1    changed=0    unreachable=0    failed=1   


[build@kvmsbasat01m playbook-iam]$ ssh kvmsbafim01 ""ls -l /tmp/iim*""
build@kvmsbafim01's password: 
-rw-r--r--. 1 build users 171715008 Sep  7 01:52 /tmp/iim_1.8.5_for_op_7.4_linux_ml.zip

[build@kvmsbasat01m playbook-iam]$ 
</code></pre>
","<ansible><ansible-playbook>","2018-09-07 06:09:20"
"929848","How to copy and paste files from one windows to another?","<p>host A : Windows 10</p>

<p>host B : RDP Windows Server 2012</p>

<p>I want to copy and paste a file or folder from Windows 10 to Windows Server.</p>

<p>Used to be able to do it in my previous work.</p>
","<windows-server-2012-r2>","2018-09-07 08:36:21"
"862472","Redirect www.site.com to https ://www.site.com without breaking email","<p>I want this:</p>
<ul>
<li><code>www.example.com</code> -&gt; <code>https://www.example.com</code></li>
<li><code>example.com</code> -&gt; <code>https://www.example.com</code></li>
</ul>
<p>Here are my URL Redirects Record<br/></p>
<h3>Record 1</h3>
<ul>
<li>Host: www</li>
<li>Value: <code>https://www.example.com/</code></li>
</ul>
<h3>Record 2</h3>
<ul>
<li>Host: @</li>
<li>Value: <code>https://www.example.com/</code></li>
</ul>
<p>Now, when I try to add CNAME record</p>
<ul>
<li>Host: www</li>
<li>Value: @</li>
</ul>
<p>Namecheap's DNS manager page says, using naked domain will prevent mails from being sent to the domain. (I am using privateemail from Namecheap)</p>
<p>(I do have an A record for ipaddress)</p>
<p>So how can I get the browser to resolve <code>https://www.example.com</code> without breaking my email?</p>
","<domain-name-system><https><cname-record><url><namecheap>","2017-07-17 05:20:24"
"862473","unable to ssh from ansible to remote host","<p>Iam trying to run a playbook of ansible on the remote host. But the first step of logging in isnt happening.
tried the following:</p>

<ul>
<li>updated the ansible/hosts file with the remote host credentials</li>
<li>Adding host_key_checking=false</li>
<li>In sshd_config file I have set the ChallengeResponseAuthentication =yes (and restarted the ssh)</li>
<li>Ansible version is 2.3</li>
</ul>

<p>The output:</p>

<pre><code>fatal: [10.236.155.69]: UNREACHABLE! =&gt; {
    ""changed"": false,
    ""msg"": ""Failed to connect to the host via ssh: 
    @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
    @    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
    @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
    IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!
    Someone could be eavesdropping on you right now (man-in-the-middle attack)!
    It is also possible that a host key has just been changed.
    The fingerprint for the ECDSA key sent by the remote host is\n47:0a:1a:05:f2:49:1e:cc:99:2a:47:d8:67:4f:4c:2e.
    Please contact your system administrator.
    Add correct host key in /root/.ssh/known_hosts to get rid of this message.
    Offending ECDSA key in /root/.ssh/known_hosts:2
      remove with: ssh-keygen -f \""/root/.ssh/known_hosts\"" -R 10.236.155.69
    Keyboard-interactive authentication is disabled to avoid man-in-the-middle attacks.
    Permission denied (publickey,password,keyboard-interactive).
    "",
    ""unreachable"": true
}
</code></pre>
","<linux><ssh><ansible-playbook>","2017-07-17 05:22:44"
"929849","Why can't I set two vhost in my apache?","<p>Apache was installed on my debian,i want to bind two domain name with different directroy.</p>

<pre><code>cat  /etc/hosts
127.0.0.1  hwy.local  www.hwy.local  
127.0.0.1  test.app   www.test.app
</code></pre>

<p>Two domain name all binded with  127.0.0.1.</p>

<p>cat   /etc/apache2/sites-available/000-default.conf</p>

<pre><code>&lt;VirtualHost *:80&gt;
    ServerName www.hwy.local
    ServerAdmin webmaster@localhost
    DocumentRoot /var/www/html
    ErrorLog ${APACHE_LOG_DIR}/error_hwy.log
    CustomLog ${APACHE_LOG_DIR}/access_hwy.log combined
        &lt;Directory /var/www/html&gt;
            Options Indexes FollowSymLinks MultiViews
            AllowOverride None
            Order allow,deny
            allow from all
        &lt;/Directory&gt;
&lt;/VirtualHost&gt;
&lt;VirtualHost *:80&gt;

ServerName www.test.app
ServerAdmin webmaster@localhost
DocumentRoot  /home/debian9/app
ErrorLog ${APACHE_LOG_DIR}/error_app.log
CustomLog ${APACHE_LOG_DIR}/access_app.log combined
    &lt;Directory /home/debian9/app&gt;
        Options Indexes FollowSymLinks MultiViews
        AllowOverride None
        Order allow,deny
        allow from all
    &lt;/Directory&gt;
&lt;/VirtualHost&gt;
</code></pre>

<p>Save the same file <code>test.html</code>  in both <code>/var/www/html</code> and <code>/home/debian9/app</code>.</p>

<pre><code>&lt;p&gt;it is a test&lt;/p&gt;
</code></pre>

<p>Why <code>www.hwy.local/test.html</code> can open it ,<code>www.test.app</code>  run into error.</p>

<pre><code>This site can’t be reached 
</code></pre>
","<apache-2.4><hostname>","2018-09-07 08:40:53"
"929855","How to connect a MySQL database on a remote machine","<p>My apologize, this question may seem obvious...but I'm not an expert in system administration.</p>

<p>I have a Windows server (on my Intranet) running Apache 2.2 and MySQL 5.1. The server has PHPMyAdmin  installed.</p>

<p>When I open <a href=""http://servername/phpmyadmin"" rel=""nofollow noreferrer"">http://servername/phpmyadmin</a> I can see, edit the SQL database (note that I'm not asked for any login/password but I see in PHPMyAdmin that I'm logged as root@localhost).</p>

<p>Now, when I try to open the database from a tool installed on y machine (HeidiSQL), I can't get the connection right.</p>

<p>When I specify no user name, it reports <code>Access denied for 'ODBC'@'my local machine name'</code>. I tried to specify <code>root@servername</code>, <code>root@%</code>....but 'my local machine name' always gets appended to the name I enter and connexion is refused.</p>

<p>How can I get this HeidiSQL tool be able to connect the database? What user name should I specifiy?</p>
","<apache-2.2><mysql>","2018-09-07 09:08:51"
"929872","can't connect mysql server on windows server from home computer","<p>I install MySQL Server on a Windows Server from Amazon EC2 with all default config (port 3306 is open).
<a href=""https://i.sstatic.net/cb4sh.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cb4sh.png"" alt=""enter image description here""></a></p>

<p>I edited Security Group on AWS to open port 3306 like this
<a href=""https://i.sstatic.net/Xc61S.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Xc61S.png"" alt=""enter image description here""></a></p>

<p>But every time i try to connect MySQL server from my home computer, this error show up
<a href=""https://i.sstatic.net/Nto0h.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Nto0h.png"" alt=""enter image description here""></a></p>

<p>Any one can show me what i'm wrong?</p>
","<mysql><amazon-web-services><amazon-ec2><windows-server-2012-r2>","2018-09-07 10:14:24"
"1035136","Lets Encrypt with CNAME (cross domain) on k8s","<p>Like the gitbook provide a <a href=""https://docs.gitbook.com/hosting/custom-domains"" rel=""nofollow noreferrer"">custom domain</a> I've to design
a service and user can register their custom domain. For example, I've a service
<strong>hosting.example.com</strong> and user can register any domain based on this service. In the case
I wish user JUST edit the CNAME, for example <strong>sub1.example.net</strong> CNAME TO <strong>hosting.example.com</strong>.</p>
<p>My service already process the virtual host and can provide service normally on HTTP. Now I have to provide HTTPS with lets encrypt. I've survey the <a href=""https://github.com/acmesh-official/acme.sh/wiki/DNS-alias-mode"" rel=""nofollow noreferrer"">acem.sh</a> but HAVE TO extra CNAME <strong>_acme-challenge.sub1.example.net</strong>. I want to reduct this step and keep user just edit DNS one and only one CNAME record.</p>
<p>Since my service is based on the K8S environment and HTTP-01 challenge is not suitable on this case.</p>
<p>[UPDATE]
Since I've using the wildcard CA, the DNS 01 seems the only possible method.</p>
","<lets-encrypt>","2020-09-24 05:53:57"
"788687","Proper syntax for monitoring an IP in .conf files, Icinga2","<p>I currently have the code </p>

<pre><code>object Host ""rqbhost"" {
address = ""xx.xx.xx.xx""
check_command = ""hostalive""
}

object Service ""ping4"" {
host_name = ""rqbhost""
check_command = ""ping4""
}

object Service ""http"" {
host_name = ""rqbhost""
check_command = ""http""
}
</code></pre>

<p>This will not allow me to start the icinga service, and the error logs do not let me know of why they are incorrect. I have tried the documentation for icinga2 but it has been woefully unhelpful for such a simple task as mine.</p>

<p><a href=""http://pastebin.com/hz9qPb9M"" rel=""nofollow noreferrer"">the error logs here</a> </p>

<pre><code>:/etc/icinga2/conf.d/services.conf(25):  */
:/etc/icinga2/conf.d/services.conf(26): apply Service ""ping4"" {
:^^^^^^^^^^^^^^^^^^^^^
:/etc/icinga2/conf.d/services.conf(27):   import ""generic-service""
:/etc/icinga2/conf.d/services.conf(28):
:[2016-07-08 13:59:24 +0000] critical/config: 1 error
:icinga2.service: control process exited, code=exited status=1
</code></pre>
","<syntax><icinga2>","2016-07-08 13:53:28"
"1002297","Use one application for multiple user in windows server","<p>i have a system with windows server, can i install an application (like ArcGis) 
and multiple user can use this application as single instance? 
for more detail, i need use one system for sharing multiple instance of an application on windows. is it possible? </p>

<p>i just have one system with almost 5 user. this user need to interact with specific application on this system in same time (so they cant use remote). how can i manage that? any tools? any approach?</p>
","<windows><remote-desktop><remote-access><multiple-instances>","2020-02-08 09:41:01"
"788734","RAID0 SSDs slow on HP ProLiant","<p><em>TL;DR Is the RAID controller on the HP ProLiant DL360 G7 known to be slow or problematic? My RAID0 set is performing horribly.</em></p>

<p>I have an HP ProLiant DL360 G7. It came with a 160GiB SAS HDD, and I created a logical disk for it and set up Ubuntu Server on there. I inserted three consumer-grade SSDs (ADATA Premier SP550 960GB) that I knew worked with this server and created a RAID0 logical drive in ORCA including just the three SSDs. I then made the mistake of loading my entire 500GiB database on there, a long process, before I had run any benchmarks.</p>

<p>These SSDs are supposed to get 500GiB/s read... each. I've got 2 in RAID0 (using the Linux software RAID) on a consumer-level machine, and I see 1000GiB/s read with <code>hdparm -t</code>. I'd think 3 in RAID0 on the server would get 1500GiB/s.</p>

<p>I get ~210MiB/s read on that SSD array, wow. Yes, I made sure to run the test with the database stopped and nothing else running. I've heard bad things about hardware RAID in the past. Is the RAID controller on this machine just slow? It'll be a big pain to migrate the DB, put each SSD into a separate logical disk, then set up the Linux software RAID, but I'm considering doing it.</p>

<p><strong>Update:</strong> I've added a battery-backed-up disk cache module. Now seeing 648MiB/s with <code>hdparm</code>, which I know isn't the best benchmark to use. So I tried running my actual workload on this machine, i.e. a process that repeatedly runs different heavy PostgreSQL queries concurrently, 24 at a time. I'm not seeing a disk bottleneck anymore. I suppose my problem is solved for now.</p>
","<performance><hp-proliant><ssd>","2016-07-08 17:47:33"
"930086","On boot, server pulls full load of both PSUs -- any way to limit this or mitigate load spike?","<p>I have a giant 24u server (Supermicro 4U X9DRI-F w/ 2x Xeon E5-2680) with 2 1200w PSUs. However, I only have 4 drives in right now, and my actual power draw (under the most stressful loads) is likely not more than 500w. </p>

<p>I have these PSUs plugged in to one 1800J/20A UPS. I (naively) figured this would be more than enough since I don't draw anywhere near the total load my server can pull (except, as I discovered, during startup). </p>

<p>I had <em>thought</em> that my OS (unRaid) deliberately kept HDDs offline so as to avoid power spiking on startup, but I think maybe the initial boot is outside its control.</p>

<p>When I boot, my UPS beeps for a while, telling me I'm overloading it. Once the OS boots, the UPS calms down and hums along fine. Even when the HDDs are switched on, it's fine. </p>

<p>So, is there anything in (maybe) the BIOS I can do to set the PSUs to draw less max power? Or would a solution be to get a second UPS and have the 2nd PSU plug in to that?</p>
","<electrical-power><ups><power-supply-unit>","2018-09-09 04:23:45"
"862707","more than one freeradius user and database","<p>I want to make free radius managing system. 
there are many users who can use this system.
every user will have his own database.
for example, I have tow users the first one admin1 and the second admin2
the first user will have database1 and the second will have database2 
there is one online free radius server  which will be using for authenticating and accounting 
in free radius, as you know there is one sql.conf file which contains the database information 
I know I can make more than one instance of SQL for every user but when the system will have 100 users, for example, is this the right way to make 100 instances of database or there is another way to make this.
when the user open my online system he must have a new account when he click on register button he must insert his database information (host, username, password)
this information must go SQL.conf file in free radius to tell free radius to connect this database 
are there any one have an Idea about my problem 
I hope I explained my problem clearly</p>
","<mysql><radius><freeradius>","2017-07-18 07:17:40"
"862842","How to setup hostname to website in local IIS with HTTPS","<p>I am using Windows 7 with IIS 7.5.
I need to setup local websites on the same IP but on different ports with hostnames which I can access locally by using those hostnames.</p>

<p>For e.g.<br>
Website name: apple website<br>
Path to directory: C:\inetpub\wwwroot\apple<br>
Hostname: applewebsite.com<br>
Port: 44310</p>

<p>Website name: orange website<br>
Path to directory: C:\inetpub\wwwroot\orange<br>
Hostname: orangewebsite.com<br>
Port: 44311</p>

<p>Currently I can access the website with https:// localhost:44310 or https:// localhost:44311, but I would like that my local websites should get resolved with hostnames: <a href=""https://applewebsite.com"" rel=""nofollow noreferrer"">https://applewebsite.com</a> and <a href=""https://orangewebsite.com"" rel=""nofollow noreferrer"">https://orangewebsite.com</a></p>

<p>I know that hosts name are stored in <code>C:\Windows\System32\drivers\etc\hosts</code> folder but I know that you can't use ports but only ip-name pairs.</p>

<p>In IIS manager when I try to bind a website, when I change from http protocol to https, the text field ""hostname"" become grayed.<br>
<em>Update:</em> if the Common Name (CN) is typed in this way <code>*.websitename.com</code> the text field ""hostname"" can be edited.</p>
","<domain-name-system><iis><windows-7><https><iis-7.5>","2017-07-18 16:17:47"
"1035650","Strange Virus/Spyware blocked notification on frequently used vendor site","<p><a href=""https://i.sstatic.net/S1cRG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/S1cRG.png"" alt=""Strange malware warning"" /></a></p>
<p>See screen shot above.</p>
<p>This Virus warning appears when a user attempts to download an .exe from a frequently used Vendor website. This just started a few days ago. As far as I'm aware we don't have any content filtering / A/V rules on our firewall device and the vendor domain is whitelisted there as well. I thought it may be Windows defender blocking it but I couldn't find where to whitelist downloads, I can only add exceptions for files/programs that are already on the machine. Is this being blocked by some kind of Group policy?</p>
<p>This issue effects all PC's on the network and all users. It is a windows 10 pro environment at a Server 2012R2 domain level.</p>
<p>If anyone has seen this error message or knows where it comes from, can you point me in the right direction? I'd like to whitelist the file or remove/edit the filter.</p>
<p>Edit: the Firewall device is a Meraki Mx64. AMP is disabled.</p>
","<windows><security><group-policy><anti-virus><windows-defender>","2020-09-28 20:44:47"
"1035703","Cannot connect to virtual machine with RDP","<p>I have set up 3 virtual machines running win10.
Previously everything has been working fine. However, now sometimes I cannot connect to the virtual machines with RDP.
The strange thing is that the server software (Traccar) and its webinterface is responding fine.
Any idea?</p>
<p>As I said this was all working fine a couple of months ago, win firewall and Azure ports are configured accordingly.</p>
","<windows><azure><rdp><windows-10>","2020-09-29 09:35:47"
"930466","VPN connection for SSH access to a single publicly available","<p>My goal is to have a VPN tunnel on a single port on a server's public IP and be able to SSH into the server with an additional VPN layer of encryption.</p>

<p>DETAILS OF THE SERVER CONFIGURATION:</p>

<p>I am running CentOS 7 on both server and clients. The documents I've read so far seem to focus on a configuration that runs through the browser and relays internet traffic. I don't need the server to relay anything. Can I access the server's SSH port through VPN and leave internet traffic (80/443) on the server and client alone.  The server must still be able to serve 80/443 to the public via Apache and client access internet as normal.</p>
","<ssh><vpn><openvpn><centos7><ssh-keys>","2018-09-11 19:00:57"
"789308","VPN connection fail DNS settings","<p>I have a problem with <strong>DNS</strong> settings in <strong>Ubuntu 16.04</strong>.
First I configure my VPN connection using <code>.ovpn</code> file with command: </p>

<pre><code>sudo openvpn vpn_config_file.ovpn
</code></pre>

<p>The interface <code>tun0</code> is up but it's not getting any traffic:</p>

<pre><code>tun0      Link encap:UNSPEC  HWaddr 00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00  
          inet addr:192.168.200.66  P-t-P:192.168.200.65  Mask:255.255.255.255
          UP POINTOPOINT RUNNING NOARP MULTICAST  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:100 
          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)
</code></pre>

<p>I try to make a symlink symlink from <code>/etc/resolv.conf</code> to <code>/run/resolvconf/resolv.conf</code> but the problem is the same</p>

<p>The issue is that I'm not getting any traffic on the <code>tun0</code> interface. At first sight, I thought it's a DNS issue. 
What could be the problem and how could I possibly fix this?</p>
","<domain-name-system><vpn><symlink><ubuntu-16.04>","2016-07-12 09:04:54"
"930602","SSH into Google Compute Engine using automated deployment (i.e., CircleCI)","<p>I am trying to SSH into my Compute Engine instance using CircleCI with a command that looks like this in config.yml:</p>

<p><code>gcloud --quiet compute ssh [INSTANCE_NAME] --zone northamerica-northeast1-a --project [PROJECT_NAME]
cd /var/www/dev
</code></p>

<p>I can successfully authenticate and see the command prompt in CircleCI logs. However, the commands after SSH are not being carried out. What am I doing wrong here?</p>

<pre><code>The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Wed Sep 12 15:53:55 2018 from 18.212.180.159

[USER_NAME]:~$
</code></pre>

<p>I expect to have cd command run after successful authentication.</p>
","<google-cloud-platform><google-compute-engine>","2018-09-12 16:02:49"
"1035839","Should I assign my VPN a static IP or use a DHCP reservation?","<p>I'm setting up a Raspberry Pi with OpenVPN on my home network. The router for my network is a NetGear Orbi.</p>
<p>In the router's configuration I'm able to assign a static internal IP address (say 10.0.1.200) to the VPN server based on its MAC address.</p>
<p>I could also set the VPN server to use a static internal IP address (10.0.1.200) by adding an entry to its /etc/dhcpcd.conf file.</p>
<p>So one way I'm telling the router to assign a static IP address to the VPN server using DHCP, and the other way I'm telling the VPN server directly to use a static address.</p>
<p>Can I do both (for the same IP address, obviously) or will they interfere with each other?
Should I do both?
If not, which way is preferable?</p>
","<routing><dhcp><static-ip>","2020-09-30 10:50:28"
"930689","ScriptAlias: How to mark DocumentRoot to a target directory","<p>I am hosting a git apache server, and I need to trigger a script with ScriptAlias. I was wondering if it was possible to to use ScriptAlias in this fashion, so it is triggered when there is a GET request for the DocumentRoot (eg. <code>https://domain</code>) only:</p>

<pre><code>ScriptAlias / /path/to/script
</code></pre>

<p>Unfortunately, this doesn't work, so my workaround is to use </p>

<pre><code>ScriptAlias /git /path/to/script
</code></pre>

<p>(git is just a placeholder word)
which means my git repo location would be <code>https://domain/git/foo/bar.git</code> Is there a way to simplify this to <code>https://domain/foo/bar.git</code>?</p>

<p>Much thanks</p>

<p><strong>Edit: to be more concise</strong>
I want to map the URL <code>https://domain/</code> to a cgi script in my filesystem (outside of the DocumentRoot directory). And <code>ScriptAlias / /path/to/script</code> doesn't seem to work.</p>
","<apache-2.4>","2018-09-13 07:03:06"
"789409","After upgrading to fedora 24, unable to login to graphical.target","<p>I just recently upgraded to <strong>Fedora 24</strong> </p>

<p>When I start it it just gives me the <strong>tty1</strong> console without any <strong>GUI</strong>. I login as root and try:</p>

<pre><code>$ systemctl isolate graphical.target
</code></pre>

<p>but it gives me the error:</p>

<pre><code> PolicyKit deamon disconnected from the bus.We are no longer a registered authentication agent
</code></pre>

<p>I also tried Ctrl+Alt+F7 but it was just black</p>

<p>I'd like to get the <strong>GUI</strong> back. What has messed up, and how do I fix it? Or what else could I try? Forgive me if I haven't explained well enough.</p>
","<fedora><upgrade><graphical-user-interface>","2016-07-12 17:21:52"
"789498","Failed SMTP Authentication - TOSHIBA e-STUDIO255","<p>I do have TOSHIBA e-STUDIO255 and i have configured scan to email option. about one week it's worked perfectly but now it's keep giving me this error ""Failed SMTP Authentication"". i have not change any settings on printer. smtp settings as follows.</p>

<p>SMTP client:</p>

<pre><code>Enable SMTP Client              : Enable  
Enable SSL                      : Accept all certificates without CA  
SSL/TLS                         : STARTTLS  
SMTP Server Address             : smtp.gmail.com  
POP Before SMTP                 : Disable  
Authentication                  : Login  
Login Name                      : abc@gmail.com  
Maximum Email / InternetFax Size: 30 MB  
Port Number                     : 25  
SMTP Client Connection Timeout(1-180) : 30 Seconds 
</code></pre>

<p>SMTP server:</p>

<pre><code>Enable SMTP Server    : Enable  
Port Number           : 25  
Email Address         : fax@testdomain.com  
Enable OffRamp Gateway: Disable  
OffRamp Security     : Enable  
OffRamp Print        : Enable  
</code></pre>

<p>Can anyone help me on this?</p>
","<email><network-printer><smtp-auth><user-settings><mailscanner>","2016-07-13 06:33:20"
"1036023","Postfix, dovecot . emails work when ssh'ed into server, but cannot connect to thunderbird","<p>I have a droplet on digital ocean with Ubuntu 18. I have followed the following <a href=""https://github.com/LukeSmithxyz/emailwiz"" rel=""nofollow noreferrer"">Github</a> <a href=""https://www.youtube.com/watch?v=9zP7qooM4pY&amp;t=679s&amp;ab_channel=LukeSmith"" rel=""nofollow noreferrer"">Youtube</a> tutorial to get email server up and running .
The problem is however when i am logged into the server i can send and receive emails using mutt. But when i try to connect it to thunderbird it gives me error &quot;Thunderbird failed to find the settings for your email account &quot;. I have tried it both auto and manual ways with these <a href=""https://github.com/LukeSmithxyz/emailwiz/blob/master/README.md#logging-in-from-thunderbird-or-mutt-and-others-remotely"" rel=""nofollow noreferrer"">settings</a></p>
<p>I am not very expert at doing this. It is my first time for handling such activity. The only change i made is commented out this <a href=""https://github.com/LukeSmithxyz/emailwiz/blob/0bdbbbef6d5fbdb9b6283b8601fcdcb346af5820/emailwiz.sh#L130"" rel=""nofollow noreferrer"">line</a> as i had no such file in (/usr/share/dovecot/dh.pem). Please help me resolve this issue.</p>
<p>i am using the same postfix and dovecot settings as provided in <a href=""https://github.com/LukeSmithxyz/emailwiz/blob/master/emailwiz.sh"" rel=""nofollow noreferrer"">this</a> link. some mail.log is given below i believe it's when i try to connect to the server via thunderbird.</p>
<pre><code>Oct  1 15:41:19 mail postfix/smtpd[16024]: connect from unknown[121.178.116.27]
Oct  1 15:41:19 mail postfix/smtpd[16025]: connect from unknown[121.178.116.27]
Oct  1 15:41:19 mail postfix/smtpd[16024]: improper command pipelining after EHLO from unknown[121.178.116.27]: QUIT\r\n
Oct  1 15:41:19 mail postfix/smtpd[16024]: disconnect from unknown[121.178.116.27] ehlo=1 quit=1 commands=2
Oct  1 15:41:19 mail postfix/smtpd[16025]: improper command pipelining after EHLO from unknown[121.178.116.27]: QUIT\r\n
Oct  1 15:41:19 mail postfix/smtpd[16025]: disconnect from unknown[121.178.116.27] ehlo=1 quit=1 commands=2
Oct  1 15:43:26 mail postfix/smtpd[16108]: connect from unknown[141.98.10.136]
Oct  1 15:43:26 mail postfix/smtpd[16108]: disconnect from unknown[141.98.10.136] ehlo=1 auth=0/1 quit=1 commands=2/3
Oct  1 15:44:59 mail postfix/smtpd[16108]: connect from unknown[45.249.91.180]
</code></pre>
","<postfix><dovecot><ubuntu-18.04><digital-ocean>","2020-10-01 16:20:21"
"1036106","Starting service inside shell script via PHP","<p>So i'm looking for a way to start Plex Media Server service using PHP redirect. I basically want to start and stop the server by going to the certain URL. I'm using Apache and here's my code and commands i'm using:</p>
<p>My /var/www/site/index.php:</p>
<pre><code>&lt;?php
  $output = shell_exec('/var/www/site/plex.sh');
  echo &quot;&lt;h1&gt;Output: &quot; . $output . &quot;&lt;/h1&gt;&quot;;
?&gt;
</code></pre>
<p>/var/www/site/plex.sh</p>
<pre><code>#!/bin/bash

sudo service plexmediaserver start
echo &quot;Done&quot;
</code></pre>
<p>After going to my website, i get header &quot;Output: Done&quot; as expected. But Plex process is still inactive (service status). Permissions for site directory and files are set for &quot;www-data&quot; user and plex.sh is executable (chmod +x). Permissions for directory and files are rwxrwxrwx and user running apache is www-data.</p>
<p>Are commands wrong or is it something else?</p>
","<php><bash><apache2><service>","2020-10-02 07:57:25"
"930967","Creating users via Ansible and with_subelements","<p>I'm trying to write an Ansible playbook which will allow me to define a list of groups to apply to a list of users, across a list of defined servers.</p>

<p>It seems to be getting close to working except the users being created are the literal list value such as <code>['mike']</code> in <code>/etc/passwd</code>.</p>

<p>How do I tell Ansible to use the <em>value</em> of the <code>which_users</code>?</p>

<pre><code>---
- hosts: all
  gather_facts: false
  vars:
    local_group_info:

      - name : developer group
        which_users  :
          - mike
          - george
        which_groups :
          - adm
          - www-data
        on_hosts  :
          - test.sv1.org
          - punchy.sv1.org

      - name: admin group
        which_users:
          - kelly
        which_groups:
          - sudo
        on_hosts  :
          - test.sv1.org

  tasks:
    - name: Add users to local groups if current host matches
      when: inventory_hostname in item.0.on_hosts or 'all' in item.0.on_hosts
      debug:
        msg: ""user {{ item.0.which_users }} should be in group {{ item.1 }}""
      with_subelements:
        - ""{{ local_group_info }}""
        - which_groups
</code></pre>

<p>output:</p>

<pre><code>PLAY [all] **************************************************************************************************************************

TASK [Add users to local groups if current host matches] ****************************************************************************
ok: [test.sv1.org] =&gt; (item=[{u'which_users': [u'mike', u'george'], u'name': u'developer group', u'on_hosts': [u'test.sv1.org', u'punchy.sv1.org']}, u'adm']) =&gt; {
    ""msg"": ""user [u'mike', u'george'] should be in group adm""
}
ok: [test.sv1.org] =&gt; (item=[{u'which_users': [u'mike', u'george'], u'name': u'developer group', u'on_hosts': [u'test.sv1.org', u'punchy.sv1.org']}, u'www-data']) =&gt; {
    ""msg"": ""user [u'mike', u'george'] should be in group www-data""
}
ok: [test.sv1.org] =&gt; (item=[{u'which_users': [u'kelly'], u'name': u'admin group', u'on_hosts': [u'test.sv1.org']}, u'sudo']) =&gt; {
    ""msg"": ""user [u'kelly'] should be in group sudo""
}

PLAY RECAP **************************************************************************************************************************
test.sv1.org       : ok=1    changed=0    unreachable=0    failed=0   
</code></pre>
","<ansible>","2018-09-14 19:33:48"
"930996","Using htaccess change the website default index file from root folder to sub folder, site other pages should still work from root folder","<p>Using htaccess how to change the PHP website default home page ie index.php file from root folder ie ""html"" to sub folder(say it 'subfolder') but site should still work as before ie the home page will be from 'subfolder' but other pages will still using same old root ""html"" folder.</p>

<p>NOTE : </p>

<ul>
<li><p>Website have SSL ie (https)</p></li>
<li><p>Home page Browser URL should always be <a href=""https://www.mywebsite.com/"" rel=""nofollow noreferrer"">https://www.mywebsite.com/</a></p></li>
<li><p>root folder is ""html"".</p></li>
<li><p>The directory path of ""subfolder"" is ""html/subfolder"", this folder only have only one file ie 'index.php'</p></li>
<li><p>So there should be no change in any page URL (including home page).</p></li>
<li><p>NO Redirect</p></li>
</ul>
","<apache-2.4><php5><cakephp>","2018-09-15 00:50:56"
"930999","Is TLS necessary for security in a properly secured network?","<p>TLS provides a great way to trust content between two sources in an untrusted, multiple  hop network.  However, with VPCs and network namespaces, it seems to me that TLS doesn't actually provide any security that isnt already inherent in the network itself.  That said, I've seen several posts (such as [1]) implying that its important for people to enable such communication.</p>
<p>This post does not attempt to challenge TLS, but rather ask, how do we know when it's actually required?</p>
<p>Is there a hierarchical framework for deciding <strong>when</strong> TLS provides increased security based on (1) security needs (2) encryption quality (3) machine isolation, which can be used when deciding on security protocols?</p>
<p>[1] <a href=""https://stackoverflow.com/questions/44559754/secure-traffic-between-pods-in-cluster"">https://stackoverflow.com/questions/44559754/secure-traffic-between-pods-in-cluster</a></p>
","<security><ssl>","2018-09-15 01:34:18"
"1036227","Can't send outgoing mail to another domain on iredmail","<pre><code>sudo tail /var/log/maillog
Oct  3 06:39:28 instance-23 postfix/master[63679]: warning: process /usr/libexec/postfix/trivial-rewrite pid 64274 exit status 1
Oct  3 06:39:28 instance-23 postfix/master[63679]: warning: /usr/libexec/postfix/trivial-rewrite: bad command startup -- throttling
Oct  3 06:39:54 instance-23 postfix/qmgr[63681]: warning: problem talking to service rewrite: Connection timed out
Oct  3 06:40:25 instance-23 postfix/postscreen[64336]: fatal: myhostname and relayhost parameter settings must not be identical: mail.duchi.net
Oct  3 06:40:26 instance-23 postfix/master[63679]: warning: process /usr/libexec/postfix/postscreen pid 64336 exit status 1
Oct  3 06:40:26 instance-23 postfix/master[63679]: warning: /usr/libexec/postfix/postscreen: bad command startup -- throttling
Oct  3 06:40:28 instance-23 postfix/trivial-rewrite[64337]: fatal: myhostname and relayhost parameter settings must not be identical: mail.duchi.net
Oct  3 06:40:29 instance-23 postfix/master[63679]: warning: process /usr/libexec/postfix/trivial-rewrite pid 64337 exit status 1
Oct  3 06:40:29 instance-23 postfix/master[63679]: warning: /usr/libexec/postfix/trivial-rewrite: bad command startup -- throttling
Oct  3 06:40:55 instance-23 postfix/qmgr[63681]: warning: problem talking to service rewrite: Connection timed out
</code></pre>
","<iredmail>","2020-10-03 06:48:45"
"931101","Even with root user Im receiving 'Operation not permitted' when try creating gluster volume between Ubuntu 14.04 LXC containers","<p>Even with root user Im receiving 'Operation not permitted' when try creating gluster volume between Ubuntu 14.04 LXC containers.<br>
Need test the solution locally before install it on prodution and even locally i cant install.  </p>

<pre><code>xfce4-terminal -T LocalTerm  
</code></pre>

<p>exit  </p>

<p>At LocalTerm:  </p>

<pre><code>PS1='[\u@\h]-[\D{%T}]-[\W] =&gt; '  

lxc profile copy default default-bkp  
lxc profile set default raw.lxc lxc.apparmor.profile=unconfined  

clear   
lxc stop ubt1404X64C001  
lxc stop ubt1404X64C002  

lxc delete ubt1404X64C001  
lxc delete ubt1404X64C002  

lxc launch ubuntu:14.04 ubt1404X64C001  
lxc launch ubuntu:14.04 ubt1404X64C002  

clear  
sleep 10  

lxc list --format csv  

lxc config get ubt1404X64C001 security.privileged  
lxc config set ubt1404X64C001 security.privileged true  
lxc config get ubt1404X64C002 security.privileged  
lxc config set ubt1404X64C002 security.privileged true  
lxc restart ubt1404X64C001  
lxc restart ubt1404X64C002    

xfce4-terminal -T ubt1404X64C001Term -e ""bash -c 'lxc exec ubt1404X64C001 -- /bin/bash; exec bash; exit 0;'""   
xfce4-terminal -T ubt1404X64C002Term -e ""bash -c 'lxc exec ubt1404X64C002 -- /bin/bash; exec bash; exit 0;'""   
</code></pre>

<p>At ubt1404X64C001Term and ubt1404X64C002Term:  </p>

<pre><code>PS1='[\u@\h]-[\D{%T}]-[\W] =&gt; '  
echo '' &gt;&gt; /etc/hosts  
apt-get update   

apt-get upgrade  
apt-get dist-upgrade  
apt-get autoremove  
apt-get install netcat net-tools  
lsb_release -a  
    No LSB modules are available.  
    Distributor ID: Ubuntu  
    Description:    Ubuntu 14.04.5 LTS  
    Release:    14.04  
    Codename:   trusty  
uname -a  
    Linux ubt1404X64C002 4.15.0-36-generic #39-Ubuntu SMP Mon Sep 24 16:19:09 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux  
cat /etc/fstab  
    LABEL=cloudimg-rootfs   /    ext4   defaults    0 0       
apt-get install attr fuse libaio1 liburcu-dev libxml2 python2.7 python-pip rpcbind   
echo '' &gt; ~/testAttr  
setfattr -n user.name -v ""UserNameValue"" ~/testAttr  
setfattr -n trusted.glusterfs.test -v ""working"" ~/testAttr  
getfattr -d -m - ~/testAttr  
    trusted.glusterfs.test=""working""  
    user.name=""UserNameValue""  
add-apt-repository ppa:gluster/glusterfs-4.1  
apt-get update  
    W: Failed to fetch   
    http://ppa.launchpad.net/gluster/glusterfs-4.1/ubuntu/dists  
    /trusty/main/binary-amd64/Packages  404  Not Found  
    E: Some index files failed to download. They have been ignored, or old ones used instead.  
    but available to: artful,bionic,cosmic,devel and xenial  
apt-get install glusterfs-server glusterfs-client open-iscsi watchdog    
glusterfs --version | grep built  
    glusterfs 3.4.2 built on Jan 14 2014 18:05:35  
    Not installed 4.1 version because apt-get update failed with error above to trusty version  
</code></pre>

<p>At ubt1404X64C002Term:  </p>

<pre><code>ifconfig eth0 | grep 'inet '  
    20.30.40.50  
echo '10.20.30.40 ubt1404X64C001' &gt;&gt; /etc/hosts   
cat /etc/hostname  
ping ubt1404X64C001  
iptables -I INPUT -p all -s ubt1404X64C001 -j ACCEPT  
</code></pre>

<p>At ubt1404X64C001Term:  </p>

<pre><code>ifconfig eth0 | grep 'inet '  
    10.20.30.40  
echo '20.30.40.50 ubt1404X64C002' &gt;&gt; /etc/hosts  
cat /etc/hostname  
ping ubt1404X64C002  
iptables -I INPUT -p all -s ubt1404X64C002 -j ACCEPT  

mkdir -p /root/tomirrorwith/glusterfs  
cd /root/tomirrorwith/glusterfs   
gluster peer probe ubt1404X64C002  
    peer probe: success  
gluster peer status   
    Hostname: ubt1404X64C002    
    Port: 24007    
    State: Peer in Cluster (Connected)    
gluster volume create gv0 replica 2 ubt1404X64C001:/root/tomirrorwith/glusterfs ubt1404X64C002:/root/tomirrorwith/glusterfs force  
    volume create: gv0: failed  
    ------  
    /var/log/glusterfs/cli.log at ubt1404X64C001  
        W [rpc-transport.c:175:rpc_transport_load] 0-rpc-transport: missing 'option transport-type'. defaulting to ""socket""  
        I [socket.c:3480:socket_init] 0-glusterfs: SSL support is NOT enabled  
        I [socket.c:3495:socket_init] 0-glusterfs: using system polling thread  
        I [cli-cmd-volume.c:392:cli_cmd_volume_create_cbk] 0-cli: Replicate cluster type found. Checking brick order.  
        I [cli-cmd-volume.c:304:cli_cmd_check_brick_order] 0-cli: Brick order okay  
        I [cli-rpc-ops.c:805:gf_cli_create_volume_cbk] 0-cli: Received resp to create volume  
        I [input.c:36:cli_batch] 0-: Exiting with: -1   
    ------  
    /var/log/glusterfs/etc-glusterfs-glusterd.vol.log at ubt1404X64C002  
        E [glusterd-op-sm.c:3719:glusterd_op_ac_stage_op] 0-management: Stage failed on operation 'Volume Create', Status : -1  
    ------  
</code></pre>

<p>TRY FIX At C001TermUbt1404X64:  </p>

<pre><code>    cd /sys/module/fuse/parameters/  
    echo Y &gt; userns_mounts  
        ERROR:  
            bash: userns_mounts: Permission denied  
</code></pre>

<p><a href=""https://github.com/gluster/glusterfs/issues/511"" rel=""nofollow noreferrer"" title=""The same question on Github"">The same question on Github</a><br>
<a href=""https://discuss.linuxcontainers.org/t/even-with-root-user-im-receiving-operation-not-permitted-when-try-creating-gluster-volume-between-ubuntu-14-04-lxc-containers/2699"" rel=""nofollow noreferrer"" title=""The same question on discuss.linuxcontainers.org"">The same question on discuss.linuxcontainers.org</a></p>
","<linux><ubuntu-14.04><lxc><glusterfs><apparmor>","2018-09-16 02:08:27"
"931172","False positives when scanning CentOS7 with OpenSCAP","<p>I just installed OpenSCAP Benchmark scanner on a CentOS7 box I had stigged by hand. There are a huge number of false positives showing up and I'm not sure if it's a bug or somehow it's not remediated when it should be.</p>

<p>I followed the STIGs from STIGViewer directly from DISA and it includes what commands or things to do for remediation so I have a hard time believing a lot of these findings.</p>

<p>I can provide some examples if need be. Please let me know if this is somehow normal or where I can go to submit bugs.</p>
","<centos7><openscap>","2018-09-16 19:34:07"
"931174","Centos 7 curl HTTPS error (1)","<h2>The curl error</h2>
<pre><code># curl https://www.google.com
curl: (1) Protocol &quot;https&quot; not supported or disabled in libcurl
</code></pre>
<h2>Background info</h2>
<p>I start listing my specs:</p>
<ul>
<li>CentOS Linux release 7.5.1804 (Core)</li>
</ul>
<p><code># curl -V curl 7.61.1 (x86_64-pc-linux-gnu) libcurl/7.61.1 Release-Date: 2018-09-05 Protocols: dict file ftp gopher http imap pop3 rtsp smtp telnet tftp  Features: AsynchDNS IPv6 Largefile UnixSockets</code></p>
<pre><code># curl-config --configure
 --with-ssl
# curl-config --ca

# curl-config --features
IPv6
UnixSockets
AsynchDNS
# 
</code></pre>
<p>I have some certificates (i just searched .crt files XD) in these locations:</p>
<ul>
<li>/etc/pki/ca-trust/extracted/openssl</li>
<li>/etc/pki/ca-trust/source</li>
<li>/etc/pki/tls/certs</li>
<li>/tmp/curl-7.61.1/tests/certs</li>
<li>/usr/share/pki/ca-trust-legacy</li>
<li>/usr/src/ca-certificates/etc/pki/tls/certs</li>
<li>/usr/src/ca-certificates/usr/share/pki/ca-trust-legacy/</li>
</ul>
<p>I read (and I can't find it anymore) about problems in certificate from centos 6 to centos 7.</p>
<p>I installed everything from yum, nothing from source.</p>
<p>Am I missing something?!</p>
","<https><centos7><curl><php7>","2018-09-16 20:45:57"
"931178","Find what amount of available space do I have on my linux machine","<p>I have a custom amazon linux distribution Linux ip-xxx-xx-xx-xxx 4.9.85-38.58.amzn1.x86_64 #1 SMP Wed Mar 14 01:17:26 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux</p>

<p>And my <strong>df</strong> command looks like this:</p>

<p><img src=""https://i.sstatic.net/vRzVp.png"" alt=""df command output""></p>

<p>As far as I can tell, my /dev/xvda1 filesytem is mounted on / and has 8Gb available (0% free). the filesystem devtmpfs is mounted on /dev and has 8Gb of available space(99% free) and tmpfs also has 8Gb of space and 100% free space so I guess I should have a total of 24Gb but if I issue a <strong>lsblk</strong> command to see my drives I get this:</p>

<p><img src=""https://i.sstatic.net/edkIZ.png"" alt=""lsblk command output""></p>

<p>Meaning that I only have 1 8Gb drive attached, so what's going on?</p>
","<linux>","2018-09-16 21:08:37"
"931204","How to completely block ssh using firewalld","<p>I have private network set up on a static ip, and I have configured my router at <code>192.168.1.1</code> to forward port XX to port 22 on my local server in order to allow ssh access. I am able to ssh in just fine with this configuration, but then I go to thinking -- why am I allowed to do this? I am fairly new to firewalld, but I found out that I can type <code>sudo firewall-cmd --list-all-zones</code>, in order to see the various ports and services I have open in each of the zones. </p>

<p>However, to my dismay, even after removing the ssh service from all zones, and removing port 22 completely, I am still able to ssh to my local server over the internet by typing <code>ssh -p XX paul@xxx.xxx.xx.xxx</code>. </p>

<p>I tried googling ""how to completely disable ssh access using firewalld"", but all of the results are referring to blocking the service <em>except</em> one or a few specific IPs. In order to test the security of my network, I want to be able to completely disallow ssh access to mny server. But given that port 22 and the ssh service are completely abset from my firewalld config, how come the firewall isn't working? Why am I still able to ssh in?</p>

<p>EDIT: here is the output when I type <code>sudo firewall-cmd --list-all-zones</code>:</p>

<pre><code>block
  target: %%REJECT%%
  icmp-block-inversion: no
  interfaces: 
  sources: 
  services: 
  ports: 
  protocols: 
  masquerade: no
  forward-ports: 
  source-ports: 
  icmp-blocks: 
  rich rules: 


dmz
  target: default
  icmp-block-inversion: no
  interfaces: 
  sources: 
  services: 
  ports: 
  protocols: 
  masquerade: no
  forward-ports: 
  source-ports: 
  icmp-blocks: 
  rich rules: 


drop
  target: DROP
  icmp-block-inversion: no
  interfaces: 
  sources: 
  services: 
  ports: 
  protocols: 
  masquerade: no
  forward-ports: 
  source-ports: 
  icmp-blocks: 
  rich rules: 


external
  target: default
  icmp-block-inversion: no
  interfaces: 
  sources: 
  services: 
  ports: 
  protocols: 
  masquerade: yes
  forward-ports: 
  source-ports: 
  icmp-blocks: 
  rich rules: 


home
  target: default
  icmp-block-inversion: no
  interfaces: 
  sources: 
  services: mdns samba-client dhcpv6-client
  ports: 
  protocols: 
  masquerade: no
  forward-ports: 
  source-ports: 
  icmp-blocks: 
  rich rules: 


internal
  target: default
  icmp-block-inversion: no
  interfaces: 
  sources: 
  services: mdns samba-client dhcpv6-client
  ports: 
  protocols: 
  masquerade: no
  forward-ports: 
  source-ports: 
  icmp-blocks: 
  rich rules: 


public (active)
  target: default
  icmp-block-inversion: no
  interfaces: enp0s25 wlp3s0
  sources: 
  services: 
  ports: 8545/tcp 5001/tcp 33/tcp
  protocols: tcp udp
  masquerade: no
  forward-ports: 
  source-ports: 5001/tcp
  icmp-blocks: 
  rich rules: 


trusted
  target: ACCEPT
  icmp-block-inversion: no
  interfaces: 
  sources: 
  services: 
  ports: 
  protocols: 
  masquerade: no
  forward-ports: 
  source-ports: 
  icmp-blocks: 
  rich rules: 


work
  target: default
  icmp-block-inversion: no
  interfaces: 
  sources: 
  services: dhcpv6-client
  ports: 
  protocols: 
  masquerade: no
  forward-ports: 
  source-ports: 
  icmp-blocks: 
  rich rules: 
</code></pre>

<p>As you can see, the <code>ssh</code> service should not be available in any of my zones. And yet it works just fine. I assume that there is some assumption that port 22 is open by default, even though it is not explicit? Or am I missing something?</p>

<p>-Paul</p>
","<ssh><firewalld>","2018-09-17 00:21:05"
"931219","My RDS Instance Disk is free but Billing Dashboard Showing high disk usage","<p>I am using t2.micro instance to learn experience RDS performance. My issue is that my All databases are less than 10 mb but Billing Console is showing that i have used 8 Gigabytes of storage. How? I am very confused because my database usage is 10 mb and billing dashboard showing 8 Gb .
I am attaching the screenshots link below to help you understand.</p>

<p><a href=""https://i.sstatic.net/P5Zn0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/P5Zn0.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.sstatic.net/MawT0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MawT0.png"" alt=""enter image description here""></a></p>
","<mysql><amazon-web-services><rds>","2018-09-17 06:12:22"
"864856","How to use iptables on varnish cache server to access only by one network and backend server","<p>Hi i  want to allow only one network say <code>10.10.0.0</code> to access my varnish edge cache server at <code>192.168.1.10:80</code> port also block other 80 port request but the problem is cache server points to a origin server which listen on 80 port in in <code>192.168.2.0</code> network.</p>

<p>How could i enable only these two networks to request and fetch inside and output 80 port connections</p>

<p>See attached image:</p>

<p><a href=""https://i.sstatic.net/FdU4x.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/FdU4x.png"" alt=""enter image description here""></a></p>
","<linux><centos><iptables><cache><varnish>","2017-07-24 14:10:29"
"931315","Backup and Restore Centos 7 using Dump","<p>I am very new in Linux.</p>

<p>I have some applications installed in one of my CentOS 7 VPX. I want move my hosting to another provider, so I want create backup of my full system so I can restore it in the new hosting.</p>

<p>I have found <a href=""https://serverfault.com/questions/120431/how-to-backup-a-full-centos-server"">this question</a> on this same site, but I have some doubts.</p>

<p>What is </p>

<blockquote>
  <p>blowfish user@backupserver.example.com</p>
</blockquote>

<p>? </p>

<p>Where can I find backup file <code>dump.gz</code> and how to download it?</p>

<p>Thanks</p>
","<backup><centos7><backup-restoration><dump>","2018-09-17 16:16:10"
"931377","IAM policy for restricting Glue resources","<p>I want to limit a user to a specific database. User should be able to do all the actions to that database and its tables.</p>

<p>All the examples I found in the documentations limit the Action, but never Resource.</p>

<p>The example below gives user access to all the databases and tables. What should be put in <code>Resource</code> field to restrict access to a single database?</p>

<pre><code>{
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
        ""Action"": [
          ""glue:*""
        ],
        ""Effect"": ""Allow"",
        ""Resource"": [
            ""*""
        ]
    },
    ]
}
</code></pre>
","<amazon-web-services><amazon-iam>","2018-09-17 23:32:49"
"931396","Bind9 reverese look up error with CNAME and 192","<p>I'm trying to configure my bind9 resolution file to take in CNAME and a reverse ipv4 of 192.168.1.112 but everytime i add them it just doesn't work as it shows in the video tutorials</p>

<blockquote>
  <p>This is my zone  file</p>
</blockquote>

<pre><code>zone ""localhost"" {
   type master;
   file ""/etc/bind/db.local112.zone"";
</code></pre>

<hr/>

<blockquote>
  <p>This is the db.local112.zone</p>
</blockquote>

<pre><code>$TTL    604800
@   IN  SOA example.com. root.example.email.com. (
             11     ; Serial
         604800     ; Refresh
          86400     ; Retry
        2419200     ; Expire
         604800 )   ; Negative Cache TTL

IN  NS  example.com.
IN  A   192.168.1.12
IN  A   192.168.1.13
</code></pre>

<p>This works fine and outputs the 2 address's listed but this next file outputs nothing, i would like to add an email, CNAME and reverse look up successfully</p>

<blockquote>
  <p>This is my db.local11.zone re-configured</p>
</blockquote>

<pre><code>$TTL    604800
@       IN      SOA     ns.example.com. root.localhost. (
                        111         ; Serial
                     604800         ; Refresh
                      86400         ; Retry
                    2419200         ; Expire
                     604800 )       ; Negative Cache TTL

    IN      NS      example.com.
    IN      A       192.168.1.12
    IN      A       192.168.1.13

ns    IN    A       192.168.1.12

exampleXYZ.com.        IN      CNAME   exampleXYZ.com.
exampleXYZ.com.        IN      A       192.168.1.112


12.      IN     PTR      exampleXYZ.com.

smtp                   IN      A               192.168.10.1
                       IN      MX              10   smtp
</code></pre>

<p>After a <strong>service bin9 reload</strong> i can still use dig on the example.com but not on dig exampleXYZ.com or dig 192.168.1.112</p>
","<bind>","2018-09-18 03:21:27"
"1036640","Daemons and runlevels Linux","<p>Linux
In class they ask me this and we have not given it and also the notes are out of date using chkconfig for example, I can't find a way to solve it.</p>
<ol start=""3"">
<li><p>Make it impossible to use the printer in runlevel 2, for this you must ensure that its daemon (cups) does not run when entering that level.</p>
</li>
<li><p>Make that for level # 5, the cron daemon will not run.</p>
</li>
</ol>
<p>I need an explanation of this because I can't access the level, I just get it to work as a service, thanks guys!!</p>
","<linux>","2020-10-06 16:15:44"
"1036667","How to start up bind9 server","<p>I tried everything to start bind9 server and I finally found this&gt; run named -g</p>
<p>I do not know what else to do.</p>
<pre><code>root@Microknoppix:/home/knoppix# named -g
06-Oct-2020 20:34:13.743 starting BIND 9.11.5-P4-5.1+deb10u2-Debian (Extended Support Version) &lt;id:998753c&gt;
06-Oct-2020 20:34:13.743 running on Linux i686 5.3.5 #17 SMP PREEMPT Wed Oct 23 17:54:30 CEST 2019
06-Oct-2020 20:34:13.743 built with '--build=i686-linux-gnu' '--prefix=/usr' '--includedir=/usr/include' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--sysconfdir=/etc' '--localstatedir=/var' '--disable-silent-rules' '--libdir=/usr/lib/i386-linux-gnu' '--libexecdir=/usr/lib/i386-linux-gnu' '--disable-maintainer-mode' '--disable-dependency-tracking' '--libdir=/usr/lib/i386-linux-gnu' '--sysconfdir=/etc/bind' '--with-python=python3' '--localstatedir=/' '--enable-threads' '--enable-largefile' '--with-libtool' '--enable-shared' '--enable-static' '--with-gost=no' '--with-openssl=/usr' '--with-gssapi=/usr' '--with-libidn2' '--with-libjson=/usr' '--with-lmdb=/usr' '--with-gnu-ld' '--with-geoip=/usr' '--with-atf=no' '--enable-ipv6' '--enable-rrl' '--enable-filter-aaaa' '--enable-native-pkcs11' '--with-pkcs11=/usr/lib/softhsm/libsofthsm2.so' '--with-randomdev=/dev/urandom' '--enable-dnstap' 'build_alias=i686-linux-gnu' 'CFLAGS=-g -O2 -fdebug-prefix-map=/build/bind9-q3FpOH/bind9-9.11.5.P4+dfsg=. -fstack-protector-strong -Wformat -Werror=format-security -fno-strict-aliasing -fno-delete-null-pointer-checks -DNO_VERSION_DATE -DDIG_SIGCHASE' 'LDFLAGS=-Wl,-z,relro -Wl,-z,now' 'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2'
06-Oct-2020 20:34:13.743 running as: named -g
06-Oct-2020 20:34:13.743 compiled by GCC 8.3.0
06-Oct-2020 20:34:13.743 compiled with OpenSSL version: OpenSSL 1.1.1d  10 Sep 2019
06-Oct-2020 20:34:13.743 linked to OpenSSL version: OpenSSL 1.1.1d  10 Sep 2019
06-Oct-2020 20:34:13.743 compiled with libxml2 version: 2.9.4
06-Oct-2020 20:34:13.743 linked to libxml2 version: 20904
06-Oct-2020 20:34:13.743 compiled with libjson-c version: 0.12.1
06-Oct-2020 20:34:13.743 linked to libjson-c version: 0.12.1
06-Oct-2020 20:34:13.743 threads support is enabled
06-Oct-2020 20:34:13.743 ----------------------------------------------------
06-Oct-2020 20:34:13.743 BIND 9 is maintained by Internet Systems Consortium,
06-Oct-2020 20:34:13.743 Inc. (ISC), a non-profit 501(c)(3) public-benefit 
06-Oct-2020 20:34:13.743 corporation.  Support and training for BIND 9 are 
06-Oct-2020 20:34:13.743 available at https://www.isc.org/support
06-Oct-2020 20:34:13.743 ----------------------------------------------------
06-Oct-2020 20:34:13.743 adjusted limit on open files from 500000 to 1048576
06-Oct-2020 20:34:13.743 found 4 CPUs, using 4 worker threads
06-Oct-2020 20:34:13.743 using 3 UDP listeners per interface
06-Oct-2020 20:34:13.744 using up to 4096 sockets
06-Oct-2020 20:34:13.748 loading configuration from '/etc/bind/named.conf'
06-Oct-2020 20:34:13.748 /etc/bind/named.conf:9: open: /etc/bind/named.conf.options: file not found
06-Oct-2020 20:34:13.748 loading configuration: file not found
06-Oct-2020 20:34:13.748 exiting (due to fatal error)
</code></pre>
","<bind><knoppix>","2020-10-06 19:37:22"
"931638","SSH connection permission denied","<p>I am trying to login into remote server without password using ssh but I am get the error:</p>

<pre><code>Permission denied (public key)
</code></pre>

<p>To copy <code>.pub</code> key to the server, I had:</p>

<pre><code>PasswordAuthentication = yes (in the sshd_config file).
</code></pre>

<p>After I copied the <code>.pub</code> key, the i disabled the password in the <code>ssh_config</code> I had:</p>

<pre><code>PasswordAuthentication = no
</code></pre>

<p>Now when I tried to ssh:</p>

<pre><code>ssh 'server@192.168.xx.x'
</code></pre>

<p>I was getting the error:</p>

<pre><code>Permission denied (public key)
</code></pre>

<p>How can I solve the problem?</p>

<p>I have changed the authorized_keys permission, the <code>.ssh/</code> permission and so on, but the problem persist. I have even used the <code>ssh-agent $SHELL &amp;&amp; ssh-add</code> command.</p>
","<ssh>","2018-09-19 08:44:39"
"790371","Failed while installing Oracle fusion Middleware 11g Release 1 (11.1.1.7)","<p>I am trying to install oracle fusion Middleware 11g Release 1 (11.1.1.7) on Windows 2012 R2 64-bit with Java 7 update 79. But I am receiving below error while running installation wizard:</p>

<pre><code>Checking operating system certification
Check complete. The overall result of this check is: Not executed &lt;&lt;&lt;&lt;
</code></pre>

<p><a href=""https://i.sstatic.net/2hNmX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2hNmX.png"" alt=""enter image description here""></a></p>

<p>As per Oracle documentation (<a href=""http://www.oracle.com/technetwork/middleware/ias/downloads/fusion-certification-100350.html"" rel=""nofollow noreferrer"">Oracle Fusion Middleware Supported System Configurations</a>). The said OS should support 11g Middleware Release 1</p>

<p><strong>Note:</strong> I can ignore the error and continue with the installation wizard. But will that lead to unexpected behavior later ? What it is the cause of this error and how to resolve it ?</p>
","<java><oracle><weblogic>","2016-07-18 05:25:54"
"1036960","Can I use Group Policy to configure SSO for SharePoint","<p>I am trying to set up SSO in Group policy so when users go to our SharePoint they are automatically signed in instead of having to sign in every time they go to the website. I cannot find anything useful online as far as walkthroughs. Can anyone explain to me how to do this or send me a link to a useful article?</p>
","<windows><active-directory><group-policy><sharepoint>","2020-10-08 19:01:07"
"931798","How to create failure log file owned by syslog while running systemd service?","<p>I am running Go binary as a service on ubuntu 16.04,and not able to create log file which is owned by syslog. I have created a services for myapp.service and it looks like as follows:</p>

<pre><code>[Unit]
Description=myapp

[Service]
Type=simple
Restart=always
RestartSec=30
WorkingDirectory= /home/go/src/myapp
ExecStart=/home/go/src/myapp/myapp  

# myapp.log owned by syslog
PermissionsStartOnly=true
ExecStartPre=/bin/mkdir -p /var/log/myapp
ExecStartPre=/bin/chown syslog:adm /var/log/myapp
ExecStartPre=/bin/chmod 755 /var/log/myapp
StandardOutput=syslog
StandardError=syslog
SyslogIdentifier=myapp                           

[Install]
WantedBy=multi-user.target
</code></pre>

<p>Now It just creates a folder inside /var/log/myapp . But didn't creates any myapp.log file inside myapp folder. I can check these logs using command ::</p>

<pre><code>sudo journalctl -f -u myapp
</code></pre>

<p>It will show all logs inside terminal. I have following questions ::</p>

<ol>
<li>how can I create a log file, which will maintain all logs(inside
/var/log/myapp/myapp.log).</li>
<li>And I also want to categorize logs, like I just want to maintain
failure logs only, so after created log file how to write a failure logs   inside myapp.log. For now it shown all logs(both success failure)
inside terminal.</li>
<li>I am hitting golang api on port 8080, but whenever I checked port it
doesn't seems busy. I didn't understand why port 8080 is not busy when I am running it as a service.Although it gives response in both cases but after stopping service and if running app in foreground it shows port 8080 is busy. Is there any reason for this?</li>
</ol>
","<ubuntu-16.04><systemd>","2018-09-20 03:50:48"
"1036977","Confused about OpenSSH and Pure-FTPd over port 22","<p>I'm running Ubuntu 20 and have installed Pure-FTPd. When I SFTP on port 22 with FileZilla the log shows &quot;We claim SSH-2.0 FileZilla ... Remote version SSH-2.0 OpenSSH...&quot; On login with FileZilla, auth.log shows an ssh2 connection.</p>
<p>I'm confused about why I don't see something like &quot;Welcome to Pure-FTPd [privsep] [TLS]&quot;. The service is running. I haven't read anywhere that we need to do anything to tell OpenSSH to hand over SFTP requests to another process. I'm guessing that something else needs to be set under /etc/ssh/sshd_config.d ?</p>
<p>It doesn't look like the PureFTPd service is processing these requests at all, with its own user db or other settings.</p>
<p>Thanks.</p>
","<ssh><sftp><pureftpd>","2020-10-08 21:11:43"
"865389","Dell Precision T5600 SAS issue","<p>I'm currently having some issues with my Dell Precision T5600 Workstation (more specifically the Intel C600 SAS Controller).</p>

<p>My problem is that my 2x2TB SAS harddisks are completely unreadable for my computer. They are connected to HDD0 and HDD1 on my motherboard. I've got a 250GB SSD connected to SATA0, where Windows Server 2012 R2 is installed.
I've installed both the SATA and SAS controller (Intel C600) drivers, but the SAS drives doesn't show up in Windows or BIOS. I also believe the SAS controller is onboard (picture: <a href=""https://i.sstatic.net/yfvQr.jpg"" rel=""nofollow noreferrer"">https://i.sstatic.net/yfvQr.jpg</a>). I can confirm that my SAS disks works, as they've been tested on another computer recently. However, if I connect a normal SATA disk to any of the HDD0-3 with the same SAS cable, Windows and BIOS finds it immediately.</p>

<p>Somewhere on the interwebs I read it could be because I need something called Intel RAID C600 Upgrade Key, but I've never heard of this. I've tried updating the BIOS from version A9 to A15, which according to Dell support is the newest version. If possible, I'd like to avoid setting up a RAID for the two hard drives.</p>

<p>I've been googling for over 4 hours now and I'm really getting tired of searching. I'm considering throwing away my SAS drives and just replace them with SATA drives.</p>

<p>TL;DR - My T5600 (Windows and BIOS) is unable to read my SAS drives even when the drivers are installed. Normal SATA drives works perfectly with the same cables/connectors.</p>

<p>An additional picture of my device manager in Windows: <a href=""https://i.sstatic.net/RQBTj.jpg"" rel=""nofollow noreferrer"">https://i.sstatic.net/RQBTj.jpg</a></p>

<p>I appreciate any input on this matter! Thank you.</p>
","<raid><hardware><dell><sas><drivers>","2017-07-27 05:10:06"
"931809","Replication of Postgresql vice cersa","<p>I have two Linux servers having Postgresql-10 on each of them. On both of them, there is a database, say dbexample, having same schema, table, everything. Both of them are capturing data from an application and storing them on their database (dbexample).Let me tell you since both are capturing data from the same application, there is no mismatch in data at a time. </p>

<p>Now, what is my problem? At a time server 'A' fails due to any reason. That moment it is not able to capture data from the application. But since second is UP it must have captured all the data for that time being when server A was DOWN. </p>

<p>Now, When Server A is UP I want all the data to be replicated on this server A from server B for that time, so that I can have all the data same on both the servers. </p>

<p>Considering the same scenario in case of failure of server B, I want the things vice-versa, i.e. data from Server A to be replicated to Server B.</p>

<p>I have gone through many tutorials and have come-up with below solutions: </p>

<ol>
<li><p>To set Logical replication between both servers and make Publisher and Subscriber of each other. (But that is possible one way not for vice-versa, if I am not wrong). So please suggest what could be the possible solution.</p></li>
<li><p>To arrange a server C and restore the schema only of database, and make Server A publisher and Server C its subscriber. Server C publisher and Server B its subscriber. Server B Publisher and Server A its subscriber. It will be like:-</p></li>
</ol>

<p>A will pull data from B, </p>

<p>B will pull data from C, and </p>

<p>C will pull data from A.</p>

<p>Is it feasible when A and B already have data in their databases?</p>

<p>Kindly suggest which one is better way or any other solution you have. Sorry for my language if I am unable to express my problem. Ask me for any clarification.</p>
","<postgresql><replication>","2018-09-20 06:37:19"
"790666","Local Port Forwarding does not work when only port 22 is allowed by Iptables","<p>I'm trying to perfom a Local Port Forwarding using SSH tunneling.</p>

<p><strong>Machine 1:</strong> the rules of iptables are:</p>

<pre><code>Chain INPUT (policy ACCEPT)
target     prot opt source               destination         

Chain FORWARD (policy ACCEPT)
target     prot opt source               destination         

Chain OUTPUT (policy DROP)
target     prot opt source               destination         
ACCEPT     tcp  --  anywhere             anywhere             tcp dpt:ssh
</code></pre>

<p><strong>Machine 2:</strong> ssh server (sshd) accepting traffic from port 22.</p>

<p><strong>SSH command:</strong> </p>

<blockquote>
  <p>ssh lub2@10.0.2.6 -L 8080:209.188.89.221:80</p>
</blockquote>

<p>Where: 209.188.89.221 is a random HTTP webpage,
        10.0.2.6 is the IP of Machine 2 (and lub2 the username)</p>

<p>This way I should be abble to access the webpage (209.188.89.221) using <a href=""http://127.0.0.1:8080"" rel=""nofollow noreferrer"">http://127.0.0.1:8080</a> as a URL (from Machine 1), but it loads and no result.</p>

<p>In brief, if I open all the ports on Machine 1, the port forwarding works but when I open just the 22 (and all the other close) it does not. Do you have an explanation ?</p>

<p>Cheers</p>
","<iptables><port-forwarding><ssh-tunnel>","2016-07-19 12:08:43"
"1037081","php core script not getting called from Crontab","<p>I am using core PHP and written a simple script that needs to be called automatically. For testing purpose I have just written a print statement as below,</p>
<pre><code>$host = 'localhost';
$user = 'admin';
$pass = 'password';
$db = 'TestDB';
print ($user);
</code></pre>
<p>When I execute the script from the command line as below, it works fine</p>
<pre><code>php /Applications/MAMP/htdocs/project/services/test.php &gt; ~/cronOutput/test.txt 
</code></pre>
<p>But when I call this from Cron, its not at all getting called. Below is the crontab command that I have,</p>
<pre><code>* * * * * /usr/bin/php /Applications/MAMP/htdocs/project/services/test.php &gt; ~/cronOutput/test.txt 
* * * * * php /Applications/MAMP/htdocs/project/services/test.php &gt; ~/cronOutput/test.txt
</code></pre>
<p>Both the commands above does not work.</p>
","<php><cron><scheduler><cron.daily><incrontab>","2020-10-09 14:04:53"
"1037095","PHP Session issue for AddOn domain","<p>I have installed cPanel (EasyApache) for a domain (Domain1.com)
Later i have added a new add on domain (Domain2.com), but when i am starting php session for domain2.com it is setting domain1.con cookie in chrome network response.</p>
<p>set-cookie: PHPSESSID=51gjk7jr3qnscrb250j97mibh6; expires=Sat, 09-Oct-2021 15:00:35 GMT; Max-Age=31536000; path=/; domain=.domain1.com</p>
<p>Also chrome is giving warning for this &quot;This Set-Cookie was blocked because its Domain attribute was invalid with regard to the current host url&quot;.</p>
<p><strong>Directory structure</strong></p>
<pre><code>public_html/ (for domain1.com)
public_html/mysubdomain/domain2/ (for domain2.com)
</code></pre>
<p><strong>PHP Version used</strong></p>
<pre><code>php5.6 for domain1.com
php7.1 for domain2.com
</code></pre>
<p>Is their any setting in php.ini or any other for fix this ?</p>
","<session>","2020-10-09 15:12:31"
"790793","SMTP recipient callout using static sender address","<p>Our mail provider provides the MXs for our domains, performs some screening on the mails (spam checks etc.) and then delivers the mails to our internal mail server. (They use Cisco Ironport.)</p>

<p>They have configured recipient verification/callout/call-ahead using a static MAIL FROM address (test@isp.com). This poses problems for internal email accounts which have a restricted set of external domains from which they can receive mail. (The ISP thinks the account does not exist, and bounces the mails, while the account does indeed exist but accepts only mail from certain domains of which the static mail from address used for the callout is not a part.)</p>

<p>Do you think they are doing this correctly? I did not find a best practice or an RFC on that subject... Any idea why they would not use the empty envelope sender to perform the check? (i.e. <code>MAIL FROM:&lt;&gt;</code>)</p>

<p>It seems there are three possibilites to perform the callout:</p>

<ul>
<li>Use the (from,to) tuple from the original mail (since the Ironport apparently caches the result, it probably can't do this, or the cache would be very ineffective). However, this is what e.g. the Exim manual suggests.</li>
<li>Use the empty envelope sender as outlined above. Unless the recipient would refuse bounces or delivery notifications or the like, this would probably work.</li>
<li>Use a static FROM address. This appears to be the most stupid choice.</li>
</ul>
","<smtp>","2016-07-19 20:54:32"
"865582","Install ProFTPD Using non-default modules mod_sftp","<p><a href=""http://www.proftpd.org/docs/contrib/mod_sftp.html"" rel=""nofollow noreferrer"">ProFTPD module mod_sftp</a></p>

<p>I want to set up a file server with sftp and virtual account,so i use ProFTPD.</p>

<p>system info:red hat enterprise linux 6.8-x86_64</p>

<p>proftpd info:proftpd-1.3.6</p>

<p>OpenSSL 1.0.1e-fips 11 Feb 2013</p>

<p>error: openssl/rand.h: No such file or directory</p>

<p>what i do:</p>

<pre><code>mkdir /usr/local/proftpd
mkdir /etc/proftpd
cd /usr/local/src
wget ftp://ftp.proftpd.org/distrib/source/proftpd-1.3.6.tar.gz
tar -zxvf proftpd-1.3.6.tar.gz 
cd /usr/local/src/proftpd-1.3.6    
./configure --prefix=/usr/local/proftpd --sysconfdir=/etc/proftpd --enable-openssl --with-modules=mod_sftp
make
</code></pre>

<p>ERROR INFO:</p>

<pre><code>--------------
Build Summary
--------------
Building the following static modules:
  mod_ident
  mod_quotatab
  mod_quotatab_file
  mod_sftp
  mod_cap

--------------
[root@slave1 proftpd-1.3.6]# make &amp;&amp; make install
echo \#define BUILD_STAMP \""`date +""%a %b %e %Y %H:%M:%S %Z""`\"" &gt; include/buildstamp.h
cd lib/ &amp;&amp; make lib
make[1]: Entering directory `/usr/local/src/proftpd-1.3.6/lib'
gcc -DHAVE_CONFIG_H  -DLINUX  -I.. -I../include  -g2 -O2 -Wall -fno-omit-
........
gcc -DHAVE_CONFIG_H  -DLINUX  -I.. -I../include  -g2 -O2 -Wall -fno-omit-frame-pointer -c table.c
table.c:30:26: error: openssl/rand.h: No such file or directory
table.c: In function ‘tab_get_seed’:
table.c:366: warning: implicit declaration of function ‘RAND_bytes’
make[1]: *** [table.o] Error 1
make[1]: Leaving directory `/usr/local/src/proftpd-1.3.6/src'
make: *** [src] Error 2
</code></pre>

<p>Installation</p>

<pre><code>The mod_sftp module is distributed with ProFTPD. For including mod_sftp as a staticly linked module, use:
  $ ./configure --enable-openssl --with-modules=mod_sftp ...
Alternatively, mod_sftp can be built as a DSO module:
  $ ./configure --enable-dso --enable-openssl --with-shared=mod_sftp ...
Then follow the usual steps:
  $ make
  $ make install
Note that if the libsodium library is available, mod_sftp will auto-detect this, which enables other supported algorithms. You can ensure that mod_sftp compiles with the libsodium library like so:
  $ ./configure --enable-openssl --with-modules=mod_sftp ... \
    --with-includes=/path/to/libsodium/include \
    --with-libraries=/path/to/libsodium/lib
</code></pre>
","<linux><sftp><proftpd>","2017-07-28 01:15:50"
"790852","Continuous Log Rotation","<p>I have implemented Log rotation as :-</p>

<pre><code>    /data/mule/logs/*.log {
    rotate 1
    size 20M
    missingok
    noolddir
    nomail
    notifempty
    create 644 root root
    compress
    }
</code></pre>

<p>But the logs are not getting rotated continuously if the size increases beyond limit. It rotates only once in a day.</p>

<p>Can anyone help me to continuously check the logs in the path and if it exceeds the limit of 20mb it should be rotated.</p>

<hr>

<p>I have configured in the crontab to run every 5 minutes.
However I am getting below error while running it :</p>

<pre><code>    /etc/logrotate.d/mule: line 1: /data/mule/logs/app.zip.log: Text file busy
    /etc/logrotate.d/mule: line 2: rotate: command not found
    /etc/logrotate.d/mule: line 3: minsize: command not found
    /etc/logrotate.d/mule: line 4: missingok: command not found
    /etc/logrotate.d/mule: line 5: noolddir: command not found
    /etc/logrotate.d/mule: line 6: nomail: command not found
    /etc/logrotate.d/mule: line 7: notifempty: command not found
    /etc/logrotate.d/mule: line 8: create: command not found
    /etc/logrotate.d/mule: line 9: compress: command not found
    /etc/logrotate.d/mule: line 10: syntax error near unexpected token `}'
    /etc/logrotate.d/mule: line 10: `}'
</code></pre>

<p>I have added the below in my crontab :-</p>

<pre><code>    */5 * * * * /etc/logrotate.d/mule
</code></pre>
","<linux><logrotate>","2016-07-20 07:39:43"
"791005","64bit printer driver on 32bit Windows server","<p>I'm attempting to add a 64bit driver to a 32bit windows 2003 server, each time I get the below prompt, I locate the disk image from the network and hit ok, but nothing is copied and this is displayed again. </p>

<p>I'm doing this through additional drivers and the drivers section
 (right clicking printers etc) </p>

<p><a href=""https://i.sstatic.net/k9j79.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/k9j79.png"" alt=""enter image description here""></a></p>
","<windows-server-2003><shared-printers>","2016-07-20 19:15:31"
"932278","Replace HDD Cage with another one","<p>I have a HP Rackmountable 2U Server (HP ProLiant SE326M1). This Server has a P410i Raid Card with a hdd cage for 25x 2.5"" SAS HDDs. Since 2.5"" SAS HDDs are pretty expansive, I want to replace the cage with a cage for 3.5"" HDDs. I tried searching on google and ebay for another cage, but didn't find any for rack servers. Is there a easy way to replace them and find another one or do I need to buy another server?</p>

<p>By Server Cage i mean the whole front where you can insert the HDDs:
<a href=""https://i.sstatic.net/2v07s.png"" rel=""nofollow noreferrer"">Picture</a>
Thank you for the assist.</p>
","<rack><rackmount>","2018-09-23 17:10:38"
"865887","Unable to logon to SQL Server with local SQL user identity","<p>When I create a new user logon and password, the password immediately changes to an unknown, 15-character password.</p>
<p>Logon using the account results in error 18456.</p>
<p>I create a new logon like <a href=""https://i.sstatic.net/HoSXT.png"" rel=""nofollow noreferrer"">this</a> and click ok</p>
<p>Then if I check the properties for the logon immediately after, it looks like <a href=""https://i.sstatic.net/J1WmT.png"" rel=""nofollow noreferrer"">this</a></p>
<p>Windows 10, and SQL Server Express.</p>
","<sql-server><password><users><reset>","2017-07-30 08:47:39"
"791107","is it possible that I can move contents of 2 lvm disks to one big lvm","<p>Is it possible that I can move contents of 2 LVM disks to one big LVM, Say for example, I have 5 LVMs created on 2 PV disks xvdb1 and xvdd1. </p>

<p>Is it possible to merge them both to say xvdf1? I have 5 partitions on these 5 LVMs. (Each partition on each LVM) </p>

<p>Will there be any disruption or corruption in the application data on the disks if I move them with <strong>pvmove</strong>?</p>
","<lvm>","2016-07-21 08:35:44"
"791164","Can PHP have not root permissions, if it is runned via root cron job?","<p>In <code>/etc/crontab</code> i see a job like this: </p>

<p><code>15 15  *   *   *   root    /usr/local/bin/php /path/to/script.php</code></p>

<p>So, with some other settings, cat this php script have not root access, when runned?</p>

<p>My system administrator says, that it's safe and PHP script have no root access. But i'm doubt.</p>
","<php><cron><root>","2016-07-21 12:48:53"
"1038553","How to use public IPs from remote network in my local network","<p><a href=""https://i.sstatic.net/zOYBR.jpg"" rel=""nofollow noreferrer"">The image shows the current remote setup and proposed local network</a></p>
<p>I have 2 networks at different locations. For your better understanding one network I named remote network and another is local network. Both network have 1 linux box (CentOS 6.10) each.
Remote Network scenario:
The remote Linux have Public IP on its eth0 (IP 202.52.151.58, Subnet 255.255.255.224, GW 202.52.151.58). The Linux box connected to a Router(IP 202.52.151.62). The router have 202.52.151.32/27 IP Block aasigned.</p>
<p>Local Network scenario:
The Local Linux eth0 is connected to the internet router with private IP 192.168.1.127. The Linux eth1 is connected to 2 Hosts (1 Windows PC and 1 Voip Gateway) through a Switch. I want to assign the remote network's Public IP for the the hosts at the local network.</p>
<p>Please help me to do so.</p>
","<linux-networking><tunneling>","2020-10-13 14:19:19"
"1004706","need to install open ipmi on a remote HP DL20 server","<p>I have an ubuntu(18.04.2 LTS) image running on a HP DL20, This server has a dedicated iLO, but i fear some settings are wrong as i cant reach the iLO. I would like to install open ipmi on the server to check gateway/subnet thats set in the bios. However my server does not have external internet access as in i cant run 'apt-get install -y openipmi"". I could however scp a file from my SFTP server to the machine and install locally. However I cant figure out how to get the local package. Yesterday i found a tar.gz for open ipmi and de-compressed it, yet when i ran the instructions in the read me, i would get a error in the ./config script. and then when i would do ""make"" it would stop. I'm assuming I need some kind of make package installed as well? is there a simpler way to install open ipmi? im all out of ideas here</p>
","<ubuntu><apt><ipmi><ilo><ipmitool>","2020-02-26 16:57:32"
"791277","PowerShell: Getting a ""does not support user interaction"" error when no input is required","<p>I'm running a PowerShell script during a TFS 2015-based deployment. It won't run because PS thinks it needs user interaction, but, when I copy-and-paste the command and arguments into the PS console and call it directly, it works exactly as expected (with no user input). </p>

<p>Any suggestions? Thanks.</p>

<p>This is the top. I can't include more due to licensing reasons:</p>

<pre><code>[CmdletBinding()]
param(
    [ValidateNotNullOrEmpty()]
    [String]
    $LocalDataPath, # .

    [ValidateNotNullOrEmpty()]
    [String]
    $ApiEndpointUrlPrefix, # http://hostname/ReportServer

    [ValidateNotNullOrEmpty()]
    [String]
    $DataSourceUrlRelPath = ""Data Sources"",

    [ValidateNotNullOrEmpty()]
    [String]
    $DatasetUrlRelPath = ""Datasets"",

    [ValidateNotNullOrEmpty()]
    [String]
    $ReportUrlRelPath = ""Reports"",

    [ValidateNotNullOrEmpty()]
    [String]
    $RootUrlAbsPath = ""/Root""
)
</code></pre>
","<powershell><team-foundation-server>","2016-07-21 22:24:42"
"866144","Windows Server 2016 Datacenter, all 19 shares suddenly accessible to everyone","<p>I am currently running a Windows Server 2016 Datacenter Virtual Machine in Azure. This server has 19 file shares on it. Each of these shares is devoted to a single customer of the company I work for. While troubleshooting a connection issue for a client, I found that all 19 of the file shares had been reconfigured so that the ""Everyone"" principal had full control. They where originally setup so that only the server administrator and a single, client specific account would have access to each file share. </p>

<p>1) Is there a way for me to determine how this happened?
I did not have auditing configured on the server.</p>

<p>2) Is it possible that this happened without human intervention? Could some obscure windows bug have caused this?</p>

<p>3) How to I prevent this from happening again? I used the lastpass password generator when creating credentials for the group of people who administer this server. None of the accounts associated with customer file shares are authorized to remote into the server.</p>
","<windows><security><server-message-block><share>","2017-07-31 18:51:08"
"1038710","separate email server for the subdomain","<p>I am having some serious problems setting up the proper DNS addresses for my domain.</p>
<p>My DNS records are like</p>
<pre><code>mail              A         ip1 (shared hosting provider)
mail              A         ip2 (VPS mail server)
info              A         ip2
mail.info         CNAME     domain.com
info.domain.com   MX    5   mail.info
</code></pre>
<p>Now, from server 2. I am able to send emails but when someone replies to the email then I am not receiving the email. there might be some issues in these above DNS records. can someone please help me figure it out?</p>
","<domain-name-system><email><email-server><mx-record><dns-zone>","2020-10-14 15:30:25"
"866159","Sendmail ""Connection refused by mail.example.com"" when attempting to send emails","<p>I'm trying to have a virtually hosted PHP site send emails to a particular Gmail address when the site's contact form is correctly filled out and submitted.  Unfortunately, nothing is being sent out.  In <code>/var/log/mail.log</code> I have numerous instances of things like this:</p>

<pre><code>Jul 31 16:38:51 picus sm-mta[28576]: v6SL4xII018234: to=&lt;person@gmail.com&gt;,
delay=2+23:33:52, xdelay=00:00:00, mailer=esmtp, pri=38910659, 
relay=mail.example.com., dsn=4.0.0, stat=Deferred:Connection refused by 
mail.example.com.
</code></pre>

<p>I'm completely new to handling an email server, and simply wanted to use Sendmail since it's built into the distro I'm using and Swiftmailer supports it, so I'm not sure how to fix the problem.</p>

<p>My <code>sendmail.mc</code> file looks like:</p>

<pre><code>divert(-1)dnl
#-----------------------------------------------------------------------------
# $Sendmail: debproto.mc,v 8.14.4 2014-10-02 17:54:06 cowboy Exp $
#
# Copyright (c) 1998-2010 Richard Nelson.  All Rights Reserved.
#
# cf/debian/sendmail.mc.  Generated from sendmail.mc.in by configure.
#
# sendmail.mc prototype config file for building Sendmail 8.14.4
#
# Note: the .in file supports 8.7.6 - 9.0.0, but the generated
#       file is customized to the version noted above.
#
# This file is used to configure Sendmail for use with Debian systems.
#
# If you modify this file, you will have to regenerate /etc/mail/sendmail.cf
# by running this file through the m4 preprocessor via one of the following:
#       * make   (or make -C /etc/mail)
#       * sendmailconfig
#       * m4 /etc/mail/sendmail.mc &gt; /etc/mail/sendmail.cf
# The first two options are preferred as they will also update other files
# that depend upon the contents of this file.
#
# The best documentation for this .mc file is:
# /usr/share/doc/sendmail-doc/cf.README.gz
#
#-----------------------------------------------------------------------------
divert(0)dnl
#
#   Copyright (c) 1998-2005 Richard Nelson.  All Rights Reserved.
#
#  This file is used to configure Sendmail for use with Debian systems.
#
define(`_USE_ETC_MAIL_')dnl
include(`/usr/share/sendmail/cf/m4/cf.m4')dnl
VERSIONID(`$Id: sendmail.mc, v 8.14.4-8 2014-10-02 17:54:06 cowboy Exp $')
OSTYPE(`debian')dnl
DOMAIN(`debian-mta')dnl
dnl # Items controlled by /etc/mail/sendmail.conf - DO NOT TOUCH HERE
undefine(`confHOST_STATUS_DIRECTORY')dnl        #DAEMON_HOSTSTATS=
dnl # Items controlled by /etc/mail/sendmail.conf - DO NOT TOUCH HERE
dnl #
dnl # General defines
dnl #
dnl # SAFE_FILE_ENV: [undefined] If set, sendmail will do a chroot()
dnl #   into this directory before writing files.
dnl #   If *all* your user accounts are under /home then use that
dnl #   instead - it will prevent any writes outside of /home !
dnl #   define(`confSAFE_FILE_ENV',             `')dnl
dnl #
dnl # Daemon options - restrict to servicing LOCALHOST ONLY !!!
dnl # Remove `, Addr=' clauses to receive from any interface
dnl # If you want to support IPv6, switch the commented/uncommentd lines
dnl #
FEATURE(`no_default_msa')dnl
dnl DAEMON_OPTIONS(`Family=inet6, Name=MTA-v6, Port=smtp, Addr=::1')dnl
DAEMON_OPTIONS(`Family=inet,  Name=MTA-v4, Port=smtp, Addr=127.0.0.1')dnl
dnl DAEMON_OPTIONS(`Family=inet6, Name=MSP-v6, Port=submission, M=Ea, Addr=::1')dnl
DAEMON_OPTIONS(`Family=inet,  Name=MSP-v4, Port=submission, M=Ea, Addr=127.0.0.1')dnl
dnl #
dnl # Be somewhat anal in what we allow
define(`confPRIVACY_FLAGS',dnl
`needmailhelo,needexpnhelo,needvrfyhelo,restrictqrun,restrictexpand,nobodyreturn,authwarnings')dnl
dnl #
dnl # Define connection throttling and window length
define(`confCONNECTION_RATE_THROTTLE', `15')dnl
define(`confCONNECTION_RATE_WINDOW_SIZE',`10m')dnl
dnl #
dnl # Features
dnl #
dnl # use /etc/mail/local-host-names
FEATURE(`use_cw_file')dnl
dnl #
dnl # The access db is the basis for most of sendmail's checking
FEATURE(`access_db', , `skip')dnl
dnl #
dnl # The greet_pause feature stops some automail bots - but check the
dnl # provided access db for details on excluding localhosts...
FEATURE(`greet_pause', `1000')dnl 1 seconds
dnl #
dnl # Delay_checks allows sender&lt;-&gt;recipient checking
FEATURE(`delay_checks', `friend', `n')dnl
dnl #
dnl # If we get too many bad recipients, slow things down...
define(`confBAD_RCPT_THROTTLE',`3')dnl
dnl #
dnl # Stop connections that overflow our concurrent and time connection rates
FEATURE(`conncontrol', `nodelay', `terminate')dnl
FEATURE(`ratecontrol', `nodelay', `terminate')dnl
dnl #
dnl # If you're on a dialup link, you should enable this - so sendmail
dnl # will not bring up the link (it will queue mail for later)
dnl define(`confCON_EXPENSIVE',`True')dnl
dnl #
dnl # Dialup/LAN connection overrides
dnl #
include(`/etc/mail/m4/dialup.m4')dnl
include(`/etc/mail/m4/provider.m4')dnl
dnl #
dnl # Default Mailer setup
MAILER_DEFINITIONS
MAILER(`local')dnl
MAILER(`smtp')dnl

dnl # Masquerading options
FEATURE(`always_add_domain')dnl
MASQUERADE_AS(`debian')dnl
FEATURE(`allmasquerade')dnl
FEATURE(`masquerade_envelope')dnl
</code></pre>

<p>I think it may be a DNS issue, however, as I cannot connect to the server via telnet on port 25.  The port seems to be open in my iptables:</p>

<pre><code>$ sudo iptables -L -nv --line-numbers

Chain INPUT (policy ACCEPT 0 packets, 0 bytes)
num   pkts bytes target     prot opt in     out     source               destination         
1        0     0 DROP       all  --  *      *       46.7.84.73           0.0.0.0/0           
2     4047  304K fail2ban-ssh-ddos  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            multiport dports 22
3     4926  376K fail2ban-recidive  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0           
4     4048  304K fail2ban-ssh  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            multiport dports 22
5      213 26142 ACCEPT     all  --  lo     *       0.0.0.0/0            0.0.0.0/0           
6        0     0 REJECT     all  --  !lo    *       127.0.0.0/8          0.0.0.0/0            reject-with icmp-port-unreachable
7        2    60 ACCEPT     icmp --  *      *       0.0.0.0/0            0.0.0.0/0            state NEW icmptype 8
8       26  1456 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:22 state NEW
9       14   816 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:80 state NEW
10      16   932 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:443 state NEW
11       0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:587 state NEW
12       4   240 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:25 state NEW
13    4903  378K ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            state RELATED,ESTABLISHED
14     114  7692 LOG        all  --  *      *       0.0.0.0/0            0.0.0.0/0            limit: avg 5/min burst 5 LOG flags 0 level 7 prefix ""iptables_INPUT_denied: ""
15     130 13980 REJECT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            reject-with icmp-port-unreachable

Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
num   pkts bytes target     prot opt in     out     source               destination         
1        0     0 DROP       all  --  *      *       46.7.84.73           0.0.0.0/0           
2        0     0 LOG        all  --  *      *       0.0.0.0/0            0.0.0.0/0            limit: avg 5/min burst 5 LOG flags 0 level 7 prefix ""iptables_FORWARD_denied: ""
3        0     0 REJECT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            reject-with icmp-port-unreachable

Chain OUTPUT (policy ACCEPT 129 packets, 11501 bytes)
num   pkts bytes target     prot opt in     out     source               destination         
1        0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:587
2      105 15686 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:25

Chain fail2ban-recidive (1 references)
num   pkts bytes target     prot opt in     out     source               destination         
1     4926  376K RETURN     all  --  *      *       0.0.0.0/0            0.0.0.0/0           

Chain fail2ban-ssh (1 references)
num   pkts bytes target     prot opt in     out     source               destination         
1       10  1276 REJECT     all  --  *      *       91.197.232.103       0.0.0.0/0            reject-with icmp-port-unreachable
2     4038  303K RETURN     all  --  *      *       0.0.0.0/0            0.0.0.0/0     
</code></pre>

<p>The result of <code>$ netstat -tan | grep LISTEN | grep 127.0.0.1 | sort</code> is:</p>

<pre><code>tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN     
tcp        0      0 0.0.0.0:443             0.0.0.0:*               LISTEN     
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN     
tcp6       0      0 :::443                  :::*                    LISTEN     
tcp6       0      0 :::80                   :::*                    LISTEN 
</code></pre>

<p>But the result of <code>$ netstat -tan | grep 25</code> is</p>

<pre><code>tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN 
</code></pre>

<p>Is there a reason why it's not showing up in the first group?  </p>
","<sendmail><linode>","2017-07-31 20:52:58"
"932624","IUS not pulling latest PHP packages","<p>I have a RHEL 7 64 bit machine that that IUS installed on it.  We are trying to get the latest hotfix version of php 7.1.  The version I am trying to download is 7.1.22, and we are on 7.1.21.  I have cleaned our cache with <code>yum clean all</code> and then rebuilt it using <code>yum makecache</code>.</p>

<p>Here is the IUS webpage - <a href=""https://dl.iuscommunity.org/pub/ius/stable/Redhat/7/x86_64/repoview/letter_p.group.html"" rel=""nofollow noreferrer"">https://dl.iuscommunity.org/pub/ius/stable/Redhat/7/x86_64/repoview/letter_p.group.html</a></p>

<p>I have installed the RPM with IUS installed, and enabled the IUS repositories.</p>

<pre><code>[ius-archive]
name=IUS Community Packages for Enterprise Linux 7 - $basearch - Archive
#baseurl=https://dl.iuscommunity.org/pub/ius/archive/Redhat/7/$basearch
mirrorlist=https://mirrors.iuscommunity.org/mirrorlist?repo=ius-el7-archive&amp;arch=$basearch&amp;protocol=http
failovermethod=priority
enabled=1
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/IUS-COMMUNITY-GPG-KEY
</code></pre>

<p>However when I do a yum upgrade on any of the php packages that need to be updated (even if I do something like <code>yum upgrade php71u</code>), I get - <code>No packages marked for update</code>.</p>

<p>Here is a snapshot of the packages on the server - </p>

<pre><code>[root@myserver yum.repos.d]# rpm -qa | grep php71u
php71u-bcmath-7.1.21-1.ius.el7.x86_64
php71u-pdo-7.1.21-1.ius.el7.x86_64
php71u-tidy-7.1.21-1.ius.el7.x86_64
php71u-xml-7.1.21-1.ius.el7.x86_64
php71u-mcrypt-7.1.21-1.ius.el7.x86_64
php71u-xmlrpc-7.1.21-1.ius.el7.x86_64
php71u-json-7.1.21-1.ius.el7.x86_64
php71u-dba-7.1.21-1.ius.el7.x86_64
php71u-common-7.1.21-1.ius.el7.x86_64
php71u-ldap-7.1.21-1.ius.el7.x86_64
php71u-pecl-igbinary-2.0.5-2.ius.el7.x86_64
php71u-devel-7.1.21-1.ius.el7.x86_64
php71u-opcache-7.1.21-1.ius.el7.x86_64
php71u-cli-7.1.21-1.ius.el7.x86_64
</code></pre>

<p>However when I run a <code>yum update php71u-cli</code>, it tells me <code>No packages marked for update</code>.  However, on the IUS stable site, the package is on version 7.1.22 (which is what I want to update to).  <a href=""https://dl.iuscommunity.org/pub/ius/stable/Redhat/7/x86_64/repoview/php71u-cli.html"" rel=""nofollow noreferrer"">https://dl.iuscommunity.org/pub/ius/stable/Redhat/7/x86_64/repoview/php71u-cli.html</a>.</p>

<p>This problem is replicated for each php package that I try to update to version 7.1.22.  When I try to do a yum install of that specific version, it gives me the same error, and when I do a <code>yum --showduplicates list php71u-cli</code> I dont see the expected version inside of the list.</p>

<p>How can I get the 7.1.22 hotfix from IUS?</p>

<p>Thanks.</p>
","<linux><php><redhat><yum>","2018-09-25 20:03:06"
"866196","Postfix stop continuously","<p>I just ask this question in <a href=""https://unix.stackexchange.com"">unix.stackexchange.com</a> but I did not get any answer. I hope will lucky in here at least. Please ignore the question's stupidity. I am a noob to linux.</p>

<p>I was successfully configured Sendgrid into a Google Cloud Instance which is CPanel installed on CentOS. <a href=""https://cloud.google.com/compute/docs/tutorials/sending-mail/using-sendgrid"" rel=""nofollow noreferrer"">Here</a> I have done it.</p>

<p>When install Postfix I have to remove currently installed exim4. Installation is successful, But I found sometimes Postfix is stopped. If I start using <code># postfix start</code> again it will stopped in few minutes.</p>

<p>Below is the maillog:</p>

<pre><code># tail -n 30 /var/log/maillog

Jul 28 04:24:33 myhost postfix/qmgr[15174]: BAD9D110BEAB3: from=&lt;root@myhost.localdomain&gt;, size=3665, nrcpt=1 (queue active)
Jul 28 04:24:33 myhost postfix/smtp[15181]: BAD9D110BEAB3: to=&lt;someperson@mycompany.com.au&gt;, relay=smtp.sendgrid.net[167.89.125.25]:2525, delay=152, delays=152/0.02/0.1/0.03, dsn=2.0.0, status=sent (250 Ok: queued as q4j-M5JVRM61Sw9du237_g)
Jul 28 04:24:33 myhost postfix/qmgr[15174]: BAD9D110BEAB3: removed
Jul 28 04:24:33 myhost postfix/smtp[15177]: AA9A8110BEAA3: to=&lt;someperson@mycompany.com.au&gt;, relay=smtp.sendgrid.net[167.89.125.25]:2525, delay=577, delays=576/0.02/0.11/0.06, dsn=2.0.0, status=sent (250 Ok: queued as AcEHW3P5Q5K7PgnA0qvsYQ)
Jul 28 04:24:33 myhost postfix/qmgr[15174]: AA9A8110BEAA3: removed
Jul 28 04:24:33 myhost postfix/smtp[15180]: B5A5B110BEAB2: to=&lt;someperson@mycompany.com.au&gt;, relay=smtp.sendgrid.net[167.89.125.25]:2525, delay=152, delays=152/0.01/0.09/0.06, dsn=2.0.0, status=sent (250 Ok: queued as QumlXDnNQHekR3TnPfC_Kg)
Jul 28 04:24:33 myhost postfix/qmgr[15174]: B5A5B110BEAB2: removed
Jul 28 04:24:33 myhost postfix/smtp[15179]: B29A2110BEAA4: to=&lt;someperson@mycompany.com.au&gt;, relay=smtp.sendgrid.net[167.89.125.25]:2525, delay=577, delays=576/0.02/0.18/0.02, dsn=2.0.0, status=sent (250 Ok: queued as k-HSDlzjTPiR3mxxXWZj2A)
Jul 28 04:24:33 myhost postfix/qmgr[15174]: B29A2110BEAA4: removed
Jul 28 04:29:35 myhost spamc[15925]: connect to spamd on ::1 failed, retrying (#1 of 3): Connection refused
Jul 28 04:29:36 myhost spamd[1195]: zoom: able to use 996/997 'body_0' compiled rules (99.899%)
Jul 28 04:29:37 myhost spamd[1195]: spamd: server started on IO::Socket::INET [_]:783 (running version 3.4.1)
Jul 28 04:29:37 myhost spamd[1195]: spamd: server pid: 1195
Jul 28 04:29:37 myhost spamd[1195]: spamd: server successfully spawned child process, pid 15934
Jul 28 04:29:37 myhost spamd[1195]: prefork: child states: B
Jul 28 04:29:37 myhost spamd[15934]: spamd: connection from localhost [127.0.0.1]:39080 to port 783, fd 5
Jul 28 04:29:37 myhost spamd[1195]: spamd: server successfully spawned child process, pid 15935
Jul 28 04:29:37 myhost spamd[1195]: prefork: adjust: 0 idle children less than 1 minimum idle children. Increasing spamd children: 15935 started.
Jul 28 04:29:37 myhost spamd[1195]: prefork: child states: BI
Jul 28 04:29:37 myhost spamd[1195]: prefork: child states: II
Jul 28 04:29:37 myhost spamd[1195]: prefork: adjust: 2 idle children more than 1 maximum idle children. Decreasing spamd children: 15935 killed.
Jul 28 04:29:37 myhost spamd[1195]: prefork: child states: IK
Jul 28 04:29:37 myhost spamd[1195]: spamd: handled cleanup of child pid [15935] due to SIGCHLD: interrupted, signal 2 (0002)
Jul 28 04:29:37 myhost dovecot: pop3-login: Aborted login (no auth attempts in 0 secs): user=&lt;&gt;, rip=127.0.0.1, lip=127.0.0.1, secured, session=&lt;6cb+IFlVIr1/AAAB&gt;
Jul 28 04:29:38 myhost dovecot: lmtp(15985): Connect from local
Jul 28 04:29:38 myhost dovecot: lmtp(15985): Disconnect from local: Successful quit
Jul 28 04:29:38 myhost dovecot: imap-login: Login: user=&lt;__cpanel__service__auth__imap__x4omw2slm18rtsvbmuqztk47y34ogk65i4b9jtgduyu1py...&gt;, method=PLAIN, rip=127.0.0.1, lip=127.0.0.1, mpid=15998, secured, session=&lt;lGoLIVlV9Ld/AAAB&gt;
Jul 28 04:29:38 myhost dovecot: imap(__cpanel__service__auth__imap__x4omw2slm18rtsvbmuqztk47y34ogk65i4b9jtgduyu1pylltnu7jebtjldvddpu): Logged out in=11, out=470, bytes=11/470
Jul 28 04:29:38 myhost postfix/master[15172]: terminating on signal 15
Jul 28 04:29:40 myhost postfix/sendmail[16036]: fatal: Recipient addresses must be specified on the command line or via the -t option
</code></pre>

<p>I want to know what is happening here and a way to keep Postfix continuously running?</p>
","<postfix><centos6><exim><google-cloud-platform><sendgrid>","2017-08-01 04:43:13"
"866220","Hundreds of fake e-mails in my mail queue every day","<p>I run postfix/dovecot/spamassasin/clamav/amavis mail server. Recently (maybe 1 month) I find about 500 undelivered e-mails in my mailq. I delete them, but I guess it is not a helathy permanent solution to my problem.</p>

<p>And then I have problems to deliver e-mails due to the sending MTA's poor reputation.</p>

<p>Example of log entry in mailq:</p>

<pre><code>A19EC23DE0     4613 Tue Aug  1 06:57:01  MAILER-DAEMON
(delivery temporarily suspended: connect to hayatdiyaliz.com[185.140.110.3]:25: Connection refused)
jack3p0gz7b@hayatdiyaliz.com
</code></pre>

<p>There are about 20 ""fake"" domains like hayatdiyaliz.com and 20-50 e-mails per domain. The mailboxes' names are always like this ""name"".""multiple_random_characters""@domain.com</p>

<pre><code>jack8d7413@domain.com
emily8186ga@domain.com
william564av4@domain.com
</code></pre>

<p>Do you have any idea how to prevent this situation and what steps should I take to do something about the MTA's poor reputation?</p>

<p>I added this to my postfixs main.cf, but that did not help:</p>

<pre><code>smtpd_sender_restrictions = reject_unlisted_sender
</code></pre>

<p>Thank you</p>

<p>EDIT: the <a href=""https://serverfault.com/questions/419407"">Fighting Spam - What can I do as an: Email Administrator, Domain Owner, or User?</a> question is too wide-range, I thought maybe someone had this particular problem, It used to be okay, all test I tried online were OK, deliverability tests, open relay test, etc.</p>
","<postfix><spam><queue>","2017-08-01 08:04:51"
"791488","Need to run httpd and jetty on the same centos7 server","<p>I have a VPS with CentOS7 with httpd + php running on 80 port and let's say with <code>domainOne.tld</code>.</p>

<p>All of the web files are in <code>/var/www/html</code>.</p>

<p>Now I need to run a java application on jetty on another domain, let's say <code>domainTwo.tld</code>.</p>

<p>If I just install jetty it will be on 8080 port while httpd is on 80. They should work okay I guess, but is it possible to attach <code>domainTwo.tld</code> to jetty's 8080 port while it not be accessed on <code>domainOne.tld:8080</code> ?</p>
","<httpd><jetty>","2016-07-22 18:34:37"
"791530","An active directory domain could not be contacted","<p>I have this problem with joining client PCs to the domain. I think i did everything right, set up AD DS, DHCP, DNS on my server, turned off the DHCP server on my router. I ensured that the Preferred DNS server on the client computer is the IP of my Server and then I tried to join the domain but got an error saying </p>

<pre><code>The Active Directory Domain controller could not be contacted
</code></pre>

<p>I googled around and noticed most of the solutions suggested to give the client computer a static IP, set the default gateway to the router's IP address and set the preferred DNS to the server's IP address. Followed the exact step and I was able to join the domain. The problem is I have up to 100 PCs I want to join to the domain and its going to be a total pain to start remembering where I stopped with the last manual IP assignment and also prevent clashes. I noticed that if i didn't do the static IP configuration on the client side, running IP config revealed some IP address that looked like public IPs which is different from what my ISP assigned to us.</p>

<p>In summary, is the server DHCP configuration not supposed to be the one handling out IPs? If yes, then why do i still need to give client PCs static IPs to be able to join the domain? Why can't I join the domain without configuring static IP on the client side?. Any help is much appreciated.
PS: I use windows server 2012 Standard
IP Settings from DC
<a href=""https://i.sstatic.net/d9nc4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/d9nc4.png"" alt=""enter image description here""></a>
IpConfig /all result for DC</p>

<pre><code>C:\Users\Administrator&gt;ipconfig /all

Windows IP Configuration

   Host Name . . . . . . . . . . . . : zedcrestcapital
   Primary Dns Suffix  . . . . . . . : zedcrest.com
   Node Type . . . . . . . . . . . . : Mixed
   IP Routing Enabled. . . . . . . . : No
   WINS Proxy Enabled. . . . . . . . : No
   DNS Suffix Search List. . . . . . : zedcrest.com

Ethernet adapter Ethernet:

   Connection-specific DNS Suffix  . :
   Description . . . . . . . . . . . : Microsoft Hyper-V Network Adapter
   Physical Address. . . . . . . . . : 00-15-5D-01-15-02
   DHCP Enabled. . . . . . . . . . . : No
   Autoconfiguration Enabled . . . . : Yes
   Link-local IPv6 Address . . . . . : fe80::6db9:db40:551f:87d3%12(Preferred)
   IPv4 Address. . . . . . . . . . . : 192.168.1.3(Preferred)
   Subnet Mask . . . . . . . . . . . : 255.255.255.0
   Default Gateway . . . . . . . . . : 192.168.1.1
   DNS Servers . . . . . . . . . . . : ::1
                                       192.168.1.3
                                       192.168.1.1
   NetBIOS over Tcpip. . . . . . . . : Enabled

Tunnel adapter isatap.{E7C36792-32F9-4B58-B251-DEC330C868E4}:

   Media State . . . . . . . . . . . : Media disconnected
   Connection-specific DNS Suffix  . :
   Description . . . . . . . . . . . : Microsoft ISATAP Adapter
   Physical Address. . . . . . . . . : 00-00-00-00-00-00-00-E0
   DHCP Enabled. . . . . . . . . . . : No
   Autoconfiguration Enabled . . . . : Yes
</code></pre>

<p>DHCP Scope Setting for DC
<a href=""https://i.sstatic.net/gRm7o.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gRm7o.png"" alt=""enter image description here""></a>
DNS Settings for DC
<a href=""https://i.sstatic.net/oZXec.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oZXec.png"" alt=""enter image description here""></a></p>

<p>Error on Client Side while joining Domain</p>

<pre><code>Note: This information is intended for a network administrator.  If you are not your network's administrator, notify the administrator that you received this information, which has been recorded in the file C:\WINDOWS\debug\dcdiag.txt.

The following error occurred when DNS was queried for the service location (SRV) resource record used to locate an Active Directory Domain Controller (AD DC) for domain ""zedcrest.com"":

The error was: ""This operation returned because the timeout period expired.""
(error code 0x000005B4 ERROR_TIMEOUT)

The query was for the SRV record for _ldap._tcp.dc._msdcs.zedcrest.com

The DNS servers used by this computer for name resolution are not responding. This computer is configured to use DNS servers with the following IP addresses:

192.168.1.3

Verify that this computer is connected to the network, that these are the correct DNS server IP addresses, and that at least one of the DNS servers is running.
</code></pre>

<p>IP settings of Client</p>

<p><a href=""https://i.sstatic.net/Xf6Bl.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Xf6Bl.png"" alt=""enter image description here""></a></p>

<p>ipconfig /all result for client</p>

<pre><code>Windows IP Configuration

   Host Name . . . . . . . . . . . . : ZEDVANCE-PC
   Primary Dns Suffix  . . . . . . . :
   Node Type . . . . . . . . . . . . : Mixed
   IP Routing Enabled. . . . . . . . : No
   WINS Proxy Enabled. . . . . . . . : No

Ethernet adapter Ethernet:

   Media State . . . . . . . . . . . : Media disconnected
   Connection-specific DNS Suffix  . :
   Description . . . . . . . . . . . : Realtek PCIe FE Family Controller
   Physical Address. . . . . . . . . : F8-A9-63-88-76-A6
   DHCP Enabled. . . . . . . . . . . : No
   Autoconfiguration Enabled . . . . : Yes

Wireless LAN adapter Wi-Fi:

   Media State . . . . . . . . . . . : Media disconnected
   Connection-specific DNS Suffix  . :
   Description . . . . . . . . . . . : Ralink RT3290 802.11bgn Wi-Fi Adapter
   Physical Address. . . . . . . . . : 90-48-9A-0C-12-65
   DHCP Enabled. . . . . . . . . . . : Yes
   Autoconfiguration Enabled . . . . : Yes

Wireless LAN adapter Local Area Connection* 13:

   Media State . . . . . . . . . . . : Media disconnected
   Connection-specific DNS Suffix  . :
   Description . . . . . . . . . . . : Microsoft Wi-Fi Direct Virtual Adapter #3
   Physical Address. . . . . . . . . : 48-EE-0C-26-2B-E4
   DHCP Enabled. . . . . . . . . . . : Yes
   Autoconfiguration Enabled . . . . : Yes

Wireless LAN adapter Wi-Fi 3:

   Connection-specific DNS Suffix  . :
   Description . . . . . . . . . . . : D-Link DWA-121 Wireless N USB Adapter
   Physical Address. . . . . . . . . : 48-EE-0C-26-2B-E4
   DHCP Enabled. . . . . . . . . . . : Yes
   Autoconfiguration Enabled . . . . : Yes
   Link-local IPv6 Address . . . . . : fe80::1926:c27f:5f08:8bd6%21(Preferred)
   Autoconfiguration IPv4 Address. . : 169.254.139.214(Preferred)
   Subnet Mask . . . . . . . . . . . : 255.255.0.0
   Default Gateway . . . . . . . . . :
   DHCPv6 IAID . . . . . . . . . . . : 323546636
   DHCPv6 Client DUID. . . . . . . . : 00-01-00-01-1D-81-30-3C-F8-A9-63-88-76-A6
   DNS Servers . . . . . . . . . . . : 192.168.1.3
   NetBIOS over Tcpip. . . . . . . . : Enabled

Ethernet adapter Bluetooth Network Connection:

   Media State . . . . . . . . . . . : Media disconnected
   Connection-specific DNS Suffix  . :
   Description . . . . . . . . . . . : Bluetooth Device (Personal Area Network)
   Physical Address. . . . . . . . . : 90-48-9A-0C-12-66
   DHCP Enabled. . . . . . . . . . . : Yes
   Autoconfiguration Enabled . . . . : Yes

Tunnel adapter isatap.{D178DE62-2A03-433F-914A-0750C974FB9E}:

   Media State . . . . . . . . . . . : Media disconnected
   Connection-specific DNS Suffix  . :
   Description . . . . . . . . . . . : Microsoft ISATAP Adapter
   Physical Address. . . . . . . . . : 00-00-00-00-00-00-00-E0
   DHCP Enabled. . . . . . . . . . . : No
   Autoconfiguration Enabled . . . . : Yes

Tunnel adapter Local Area Connection* 11:

   Media State . . . . . . . . . . . : Media disconnected
   Connection-specific DNS Suffix  . :
   Description . . . . . . . . . . . : Microsoft Teredo Tunneling Adapter
   Physical Address. . . . . . . . . : 00-00-00-00-00-00-00-E0
   DHCP Enabled. . . . . . . . . . . : No
   Autoconfiguration Enabled . . . . : Yes
</code></pre>
","<domain-name-system><active-directory><windows-server-2012><domain-controller><dhcp-server>","2016-07-22 22:32:18"
"791535","Configure DC & Firewall/ Router such that clients can use Internet even if DC is down?","<p><a href=""https://serverfault.com/questions/538122/how-do-windows-domain-clients-behave-if-the-dc-is-offline"">How do Windows domain clients behave if the DC is offline?</a>  </p>

<ul>
<li>Read this and am getting some symptoms. Curious about the answer for this scenario. </li>
</ul>

<p><a href=""https://serverfault.com/questions/699432/when-my-windows-server-2012-r2-essentials-is-down-clients-cant-go-on-the-inter"">When my Windows Server 2012 R2 Essentials is down, clients can&#39;t go on the internet. Is there a way that they could?</a></p>

<ul>
<li>Read this but its for 2012 and talks about having a Windows server. Is this not possible to do in CONJUNCTION with a dedicated Firewall/ Router box. </li>
</ul>

<p><a href=""https://serverfault.com/questions/128728/always-a-path-to-the-internet-even-in-windows-sbs-is-off"">Always a path to the internet even in Windows SBS is off</a>  </p>

<ul>
<li>This link says its not a good idea, but.. <strong>Is there anyway to do a SEPARATION of CONCERNS? Let external requests be routed out and internal requests go to the DC?</strong>  </li>
<li>Wondering if there's some part of this we can leverage:<br>
<a href=""https://serverfault.com/questions/214418/moving-the-dhcp-dns-services-from-a-windows-server-active-directory-to-a-linux?rq=1"">Moving the DHCP/DNS services from a Windows server (Active Directory) to a Linux machine</a>  </li>
<li><a href=""https://serverfault.com/questions/618021/prevent-throttle-lan-clients-using-domain-controller-as-gateway?rq=1"">Prevent / Throttle LAN clients using domain controller as gateway</a>  </li>
</ul>

<p><strong>SBS 2008/ Server 2008 DC Environment.</strong> </p>

<p>Originally DHCP was managed by DC, but now its been disabled. </p>

<p>We figured that if DHCP was handed off from the DC, domain clients could still carry on with their usual Internet/ Email activity via ISP connectivity.  </p>

<p>But, for some reason when the DC is down, clients lose Net Access. </p>

<p>A dedicated Router/ Firewall box handles DHCP now. Let's say it uses 2 DNS entries:<br>
ISP DNS 1 or Google DNS 1<br>
ISP DNS 2 or Google DNS 2  </p>

<p>Now, these Internet DNS will not understand or handle LAN / Local Domain DNS needs that the DC understands.  </p>

<p>How should we configure the DC so that it works well (to service local domain DNSes) in co operation with the ISP / Google DNSes for external. </p>

<p>So, that even if the DCs are shut down clients can still access the Internet.  </p>

<p>Could we configure them both such that External Internet requests continue forward via Router/ Firewall and ISP DNS (even if DC is down) while Internal Domain requests go to / via the DC. </p>

<p>Thoughts? Maybe use some kind of DNSMasq/ forward/ redirection from router/ firewall to the DC? </p>
","<windows-server-2008><domain-name-system><windows-sbs-2008><windows-sbs><dnsmasq>","2016-07-22 23:27:52"
"932801","Whitelist Server in SpamAssassin","<p>Iss possible add a server with various subdomains in SpamAssassin Whitelist, like *.example.com?</p>

<p>This is the host instance with subdomains:</p>

<pre><code>email@example-2.com.br hm1480-p-60.example.com [IP-ADDRES-1]
email@example-3.com.br mail28157.hm1315.example.com [IP-ADDRESS-2]
email@example.net mail926.hm1479.example.com [IP-ADDRESS-3]
</code></pre>

<p>I looked for information in the SpamAssassin documentation, but it only tells how to block for a server and a sender domain.</p>

<p><a href=""https://spamassassin.apache.org/full/3.2.x/doc/Mail_SpamAssassin_Conf.html#whitelist_and_blacklist_options"" rel=""nofollow noreferrer"">https://spamassassin.apache.org/full/3.2.x/doc/Mail_SpamAssassin_Conf.html#whitelist_and_blacklist_options</a></p>
","<exim><spamassassin><sa-exim>","2018-09-26 17:11:15"
"791592","phpmyadmin login on 8080 broken?","<p>typical stack, Nginx, varnish, php 7.</p>

<p>If I set the vhost in nginx to 80 I can login without difficulty.  If I move it to 8080 I cant (verified varnish/nginx listening on correct port when troubleshooting.  no issues with wordpress, also tried adminer which works fine, may just use that or another alternative at this point</p>

<p>the blofish secret is set in config.inc.php and PmaAbsoluteUri was set as well, using cookie auth.  php can write to the session directory, mcrypt is enabled and present in phpinfo</p>

<p>I configured varnish to ignore the 
I've run out of productive troubleshooting.</p>

<p>using the latest phpmyadmin version directly from the official site</p>
","<nginx><virtualhost><phpmyadmin>","2016-07-23 17:39:22"
"791605","Megaraid on Lenovo ThinkServer TD200 virtual drive degraded","<p>I have a Lenovo TD200, purchased in around 2009, running RAID 10 with 4 hard drives and Fedora 21. I replaced the backplane alas keep getting the ""PD Missing"" and ""Failed to start operation on drive"" messages in the MegaRAID preboot. I tried to force the drive off line per this tutorial packetmischief.ca/2011/03/31/monitoring-direct-attached-storage-under-esxi but it still fails. Some screen shots show the messages including PD Missing.</p>

<p><img src=""https://i.sstatic.net/x1kJ4.jpg"" alt=""PD Missing"">  </p>

<p><img src=""https://i.sstatic.net/h7Cjb.jpg"" alt=""Failed to start operation on drive""></p>
","<megaraid>","2016-07-23 21:54:53"
"1039023","Bind DNS configuration is not showing A record for domain","<p>I'm new to DNS setup and followed several instructions on setting up Bind DNS. Below is my zone file:</p>
<pre><code>;
$TTL    604800
@       IN      SOA     sandbox.svr. root.viper.sandbox.svr. (
                              13        ; Serial
                         604800         ; Refresh
                          86400         ; Retry
                        2419200         ; Expire
                         604800 )       ; Negative Cache TTL
;

; Name servers
@       IN      NS      sandbox.svr.
@       IN      A       192.168.1.7
; A records for name servers

viper.sandbox.svr   IN  A       192.168.1.7
viper           IN      A       192.168.1.7


;
; BIND reverse data file for local loopback interface
;
$TTL    604800
@       IN      SOA    viper.sandbox.svr. root.sandbox.svr. (
                              11        ;Serial
                         604800         ; Refresh
                          86400         ; Retry
                        2419200         ; Expire
                         604800 )       ; Negative Cache TTL
;

; name servers - NS Records

@       IN      NS      viper.sandbox.svr.

; PTR Records
10      IN      PTR     viper.sandbox.svr.
</code></pre>
<p>When I run</p>
<pre><code>dig sandbox.svr 
</code></pre>
<p>It doesn't return any A record.</p>
<p>but when I test with:</p>
<pre><code>  dig viper.sandbox.svr
</code></pre>
<p>it does return A records:</p>
<pre><code>root@viper:/var/log# dig viper.sandbox.svr

; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; viper.sandbox.svr
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 489
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 65494
;; QUESTION SECTION:
;viper.sandbox.svr.     IN  A

;; ANSWER SECTION:
viper.sandbox.svr.  0   IN  A   192.168.1.7

;; Query time: 0 msec
;; SERVER: 127.0.0.53#53(127.0.0.53)
;; WHEN: Fri Oct 16 20:16:56 WAT 2020
;; MSG SIZE  rcvd: 62

</code></pre>
<p>What I could I be doing wrong? My config is for a local environment and running on Ubuntu 20.04 Desktop.</p>
<p>Please see the syslog output below.</p>
<pre><code>  
Oct 18 08:52:59 viper named[312379]: using default UDP/IPv4 port range: [32768, 60999]
Oct 18 08:52:59 viper named[312379]: using default UDP/IPv6 port range: [32768, 60999]
Oct 18 08:52:59 viper named[312379]: listening on IPv4 interface lo, 127.0.0.1#53
Oct 18 08:52:59 viper named[312379]: listening on IPv4 interface enp0s31f6, 192.168.1.7#53
Oct 18 08:52:59 viper named[312379]: listening on IPv4 interface virbr0, 192.168.122.1#53
Oct 18 08:52:59 viper named[312379]: listening on IPv4 interface br-048207a4c5eb, 172.19.0.1#53
Oct 18 08:52:59 viper named[312379]: listening on IPv4 interface br-22b8ad392785, 172.23.0.1#53
Oct 18 08:52:59 viper named[312379]: listening on IPv4 interface br-48aff5a17a13, 172.18.0.1#53
Oct 18 08:52:59 viper named[312379]: listening on IPv4 interface br-7ce2fd66ca47, 172.21.0.1#53
Oct 18 08:52:59 viper named[312379]: listening on IPv4 interface br-c5039c7c4806, 172.20.0.1#53
Oct 18 08:52:59 viper named[312379]: listening on IPv4 interface docker0, 172.17.0.1#53
Oct 18 08:52:59 viper named[312379]: listening on IPv4 interface br-c89505e1a15e, 172.22.0.1#53
Oct 18 08:52:59 viper named[312379]: unable to set effective uid to 0: Operation not permitted
Oct 18 08:52:59 viper named[312379]: generating session key for dynamic DNS
Oct 18 08:52:59 viper named[312379]: unable to set effective uid to 0: Operation not permitted
Oct 18 08:52:59 viper named[312379]: sizing zone task pool based on 7 zones
Oct 18 08:52:59 viper named[312379]: none:100: 'max-cache-size 90%' - setting to 14184MB (out of 15760MB)
Oct 18 08:52:59 viper named[312379]: obtaining root key for view _default from '/etc/bind/bind.keys'
Oct 18 08:52:59 viper named[312379]: set up managed keys zone for view _default, file 'managed-keys.bind'
Oct 18 08:52:59 viper named[312379]: none:100: 'max-cache-size 90%' - setting to 14184MB (out of 15760MB)
Oct 18 08:52:59 viper named[312379]: configuring command channel from '/etc/bind/rndc.key'
Oct 18 08:52:59 viper named[312379]: command channel listening on 127.0.0.1#953
Oct 18 08:52:59 viper named[312379]: configuring command channel from '/etc/bind/rndc.key'
Oct 18 08:52:59 viper named[312379]: couldn't add command channel ::1#953: address not available
Oct 18 08:52:59 viper named[312379]: managed-keys-zone: loaded serial 40
Oct 18 08:52:59 viper named[312379]: zone 1.168.192.in-addr.arpa/IN: loaded serial 11
Oct 18 08:52:59 viper named[312379]: zone 127.in-addr.arpa/IN: loaded serial 1
Oct 18 08:52:59 viper named[312379]: zone 0.in-addr.arpa/IN: loaded serial 1
Oct 18 08:52:59 viper named[312379]: zone 255.in-addr.arpa/IN: loaded serial 1
Oct 18 08:52:59 viper named[312379]: zone localhost/IN: loaded serial 2
Oct 18 08:52:59 viper named[312379]: zone sandbox.svr/IN: loaded serial 197
Oct 18 08:52:59 viper named[312379]: all zones loaded
Oct 18 08:52:59 viper named[312379]: running
Oct 18 08:52:59 viper named[312379]: zone sandbox.svr/IN: sending notifies (serial 197)
Oct 18 08:52:59 viper named[312379]: address not available resolving './NS/IN': 2001:500:200::b#53
Oct 18 08:52:59 viper named[312379]: address not available resolving './NS/IN': 2001:500:1::53#53
Oct 18 08:52:59 viper named[312379]: address not available resolving './NS/IN': 2001:500:2::c#53
Oct 18 08:52:59 viper named[312379]: address not available resolving './NS/IN': 2001:503:c27::2:30#53
Oct 18 08:52:59 viper named[312379]: address not available resolving './NS/IN': 2001:500:2d::d#53
Oct 18 08:52:59 viper named[312379]: address not available resolving './NS/IN': 2001:500:12::d0d#53
Oct 18 08:52:59 viper named[312379]: address not available resolving './NS/IN': 2001:503:ba3e::2:30#53
Oct 18 08:52:59 viper named[312379]: address not available resolving './NS/IN': 2001:500:2f::f#53
Oct 18 08:52:59 viper named[312379]: address not available resolving './NS/IN': 2001:500:9f::42#53
Oct 18 08:52:59 viper named[312379]: address not available resolving './NS/IN': 2001:7fe::53#53
Oct 18 08:52:59 viper named[312379]: address not available resolving './NS/IN': 2001:500:a8::e#53
Oct 18 08:52:59 viper named[312379]: address not available resolving './NS/IN': 2001:7fd::1#53
Oct 18 08:52:59 viper named[312379]: address not available resolving './NS/IN': 2001:dc3::35#53
Oct 18 08:53:00 viper named[312379]: managed-keys-zone: Key 20326 for zone . is now trusted (acceptance timer complete)
Oct 18 08:53:00 viper named[312379]: resolver priming q 
</code></pre>
<p>I tried to troubleshoot by removing my internet connection and noticed that dig responded with the A records but once connected to the intenet, it doesn't. Is there a way to make the <strong>local</strong> DNS resolve for local query and forward public name to say google dns server?</p>
","<domain-name-system><bind><ubuntu-20.04>","2020-10-16 19:19:38"
"791617","Overwriting /etc/ Files between two servers","<p>I'm the process of migrating an NIS server from one machine running Ubuntu 9.10 to a new one running Ubuntu 14.04.4 Server, and following these instructions: </p>

<p><a href=""https://serverfault.com/questions/503363/how-do-i-replace-an-nis-master-server"">How do I replace an NIS master server?</a></p>

<p>The plan is to run the NIS makefile on the new machine, set up one test NIS client for that machine's domain to make sure the ""map"" gets pushed out correctly, and if it works without issues, to go ahead and add the remaining clients' IP addresses. </p>

<p>In step 3 of that guide, it says to copy over all of the passwords, groups, shadow files from the old server to the new one. I will try to do this with an <code>SCP -P</code> command, where the local will be the 14.04.4 machine and the remote will be the old machine. I've noticed in the 14.04.4 machine that in <code>/etc/</code> there already exists passwords, groups, and shadow files. I have not added any users to the 14.04.4 machine. Should I be concerned about overwriting any of these files, and will SCP allow me to overwrite the files?</p>

<p>I'm not an expert in NIS and have never attempted anything like this before so any advice is greatly appreciated. I cannot switch to RHEL or any other *nix OS as this is in a university lab environment so choice of OS is not my call. </p>
","<linux><ubuntu><migration><scp><nis>","2016-07-24 01:42:39"
"791665","Can not connect to the host itself","<p>Well, the problem is simple. I have the site based on apache and trying to execute cron job at this site from the same server. Let's say my site <a href=""http://example.com"" rel=""nofollow noreferrer"">http://example.com</a> and cronjob is </p>

<pre><code>/usr/bin/curl http://example.com/cron.php
</code></pre>

<p>It does not work, error is ""curl: (7) couldn't connect to host"".</p>

<p>Why this could happen?</p>

<p>P.S. The site is working fine and accessible from any other external machine/client.</p>

<p>Here is an output of iptables -S</p>

<pre><code>-P INPUT ACCEPT
-P FORWARD ACCEPT
-P OUTPUT ACCEPT
-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT
-A INPUT -p icmp -j ACCEPT
-A INPUT -i lo -j ACCEPT
-A INPUT -p tcp -m state --state NEW -m tcp --dport 443 -j ACCEPT
-A INPUT -p tcp -m state --state NEW -m tcp --dport 80 -j ACCEPT
-A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT
-A INPUT -p tcp -m state --state NEW -m tcp --dport 2222 -j ACCEPT
-A INPUT -j REJECT --reject-with icmp-host-prohibited
-A FORWARD -j REJECT --reject-with icmp-host-prohibited
</code></pre>

<p>Ping failed: (ping example.com) - here I've used my server domain, of course:</p>

<pre><code>134 packets transmitted, 0 received, 100% packet loss, time 136759ms
</code></pre>

<p>Verbose curl -v :</p>

<pre><code>* About to connect() to myserver.com port 443 (#0)
*   Trying x.x.x.x... Connection timed out
* couldn't connect to host
* Closing connection #0
curl: (7) couldn't connect to host
</code></pre>

<p>x.x.x.x is a external IP of my host.</p>

<p>Result of traceroute myserver.com</p>

<pre><code>traceroute to myserver.com (x.x.x.x), 30 hops max, 60 byte packets
 1  * * *
 2  * * *
 3  * * *
 4  * * *
 5  * * *
 6  * * *
 7  * * *
 8  * * *
 9  * * *
10  * * *
11  * * *
12  * * *
13  * * *
14  * * *
15  * * *
16  * * *
17  * * *
18  * * *
19  * * *
20  * * *
21  * * *
22  * * *
23  * * *
24  * * *
25  * * *
26  * * *
27  * * *
28  * * *
29  * * *
30  * * *
</code></pre>
","<centos><iptables><nat><curl>","2016-07-24 16:44:35"
"866611","How long it takes for domain registrar to update TLD name servers","<p>We accidentally changed the nameserver settings for our example.be.</p>
<p>The changes were rolled back within minutes. So nameservers were set to the previous ones, provided by One.com. After 16 hours, our website still not working. But are worried that the TLD name servers still do not have nameserver records for our domain.</p>
<p><code>dig mydomain.be +trace</code> or <code>dig example.be NS @a.ns.dns.be</code> all reply NXDOMAIN and no nameservers are returned for the domain.</p>
<p>Is it normal that after 16 hours of waiting, TLD name servers still have not been updated and there are no records of our domain?</p>
<hr />
<p><strong>UPDATE</strong></p>
<p>Finally, we got a helpful response from our domain registrar. Here is what they told us:</p>
<blockquote>
<p>this should be fixed now there was an error on the propagation in which the propagation has stuck.</p>
<p>the hostmasters are continually monitoring this unknown error to check if this is a one time error or not</p>
</blockquote>
<p>I believe the problem was that our domain registrar did not propagate updated nameservers to the TLD name servers, that are responsible for .be domain names.</p>
<p>In one minute after their response on the support chat, I could run <code>dig mydomain.be +trace</code> and see that the changes were propagated.</p>
","<domain-name-system><dns-zone><dig><tld>","2017-08-03 07:30:21"
"792031","Nagios - external monitoring host without real connect - how do?","<p>I've got a problem. I have a few external servers that need monitoring by Nagios, but they are in remote VPN networks. I must send monitoring data from vpn's host to Nagios who is working on public server, but I cant connect to any VPN.</p>

<p>My idea is sending the data form hosts to database on Nagios localhost server, and read it to monitoring services values. </p>

<p>My question is; how define hosts without real connection do so Nagios do not not spoke ""Host is down"" or something like this. For this moment I change IP value on 127.0.0.1 for each from external server - and host is UP and on nagios information website and I can add next services for each and read status from local database but is better solution for this ? I just want to add a host without an Ip address and running service in local environment - read from local database.</p>
","<linux><networking><nagios><server-settings>","2016-07-26 14:29:16"
"1005519","Windows Virtual Desktop Max Disconnection Time","<p>We created some new virtual desktop pools on azure.
Now we want to configure that every session which is disconnected ends immediately.</p>

<p>We configured this registry key:</p>

<p>Registry Hive: HKEY_LOCAL_MACHINE
Subkey: \Software\Policies\Microsoft\Windows NT\Terminal Services\
Value Name: MaxDisconnectionTime
Type: REG_DWORD
Value: 0x0000ea60 (60000)</p>

<p>Sadly we still have a lot of disconnected sessions :(
Could anyone help us?</p>
","<windows><azure><virtualization><virtual-machines>","2020-03-04 08:37:12"
"1005525","Unable to delete project in Google Cloud Platform","<p>I want to delete project from Google Cloud Platform but not able to do it.</p>

<p>Here you can see what the problem I'm facing.</p>

<p><a href=""https://i.sstatic.net/CAp0c.jpg"" rel=""nofollow noreferrer"">Screen Shot</a></p>

<p><strong>Update</strong></p>

<p>I got this logs when I try to delete using <code>gcloud</code></p>

<p><code>ERROR: (gcloud.projects.delete) FAILED_PRECONDITION: Project [19823334588] has active child resources. You must delete child resources before deleting the project.
- '@type': type.googleapis.com/google.rpc.PreconditionFailure
  violations:
  - description: Active child resource [services/362464855041] is blocking project
      deletion.
    subject: services/362464855041
    type: CHILD
  - description: Active child resource [services/405735437660] is blocking project
      deletion.
    subject: services/405735437660
    type: CHILD
  - description: Active child resource [services/689680261786] is blocking project
      deletion.
    subject: services/689680261786
    type: CHILD
- '@type': type.googleapis.com/google.rpc.Help
  links:
  - description: Troubleshooting project deletion
    url: https://cloud.google.com/resource-manager/docs/troubleshooting-project-deletion
- '@type': type.googleapis.com/google.rpc.ResourceInfo
  resourceName: projects/19823334588
  resourceType: PROJECT</code></p>
","<google-cloud-platform>","2020-03-04 09:47:48"
"792162","windows server 2008 r2 blue screen error","<p>My server is giving an error like below.</p>

<pre><code>@@@@@@@@@@@@@@@
HARDWARE MALFUNCTION
CALL YOUR HARDWARE VENDOR FOR SUPPIRT.
NMI:PARITY CHECK MEMORY/PARITY ERROR.
THE SYSTEM HAS HALTED.
@@@@@@@@@@@@@@@
</code></pre>

<p>And we called to hp support team the did check and he checked some logs and he said will not occur this problem.after few days again samething is showing an error.hp team is saying that is not hardware problem and which is software problem and followup microsoft team.</p>

<p>please let me now the solution regarding this problem as soon as possible.</p>
","<windows-server-2008-r2>","2016-07-27 06:08:33"
"933447","Need help installing ejabberd on Centos/RHEL7 with SELinux and reverse proxy","<p>I've been trying to get a web client to use EJabberd on the recommended port (5281) through a reverse proxy (HAProxy) on Centos7 with SELinux enabled.  I am not familiar with SELinux context rules.</p>

<p>The basic problem seems to be that HAProxy cannot open port 5281.  I've installed the <code>ejabberd-selinux</code> package thinking that would help on both the proxy system and the system running EJabber.  The ejabberd-selinux seems to configure the standard ports except 5281 which I subsequently attempted to configure myself.</p>
","<centos><haproxy><selinux><ejabberd>","2018-10-01 12:26:29"
"792276","Opening port for webservice in CentOS","<p>I'm trying to open port for webservice in CentOS to access on the same network. Ports 8443 and 8446, they should be open and listening <strong>netstat -an | grep ""LISTEN""</strong> results in:</p>

<pre><code>tcp        0      0 0.0.0.0:8080            0.0.0.0:*               LISTEN
tcp        0      0 0.0.0.0:8443            0.0.0.0:*               LISTEN
tcp        0      0 0.0.0.0:8446            0.0.0.0:*               LISTEN
tcp        0      0 0.0.0.0:9999            0.0.0.0:*               LISTEN
tcp6       0      0 :::80                   :::*                    LISTEN
</code></pre>

<p>And all traffic should be allowed <strong>iptables -L</strong> gives</p>

<pre><code>target     prot opt source               destination
ACCEPT     tcp  --  anywhere             anywhere             tcp dpt:http
</code></pre>

<p>I can access 8080 and 9999 but not 8443 or 8446.</p>

<p><strong>EDIT:</strong> output of <strong>iptables -L -n | less</strong></p>

<pre><code>Chain INPUT (policy ACCEPT)
target     prot opt source               destination
ACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            tcp dpt:80
ACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            tcp dpt:5432

Chain FORWARD (policy ACCEPT)
target     prot opt source               destination
ACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            tcp dpt:5432
</code></pre>
","<centos>","2016-07-27 14:59:52"
"1039810","Save as on file server grayed out","<p>When I try to save a file on my Windows fileserver (a failover cluster with 2 nodes), the window is locked and it's not possible to navigate through the folders, and it's not possible to save files in the actual folder.</p>
<p>The only way is to press the &quot;ESC&quot; key, and at that point it is possible to navigate and save files.</p>
<p>Below a screenshot that hopefully explains my issue:</p>
<p><img src=""https://i.sstatic.net/qV7UN.png"" alt=""my error"" /></p>
","<windows><active-directory><cluster><file-server>","2020-10-23 08:34:14"
"866990","elastic beanstalk ec2 eb ssh suddenly timeout error from yesterday","<p>Ouput INFO: Attempting to open port 22.<br/>
INFO: SSH port 22 open.<br/>
ssh: connect to host xx.xxx.xxx.xxx port 22: Operation timed out <br/>
INFO: Closed port 22 on ec2 instance security group.<br/>
ERROR: An error occurred while running ssh.</p>
","<ssh><amazon-ec2><amazon-web-services>","2017-08-05 05:49:04"
"1040054","Are there known issues on DNS propogation with unusual top-level domains?","<p>Trying to set-up simple A record for example.<strong>software</strong> domain. I'm finding that the DNS isn't propagating for a large number of locations. We changed the registrar recently. Originally it was registered on Google domains, and I believe it had propagated correctly with that registrar.</p>
<p>How can I get my domain to properly propagate globally?</p>
<p><img src=""https://i.sstatic.net/av4Bk.png"" alt=""DNS Check"" /></p>
","<domain-name-system>","2020-10-26 00:20:14"
"1006049","How to log a single-board computer events?","<p>I have a single-board computer, I want to knows whats going on when it boot up and start up because i never get access to its BIOS, can I log the events/activities happening on the board when it get started?</p>

<p><strong>EDIT 1</strong></p>

<p>the logging im trying to achieve is for instances, Java/C#/Python program we can have a system log to monitor whenever it get started, from initializing system components/classes/objects, to API/functions/Server started, we can have the entire program starting process knowing.</p>

<p>I want to achieve this kind of logging monitor to my single-board computer, is it possible? For example our computer, we know it boot up from BIOS, Operating System, our startup processes, but don't know the event what is actually going on, how much RAM space did operating system allocating when boot up? how CPU processing all these boot up procedures?</p>

<p>similar to these, can we have the logging? how to log the entire procedure?</p>

<p><strong>EDIT 2</strong></p>

<p>my question are also bring in a thought of how hardware engineer knowing their component works properly, after input/install firmware?</p>

<p><strong>EDIT 3</strong></p>

<p>What i want to achieve is similar to Android Device logcat, logging every events from system boot up</p>

<p><strong>EDIT 4</strong></p>

<p>I'm using Antminer L3_IO_Board_V1.2</p>
","<hardware><debugging><assembly>","2020-03-08 03:35:09"
"792355","Restarting Service w/ `chkconfig`?","<p>After running:</p>

<pre><code>sudo chkconfig --add X
sudo chkconfig X on
</code></pre>

<p>I ran <code>chkconfig --list X</code>:</p>

<pre><code>$ chkconfig --list X
X           0:off   1:off   2:on    3:on    4:on    5:on    6:off
</code></pre>

<p>My understanding is that, after <code>kill -9</code>-ing it, the service will restart.</p>

<p>However, after running <code>kill -9 $PID</code> where <code>$PID</code> is the PID of the service's process, it did not restart.</p>

<p>In other words, running <code>ps -ef | grep X</code> returned only the <code>grep</code> result.</p>

<p>How can I use <code>chkconfig</code> to ensure that, after my service crashes, it will restart?</p>
","<linux><rhel6><chkconfig>","2016-07-27 21:14:12"
"933477","Active Directory - Nearly all DNS zones and entries are not created","<p>This problem seems exactly the same as <a href=""https://serverfault.com/questions/73875/active-directory-dns-entries-missing-after-installation"">question 73875</a> albeit for a different version of windows and I've tried the accepted answer to no avail, it seems that in my case the problem is not transient and creating the domain a second time changed nothing.</p>

<p>All entries are missing except one ns entry for the domain controller. This is a new server, up to date. It is also my first shot at AD, domains etc but I'm positive I've followed the guide everyone posts (including on technet) to the letter.</p>

<p>I provide a <a href=""https://i.sstatic.net/DBYa6.png"" rel=""nofollow noreferrer"">screenshot of DNS Manager</a>.</p>

<pre><code>C:\Users\Administrator.PDC&gt;dcdiag /test:registerindns /dnsdomain:xxx /v
Starting test: RegisterInDNS
  DNS configuration is sufficient to allow this domain controller to dynamically register the domain controller Locator records in DNS.

  The DNS configuration is sufficient to allow this computer to dynamically register the A record corresponding to its DNS name.

  ......................... PDC passed test RegisterInDNS
</code></pre>

<p>I'm unable to properly understand the output of dcdiag DNS tests, except that delegation seems to be ok, but I include it as well.</p>

<pre><code>C:\Users\Administrator.PDC&gt;dcdiag /test:DNS /dnsdomain:mydomain /v

Directory Server Diagnosis

Performing initial setup:
Trying to find home server...
* Verifying that the local machine PDC, is a Directory Server.
Home Server = PDC
* Connecting to directory service on server PDC.
* Identified AD Forest.
Collecting AD specific global data
* Collecting site info.
Calling ldap_search_init_page(hld,CN=Sites,CN=Configuration,DC=xx,DC=xx,DC=xx,DC=xx,LDAP_SCOPE_SUBTREE,(objectCategory=ntDSSiteSettings),.......
The previous call succeeded
Iterating through the sites
Looking at base site object: CN=NTDS Site Settings,CN=Default-First-Site-Name,CN=Sites,CN=Configuration,DC=xx,DC=xx,DC=xx,DC=xx
Getting ISTG and options for the site
* Identifying all servers.
Calling ldap_search_init_page(hld,CN=Sites,CN=Configuration,DC=xx,DC=xx,DC=xx,DC=xx,LDAP_SCOPE_SUBTREE,(objectClass=ntDSDsa),.......
The previous call succeeded....
The previous call succeeded
Iterating through the list of servers
Getting information for the server CN=NTDS Settings,CN=PDC,CN=Servers,CN=Default-First-Site-Name,CN=Sites,CN=Configuration,DC=xx,DC=xx,DC=xx,DC=xx
objectGuid obtained
InvocationID obtained
dnsHostname obtained
site info obtained
All the info for the server collected
* Identifying all NC cross-refs.
* Found 1 DC(s). Testing 1 of them.
Done gathering initial info.

Doing initial required tests

Testing server: Default-First-Site-Name\PDC
  Starting test: Connectivity
     * Active Directory LDAP Services Check
     The host b144abfd-0418-478c-9056-b947bc9474ad._msdcs.mydomain could not be resolved to an IP address. Check the DNS server, DHCP, server name, etc.
     Got error while checking LDAP and RPC connectivity. Please check your firewall settings.
     ......................... PDC failed test Connectivity

Doing primary tests

Testing server: Default-First-Site-Name\PDC
  Test omitted by user request: Advertising
  Test omitted by user request: CheckSecurityError
  Test omitted by user request: CutoffServers
  Test omitted by user request: FrsEvent
  Test omitted by user request: DFSREvent
  Test omitted by user request: SysVolCheck
  Test omitted by user request: KccEvent
  Test omitted by user request: KnowsOfRoleHolders
  Test omitted by user request: MachineAccount
  Test omitted by user request: NCSecDesc
  Test omitted by user request: NetLogons
  Test omitted by user request: ObjectsReplicated
  Test omitted by user request: OutboundSecureChannels
  Test omitted by user request: Replications
  Test omitted by user request: RidManager
  Test omitted by user request: Services
  Test omitted by user request: SystemLog
  Test omitted by user request: Topology
  Test omitted by user request: VerifyEnterpriseReferences
  Test omitted by user request: VerifyReferences
  Test omitted by user request: VerifyReplicas

  Starting test: DNS

     DNS Tests are running and not hung. Please wait a few minutes...
     See DNS test in enterprise tests section for results
     ......................... PDC passed test DNS

Running partition tests on : DomainDnsZones
  Test omitted by user request: CheckSDRefDom
  Test omitted by user request: CrossRefValidation

Running partition tests on : ForestDnsZones
  Test omitted by user request: CheckSDRefDom
  Test omitted by user request: CrossRefValidation

Running partition tests on : Schema
  Test omitted by user request: CheckSDRefDom
  Test omitted by user request: CrossRefValidation

Running partition tests on : Configuration
  Test omitted by user request: CheckSDRefDom
  Test omitted by user request: CrossRefValidation

Running partition tests on : bo
  Test omitted by user request: CheckSDRefDom
  Test omitted by user request: CrossRefValidation

Running enterprise tests on : mydomain
  Starting test: DNS
     Test results for domain controllers:

        DC: PDC.mydomain
        Domain: mydomain


           TEST: Authentication (Auth)
              Authentication test: Successfully completed

           TEST: Basic (Basc)
              Error: No LDAP connectivity
              The OS Microsoft Windows Server 2016 Standard (Service Pack level: 0.0) is supported.
              NETLOGON service is running
              kdc service is running
              DNSCACHE service is running
              DNS service is running
              DC is a DNS server
              Network adapters information:
              Adapter [00000000] Red Hat VirtIO Ethernet Adapter:
                 MAC address is 32:17:9C:64:E5:3A
                 IP Address is static
                 IP address: 123.456.789.16
                 DNS servers:
                    Warning:
                    127.0.0.1 (pdc.mydomain.) [Invalid]
                    Warning: adapter [00000000] Red Hat VirtIO Ethernet Adapter has invalid DNS server: 127.0.0.1 (pdc.mydomain.)
              Error: all DNS servers are invalid
              No host records (A or AAAA) were found for this DC
              The SOA record for the Active Directory zone was found
              The Active Directory zone on this DC/DNS server was found primary
              Root zone on this DC/DNS server was not found

           TEST: Forwarders/Root hints (Forw)
              Recursion is enabled
              Forwarders Information:
                 123.456.789.250 (&lt;name unavailable&gt;) [Valid]
                 123.456.789.4 (&lt;name unavailable&gt;) [Valid]

           TEST: Delegations (Del)
              Delegation information for the zone: mydomain.
                 Delegated domain name: _msdcs.mydomain.
                    DNS server: pdc.mydomain. IP:123.456.789.16 [Valid]

           TEST: Dynamic update (Dyn)
              Test record dcdiag-test-record added successfully in zone mydomain
              Test record dcdiag-test-record deleted successfully in zone mydomain

        TEST: Records registration (RReg)
           Error: Record registrations cannot be found for all the network adapters

     Summary of test results for DNS servers used by the above domain controllers:

        DNS server: 123.456.789.16 (pdc.mydomain.)
           1 test failure on this DNS server
           Name resolution is not functional. _ldap._tcp.mydomain. failed on the DNS server 123.456.789.16
           [Error details: 9003 (Type: Win32 - Description: DNS name does not exist.)]
           DNS delegation for the domain  _msdcs.mydomain. is operational on IP 123.456.789.16


        DNS server: 123.456.789.250 (&lt;name unavailable&gt;)
           All tests passed on this DNS server

        DNS server: 123.456.789.4 (&lt;name unavailable&gt;)
           All tests passed on this DNS server

     Summary of DNS test results:

                                        Auth Basc Forw Del  Dyn  RReg Ext
        _________________________________________________________________
        Domain: mydomain
           PDC                          PASS FAIL PASS PASS PASS FAIL n/a

     ......................... mydomain failed test DNS
  Test omitted by user request: LocatorCheck
  Test omitted by user request: Intersite
</code></pre>

<p>I add that this domain is being deployed for test/evaluation in an existing unix infrastructure, therefore the wizard complains about not being able to get a delegation. This is expected, however I already configured the delegation in my primary dns (tinydns) and dcdiag is ok with it. In my opinion it's not even part of the equation yet as the zones must exist in the pdc first but I may be wrong of course.</p>

<p>This is the relevant tinydns configuration.</p>

<pre><code>&amp;_udp.mydomain::pdc.mydomain:::
&amp;_tcp.mydomain::pdc.mydomain:::
&amp;_sites.mydomain::pdc.mydomain:::
&amp;_msdcs.mydomain::pdc.mydomain:::
=pdc.mydomain:123.456.789.16:::
6pdc.mydomain:longipv6address:::
</code></pre>

<p>For testing purposes I disabled temporarily ipv6 on the pdc, I also set its only DNS server as 127.0.0.1 as shown on multiple guides. The firewall allows outbound traffic and all inbound from local networks (I also tried disabling temporarily the firewall, couldn't spot a difference). All tests are conducted on the pdc itself.</p>
","<domain-name-system><active-directory><windows-server-2016>","2018-10-01 15:06:08"
"1006059","Postfix 3.4.9 SSL issues -- no shared cipher from servers using TLSv1","<p><strong>Edit 1</strong>: I've narrowed it down to TLSv1 that both servers listed below (no others have failed yet) are attempting to use. I'll be contacting their webmasters requesting they stop using an insecure protocol but in the mean time would still like to figure out what cipher they're attempting to use and enable it for now. I've adjusted the title accordingly.</p>

<p><strong>Edit 2</strong>: Added Postfix version to title</p>

<p><strong>Edit 3</strong>: I've managed to use </p>

<pre><code>smtpd_discard_ehlo_keyword_address_maps = hash:/etc/postfix/smtpd_discard_ehlo_keywords
</code></pre>

<p>with /etc/postfix/smtpd_discard_ehlo_keywords (filename unimportant) containing: </p>

<pre><code>&lt;broken-server-ip&gt; starttls
</code></pre>

<p>I'm still curious as to what TLSv1 cipher these servers are using but now that I can just turn off TLS per host for these cases, I'm content.  Leaving this here for anyone else who needs it.</p>

<p><strong>----Original----</strong></p>

<p>I started off running Debian Buster, and in trying to correct this have since moved to sid while troubleshooting this issue.</p>

<p>I've been fighting an issue where I'm unable to receive mail from a couple servers (mxa3.ubusinessmotion.net and ny-smtp-dmz02.dmz.priceline.com mainly)</p>

<p>Here are the relevant lines from my postfix log: </p>

<pre><code>Mar  8 00:34:31 froxlor postfix/smtpd[67494]: setting up TLS connection from
mxa3.ubusinessmotion.net[208.27.251.227]
Mar  8 00:34:31 froxlor postfix/smtpd[67494]: mxa3.ubusinessmotion.net[208.27.251.227]: TLS cipher list ""aNULL:-aNULL:HIGH:MEDIUM:LOW:EXPORT:+RC4:@STRENGTH""
Mar  8 00:34:31 froxlor postfix/smtpd[67494]: SSL_accept:before SSL initialization
Mar  8 00:34:31 froxlor postfix/smtpd[67494]: SSL_accept:before SSL initialization
Mar  8 00:34:31 froxlor postfix/smtpd[67494]: SSL3 alert write:fatal:handshake failure
Mar  8 00:34:31 froxlor postfix/smtpd[67494]: SSL_accept:error in error
Mar  8 00:34:31 froxlor postfix/smtpd[67494]: SSL_accept error from mxa3.ubusinessmotion.net[208.27.251.227]: -1
Mar  8 00:34:31 froxlor postfix/smtpd[67494]: warning: TLS library problem: error:1417A0C1:SSL routines:tls_post_process_client_hello:no shared cipher:../ssl/statem/statem_srvr.c:2257:
Mar  8 00:34:31 froxlor postfix/smtpd[67494]: lost connection after STARTTLS from mxa3.ubusinessmotion.net[208.27.251.227]
</code></pre>

<p>and</p>

<pre><code>Mar  8 00:18:31 froxlor postfix/smtpd[37732]: connect from ny-smtp-dmz02.dmz.priceline.com[64.6.20.6]
Mar  8 00:18:31 froxlor postfix/smtpd[37732]: setting up TLS connection from ny-smtp-dmz02.dmz.priceline.com[64.6.20.6]
Mar  8 00:18:31 froxlor postfix/smtpd[37732]: ny-smtp-dmz02.dmz.priceline.com[64.6.20.6]: TLS cipher list ""aNULL:-aNULL:HIGH:MEDIUM:LOW:EXPORT:+RC4:@STRENGTH""
Mar  8 00:18:31 froxlor postfix/smtpd[37732]: SSL_accept:before SSL initialization
Mar  8 00:18:31 froxlor postfix/smtpd[37732]: SSL_accept:before SSL initialization
Mar  8 00:18:31 froxlor postfix/smtpd[37732]: SSL3 alert write:fatal:protocol version
Mar  8 00:18:31 froxlor postfix/smtpd[37732]: SSL_accept:error in error
Mar  8 00:18:31 froxlor postfix/smtpd[37732]: SSL_accept error from ny-smtp-dmz02.dmz.priceline.com[64.6.20.6]: -1
Mar  8 00:18:31 froxlor postfix/smtpd[37732]: warning: TLS library problem: error:14209102:SSL routines:tls_early_post_process_client_hello:unsupported protocol:../ssl/statem/statem_srvr.c:1660:
Mar  8 00:18:31 froxlor postfix/smtpd[37732]: lost connection after STARTTLS from ny-smtp-dmz02.dmz.priceline.com[64.6.20.6]
Mar  8 00:18:31 froxlor postfix/smtpd[37732]: disconnect from ny-smtp-dmz02.dmz.priceline.com[64.6.20.6] ehlo=1 starttls=0/1 commands=1/2
</code></pre>

<p>and just for completeness here's one that comes through properly: </p>

<pre><code>Mar  8 00:45:01 froxlor postfix/smtpd[67806]: connect from a27-96.smtp-out.us-west-2.amazonses.com[54.240.27.96]
Mar  8 00:45:01 froxlor postfix/smtpd[67806]: setting up TLS connection from a27-96.smtp-out.us-west-2.amazonses.com[54.240.27.96]
Mar  8 00:45:01 froxlor postfix/smtpd[67806]: a27-96.smtp-out.us-west-2.amazonses.com[54.240.27.96]: TLS cipher list ""aNULL:-aNULL:HIGH:MEDIUM:LOW:EXPORT:+RC4:@STRENGTH""
Mar  8 00:45:01 froxlor postfix/smtpd[67806]: SSL_accept:before SSL initialization
Mar  8 00:45:01 froxlor postfix/smtpd[67806]: SSL_accept:before SSL initialization
Mar  8 00:45:01 froxlor postfix/smtpd[67806]: SSL_accept:SSLv3/TLS read client hello
Mar  8 00:45:01 froxlor postfix/smtpd[67806]: SSL_accept:SSLv3/TLS write server hello
Mar  8 00:45:01 froxlor postfix/smtpd[67806]: SSL_accept:SSLv3/TLS write certificate
Mar  8 00:45:01 froxlor postfix/smtpd[67806]: SSL_accept:SSLv3/TLS write key exchange
Mar  8 00:45:01 froxlor postfix/smtpd[67806]: SSL_accept:SSLv3/TLS write server done
Mar  8 00:45:01 froxlor postfix/smtpd[67806]: SSL_accept:SSLv3/TLS write server done
Mar  8 00:45:01 froxlor postfix/smtpd[67806]: SSL_accept:SSLv3/TLS read client key exchange
Mar  8 00:45:01 froxlor postfix/smtpd[67806]: SSL_accept:SSLv3/TLS read change cipher spec
Mar  8 00:45:01 froxlor postfix/smtpd[67806]: SSL_accept:SSLv3/TLS read finished
Mar  8 00:45:01 froxlor postfix/smtpd[67806]: SSL_accept:SSLv3/TLS write change cipher spec
Mar  8 00:45:01 froxlor postfix/smtpd[67806]: SSL_accept:SSLv3/TLS write finished
Mar  8 00:45:01 froxlor postfix/smtpd[67806]: Anonymous TLS connection established from a27-96.smtp-out.us-west-2.amazonses.com[54.240.27.96]: TLSv1.2 with cipher ECDHE-ECDSA-AES256-SHA384 (256/256 bits)
Mar  8 00:45:02 froxlor postfix/smtpd[67806]: A1B9E1C0096: client=a27-96.smtp-out.us-west-2.amazonses.com[54.240.27.96]
Mar  8 00:45:02 froxlor postfix/cleanup[67821]: A1B9E1C0096: message-id=&lt;01010170b951a112-269a702a-ede4-4556-849b-e61b7f433063-000000@us-west-2.amazonses.com&gt;
Mar  8 00:45:03 froxlor postfix/qmgr[67485]: A1B9E1C0096: from=&lt;user@theirdomain.tld&gt;, size=81292, nrcpt=1 (queue active)
Mar  8 00:45:04 froxlor postfix/pipe[67823]: A1B9E1C0096: to=&lt;user@mydomain.tld&gt;, relay=dovecot, delay=2.6, delays=2.1/0.01/0/0.47, dsn=2.0.0, status=sent (delivered via dovecot service)
</code></pre>

<p>I started with default TLS settings in /etc/postfix/main.cf and have since tried setting smtpd_tls_ciphers = export (and low, medium, high) and enabling/disabling SSLv2, SSLv3, TLSv1, TLSv1.1, TLSv1.2, TLSv1.3 along with tls_ssl_options = NO_COMPRESSION and a bunch of other enabling and disabling of options that I unfortunately don't remember exactly at this point.</p>

<p>I'm using LetsEncrypt certificates generated by tools installed and configured with Froxlor and haven't messed with any of those settings.</p>

<p>Testing is a bit of a pain because they only retry every half hour or so, but (I think) I've narrowed it down to either SSLv3 or TLSv1 since if I disable those the error changes from ""no shared cipher"" to ""unsupported protocol"". I also haven't found a way to see what protocol they're trying to use even with the TLS logging set to 10.</p>

<p>The most recent thing I've done is build libssl1.1 from source with enable-weak-ssl-ciphers thinking maybe by default the ciphers being used by these senders weren't in the default package.</p>

<p>I'm probably overlooking something but none of my searching has borne fruit and I'm running out of hair to pull out.  If anyone can give me a clue I'd greatly appreciate it. If I've left out any information I'll be happy to provide it. Thank you all in advance for taking the time to read this!</p>
","<debian><postfix><openssl><lets-encrypt><starttls>","2020-03-08 09:09:59"
"867099","Looking for how to add a new member","<p>Where can I now find the admin members page in the appengine.google.com console?</p>

<p>It looks like Google changed to a new interface in the appengine console. I'm trying to add a new member but I can't find where to do it. </p>
","<google-cloud-platform><google-app-engine>","2017-08-06 08:53:18"
"1040306","Why can't Loopback Address be used as source address inside packet?","<p>We know that we can ping Localhost which means it can be used as Destination Address.</p>
<p>Why can't Loopback Address be used as Source Address inside packet?</p>
","<networking><ip>","2020-10-28 01:51:25"
"792381","How to test to observe Latency Differences when running Cassandra on HDD vs SSD","<p>We had a 3 node cluster running on HDDs. After migrating to SSDs, we ran a load test but haven't seen any latency difference.</p>

<p>So we are thinking about running a stress test. Is there any threshold or parameters we should set or any general recommendation to find the utmost difference?</p>
","<cassandra><stress-testing><datastax-enterprise>","2016-07-28 03:33:05"
"933565","Ubuntu Static Network Issue","<pre><code>$ ip address flush eth0 #i flushed out the old ip
$ route add default gw 192.xx.xx.xx eth0 #adding g.way
</code></pre>

<p>i get this error</p>

<pre><code>SIOCADDRT: Network is unreachable.
</code></pre>

<p>I use Ubuntu on VM box over wifii. Network settings - Briged adapter.</p>
","<linux><ubuntu>","2018-10-02 04:24:42"
"933697","FreeBSD shutdown privileges","<p>I need to create a user and deny them permission to turn off the machine.
Does someone know how to change the permissions of a normal user to not allow me to turn off?</p>
","<permissions><freebsd><users><user-permissions>","2018-10-02 21:54:53"
"792587","Repair windows server 2008 on dell poweredge t710","<p>I really have a big problem. My server could not boot.  I have an IMAGE BACKUP but when I try to recover it from external storage it asks me for a driver.  </p>

<p>I don't know which driver to use.  I have a dell poweredge t710.</p>
","<windows-server-2008><raid><sata>","2016-07-28 22:58:19"
"792666","why does a google compute VPS instance challenge for a ssh passphrase?","<p>For this <strong>already generated</strong> <em>dummy</em> key I'm looking to add it to the instance below.  What passphrase is being challenged for and by whom?</p>

<pre><code>thufir@mordor:~$ 
thufir@mordor:~$ ssh-keygen -t rsa -b 768 -f ~/.ssh/gcloud -C thufir
Generating public/private rsa key pair.
Created directory '/home/thufir/.ssh'.
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /home/thufir/.ssh/gcloud.
Your public key has been saved in /home/thufir/.ssh/gcloud.pub.
The key fingerprint is:
SHA256:3paYqopihdHBHzW8F9zT+yMaB/JHn6LiMAtvJbW3YPI thufir
The key's randomart image is:
+----[RSA 768]----+
|  .   oo. . .    |
|   o . ..o o .   |
|  . o . . . . .  |
| . . . . + . o   |
|  o     S + o o .|
| . .   + O = = = |
|  .  . oX * B o .|
|.o    oo+E +     |
|+ ....oo...      |
+----[SHA256]-----+
thufir@mordor:~$ 
thufir@mordor:~$ nano .ssh/gcloud.pub 
thufir@mordor:~$ 
thufir@mordor:~$ chmod 400 .ssh/gcloud.pub 
thufir@mordor:~$ 
thufir@mordor:~$ cat .ssh/gcloud.pub 
thufir:ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAAAYQDLehQya4f/rKixCbh2EIB0XvTocDlZnXds+xogHmYW6naXf/9+Uo27rnrnG9P69mQPCR85s9ZC+SfiuEUJ3CVn3XFrCr6wB3TBcjobRYgI62aQgCwyYx0Osc1yJfmxqRU= thufir  google-ssh {""userName"":""thufir"",""expireOn"":""2018-12-04T20:12:00+0000""}
thufir@mordor:~$ 
thufir@mordor:~$ ssh-keygen -f .ssh/gcloud -y
Enter passphrase: 
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAAAYQDLehQya4f/rKixCbh2EIB0XvTocDlZnXds+xogHmYW6naXf/9+Uo27rnrnG9P69mQPCR85s9ZC+SfiuEUJ3CVn3XFrCr6wB3TBcjobRYgI62aQgCwyYx0Osc1yJfmxqRU=
thufir@mordor:~$ 
thufir@mordor:~$ gcloud compute instances list
Listed 0 items.
thufir@mordor:~$ 
thufir@mordor:~$ gcloud compute instances create &lt;instance_name&gt;
Created [https://www.googleapis.com/compute/v1/projects/&lt;project&gt;/zones/&lt;zone&gt;/instances/&lt;instance_name&gt;].
NAME  ZONE        MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP     STATUS
&lt;instance_name&gt;   &lt;zone&gt;  n1-standard-1               &lt;internal_ip&gt;   &lt;external_ip&gt;  RUNNING
thufir@mordor:~$ 
thufir@mordor:~$ 
thufir@mordor:~$ gcloud compute instances add-metadata &lt;instance_name&gt; --metadata-from-file ssh-keys=.ssh/gcloud.pub
Updated [https://www.googleapis.com/compute/v1/projects/&lt;project&gt;/zones/&lt;zone&gt;/instances/&lt;instance_name&gt;].
thufir@mordor:~$ 
thufir@mordor:~$ nano .ssh/config 
thufir@mordor:~$ 
thufir@mordor:~$ cat .ssh/config 
Host gcloud
  HostName &lt;external_ip&gt;
  IdentityFile /home/thufir/.ssh/gcloud.pub
  User thufir
thufir@mordor:~$ 
thufir@mordor:~$ ll .ssh
total 80
drwx------  2 thufir thufir  4096 Jul 29 02:42 ./
drwx------ 71 thufir thufir 36864 Jul 29 02:33 ../
-rw-rw-r--  1 thufir thufir    96 Jul 29 02:42 config
-rw-------  1 thufir thufir   791 Jul 29 02:33 gcloud
-r--------  1 thufir thufir   255 Jul 29 02:34 gcloud.pub
thufir@mordor:~$ 
thufir@mordor:~$ ssh gcloud
The authenticity of host '&lt;external_ip&gt; (&lt;external_ip&gt;)' can't be established.
ECDSA key fingerprint is SHA256:ek2g0GTj6Dxtb4vvwTNXRJgRKXnaLjoWZRv1lZ4pJ30.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '&lt;external_ip&gt;' (ECDSA) to the list of known hosts.
Enter passphrase for key '/home/thufir/.ssh/gcloud.pub': 
Enter passphrase for key '/home/thufir/.ssh/gcloud.pub': 
Enter passphrase for key '/home/thufir/.ssh/gcloud.pub': 
Permission denied (publickey).
thufir@mordor:~$ 
thufir@mordor:~$ ssh-keygen -f .ssh/gcloud -y
Enter passphrase: 
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAAAYQDLehQya4f/rKixCbh2EIB0XvTocDlZnXds+xogHmYW6naXf/9+Uo27rnrnG9P69mQPCR85s9ZC+SfiuEUJ3CVn3XFrCr6wB3TBcjobRYgI62aQgCwyYx0Osc1yJfmxqRU=
thufir@mordor:~$ 
thufir@mordor:~$ 
</code></pre>

<p>So far as I can verify, the passphrase I'm entering by hand is correct.  However, when connecting to the Google cloud computer VPS instance, I'm being challenged for a passphrase.</p>
","<ssh><vps><cloud><google-compute-engine><gcloud>","2016-07-29 10:12:49"
"792694","Centos 7 - Sendmail google relay issue","<p>So since my isp's smtp servers didn't work i decided to use google's servers, with port 587 since it isn't blocked. So i followed this tutorial: <a href=""https://linuxconfig.org/configuring-gmail-as-sendmail-email-relay"" rel=""nofollow noreferrer"">https://linuxconfig.org/configuring-gmail-as-sendmail-email-relay</a>.
And now when i try to send i get this:</p>

<pre><code>sudo /usr/sbin/sendmail -v -q

Running /var/spool/mqueue/u6TBoGKj027936 (sequence 1 of 3)
&lt;jcoatanea@gmail.com&gt;... Connecting to gmail-smtp-msa.l.google.com. port 587 via relay...
220 smtp.gmail.com ESMTP n128sm2730927lfb.45 - gsmtp
&gt;&gt;&gt; EHLO dynavio.com
250-smtp.gmail.com at your service, [87.92.41.2]
250-SIZE 35882577
250-8BITMIME
250-STARTTLS
250-ENHANCEDSTATUSCODES
250-PIPELINING
250-CHUNKING
250 SMTPUTF8
&gt;&gt;&gt; STARTTLS
220 2.0.0 Ready to start TLS
&gt;&gt;&gt; EHLO dynavio.com
250-smtp.gmail.com at your service, [87.92.41.2]
250-SIZE 35882577
250-8BITMIME
250-AUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH
250-ENHANCEDSTATUSCODES
250-PIPELINING
250-CHUNKING
250 SMTPUTF8
&gt;&gt;&gt; QUIT
221 2.0.0 closing connection n128sm2730927lfb.45 - gsmtp
&lt;jcoatanea@gmail.com&gt;... Deferred: Temporary AUTH failure
</code></pre>

<p>The mail still isn't getting sent, why does setting up sendmail have to be this hard?</p>

<p><strong>Edit:</strong></p>

<pre><code>Ok, i made some changes and now am getting this:
jcoatanea@live.com... Connecting to gmail-smtp-msa.l.google.com. port 587 via relay...
220 smtp.gmail.com ESMTP g69sm5267646lji.44 - gsmtp
&gt;&gt;&gt; EHLO dynavio.com
250-smtp.gmail.com at your service, [87.92.41.2]
250-SIZE 35882577
250-8BITMIME
250-STARTTLS
250-ENHANCEDSTATUSCODES
250-PIPELINING
250-CHUNKING
250 SMTPUTF8
&gt;&gt;&gt; STARTTLS
220 2.0.0 Ready to start TLS
&gt;&gt;&gt; EHLO dynavio.com
250-smtp.gmail.com at your service, [87.92.41.2]
250-SIZE 35882577
250-8BITMIME
250-AUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH
250-ENHANCEDSTATUSCODES
250-PIPELINING
250-CHUNKING
250 SMTPUTF8
&gt;&gt;&gt; AUTH LOGIN
334 VXNlcm5hbWU6
&gt;&gt;&gt; ZHluYXZpby5jb29wQGdtYWlsLmNvbQ==
334 UGFzc3dvcmQ6
&gt;&gt;&gt; MTYwOWVlY2M=
534-5.7.14 &lt;https://accounts.google.com/signin/continue?sarp=1&amp;scc=1&amp;plt=AKgnsbuy
534-5.7.14 o2OFUpUafLjLSp0oOwV56X1PMcDOdU_NSJ-tIxcNg4eG05FIMmDPNp0lw6yyn8rt-2JPop
534-5.7.14 wrj3m4AoX4_RYtua4gpoJpBLGbEhdv8cYND0LB7zZFpXFEsZkSAl6Admnz7ocBgzgFmcFJ
534-5.7.14 AWgkWUrUC_TdY1rGh3-E54jrVPt8hkcrW-Kg5cgW10gI8GNgYGfSJcf_p7iS3jeFftT4Ij
534-5.7.14 ETjjxhxKR9Vfoh8TBvSX1W0o40YdA&gt; Please log in via your web browser and
534-5.7.14 then try again.
534-5.7.14  Learn more at
534 5.7.14  https://support.google.com/mail/answer/78754 g69sm5267646lji.44 - gsmtp
&gt;&gt;&gt; AUTH PLAIN cm9vdABkeW5hdmlvLmNvb3BAZ21haWwuY29tADE2MDllZWNj
534-5.7.14 &lt;https://accounts.google.com/signin/continue?sarp=1&amp;scc=1&amp;plt=AKgnsbv8
534-5.7.14 nN0UFd92qcwx3IrSgUlbzsnnHjlfZHm50wV-weu4xEYV8zyIJ4aaVHerzMekBa1o8ApYHO
534-5.7.14 EzwE6hUwDlC1boJKwkoinzqeWSgMLZiY6ki6fMeizSL_OnMcoUJdlva7dg7JUWpnyGiYco
534-5.7.14 l82HioM6NWMIgIdUE8wB35JNnNMOhlmkb0wuG2pffQfPqY6oZW_jR5VWkXl2c5UeOBYK_x
534-5.7.14 rAQNHWWuunvC1fvMbyDL67jtIY0wM&gt; Please log in via your web browser and
534-5.7.14 then try again.
534-5.7.14  Learn more at
534 5.7.14  https://support.google.com/mail/answer/78754 g69sm5267646lji.44 - gsmtp
&gt;&gt;&gt; MAIL From:&lt;duser@dynavio.com&gt; SIZE=67
530-5.5.1 Authentication Required. Learn more at
530 5.5.1  https://support.google.com/mail/answer/14257 g69sm5267646lji.44 - gsmtp
duser... Connecting to local...
duser... Sent
Closing connection to gmail-smtp-msa.l.google.com.
&gt;&gt;&gt; QUIT
221 2.0.0 closing connection g69sm5267646lji.44 - gsmtp
</code></pre>

<p><strong>Edit#2:</strong>
So now it's working, but the from header doesn't work for some reason.</p>
","<centos><sendmail><centos7>","2016-07-29 11:55:46"
"792744","RAID 1 on top of spanned RAID","<p>Currently I have three drives with 500GB, 1.5TB and 2TB and would like to know if it's possible with mdadm to build a RAID 1 on top of the 2TB drive and the other two pooled together.</p>

<p>Would this work if I first created a 2TB RAID with <code>-level=linear</code> and then a RAID 1 on top of it and the 2TB drive? I've not tried this before and would like to know if this setup would likely work or if there will be problems with performance or something else.</p>

<p>Edit: Can the people downvoting please leave a comment explaining why? The answer to this question does not seem obvious to me, so why all the downvotes?</p>
","<linux><raid><mdadm>","2016-07-29 14:53:38"
"867483","Can't start MySql service AWS/Ubuntu","<p>I can't start mysql service. It run smooth for months, then with no change it just stopped.</p>

<p><code>sudo service mysql start</code> gives me this <code>Job for mysql.service failed because the control process exited with error code. See ""systemctl status mysql.service"" and ""journalctl -xe"" for details.
</code>
<code>systemctl status mysql.service</code> outputs this:</p>

<pre><code>● mysql.service - MySQL Community Server
   Loaded: loaded (/lib/systemd/system/mysql.service; enabled; vendor preset: enabled)
   Active: activating (start-post) (Result: exit-code) since Tue 2017-08-08 11:39:48 UTC; 23s ago
  Process: 29500 ExecStart=/usr/sbin/mysqld (code=exited, status=1/FAILURE)
  Process: 29492 ExecStartPre=/usr/share/mysql/mysql-systemd-start pre (code=exited, status=0/SUCCESS)
 Main PID: 29500 (code=exited, status=1/FAILURE);         : 29501 (mysql-systemd-s)
    Tasks: 2
   Memory: 1.6M
      CPU: 232ms
   CGroup: /system.slice/mysql.service
           └─control
             ├─29501 /bin/bash /usr/share/mysql/mysql-systemd-start post
             └─29554 sleep 1

Aug 08 11:39:48 ip-172-31-21-240 systemd[1]: mysql.service: Service hold-off time over, scheduling restart.
Aug 08 11:39:48 ip-172-31-21-240 systemd[1]: Stopped MySQL Community Server.
Aug 08 11:39:48 ip-172-31-21-240 systemd[1]: Starting MySQL Community Server...
Aug 08 11:39:48 ip-172-31-21-240 systemd[1]: mysql.service: Main process exited, code=exited, status=1/FAILURE
</code></pre>

<p><code>journalctl -xe</code> outputs as follows</p>

<pre><code>-- Subject: Unit mysql.service has begun start-up
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
--
-- Unit mysql.service has begun starting up.
Aug 08 11:31:14 ip-172-31-21-240 kernel: audit: type=1400 audit(1502191874.172:10246): apparmor=""DENIED"" operation=""open"" profile=""/usr/sbin/mysqld"" name=""/proc/28031/status"" pid=28031 comm=""mysqld
Aug 08 11:31:14 ip-172-31-21-240 kernel: audit: type=1400 audit(1502191874.172:10247): apparmor=""DENIED"" operation=""open"" profile=""/usr/sbin/mysqld"" name=""/sys/devices/system/node/"" pid=28031 comm=
Aug 08 11:31:14 ip-172-31-21-240 kernel: audit: type=1400 audit(1502191874.172:10248): apparmor=""DENIED"" operation=""open"" profile=""/usr/sbin/mysqld"" name=""/proc/28031/status"" pid=28031 comm=""mysqld
Aug 08 11:31:14 ip-172-31-21-240 audit[28031]: AVC apparmor=""DENIED"" operation=""open"" profile=""/usr/sbin/mysqld"" name=""/proc/28031/status"" pid=28031 comm=""mysqld"" requested_mask=""r"" denied_mask=""r""
Aug 08 11:31:14 ip-172-31-21-240 audit[28031]: AVC apparmor=""DENIED"" operation=""open"" profile=""/usr/sbin/mysqld"" name=""/sys/devices/system/node/"" pid=28031 comm=""mysqld"" requested_mask=""r"" denied_m
Aug 08 11:31:14 ip-172-31-21-240 audit[28031]: AVC apparmor=""DENIED"" operation=""open"" profile=""/usr/sbin/mysqld"" name=""/proc/28031/status"" pid=28031 comm=""mysqld"" requested_mask=""r"" denied_mask=""r""
Aug 08 11:31:14 ip-172-31-21-240 systemd[1]: mysql.service: Main process exited, code=exited, status=1/FAILURE
Aug 08 11:31:44 ip-172-31-21-240 systemd[1]: Failed to start MySQL Community Server.
</code></pre>

<p>Last entry of error.log</p>

<pre><code>2017-08-08T11:50:23.690152Z 0 [Warning] Changed limits: max_open_files: 1024 (requested 5000)
2017-08-08T11:50:23.690188Z 0 [Warning] Changed limits: table_open_cache: 431 (requested 2000)
2017-08-08T11:50:23.836308Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
2017-08-08T11:50:23.837266Z 0 [Note] /usr/sbin/mysqld (mysqld 5.7.19-0ubuntu0.16.04.1) starting as process 31307 ...
2017-08-08T11:50:23.840500Z 0 [Note] InnoDB: PUNCH HOLE support available
2017-08-08T11:50:23.840521Z 0 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
2017-08-08T11:50:23.840525Z 0 [Note] InnoDB: Uses event mutexes
2017-08-08T11:50:23.840529Z 0 [Note] InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier
2017-08-08T11:50:23.840532Z 0 [Note] InnoDB: Compressed tables use zlib 1.2.8
2017-08-08T11:50:23.840537Z 0 [Note] InnoDB: Using Linux native AIO
2017-08-08T11:50:23.840751Z 0 [Note] InnoDB: Number of pools: 1
2017-08-08T11:50:23.840845Z 0 [Note] InnoDB: Using CPU crc32 instructions
2017-08-08T11:50:23.842139Z 0 [Note] InnoDB: Initializing buffer pool, total size = 128M, instances = 1, chunk size = 128M
2017-08-08T11:50:23.842166Z 0 [ERROR] InnoDB: mmap(137428992 bytes) failed; errno 12
2017-08-08T11:50:23.842171Z 0 [ERROR] InnoDB: Cannot allocate memory for the buffer pool
2017-08-08T11:50:23.842176Z 0 [ERROR] InnoDB: Plugin initialization aborted with error Generic error
2017-08-08T11:50:23.842180Z 0 [ERROR] Plugin 'InnoDB' init function returned error.
2017-08-08T11:50:23.842183Z 0 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.
2017-08-08T11:50:23.842186Z 0 [ERROR] Failed to initialize plugins.
2017-08-08T11:50:23.842188Z 0 [ERROR] Aborting

2017-08-08T11:50:23.842191Z 0 [Note] Binlog end
2017-08-08T11:50:23.842219Z 0 [Note] Shutting down plugin 'MyISAM'
2017-08-08T11:50:23.842361Z 0 [Note] /usr/sbin/mysqld: Shutdown complete
</code></pre>
","<ubuntu><mysql><amazon-web-services>","2017-08-08 11:42:16"
"1040768","DHCP Server stops handing out leases","<p>I currently have a DC running Windows Sever 2008 R2 (yes I know) and for the past two weeks, when employees travel back to the office, they can't connect to the internet unless I do one of two things.</p>
<ol>
<li>Reboot the DC</li>
<li>Give them a static IP</li>
</ol>
<p>The DC also hosts the DHCP and DNS roles. So I've checked and checked for errors but can't find anything and nothing has changed on that DC in that time. Does anyone have any thoughts or recommendations to what I can do? Thank you.</p>
<p>Happy Halloween!</p>
","<windows-server-2008-r2><dhcp><connection><internet>","2020-10-31 15:29:41"
"867516","How does the host OS process frames/packets meant for VMs?","<p>I've been reading up on how VMs communicate on the internet using vNICs, vSwitches etc. Would like to confirm a few things</p>

<ol>
<li><p>For outbound traffic, the pNIC can be overridden send frames/packets with ip/MAC of the VM instead of host ip and hardcoded NIC MAC. Correct ? </p></li>
<li><p>If and when this traffic hits a switch, MAC table of switch will have multiple entries for a particular port( host+VM(s) ). Correct ? </p></li>
<li><p>For inbound traffic, switch directs all traffic from step 2 to that port. is the NIC only responsible for layer 2 processing ( ie does it only extract the IP packet from a frame and pass it to OS ?? ) .</p></li>
<li><p>A hardware hypervisor like esxi is programmed to look  at the destination ip of the packet and route it to the corresponding VM. Correct ? </p></li>
<li><p>If so, this means that the esxi/host os will receive all packets for both itself and its VMs. when are IP packets procesed to extract transport layer data ? For example, if I send a ssh command to a VM, when does the host process this packet and figure out it is not meant for itself but for the VM?</p></li>
</ol>
","<networking><ip><virtual-machines><nic>","2017-08-08 13:59:15"
"792855","Can't connect to virtual server that shows 99% CPU usage","<p>Our CPU usage showing 99% since last 24 hours due to this usage we can't able to connect the server through SSH. We are using this server for our Magento 2.0 webstore.</p>

<p>We have rebooted the server lots of time but after rebooting the server same CPU usage in few secs. So, we can't able to trace the server error or server error log as well. </p>

<p>Can anyone please suggest how we can fix this issue?</p>

<p><em>Server Information:</em> </p>

<p>OS: Ubuntu </p>

<p>Web Server: Apache</p>

<p>Zone: europe-west1-c</p>

<p>Machine Type: 1 vCPU, 3.75 GB</p>

<p>Thanks,</p>

<p>Rinjal Patel</p>
","<ubuntu><apache-2.4><cpu-usage><server-crashes><magento>","2016-07-30 08:09:30"
"867565","How I can change XML files are handled by Nginx (Debian)","<p>On my server, XML files are handled by Nginx. They should be handled by Apache so that .htaccess file is working</p>

<p>I have checked the cURL not working properly</p>

<p>When I run the command <code>curl -I https://www.example.com/sitemap.xml</code></p>

<pre><code>curl: (7) Failed to connect
Failed to connect on all port

nginx -t
nginx: [emerg] BIO_new_file(""/home/admin/conf/web/ssl.xvids.cc.pem"") failed (SSL: error:02001002:system library:fopen:No such file or directory:fopen('/home/admin/conf/web/ssl.xvids.cc.pem','r') error:2006D080:BIO routines:BIO_new_file:no such file)
</code></pre>

<p>nginx: configuration file /etc/nginx/nginx.conf test failed</p>

<p><strong>/var/log/syslog</strong></p>

<pre><code>Aug  9 06:25:25 srv named[30476]: client 104.42.128.152#51441 (hostsahibi2.dnsservis.com): query (cache) 'hostsahibi2.dnsservis.com/A/IN' denied
</code></pre>

<p><strong>ls -l /home/admin/conf/web/</strong></p>

<pre><code>total 144
-rw-r----- 1 root admin 11402 Aug  9 09:53 apache2.conf
-rw-r----- 1 root admin 12517 Aug  9 10:18 nginx.conf
-rw-r----- 1 root root  11268 Jul 24 15:02 nginx.conf.backup
-rw-r--r-- 1 root root    147 Aug  9 09:53 nginx.dev.xvids.cc.conf_letsencrypt
-rw-r--r-- 1 root root    147 Jul 20 20:05 nginx.xchum.com.conf_letsencrypt
-rw-r--r-- 1 root root    147 Jul 19 13:39 nginx.xvids.cc.conf_letsencrypt
-rw-r--r-- 1 root root    147 Jul 20 20:52 nginx.xvids.in.conf_letsencrypt
drwxr-xr-x 2 root root   4096 Jul 19 13:35 old_server
-rw-r--r-- 1 root root     68 Jul 18 20:56 pam.htpasswd
-rw-r----- 1 root admin  4591 Aug  9 10:43 sapache2.conf
-rw-r----- 1 root admin  4903 Aug  9 10:53 snginx.conf
-rw-r----- 1 root admin  5186 Aug  9 10:21 snginx.conf.save
-rw-r----- 1 root root   1647 Aug  9 09:53 ssl.dev.xvids.cc.ca
-rw-r----- 1 root root   2134 Aug  9 09:53 ssl.dev.xvids.cc.crt
-rw-r----- 1 root root   3243 Aug  9 09:53 ssl.dev.xvids.cc.key
-rw-r----- 1 root root   3782 Aug  9 09:53 ssl.dev.xvids.cc.pem
-rw-r----- 1 root root   1647 Jul 20 20:31 ssl.xchum.com.ca
-rw-r----- 1 root root   2147 Jul 20 20:31 ssl.xchum.com.crt
-rw-r----- 1 root root   3243 Jul 20 20:31 ssl.xchum.com.key
-rw-r----- 1 root root   3795 Jul 20 20:31 ssl.xchum.com.pem
-rw-r----- 1 root root   5625 Aug  9 10:43 ssl.xvids.cc.ca
-rw-r----- 1 root root   2223 Aug  9 10:43 ssl.xvids.cc.crt
-rw-r----- 1 root root   3243 Aug  9 10:43 ssl.xvids.cc.key
-rw-r----- 1 root root   7849 Aug  9 10:43 ssl.xvids.cc.pem
</code></pre>
","<linux><apache-2.2><nginx><bind><curl>","2017-08-08 16:54:17"
"1040864","How to setup own dns server","<p>Suppose i have 3 Applications.
example1.com
example2.com
example3.com
all domains are in different registrar lets say  example1.com is in Godaddy, example2.com is in Bigrock and example3.com is in CrazyDomains.
My all this application are on different servers they all have dedicated ips also suppose.
Suppose app1 current ip is 1.1.1.1  ,app2 current ip is 2.2.2.2 ,app3 current ip is 3.3.3.3 app4 current ip is 4.4.4.4.</p>
<p>Lets say if i change servers of all above and now all above have different ips like
app1 11.11.11.11 ,app2 22.22.22.22 etc...
i have to update new ips to different domain registrar again with A record.
but what if i have something like my own 1 server which ip is permanent and lets say its 7.7.7.7 so i'll add this ip to all domains.</p>
<p>now this my new server will redirect to different hosts.</p>
<p>just like we manage DNS in Cpanel but in my case i don't want to update DNS to mine server i'll just update A records only. DNS will be by default managed by domain registrar.
Benefit will be for me is if i change if of my any app i dont have to update ip in domain registrar's DNS setting instead i'll directly update to my own server.</p>
","<domain-name-system><reverse-proxy>","2020-11-01 16:49:42"
"867673","Azure Services no longer deactivating when reaching my Azure Spending Limit","<p>I have an MSDN account which gives $150 Azure credit per month. On my Azure billing account, I've also set a spending limit of $0.</p>

<p>Sometimes, I spin up a few machines for what I'm working on. And find that towards the end of the month my credit is exhausted and because I have a spending limit of $0, the services are suspended.</p>

<p>This month, however, I've noticed that I've exceeded my credit and the VMs I've got running haven't been suspended. I've checked and my spending limit is still $0, but I also have no credit. </p>

<p>Has something changed with Azure policy? How can ensure I don't get charged?</p>

<p>Edit: I've attached 2 screenshots from my billing portal showing the account does have a spending limit and no credit attached to it.</p>

<p><a href=""https://i.sstatic.net/MlIWH.png"" rel=""nofollow noreferrer"">Azure Billing Portal Screenshot</a></p>

<p><a href=""https://i.sstatic.net/WXAmr.png"" rel=""nofollow noreferrer"">Active Services on Azure showing cost > credit balance</a></p>
","<azure>","2017-08-09 09:25:54"
"1041055","WMI Remote Monitoring getting The RPC Server is unavailable. Windows Server 2019","<p>I'm having trouble connecting to a remote machine via WMI. Getting the error message</p>
<p><code>The RPC Server is unavailable</code></p>
<ol>
<li>I've ensured that the <code>Remote Procedure Call</code> service is turned on.</li>
<li>I've opened all necessary ports in the firewall.</li>
<li>I've turned off the firewall altogether to eliminate any possibility it's the firewall for testing. Both on Server &amp; Client</li>
<li>I've ran the following script via powershell remotely and get the error message.</li>
<li>If I run the same powershell script through another server on the same network it works! I get my drive info listed. But it won't work across the internet from my office?!? What am I doing wrong?</li>
</ol>
<p><code>Get-WmiObject -Namespace &quot;root\cimv2&quot; -Class Win32_LogicalDisk -ComputerName 192.168.1.1 -Credential MyDomain\Administrator</code> I did replace the IP to my public IP on this command.</p>
<p>I can ping the IP and <strong>do</strong> get replies.</p>
","<wmi><windows-server-2019>","2020-11-03 03:07:52"
"934236","How can I prevent BIND from sending ""Authoritative answers can be found from:""?","<p>Here is an example from performing an nslookup from a Unix server (IPs and domains all fake):</p>

<pre><code>$ nslookup
&gt; set type=srv
&gt; _kerberos._tcp.example.com
Server:         192.168.1.100
Address:        192.168.1.100#53

Non-authoritative answer:
_kerberos._tcp.example.com service = 0 100 88 dc01.example.com.
_kerberos._tcp.example.com service = 0 100 88 dc02.example.com.


Authoritative answers can be found from:
.       nameserver = h.root-servers.net.
.       nameserver = e.root-servers.net.
.       nameserver = a.root-servers.net.
.       nameserver = l.root-servers.net.
.       nameserver = d.root-servers.net.
.       nameserver = f.root-servers.net.
.       nameserver = k.root-servers.net.
.       nameserver = i.root-servers.net.
.       nameserver = b.root-servers.net.
.       nameserver = m.root-servers.net.
.       nameserver = c.root-servers.net.
.       nameserver = j.root-servers.net.
.       nameserver = g.root-servers.net.
dc00.example.com      internet address = 192.168.1.200
dc01.example.com      internet address = 192.168.1.201
</code></pre>

<p>Is there a way I can prevent BIND from sending that extra section and just return the Non-authoritative answers?</p>

<p>EDIT 1: I took out the part where I said I don't think Windows nslookup knows what to do with the results. It does know, but it just doesn't label it as such.</p>

<p>EDIT 2: When using a Windows DNS server, it doesn't include the root servers as part of the answer.</p>

<p>EDIT 3: We've only noticed this with SRV records.</p>
","<domain-name-system><bind><srv-record>","2018-10-05 23:04:36"
"793071","MySQL Server won't start - [ERROR] COLLATION","<p>Ok, first, sorry for this question, I usually search a lot before post a question on any forum or Q&amp;A.
But now I'm on panic, I need help (also to keep my job).  :)</p>

<p>This morning the server I manage who host many websites of clients of the company where I work had a problem.
The error when I try to visit one of the website is simply:</p>

<blockquote>
  <p>Error establishing a database connection</p>
</blockquote>

<p>First I've tried to login into plesk, but the error was the same (failed to connect to the DB)
So I tried to restart the entire VPS, but nothing to do the error still there, so I connected through ssh and I tried with:</p>

<pre><code>sudo /etc/init.d/mysql start
</code></pre>

<p>I check for the mysql logs and it was empty except for this:</p>

<blockquote>
  <p>160801 08:07:11 mysqld_safe Starting mysqld daemon with databases from
  /var/lib/mysql 160801  8:07:11 [Warning] Using unique option prefix
  key_buffer instead of key_buffer_size is deprecated and will be
  removed in a future release. Please use the full name instead. 160801 
  8:07:11 [Note] /usr/sbin/mysqld (mysqld 5.5.46-0ubuntu0.14.04.2)
  starting as process 5448 ... 160801  8:07:11 [ERROR] COLLATION
  'utf8mb4_unicode_ci' is not valid for CHARACTER SET 'utf8' 160801 
  8:07:11 [ERROR] Aborting</p>
  
  <p>160801  8:07:11 [Note] /usr/sbin/mysqld: Shutdown complete</p>
  
  <p>160801 08:07:11 mysqld_safe mysqld from pid file
  /var/run/mysqld/mysqld.pid ended</p>
</blockquote>

<p>I really don't know what happened but I think the problem could be that error in the log. </p>
","<mysql><vps><ubuntu-14.04>","2016-08-01 06:37:43"
"1041101","Widows 10 Active directory user profile desktop not responding after restart and login in outside domain network","<p>Windows Server 2012 R2 active directory profile has been configured in windows 10 pro-64-bit laptop and was working fine early days and recently noticed when we restarted the laptop in outside domain network and after logged in desktop freeze and not responding unable to access task manager or any other software and folders. and the local administrator profile was working fine.</p>
<p>When I connect the laptop to the domain network and tried to log in to the same AD profile and started working and there is no lagging.</p>
<p>what could be the issues? please advise.</p>
","<windows><active-directory><windows-server-2012-r2><windows-10>","2020-11-03 12:31:14"
"793235","Redirect specific intercepted path in Squid","<p>I'd like to redirect users to /app/acme when their request is like this</p>

<pre><code>http://our-internal-app or
http://our-internal-app/
</code></pre>

<p>If squid can intercept these requests, what would be the configuration for it?</p>
","<redirect><squid>","2016-08-01 20:34:39"
"793265","How to check my network for IP spoofing availability?","<p>I want to test if my network or even my ISP blocks spoofed IP packets. I am running a Debian Linux OS.</p>
<p>I found this nice tool, but it's not working on my system ..
<a href=""https://www.caida.org/projects/spoofer/"" rel=""nofollow noreferrer"">https://www.caida.org/projects/spoofer/</a></p>
<p>Does somebody know another way how to check this?</p>
","<ip><linux-networking><testing><spoofing>","2016-08-02 03:10:25"
"934440","How to use SonarSwift(Sonarqube) plugin in Jenkins in Ubuntu 16.04?","<p>I have installed <code>lizard</code> but I'm getting this error when I build my job in Jenkins:</p>

<pre><code>11:00:22.363 ERROR - Lizard Report not found /var/lib/jenkins/workspace/xxxxxxxx/sonar-reports/lizard-report.xml
java.io.FileNotFoundException: /var/lib/jenkins/workspace/xxxxxxx/sonar-reports/lizard-report.xml (No such file or directory)
    at java.io.FileInputStream.open0(Native Method)
    at java.io.FileInputStream.open(FileInputStream.java:195)
    at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:138)
    at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:93)
    at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)
    at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)
    at com.sun.org.apache.xerces.internal.impl.XMLEntityManager.setupCurrentEntity(XMLEntityManager.java:623)
    at com.sun.org.apache.xerces.internal.impl.XMLVersionDetector.determineDocVersion(XMLVersionDetector.java:148)
    at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:806)
    at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:771)
    at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141)
    at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:243)
    at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:339)
    at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:205)
    at com.backelite.sonarqube.swift.complexity.LizardReportParser.parseReport(LizardReportParser.java:61)
    at com.backelite.sonarqube.swift.complexity.LizardSensor.parseReportsIn(LizardSensor.java:51)
    at com.backelite.sonarqube.swift.complexity.LizardSensor.execute(LizardSensor.java:73)
    at org.sonar.scanner.sensor.SensorWrapper.analyse(SensorWrapper.java:53)
    at org.sonar.scanner.phases.SensorsExecutor.executeSensor(SensorsExecutor.java:88)
    at org.sonar.scanner.phases.SensorsExecutor.execute(SensorsExecutor.java:82)
    at org.sonar.scanner.phases.SensorsExecutor.execute(SensorsExecutor.java:68)
    at org.sonar.scanner.phases.AbstractPhaseExecutor.execute(AbstractPhaseExecutor.java:88)
    at org.sonar.scanner.scan.ModuleScanContainer.doAfterStart(ModuleScanContainer.java:177)
    at org.sonar.core.platform.ComponentContainer.startComponents(ComponentContainer.java:135)
    at org.sonar.core.platform.ComponentContainer.execute(ComponentContainer.java:121)
    at org.sonar.scanner.scan.ProjectScanContainer.scan(ProjectScanContainer.java:291)
    at org.sonar.scanner.scan.ProjectScanContainer.scanRecursively(ProjectScanContainer.java:286)
    at org.sonar.scanner.scan.ProjectScanContainer.doAfterStart(ProjectScanContainer.java:264)
    at org.sonar.core.platform.ComponentContainer.startComponents(ComponentContainer.java:135)
    at org.sonar.core.platform.ComponentContainer.execute(ComponentContainer.java:121)
    at org.sonar.scanner.task.ScanTask.execute(ScanTask.java:48)
    at org.sonar.scanner.task.TaskContainer.doAfterStart(TaskContainer.java:84)
    at org.sonar.core.platform.ComponentContainer.startComponents(ComponentContainer.java:135)
    at org.sonar.core.platform.ComponentContainer.execute(ComponentContainer.java:121)
    at org.sonar.scanner.bootstrap.GlobalContainer.executeTask(GlobalContainer.java:121)
    at org.sonar.batch.bootstrapper.Batch.doExecuteTask(Batch.java:116)
    at org.sonar.batch.bootstrapper.Batch.execute(Batch.java:71)
    at org.sonar.runner.batch.IsolatedLauncher.execute(IsolatedLauncher.java:48)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.sonar.runner.impl.BatchLauncher$1.delegateExecution(BatchLauncher.java:87)
    at org.sonar.runner.impl.BatchLauncher$1.run(BatchLauncher.java:75)
    at java.security.AccessController.doPrivileged(Native Method)
    at org.sonar.runner.impl.BatchLauncher.doExecute(BatchLauncher.java:69)
    at org.sonar.runner.impl.BatchLauncher.execute(BatchLauncher.java:50)
    at org.sonar.runner.api.EmbeddedRunner.doExecute(EmbeddedRunner.java:102)
    at org.sonar.runner.api.Runner.execute(Runner.java:100)
    at org.sonar.runner.Main.executeTask(Main.java:70)
    at org.sonar.runner.Main.execute(Main.java:59)
    at org.sonar.runner.Main.main(Main.java:53)
</code></pre>

<p>I have installed the Sonarswift plugin in Sonarqube manually. It seems that issue is with <code>Lizard</code>. Jenkins and Sonarqube server are on different machine. Let me know if I'm missing something.</p>
","<jenkins><sonarqube>","2018-10-08 05:41:28"
"868228","Colocation server: where to install ESXi?","<p>Sorry if this is a newbie question. I am colocating a 1U server and want to install ESXi on it. Given that a colo provider likely won't allow a USB key sticking out of the server, <strong>what is the preferred way to install ESXi in production cases?</strong></p>
","<virtualization><vmware-esxi><vmware-vsphere><vmware-vcenter><colocation>","2017-08-11 17:58:47"
"793399","MSI on Windows 10 won't register OCX (even when run as admin)","<p>Got an MSI built using a Visual Studio Installer project.  The resulting MSI was developed on Windows 7 some years back, and works fine there. </p>

<p>There is a merge module (MSM) that is supposed to dump an OCX to the 32-bit system folder (usually <code>C:\Windows\SysWOW64</code>) and register it in <code>HKEY_CLASSES_ROOT</code>. </p>

<p>When I run the installer on Windows 10 (as a user in the local Administrators group), it adds the OCX file to the system folder but the registration fails silently (the COM components are just <em>not</em> in the registry when I check later.)</p>

<p>I tried running the MSI from an Administrator command prompt using <code>msiexec /i</code> but it didn't make the registration happen. </p>

<p>The only way to make it work was, from the Admin command prompt, manually register the OCX file using <code>regsvr32</code>. </p>

<p>Any insight here?  Do I need to alter the MSI somehow?  Or run it differently? </p>
","<windows><windows-registry><windows-10><msi><windows-installer>","2016-08-02 15:14:32"
"1041433","Bind9 transfer to new slave failing","<p>I built a master bind server using Ubuntu 20.04 and webmin. The master is working normally except for transfers to the slave. The slave is also new Ubuntu 20.04 with webmin. My serials are incrementing on any changes and it is sending a notify to the slave but no transfer occurs. I'm sure I've done something stupid but i'm at a loss and need some help please.</p>
<p>from the slave syslog</p>
<pre><code>client @0x7eff48044910 192.40.120.9#33471/key 1: received notify for zone 'telpage.net': TSIG '1': not authoritative
</code></pre>
<p>Her is my Master named.conf</p>
<pre><code>// If you are just adding zones, please do that in /etc/bind/named.conf.local

include &quot;/etc/bind/named.conf.options&quot;;
include &quot;/etc/bind/named.conf.local&quot;;
include &quot;/etc/bind/named.conf.default-zones&quot;;
key 1 {
    algorithm hmac-md5;
    secret &quot;xxxxxxxxxxxxxxxxxxxxxxxx&quot;;
    };
server 192.40.120.10 {
    keys {
        1;
        };
    transfer-format one-answer;
    };
controls {
    inet 'master ip' port 953 allow { &quot;master ip&quot;; &quot;slave ip&quot;; } keys { rndc-key; 1; };
    };
key rndc-key {
    algorithm hmac-sha256;
    secret &quot;xxxxxx&quot;;
    };
</code></pre>
<p>named.conf.options</p>
<pre><code>//========================================================================
    dnssec-validation auto;

    listen-on-v6 { any; };
    forwarders {
        &quot;forwarder 1&quot;;
        &quot;forwarder 2&quot;;
        };
    forward first;
    allow-recursion {
        &quot;client ip&quot;;
        &quot;client ip&quot;;
        };
    allow-query {
        any;
        };
    dnssec-enable yes;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    auth-nxdomain yes;
};
</code></pre>
<p>named.conf.local</p>
<pre><code>//
// Do any local configuration here
//

// Consider adding the 1918 zones here, if they are not used in your
// organization
//include &quot;/etc/bind/zones.rfc1918&quot;;

zone &quot;telpage.net&quot; {
    type master;
    file &quot;/var/lib/bind/telpage.net.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    allow-transfer {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;amandajoneslaw.com&quot; {
    type master;
    file &quot;/var/lib/bind/amandajoneslaw.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;brunswickco.com&quot; {
    type master;
    file &quot;/var/lib/bind/brunswickco.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;buckwaterplantation.com&quot; {
    type master;
    file &quot;/var/lib/bind/buckwaterplantation.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;chapmanlumber.com&quot; {
    type master;
    file &quot;/var/lib/bind/chapmanlumber.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;consciencestream.com&quot; {
    type master;
    file &quot;/var/lib/bind/consciencestream.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;dickensconstruction.com&quot; {
    type master;
    file &quot;/var/lib/bind/dickensconstruction.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;elliottsadler.com&quot; {
    type master;
    file &quot;/var/lib/bind/elliottsadler.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;emporiaciviccenter.org&quot; {
    type master;
    file &quot;/var/lib/bind/emporiaciviccenter.org.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;emporiamedical.com&quot; {
    type master;
    file &quot;/var/lib/bind/emporiamedical.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;emporianews.com&quot; {
    type master;
    file &quot;/var/lib/bind/emporianews.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;flyemv.com&quot; {
    type master;
    file &quot;/var/lib/bind/flyemv.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;flyemv.org&quot; {
    type master;
    file &quot;/var/lib/bind/flyemv.org.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;franklinbraid.com&quot; {
    type master;
    file &quot;/var/lib/bind/franklinbraid.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;goodearthpeanuts.com&quot; {
    type master;
    file &quot;/var/lib/bind/goodearthpeanuts.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;greensvillecountyva.gov&quot; {
    type master;
    file &quot;/var/lib/bind/greensvillecountyva.gov.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;jarrattfire.org&quot; {
    type master;
    file &quot;/var/lib/bind/jarrattfire.org.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;jlwalston.com&quot; {
    type master;
    file &quot;/var/lib/bind/jlwalston.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;jrallpc.com&quot; {
    type master;
    file &quot;/var/lib/bind/jrallpc.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;lakegastonassoc.com&quot; {
    type master;
    file &quot;/var/lib/bind/lakegastonassoc.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;lastday.net&quot; {
    type master;
    file &quot;/var/lib/bind/lastday.net.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;lgaston.org&quot; {
    type master;
    file &quot;/var/lib/bind/lgaston.org.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;midatlanticinfosec.com&quot; {
    type master;
    file &quot;/var/lib/bind/midatlanticinfosec.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;midatlantictower.com&quot; {
    type master;
    file &quot;/var/lib/bind/midatlantictower.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;modsbyus.com&quot; {
    type master;
    file &quot;/var/lib/bind/modsbyus.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;motorolaradio.com&quot; {
    type master;
    file &quot;/var/lib/bind/motorolaradio.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;naynaysartbox.com&quot; {
    type master;
    file &quot;/var/lib/bind/naynaysartbox.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;omnitowers.com&quot; {
    type master;
    file &quot;/var/lib/bind/omnitowers.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;osg-armor.com&quot; {
    type master;
    file &quot;/var/lib/bind/osg-armor.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;sadlerbrosoil.com&quot; {
    type master;
    file &quot;/var/lib/bind/sadlerbrosoil.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;sadlerfanclub.com&quot; {
    type master;
    file &quot;/var/lib/bind/sadlerfanclub.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;southsideccjb.com&quot; {
    type master;
    file &quot;/var/lib/bind/southsideccjb.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;telpage.com&quot; {
    type master;
    file &quot;/var/lib/bind/telpage.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;thevirginiapeanutfestival.com&quot; {
    type master;
    file &quot;/var/lib/bind/thevirginiapeanutfestival.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;virginiacarolina.com&quot; {
    type master;
    file &quot;/var/lib/bind/virginiacarolina.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;whitman-properties.com&quot; {
    type master;
    file &quot;/var/lib/bind/whitman-properties.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;wrobinsonlaw.com&quot; {
    type master;
    file &quot;/var/lib/bind/wrobinsonlaw.com.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
zone &quot;ymcaofeg.org&quot; {
    type master;
    file &quot;/var/lib/bind/ymcaofeg.org.hosts&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    notify yes;
    };
</code></pre>
<p>named.conf.default-zones</p>
<pre><code>// prime the server with knowledge of the root servers
zone &quot;.&quot; {
    type hint;
    file &quot;/usr/share/dns/root.hints&quot;;
};

// be authoritative for the localhost forward and reverse zones, and for
// broadcast zones as per RFC 1912

zone &quot;localhost&quot; {
    type master;
    file &quot;/etc/bind/db.local&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    allow-transfer {
        &quot;slave ip&quot;;
        };
};

zone &quot;127.in-addr.arpa&quot; {
    type master;
    file &quot;/etc/bind/db.127&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    allow-transfer {
        &quot;slave ip&quot;;
        };
};

zone &quot;0.in-addr.arpa&quot; {
    type master;
    file &quot;/etc/bind/db.0&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    allow-transfer {
        &quot;slave ip&quot;;
        };
};

zone &quot;255.in-addr.arpa&quot; {
    type master;
    file &quot;/etc/bind/db.255&quot;;
    also-notify {
        &quot;slave ip&quot;;
        };
    allow-transfer {
        &quot;slave ip&quot;;
        };
};
</code></pre>
<p>On the slave:</p>
<p>named.conf</p>
<pre><code>// This is the primary configuration file for the BIND DNS server named.
//
// Please read /usr/share/doc/bind9/README.Debian.gz for information on the 
// structure of BIND configuration files in Debian, *BEFORE* you customize 
// this configuration file.
//
// If you are just adding zones, please do that in /etc/bind/named.conf.local

include &quot;/etc/bind/named.conf.options&quot;;
include &quot;/etc/bind/named.conf.local&quot;;
include &quot;/etc/bind/named.conf.default-zones&quot;;
key rndc-key {
    algorithm hmac-sha256;
    secret &quot;xxxxxxx&quot;;
    };
controls {
    inet &quot;slave ip&quot; port 953 allow { &quot;slave ip&quot;; &quot;master ip&quot;; } keys { rndc-key; 1; };
    };
server &quot;master ip&quot; {
    keys {
        1;
        };
    };
key 1 {
    algorithm hmac-md5;
    secret &quot;xxxxxxxx&quot;;
    };
logging {
    channel bind_log {
        null;
        };
    };
</code></pre>
<p>named.conf.options</p>
<pre><code>options {
    directory &quot;/var/cache/bind&quot;;

    // If there is a firewall between you and nameservers you want
    // to talk to, you may need to fix the firewall to allow multiple
    // ports to talk.  See http://www.kb.cert.org/vuls/id/800113

    // If your ISP provided one or more IP addresses for stable 
    // nameservers, you probably want to use them as forwarders.  
    // Uncomment the following block, and insert the addresses replacing 
    // the all-0's placeholder.

    // forwarders {
    //  0.0.0.0;
    // };

    //========================================================================
    // If BIND logs error messages about the root key being expired,
    // you will need to update your keys.  See https://www.isc.org/bind-keys
    //========================================================================
    dnssec-validation auto;

    listen-on-v6 { any; };
    transfer-source &quot;master ip&quot;;
    allow-query {
        any;
        };
    forwarders {
        &quot;forwarder 1&quot;;
        &quot;forwarder 2&quot;;
        };
    allow-transfer {
        &quot;master ip&quot;;
        };
    transfer-format one-answer;
};
</code></pre>
<p>named.conf.local</p>
<pre><code>//
// Do any local configuration here
//

// Consider adding the 1918 zones here, if they are not used in your
// organization
//include &quot;/etc/bind/zones.rfc1918&quot;;

zone &quot;telpage.net&quot; {
    type slave;
    file &quot;/var/lib/bind/telpage.net.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;amandajoneslaw.com&quot; {
    type slave;
    file &quot;/var/lib/bind/amandajoneslaw.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;brunswickco.com&quot; {
    type slave;
    file &quot;/var/lib/bind/brunswickco.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;buckwaterplantation.com&quot; {
    type slave;
    file &quot;/var/lib/bind/buckwaterplantation.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;chapmanlumber.com&quot; {
    type slave;
    file &quot;/var/lib/bind/chapmanlumber.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;consciencestream.com&quot; {
    type slave;
    file &quot;/var/lib/bind/consciencestream.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;dickensconstruction.com&quot; {
    type slave;
    file &quot;/var/lib/bind/dickensconstruction.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;elliottsadler.com&quot; {
    type slave;
    file &quot;/var/lib/bind/elliottsadler.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;emporiaciviccenter.org&quot; {
    type slave;
    file &quot;/var/lib/bind/emporiaciviccenter.org.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;emporiamedical.com&quot; {
    type slave;
    file &quot;/var/lib/bind/emporiamedical.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;emporianews.com&quot; {
    type slave;
    file &quot;/var/lib/bind/emporianews.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;flyemv.com&quot; {
    type slave;
    file &quot;/var/lib/bind/flyemv.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;flyemv.org&quot; {
    type slave;
    file &quot;/var/lib/bind/flyemv.org.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;franklinbraid.com&quot; {
    type slave;
    file &quot;/var/lib/bind/franklinbraid.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;goodearthpeanuts.com&quot; {
    type slave;
    file &quot;/var/lib/bind/goodearthpeanuts.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;greensvillecountyva.gov&quot; {
    type slave;
    file &quot;/var/lib/bind/greensvillecountyva.gov.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;jarrattfire.org&quot; {
    type slave;
    file &quot;/var/lib/bind/jarrattfire.org.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;jlwalston.com&quot; {
    type slave;
    file &quot;/var/lib/bind/jlwalston.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;jrallpc.com&quot; {
    type slave;
    file &quot;/var/lib/bind/jrallpc.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;lakegastonassoc.com&quot; {
    type slave;
    file &quot;/var/lib/bind/lakegastonassoc.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;lastday.net&quot; {
    type slave;
    file &quot;/var/lib/bind/lastday.net.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;lgaston.org&quot; {
    type slave;
    file &quot;/var/lib/bind/lgaston.org.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;midatlanticinfosec.com&quot; {
    type slave;
    file &quot;/var/lib/bind/midatlanticinfosec.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;midatlantictower.com&quot; {
    type slave;
    file &quot;/var/lib/bind/midatlantictower.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;modsbyus.com&quot; {
    type slave;
    file &quot;/var/lib/bind/modsbyus.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;motorolaradio.com&quot; {
    type slave;
    file &quot;/var/lib/bind/motorolaradio.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;naynaysartbox.com&quot; {
    type slave;
    file &quot;/var/lib/bind/naynaysartbox.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;omnitowers.com&quot; {
    type slave;
    file &quot;/var/lib/bind/omnitowers.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;osg-armor.com&quot; {
    type slave;
    file &quot;/var/lib/bind/osg-armor.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;sadlerbrosoil.com&quot; {
    type slave;
    file &quot;/var/lib/bind/sadlerbrosoil.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;sadlerfanclub.com&quot; {
    type slave;
    file &quot;/var/lib/bind/sadlerfanclub.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;southsideccjb.com&quot; {
    type slave;
    file &quot;/var/lib/bind/southsideccjb.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;telpage.com&quot; {
    type slave;
    file &quot;/var/lib/bind/telpage.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;thevirginiapeanutfestival.com&quot; {
    type slave;
    file &quot;/var/lib/bind/thevirginiapeanutfestival.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;virginiacarolina.com&quot; {
    type slave;
    file &quot;/var/lib/bind/virginiacarolina.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;whitman-properties.com&quot; {
    type slave;
    file &quot;/var/lib/bind/whitman-properties.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;wrobinsonlaw.com&quot; {
    type slave;
    file &quot;/var/lib/bind/wrobinsonlaw.com.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
zone &quot;ymcaofeg.org&quot; {
    type slave;
    file &quot;/var/lib/bind/ymcaofeg.org.hosts&quot;;
    masters {
        &lt;master ip&gt;;
    };
    };
</code></pre>
<p>named.conf.default-zones</p>
<pre><code>// prime the server with knowledge of the root servers
zone &quot;.&quot; {
    type hint;
    file &quot;/usr/share/dns/root.hints&quot;;
};

// be authoritative for the localhost forward and reverse zones, and for
// broadcast zones as per RFC 1912


zone &quot;127.in-addr.arpa&quot; {
    type slave;
    file &quot;/etc/bind/db.127&quot;;
};

zone &quot;0.in-addr.arpa&quot; {
    type master;
    file &quot;/etc/bind/db.0&quot;;
};

zone &quot;255.in-addr.arpa&quot; {
    type master;
    file &quot;/etc/bind/db.255&quot;;
};
</code></pre>
","<bind>","2020-11-05 16:55:16"
"793677","Bash script in cron job - email when a file has been modified","<p>I looked at <a href=""https://stackoverflow.com/questions/14164383/check-a-files-modified-date-and-email-if-it-has-changed"">this topic</a> to write a bash script that checks if a file has been modified every 60 min and sends me an email using ssmtp if that is the case. Here is its content (<code>monitoring.sh</code>):</p>

<pre><code>#!/bin/bash

[[ -z `find /home/myuser/sites/mysite/logs/nginx/error.log -mmin -60` ]]

if [ $? -eq 0 ]
then
    echo -e ""nothing has changed""
else
    echo -e ""Something went wrong!"" | ssmtp -vvv myemail@gmail.com
fi
</code></pre>

<p>I then added this script to an hourly cronjob:</p>

<pre><code> 01 * * * * /home/myuser/sites/mysite/logs/nginx/monitoring.sh
</code></pre>

<p>This does not work at all. Looking at my emails (the sender account that ssmtp uses), the script runs every hour and echoes <code>nothing has changed</code> even if the <code>error.log</code> file has been modified.</p>

<p>Maybe using <code>inotifywait</code> would be more appropriate? Any ideas?</p>

<p>Thanks in advance.</p>

<p>EDIT: If I run this script manually after modifying the <code>error.log</code> file, it works and I receive the email. </p>
","<bash><shell-scripting>","2016-08-03 16:55:00"
"934619","Only 2T for using after I fdisk the disk","<p>I have a 4T disk:</p>

<pre><code>Disk /dev/sdc: 4000.8 GB, 4000787030016 bytes
255 heads, 63 sectors/track, 486401 cylinders, total 7814037168 sectors
Units = section of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 4096 bytes
I/O size (minimum/optimal): 4096 bytes / 4096 bytes
Disk identifier: 0x00000000
</code></pre>

<p>After I use fdisk for partitioned to 2 parts. but I only get 2T for using.</p>

<p>I only get 2T for using:</p>

<pre><code>/dev/sdc1       939G   72M  891G    1% /media/summer/sdc1
/dev/sdc2       1.1T   71M 1023G    1% /media/summer/sdc2
</code></pre>

<p>I am using CentOS 7.2 system. </p>

<hr>

<p><strong>EDIT-01</strong></p>

<p>When I take partition:</p>

<pre><code>Disk /dev/sdc: 4000.8 GB, 4000787030016 bytes
255 heads, 63 sectors/track, 486401 cylinders, total 7814037168 sectors
Units = sector of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 4096 bytes
I/O size (minimum/optimal): 4096 bytes / 4096 bytes
Disk identifier: 0x00000000

   device launch      start          end     blocks  Id  system

command(type m for help)： n
Partition type:
   p   primary (0 primary, 0 extended, 4 free)
   e   extended
Select (default p): p
part number (1-4，defualt 1)： 
will use default 1
start sector (2048-4294967295，default 2048)： 
will use default 2048
Last sector, +sector or +size{K,M,G} (2048-4294967294，default 4294967294)：
</code></pre>

<p>You see the <code>total sectors</code> are <code>7814037168</code>, but when I take partition, the choice is <code>2048-4294967294</code>. </p>
","<ubuntu><hard-drive>","2018-10-09 08:35:09"
"1041438","Ways to find out if an old server is still in use?","<p>I received a list of 200 servers on the internal domain and I am tasked with determining if they could be end of life. Unfortunately, I've only worked here a month, so I do not know what most of these are even for. I've sent out the list to many teams and only got a couple of responses. They want this list by EOD tomorrow. 90% are VMware VMs. Most are on Windows Server 2008. A lot of SQL servers and IIS servers based on their names.</p>
<p>Ways to find out if a server is still in use?</p>
<ul>
<li>I don't think I can rely on the last login, because they are SQL and IIS servers. Most of them haven't been logged into in a long time.</li>
<li>I was thinking of looking up the latest events in the Event Log, but that would take forever to do manually, and I wouldn't be able to meet my deadline.</li>
</ul>
","<windows><windows-server-2008><powershell><maintenance>","2020-11-05 17:23:01"
"793684","PHP - Multicore for hashing","<p>I have a server with 2 intel xeons on it, this gives me quite a lot of cores. And on that server i have an app, that uses php to hash passwords in blowfish. But the issue is php is not using all the cores it could, so the hashing becomes slow as hell. How could i make multicore usage possible for php hashing?</p>
","<php><centos7><multi-core><multi-threading>","2016-08-03 17:34:05"
"1007280","AD query - get bitlocker active computers","<p>I have query in Active Directory which should return all computers with bitlocker not active based on this script:</p>

<p><a href=""https://community.spiceworks.com/topic/1083065-bitlocker-status-on-all-computers"" rel=""nofollow noreferrer"">https://community.spiceworks.com/topic/1083065-bitlocker-status-on-all-computers</a></p>

<p>Script works fine so the values are there.</p>

<p>But my query is not returning anything.
It looks like this:</p>

<pre><code>(&amp;(&amp;(objectCategory=computer)(objectClass=msFVE-RecoveryInformation)(!(msFVE-RecoveryPassword=*))))
</code></pre>

<p>I want to return computers which dont have active bitlocker.</p>

<p>Can someone point me to right direction where could be mistake, still learning with ldap queries.</p>

<p>//EDIT</p>

<p>So i found out msFVE-RecoveryInformation is object by it self, when i do:</p>

<pre><code>(&amp;(&amp;(objectClass=msFVE-RecoveryInformation)(msFVE-RecoveryPassword=*)))
</code></pre>

<p>It filters objects with recovery keys but name of computers is coded in atribute ""distinguishedName like this:</p>

<pre><code>CN=2020-02-10T16:32:51\+01:00{4C74584A-BF6C-4AFA-9E46-582DDFC207A6},CN=NAMEOFNTB,OU=computers,OU=blabla,DC=test,DC=local
</code></pre>

<p>Somehow i can filter only computers with this attribute?</p>
","<active-directory><ldap><query>","2020-03-17 16:28:15"
"868351","Pointing MX at mail server where domain isn't hosted?","<p>I am working on a system where we have Moodle software on one domain (I'll call it moodlehost.com) which is making a LOGIN connection to a SMTP server on another domain (I'll call it smtphost.com). I was brought in to work on the Moodle side of things, but I've ended up going some DNS work too, though I'm not an expert on DNS or server admin. </p>

<p>I have the Moodle software on moodlehost.com logging into the SMTP server on smtphost.com, with SPF on smtphost.com configured to approve email from the moodlehost.com IP. It's working nicely and passing all spam-filter checks with flying colors. Note: The MX for smtphost.com is a mail server which I'll call underlyingsmtp.com. </p>

<p><em>** Edit per request from Tim below: To establish a connection to the SMTP server from Moodle, I type in smtphost.com with port 587. However, the MX record for smtphost.com points to a different mail server, underlyingsmtp.com. (Now that you ask about it, I recall that the MX is only for INCOMING mail, right? So the MX record for smtphost.com would be irrelevant to outgoing mail sent through smtphost.com? In that case, my question is the same: Why would I point the MX record for moodlehost.com at the same address as the MX record on smtphost.com?)</em></p>

<p>The system admin who runs the system I'm working on wants me to set MX records on moodlehost.com so they point to underlyingsmtp.com, even though moodlehost.com doesn't exist on that server and any incoming mail @moodlehost.com bounces as a result. His explanation is ""Mixing the sends is (mostly) where a backlisting can possibly occur. All or nothing is best with forwarded mail, that's all."" </p>

<p>Again, I'm not a server guy, but this doesn't make any sense to me. As soon as moodlehost.com hands off the outgoing mail to the SMTP server on smtphost.com it's out of the picture, and the outgoing emails are all passing SPF, DKIM, and DMARC. I don't see how pointing the moodlehost.com MX at underlyingsmtp.com helps, and it actually seems like it might look suspicious if we're pointing MX at a mail server where we don't have an account so any incoming mail bounces. </p>

<p>Could anyone help me understand if pointing the MX at underlyingsmtp.com makes sense, and if so, why? Thanks very much for your time. </p>

<p><em>** Edit: We don't anticipate having any incoming mail to moodlehost.com, but if we do it certainly can't and won't be handled by underlyingsmtp.com. The server that HAS been handling incoming mail is moodlehost.com. It's the only one that would do so. If we point the MX records elsewhere, mail will simply bounce.</em></p>

<p><strong>My question is this: If I have a script on site A making an SMTP connection to server B in order to send mail, is there any reason why site A's MX records would also need to point to server B?</strong> </p>
","<domain-name-system><email><email-server><mx-record>","2017-08-12 21:19:25"
"868361","Unable to ssh localhost without password despite proper perms, key in authorized_keys","<p>I have a key <code>~/.ssh/id_rsa</code> and I added the pub key to my authorized keys:</p>

<pre><code>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys
</code></pre>

<p>I also changed my permissions to 600:</p>

<pre><code>sudo chmod 600 ~/.ssh/authorized_keys
</code></pre>

<p>I checked and <code>/etc/ssh/sshd_config</code> is set for public key auth and has password auth as no.</p>

<pre><code>PubkeyAuthentication yes
PasswordAuthentication no
</code></pre>

<p>I tried adding:</p>

<pre><code>ssh-keyscan -t rsa localhost &gt; /etc/ssh/ssh_known_hosts
</code></pre>

<p>I copied my id:</p>

<pre><code>ssh-copy-id myusername@127.0.0.1
</code></pre>

<p>I restarted the service:</p>

<pre><code>sudo service sshd restart
</code></pre>

<p>But despite all that, it still asks for a password! How can i make it not ask for one?</p>

<p><em>More Info:</em></p>

<p>When I do <code>ssh localhost</code> it asks for a password, but when I do <code>myusername@localhost</code> it works! Why is this?</p>
","<linux><ssh><localhost><opensuse><hadoop>","2017-08-12 23:08:17"
"1041534","Can somebody help me on how to zip these individual log files within while loop","<p>I am able to break o/p of a big logfile (filename.log) into individual log files of 1 min each (filename.log.140108) using while loop, but I want these files to be saved as zip files due to capacity issue on VM. Can anyone pls help ??</p>
<pre><code>#!/bin/bash
log_file=/home/tmp/filename.log
tmp_log_file=/home/tmp/filename.log.$$
while true;
do
    sleep 60
    cp $log_file $tmp_log_file
    &gt;$log_file
    mv $tmp_log_file $log_file.$(date +%M%D%Y%H%M) // CODE
</code></pre>
<p>----------- current output ---------------</p>
<pre><code>-rw-r--r-- 1 root     root    16789643 Nov  6 14:05 filename.log // Master log file
-rw-r--r-- 1 root     root     2277376 Nov  6 14:01 filename.log.140108 // 1 min log made from master log
-rw-r--r-- 1 root     root     3862528 Nov  6 14:02 filename.log.140208
-rw-r--r-- 1 root     root     5558272 Nov  6 14:03 filename.log.140308
-rw-r--r-- 1 root     root     7147520 Nov  6 14:04 filename.log.140408
</code></pre>
<p>------------ expected output -----------------</p>
<pre><code>rw-r--r-- 1 root     root     2277376 Nov  6 14:01 filename.log.140108.gz
-rw-r--r-- 1 root     root     3862528 Nov  6 14:02 filename.log.140208.gz
-rw-r--r-- 1 root     root     5558272 Nov  6 14:03 filename.log.140308.gz
-rw-r--r-- 1 root     root     7147520 Nov  6 14:04 filename.log.140408.gz  
</code></pre>
","<linux><bash><unix><log-files><shell-scripting>","2020-11-06 13:59:32"
"868390","rsyslog with elasticsearch and forwarding custom logs","<p>I have configured rsyslogd Log Server With Elasticsearch and Kibana. I am able to forward the <em>/var/log/messages</em> from the client Linux servers (CentOS 7, RHEL 6) to the central Linux Log server (CentOS 7). </p>

<p>However, I have a custom log file (eg:<em>/var/log/usercommands</em>) in all client servers, which needs to be forwarded to the central Log server. This log file records all commands executed by all users in the respective client server via command line. I have added below rule to forward the logs from clients to server. But, with the below rule, my custom log file (<em>/var/log/usercommands</em>) is not getting forwarded to central Log server.</p>

<pre><code> *.* @@remotehost:514
</code></pre>

<p>Any help on what configuration need to be added to rsyslogd.conf of the clients to forward this custom log file as well to central Log server?</p>
","<log-files><rsyslog><elasticsearch><logserver>","2017-08-13 07:03:50"
"794800","Need to make sure about chkdsk issue","<p><img src=""https://i.sstatic.net/FRCMN.png"" alt=""enter image description here""></p>

<p>As you can see, i ran chkdsk /r on hard drive which got some bad sectors and this is screenshot for last moment before i stopped chkdsk. i canceled chkdsk after ran some minutes because i heard run chkdsk /r on hard drive with bad sectors can lose data.</p>

<p>So according to this chkdsk prompt window screenshot, was that possible about that data was removed/fixed or anything changed by chkdsk? i am asking about that chkdsk could do something to data even no errors are found on prompt window.</p>

<p>Actually i MS support engineer gave same this answer.</p>

<p><strong>""From the above description I understand that you were able to cancel the chkdsk program within 3%. usually chkdsk command examine all the files and then start the fixes. However in this case you wouldn’t have lost any data.""</strong></p>

<p>So, i thought i need verify that answer i got, so i had additional question.</p>

<p>My question was...</p>

<p><strong>""If chkdsk finds error/corrupted/any kind of problem while running, is it strictly + immediately shows messages on prompt window? or possible to miss to show messages on window about what chkdsk did to data?""</strong></p>

<p>And i got this answer from another place.</p>

<p><strong>""When chkdsk finds a bad sector, it momentarily flashes a message like ""recovering sector xxxx"" while in stage 4. Did you see a message like this? If not, then your data has not been altered.""</strong></p>

<p>So on... i think i should make conclusion now.</p>

<p>For that, i just found these messages about stage 4 on chkdsk from somebody's log.</p>

<p><strong>Stage 4: Looking for bad clusters in user file data ...
Windows replaced bad clusters in file 2813
of name $PATH1.MKV.
Windows replaced bad clusters in file 2863
of name $PATH2.MKV.
Windows replaced bad clusters in file 2881
of name $PATH3.MKV.
Windows replaced bad clusters in file 2891
of name $PATH4.MKV.</strong></p>

<p>or</p>

<p><strong>Stage 4: Looking for bad clusters in user file data ...
Read failure with status 0xc0000185 at offset 0xb2b4000 for 0x10000 bytes.
A disk read error occurredc0000185
The disk does not have enough space to replace bad clusters
detected in file 73702 of name \PROGRA~1\WI7DB9~1\MIE81F~1.0_X\MRT100~1.DLL.</strong></p>

<p>I didn't see any kind of reports ↑ while chkdsk is running (as my screenshot exactly showing)
I wonder these messages are messages that he mean <strong>""recovering sector xxxx""</strong> or he was meaning another kind of messages?</p>

<p>Any messages like these are not written on my chkdsk prompt window but i do not remember some message was 'momentarily flashes' on chkdsk prompt window while chkdsk was running.</p>

<hr>

<p>So plus according to my screenshot, MS support engineer's answer <strong>""However in this case you wouldn’t have lost any data.""</strong> is 100% right?</p>

<p>In other word, <strong>Are data clearly untouched and sectors are not remarked by chkdsk in my case(in screenshot)?</strong></p>
","<chkdsk>","2016-08-04 09:33:43"
"934755","why linux assigns ephemeral ports out of its range","<p>I have a server running mysql, ssh and other services. I want to see the range for ephemeral ports available for the OS running:</p>

<pre><code>cat /proc/sys/net/ipv4/ip_local_port_range
</code></pre>

<p>which returns 32768  -  61000.
When I run </p>

<pre><code>sudo netstat -anp
</code></pre>

<p>I get in the foreign addresses, many ips and also ports outside the range stated before.
Example:</p>

<pre><code>Proto Recv-Q Send-Q Local Address               Foreign Address             State       PID/Program name
tcp        0      0 10.251.1.62:3306            207.81.181.89:63192         ESTABLISHED 1483/mysqld
</code></pre>

<p>Which looks to me that my server is assigning port 63192 to the remote host, which is not it's allocated range.
 How is this possible? am I not looking the ephemeral ports in the right place? am I not understanding properly the output of netstat? Am I not understanding how ephemeral ports work at all?
Thanks.</p>
","<networking><linux-networking><port><tcp>","2018-10-09 23:50:13"
"868437","Opposite of smtpd_tls_ask_ccert for SMTP?","<p>What is the way to offer my cert when sending outgoing mail?</p>

<p>All outgoing mail says ""No Client Certificate offered"".</p>

<p>EDIT:
Self signing a new certificate didn't help either.</p>

<p>This what I have in the TLS section of the main.cf file:</p>

<pre><code># TLS parameters
smtp_tls_security_level = may
smtpd_tls_security_level = may
smtp_tls_mandatory_protocols = TLSv1
smtpd_tls_received_header = yes
smtpd_tls_auth_only = yes
smtp_tls_note_starttls_offer = yes
smtp_tls_ciphers = export
smtpd_tls_cert_file=/etc/postfix/public.pem
smtpd_tls_key_file=/etc/postfix/private.pem
smtpd_tls_CApath = /etc/postfix/
smtpd_tls_CAfile = /etc/postfix/comodoca.pem
smtpd_use_tls=yes
smtp_tls_cert_file=/etc/postfix/client.pem
smtp_tls_key_file = /etc/postfix/clientprivate.pem
smtp_tls_CApath = /etc/postfix/
smtp_tls_CAfile = /etc/postfix/clientca.pem
smtp_use_tls=yes
smtpd_tls_session_cache_database = btree:${data_directory}/smtpd_scache
smtp_tls_session_cache_database = btree:${data_directory}/smtp_scache
smtpd_tls_exclude_ciphers = RC4,MD5, aNULL
smtp_tls_exclude_ciphers = RC4, MD5, aNULL
smtpd_tls_ask_ccert = yes
</code></pre>
","<postfix>","2017-08-13 16:53:16"
"934784","Change data from one logical drive to another","<p>I have a Fujitsu rx200S8 that has 2x500GB drives in a raid 0 configuration. I added 2x1TB new disks and created another raid 0.</p>

<p>How can I migrate the data from the old disks to the new ones without losing anything? The OS is 2012, and I have 10 VM on Hyper-V on it. My objective is to remove the old disks, i need them for another server. </p>
","<windows><raid>","2018-10-10 07:26:09"
"795084","Windows 10 1607 keeps connecting to internet in the background","<p>I just installed windows 10 version 1607 and since yesterday i am seeing hidden connections to the internet.</p>

<p>I have wireshark installed and i see a constant connection to ips such as 23.14.84.171, 23.14.84.160, 216.58.192.110, 208.65.155.48, 13.107.4.50,
23.14.84.161,23.14.84.177, 23.14.84.168, 8.253.0.30, 8.253.0.62, 8.253.0.78. </p>

<p>Most of them identify as akamai technologies, but they also resolve to 
I am using a local WSUS server to deliver windows updates and i know it's working because i have deployed third party apps and i saw them being installed.</p>

<p>I uninstalled all metro apps through powershell, both provisioned apps and all apps from my user, including the windows store.</p>

<p>I disabled the windows store and metro app updates from gpedit. I also disabled some windows update scheduled taks. I have disabled windows 10's function of using peer to peer to download updates and even set the new working hours to avoid reboots. With all these the machine keeps downloading something.</p>

<p>I have uploaded a capture of sysinternal's tcpview to show all connections to akamai over at <a href=""https://i.sstatic.net/msebH.jpg"" rel=""nofollow noreferrer"">https://i.sstatic.net/msebH.jpg</a></p>

<p>The question is, is there a way for me to download whatever windows needs at wsus or am i stuck in waiting until it finishes? It has been 2 days downloading stuff (our internet speed is just 2MB and 200 users are behing the same connection)</p>

<p>Update: i got tired and created a firewall rule to block all those ips, but would like to know if there is a legit method to stop windows from doing that.</p>
","<wsus><windows-update>","2016-08-05 14:25:08"
"868461","Caching of jpg files fails in Nginx","<p>I'm using a minimal Ubuntu server 16.04 with Nginx 1.10.3 (to which I moved from Apache 2 days ago). On this environment, I run a minimal Wordpress site (5 plugins, no customization).</p>

<pre><code>location ~* \.(jpg|jpeg|png|gif|ico|css|js)$ {
    expires 365d;
}

location ~* \.(pdf)$ {
    expires 30d;
}
</code></pre>

<p>I pasted that code inside the server block in the <code>default.conf</code> file of Nginx (to use it with other sites if I'd need), and restarted:</p>

<pre><code>systemctl restart nginx.service
</code></pre>

<p>Yet when running GPI test I still get “Leverage browser caching” for many jpg files, even though “jpg” is already included in the directive.</p>

<p>All documentation I found, so far, deals exactly with what I already tried. Any ideas? Maybe most doc is outdated? Maybe that’s a bug in the current release of Nginx?</p>
","<nginx><cache>","2017-08-13 21:04:18"
"795116","After setting up fail2ban w/Permanently Ban Repeat Offenders to fail2ban - I'm not be getting any emails of bans w/detailed info anymore","<p>After installing fail2ban 0.9.3 on my uBuntu server 16.04 and then following the steps located from this link - <a href=""https://wireflare.com/blog/permanently-ban-repeat-offenders-with-fail2ban/"" rel=""nofollow noreferrer"">Perma Ban Repeat Offenders</a> I did service fail2ban reload and got the emails showing the services stopped and started.  but,  I seem to not be getting emails showing what ip address has been banned and the detailed information that I use to get. I use to get these emails like all the time.</p>

<p>Here's a link to view my jail.conf and jail.local files</p>

<p><a href=""http://drops.articulate.com/18XUm"" rel=""nofollow noreferrer"">Link to jail local and conf files</a></p>
","<email><fail2ban><ubuntu-16.04>","2016-08-05 16:59:06"
"868520","Disable all gratuitious ARP, TCP & HTML queries Windows 7","<p>We have a machine controller running Microchip firmware worked 100% with LINUX computers for 14 years via LAN (eth0).
Now have requirement to run using Windows 7.
Unfortunately Windows sends out a whole heap of gratuitous requests on a regular basis. Eventually these crash the microcontroller.
I've been through the loop of disabling LLMNR, IGMP, SSDP, Routing, IPV6, ARP &amp; NBT-NS. Helped but not solved.
If I could stop the svchost requests it would help since the ARP requests don't seem to be an issue.</p>

<p>EDIT: svchost disabled :o)) ""Disable Active Internet Probing (NCSI) in Windows""</p>

<pre><code>{Frame       Time Date                               Time Offset          Process Name            Source                       Destination              Protocal     Description
1   10:59:48 14/08/2017 12.3275999                          NetmonFilter    NetmonFilter:Updated Capture Filter: None   
2   10:59:48 14/08/2017 12.3275999                          NetworkInfoEx   NetworkInfoEx:Network info for , Network Adapter Count = 1  
3   10:59:48 14/08/2017 12.3275999          192.168.1.20    192.168.1.6 ARP ARP:Request, 192.168.1.20 asks for 192.168.1.6  
4   10:59:48 14/08/2017 12.3282422          192.168.1.6 192.168.1.20    ARP ARP:Response, 192.168.1.6 at 00-04-A3-00-A9-F9  
5   10:59:49 14/08/2017 12.4985887          0.0.0.0     192.168.1.20     ARP    ARP:Request, 0.0.0.0 asks for 192.168.1.20  
6   10:59:49 14/08/2017 12.4986781          192.168.1.20    224.0.0.22      IGMP    IGMP:IGMPv3 Membership Report   {IPv4:1}
7   10:59:49 14/08/2017 12.4992127          192.168.1.6 0.0.0.0     ARP ARP:Response, 192.168.1.6 at 00-04-A3-00-A9-F9  
8   10:59:49 14/08/2017 12.9986822          192.168.1.20    224.0.0.22      IGMP    IGMP:IGMPv3 Membership Report   {IPv4:1}
9   10:59:50 14/08/2017 13.4986212          0.0.0.0     192.168.1.20    ARP ARP:Request, 0.0.0.0 asks for 192.168.1.20  
10  10:59:50 14/08/2017 13.4992550          192.168.1.6 0.0.0.0     ARP ARP:Response, 192.168.1.6 at 00-04-A3-00-A9-F9  
11  10:59:51 14/08/2017 14.4986969          0.0.0.0     192.168.1.20    ARP ARP:Request, 0.0.0.0 asks for 192.168.1.20  
12  10:59:51 14/08/2017 14.4994162          92.168.1.6      0.0.0.0     ARP ARP:Response, 192.168.1.6 at 00-04-A3-00-A9-F9  
13  10:59:52 14/08/2017 15.5147308          192.168.1.20    192.168.1.6 ARP ARP:Request, 192.168.1.20 asks for 192.168.1.6  
14  10:59:52 14/08/2017 15.5153513          192.168.1.6 192.168.1.20    ARP ARP:Response, 192.168.1.6 at 00-04-A3-00-A9-F9  
15  10:59:52 14/08/2017 15.5396148          192.168.1.20    192.168.1.6 ARP ARP:Request, 192.168.1.20 asks for 192.168.1.6  
16  10:59:52 14/08/2017 15.5402584          192.168.1.6 192.168.1.20    ARP ARP:Response, 192.168.1.6 at 00-04-A3-00-A9-F9  
17  10:59:52 14/08/2017 15.5528560          192.168.1.20    192.168.1.6 ARP ARP:Request, 192.168.1.20 asks for 192.168.1.6  
18  10:59:52 14/08/2017 15.5535065          192.168.1.6 192.168.1.20    ARP ARP:Response, 192.168.1.6 at 00-04-A3-00-A9-F9  
19  10:59:56 14/08/2017 19.5214914  svchost.exe 192.168.1.20    88.221.254.123  TCP TCP:Flags=......S., SrcPort=54230, DstPort=HTTP(80), PayloadLen=0, Seq=4050891366, Ack=0, Win=8192 ( Negotiating scale factor 0x8 ) = 8192  {TCP:3, IPv4:2}
20  10:59:56 14/08/2017 19.5223390  svchost.exe 88.221.254.123  192.168.1.20    TCP TCP:Flags=...A..S., SrcPort=HTTP(80), DstPort=54230, PayloadLen=0, Seq=0, Ack=4050891367, Win=4096 ( Scale factor not supported ) = 4096    {TCP:3, IPv4:2}
21  10:59:56 14/08/2017 19.5223857  svchost.exe 192.168.1.20    88.221.254.123  TCP TCP:Flags=...A...., SrcPort=54230, DstPort=HTTP(80), PayloadLen=0, Seq=4050891367, Ack=1, Win=17520 (scale factor 0x0) = 17520  {TCP:3, IPv4:2}
22  10:59:56 14/08/2017 19.5226940  svchost.exe 192.168.1.20    88.221.254.123  HTTP    HTTP:Request, GET /ncsi.txt     {HTTP:4, TCP:3, IPv4:2}
23  10:59:56 14/08/2017 19.8278302  svchost.exe 192.168.1.20    88.221.254.123  TCP TCP:[ReTransmit #22]Flags=...AP..., SrcPort=54230, DstPort=HTTP(80), PayloadLen=97, Seq=4050891367 - 4050891464, Ack=1, Win=17520 (scale factor 0x0) = 17520    {TCP:3, IPv4:2}
24  10:59:56 14/08/2017 20.4281386  svchost.exe 192.168.1.20    88.221.254.123  TCP TCP:[ReTransmit #22]Flags=...AP..., SrcPort=54230, DstPort=HTTP(80), PayloadLen=97, Seq=4050891367 - 4050891464, Ack=1, Win=17520 (scale factor 0x0) = 17520    {TCP:3, IPv4:2}
25  10:59:58 14/08/2017 21.6281447  svchost.exe 192.168.1.20    88.221.254.123  TCP TCP:[ReTransmit #22]Flags=...AP..., SrcPort=54230, DstPort=HTTP(80), PayloadLen=97, Seq=4050891367 - 4050891464, Ack=1, Win=17520 (scale factor 0x0) = 17520    {TCP:3, IPv4:2}
26  10:59:59 14/08/2017 22.8281828  svchost.exe 192.168.1.20    88.221.254.123  TCP TCP:[ReTransmit #22]Flags=...AP..., SrcPort=54230, DstPort=HTTP(80), PayloadLen=97, Seq=4050891367 - 4050891464, Ack=1, Win=17520 (scale factor 0x0) = 17520    {TCP:3, IPv4:2}
27  11:00:00 14/08/2017 24.0282154  svchost.exe 192.168.1.20    88.221.254.123  TCP TCP:[ReTransmit #22]Flags=...AP..., SrcPort=54230, DstPort=HTTP(80), PayloadLen=97, Seq=4050891367 - 4050891464, Ack=1, Win=17520 (scale factor 0x0) = 17520    {TCP:3, IPv4:2}
28  11:00:01 14/08/2017 24.9982180          192.168.1.20    192.168.1.6 ARP ARP:Request, 192.168.1.20 asks for 192.168.1.6  
29  11:00:01 14/08/2017 24.9988881          192.168.1.6 192.168.1.20    ARP ARP:Response, 192.168.1.6 at 00-04-A3-00-A9-F9  
30  11:00:02 14/08/2017 26.4282788  svchost.exe 192.168.1.20    88.221.254.123  TCP TCP:[ReTransmit #22]Flags=...AP..., SrcPort=54230, DstPort=HTTP(80), PayloadLen=97, Seq=4050891367 - 4050891464, Ack=1, Win=17520 (scale factor 0x0) = 17520    {TCP:3, IPv4:2}
31  11:00:07 14/08/2017 31.2258517  svchost.exe 192.168.1.20    88.221.254.123  TCP TCP:[ReTransmit #22]Flags=...AP..., SrcPort=54230, DstPort=HTTP(80), PayloadLen=97, Seq=4050891367 - 4050891464, Ack=1, Win=17520 (scale factor 0x0) = 17520    {TCP:3, IPv4:2}
32  11:00:17 14/08/2017 40.8293165  svchost.exe 192.168.1.20    88.221.254.123  TCP TCP:Flags=...A.R.., SrcPort=54230, DstPort=HTTP(80), PayloadLen=0, Seq=4050891464, Ack=1, Win=0 (scale factor 0x0) = 0  {TCP:3, IPv4:2}
</code></pre>
","<arp><igmp>","2017-08-14 10:26:24"
"935079","Redirect DNS to specific path","<p>A given client needs to point a domain to my IP address. The problem is that when accessing through this subdomain it needs to ""drop"" at a certain URL.</p>

<p>For example:</p>

<p>sub.domainofmyclient.com => mydomain.com/<strong>foo/bar</strong></p>

<p>How can I achieve this?</p>

<p>PS: Server is VPS with Apache (with WHM/cPanel).</p>
","<domain-name-system><redirect>","2018-10-11 14:48:47"
"1041952","iptables -I INPUT -p TCP -j ACCEPT","<pre><code>iptables -I INPUT -p TCP -j ACCEPT
</code></pre>
<p>Is executing the above command would increase security risks on the Centos 7 server?
And how to roll back that command?</p>
<p>Is below command okay for rolling back above command?</p>
<pre><code>iptables -D INPUT -p TCP -j ACCEPT
</code></pre>
<p>and what does this do?</p>
","<linux><centos><iptables>","2020-11-10 05:47:27"
"1007667","Slow domain user logon after promotion of ADC to DC","<p>I had DC and ADC server of Windows 2016 R2 in a domain environment. The DC crashed and I promoted ADC to DC and shifted all five FSMO roles to new DC.</p>

<p>After the crashed DC was removed from the network, the Users are facing very slow logins. It is taking 4 to 5 minutes of login.</p>

<p>Clients are of Windows 7 and Windows 10.</p>

<p>DNS is pointing to right servers.</p>

<p><strong>DCDIAG result are:</strong></p>

<pre><code>C:\&gt;dcdiag

Directory Server Diagnosis

Performing initial setup:
   Trying to find home server...
   Home Server = DATABASE
   * Identified AD Forest.
   Done gathering initial info.

Doing initial required tests

   Testing server: Default-First-Site-Name\DATABASE
      Starting test: Connectivity
         The host 5167d093-8105-4ade-86c7-403fb0b3647f._msdcs.DomainName.COM could not be resolved to an IP address. Check
         the DNS server, DHCP, server name, etc.
         Got error while checking LDAP and RPC connectivity. Please check your firewall settings.
         ......................... DATABASE failed test Connectivity

Doing primary tests

   Testing server: Default-First-Site-Name\DATABASE
      Skipping all tests, because server DATABASE is not responding to directory service requests.


   Running partition tests on : ForestDnsZones
      Starting test: CheckSDRefDom
         ......................... ForestDnsZones passed test CheckSDRefDom
      Starting test: CrossRefValidation
         ......................... ForestDnsZones passed test CrossRefValidation

   Running partition tests on : DomainDnsZones
      Starting test: CheckSDRefDom
         ......................... DomainDnsZones passed test CheckSDRefDom
      Starting test: CrossRefValidation
         ......................... DomainDnsZones passed test CrossRefValidation

   Running partition tests on : Schema
      Starting test: CheckSDRefDom
         ......................... Schema passed test CheckSDRefDom
      Starting test: CrossRefValidation
         ......................... Schema passed test CrossRefValidation

   Running partition tests on : Configuration
      Starting test: CheckSDRefDom
         ......................... Configuration passed test CheckSDRefDom
      Starting test: CrossRefValidation
         ......................... Configuration passed test CrossRefValidation

   Running partition tests on : DomainName
      Starting test: CheckSDRefDom
         ......................... DomainName passed test CheckSDRefDom
      Starting test: CrossRefValidation
         ......................... DomainName passed test CrossRefValidation

   Running enterprise tests on : DomainName.COM
      Starting test: LocatorCheck
         ......................... DomainName.COM passed test LocatorCheck
      Starting test: Intersite
         ......................... DomainName.COM passed test Intersite
</code></pre>
","<windows><active-directory><domain-controller><slow-connection>","2020-03-20 09:06:32"
"795152","Serving Intranet Website to specific clients using internet & VPN","<p>I own a company with three branches in different locations. I developed a portal application using php and mysql which run on Xampp latest version. I have a server computer in my main office where I installed my portal application. Now I want to serve this application to only my three branches. These three branches get access to internet. So using internet, how can my three branches access the portal site which I am serving from my main office using internet and IP address? Is this possible to setup in Windows OS? SO without spending much money for hosting and domain, I thought this suffice my needs. Please advise and share. Thanks.</p>
","<vpn><apache-2.4><intranet>","2016-08-05 19:29:04"
"935116","getting ""/dev/sdb is apparently in use by the system"" after adding an independent disk to the machine","<p>In my centos7 when I add and partition a new disk to the machine and try to make files system on top of it , I get this annoying error message </p>

<p><code>/dev/sdb is apparently in use by the system; will not make a filesystem here</code></p>

<p>The point is that the disk is an ISCSI one and it is pretty independent. I have another SATA device as well. I don't know what is keeping this newly partitioned disk busy, and rebooting the server seems pointless and I also tried <code>partprobe</code>, but no luck!</p>

<p>Here is my <code>fdisk -l</code> output</p>

<pre><code>Disk /dev/sda: 21.5 GB, 21474836480 bytes, 41943040 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes`
Disk label type: dos`
Disk identifier: 0x00004776`

Device    Boot      Start    End      Blocks   Id  System

/dev/sda1   *        2048     2099199     1048576   83  Linux

/dev/sda2         2099200    41943039    19921920   8e  Linux LVM

Disk /dev/mapper/cl-root: 18.2 GB, 18249416704 bytes, 35643392 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes


Disk /dev/mapper/cl-swap: 2147 MB, 2147483648 bytes, 4194304 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes


Disk /dev/sdb: 1265 MB, 1265734656 bytes, 2472138 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk label type: dos
Disk identifier: 0x0000a7c3

Device Boot      Start         End      Blocks   Id  System
/dev/sdb1            2048     2342911     1170432   83  Linux
</code></pre>
","<linux><centos7><mkfs>","2018-10-11 19:11:53"
"868558","validate file content with bash Regular Expressions + Linux","<p>how to validate the following file content</p>

<p>that should be include single integer/float number 
by bash Regular Expression or any other idea with awk/sed</p>

<pre><code> cat  /var/VERSION/Version_F35_project_usa
 2.8
</code></pre>
","<linux><bash><sed><awk><regular-expressions>","2017-08-14 14:17:39"
"795183","UFW deny not working","<p>The basic configuration is the following:</p>

<pre><code>To                         Action      From
--                         ------      ----
Anywhere                   DENY        xx.xxx.xxx
22/tcp                     ALLOW       Anywhere
80/tcp                     ALLOW       Anywhere
22/tcp (v6)                ALLOW       Anywhere (v6)
80/tcp (v6)                ALLOW       Anywhere (v6)
</code></pre>

<p>Not using ipv6, verivied ipv4.  varnish and nginx logs show the ipv4 address that should be nuked from space.</p>

<p>ubuntu 16.04, </p>

<p>ufw 0.35
iptables v1.6.0</p>

<p>tried:</p>

<pre><code> ufw insert 1 deny from xx.xxx.xxx

ufw show raw: starts with
Chain INPUT (policy DROP 0 packets, 0 bytes)
    pkts      bytes target     prot opt in     out     source               destination         
       0        0 DROP       all  --  *      *       xx.xxx.xxx       0.0.0.0/0
</code></pre>

<p>In spite of this I can still pull a webpage down.  Its behind cloudflare and I get an nginx 404 (catch all 404) when going directly to the ip address (bypassing cloudflare completely.   Varnish on port 80 with nginx at 8080 on the backend, but the request should be completely ignored?</p>
","<ubuntu><networking><iptables><ufw>","2016-08-06 01:15:52"
"935150","Google Cloud - Virtual machine down (RDP)","<p>I have a VM on google cloud and I have been using it for few years. Every now and then, I suddenly won't be able to connect anymore via RDP. It simply says that the IP is unreachable, like when a server is down or doesn't exist.</p>

<p>Another thing is that the server also restarts by itself (seems like a failure, which is something I would expect from windows).</p>

<p>For the outage scenario, what I would normally do is access the dashboard (cloud.google.com) > virtual machines > stop and start the VM, but the issue with that is because the IP gets changed to a new one. This is the frustrating bit because I have applications pointing to this IP and other people using this IP to connect via RDP as well.</p>

<p>Any way around this?</p>

<p>Thanks</p>
","<google-cloud-platform><windows-server-2016>","2018-10-11 23:29:44"
"795205","Controlling access rights on PD that is shared between several Kubernetes services","<p>I have a PD that is shared between two Kubernetes services. Once service runs a server application inside a container currently as <code>root</code> (I will probably change this to, say, <code>user1</code>), the other service runs Docker's <a href=""https://hub.docker.com/_/postgres/"" rel=""nofollow noreferrer"">Postgres image</a> as <code>postgres</code>. I would like to make sure that both <code>root</code> (eventually <code>user1</code>) and <code>postgres</code> have read-write access to certain directories below the PD's mount point, for instance <code>postgres</code> should ""own"" the directory <code>/mnt/disk/my-pd/pgdata</code>.</p>

<p>Can this be configured on the level of a Kubernetes spec? If not and if I have to configure it manually ""outside"", can I make assumptions about how UIDs and GIDs relate to each other throughout the cluster, i.e. does Kubernetes (or do standard Docker images such as <code>postgres</code>) include some kind of directory service that would keep the UID e.g. for <code>postgres</code> in sync throughout the cluster?</p>

<p>Here is the Postgres-related portion of my spec in its current form:</p>

<pre><code>kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  name: postgres-deployment
spec:
  template:
    metadata:
      labels:
        app: postgres-app
    spec:
      containers:
      - name: postgres-container
        image: postgres
        env:
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-secret
              key: postgres-password.txt
        - name: PGDATA
          value: /mnt/disk/my-pd/pgdata
        - name: POSTGRES_DB
          value: mydb
        ports:
        - protocol: TCP
          containerPort: 5432
        volumeMounts:
        - name: my-volume
          mountPath: /mnt/disk/my-pd
      volumes:
      - name: my-volume
        gcePersistentDisk:
          pdName: my-pd
          fsType: ext4
</code></pre>
","<postgresql><kubernetes><google-kubernetes-engine>","2016-08-06 07:21:31"
"935152","Debian 9 dummy network adapter - works on Local but not Remote?","<p>I'm trying to set up an IPSEC tunnel between my remote external server and a locally situated machine.  My remote has a naked public IP4 in a data-centre.  My nearby server has a local NAT hidden 192.168.x.y address, however I have forwarded all the required ports and protocols to it; UDP 500, UDP 4500, ah, esp, as well as removing the router helper bindings.  Both are running Debian 9 Stretch.  My router is fine, that is sorted out already.  The remote server can see the local on ESP, AH and IKE using nmap.</p>

<p>On the local, I did this (internal 100 range IP changed)</p>

<pre><code>modprobe dummy
ip link add name dummy0 type dummy
ip address add 100.64.1.1/32 dev dummy0
</code></pre>

<p>and when I do </p>

<pre><code>ifconfig
</code></pre>

<p>it shows up like this</p>

<pre><code>dummy0: flags=195&lt;UP,BROADCAST,RUNNING,NOARP&gt;  mtu 1500
        inet 100.64.1.1  netmask 255.255.255.255  broadcast 0.0.0.0
        inet6 fe80::xxxx:xxxx:xxxx:xxxx  prefixlen 64  scopeid 0x20&lt;link&gt;
        ether xx:xx:xx:xx:xx:xx  txqueuelen 1000  (Ethernet)
        RX packets 0  bytes 0 (0.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 189  bytes 70950 (69.2 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
</code></pre>

<p>When we go to the external server,</p>

<pre><code>modprobe dummy
</code></pre>

<p>it shows up with lsmod:</p>

<pre><code>root@host:~# lsmod  | grep dummy
dummy                  16384  0
root@host:~#
</code></pre>

<p>, and the ifconfig command shows no dummy virtual ethernet port, after performing a similar set of subsequent commands.</p>

<p>Why is dummy0 not showing?</p>

<p>Any ideas what is wrong?</p>

<p>[Edit- added 13/10/2018 22:18 BST]
I've added this diagram for clarity.  This is what I'm setting up:</p>

<p><a href=""https://i.sstatic.net/Aw7cM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Aw7cM.png"" alt=""Description of Data Centre to Locally hosted IPSEC tunnel solution with StrongSwan on Debian 9""></a>.  The tunnel for Cyrus replication is up.</p>
","<debian><iptables><linux-networking><ipsec><strongswan>","2018-10-12 00:25:09"
"935182","Kubernates assing multiple endpoints","<p>i have the following configurations on kubernates:</p>

<pre><code>apiVersion: apps/v1beta2
kind: Deployment
metadata:
  name: {{ include ""e-lucene-eva.fullname"" . }}
  labels:
    app.kubernetes.io/name: {{ include ""e-lucene-eva.name"" . }}
    helm.sh/chart: {{ include ""e-lucene-eva.chart"" . }}
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app.kubernetes.io/name: {{ include ""e-lucene-eva.name"" . }}
      app.kubernetes.io/instance: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app.kubernetes.io/name: {{ include ""e-lucene-eva.name"" . }}
        app.kubernetes.io/instance: {{ .Release.Name }}
    spec:
      containers:
        - name: {{ .Chart.Name }}
          image: ""{{ .Values.image.repository }}:{{ .Values.image.tag }}""
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: evaptr
              containerPort: 8089
          resources:
            resources:
            limits:
              cpu: 150m
              memory: 1528Mi
            requests:
              cpu: 100m
              memory: 664Mi


apiVersion: v1
kind: Service
metadata:
  name: st-evabot-adm-backend-service
  namespace: st-evabot
spec:
  ports:
    - targetPort: bck-port
      port: 80
      protocol: TCP
  selector:
    app: evabot
    tier: backend
</code></pre>

<p>but when i run kubectl get endpoints:</p>

<pre><code>NAME                                       ENDPOINTS                                                    AGE
evabot-db-service                          191.255.54.169:27017                                         19h
evabot-lucene-service                      191.255.48.148:8089,191.255.54.169:8089,191.255.55.35:8089   13h
</code></pre>

<p>why i have 3 ip ? and other has just 1?
Another question is .. these ips are for the connections with other pods ? </p>

<pre><code>apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: ""1""
  creationTimestamp: null
  generation: 1
  labels:
    app.kubernetes.io/instance: st-evabot-evabot-lucene
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/name: e-lucene-eva
    helm.sh/chart: e-lucene-eva-0.1.1
  name: st-evabot-evabot-lucene-e-lucene-eva
  selfLink: /apis/extensions/v1beta1/namespaces/st-evabot/deployments/st-evabot-evabot-lucene-e-lucene-eva
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/instance: st-evabot-evabot-lucene
      app.kubernetes.io/name: e-lucene-eva
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app.kubernetes.io/instance: st-evabot-evabot-lucene
        app.kubernetes.io/name: e-lucene-eva
    spec:
      containers:
      - image: registry-dgt.eni.com/eni/evabot-lucene:1.0.4
        imagePullPolicy: IfNotPresent
        name: e-lucene-eva
        ports:
        - containerPort: 8089
          name: evaptr
          protocol: TCP
        resources:
          limits:
            cpu: 150m
            memory: 1528Mi
          requests:
            cpu: 100m
            memory: 664Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status: {}



evabot-adm-ui-service                      ClusterIP   191.255.46.131   &lt;none&gt;        3031/TCP       21h       app=evabot,tier=frontend
evabot-lucene-service                      ClusterIP   191.255.46.28    &lt;none&gt;        8089/TCP       2h        app=evabot
st-evabot-adm-backend-service              NodePort    191.255.45.200   &lt;none&gt;        80:31971/TCP   8m        app=evabot,tier=backend
st-evabot-db-service                       ClusterIP   191.255.47.200   &lt;none&gt;        27017/TCP      31m       app=mongo-proxy
st-evabot-evabot-db-mongodb-replicaset     ClusterIP   None             &lt;none&gt;        27017/TCP      3d        app=mongodb-replicaset,release=st-evabot-evabot-db
st-evabot-fe-evabot-web-apache-webserver   ClusterIP   191.255.45.37    &lt;none&gt;        80/TCP         1d        app=apache-webserver,release=st-evabot-fe-evabot-web
tiller-deploy                              ClusterIP   191.255.44.63    &lt;none&gt;        44134/TCP      6d        app=helm,name=tiller
</code></pre>
","<kubernetes>","2018-10-12 08:07:30"
"795302","how to recover from chmod 0644 -R /*","<p>I had a big mistake</p>

<p>I run this command on a centos 6.8 server:</p>

<pre><code>chmod 0644 -R /* instead of chmod 0644 -R ./*
</code></pre>

<p>now directory listing on root directory / and all files failed
when I run this command: </p>

<pre><code>chmod 0775 -R /*
</code></pre>

<p>I get this error: </p>

<pre><code>bash: /bin/chmod: Permission denied
</code></pre>

<p>how can I fix it without reinstall my centos?</p>
","<linux><centos>","2016-08-07 03:20:05"
"1042156","Email Two-Factor Authentication on FortiGate","<p>I'm currently implementing Multi-Factor Authentication on FortiGate, using mailer system and refer to this document <a href=""https://kb.fortinet.com/kb/documentLink.do?externalID=FD45585#:%7E:text=NOTE%3A%20Email%20based%20two%2Dfactor,as%20usual%20during%20access%20attempt"" rel=""nofollow noreferrer"">link</a>.</p>
<p>There is no options to change the subject or content of email, because currently the subject looks like <code>Authcode: {number}</code> and the content <code>Your authentication token code is XXXXXX</code>.</p>
<p>Is it possible to change that? is there any  way to do that?</p>
","<fortigate><fortinet>","2020-11-11 13:43:10"
"868771","Hosting migration - no email downtime","<p>I need to move the current hosting to a new location with interrupting email.
Currently the MX done by a 3rd party hosting company.</p>

<ul>
<li>Does not matter if users see old or new site during move.</li>
<li>Email cannot be interrupted during the move 24/7 business.</li>
</ul>

<p>The image shows the logic on migration.</p>

<p><a href=""https://i.sstatic.net/DsGVx.png"" rel=""nofollow noreferrer"">DNS Graph</a></p>

<p>If you see any flaw, please let me know in detail.</p>

<p>Thank you-</p>

<p>Edit: 
Answer to mythofechelon</p>

<p>Only the web hosting provider is going to change.
Though the current web hosting is providing some DNS, MX records point to it.</p>
","<domain-name-system><migration><web-hosting>","2017-08-15 17:03:13"
"935466","Accessing Splunk instance via AWS","<p>I signed up for a Udemy course on Splunk. I got through to the lecture on setting up an AWS instance of Splunk.</p>

<p>Amazon says, ""Get the IP of your instance, append <code>:8000</code>"" and dump it in a web browser to access your Splunk admin panel. The instructor says the same thing. For him, it appears to work as advertised. Despite me following his instructions verbatim four times it doesn't work for me, despite the video showing his instructions working.</p>

<p>Here's my EC2 Management Console:</p>

<p><a href=""https://i.sstatic.net/92ww4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/92ww4.png"" alt=""enter image description here""></a></p>

<p>Here are the instructions:</p>

<p><a href=""https://i.sstatic.net/P44WF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/P44WF.png"" alt=""enter image description here""></a></p>

<p>When I dump in my IP appended with <code>:8000</code> into a browser instance (ex: <code>123.456.789.1:8000</code> it times out despite the Amazon Management Console saying it's running with a public DNS entry and an IP address.</p>

<p>I tried pinging the DNS entry and the IP address and there's no response.</p>

<p><strong>Two questions:</strong></p>

<ol>
<li>How did I muck up?</li>
<li>How do I fix it?</li>
</ol>
","<amazon-web-services><splunk>","2018-10-14 14:25:58"
"795556","Supporting Http2 on Amazon Linux with Apache with OpenSSL 1.0.1","<p>I'm trying to run an http2 web server on Amazon linux over Apache.
From what I understand OpenSSL 1.0.2 is required to use Http2. However, my current distribution only supports OpenSSL 1.0.1. I've checked RedHat and its the same.</p>

<p>Is there an easy way to do this?</p>
","<redhat><openssl><amazon-linux><http2>","2016-08-08 15:19:44"
"935522","What are the name servers for Alibaba Cloud DNS","<p>I would like to use Alibaba Cloud Basic DNS for an existing domain, but I did not figure out how to request for name servers. It should be something like <code>ns1.aliyun.com</code>, <code>ns2..</code></p>

<p>I read through the <a href=""https://www.alibabacloud.com/help/product/34269.htm?spm=a2c63.p38356.a1.2.709d108cwwOKHL"" rel=""nofollow noreferrer"">documentation</a> and tried to figure out but no mention of name servers nor actions what can instantiate them.</p>

<p>As you see in the screenshot DNS Server is empty.
<a href=""https://i.sstatic.net/f26E6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/f26E6.png"" alt=""Instance Details of a domain in Aliyun console""></a></p>
","<dns-hosting><alibaba-cloud>","2018-10-15 04:28:26"
"1042464","Can ping and RDP VPN's servers but cannot http/sftp","<p>I am successfully connected to a VPN network, I can RDP to remote PC and ping the servers (remote PC and servers in VPN's network). However, I cannot http/sftp the server's sites through home's modem/router (the site takes a lot of time to load and then return Network Error or Timeout error). When I connect to VPN with mobile hotspot, I can http/sftp to the site without any problem.</p>
<p>I checked the home's network subnet and the VPN's network subnet and both are different:</p>
<p>Home network subnet: is <code>192.168.0.X</code><br>
VPN network subnet: is <code>192.168.1.X</code><br>
Subnet mask is: <code>255.255.255.0</code></p>
<p>Additionally, I disabled the firewall in my router with no help. What else I can do.</p>
<p><strong>Case</strong>:</p>
<p>Assume I have a site hosted in server with IP: 192.168.1.50. From my local network, I can ping the server after connecting to VPN but I cannot browse the site using <a href=""http://192.168.1.50"" rel=""nofollow noreferrer"">http://192.168.1.50</a> or sftp to 192.168.1.50.</p>
<p><strong>Update</strong></p>
<p>Here is the result after running <code>tracert -c 192.168.1.50</code>:</p>
<pre><code>Tracing route to 192.168.1.50 over a maximum of 30 hops

  1    18 ms    12 ms    19 ms  10.8.0.1
  2    13 ms    20 ms    12 ms  192.168.1.50

Trace complete.
</code></pre>
<p>server route info:</p>
<pre><code>0.0.0.0         192.168.1.1     0.0.0.0         UG    0      0        0 eno1
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
172.25.0.0      0.0.0.0         255.255.0.0     U     0      0        0 br-8730262616cd
192.168.1.0     0.0.0.0         255.255.255.0   U     0      0        0 eno1
</code></pre>
<p><strong>Wireshark</strong>:</p>
<p>I installed and ran wireshark in my local PC and tried to call server with IP 192.168.1.150. After connecting to VPN, I was assigned IP 10.8.0.10. The handshake and TLS was successful but wireshark gave a warning on the last TCP exchange:</p>
<p><a href=""https://i.sstatic.net/X0Btg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/X0Btg.png"" alt=""enter image description here"" /></a></p>
","<networking><vpn><router>","2020-11-13 18:27:51"
"935567","Installing AIDE in Alpine","<p>I'm trying to make my Alpine instance more secure by installing AIDE but it doesn't seem to be in the repos:</p>

<pre><code>#apk add aide
fetch http://dl-cdn.alpinelinux.org/alpine/v3.8/main/x86_64/APKINDEX.tar.gz
fetch http://dl-cdn.alpinelinux.org/alpine/v3.8/community/x86_64/APKINDEX.tar.gz
ERROR: unsatisfiable constraints:
  aide (missing):
    required by: world[aide]
The command '/bin/sh -c apk add aide' returned a non-zero code: 1
</code></pre>

<p>Is there a way to install it in Alpine?</p>
","<alpine><aide>","2018-10-15 10:29:31"
"935572","Google Cloud Compute Engine CentOS 7 Sudo command","<p>Till Friday 12/10/2018 I was able to run sudo commands (Ex:<code>sudo mongodump --db iot_ac</code>). Today When I tried to run any command as root(sudo),i get the below message(Connected through SSH):</p>

<blockquote>
  <p>We trust you have received the usual lecture from the local System
  Administrator. It usually boils down to these three things:</p>
  
  <ol>
  <li><p>Respect the privacy of others.</p></li>
  <li><p>Think before you type.</p></li>
  <li><p>With great power comes great responsibility.</p></li>
  </ol>
  
  <p><code>[sudo] password for platformcloudtest:</code></p>
</blockquote>

<p>Also not sure where I can find the password also, Please Help.</p>
","<centos7><google-cloud-platform><google-compute-engine><sudo><google>","2018-10-15 11:40:42"
"795605","Email server making TCP connections from source port TCP 25 to destination TCP 6666/66667","<p>I'm hoping you guys can help shed a bit of light of what's going on here.</p>

<p>I have recently set up some egress filtering rules to prevent unwanted egress connections on my network. In doing so I see some bizarre traffic that looks like there is traffic leaving my email server on TCP port 25 and is destined for TCP 6666/6667.</p>

<p>The connections have happened only 3 or 4 times in the last 11 days or so and it looks as though anywhere between 8 - 15 packets are sent and that's it. There seem to be no lasting connection between the IP's and I haven't be able to run a <code>netstat -ab</code> command while it's happening to try to find out what process is initiating the connection.</p>

<p>The server is tightly controlled, has Anti-Spam &amp; Anti-virus running on it and is not used to browse the internet or anything like that. Windows patches are installed monthly and I don't see any behaviour that would indicate that the system is part of a botnet (no excessive MS Exchange queues).</p>

<p>Does anyone have any suggestions as to what's going on or can shed a bit of light as to how I can uncover what's happening here? What I'm hoping is happening is that the foreign server is starting the communication on that port and the email server is simply responding to the request but I'm not sure that's the case. The firewall is a Mikrotik with RouterOS.</p>

<p>A log in the router looks like this</p>

<pre><code>in:ether1 out:ether12, src-mac:00:0C:65:12:92:12, proto TCP(ACK,FIN) 172.16.0.10:25-&gt;205.211.XXX.XXX:6666 NAT(172.16.0.10:25-&gt;YYY.YYY.YYY.YYY:25-&gt;205.211.XXX.XXX:6666 len 52
</code></pre>

<p>172.16.0.10 = private email server IP</p>

<p>205.211.XXX.XXX = destination IP</p>

<p>YYY.YYY.YYY.YYY = our public IP</p>
","<exchange-2010><filtering>","2016-08-08 18:44:18"
"1042506","How do megasites like Youtube perform backups?","<p>How do megasites like Youtube perform backups? According to <a href=""https://www.quora.com/Where-does-YouTube-store-so-many-videos"" rel=""nofollow noreferrer"">https://www.quora.com/Where-does-YouTube-store-so-many-videos</a>, 2014 they stored 76 PB every year, a number that most certainly has increased a lot since then. Is it even possible to backup this or do they solve the problems backups usually are used for in some other way (such as so much redundancy that it simply can't fail)?</p>
","<backup><big-data>","2020-11-14 02:44:48"
"935645","How to get Skype for Business and Teams working","<p>Users in our company have been ignoring SfB and using their own choice of software to get features missing from SfB. This has meant that we have users using different software (HipChat, Stride, Slack and SfB) and we are now looking to get all users use Teams instead, company wide.</p>

<p>To achieve this we have started rolling out Teams to the bigger teams as a proof of concept to ensure that it fulfills everyone's needs.</p>

<p>The issue is that the users who are not using Teams are only using SfB and our Teams users need to run both applications. I understand it is possible to communicate with SfB users from Teams, but I am struggling to find anything that shows me how to do it.</p>

<p>Is it possible or have I misunderstood? If it is, how do I configure it?</p>

<p><strong>What I am seeing... or not as the case maybe</strong></p>

<p>I see the status of SfB users in Teams, but if I message one of them they only receive an email about a message being sent to them in Teams. Since they don't have access to Teams they cannot respond.</p>

<p>If a SfB user messages me, I see the message in SfB if running, or I get an email to say I missed their message.</p>
","<microsoft-office-365><skype-for-business-online>","2018-10-15 20:40:47"
"795760","Distinguishing Mobile and Non-mobile Devices on Wi-Fi","<p>I was at a restaurant recently and the public Wifi access page of the restaurant had the following message:</p>

<p><img src=""https://i.sstatic.net/rGBQP.png"" alt=""image""></p>

<p><strong>PEAK HOURS</strong>
To better serve customers, Wi-Fi access is limited to one 30 minute session for non-mobile devices between 11 am and 2 pm.</p>

<p>I was curious as to how the mobile and non-mobile devices are differentiated on Wi-Fi networks?</p>
","<networking><router><wifi><mobile-devices>","2016-08-09 13:41:08"
"935751","How to debug why I'm loosing connection to my Docker container when VPN is up?","<p>I'm using Docker 18.06.1-ce on Ubuntu 16, and I have a container exposing 8012 port. When I do</p>

<pre><code>curl ""http://localhost:8012/""
</code></pre>

<p>I get response </p>

<pre><code>Error: Requested URL / not found
</code></pre>

<p>But when VPN is up (openfortivpn) after long time waiting I get </p>

<pre><code>curl: (56) Recv failure: Connection reset by peer
</code></pre>

<p>When I mess my port number, I instantly get error:</p>

<pre><code>curl ""http://localhost:8011/""
curl: (7) Failed to connect to localhost port 8011: Connection refused
</code></pre>

<p>How to debug and fix this? Obviously localhost pings, and traceroute to it is one hop. </p>
","<vpn><docker>","2018-10-16 13:36:39"
"795793","Service named failed to start (bad zone)","<p>I am trying to create a new master zone for the domain name innobignet.local but keeping getting errors.</p>
<p>I'm using WebMin to configure the new master zone, so it should be syntactically correct. I've also manually checked the zone file and config file and they both seem correct to me.</p>
<p>Here's the error log:</p>
<pre><code>-- Unit named.service has begun starting up.
Aug 09 22:07:58 centos7-test.innobignet.local bash[5496]: zone            localhost.localdomain/IN: loaded serial 0
Aug 09 22:07:58 centos7-test.innobignet.local bash[5496]: zone localhost/IN: loaded serial 0
Aug 09 22:07:58 centos7-test.innobignet.local bash[5496]: zone 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa/IN: loaded serial 0
Aug 09 22:07:58 centos7-test.innobignet.local bash[5496]: zone 1.0.0.127.in-addr.arpa/IN: loaded serial 0
Aug 09 22:07:58 centos7-test.innobignet.local bash[5496]: zone 0.in-addr.arpa/IN: loaded serial 0
Aug 09 22:07:58 centos7-test.innobignet.local bash[5496]: zone innobignet.local/IN: NS 'centos7-test.innobignet.local' has no address records (A or AAAA)
Aug 09 22:07:58 centos7-test.innobignet.local bash[5496]: zone innobignet.local/IN: not loaded due to errors.
Aug 09 22:07:58 centos7-test.innobignet.local bash[5496]: _default/innobignet.local/IN: bad zone
Aug 09 22:07:58 centos7-test.innobignet.local systemd[1]: named.service: control process exited, code=exited status=1
Aug 09 22:07:58 centos7-test.innobignet.local systemd[1]: Failed to start Berkeley Internet Name Domain (DNS).
-- Subject: Unit named.service has failed
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
--
-- Unit named.service has failed.
--
-- The result is failed.
Aug 09 22:07:58 centos7-test.innobignet.local systemd[1]: Unit named.service entered failed state.
Aug 09 22:07:58 centos7-test.innobignet.local systemd[1]: named.service failed.
</code></pre>
<p>Here's config file.</p>
<pre><code>options {
    listen-on port 53 {
            192.168.1.7;
            };

    directory       &quot;/var/named&quot;;
    dump-file       &quot;/var/named/data/cache_dump.db&quot;;
    statistics-file &quot;/var/named/data/named_stats.txt&quot;;
    memstatistics-file &quot;/var/named/data/named_mem_stats.txt&quot;;

    recursion yes;
    dnssec-enable yes;
    dnssec-validation yes;

    /* Path to ISC DLV key */
    bindkeys-file &quot;/etc/named.iscdlv.key&quot;;

    managed-keys-directory &quot;/var/named/dynamic&quot;;

    pid-file &quot;/run/named/named.pid&quot;;
    session-keyfile &quot;/run/named/session.key&quot;;
    forwarders {
            203.12.160.35;
            203.12.160.36;
            8.8.8.8;
            };
};

logging {
    channel default_debug {
            file &quot;data/named.run&quot;;
            severity dynamic;
    };
};

zone &quot;.&quot; IN {
    type hint;
    file &quot;named.ca&quot;;
};

include &quot;/etc/named.rfc1912.zones&quot;;
include &quot;/etc/named.root.key&quot;;

zone &quot;innobignet.local&quot; {
    type master;
    file &quot;/var/named/innobignet.local.hosts&quot;;
    };
</code></pre>
<p>Here's my zone file:</p>
<pre><code>$ttl 38400
innobignet.local.       IN      SOA     centos7-test.innobignet.local. bob.green.live.com (
                        1470744461
                        10800
                        3600
                        604800
                        38400 )
innobignet.local.       IN      NS      centos7-test.innobignet.local.
</code></pre>
","<bind><centos7><nameserver>","2016-08-09 15:38:28"
"1042709","Can't create free app service plan","<p>I am trying to create a free linux app service plan. However ARM keeps telling me:</p>
<blockquote>
<p>This subscription has reached the limit of 1 Free Linux app service
plan(s) it can create in this region. Please choose a different sku
or region.</p>
</blockquote>
<p>This however is misleading as there is not a single app service plan. Not a free one, none in the resource group nor in the whole subscription.</p>
<p>It's very likely that there <strong>was</strong> a free linux/windows based app service plan earlier, that got deleted at some point. Does Azure keep record of that?</p>
","<azure><azure-web-apps><azure-app-services>","2020-11-16 10:56:42"
"935841","DKIM public key & DKIM from DNS are the same but don't match because of spacing","<p>Please take a look at this picture of mail-tester.com saying that <a href=""https://i.sstatic.net/xzII233.png"" rel=""nofollow noreferrer"">Your DKIM signature is not valid</a>.</p>

<p>As you can see, the DKIM signature and public key are identical in term of text (characters) but the only problem is the spaces between them. I do not know how to fix this, or what is the proper way to edit a DKIM from zone record. Currently, I try to copy the DKIM signature shown in the page and manually paste it over to the DNS TXT record (public key). After refreshing the page, it passes the test, but after making another request, the spaces disappear and the test failed again.</p>

<p>What is the correct way to correct the DKIM public key so it matches the DKIM of the signature?</p>

<p>Thank you.</p>
","<domain-name-system><dns-zone><dkim><opendkim>","2018-10-16 22:52:54"
"869399","Did my colocation provider give me the wrong gateway given the address block?","<p>I'm colocating a 1U server with a 1U pfsense firewall (first time doing this) and the datacenter gave me the following:</p>

<pre><code>IPv4 address block: x.x.x.225/28
Gateway: x.x.x.226
Subnet mask: 255.255.255.240
</code></pre>

<p>Using the available online subnet calculators, I see that my usable IP range is <code>x.x.x.225 - x.x.x.238</code></p>

<p>The provider is giving me a single cable from their switch into my firewall.</p>

<ol>
<li><p>What IP address should I give the firewall and did they make a
mistake on the gateway? That gateway seems to be one of my own
usable IPs.</p></li>
<li><p>What IP address should I set as a gateway in the firewall itself?</p></li>
</ol>
","<networking><colocation>","2017-08-18 16:57:00"
"869406","HP DL360 G8 - Configure ILO port as normal NIC remotely","<p>I'm trying to configure the dedicated ILO mgmt port on a dl360 g8 server to be used as a regular NIC - the host is running one bare-metal machine (no esx). Can I configure this NIC as normal network in DHCP and then find the DHCP record to connect to it and give the machine it's fixed IP?</p>

<p>Note: this is an emergency situation after some incompetent engineer intervened on-site and pulled out all the cables from the non-ILO NIC's. This will be a work-around solution for the weekend.</p>

<p>Thanks!</p>
","<networking><hp><ilo>","2017-08-18 18:35:12"
"935899","sudo user is not allowed to execute systemctl","<p>I'm trying to allow a user to use sudo to manage a custom systemctl service, this however seems to fail and I can't figure out why.</p>

<pre><code>[root@testvm sudoers.d]# ll
total 16
-r--r-----. 1 root root 334 Oct  9 15:42 20_appgroup
-r--r-----. 1 root root 104 Sep 17 11:24 98_admins
</code></pre>

<p>The 'appgroup' contains this;</p>

<pre><code>[root@testvm sudoers.d]# cat 20_appgroup
%appgroup    ALL= /usr/bin/systemctl restart test.service, 
/usr/bin/systemctl start test.service, /usr/bin/systemctl stop 
test.service, /usr/bin/systemctl status test.service
</code></pre>

<p>I have double checked that the user is member of the appgroup, however when this user runs <strong>sudo systemctl start test.service</strong> this results in an error saying;</p>

<pre><code>Sorry, user tester is not allowed to execute '/usr/bin/systemctl start test' as root on testvm.
</code></pre>

<p>Any thought on what could be the issue?</p>
","<linux><sudo><groups><rhel7><systemctl>","2018-10-17 08:58:39"
"1008773","Metal-as-a-Service in an existing environment","<p>We're looking at MaaS for our bare metal server environment to replace old school kickstart scripts. MaaS works great if you want to go with the full ""reimage everything"" train. However, we have many machines where we don't have the time (or it's extremely disruptive) to re-image... in order to ""cut over"" to MaaS.</p>

<p>In short: Is it possible to add a machine to MaaS in such a way that it allows said machine to boot as normally, like it does with machines that are re-imaged? I understand that there is testing and inventory collection steps in MaaS, which is perfectly ok. I just can't re-image thousands of machines just to use MaaS. </p>
","<maas>","2020-03-27 23:12:40"
"796140","Why does my server get so many CLOSE_WAIT connections?","<p>I need help because my Linux Ubuntu server are getting too many SQL connections, and when I checked using netstat -t there are many connections like these:</p>
<pre><code>tcp6 1 0 websitesaya.co.id:http 98-142-172-163.re:37854 CLOSE_WAIT 
tcp6 1 0 websitesaya.co.id:http 98-142-172-163.re:34962 CLOSE_WAIT 
tcp6 1 0 websitesaya.co.id:http 98-142-172-163.re:51678 CLOSE_WAIT 
tcp6 0 0 websitesaya.co.id:http 157-171-172-163.r:44102 CLOSE_WAIT 
tcp6 0 0 websitesaya.co.id:http vmi80876.contabo.:46980 CLOSE_WAIT 
tcp6 0 531 websitesaya.co.id:http ks.kgovps.com:35146 LAST_ACK 
tcp6 0 0 websitesaya.co.id:http 98-142-172-163.re:55052 CLOSE_WAIT 
tcp6 1 0 websitesaya.co.id:http 157-171-172-163.r:36082 CLOSE_WAIT 
tcp6 0 0 websitesaya.co.id:http 157-171-172-163.r:33698 CLOSE_WAIT 
tcp6 0 0 websitesaya.co.id:http 157-171-172-163.r:59778 CLOSE_WAIT 
tcp6 0 0 websitesaya.co.id:http 157-171-172-163.r:51166 CLOSE_WAIT 
tcp6 0 0 websitesaya.co.id:http vmi80876.contabo.:49693 CLOSE_WAIT 
tcp6 0 0 websitesaya.co.id:http 98-142-172-163.re:52406 CLOSE_WAIT 
tcp6 1 0 websitesaya.co.id:http 157-171-172-163.r:53266 CLOSE_WAIT 
tcp6 0 639 websitesaya.co.id:http 98-142-172-163.re:58032 LAST_ACK 
tcp6 1 0 websitesaya.co.id:http ks.kgovps.com:59676 CLOSE_WAIT 
</code></pre>
<p>and still hundreds or more like that</p>
<p>And no matter how many times I restarted the server or disconnect and reconnect. Those strage connections keeps appearing and appearing again.</p>
<p>Here's what I've tried:</p>
<ul>
<li>I cannot block those incoming connections from Linux firewall because every time I want to block them, there are errors such as &quot;157-171-172-163.r not found&quot; Then how in the world I can block this IPs from trying to connect to my server?</li>
<li>I cannot kill those process on MySQL Workbench because there are errors &quot;Cannot kill threads 0&quot; or something like that</li>
</ul>
<p>[Update] Other helpful advice from other forum point that this might be SYN Flood Attack</p>
","<mysql><firewall><tcp><ipv6><ddos>","2016-08-11 03:07:16"
"796149","I am stuck out of my host server ssh","<p>I have a dedicated server and I wanted to login to ssh and I changed the /etc/ssh/sshd_config file but then I could not login my ssh of server. When I run this command I get;</p>

<pre><code>OpenSSH_7.2p2 Ubuntu-4ubuntu1, OpenSSL 1.0.2g-fips  1 Mar 2016
debug1: Reading configuration data /etc/ssh/ssh_config
debug1: /etc/ssh/ssh_config line 19: Applying options for *
debug2: resolving ""akillisaha.com"" port 22
debug2: ssh_connect_direct: needpriv 0
debug1: Connecting to akillisaha.com [5.135.141.20] port 22.
debug1: Connection established.
debug1: identity file /home/selcuk/.ssh/id_rsa type 1
debug1: key_load_public: No such file or directory
debug1: identity file /home/selcuk/.ssh/id_rsa-cert type -1
debug1: key_load_public: No such file or directory
debug1: identity file /home/selcuk/.ssh/id_dsa type -1
debug1: key_load_public: No such file or directory
debug1: identity file /home/selcuk/.ssh/id_dsa-cert type -1
debug1: key_load_public: No such file or directory
debug1: identity file /home/selcuk/.ssh/id_ecdsa type -1
debug1: key_load_public: No such file or directory
debug1: identity file /home/selcuk/.ssh/id_ecdsa-cert type -1
debug1: key_load_public: No such file or directory
debug1: identity file /home/selcuk/.ssh/id_ed25519 type -1
debug1: key_load_public: No such file or directory
debug1: identity file /home/selcuk/.ssh/id_ed25519-cert type -1
debug1: Enabling compatibility mode for protocol 2.0
debug1: Local version string SSH-2.0-OpenSSH_7.2p2 Ubuntu-4ubuntu1
debug1: Remote protocol version 2.0, remote software version OpenSSH_5.3
debug1: match: OpenSSH_5.3 pat OpenSSH_5* compat 0x0c000000
debug2: fd 3 setting O_NONBLOCK
debug1: Authenticating to akillisaha.com:22 as 'ahmetb'
debug1: SSH2_MSG_KEXINIT sent
debug1: SSH2_MSG_KEXINIT received
debug2: local client KEXINIT proposal
debug2: KEX algorithms: curve25519-sha256@libssh.org,ecdh-sha2-nistp256,ecdh-sha2-nistp384,ecdh-sha2-nistp521,diffie-hellman-group-exchange-sha256,diffie-hellman-group-exchange-sha1,diffie-hellman-group14-sha1,ext-info-c
debug2: host key algorithms: ssh-rsa-cert-v01@openssh.com,rsa-sha2-512,rsa-sha2-256,ssh-rsa,ecdsa-sha2-nistp256-cert-v01@openssh.com,ecdsa-sha2-nistp384-cert-v01@openssh.com,ecdsa-sha2-nistp521-cert-v01@openssh.com,ssh-ed25519-cert-v01@openssh.com,ecdsa-sha2-nistp256,ecdsa-sha2-nistp384,ecdsa-sha2-nistp521,ssh-ed25519
debug2: ciphers ctos: chacha20-poly1305@openssh.com,aes128-ctr,aes192-ctr,aes256-ctr,aes128-gcm@openssh.com,aes256-gcm@openssh.com,aes128-cbc,aes192-cbc,aes256-cbc,3des-cbc
debug2: ciphers stoc: chacha20-poly1305@openssh.com,aes128-ctr,aes192-ctr,aes256-ctr,aes128-gcm@openssh.com,aes256-gcm@openssh.com,aes128-cbc,aes192-cbc,aes256-cbc,3des-cbc
debug2: MACs ctos: umac-64-etm@openssh.com,umac-128-etm@openssh.com,hmac-sha2-256-etm@openssh.com,hmac-sha2-512-etm@openssh.com,hmac-sha1-etm@openssh.com,umac-64@openssh.com,umac-128@openssh.com,hmac-sha2-256,hmac-sha2-512,hmac-sha1
debug2: MACs stoc: umac-64-etm@openssh.com,umac-128-etm@openssh.com,hmac-sha2-256-etm@openssh.com,hmac-sha2-512-etm@openssh.com,hmac-sha1-etm@openssh.com,umac-64@openssh.com,umac-128@openssh.com,hmac-sha2-256,hmac-sha2-512,hmac-sha1
debug2: compression ctos: none,zlib@openssh.com,zlib
debug2: compression stoc: none,zlib@openssh.com,zlib
debug2: languages ctos: 
debug2: languages stoc: 
debug2: first_kex_follows 0 
debug2: reserved 0 
debug2: peer server KEXINIT proposal
debug2: KEX algorithms: diffie-hellman-group-exchange-sha256,diffie-hellman-group14-sha1,diffie-hellman-group-exchange-sha1
debug2: host key algorithms: ssh-rsa,ssh-dss
debug2: ciphers ctos: aes256-ctr,aes128-ctr
debug2: ciphers stoc: aes256-ctr,aes128-ctr
debug2: MACs ctos: hmac-sha2-512,hmac-sha2-256,hmac-ripemd160
debug2: MACs stoc: hmac-sha2-512,hmac-sha2-256,hmac-ripemd160
debug2: compression ctos: none,zlib@openssh.com
debug2: compression stoc: none,zlib@openssh.com
debug2: languages ctos: 
debug2: languages stoc: 
debug2: first_kex_follows 0 
debug2: reserved 0 
debug1: kex: algorithm: diffie-hellman-group-exchange-sha256
debug1: kex: host key algorithm: ssh-rsa
debug1: kex: server-&gt;client cipher: aes128-ctr MAC: hmac-sha2-256 compression: none
debug1: kex: client-&gt;server cipher: aes128-ctr MAC: hmac-sha2-256 compression: none
debug1: SSH2_MSG_KEX_DH_GEX_REQUEST(2048&lt;8192&lt;8192) sent
debug1: got SSH2_MSG_KEX_DH_GEX_GROUP
debug2: bits set: 4113/8192
debug1: SSH2_MSG_KEX_DH_GEX_INIT sent
debug1: got SSH2_MSG_KEX_DH_GEX_REPLY
debug1: Server host key: ssh-rsa SHA256:+BLBEx4qLkEES1Ull8bhGEXMHIfJ0n1n8oty66H0jPw
debug1: Host 'akillisaha.com' is known and matches the RSA host key.
debug1: Found key in /home/selcuk/.ssh/known_hosts:7
debug2: bits set: 4150/8192
debug2: set_newkeys: mode 1
debug1: rekey after 4294967296 blocks
debug1: SSH2_MSG_NEWKEYS sent
debug1: expecting SSH2_MSG_NEWKEYS
debug2: set_newkeys: mode 0
debug1: rekey after 4294967296 blocks
debug1: SSH2_MSG_NEWKEYS received
debug2: key: /home/selcuk/.ssh/id_rsa (0x55f67f484260), agent
debug2: key: /home/selcuk/.ssh/id_dsa ((nil))
debug2: key: /home/selcuk/.ssh/id_ecdsa ((nil))
debug2: key: /home/selcuk/.ssh/id_ed25519 ((nil))
debug2: service_accept: ssh-userauth
debug1: SSH2_MSG_SERVICE_ACCEPT received
debug1: Authentications that can continue: publickey,gssapi-keyex,gssapi-with-mic
debug1: Next authentication method: gssapi-keyex
debug1: No valid Key exchange context
debug2: we did not send a packet, disable method
debug1: Next authentication method: gssapi-with-mic
debug1: Unspecified GSS failure.  Minor code may provide more information
No Kerberos credentials available

debug1: Unspecified GSS failure.  Minor code may provide more information
No Kerberos credentials available

debug1: Unspecified GSS failure.  Minor code may provide more information


debug1: Unspecified GSS failure.  Minor code may provide more information
No Kerberos credentials available

debug2: we did not send a packet, disable method
debug1: Next authentication method: publickey
debug1: Offering RSA public key: /home/selcuk/.ssh/id_rsa
debug2: we sent a publickey packet, wait for reply
debug1: Authentications that can continue: publickey,gssapi-keyex,gssapi-with-mic
debug1: Trying private key: /home/selcuk/.ssh/id_dsa
debug1: Trying private key: /home/selcuk/.ssh/id_ecdsa
debug1: Trying private key: /home/selcuk/.ssh/id_ed25519
debug2: we did not send a packet, disable method
debug1: No more authentication methods to try.
Permission denied (publickey,gssapi-keyex,gssapi-with-mic).
</code></pre>

<p>I have to login to my ssh and change the config file. But tha's not possible. Please help me . . ..</p>
","<ubuntu><ssh><ovh>","2016-08-11 04:55:11"
"935953","Cannot create VPS virtualizer","<p>I have installed Virtualizor (KVM) on a CentOS server and I'm trying to create a VPS<br>
I have added storage and an IP pool for my VMs<br>
for creating storage I followed this guide<br>
<a href=""https://www.virtualizor.com/wiki/Add_New_Storage"" rel=""nofollow noreferrer"">https://www.virtualizor.com/wiki/Add_New_Storage</a><br>
when I try to create a VPS it fails and gives me the following logs</p>

<pre><code>sh: /bin/chattr: Permission denied
Starting Task : 12
Syncing up with Slave
Creating VPS : 3
File descriptor 3 (socket:[147315]) leaked on lvcreate invocation. Parent         
PID 43360: /usr/local/emps/bin/php
File descriptor 4 (socket:[147332]) leaked on lvcreate invocation. Parent 
PID 43360: /usr/local/emps/bin/php
File descriptor 5 (socket:[164259]) leaked on lvcreate invocation. Parent 
PID 43360: /usr/local/emps/bin/php
selabel_open failed: No such file or directory
selabel_open failed: No such file or directory
WARNING: dos signature detected on /dev/vg1/vsv1003-doguu2eku8qqo2f6- 
d8m51fxht2j4olsu at offset 510. Wipe it? [y/n]: [n]
Aborted wiping of dos.
1 existing signature left on the device.
File descriptor 3 (socket:[147315]) leaked on lvdisplay invocation. Parent 
PID 43360: /usr/local/emps/bin/php
File descriptor 4 (socket:[147332]) leaked on lvdisplay invocation. Parent 
PID 43360: /usr/local/emps/bin/php
File descriptor 5 (socket:[164259]) leaked on lvdisplay invocation. Parent 
PID 43360: /usr/local/emps/bin/php
selabel_open failed: No such file or directory
selabel_open failed: No such file or directory
File descriptor 3 (socket:[147315]) leaked on lvresize invocation. Parent 
PID 43360: /usr/local/emps/bin/php
File descriptor 4 (socket:[147332]) leaked on lvresize invocation. Parent 
PID 43360: /usr/local/emps/bin/php
File descriptor 5 (socket:[164259]) leaked on lvresize invocation. Parent 
PID 43360: /usr/local/emps/bin/php
selabel_open failed: No such file or directory
selabel_open failed: No such file or directory
New size (25600 extents) matches existing size (25600 extents).
sfdisk: Warning: The partition table looks like it was made
for C/H/S=*/4/32 (instead of 13054/255/63).
For this listing I'll assume that geometry.

Checking that no-one is using this disk right now ...
BLKRRPART: Invalid argument
OK
sfdisk: Warning: The partition table looks like it was made
for C/H/S=*/4/32 (instead of 13054/255/63).
For this listing I'll assume that geometry.

Warning: partition 1 does not end at a cylinder boundary
Warning: partition 2 does not start at a cylinder boundary
Warning: partition 2 does not end at a cylinder boundary
Warning: no primary partition is marked bootable (active)
This does not matter for LILO, but the DOS MBR will not boot this disk.
BLKRRPART: Invalid argument
If you created or changed a DOS partition, /dev/foo7, say, then use dd(1)
to zero the first 512 bytes:  dd if=/dev/zero of=/dev/foo7 bs=512 count=1
(See fdisk(8).)
mkswap: /dev/mapper/vg1-vsv1003--doguu2eku8qqo2f6--d8m51fxht2j4olsu2: 
warning: wiping old swap signature.
device-mapper: remove ioctl on vg1-vsv1003--doguu2eku8qqo2f6-- 
d8m51fxht2j4olsu2  failed: Device or resource busy
e2fsck 1.42.9 (28-Dec-2013)
resize2fs 1.42.9 (28-Dec-2013)
setlocale: No such file or directory
error: failed to connect to the hypervisor
error: Failed to connect socket to '/var/run/libvirt/libvirt-sock': No such 
file or directory
Creation Done
Setting Root / Admin Password
setlocale: No such file or directory
error: failed to connect to the hypervisor
error: Failed to connect socket to '/var/run/libvirt/libvirt-sock': No such 
file or directory
Building DHCP
Redirecting to /bin/systemctl restart dhcpd.service
Starting VPS
setlocale: No such file or directory
error: failed to connect to the hypervisor
error: Failed to connect socket to '/var/run/libvirt/libvirt-sock': No such 
file or directory
setlocale: No such file or directory
error: failed to connect to the hypervisor
error: Failed to connect socket to '/var/run/libvirt/libvirt-sock': No such 
file or directory
setlocale: No such file or directory
error: failed to connect to the hypervisor
error: Failed to connect socket to '/var/run/libvirt/libvirt-sock': No such 
file or directory
Fetching VPS Status
setlocale: No such file or directory
error: failed to connect to the hypervisor
error: Failed to connect socket to '/var/run/libvirt/libvirt-sock': No such 
file or directory  
</code></pre>

<p>I also noticed that libvirtd service fails to start with the following errors</p>

<pre><code>Oct 17 09:12:40 master libvirtd[42928]: 2018-10-17 13:12:40.091+0000: 42944: 
info : hostname: master  
Oct 17 09:12:40 master libvirtd[42928]: 2018-10-17 13:12:40.091+0000: 42944: 
error : virSecuritySELinuxQEMUInitialize:634 : cannot open SELinux   
label_handle: No such file or directory   
Oct 17 09:12:40 master libvirtd[42928]: 2018-10-17 13:12:40.093+0000: 42944: 
error : qemuSecurityInit:447 : internal error: Failed to initialize security 
drivers   
Oct 17 09:12:40 master libvirtd[42928]: 2018-10-17 13:12:40.093+0000: 42944: 
error : virStateInitialize:775 : Initialization of QEMU state driver failed: 
internal error: Failed to initialize security drivers   
Oct 17 09:12:40 master libvirtd[42928]: 2018-10-17 13:12:40.093+0000: 42944: 
error : daemonRunStateInit:837 : Driver state initialization failed   
</code></pre>

<p>Anybody has any idea what is wrong with my installation?</p>
","<centos7><kvm-virtualization>","2018-10-17 13:38:15"
"796242","Disable Raid 5 on an existing system","<p>I have a server running in RAID 5 (software raid with 3 disks).
Is there a way to disable the RAID 5 and just work on 1 disk (without formatting the pc)?</p>

<p>(+Does changing RAID count as a hardware change and does it affect the Windows OS activation?)</p>
","<windows><windows-server-2008><raid><software-raid><raid5>","2016-08-11 13:16:17"
"796279","How can I change the path of my php.ini conf?","<p>A few days ago, I compiled PHP 5.5 from sources and installed it on my Debian 8.5 server but I got some issues. I decided to remove it and re-install PHP through package. But I still got issues because the path of my php.ini did not change.</p>

<p>What I actually have is :</p>

<pre><code>kevin@debian:~$ php --ini
Configuration File (php.ini) Path: /usr/local/lib
Loaded Configuration File: /usr/local/lib/php.ini
Scan for additional .ini files in: (none)
</code></pre>

<p>I would like to retrieve a default configuration like this :</p>

<pre><code>kevin@debian:~$ php --ini
Configuration File (php.ini) Path: /etc/php/5.6/cli
Loaded Configuration File: /etc/php/5.6/cli/php.ini
Scan for additional .ini files in: /etc/php/5.6/cli/conf.d
</code></pre>

<p>I tried to re-install php5, php5-fpm... through apt-get but my path never changed.</p>

<p>Is there any command on Debian to change the default path of my php.ini conf ?</p>

<p>Thx.</p>

<p><strong>Edit :</strong> </p>

<p>which php :</p>

<pre><code>kevin@debian:~$ which php
/usr/local/bin/php
</code></pre>

<p>php -v :</p>

<pre><code>kevin@debian:~$ php -v
PHP 5.5.15 (cli)
</code></pre>
","<php.ini><debian-jessie>","2016-08-11 15:39:47"
"936141","0B size partition","<p>I have one 0B partition in my disk as the 2nd partition in my CentOS machine.</p>

<p><img src=""https://i.sstatic.net/gAM3M.png"" alt=""image fdisk output""></p>

<p>How can I delete it? <br>
In fdisk , it shows could not delete the partition. <br>
I want to increase vda1 size using growpart command.</p>
","<centos><partition><fdisk>","2018-10-18 12:05:49"
"1043063","Why a MultiAZ db.t2.small reserved instance costs two normailzed units?","<p>I have a MultiAZ Aurora MySQL cluster with 2 db.t2.small instances, the Master and the Read Replica</p>
<p>Now I want to buy MultiAZ Reserved Instances for both db.t2.small instances but when doing the purchase I see that each Reserved Instance costs 2 normalized units</p>
<p><a href=""https://i.sstatic.net/AbzmE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AbzmE.png"" alt=""Reserved MultiAZ db.t2.small Pricing details"" /></a></p>
<p>On the other hand if I choose SingleAZ which is not the case of my cluster, I get 1 normalized unit which is the corresponding size for a single db.t2.small instance</p>
<p><a href=""https://i.sstatic.net/5n8nB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5n8nB.png"" alt=""Reserved SingleAZ db.t2.small Pricing details"" /></a></p>
<p>Why do normalized units and corresponding usage charges get duplicated for MultiAZ Reserved Instances? I did not find clear enough documentation on AWS pages [<a href=""https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithReservedDBInstances.html"" rel=""nofollow noreferrer"">1</a>, <a href=""https://aws.amazon.com/rds/reserved-instances/"" rel=""nofollow noreferrer"">2</a>]</p>
<p>Would a single db.t2.small Reserved Instance be enough to cover the entirety of both db.t2.small at my MultiAZ Aurora MySQL cluster or should I buy 2 db.t2.small Reserved Instances for it to be a proper &quot;Reserved Instances Aurora cluster&quot;?</p>
<p>PD: For both purchasing configurations I am specifying only 1 db.t2.small Reserved Instance and No Upfront payment</p>
","<amazon-web-services><amazon-rds>","2020-11-18 14:38:18"
"936176","Network setting for similar looking subnets","<p>I have a CentOS 7 PC (with 2 NIC cards) and 2 servers. I'd like to access these servers from the 2 NICs in a different way. Something like one server can be reached only from NIC1, and the other server should be accessed only from NIC2. I've realized that I'm unable to put them into the same subnet using the IP addresses 10.10.10.{1-4}/29 for some reason.</p>

<p>However, should this work?</p>

<pre><code>NIC1: 10.10.10.1/30     -&gt;     Server1: 10.10.10.2/30
NIC2: 10.10.10.4/30     -&gt;     Server2: 10.10.10.5/30
</code></pre>

<p>I'm always confused with networking and subnetting. Please advise.</p>
","<networking><centos><subnet><netmask>","2018-10-18 14:41:58"
"936193","openSUSE Linux - Name based virtual host | Apache/2.4.33","<p>I desperately try to setup a name based virtual host for use in my local network, for hours.</p>

<p>This is my configuration, the path is correct</p>

<p><strong>/etc/apache2/vhosts.d/hausfux.conf</strong></p>

<pre><code>&lt;VirtualHost 192.168.40.44:80&gt;

    ServerName hausfux.dev

    DocumentRoot ""/srv/www/htdocs/hausfux/devLocal""
    ErrorLog /var/log/apache2/hausfux_error.log
    CustomLog /var/log/apache2/hausfux_access.log common

&lt;/VirtualHost&gt;
</code></pre>

<p>I informed myself <a href=""https://httpd.apache.org/docs/current/en/vhosts/name-based.html"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Then I added this line to my <strong>/etc/hosts</strong> file.</p>

<pre><code>192.168.40.44        hausfux.dev
</code></pre>

<p>I also added the same line on my windows clients hosts file.</p>

<p>Then I restarted apache2 with <code>systemctl restart apache2</code>.</p>

<p>If I enter <code>http://hausfux.dev</code> in my windows clients browser, then it automatically changes to <code>https://hausfux.dev</code> and it fails.</p>

<p>If I try to open the project as I used to,with the full absolute link, e.g. <code>http://192.168.40.44/hausfux/devLocal</code> then I get Status Error Code 500 (server internal error) now. If I remove my virtual host and restart apache2 then the site works again.</p>

<p>What am I doing wrong?</p>

<p>PS: There are no errors in <code>/var/log/apache2/hausfux_error.log</code></p>

<hr>

<p><strong>Update</strong></p>

<p>If I use the following configuration, then it works if I type in the servers IP Adress in the browser at my windows client (<a href=""http://192.168.40.44"" rel=""nofollow noreferrer"">http://192.168.40.44</a>).</p>

<pre><code>NameVirtualHost *:80

&lt;VirtualHost *:80&gt;

    ServerName hausfux.dev

    DocumentRoot ""/srv/www/htdocs/hausfux/devLocal""
    ErrorLog /var/log/apache2/hausfux_error.log
    CustomLog /var/log/apache2/hausfux_access.log common

    &lt;Directory ""/srv/www/htdocs/hausfux/devLocal""&gt;
        Require all granted
    &lt;/Directory&gt;

&lt;/VirtualHost&gt;
</code></pre>

<p>But my goal is, to just enter a name, e.g. <code>http://hausfux.dev</code> instead of the ip address. Is this possible in OpenSUSE or should I use another OS, e.g. Ubuntu?</p>

<p><strong>Update:</strong></p>

<p>Response from pinging my DNS-Server:</p>

<blockquote>
  <p>PING 192.168.40.20 (192.168.40.20) 56(84) bytes of data. 64 bytes from
  192.168.40.20: icmp_seq=1 ttl=64 time=0.126 ms 64 bytes from 192.168.40.20: icmp_seq=2 ttl=64 time=0.149 ms</p>
</blockquote>
","<apache-2.4><virtualhost><opensuse>","2018-10-18 16:20:58"
"796416","Failed to create shadow copy (CopyFile) While Loading an .aspx page in mono-server","<h1>Failed to create shadow copy (CopyFile)</h1>

<h2>Description: HTTP 500. Error processing request</h2>

<p><strong>Before I begin my question; below is link to my previous problem, It was 503 error; Resolved by enabling seboolian of</strong> <strong><em>httpd_execmem</em></strong>:<br>
<a href=""https://serverfault.com/questions/795947/mono-crashes-while-selinux-enforced-how-to-get-mono-server-worked-while-selinux"">Mono crashes while SELinux enforced, How to get mono server worked while SELinux is enforced?</a>
<br><br><strong>Now, here I'm stuck --</strong><br><br>
<strong><em>1) When i try to access .aspx page:</em></strong></p>

<pre><code># elinks --dump 127.0.0.1/ASP-Portal/index.aspx

                   Server Error in '/ASP-Portal' Application
   Failed to create shadow copy (CopyFile).
   Description: HTTP 500. Error processing request.

   Stack Trace:

System.ExecutionEngineException: Failed to create shadow copy (CopyFile).
  at (wrapper managed-to-native) System.Reflection.Assembly:LoadFrom (string,bool)
  at System.Reflection.Assembly.LoadFrom (System.String assemblyFile) [0x00000] in &lt;filename unknown&gt;:0
  at System.Web.Compilation.BuildManager.LoadAssembly (System.String path, System.Collections.Generic.List`1 al) [0x00000] in &lt;filename unknown&gt;:0
  at System.Web.Compilation.BuildManager.GetReferencedAssemblies () [0x00000] in &lt;filename unknown&gt;:0
  at System.Web.Compilation.AssemblyBuilder.BuildAssembly (System.Web.VirtualPath virtualPath, System.CodeDom.Compiler.CompilerParameters options) [0x00000] in &lt;filename unknown&gt;:0
  at System.Web.Compilation.AssemblyBuilder.BuildAssembly (System.CodeDom.Compiler.CompilerParameters options) [0x00000] in &lt;filename unknown&gt;:0
  at System.Web.Compilation.AppCodeAssembly.Build (System.String[] binAssemblies) [0x00000] in &lt;filename unknown&gt;:0
  at System.Web.Compilation.AppCodeCompiler.Compile () [0x00000] in &lt;filename unknown&gt;:0
  at System.Web.HttpApplicationFactory.InitType (System.Web.HttpContext context) [0x00000] in &lt;filename unknown&gt;:0

   Version information: Mono Runtime Version: 2.10.2 (tarball Mon Aug 8
   13:09:50 IST 2016); ASP.NET Version: 2.0.50727.1433
</code></pre>

<p><strong><em>2) audit.log status:</em></strong></p>

<pre><code># cat /var/log/audit/audit.log | audit2allow

#============= httpd_sys_script_t ==============
#!!!! This avc is allowed in the current policy

allow httpd_sys_script_t inotifyfs_t:dir read;
#!!!! This avc is allowed in the current policy

allow httpd_sys_script_t self:process execmem;
allow httpd_sys_script_t tmp_t:file { write getattr };
allow httpd_sys_script_t tmpfs_t:dir read;
allow httpd_sys_script_t tmpfs_t:filesystem getattr;

#============= httpd_t ==============
#!!!! This avc is allowed in the current policy

allow httpd_t httpd_sys_rw_content_t:sock_file unlink;
allow httpd_t self:capability { sys_admin ipc_owner };

#============= xdm_t ==============
#!!!! This avc can be allowed using the boolean 'allow_polyinstantiation'

allow xdm_t admin_home_t:dir read;
allow xdm_t admin_home_t:file read;
</code></pre>

<p><strong><em>3) Context of directory and files where i have stored my testing ASP.NET files provided by mono-project.com</em></strong></p>

<pre><code># ls -Z /var/www/html/ASP-Portal/

drwxr-xr-x. root root system_u:object_r:httpd_sys_rw_content_t:s0 1.1
drwxr-xr-x. root root system_u:object_r:httpd_sys_rw_content_t:s0 2.0
drwxr-xr-x. root root system_u:object_r:httpd_sys_rw_content_t:s0 App_Code
drwxr-xr-x. root root system_u:object_r:httpd_sys_rw_content_t:s0 bin
drwxr-xr-x. root root system_u:object_r:httpd_sys_rw_content_t:s0 controls
-rwxr-xr-x. root root system_u:object_r:httpd_sys_rw_content_t:s0 extensions.dll
-rw-r--r--. root root system_u:object_r:httpd_sys_rw_content_t:s0 favicon.ico
-rw-r--r--. root root system_u:object_r:httpd_sys_rw_content_t:s0 global.asax
-rw-r--r--. root root system_u:object_r:httpd_sys_rw_content_t:s0 index.aspx
-rw-r--r--. root root system_u:object_r:httpd_sys_rw_content_t:s0 missing_components.aspx
-rw-r--r--. root root system_u:object_r:httpd_sys_rw_content_t:s0 mod-mono-server.exe.config
-rw-r--r--. root root system_u:object_r:httpd_sys_rw_content_t:s0 monobutton.png
-rw-r--r--. root root system_u:object_r:httpd_sys_rw_content_t:s0 mono.png
-rw-r--r--. root root system_u:object_r:httpd_sys_rw_content_t:s0 mono-powered-big.png
-rw-r--r--. root root system_u:object_r:httpd_sys_rw_content_t:s0 mono-xsp.css
-rw-r--r--. root root system_u:object_r:httpd_sys_rw_content_t:s0 sample.webapp
-rwxr-xr-x. root root system_u:object_r:httpd_sys_rw_content_t:s0 ServiceClient.exe
-rw-r--r--. root root system_u:object_r:httpd_sys_rw_content_t:s0 small-icon.png
drwxr-xr-x. root root system_u:object_r:httpd_sys_rw_content_t:s0 test
-rw-r--r--. root root system_u:object_r:httpd_sys_rw_content_t:s0 web.config
-rw-r--r--. root root system_u:object_r:httpd_sys_rw_content_t:s0 Web.sitemap
-rw-r--r--. root root system_u:object_r:httpd_sys_rw_content_t:s0 xsp.exe.config
</code></pre>

<p><strong><em>4) messages file status:</em></strong></p>

<pre><code># tail /var/log/messages

Aug 12 12:04:50 shadmin named[1356]: error (network unreachable) resolving 'ns1.isc.ultradns.net/A/IN': 2610:a1:1015::e8#53
Aug 12 12:04:50 shadmin named[1356]: error (network unreachable) resolving 'ns1.isc.ultradns.net/AAAA/IN': 2610:a1:1015::e8#53
Aug 12 13:04:50 shadmin named[1356]: error (network unreachable) resolving 'dlv.isc.org/DNSKEY/IN': 2001:4f8:0:2::19#53
Aug 12 13:04:50 shadmin named[1356]: error (network unreachable) resolving 'dlv.isc.org/DNSKEY/IN': 2001:500:71::30#53
Aug 12 13:04:50 shadmin named[1356]: error (network unreachable) resolving 'dlv.isc.org/DNSKEY/IN': 2001:500:2c::254#53
Aug 12 13:04:51 shadmin named[1356]: error (network unreachable) resolving 'dlv.isc.org/DNSKEY/IN': 2001:4f8:0:2::20#53
Aug 12 13:04:51 shadmin named[1356]: error (network unreachable) resolving 'pdns196.ultradns.biz/A/IN': 2001:503:7bbb:ffff:ffff:ffff:ffff:ff7e#53
Aug 12 13:04:51 shadmin named[1356]: error (network unreachable) resolving 'pdns196.ultradns.biz/AAAA/IN': 2001:500:3682::12#53
Aug 12 13:04:51 shadmin named[1356]: error (network unreachable) resolving 'ns.isc.afilias-nst.info/AAAA/IN': 2a01:8840:8::1#53
Aug 12 13:04:52 shadmin named[1356]: error (network unreachable) resolving 'pdns196.ultradns.biz/A/IN': 2610:a1:1015::e8#53
</code></pre>

<p><strong><em>5) httpd error_log status:</em></strong></p>

<pre><code># tail /var/log/httpd/error_log

[Fri Aug 12 12:58:57 2016] [notice] Digest: generating secret for digest authentication ...
[Fri Aug 12 12:58:57 2016] [notice] Digest: done
[Fri Aug 12 12:58:57 2016] [notice] Apache/2.2.15 (Unix) DAV/2 mod_mono/2.10 configured -- resuming normal operations
Listening on: /tmp/mod_mono_server_global
Root directory: /
Listening on: /tmp/mod_mono_server_global
Root directory: /
Error: Address already in use
Listening on: /tmp/mod_mono_server_shadmin.shahu.com
Root directory: /var/www/html/ASP-Portal
</code></pre>

<p><br>
Note: My hostname is <strong>shadmin.shahu.com</strong> and I saved demo .NET files in <strong>/var/www/html/ASP-Portal/</strong>
<br>
Please, tell me if I'm missing something, Thanks.</p>
","<redhat><httpd><selinux><500-error><mono>","2016-08-12 08:14:07"
"936216","How to Enable IPV6 on Windows Server 2012 R2 (Cloud Google VPS)","<p>I'm new to Cloud Google VPS. Since it's free, I am testing some Windows Server with 2012 R2 OS, but having a problem enabling IPv6 IP Address.</p>

<p><a href=""https://i.sstatic.net/vbFcc.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vbFcc.png"" alt=""https://i.sstatic.net/vbFcc.png""></a></p>

<p>Help would be greatly appreciated!</p>
","<windows><vps><ipv6><google-cloud-platform><google-compute-engine>","2018-10-18 18:51:36"
"796444","How to tune Nginx + MySQL + PHP-FPM bottleneck to resolve high server load?","<p>My server, Ubuntu 14.04.4 LTS, is running at high load, currently running a single site with Nginx + Wordpress + W3 Total Cache + Memcached.</p>

<p>I'm not sure if <code>mysql</code> is causing the workload issue.</p>

<p>Here are the screenshot for <code>htop</code> and <code>mytop</code></p>

<p>I can tell from <code>htop</code> that memory is not the bottleneck here.</p>

<p>It shows that <code>mysqld</code> and <code>php-fpm</code> are the processes using the highest resource.</p>

<p>Can someone tell me how to interpret <code>mytop</code> and does it look normal?</p>

<pre><code>1  [|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||93.6%]     Tasks: 361, 71 thr; 12 running
2  [|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||95.6%]     Load average: 19.52 22.34 19.45 
3  [|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||97.5%]     Uptime: 71 days, 08:54:08
4  [|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||96.8%]
Mem[|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||5570/8015MB]
Swp[                                                                         0/0MB]

MySQL on localhost (5.7.13)
load 23.92 23.12 19.55 43/528 10012 up 7+04:05:52 [05:30:09]
Queries: 103.6M   qps:  175 Slow:  0.0  Se/In/Up/De(%):  79/00/00/00 
Sorts:  8693 qps now:  356 Slow qps: 0.0  Threads:  37 (   1/   3) 79/00/00/00 
Key Efficiency: 50.0%  Bps in/out: 28.8k/532.7k   Now in/out: 57.6k/880.7k
</code></pre>

<p>Or should I be looking at using fastcgi_cache instead to avoid hitting php-fpm?</p>

<p><strong>UPDATE:</strong></p>

<p>I have tried restarting all my services, Nginx, PHP-FPM, and MySQL.</p>

<p>I'm working on a site with rather high traffic, 1.1M daily pageview impressions. </p>

<p>After restarting all the services, the resource goes up pretty quick again.</p>

<p>During off-peak hours, the load value is max out at around 4.00 on my 4-core machine.</p>

<p>During peak hours, the load is at 20 to even 40.</p>
","<mysql><cache><wordpress><performance-tuning><bottleneck>","2016-08-12 09:49:47"
"869894","Can't resolve server by fqdn when search domain is enabled","<p>I have a server named <code>site.dpt.myorg.local</code>, which i want to access both with short name and with fqdn.
When i try to use short <code>site</code> or <code>site.dpt</code> form, everything works just fine. However when i use fqdn - lookup works when using <code>dig</code> or <code>nslookup</code>, but fails when i use web browser, ssh and other tools.</p>

<p>I.e. ssh and ping fail with following error:</p>

<pre><code>$ ssh -vvvv site.dpt.myorg.local
OpenSSH_7.2p2, OpenSSL 1.0.2j-fips  26 Sep 2016
debug1: Reading configuration data /home/user/.ssh/config
debug1: Reading configuration data /etc/ssh/ssh_config
debug1: /etc/ssh/ssh_config line 25: Applying options for *
debug2: resolving ""site.dpt.myorg.local"" port 22
ssh: Could not resolve hostname site.dpt.myorg.local: Name or service not known

$ ping site.dpt.myorg.local
ping: unknown host site.dpt.myorg.local
</code></pre>

<p>I have a following resolv.conf file:</p>

<pre><code>search myorg.local dpt.myorg.local
nameserver 10.0.0.1
nameserver 10.0.0.2
</code></pre>

<p>My OS is Opensuse 42.3</p>

<p>Can anyone point me to the cause of this problem? Thanks!</p>
","<linux><domain-name-system><opensuse><resolv.conf><resolve>","2017-08-22 13:51:54"
"796545","Mounted VHD gives access denied during file copy","<p>I want to copy files from VM to another. I'm running windows 2008 on the VM Manager. Both machines are Server 2012. I tuned them both off. Mounted the VHDs to the manger pc. I can see the both files and folders, but cannot copy.</p>

<p>I want to move files from Windows/WinSxS to the other machine Winsxs. </p>

<pre><code>robocopy I:\Windows\WinSxs K:\Windows\WinSxs
</code></pre>

<p>After that it loops on access denied, waiting 30 seconds.</p>

<p>Why am I getting access denied. I'm using a domain admin account and administrator power shell. </p>
","<windows><virtual-machines><robocopy><vhd>","2016-08-12 18:09:23"
"936397","How do I prevent access on the web to a txt file on my server? i want it to still work, as its for a wizard. But i dont want people to access my code","<p>I'm wondering if anyone can help me please. I have some text files on my server, which run with a Wizard. The issue I am having is that I'm unable to hide the text files on the web browser.</p>

<p>I've managed to hide the other folders needed, but these are compressed folders. I have worked really hard on coding these txt files, and with the current configuration anyone can just copy them. I don't want them to be accessible at all via web browser but I still need the Text files to work, as they are required for my Wizard. Is it possible, to have it show ""403 Access Denied"" but the text file still work with my Wizard? The txt files are currently in my html/builds folder.</p>

<p>I've been stuck on this for days, and can't seem to get an answer. I've even opened a support ticket with my host, but they keep showing me how to password protect it, and when I try that option the text files no longer work on my wizard. </p>

<p>My .htaccess currently has the following code in the html folder: </p>

<p>Options +Indexes</p>

<pre><code>IndexIgnore /Addontexts
IndexIgnore /builds
IndexIgnore /apktexts
IndexIgnore /insomniacsglobalbuild.zip
IndexIgnore /insomniacspluhbuild.zip
IndexIgnore /insomniacsminigreenbuild.zip
IndexIgnore /apks
IndexIgnore /themedbuilds
IndexIgnore /apkz
IndexIgnore /Advanced.txt
IndexIgnore /addons.txt
IndexIgnore /apk.txt
IndexIgnore /autobuilds.txt
IndexIgnore /notify.txt
IndexIgnore /theme.txt
IndexIgnore /youtube.txt
</code></pre>

<p>Any help would be very much appreciated.</p>
","<web-server><.htaccess><http-status-code-403>","2018-10-20 01:41:26"
"936468","`# ssh myuser@myhost` = Permission Denied but `su myuser` THEN `$ ssh myhost` works.. why?","<p>Consider the following commands. How can this happen?</p>

<pre><code>[root@mylocal ~]# ssh myuser@myhost
Permission denied (publickey,gssapi-keyex,gssapi-with-mic).
[root@mylocal ~]# su myuser
[myuser@mylocal ~]$ ssh myhost
Welcome Last login: Sat Oct 20 16:28:48 2018 from mylocal 
[myuser@myhost~]$ 
</code></pre>

<p>please advise how to cure it so that I can do as root, </p>

<pre><code>[root@mylocal ~]# ssh myuser@myhost   
Welcome Last login: Sat Oct 20 16:28:48 2018 from mylocal 
[myuser@myhost~]$
</code></pre>

<p>thanks</p>
","<ssh><centos7>","2018-10-20 21:08:09"
"796653","Does anything changes for a client in the API communication after the command STARTTLS is sent and a response received","<p>I'm trying to understand how the command STARTTLS in IMAP works exactly and what becomes different when it's sent. After I've sent the command ""STARTTLS"" before login process and received the response ""OK Begin TLS negotiation now"" as a client, does anything change for me as a client in the subsequent API commands, that is, the format of the requests and responses, some additional information that I have to process, etc? I mean, the API level, not the low level.</p>
","<imap><network-protocols><starttls>","2016-08-13 15:47:40"
"796699","How do I create a new ESXI install and keep the old VMs","<p>I'm working to make a new ESXi install because the old install is on a flash drive that is still working but may fail because someone thought it was unimportant and left it in their pocket and washed it.  Anyways, it is still working but at risk of failing.  I already tried to use a USB cloning tool and it won't successfully boot off of that so that isn't working.  I think the best option is to create a new ESXi install on the new USB and then somehow import the datastore and the VMs but I don't know how well that will work.  If I were to create a new install on this new USB could I just open the datastore which is on a RAID array and start up the old VMs without a problem?</p>

<p>Thanks!</p>

<p>EDIT:  I'm using ESXi 6 and VMFS datastores</p>
","<vmware-esxi>","2016-08-14 01:16:38"
"936544","Reverse DNS works on our Bind9 server, not anywhere else","<p>Up until a couple of days ago, our reverse DNS for our class C IP space was working fine. Now, other mail servers are reporting that they cannot find the RDNS response for the IP address of our outgoing mail server.</p>

<p>When we try to query our own DNS server for an IP address in our class C, the server responds properly:</p>

<pre><code>$ host 206.12.82.4 206.12.82.130
Using domain server:
Name: 206.12.82.130
Address: 206.12.82.130#53
Aliases: 

4.82.12.206.in-addr.arpa domain name pointer smtp.lightspeed.ca.
4.82.12.206.in-addr.arpa domain name pointer ns2.lightspeed.ca.
</code></pre>

<p>If you ask 8.8.8.8 for the RDNS on any of our IP addresses, it fails:</p>

<pre><code>$ host 206.12.82.4 8.8.8.8
Using domain server:
Name: 8.8.8.8
Address: 8.8.8.8#53
Aliases: 

Host 4.82.12.206.in-addr.arpa not found: 2(SERVFAIL)
</code></pre>

<p>I personally don't think I have any control over this, and it's an issue with the DNS servers from the company that assigned our IP address space. This hasn't happened to me in the 17 years I've been working here.</p>
","<domain-name-system><bind>","2018-10-22 00:17:28"
"936598","WSUS error updates on some computers","<p>Hy everybody !</p>

<p>I have a problem with my WSUS server. A error appear showing that is install only at 99% some updates on computers (see my attachment).  </p>

<p>It is possible do refresh and install this updates ? </p>

<p>Thank you !</p>

<p><a href=""https://i.sstatic.net/QKLIS.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QKLIS.jpg"" alt=""enter image description here""></a></p>
","<wsus>","2018-10-22 12:11:13"
"870173","DNS static work but DNS dynamic from dhcp pool doesn't work","<p>So we have 2 dns servers,</p>

<ul>
<li><code>DNS-Server-A</code>: 20.20.33.82 (normal dns server which I don't have access to it)</li>
<li><code>DNS-Server-B</code>: 172.30.11.254 (palo alto with dns proxy)</li>
</ul>

<p>Core Switch DHCP pool configuration:</p>

<pre><code>ip dhcp pool wifi-user
 network 172.20.12.0 255.255.252.0
 default-router 172.20.12.1
 dns-server 20.20.33.82 172.30.11.254
</code></pre>

<p><code>DNS-Server-A</code> | 20.20.33.82 entry:</p>

<pre><code>name:erp | FQDN:erp.companyname.com | IP: 20.20.38.7
</code></pre>

<p><code>DNS-Server-B</code> | 172.30.11.254 (Palo Alto with DNS Proxy configuration):</p>

<pre><code>name: dns-static1
Primary: 202.x.x.x (IP from ISP)
Secondary: 20.20.33.82 (DNS-Server-A IP)

static entries:
name:ldap | FQDN:ldap.companyname.com | IP: 172.20.10.45
</code></pre>

<p>When I use dynamic/dhcp ip and dns, it'll be like this</p>

<pre><code>computer&gt;ipconfig /all

Wireless LAN adapter Wireless Network Connection:

 Connection-specific DNS Suffix  . :
 Description . . . . . . . . . . . : 1x1 11b/g/n Wireless LAN PCI Express Half
Mini Card Adapter
 Physical Address. . . . . . . . . : XX-XX-XX-XX-XX-X1
 DHCP Enabled. . . . . . . . . . . : Yes
 Autoconfiguration Enabled . . . . : Yes
 Link-local IPv6 Address . . . . . : fe80::9500:1cbf:7f25:6496%13(Preferred)
 IPv4 Address. . . . . . . . . . . : 172.20.12.72(Preferred)
 Subnet Mask . . . . . . . . . . . : 255.255.252.0
 Default Gateway . . . . . . . . . : 172.20.12.1
 DHCP Server . . . . . . . . . . . : 172.20.12.1
 DHCPv6 IAID . . . . . . . . . . . : 190896153
 DHCPv6 Client DUID. . . . . . . . : 00-01-00-01-1F-B7-4A-6F-60-D8-19-CD-36-11

 DNS Servers . . . . . . . . . . . : fe80::1%13
                                     20.20.33.82
                                     172.30.11.254
 NetBIOS over Tcpip. . . . . . . . : Enabled
</code></pre>

<p>Reset the DNS resolver</p>

<pre><code>computer&gt;ipconfig /flushdns

Windows IP Configuration

Successfully flushed the DNS Resolver Cache.
</code></pre>

<p>Ping the entry in <code>DNS-Server-A</code> (Success)</p>

<pre><code>computer&gt;ping erp.companyname.com

Pinging erp.companyname.com [20.20.38.7] with 32 bytes of data:
Reply from 20.20.38.7: bytes=32 time=11ms TTL=250
Reply from 20.20.38.7: bytes=32 time=9ms TTL=250
Reply from 20.20.38.7: bytes=32 time=17ms TTL=250
Reply from 20.20.38.7: bytes=32 time=303ms TTL=250

Ping statistics for 20.20.38.7:
    Packets: Sent = 4, Received = 4, Lost = 0 (0% loss),
Approximate round trip times in milli-seconds:
    Minimum = 9ms, Maximum = 303ms, Average = 85ms
</code></pre>

<p>Ping the entry in <code>DNS-Server-B</code> (Fail)</p>

<pre><code>C:\Users\Nugi&gt;ping ldap.companyname.com
Ping request could not find host ldap.companyname.com. Please check the na
me and try again.
</code></pre>

<p>But when I use static DNS configuration (manually change it from client side)</p>

<pre><code>Preferred DNS server: 20.20.33.82
Alternate DNS server: 172.30.11.254
</code></pre>

<p>I could ping both entries in <code>DNS-Server-A</code> and <code>DNS-Server-B</code></p>

<p>What should I do so I could ping both of the dns entries but with dynamic/dhcp configuration?</p>
","<domain-name-system><cisco><dhcp><cisco-catalyst><palo-alto-networks>","2017-08-24 01:00:23"
"1010483","Nginx config files","<p>This question is not for business use - this is for personal use. Here is my situation. I run a Windows Server 2012 R2 with a domain. I am now adding Home Assistant running on a Raspberry Pi (and running Raspbian).</p>
<p>I am retired so we travel some. So I want to be able to log into the Home Assistant when we are away from home to monitor things. I have registered my own domain on Amazon Route 53, and have it set up so that it tracks my external IP address, just like DDNS programs such as DuckDNS would do.  I initially set up port forwarding on my router to forward port 443 to the Pi and it all worked, so I know it is all set up correctly up to that point.</p>
<p>Unfortunately, I also have some other programs that require https access so I can't just leave it set up to do the port forwarding on my router.</p>
<p>All that said I am now trying to set up Nginx on the Windows Server. I have got that done, and I have it set up to run as a service. But I am struggling with 2 problems:</p>
<ol>
<li><p>I can't get it to start, now that it is a service. it keeps giving an error 1067, which I think means there is a problem with the nginx.conf file. But I am not sure if there is actually a problem with the file, or if it does not find it because I don't have a path set up correctly for the service to know where the file is located. So how do I get that done correctly?</p>
</li>
<li><p>And then the second question is what would the nginx.conf file look like to get the Windows server to listen on port 443 and if it sees a call for my Home Assistant domain, it forwards to the Raspberry Pi, otherwise it just lets the 443 traffic go on to the PC that is trying to pass traffic on port 443?</p>
</li>
</ol>
<p>I hope this all makes sense, and that I have explained it all correctly. I am into new territory for me and I am a little lost on what to do next.</p>
","<nginx><windows-service>","2020-04-01 21:57:48"
"796816","is possible to setting mail server when send email to a domain always target mx record by preference","<p>I have problem when send email to a domain(ex: example.com). in my server's logs always show error : </p>

<pre><code>Fail-info: Unable to connect to zzz.zzz.zzz.zzz : Connection timed out
Status:PROCESSED - RELAY
MBox:INBOX
</code></pre>

<p>I have checked Mx Record of the domain in <a href=""http://mxtoolbox.com"" rel=""nofollow noreferrer"">http://mxtoolbox.com</a> and show 3 mx record like this :</p>

<pre><code>Pref    Hostname            IP Address          TTL     
5       mx5.example.com     xxx.xxx.xxx.xxx     11 hrs
10      mx10.example.com    yyy.yyy.yyy.yyy     11 hrs
20      mx20.example.com    zzz.zzz.zzz.zzz     11 hrs 
</code></pre>

<p>When i TES SMTP of the mx record only <strong>pref 5</strong> is working but my email server always send mail to <strong>pref 20</strong> </p>

<p>Is possible to force my email server to send email from <strong>mx record pref 5 ?</strong>
 and how to configure it ? </p>

<p>btw sorry for my bad english &amp; this is my frist time configuring mail server </p>

<p><strong>UPDATE : My mail server use <a href=""https://www.axigen.com"" rel=""nofollow noreferrer"">https://www.axigen.com</a></strong></p>

<blockquote>
  <p><strong>SOLVED thanks to @Ryan Babchishin</strong> Solved this with configure axigen which support relaying, </p>
  
  <ol>
  <li><p>Go to Message Acceptance Settings > Advanced Setting > Add Acceptance/ Routing Rule</p></li>
  <li><p>With condition : </p>
  
  <p>a. Conditions</p>
  
  <p>a.1. Recepient domain is <strong>mx5.example.com</strong>  </p>
  
  <p>b.2. Relaying email</p>
  
  <p>b. Actions : </p>
  
  <p>b.1. Hostname/ip : <strong>mx5.example.com</strong> port <strong>25</strong> </p>
  
  <p>b.2. Setting (unchecked) Allow StartTLS</p></li>
  </ol>
  
  <p><strong>Save Configuration</strong></p>
</blockquote>

<hr>
","<email-server><mx-record><dns-zone>","2016-08-15 04:13:40"
"796819","Flush Privileges in My SQL gives HY0000 Unknown Error","<p>When I try to run <code>FLUSH PRIVILEGES;</code> in MySQL I get this error:</p>

<p><code>Error 1105 (HY000): Unknown error</code></p>

<p>I am running the very latest version of MySQL 5.7 in Windows Server.</p>

<p>Does anyone have any idea why this might be happening?</p>
","<windows><mysql><permissions><sql>","2016-08-15 04:56:45"
"936722","I can not decrease MTU on my nic","<p>I'm trying to decrease the MTU value on my machine but I receive this response:</p>

<pre><code>[root@apache1bpreprod acn]# ifconfig eth0 mtu 1350
SIOCSIFMTU: Invalid argument
</code></pre>

<p>Why? i expect problems to increase not to decrease.
Can you help me? 
Thanks a lot</p>
","<mtu>","2018-10-23 07:35:16"
"1043645","CWP | User login --> uerror -- Cannot login as a user in CentOS 8 using Centos Panel","<p>I'm really feeling very frustrated...</p>
<p>2 days now and I couldn't figure it out.
Without a good reason, I had a &quot;uerror&quot; when trying to login into CWP user panel.
<a href=""https://i.sstatic.net/nWm8h.jpg"" rel=""nofollow noreferrer"">This is a screenshot</a></p>
<p>I'm pretty sure of the credentials, also, I've done the following:</p>
<p>1- Changed the user package.
2- Tried to update CWP.
3- Restarted all services.
4- Rebooted server.
5-Deleted and re-created the account.</p>
<p>here are some error logs:</p>
<pre><code>=&gt; /usr/local/cwp/php71/var/log/php-fpm.log &lt;==
[23-Nov-2020 18:34:20] WARNING: [pool login] child 65870 said into stderr: &quot;NOTICE: PHP message:          PHP Notice:  Undefined index: intended in /usr/local/cwpsrv/var/services/users/login/index.php on line 0&quot;
[23-Nov-2020 18:34:20] WARNING: [pool login] child 65870 said into stderr: &quot;NOTICE: PHP message: PHP Notice:  Undefined index: intended in /usr/local/cwpsrv/var/services/users/login/index.php on line 0&quot;
</code></pre>
<p>apache error log: <a href=""https://pastebin.com/HEz5rN4F"" rel=""nofollow noreferrer"">https://pastebin.com/HEz5rN4F</a></p>
<p>cwp php-fpm log: <a href=""https://pastebin.com/hcHbzUCj"" rel=""nofollow noreferrer"">https://pastebin.com/hcHbzUCj</a></p>
<p>php-fpm-slowlog-cwpsrv: <a href=""https://pastebin.com/J87b10hV"" rel=""nofollow noreferrer"">https://pastebin.com/J87b10hV</a></p>
<p>php-fpm-slowlog-someuser: <a href=""https://pastebin.com/Yyi53kNc"" rel=""nofollow noreferrer"">https://pastebin.com/Yyi53kNc</a></p>
<p>Any help would be greatly appreciated  :-*</p>
","<centos><centos7><vps><centos8>","2020-11-23 21:17:54"
"870577","nslookup not working on route-53 public hosted zone","<p>I created a new record set example.my.domain.com in the public hosted zone my.domain.com and when I'm trying to do, ""nslookup example.my.example.com"", it's not working. This is the response of nslookup: </p>

<pre><code>Server:     10.224.0.2
Address:    10.224.0.2#53
** server can't find example.my.example.com: NXDOMAIN
</code></pre>

<p>I already went through these solutions...</p>

<ul>
<li><a href=""https://serverfault.com/questions/741071/sub-subdomain-records-for-public-hosted-zone-in-aws-not-working"">sub-subdomain records for public hosted zone in AWS not working</a> .  </li>
<li><a href=""https://serverfault.com/questions/606880/how-can-i-troubleshoot-a-route-53-hosted-zone"">How can I troubleshoot a Route 53 hosted zone?</a> .  </li>
<li><a href=""https://serverfault.com/questions/659058/nslookup-does-not-work-on-certain-ip-address"">nslookup does not work on certain IP address</a> . </li>
</ul>

<p>Didn't work!</p>
","<domain-name-system><amazon-web-services><dns-zone><amazon-route53>","2017-08-26 14:15:54"
"937095","Putting an e-mail domain to blacklist","<p>Is it possible for us to put a specific e-mail domain to blacklist?</p>

<p>I know you can personally tag and report as spam/scam e-mail for the incoming ones (to a specific email address, more like filtering them out), but I was wondering if you can put a specific e-mail domain to a blacklist so that domain gets filtered out in the spam folder automatically.</p>

<p>I am asking this because I own a small start-up company and I've been getting issues with our customers getting scammed by a specific e-mail domain.</p>

<p>How are the e-mails that we get from our daily spam folders get filtered out to spam folder?</p>

<p>Thanks in advance.</p>
","<email><email-server>","2018-10-25 04:19:07"
"796902","Can only access intranet, Windows server 2012 AD/DNS/File Server","<p>Okay, this machine can only ping up to it's default gateway. Which happens to be a sonicwall firewall. I have parsed the logs in the sonicwall and do not see any indication that even cares what the server sends out. As a matter of fact I see no records from the server in the log files at all.</p>

<p>DNS manager, and user management runs extremely slow, I actually have to reset accounts via command line (oh no!!) instead of using window's built in gui. </p>

<p>What I have tried(no particular order)</p>

<ul>
<li>Killed Windows Firewall/AVG</li>
<li>Ran netsh winsock reset</li>
<li>ran netsh int ip reset reset.txt</li>
<li>tried pining 8.8.8.8 (times out)</li>
<li>Ping Default Gateway (responds)</li>
<li>Set static/dynamic ip(with the mac reserved to the usually static ip in the sonic wall)</li>
<li>Reinstalled Network card</li>
<li>Unchecked IP v6</li>
<li>Removed/Reinstalled DNS Server</li>
<li>ipconfig /release /renew</li>
<li>reset the arp cache</li>
</ul>

<p>What I am going to do 
 - around noon (eastern time) I am going to reboot the sonicwall, and Internet Connection. (DONE)</p>

<p>Clients pcs that specify the server as the primary DNS server work correctly, and the servers hostname is pingable. </p>

<p>No other PCs/Servers/Devices have any internet access issues at all..</p>

<p>I'll update after the reboot, does anyone have any other ideals?</p>

<p>EDIT*
Route Print
<a href=""https://i.sstatic.net/1i3ZE.png"" rel=""nofollow noreferrer"">route print</a></p>
","<windows-server-2012><internet>","2016-08-15 14:49:02"
"1010890","proper way to start software at boot?","<p>I am so outdated.  I not designed system start/stop boot scripts since  the ""#_something"" days.</p>

<p>I have an app based on docker compose that I need to start at boot and stop before shutdown.</p>

<p>what is now considered the proper way to do that: sysctl, systemctl, service, or something else.</p>

<p>since this is dependent on docker running which depends on other things.  I am thinking systemctl</p>

<p>What is the current proper way to start software at boot?
Can you recommend good documentation and examples to that recommendation?</p>

<p>My actual usage is RHEL 7.6+ and Unbuntu 18.4+  but I would like to support the broadest base of linux/unix platforms</p>
","<linux>","2020-04-04 13:02:30"
"1043729","Forward packet from loopback interface based on routing table","<p>I have a problem in which I need to process some packets and send out to more than one interface/network.</p>
<p>My solution is to send the packet to a loopback/dummy interface and based on the Linux routing table packet shall be forwarded to corresponding interface/network.</p>
<p>Problem: packets from the application are sent to loopback/dummy interface but its not forwarded based on routing table.</p>
<p><strong>Is there any way to forward a packet from loopback/dummy interface based on routing table.?</strong></p>
<p><em>I have tried accept the packet in FORWARD chain in iptables, but packet was not coming here. I tired enabling /ipv4/ip_forwarding, this also didnt help.</em></p>
<p><strong>edit</strong>
My device is an intermediate device that collect packet from One interface (WAN) and manipulates each packet and forward it to proper destination (different LAN). Now my problem is I don't know which interface to send as there are n number of out interface. I need to direct the packet based on the ip.dst in the incoming packet.</p>
","<iptables><routing><linux-networking><forwarding><loopback>","2020-11-24 12:51:20"
"937173","Mails from PHP mail() are not received through Microsoft Exchange Online server","<p>a friend of mine is running a contact form on a website which sends the e-mails through the PHP mail() function (CMS used is Contao). The e-mails are sent without a problem to other recipients, so neither the CMS itself or the server responsible for sending are the problem. However, the two e-mails that these messages are actually supposed to be sent to, are run through Exchange Online (Office 365). It seems like there is some kind of protection going on that blocks these e-mails. Every other e-mail from actual mail clients goes through.
I couldn't find any setting in the Exchange AC regarding to this.</p>

<p>Here are the email headers from the contact form messages (sent a message from the contact form to another e-mail, where, again, it is received just fine):</p>

<pre><code>Return-Path: &lt;info@rechtsanwaelte-brauer.de&gt;
Delivered-To: hello@max-krause.com
Received: from premium31.web-hosting.com
    by premium31.web-hosting.com with LMTP id UBwiOpOw0VsKMAwAmYe65g
    for &lt;hello@max-krause.com&gt;; Thu, 25 Oct 2018 08:01:23 -0400
Return-path: &lt;info@rechtsanwaelte-brauer.de&gt;
Envelope-to: hello@max-krause.com
Delivery-date: Thu, 25 Oct 2018 08:01:23 -0400
Received: from u230.lrnc.net ([77.232.241.23]:47206)
    by premium31.web-hosting.com with esmtp (Exim 4.91)
    (envelope-from &lt;info@rechtsanwaelte-brauer.de&gt;)
    id 1gFeKJ-003PuQ-PW
    for hello@max-krause.com; Thu, 25 Oct 2018 08:01:23 -0400
Received: by u230.lrnc.net (Postfix, from userid 10020)
    id 15D73C46B8; Thu, 25 Oct 2018 14:01:03 +0200 (CEST)
To: nicole.darmstadt@gmail.com, info@rechtsanwaelte-brauer.de, nicole.brauer@rechtsanwaelte-brauer.de, webmaster@up-hill.de, cj@junglas.com, hello@max-krause.com
Subject: Nachricht =?utf-8?Q?=C3=BCber?= die Webseite
Message-ID: &lt;df7341d6b1d12752e351cd238854774a@www.brauer-rechtsanwaeltin.de&gt;
Date: Thu, 25 Oct 2018 12:01:02 +0000
From: info@rechtsanwaelte-brauer.de
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable
X-Priority: 3 (Normal)
X-Mailer: Contao Open Source CMS
</code></pre>

<p>Could anyone point me to where the problem might be?</p>
","<exchange><microsoft-office-365>","2018-10-25 12:37:42"
"870666","couldn't communicate with windows server 2016 from outside the network, but inside everything is ok","<p>I have some virtual machines on the esxi, such as windows server 2016 and one windows 7 ultimate.<br>
On the windows server, I have one active directory and it runs dhcp on it so it has a static ip address which I set.<br>
windows 7 ultimate gets its ip address from the dhcp from windows server 2016.<br>
Locally everything is ok and I can ping or create remote desktop connection to every virtual machines or esxi and etc.<br>
But outside of the network I can't do this stuff with windows server 2016.</p>

<p>I have one router between the 2 networks. From the laptop computer in the network 2 I can't see windows server 2016. But I can see windows 7.
Even I can see esxi.</p>

<p>I can ping from windows server 2016, my laptop computer in the network 2 and it is ok, but reverse communication can't happen.</p>

<ol>
<li><p>I can ping from virtual windows 7 my virtual windows server.</p></li>
<li><p>I can ping from local client every virtual machines.</p></li>
<li><p>I can ping from every virtual or other machines in the network 1, the machines on the network 2.</p></li>
<li><p>I can ping from network 2 the windows 7 or other machines on the network 1 except windows servers.</p></li>
</ol>

<p>I want to be able to ping and create remote desktop communication to my windows servers in the network1 from network2 and I can't.</p>

<p>Any help to solve this issue would be appreciated.</p>
","<vmware-esxi><router><switch><vmware-vsphere><windows-server-2016>","2017-08-27 07:47:26"
"1010966","ALB writing to 2 postgre sql instances","<p>I have nginx and postgre sql running on the same ec2 instance.</p>

<p>It serves a rails app that reads and writes data on that db.</p>

<p>I would like to have 2 instances behind an ALB service to distribute load and ensure availability.</p>

<p>What should I do to ensure data is consistent across the 2 instances?</p>
","<amazon-ec2><postgresql><cluster><amazon-alb>","2020-04-05 04:51:18"
"796971","IIS 6 ASP encoding setting","<p>I migrated an ASP site from a Win2008R2 server running IIS 6.1 to a Win2012 server running IIS 6.1.  The IIS site settings did not migrate so easily.</p>

<p>One thing I just can't solve: When I send Arabic characters through ASP and into an MS SQL sproc from the old server they arrive as UTF-8.  When I do it on the new server &ndash; into <em>the same MS SQL 2008 server</em> &ndash; the Arabic arrives as a series of '?' with an occasional '_' either <em>during or after</em> being appended as a parameter to a Server ""ADODB.Command"" object.  (I can debug the code and see that the original Arabic representation is still in the variable that's handed to the sproc.)</p>

<p>If you know what setting is probably affecting this and where to find it that would be wonderful.</p>

<p>However, if you can at least tell me how I could export the complete <em>effective</em> IIS settings on each deployment so I can find the differences and apply them to the new instance that would be very helpful too!</p>
","<iis-6><encoding><asp-classic>","2016-08-15 20:53:23"
"796972","Difference between two rules: iptables","<p>What is the difference between these two rules?</p>

<pre><code>iptables -A INPUT -p tcp --dport 80 -j DROP    
iptables -A OUTPUT -p tcp --dport 80 -j DROP
</code></pre>

<p>If I want to block access to websites that uses HTTP, should I use the second one?</p>
","<iptables>","2016-08-15 21:00:16"
"796973","Can I give detailed status information to the Elastic Beanstalk load balancer?","<p>I am using a health-check URL to allow Elastic Beanstalk to poll the health of my instances. I would like traffic to be distributed according to some internal state of each instance. Currently I just return status 200. </p>

<p>Can I send detailed health information, e.g. a ""health score"" that the load-balancer will use to direct users more intelligently?</p>
","<amazon-web-services><load-balancing><elastic-beanstalk>","2016-08-15 21:00:33"
"796990","How to set up a network connection with 2 ISP,s 2 Routers and 1 switch","<p>I want to use 2 ISP providers at the same time, to avoid downtime. Here's a diagram of what I want to do:</p>

<p><a href=""https://i.sstatic.net/6Tmar.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6Tmar.jpg"" alt=""Network Setup Diagram""></a></p>

<p>What settings should I use for the routers and can I force PCs to use ISP#1 when online and ISP#2 when 1 is down? </p>
","<router><switch><isp>","2016-08-15 22:20:44"
"937256","SSL ERROR. WWW domain name not working in my server error browser: ERR_NAME_NOT_RESOLVED","<p>Edit:
The problem was that I set only one SSL for the domain, but not one for the www.domain, i solved this by setting both in the same prompt. After that both worked. </p>

<p>Depends on what kind if SSL you using the proceed will be diferent. </p>

<hr>

<p>having this issue where my domain will only conect to the https or http, but if you type it directly or the www it will not connect to the dns servers and give error:</p>

<blockquote>
  <p>ERR_NAME_NOT_RESOLVED</p>
</blockquote>

<p>Must be something simple that im missing</p>

<p>EDIT1:</p>

<p>I was able to get it to access just with name so far. (dont know what I did maybe servers fault...)</p>

<p>As for the www, I tried setting a www name pointing to my server from the domain service, thats all I could come up with, this after getting A type record on my own system pointing to WWW and my domain. so far did not solve the problem.</p>

<p>EDIT2
here is what i did by my side:</p>

<blockquote>
  <p>www.   A   Default     myip</p>
  
  <p>www.   A   Default     domainip</p>
  
  <p>www.mysite.com.    PTR     Default     mysite.com.</p>
  
  <p>mysite.com.    A   Default     188.79.168.123</p>
  
  <p>www.mysite.com.    A   Default     188.79.168.123</p>
</blockquote>

<p>Edit3: I can now sucessfully ping it back to my server, now i may have to wait for the adress to propagate.</p>

<p>What i did is to add the full www. Adress to the dns list on the domain provider, I can open the website now from 4g, but people for example in the US may not be able (im in europe).</p>

<p>Also seems like the problem has to do with how i setup my ssl certificates, seems like i have 2 certificates one for the domain and other for the www. adress, causing some error.</p>

<p>As mentioned by answer i ran this using my phone on 4g:</p>

<blockquote>
  <p>[~]: host mysute</p>
  
  <p>www.mysite.com has address 188.79.168.123</p>
</blockquote>

<p>Seems solved. </p>
","<domain><https>","2018-10-25 20:15:13"
"797087","Configuration Managment Best Practice: List of Packages to install","<p>Package names do differ on different platforms. Some call it <code>httpd</code> some <code>apache2</code>...</p>

<p>Imagine your product supports RedHat, SuSE and Ubuntu. Each in two versions. </p>

<p>Do you keep six lists of packages to install in your configuration management?</p>

<p>AFAIK this question is valid for all common products like Salt, Puppet, Chef, Ansible</p>

<h1>Update</h1>

<p>This question got a lot of down-votes. For me this means: This question is known and a lot of people hate the situation. And: No one has an answer.</p>

<p>I found this: There is a project on github which tries to bring sanity into this package naming chaos:</p>

<p><a href=""https://github.com/unixpackage/unixpackage"" rel=""nofollow noreferrer"">https://github.com/unixpackage/unixpackage</a></p>

<blockquote>
  <p>One command to install equivalent packages in Ubuntu, Debian, CentOS, Fedora, Red Hat and Mac OS X.
  UnixPackage is a UNIX independent way of installing packages. Specify the Ubuntu package name (e.g. libpq-dev), and it will install the equivalent on your system (e.g. postgresql-libs on Arch).</p>
</blockquote>
","<puppet><chef><ansible><saltstack>","2016-08-16 10:20:44"
"1043886","Using wildcard does not resolve sub of subdomain","<p>I have a domain <code>xyz.com</code> on (<strong>xx.xxx.255.25</strong>) and <code>sub.xyz.com</code> on (<strong>xx.xxx.255.240</strong>)<br>
To resolve <code>sub</code> I added the following reocrd in BIND</p>
<blockquote>
<p><code>sub       IN     A      xx.xxx.255.240</code> <br><code>*.sub     IN     A      xx.xxx.255.240</code></p>
</blockquote>
<p><br>and worked well, but the problem is with the sub of subdomain it did not resolve.<br />
I tried adding a <code>test</code> sub and worked well:</p>
<blockquote>
<p><code>test.sub       IN     A      xx.xxx.255.240</code></p>
</blockquote>
<p>The point of the wildcard (*) is that I have multiple subs (<code>FTP, mail, SMTP...</code>)<br />
So where exactly I did wrong caused the sub of the subdomain to not resolve?
<br></p>
<H3>[UPDATE]</H3>
Resolving sub of subdomains worked as *@Tommiie* said, but the second problem appeared is with `TXT` records (SPF, DKIM, DMARC), Can I add wildacrd as `A records` to resolve them?
","<bind><subdomain><a-record><wildcard-subdomain>","2020-11-25 12:31:12"
"797165","MRTG - HTML buttons to index page","<p>My goal is to include HTML buttons in the Index page in order to classify
my MRTG page in different subcategories. Right now what I have done is to
make 6 empty graphs with a .png created by myself and it works fine. The
only problem is that now I want to add more pages and I would want to have
smaller buttons because I use 3 columns and they have the same size as a
graph.</p>

<p>I have also tried by modifying the index.html file but everytime I want to
add a graph I have to run the indexmaker so I run:</p>

<pre><code>/usr/bin/indexmaker --output=/var/www/mrtg/index.html --title='MRTG Test'
--columns=3 /etc/mrtg/cfg/mrtg.cfg
</code></pre>

<p>Does anyone know how to do this please?</p>

<p>Thank you very much.</p>
","<html><mrtg>","2016-08-16 15:43:36"
"797183","Spam emails for download?","<p>Is there a place where I could download as much spam emails as possible, having been sent from as much different worldwide locations as possible?</p>

<p>To clarify: I am <em>not</em> after databases which contain hashes of spam emails or which contain IP addresses of senders / networks which have sent spam. I am just interested in getting hold of as much real spam emails as possible in unaltered form.</p>

<p>For example, this could be a giant mbox file containing thousands or millions of such emails in their original state. Are there some anti-spam organizations or companies who provide access to such data?</p>

<p><strong>UPDATE:</strong></p>

<p>Not being a native English speaker, I just didn't know the right keywords. Just search for ""spam corpus"" or ""spam corpora"" on Google. Hoping that this helps others with the same problem and hoping that now there's no reason to downvote any more ...</p>
","<spam>","2016-08-16 17:46:29"
"797216","Who's right about when to renew Let's Encrypt certificates?","<p>I received the following, sans unsubscribe link:</p>

<blockquote>
  <p>Hello,</p>
  
  <p>Your certificate (or certificates) for the names listed below will expire in 1 days (on 14 Aug 16 23:07 +0000). Please make sure to renew your certificate before then, or visitors to your website will encounter errors.</p>
  
  <p>unixtalk.chat<br />
  www.unixtalk.chat</p>
  
  <p>For any questions or support, please visit <a href=""https://community.letsencrypt.org/"" rel=""nofollow noreferrer"">https://community.letsencrypt.org/</a>. Unfortunately, we can't provide support by email.</p>
  
  <p>If you are receiving this email in error, unsubscribe at <strong>[DELETED]</strong> (HTTP link, we know. We're working on it!)</p>
  
  <p>Regards,<br />
  The Let's Encrypt Team</p>
</blockquote>

<p>I don't remember exactly when I registered the certificate, but the domain was created 2016-02-10.</p>

<p>When I ran the following command:</p>

<pre><code>letsencrypt/letsencrypt-auto -d unixtalk.chat
</code></pre>

<p>it allowed me to create a certificate, but warned me that I was updating the certificate well before it needed to be renewed, and somewhere in there I think it advised me that such early certificate renewals would be throttled.</p>

<p>So...</p>

<p>When should I be renewing certificates obtained through Let's Encrypt?</p>

<p>Thanks,</p>
","<ssl><ssl-certificate><lets-encrypt><ssl-certificate-renewal>","2016-08-16 21:22:51"
"937528","Coreos: Trace etcd rejected connection source","<p>I have a coreos instance with etcd-member enabled. </p>

<p>In the logs, I received a bunch of requests with source ports increased by 2 each time. IMHO that looks like a program that is checking to find a valid source address to be accepted. </p>

<p>How can I trace down where they came from and what service that is? This is my iptables config, so I'm assuming its something local:</p>

<pre><code>-P INPUT DROP
-P FORWARD DROP
-P OUTPUT ACCEPT
-A INPUT -i lo -j ACCEPT
-A INPUT -i eth1 -j ACCEPT
-A INPUT -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A INPUT -p tcp -m tcp --dport 22 -j ACCEPT
-A INPUT -p tcp -m tcp --dport 80 -j ACCEPT
-A INPUT -p tcp -m tcp --dport 443 -j ACCEPT
-A INPUT -p icmp -m icmp --icmp-type 0 -j ACCEPT
-A INPUT -p icmp -m icmp --icmp-type 3 -j ACCEPT
-A INPUT -p icmp -m icmp --icmp-type 11 -j ACCEPT
</code></pre>

<p>This is the output of the etcd journal:</p>

<pre><code>Oct 27 14:40:02 &lt;HOSTNAME&gt; etcd-wrapper[924]: 2018-10-27 14:40:02.810784 I | embed: rejected connection from ""127.0.0.1:40162"" (error ""remote error: tls: bad certificate"", ServerName """")
Oct 27 14:40:02 &lt;HOSTNAME&gt; etcd-wrapper[924]: 2018-10-27 14:40:02.808002 I | embed: rejected connection from ""127.0.0.1:40158"" (error ""tls: first record does not look like a TLS handshake"", ServerName """")
Oct 27 14:39:58 &lt;HOSTNAME&gt; etcd-wrapper[924]: 2018-10-27 14:39:58.734359 I | embed: rejected connection from ""127.0.0.1:40156"" (error ""tls: first record does not look like a TLS handshake"", ServerName """")
Oct 27 14:39:58 &lt;HOSTNAME&gt; etcd-wrapper[924]: 2018-10-27 14:39:58.734101 I | embed: rejected connection from ""127.0.0.1:40152"" (error ""tls: first record does not look like a TLS handshake"", ServerName """")
Oct 27 14:39:53 &lt;HOSTNAME&gt; etcd-wrapper[924]: 2018-10-27 14:39:53.727212 I | embed: rejected connection from ""127.0.0.1:40148"" (error ""tls: first record does not look like a TLS handshake"", ServerName """")
Oct 27 14:39:53 &lt;HOSTNAME&gt; etcd-wrapper[924]: 2018-10-27 14:39:53.726941 I | embed: rejected connection from ""127.0.0.1:40144"" (error ""tls: first record does not look like a TLS handshake"", ServerName """")
Oct 27 14:39:53 &lt;HOSTNAME&gt; etcd-wrapper[924]: 2018-10-27 14:39:53.682223 I | embed: rejected connection from ""127.0.0.1:40138"" (error ""tls: first record does not look like a TLS handshake"", ServerName """")
Oct 27 14:39:53 &lt;HOSTNAME&gt; etcd-wrapper[924]: 2018-10-27 14:39:53.681992 I | embed: rejected connection from ""127.0.0.1:40136"" (error ""remote error: tls: bad certificate"", ServerName """")
Oct 27 14:39:48 &lt;HOSTNAME&gt; etcd-wrapper[924]: 2018-10-27 14:39:48.719532 I | embed: rejected connection from ""127.0.0.1:40132"" (error ""tls: first record does not look like a TLS handshake"", ServerName """")
Oct 27 14:39:48 &lt;HOSTNAME&gt; etcd-wrapper[924]: 2018-10-27 14:39:48.719305 I | embed: rejected connection from ""127.0.0.1:40128"" (error ""tls: first record does not look like a TLS handshake"", ServerName """")
Oct 27 14:39:48 &lt;HOSTNAME&gt; etcd-wrapper[924]: 2018-10-27 14:39:48.602150 I | embed: rejected connection from ""127.0.0.1:40124"" (error ""tls: first record does not look like a TLS handshake"", ServerName """")
</code></pre>
","<ssl>","2018-10-27 17:16:47"
"937562","MYSQL is consuming 100% of CPU usage","<p>Been monitoring <code>WHM &gt; Process Manager</code> for a while now, and a particular process from MYSQL keep eating up CPU Usage playing from 30%-100% from time to time</p>

<pre><code>PID : 3618
owner : mysql
CPU% : 100.60
Memory : 17.01

/usr/sbin/mysqld --basedir=/usr --datadir=/var/lib/mysql --plugin-dir=/usr/lib64/mysql/plugin --log-error=xxx.xxxxxxxxxx.com.err --open-files-limit=50000 --pid-file=xxx.xxxxxxxxxx.com.pid --socket=/var/lib/mysql/mysql.sock
</code></pre>

<p>Anyone experience the same issue and can share solution please? </p>
","<mysql><central-processing-unit><usage>","2018-10-28 08:11:52"
"937675","Windows Server 2008 R2 takes too long to start in esxi 5.5 after converting the Physical Machine into Virtual machine with P2V Software","<p>I converted the Windows Server 2008 R2 which is installed on the Physical Server into virtual machine with P2V software. I installed this Virtual machine in Vmware ESXI 5.5. This guest OS takes too long to Start (before login scree appearance) in Virtual machine. I am using Hardware of HP G6 Server.<br>
<strong>Resources Allocated to VM</strong><br>
CPU core =1<br>
Ram  =4GB<br>
Virtulization Software = ESXI 5.5<br>
Guest OS = Windows Server 2008 R2</p>

<p><strong>Steps taken for remedy</strong><br>
1) update my firmware.<br>
2) remove unavailable hardware drivers from device manager.  </p>

<p>Please find below images for resource utilization of VM.<br>
<a href=""https://i.sstatic.net/loYWp.jpg"" rel=""nofollow noreferrer"">DataStore Utitlization</a><br>
<a href=""https://i.sstatic.net/QBYUA.jpg"" rel=""nofollow noreferrer"">CPU Utilization</a><br>
<a href=""https://i.sstatic.net/RHmHQ.jpg"" rel=""nofollow noreferrer"">Disk Utilization</a><br>
<a href=""https://i.sstatic.net/LRdh9.jpg"" rel=""nofollow noreferrer"">Ram Utilization</a>  </p>

<p>Please find below link for log file of VMware.<br>
<a href=""https://drive.google.com/open?id=1tWarmJyIocenJ0fdieE2slrY75WZgI1e"" rel=""nofollow noreferrer"">Log File of VMWare</a>  </p>

<p>Please help to resolve the issue.  </p>
","<windows-server-2008-r2><vmware-esxi>","2018-10-29 11:59:35"
"1011717","IIS 8.5 two domain with two ssl issues","<p>I have windows server 2012 r2 with IIS 8.5.  and i am trying configure two domain with two different ssl.
my one domain is perfectly working in https. Problem is  but when i browse another domain it gives me following error..</p>

<p>Websites prove their identity via certificates. Firefox does not trust this site because it uses a certificate that is not valid for (domain2). The certificate is only valid for the following names: (domain1).</p>

<p>while i check Server Name Indicator in configuration.. I don't understand what is real issue.<br>
do i need to configure other thing?</p>
","<windows><ssl><iis><domain><website>","2020-04-10 08:59:36"
"797713","Is it safe to define the same server resources for two upstream pools?","<p>My concern is that with one upstream pool being shared between two servers, each server will send requests to the pool and if the pool becomes overloaded, neither server will be able to properly throttle or timeout future requests.</p>

<p>I've defined two servers, one listening to port 80, the other to port 443.  Both define an upstream pool with the same resources.  </p>

<p>web_one_80.conf:</p>

<pre><code>  upstream app_pool1 {
    server 1.1.1.1:5000
    server 1.1.1.1:5001
    server 1.1.1.1:5002
  }

  http {
    listen 80;
    location / {
      proxy_pass http://app_pool1;
    }
  }
</code></pre>

<p>web_one_443.conf:</p>

<pre><code>  upstream app_pool2 {
    server 1.1.1.1:5000
    server 1.1.1.1:5001
    server 1.1.1.1:5002
  }

  http {
    listen 443 ssl;
    location / {
      proxy_pass http://app_pool2;
    }
  }
</code></pre>

<p>What if I have two servers on two different machines sharing an upstream server?</p>

<p>web_one_80.conf:</p>

<pre><code>  upstream app_pool1 {
    server 1.1.1.1:5000
    server 1.1.1.1:5001
    server 1.1.1.1:5002
  }

  http {
    listen 80;
    location / {
      proxy_pass http://app_pool1;
    }
  }
</code></pre>

<p>web_two_80.conf:</p>

<pre><code>  upstream app_pool1 {
    server 1.1.1.1:5000
    server 2.2.2.2:5001
    server 2.2.2.2:5002
  }

  http {
    listen 80;
    location / {
      proxy_pass http://app_pool1;
    }
  }
</code></pre>

<p><strong>Full disclosure:</strong> I'm checking on another's work.  I would expect that neither of these setups should work properly. I'm happy being wrong!</p>
","<nginx>","2016-08-18 17:29:08"
"938076","setting up virtual hosts on apache","<p>I have a Apache server running under WEBMIN.</p>

<p>I have this virtual Server:</p>

<pre><code>&lt;VirtualHost pmaconfig.trekfederation.com&gt;
DocumentRoot /var/www/phpmyadmin
ServerName pmaconfig.trekfederation.com
&lt;/VirtualHost&gt;
</code></pre>

<h2>UPDATE 1...changed definition</h2>

<pre><code>&lt;VirtualHost pmaconfig.trekfederation.com:80&gt;
DocumentRoot /var/www/phpmyadmin
ServerName pmaconfig.trekfederation.com
&lt;/VirtualHost&gt;
</code></pre>

<p>What I'm expecting:  page render under /var/www/phpmyadmin when given url matching the server name.</p>

<p>What I'm seeing: directory listing of /var/www/html.</p>

<p>I do not see this directory under apache2.conf (SEE BELOW).  There are no other virtual servers.  What am I missing?</p>

<p>Thanks</p>

<h2>CONTENTS OF APACHE2.conf</h2>

<pre><code># Global configuration
#

#
# ServerRoot: The top of the directory tree under which the server's
# configuration, error, and log files are kept.
#
# NOTE!  If you intend to place this on an NFS (or otherwise network)
# mounted filesystem then please read the Mutex documentation (available
# at &lt;URL:http://httpd.apache.org/docs/2.4/mod/core.html#mutex&gt;);
# you will save yourself a lot of trouble.
#
# Do NOT add a slash at the end of the directory path.
#
#ServerRoot ""/etc/apache2""

#
# The accept serialization lock file MUST BE STORED ON A LOCAL DISK.
#
#Mutex file:${APACHE_LOCK_DIR} default

#
# The directory where shm and other runtime files will be stored.
#

DefaultRuntimeDir ${APACHE_RUN_DIR}

#
# PidFile: The file in which the server should record its process
# identification number when it starts.
# This needs to be set in /etc/apache2/envvars
#
PidFile ${APACHE_PID_FILE}

#
# Timeout: The number of seconds before receives and sends time out.
#
Timeout 300

#
# KeepAlive: Whether or not to allow persistent connections (more than
# one request per connection). Set to ""Off"" to deactivate.
#
KeepAlive On

#
# MaxKeepAliveRequests: The maximum number of requests to allow
# during a persistent connection. Set to 0 to allow an unlimited amount.
# We recommend you leave this number high, for maximum performance.
#
MaxKeepAliveRequests 100

#
# KeepAliveTimeout: Number of seconds to wait for the next request from the
# same client on the same connection.
#
KeepAliveTimeout 5


# These need to be set in /etc/apache2/envvars
User ${APACHE_RUN_USER}
Group ${APACHE_RUN_GROUP}

#
# HostnameLookups: Log the names of clients or just their IP addresses
# e.g., www.apache.org (on) or 204.62.129.132 (off).
# The default is off because it'd be overall better for the net if people
# had to knowingly turn this feature on, since enabling it means that
# each client request will result in AT LEAST one lookup request to the
# nameserver.
#
HostnameLookups Off

# ErrorLog: The location of the error log file.
# If you do not specify an ErrorLog directive within a &lt;VirtualHost&gt;
# container, error messages relating to that virtual host will be
# logged here.  If you *do* define an error logfile for a &lt;VirtualHost&gt;
# container, that host's errors will be logged there and not here.
#
ErrorLog ${APACHE_LOG_DIR}/error.log

#
# LogLevel: Control the severity of messages logged to the error_log.
# Available values: trace8, ..., trace1, debug, info, notice, warn,
# error, crit, alert, emerg.
# It is also possible to configure the log level for particular modules, e.g.
# ""LogLevel info ssl:warn""
#
LogLevel warn

# Include module configuration:
IncludeOptional mods-enabled/*.load
IncludeOptional mods-enabled/*.conf

# Include list of ports to listen on
Include ports.conf


# Sets the default security model of the Apache2 HTTPD server. It does
# not allow access to the root filesystem outside of /usr/share and /var/www.
# The former is used by web applications packaged in Debian,
# the latter may be used for local directories served by the web server. If
# your system is serving content from a sub-directory in /srv you must allow
# access here, or in any related virtual host.
&lt;Directory /&gt;
    Options FollowSymLinks
    AllowOverride None
    Require all denied
&lt;/Directory&gt;

&lt;Directory /usr/share&gt;
    AllowOverride None
    Require all granted
&lt;/Directory&gt;

&lt;Directory /var/www/&gt;
    Options Indexes FollowSymLinks
    AllowOverride None
    Require all granted
&lt;/Directory&gt;

#&lt;Directory /srv/&gt;
#   Options Indexes FollowSymLinks
#   AllowOverride None
#   Require all granted
#&lt;/Directory&gt;




# AccessFileName: The name of the file to look for in each directory
# for additional configuration directives.  See also the AllowOverride
# directive.
#
AccessFileName .htaccess

#
# The following lines prevent .htaccess and .htpasswd files from being
# viewed by Web clients.
#
&lt;FilesMatch ""^\.ht""&gt;
    Require all denied
&lt;/FilesMatch&gt;


#
# The following directives define some format nicknames for use with
# a CustomLog directive.
#
# These deviate from the Common Log Format definitions in that they use %O
# (the actual bytes sent including headers) instead of %b (the size of the
# requested file), because the latter makes it impossible to detect partial
# requests.
#
# Note that the use of %{X-Forwarded-For}i instead of %h is not recommended.
# Use mod_remoteip instead.
#
LogFormat ""%v:%p %h %l %u %t \""%r\"" %&gt;s %O \""%{Referer}i\"" \""%{User-Agent}i\"""" vhost_combined
LogFormat ""%h %l %u %t \""%r\"" %&gt;s %O \""%{Referer}i\"" \""%{User-Agent}i\"""" combined
LogFormat ""%h %l %u %t \""%r\"" %&gt;s %O"" common
LogFormat ""%{Referer}i -&gt; %U"" referer
LogFormat ""%{User-agent}i"" agent

# Include of directories ignores editors' and dpkg's backup files,
# see README.Debian for details.

# Include generic snippets of statements
IncludeOptional conf-enabled/*.conf

# Include the virtual host configurations:
IncludeOptional sites-enabled/*.conf

# vim: syntax=apache ts=4 sw=4 sts=4 sr noet
</code></pre>
","<virtualhost><apache2><webmin>","2018-10-31 16:24:21"
"797829","How to make a symlink for MySQL?","<p>Sorry guys for the silly question but I am really need of help.</p>

<p>Running <code>df -h</code> I get</p>

<pre><code>/dev/vda1        40G   38G     0 100% /
/dev/sda         99G   60M   94G   1% /mnt/volume-fra1-01
</code></pre>

<p>I tried to create a symlink between the heaviest folder of my server to the partition with more space.</p>

<pre><code>ln -s /var/www/folder1/folder2/folder3.it/ /mnt/volume-fra1-01
</code></pre>

<p>I have rebooted the server and now when I try to start mysql, this is what I get:</p>

<pre><code> * /etc/init.d/mysql: ERROR: The partition with /var/lib/mysql is too full!
</code></pre>

<p>How do I give mysql more space from the partition with 100GB?</p>

<p>Please help!!</p>
","<symlink>","2016-08-19 05:02:37"
"1011891","Apache redirect with HSTS","<p>I need to redirect all for <code>example.com</code> to <code>https://www.example.com</code> with HSTS turned on. How to do it? I got configuration in vhost80 and vhost443. What to fix in this code: </p>

<p>in vhost 80</p>

<pre><code>RewriteCond %{SERVER_NAME} =www.example.com [OR]
RewriteCond %{SERVER_NAME} =*.example.com [OR]
RewriteCond %{SERVER_NAME} =example.com
RewriteRule ^ https://%{SERVER_NAME}%{REQUEST_URI} [END,NE,R=permanent]
</code></pre>

<p>in vhost 443</p>

<pre><code>RewriteCond %{HTTP_HOST} ^example.com$ [NC]
RewriteRule (.*) https://www.example.com$1 [R=301,L]
</code></pre>
","<apache-2.4><virtualhost><redirect><hsts>","2020-04-11 15:01:34"
"797895","Managed switch in 2nd building connected to main switch stack by fiber won't use any VLANs?","<p>So I have a switch stack and a fiber that comes from it goes to another building on site. On the other end of that fiber is a netgear switch then plugged into it is several desktops and then a Wireless Access Point. I want to Tag the port the Wireless Access point is setup on so my DHCP can start handing out IPs.</p>

<p>I have a Wireless Access Point currently plugged directly into the switch stack for the main building and then in the interface I setup 2 VLANs 5 and 6. Then on my router I setup the VLANs and I am routing them to 2 different IP addresses. My DHCP server on my Windows Server 2008 is handing out IPs just fine in the main building. But I want it to do the same to the building connected to us by fiber. Should I untag the fiber port through the switch stack interface or should I tag it? I want my desktop PCs over in the other building to stay on the main network.</p>

<p>FYI: The VLANS work great in the main building that has the switch stack.</p>

<p>EDIT: </p>

<p>I created 2 wireless networks that use VLAN ID 5 and 6 which is configured on a netgear switch. Barracuda Firewall routes 2 different DHCP scopes from my DHCP server on Windows Server 2008 through the 2 VLANs. When you connect to Wireless Network A which is assigned VLAN ID 5 you get IP address 192.168.2.x and if you connect to Wireless Network B you get IP Address 192.168.3.x.</p>

<p>Currently that works as configured in the main building. The port the WAP is connected to is tagged in VLAN ID 5 and 6. I plugged in another WAP into a managed switch connected by fiber that the VLANs 5 and 6 were configured on originally. The WAP automatically started broadcasting the 2 Wireless Networks but IP addresses aren't being handed out like they are in the main building.</p>

<p>On the managed switch in the 2nd building I have created VLANs 5 and 6 in that switch as well and I tagged the port of the WAP in VLAN 5 and 6 as well as the fiber port that connects the main building.</p>
","<networking>","2016-08-19 13:03:51"
"797899","Docker renaming network interfaces","<p>I am trying to rename the network interfaces inside a docker container.</p>

<p>Right now they are <code>eth0</code>, <code>eth1</code>, <code>eth2</code>, etc. I would like them to have different names. I know how to do it in Ubuntu with modifying the <code>70-persistent-rules</code> file, but that file seems to be missing.</p>

<p>I had to use docker commands to change the ip addresses, so I was wondering if there was a docker command for this as well. The docker container I am using is an ubuntu 14.04</p>

<p>Thanks in advance!</p>
","<networking><docker><ip-address>","2016-08-19 13:11:15"
"938193","steps to troubleshoot server when it fails to attach and mount LUNs","<p>How to troubleshoot when Windows Server 2012 R2 connected to SAN via iSCSI fails to attach and mount  the LUNs from the iSCSI target</p>
","<windows-server-2012-r2><storage-area-network><iscsi>","2018-11-01 04:54:30"
"798005","windows executable and extract data from file","<p>On my CD-ROM I have this file: <code>ip.txt</code> which has the content:</p>

<p><code>ip=""192.168.1.1"" netmask=""255.255.255.0"" gateway=""123.456.789.2""</code></p>

<p>How can I extract from <code>ip.txt</code> the IP, netmask and gw and use as command to setup the IP.</p>

<p>I know the command works like this: <code>netsh interface ip set address name=""Local Area Connection"" static 192.168.0.100 255.255.255.0 192.168.0.1 1</code></p>

<p>But I do not know how to make a batch file to extract the info I need from the <code>ip.txt</code> file, do you know how that may be done?</p>

<p>Thanks.</p>
","<windows><networking><ip><scripting><command>","2016-08-19 22:12:42"
"871763",".htaccess vs php.ini in mod_php and CGI","<p>As far as I know, <em>mod_php</em> can be configured either by means of <em>php.ini</em> or <em>.htaccess</em> files while, for CGI, <em>.htaccess</em> files cannot be used.</p>

<p>Does the fact that <em>mod_php</em> can use <em>.htaccess</em> files makes it more insecure with respect to CGI?</p>
","<apache-2.4><cgi><mod-php>","2017-09-02 09:02:31"
"798112","Active Directory name and external mail","<p>We have been shifted to a new office environment, where everything is setup properly. We are using Windows Server 2012 R2, and I am facing one issue with it.</p>

<p>My domain control for the internal name is <code>example.com</code>, and my email and website use the same domain name, <code>example.com</code>. For the website, this works when using <code>www</code>, but for the email in Outlook, when connecting to the domain, the mail is getting stopped. When I turned off the DNS, email worked fine. Please help me with this problem.</p>
","<windows><domain-name-system><active-directory><email><windows-server-2012-r2>","2016-08-20 18:01:42"
"871856","Two switches one router","<p>If i have one router connected to two switches separately (as shown in diagram), will a device on switch 1 be able to communicate with a device on switch 2. </p>

<p><img src=""https://i.sstatic.net/LThJV.png"" alt=""Network Diagram""></p>

<p>I would just like to make sure before i buy the switches.(any recommendations?)
Thanks</p>
","<switch>","2017-09-03 16:37:49"
"798122","DNS or something else? I can't access my website when I add www. infront of it","<p>I run a fresh Linux Mint 18 install, Apache2 web server hosted by me. I can access the website by IP, <a href=""http://domain.tld"" rel=""nofollow noreferrer"">http://domain.tld</a> however when I try <a href=""http://www.domain.tld"" rel=""nofollow noreferrer"">http://www.domain.tld</a> I get no response. My single and only DNS configuration is [Type] A record [Host] @ [Value] X.X.X.X [TTL] Auto.</p>

<p>I never modified any file or config after installing Apache2. Did I miss something? What am I doing wrong?</p>

<p>I've been looking for 2 days all around the net and I can't find out what is it.</p>
","<domain-name-system><web>","2016-08-20 21:53:20"
"798175","Ubuntu 16.04 with software RAID 1 questions","<p>I've purchased a new dedicated server with 2 x 240GB SSD drive, and installed Ubuntu 16.04 image from the provider's control panel which by default creates the software RAID 1.</p>

<p>I also installed Webmin on this server and after looking at the Webmin's system info screen I understand that I'm only getting 204GB free disk space to use.</p>

<p><img src=""https://i.sstatic.net/y2l7P.png"" alt=""Webmin Screenshot""></p>

<p>After reading through few articles, I understand that RAID 1 mirrors Disk 1 with Disk 2 for better data security  and faster disk performance.</p>

<p>Now my questions are:</p>

<ol>
<li>Is it RAID 1 which consumes half of my disk space (entire Disk 2)?</li>
<li>I regularly backup my data on Amazon S3, do I still need RAID 1?</li>
<li>Why not just go with RAID 0, will it make any performance degradation?</li>
</ol>
","<software-raid><raid0><ubuntu-16.04>","2016-08-21 12:33:53"
"938460","What happens if a USB Ethernet Adapter has the same MAC address as the PC's network card it is connected to?","<p>What happens if a USB Ethernet Adapter has the same MAC address as the NIC?</p>

<p>Is it possible for internet connection to pass through the usb-ethernet adapter succesfully or not, in that scenario?</p>
","<networking><local-area-network><ethernet><usb><mac-address>","2018-11-03 06:35:16"
"1012302","Workgroup Server 2019 - RD Licensing - No licenses issued, 60 minute client timeout message","<p>I have 2 machines with one (Server 2019 Standard) acting as a licensing server for my non-domain workgroup and the other a 2016 Standard machine. I configured the licensing server on the same day for both machines.</p>

<p>I installed the remote desktop licensing service on 18-Mar-2020 and installed 50 user and 50 device licenses. original install date for the machine running the licensing service is Feb-2019</p>

<p>I was then able to connect with 10+ clients on either machine simultaneously.</p>

<p>Today, 27 days later, connecting clients on the machine which hosts the licensing server receive:
""there is a problem with your remote desktop license and your session will be disconnected in 60 min""</p>

<p>This error does not appear when connecting to the other machine.</p>

<p>I have 7 sessions from a single laptop between both machines. The RD Licensing Manager shows ""issued"" as 0 for both device and user CALs.</p>

<p>I read that workgroup machines don't allow user CALs, so I converted mine to device CALs. Still 0 issued.</p>

<p>I verified that the local gp on either machine was configured (comp config > admin templates > rds > rd licensing > remote desktop session host > licensing > </p>

<p>1) use specified server is configured on both servers 
2) remote desktop licensing mode was user, now set to device</p>

<p>used gpupdate /force when making changes, but the message about the 60m timeout persists on connection</p>

<p>RD licensing diagnoser ""did not identify any problems to report""</p>

<p>i rebuilt the licensing database and re-applied the per-device licenses with no effect.</p>

<p>tried deleting MSLicensing registry key with no effect.</p>

<p>there are certificate entries in:</p>

<p>Computer\HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\Terminal Server\RCM</p>

<p>...but I have not touched them</p>

<p>edit: I noticed that the license diagnoser on the 2019 machine is showing the version of the license server configuration is ""Windows Server 2016"" even though it is running on Windows Server 2019.</p>

<p>I also installed device licenses for 2016 as well, but this did not help.</p>
","<remote-desktop><remote-desktop-services><windows-server-2019>","2020-04-14 18:21:34"
"872036","Mapping Amazon AWS EC2 AMI images to pricing","<p>Amazon AWS EC2 instances give high level pricing for Linux, RHEL, Windows etc. : <a href=""https://aws.amazon.com/ec2/pricing/on-demand/"" rel=""nofollow noreferrer"">https://aws.amazon.com/ec2/pricing/on-demand/</a>  </p>

<p>When launching an instance you have a choice between many AMI images of different OSes / stacks. How do you know which pricing from the page above maps to which AMI image? I assume RHEL image will map to RHEL price. Is it safe to assume any other flavour, Ubuntu e.g., will just be the core linux price?</p>
","<amazon-ec2><amazon-web-services>","2017-09-04 21:21:31"
"1045180","Debian 10.6 Tayga package missing in ISO default repository","<p>I'd like to implement NAT64 on my Debian Linux server running on non-internet connection. But it seems that the Tayga package is not in the local repository.</p>
<p>apt install tayga -y</p>
<p>Reading package lists... Done</p>
<p>Building dependency tree</p>
<p>Reading state information... Done</p>
<p>E: Unable to locate package tayga</p>
<p>Has the tayga package been replaced other package name or no longer supported on Debian 10.6 version?
I can't find any document regarding to tayga package of debian 10.6 version.</p>
","<linux><linux-networking><nat>","2020-12-06 06:38:35"
"1045191","Can't set up DNS server with BIND9","<p>I'm trying to set up my own DNS server to manage my domain name, but it isn't working. OS is Arch Linux.</p>
<p><a href=""https://paste.ubuntu.com/p/dMrBBTXd7P/"" rel=""nofollow noreferrer"">/etc/named.conf</a></p>
<p><a href=""https://paste.ubuntu.com/p/BD2zZmfVgw/"" rel=""nofollow noreferrer"">/var/named/aaa.a.bg.zone</a></p>
<p>output of <code>dig aaa.a.bg</code>:</p>
<pre><code>; &lt;&lt;&gt;&gt; DiG 9.16.8 &lt;&lt;&gt;&gt; aaa.a.bg
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: SERVFAIL, id: 11028
;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;aaa.a.bg.                      IN  A

;; Query time: 1049 msec
;; SERVER: 192.168.1.1#53(192.168.1.1)
;; WHEN: Sun Dec 06 11:03:46 EET 2020
;; MSG SIZE  rcvd: 37
</code></pre>
<p>EDIT: The problem was that port 53 wasn't opened on the router.</p>
","<domain-name-system><bind>","2020-12-06 09:07:28"
"798460","Debugging UFW and Samba on Ubuntu Server","<p>I have two Ubuntu servers with identical Samba configs and same UFW rules for Samba's ports but the difference is that one with 14.04 works just fine and the other with 12.04 denies access from everywhere.</p>

<p>When UFW is disabled on the 12.04 server Samba connections are allowed again. Though, when UFW is enabled there are no log entries from the blocks in UFW's logs.</p>

<p>How could I debug this problem?</p>

<p><strong>Edit:</strong> I found these <code>iptables</code> rules which are used for <code>smb</code> and <code>dhcp</code> ports:</p>

<pre><code>:ufw-skip-to-policy-input - [0:0]
-A ufw-after-input -p udp -m udp --dport 137 -j ufw-skip-to-policy-input
-A ufw-after-input -p udp -m udp --dport 138 -j ufw-skip-to-policy-input
-A ufw-after-input -p tcp -m tcp --dport 139 -j ufw-skip-to-policy-input
-A ufw-after-input -p tcp -m tcp --dport 445 -j ufw-skip-to-policy-input
-A ufw-after-input -p udp -m udp --dport 67 -j ufw-skip-to-policy-input
-A ufw-after-input -p udp -m udp --dport 68 -j ufw-skip-to-policy-input
-A ufw-after-input -m addrtype --dst-type BROADCAST -j ufw-skip-to-policy-input

-A ufw-skip-to-policy-input -j DROP
</code></pre>

<p>However, removing them from <code>/etc/ufw/after.rules</code> did nothing.</p>

<p>Here is the current <code>iptables-save</code> output:</p>

<pre><code>*filter
:INPUT DROP [0:0]
:FORWARD DROP [0:0]
:OUTPUT ACCEPT [0:0]
:fail2ban-apache - [0:0]
:fail2ban-apache-noscript - [0:0]
:fail2ban-apache-overflows - [0:0]
:fail2ban-dovecot - [0:0]
:fail2ban-postfix - [0:0]
:fail2ban-ssh - [0:0]
:fail2ban-ssh-ddos - [0:0]
:fail2ban-wuftpd - [0:0]
:ufw-after-forward - [0:0]
:ufw-after-input - [0:0]
:ufw-after-logging-forward - [0:0]
:ufw-after-logging-input - [0:0]
:ufw-after-logging-output - [0:0]
:ufw-after-output - [0:0]
:ufw-before-forward - [0:0]
:ufw-before-input - [0:0]
:ufw-before-logging-forward - [0:0]
:ufw-before-logging-input - [0:0]
:ufw-before-logging-output - [0:0]
:ufw-before-output - [0:0]
:ufw-logging-allow - [0:0]
:ufw-logging-deny - [0:0]
:ufw-not-local - [0:0]
:ufw-reject-forward - [0:0]
:ufw-reject-input - [0:0]
:ufw-reject-output - [0:0]
:ufw-skip-to-policy-forward - [0:0]
:ufw-skip-to-policy-input - [0:0]
:ufw-skip-to-policy-output - [0:0]
:ufw-track-input - [0:0]
:ufw-track-output - [0:0]
:ufw-user-forward - [0:0]
:ufw-user-input - [0:0]
:ufw-user-limit - [0:0]
:ufw-user-limit-accept - [0:0]
:ufw-user-logging-forward - [0:0]
:ufw-user-logging-input - [0:0]
:ufw-user-logging-output - [0:0]
:ufw-user-output - [0:0]
-A INPUT -p tcp -m multiport --dports 25,465,143,220,993,110,995 -j fail2ban-dovecot
-A INPUT -p tcp -m multiport --dports 25,465 -j fail2ban-postfix
-A INPUT -p tcp -m multiport --dports 21,20,990,989 -j fail2ban-wuftpd
-A INPUT -p tcp -m multiport --dports 80,443 -j fail2ban-apache-overflows
-A INPUT -p tcp -m multiport --dports 80,443 -j fail2ban-apache-noscript
-A INPUT -p tcp -m multiport --dports 80,443 -j fail2ban-apache
-A INPUT -p tcp -m multiport --dports 22 -j fail2ban-ssh-ddos
-A INPUT -p tcp -m multiport --dports 22 -j fail2ban-ssh
-A INPUT -j ufw-before-logging-input
-A INPUT -j ufw-before-input
-A INPUT -j ufw-after-input
-A INPUT -j ufw-after-logging-input
-A INPUT -j ufw-reject-input
-A INPUT -j ufw-track-input
-A FORWARD -j ufw-before-logging-forward
-A FORWARD -j ufw-before-forward
-A FORWARD -j ufw-after-forward
-A FORWARD -j ufw-after-logging-forward
-A FORWARD -j ufw-reject-forward
-A OUTPUT -j ufw-before-logging-output
-A OUTPUT -j ufw-before-output
-A OUTPUT -j ufw-after-output
-A OUTPUT -j ufw-after-logging-output
-A OUTPUT -j ufw-reject-output
-A OUTPUT -j ufw-track-output
-A fail2ban-apache -j RETURN
-A fail2ban-apache-noscript -j RETURN
-A fail2ban-apache-overflows -j RETURN
-A fail2ban-dovecot -j RETURN
-A fail2ban-postfix -j RETURN
-A fail2ban-ssh -j RETURN
-A fail2ban-ssh-ddos -j RETURN
-A fail2ban-wuftpd -j RETURN
-A ufw-after-input -m addrtype --dst-type BROADCAST -j ufw-skip-to-policy-input
-A ufw-after-logging-forward -m limit --limit 3/min --limit-burst 10 -j LOG --log-prefix ""[UFW BLOCK] ""
-A ufw-after-logging-input -m limit --limit 3/min --limit-burst 10 -j LOG --log-prefix ""[UFW BLOCK] ""
-A ufw-before-forward -j ufw-user-forward
-A ufw-before-input -i lo -j ACCEPT
-A ufw-before-input -m state --state RELATED,ESTABLISHED -j ACCEPT
-A ufw-before-input -m state --state INVALID -j ufw-logging-deny
-A ufw-before-input -m state --state INVALID -j DROP
-A ufw-before-input -p icmp -m icmp --icmp-type 3 -j ACCEPT
-A ufw-before-input -p icmp -m icmp --icmp-type 4 -j ACCEPT
-A ufw-before-input -p icmp -m icmp --icmp-type 11 -j ACCEPT
-A ufw-before-input -p icmp -m icmp --icmp-type 12 -j ACCEPT
-A ufw-before-input -p icmp -m icmp --icmp-type 8 -j ACCEPT
-A ufw-before-input -p udp -m udp --sport 67 --dport 68 -j ACCEPT
-A ufw-before-input -j ufw-not-local
-A ufw-before-input -d 224.0.0.251/32 -p udp -m udp --dport 5353 -j ACCEPT
-A ufw-before-input -d 239.255.255.250/32 -p udp -m udp --dport 1900 -j ACCEPT
-A ufw-before-input -j ufw-user-input
-A ufw-before-output -o lo -j ACCEPT
-A ufw-before-output -m state --state RELATED,ESTABLISHED -j ACCEPT
-A ufw-before-output -j ufw-user-output
-A ufw-logging-allow -m limit --limit 3/min --limit-burst 10 -j LOG --log-prefix ""[UFW ALLOW] ""
-A ufw-logging-deny -m state --state INVALID -m limit --limit 3/min --limit-burst 10 -j RETURN
-A ufw-logging-deny -m limit --limit 3/min --limit-burst 10 -j LOG --log-prefix ""[UFW BLOCK] ""
-A ufw-not-local -m addrtype --dst-type LOCAL -j RETURN
-A ufw-not-local -m addrtype --dst-type MULTICAST -j RETURN
-A ufw-not-local -m addrtype --dst-type BROADCAST -j RETURN
-A ufw-not-local -m limit --limit 3/min --limit-burst 10 -j ufw-logging-deny
-A ufw-not-local -j DROP
-A ufw-skip-to-policy-forward -j DROP
-A ufw-skip-to-policy-input -j DROP
-A ufw-skip-to-policy-output -j ACCEPT
-A ufw-track-output -p tcp -m state --state NEW -j ACCEPT
-A ufw-track-output -p udp -m state --state NEW -j ACCEPT
-A ufw-user-input -p tcp -m multiport --dports 80,443 -m comment --comment ""\'dapp_Apache%20Full\'"" -j ACCEPT
-A ufw-user-input -p tcp -m tcp --dport 53 -m comment --comment ""\'dapp_Bind9\'"" -j ACCEPT
-A ufw-user-input -p udp -m udp --dport 53 -m comment --comment ""\'dapp_Bind9\'"" -j ACCEPT
-A ufw-user-input -p tcp -m tcp --dport 143 -m comment --comment ""\'dapp_Dovecot%20IMAP\'"" -j ACCEPT
-A ufw-user-input -p tcp -m tcp --dport 110 -m comment --comment ""\'dapp_Dovecot%20POP3\'"" -j ACCEPT
-A ufw-user-input -p tcp -m tcp --dport 22 -m comment --comment ""\'dapp_OpenSSH\'"" -j ACCEPT
-A ufw-user-input -p tcp -m tcp --dport 25 -m comment --comment ""\'dapp_Postfix\'"" -j ACCEPT
-A ufw-user-input -p tcp -m tcp --dport 465 -m comment --comment ""\'dapp_Postfix%20SMTPS\'"" -j ACCEPT
-A ufw-user-input -p tcp -m tcp --dport 587 -m comment --comment ""\'dapp_Postfix%20Submission\'"" -j ACCEPT
-A ufw-user-input -p udp -m multiport --dports 137,138 -m comment --comment ""\'dapp_Samba\'"" -j ACCEPT
-A ufw-user-input -p tcp -m multiport --dports 139,445 -m comment --comment ""\'dapp_Samba\'"" -j ACCEPT
-A ufw-user-limit -m limit --limit 3/min -j LOG --log-prefix ""[UFW LIMIT BLOCK] ""
-A ufw-user-limit -j REJECT --reject-with icmp-port-unreachable
-A ufw-user-limit-accept -j ACCEPT
COMMIT
</code></pre>
","<ubuntu><iptables><samba><ufw>","2016-08-23 00:34:32"
"798602","Folders starting with a dot","<p>I'm a little curious about something, I'm running Windows 7 and I had a folder called <code>.Net Framework</code> which I wanted to rename to remove the space. So I tried to call it <code>.Net_framework</code> but Windows threw an error ""You must type a file name"".</p>

<p>So there's something happening when folders start with a <code>.</code>.<br>
I tried to call it <code>.NetFramework</code> and this wasn't accepted either.</p>

<p>This isn't critical in any way, anyone have any thoughts/answers on this one?</p>

<p>Regards,
Gog</p>

<p>P.S. I called it <code>DotNet_framework</code> in the end :P</p>
","<windows><directory>","2016-08-23 14:32:51"
"872419","Mail Server Domain Serving Multiple Domains","<p>I am hosting multiple domains on an email server where the SSL certificate is for the root domain name (let's say example.com). The rDNS goes to example.com, but the alias domains on the server are emailhere.com and emailhere2.com for example. Unless someone goes digging, they won't see example.com show up as the emails are going to the alias emails, but it shows up under rDNS for the server IP. If I change example.com to a new TLD domain like 'example.network' along with the rDNS, is there potential to get blacklisted by other email servers even if the alias email domains do not change? Would it be safer to stick to a .com TLD?</p>

<p>To reword, do blacklists care about domain name TLD for the mail server if the alias domain name sending the email is something like '.com'?</p>
","<email><postfix><domain><tld>","2017-09-07 00:34:26"
"1045480","E-mail transmission example","<p>RFC standards literally force us to accept unencrypted connections on port <code>25</code>. To understand why, we have to understand how emailing works. But emailing is quite a complex topic and I created this example together with a table to try and understand everything.</p>
<p><strong>Can anyone read and tell me if I am wrong in any part of the explanation? Because I am not quite sure if my understanding of the topic is correct.</strong></p>
<hr />
<h2>EXAMPLE</h2>
<p>When user (sender) sends email through <em>&quot;mail user agent&quot;</em>  (MUA), this email is immediately transfered to the <em>&quot;mail submission agent&quot;</em> (MSA) which is or isn't on a separate machine. MSA preprocesses the email and hand it over to <em>&quot;mail transfer agent&quot;</em> (MTA) on the same machine. MTA (sender) then uses DNS and determines to which MTA (receiver) email should be sent. This portion of the transport is only done over port <code>25</code>. When the MTA (reciever) gets the email, it handles it over to the MSA on the same machine and then user (receiver) can read the email using MUA.</p>
<p>Communication between MUA &amp; MSA and MSA &amp; MTA can use secure ports but connection betweem MTA &amp; MTA can't. Table below shows which protocols are used or can be used and which ports can be used for each step of the above example. We also use ✘ and ✔ where there is a choice to indicate what a modern setup should use.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>#</th>
<th>sender</th>
<th>receiver</th>
<th>protocols we can use</th>
<th>ports of respective protocols</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>MUA</td>
<td>MSA</td>
<td>(✘) SMTP <br/>(✔) SMTPS</td>
<td>(✘) <code>25</code> <br/>(✔) <code>587</code></td>
</tr>
<tr>
<td>2</td>
<td>MSA</td>
<td>MTA</td>
<td>(✘) SMTP <br/>(✔) SMTPS</td>
<td>(✘) <code>25</code> <br/>(✔) <code>587</code></td>
</tr>
<tr>
<td>3</td>
<td>MTA</td>
<td>MTA</td>
<td>(✔) SMTP</td>
<td>(✔) <code>25</code></td>
</tr>
<tr>
<td>4</td>
<td>MTA</td>
<td>MSA</td>
<td>(✔) SMTP</td>
<td>(✔) <code>25</code></td>
</tr>
<tr>
<td>5</td>
<td>MSA</td>
<td>MUA</td>
<td>(✘) POP3 <br/>(✘) POP3S <br/>(✘) IMAP <br/>(✔) IMAPS</td>
<td>(✘) <code>110</code> <br/>(✘) <code>995</code> <br/>(✘) <code>143</code> <br/>(✔) <code>993</code></td>
</tr>
</tbody>
</table>
</div>","<email><smtp><imap><pop3>","2020-12-08 19:53:54"
"798707","Move Event Log in Windows 2012","<p>Ultimately I'm trying to have security logs written to a remote storage, </p>

<pre><code>\\Server-Name\Drive-Letter\File_Name.evtx
</code></pre>

<p>For testing I'm trying to move the default log path from <code>%SystemRoot%\System32\Winevt\Logs\Security.evtx</code> to <code>C:\Security.evtx</code> . 
This however is failing; no errors in logs.</p>

<p>I double checked the registry at <code>HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\EventLog\Security</code> and the <code>File</code> does point to <code>C:\Security.evtx</code> however logs are still written in the default <code>%SystemRoot%\System32\Winevt\Logs\Security.evtx</code>. I double checked and no group policy is in place for this.</p>

<p>Any suggestions on how to do this? I'm aware of <code>wevtutil</code> however I'd like to accomplish this using Event Viewer.</p>
","<windows><windows-server-2012-r2><windows-event-log><eventviewer>","2016-08-23 22:45:17"
"872499","Nginx rewriting a url with url ars for a specific script without having the .js extention to the map","<p>My webpage dynamically does the following request:</p>

<blockquote>
  <p><a href=""http://example.com/f1/script?param1=1&amp;param2=2&amp;...&amp;paramn=n"" rel=""nofollow noreferrer"">http://example.com/f1/script?param1=1&amp;param2=2&amp;...&amp;paramn=n</a></p>
</blockquote>

<p>And I want to my nginx server when is visited to load a file named <code>script.js</code></p>

<p>Therefore I did:</p>

<pre><code>location ~^/f1/script?.* {
 rewrite ^/f1/script?.* /f1/script.js;
}
</code></pre>

<p>But the browser fails to receive it.</p>

<p>Do you know how to make my browser to successfully receive the file?</p>

<h1>Edit 1</h1>

<p>I tried to do the following:</p>

<pre><code>location ~^/f1/ {
    try_files $uri $uri.js;
    rewrite ^(.*)$ $uri.js;
}
</code></pre>

<p>Based on <a href=""https://stackoverflow.com/questions/8970674/remove-php-file-extension-with-nginx-php-fpm#17475222"">https://stackoverflow.com/questions/8970674/remove-php-file-extension-with-nginx-php-fpm#17475222</a> but still no light in my path</p>
","<nginx><rewrite>","2017-09-07 12:19:02"
"798726","How can I install Oracle JDK on FreeBSD_10.3","<p>I've fetched jdk-8u102-i586.tar.gz from <a href=""http://www.oracle.com/technetwork/java/javase/downloads/index.html"" rel=""nofollow noreferrer"">http://www.oracle.com/technetwork/java/javase/downloads/index.html</a> </p>

<p>and already place it in /usr/ports/distfiles directory.</p>

<p>But, ""make install"" command prints same message when I first command make install without jdk file fetch. </p>

<p>How can I solve it?</p>

<p>This image is my terminal message.</p>

<hr>

<p>after I fixed command, <code>make install</code> still does not work with following message:</p>

<p><img src=""https://i.sstatic.net/TXEBy.png"" alt=""enter image description here""></p>
","<java><installation><freebsd><oracle>","2016-08-24 02:15:22"
"798769","How does IMAP/POP Server fetch emails from my SMTP server?","<p>I have a domain abc.com with MX record/SMTP server mx1.example.com. My IMAP/POP server (incoming server) is imap.example.com and pop3.example.com.</p>

<p>How does my IMAP and POP server fetch emails from mx1.example.com ? Do IMAP/POP Server use any protocols to fetch emails from mx1.example.com. How does my IMAP server knows that emails should be fetched from mx1.example.com. I am using postfix and dovecot for email service.</p>

<p>Please clear my these confusions.</p>
","<smtp><dovecot><mx-record><imap><pop3>","2016-08-24 09:06:19"
"939093","MySQL suddenly stopped to work - Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (111)","<p>My website outputs the following error:</p>

<pre><code>Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (111)
</code></pre>

<p>I tried many solutions I found online, none of them worked.</p>

<pre><code># service mysql status

Nov 08 07:51:00 vps412690.ovh.net mysql[14525]: /etc/init.d/mysql: ERROR: The partition with /var/lib/mysql is too full...iled!
Nov 08 07:51:00 vps412690.ovh.net /etc/init.d/mysql[14544]: ERROR: The partition with /var/lib/mysql is too full!
Nov 08 07:51:00 vps412690.ovh.net systemd[1]: mysql.service: control process exited, code=exited status=1
Nov 08 07:51:00 vps412690.ovh.net systemd[1]: Failed to start LSB: Start and stop the mysql database server daemon.
Nov 08 07:51:00 vps412690.ovh.net systemd[1]: Unit mysql.service entered failed state.
Nov 08 07:57:48 vps412690.ovh.net systemd[1]: Stopped LSB: Start and stop the mysql database server daemon.
</code></pre>

<p>I tried to move the contents in <code>/var/lib/mysql</code> to <code>user/lib/mysql</code> - I still get this error.</p>

<p>OS: Debian 8 (Jessie) (64-bit version)</p>

<pre><code>ps aux | grep mysql
</code></pre>

<p>shows different PID every time I run it!</p>

<p>What else I'm missing?</p>

<p>UPDATE:</p>

<p>My <code>sda1</code> partition is full</p>

<pre><code>/dev/sda1        20G   19G  156M 100% /
</code></pre>

<p>clean didn't fix it, what else I can do?</p>
","<mysql><debian>","2018-11-08 07:02:47"
"798792","Does secondary or backup MX server deliver queued emails to users?","<p>Suppose, I have a primary MX: mx1.abc.com with priority 5 and secondary mx as mx2.abc.com with Priority 10. I have independent IMAP/POP servers configured to fetch email for abc.com domain. (imap.abc.com &amp; pop3.abc.com)</p>

<p>Suppose mx1.abc.com goes down, is it possible for mx2.abc.com to deliver emails to imap/pop3.abc.com and finally to users of abc.com ?  </p>

<p>As per my understanding, secondary MX just holds the queue and forward those emails to primary mail server as soon as it gets up.</p>

<p>Is it really possible ?</p>

<p>Please clear my confusions with technical ideas. I'd love to get answers on or at least receive some hints. :-)</p>
","<email><postfix><email-server><imap><pop3>","2016-08-24 10:38:39"
"798801","Is this possible: Use css from other virtual host?","<p>Say I have a website at <code>/var/www/Electrician</code>
and I have a website at <code>/var/www/Restaurant</code></p>

<p>If these websites use separate host files can I use the same CSS to reduce space?</p>

<p>If I am not mistaken I would certainly be unable to do ../../css/style.css because the one host is restricted from access the other, I would imagine.</p>

<p>I can see putting the one website into the other website directory, but I see that leaving room for cross-site issues.</p>

<p>I will admit I am doing a poor job explaining this. Essentially I would like to share CSS stylesheets between to separate websites on the same server, which will employ separate host files.</p>
","<nginx><virtualhost><website><css><development>","2016-08-24 11:20:52"
"798803","Securing MySQL given root access on a server","<p>Let's say a hacker gains root access to a server containing password protected MySQL.</p>

<p>Well, if we they simply reset the MySQL root password, given a few simple commands, doesn't that defeat the point in having it password protected in the first place?</p>

<p>For example, is there a well-practised technique in which one can segregate MySQL and prevent such a thing from happening. The only thing I could think of was, perhaps, moving the database to a different server all together.</p>

<p>All thoughts welcome.</p>

<p><strong>Edit</strong></p>

<p>I understand that MySQL passwords allow for multiple accounts, with various permissions, and nothing more. My question is more focused on how to isolate the database, so if infiltrated one can't simply dip into your database records.</p>
","<mysql><lamp><password-reset>","2016-08-24 11:31:29"
"939138","tune2fs says ""clean with errors"". What next?","<p>tune2fs says ""clean with errors"". What next?</p>

<p>We rebooted the server, but this did not help.</p>

<p>What is not the next most feasible step?</p>

<p>This is a production system. I would like to avoid down-times.</p>

<pre><code>foo-host:~ # /sbin/tune2fs -l /dev/disk/by-uuid/111ce226-5f97-4fb7-b6cf-4b47f40236bd
tune2fs 1.42.11 (09-Jul-2014)
Filesystem volume name:   &lt;none&gt;
Last mounted on:          /
Filesystem UUID:          111ce226-5f97-4fb7-b6cf-4b47f40236bd
Filesystem magic number:  0xEF53
Filesystem revision #:    1 (dynamic)
Filesystem features:      has_journal ext_attr resize_inode dir_index filetype needs_recovery extent flex_bg sparse_super large_file huge_file uninit_bg dir_nlink extra_isize
Filesystem flags:         signed_directory_hash 
Default mount options:    user_xattr acl

Filesystem state:         clean with errors  &lt;&lt;&lt;======

Errors behavior:          Continue
Filesystem OS type:       Linux
Inode count:              1310720
Block count:              5242880
Reserved block count:     262144
Free blocks:              3244098
Free inodes:              1195746
First block:              0
Block size:               4096
Fragment size:            4096
Reserved GDT blocks:      1022
Blocks per group:         32768
Fragments per group:      32768
Inodes per group:         8192
Inode blocks per group:   512
Flex block group size:    16
Filesystem created:       Fri Jul 31 18:24:44 2015
Last mount time:          Tue Nov  6 17:48:48 2018
Last write time:          Tue Nov  6 17:48:47 2018
Mount count:              7
Maximum mount count:      5
Last checked:             Fri Jan 19 21:57:47 2018
Check interval:           0 (&lt;none&gt;)
Lifetime writes:          3380 GB
Reserved blocks uid:      0 (user root)
Reserved blocks gid:      0 (group root)
First inode:              11
Inode size:           256
Required extra isize:     28
Desired extra isize:      28
Journal inode:            8
First orphan inode:       786751
Default directory hash:   half_md4
Directory Hash Seed:      2898102d-4fb1-4758-b9ad-018ec4b2c92e
Journal backup:           inode blocks
</code></pre>
","<ext3><tune2fs>","2018-11-08 12:46:06"
"1045704","Ubuntu 18.04 keeps creating virtual interface ens160:0, how to prevent?","<p>I am running a Pi-hole server on Ubuntu 18.04 that for some reason keeps making a virtual interface ens160:0</p>
<pre><code>user@pihole2:/etc$ ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: ens160: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 00:0c:29:6e:35:36 brd ff:ff:ff:ff:ff:ff
    inet 192.168.1.108/24 brd 192.168.1.255 scope global ens160
       valid_lft forever preferred_lft forever
    inet 192.168.1.111/24 brd 192.168.1.255 scope global secondary noprefixroute ens160
       valid_lft forever preferred_lft forever
</code></pre>
<p>There is no reason I know of that a second IP is needed.</p>
<p>Netplan config below, and /etc/network/interfaces is blank.</p>
<pre><code>  GNU nano 4.8                              00-installer-config.yaml
# This is the network config written by 'subiquity'
network:
  ethernets:
        ens160:
            addresses: ['192.168.1.108/24']
            gateway4: 192.168.1.1
            nameservers:
                addresses: [192.168.1.108]
                search: [park.local]
  version: 2
</code></pre>
<p>Also, Webmin shows that this is not activated on boot. I don't know where to look next.</p>
","<ubuntu><linux-networking><ifconfig>","2020-12-10 13:59:56"
"872826","Security around Ansible servers","<p>So Ansible is the BIG thing at the moment, along with other configuration management solutions like Puppet, SaltStack and so on. At my company we are managing 500+ servers and some guys are implementing an Ansible solution to minimise the configuration management overhead. </p>

<p>Paranoid as I am, I started thinking about what can actually be done from an Ansible server if someone is not in their right mind. </p>

<p>I have searched online looking for best practices, for security hardening on Ansible servers and mostly I find articles of how Ansible can be used to harden other servers (obviously) and also I find general recommendations of how to harden a Linux server, since Ansible runs on Linux. So that got me thinking that the overall idea of having a centralised configuration management server comes with some risks; wouldn't I, as a hacker or disgruntled employee, be able to run the command <code>rm -rf /</code> on all servers or similar using the Ansible server?</p>

<p>As a rule of thumb I would suppose that having a segmented network where test, dev, pre-prod, and prod are all unavailable to each other would mean that you would AT LEAST have an Ansible server installed for each of these environments, so you don't have a server where firewall allows access to all systems. I guess that's common sense at least.</p>

<p>However still, the idea of a server that can configure an ENTIRE NETWORK of servers is insane if you think about someone who's not supposed to, getting access to the Ansible server.</p>

<p>Would one create some kind of MFA that kicks in before commands are run, or would the Ansible server be unable to create its own playbooks, and then the playbooks are mounted from a readonly server elsewhere, so the possibilities of post exploitation at least require you to control more than just the Ansible server?</p>

<p>Do we put the Ansible server on a locked-the-f-down management network that is difficult for even employees to access with MFA, special access requirements, no VPN, only onsite access, and so on?</p>

<p>Or do we accept the risk and keep good backups and protect the network properly and accept that if the Ansible server is compromised, destruction could be imminent?</p>

<p>Or am I totally wrong about the destructive capabilities of Ansible?</p>
","<security><ansible>","2017-09-09 13:01:43"
"872837","Loud Pop from DL380G5--then neither PSU lights up when plugged in","<p>I wanted to fire up my old DL380 G5 server recently, and maybe one second after I hit the power button and the fans started to rev up, I heard a loud pop and suddenly it shut down.  Now neither PSU lights the LED near the power cable.  The machine was not excessively loaded with graphics cards, many HDDs (only had one), etc.</p>

<p>Unfortunately, I saw a few strands of uncured carbon fiber floating around near the server, and maybe one was sucked into contact with a circuit or something(??).  I didn't see any evidence of this near the hard drive bay, which seemed to be where the pop came from (hard to tell).</p>

<p>The outlets work for other machines, so the building's circuit breaker was not tripped.</p>

<p>Has anyone had this sort of thing before?  If I buy replacement power supplies, what is the probability that the server will work?</p>

<p>Thanks for your time!</p>
","<hp><hp-proliant><electrical-power><power-supply-unit>","2017-09-09 15:54:49"
"872877","Is it a sufficient emergency security measure to send an immediate mail notification when someone logs in through ssh?","<p><a href=""https://serverfault.com/questions/227629/send-me-an-e-mail-whenever-someone-logs-in-via-ssh"">Send me an e-mail whenever someone logs in via SSH</a></p>

<p>Talking about this here, is this enough to prevent any damage (or more, being able to limit it through <em>being</em> notified of it) to your server should someone actually manage to log in?  </p>

<p>Additional questions:</p>

<p>Are there other ways of intrusion you will not be notified of this way?<br>
Should you log sessions additionally if you do not want to just scrape your whole server the moment someone breaks in?<br>
How do you exactly figure out <em>what</em> made your server insecure?</p>

<p>EDIT:</p>

<p>It seems people have been half-reading the question and therefore giving unfitting answers and downvoting.
First, the title has ""emergency measure"" in it, which means it is a measure for things being already too late. The backup plan IF things go wrong.</p>

<p>Second, ""to scrape the entire server the moment someone breaks in"" was also implying that. Should someone break in, in this case the entire server probably has to be thrown away and set up completely anew. So I am actually asking how to prevent that and what to do more in the case of an emergency -  someone actually broke in.</p>
","<linux><ssh><security><notification>","2017-09-10 01:16:29"
"799178","Nginx/Php Fastcgi permission issue","<p>Suddenly I started getting the following error when I try to restart nginx:</p>
<blockquote>
<p>nginx: the configuration file /etc/nginx/nginx.conf syntax is ok</p>
<p>nginx: [emerg] mkdir() &quot;/var/run/nginx-cache/site1&quot; failed (2: No such file or directory)</p>
<p>nginx: configuration file /etc/nginx/nginx.conf test failed</p>
</blockquote>
<p>So I have to create nginx-cache folder and make www-data the owner of it to restart nginx, but I'm hoping there's a better way.</p>
<p>For your information, my nginx configuration is as follows:</p>
<pre><code>fastcgi_cache_path /var/run/nginx-cache/site1 levels=1:2 keys_zone=site1:100m inactive=60m;
fastcgi_cache_path /var/run/nginx-cache/site2 levels=1:2 keys_zone=site2:100m inactive=60m;
fastcgi_cache_path /var/run/nginx-cache/site3 levels=1:2 keys_zone=site3:100m inactive=60m;
fastcgi_cache_use_stale error timeout invalid_header http_500;
fastcgi_ignore_headers Cache-Control Expires Set-Cookie;

server {
....
location ~ \.php$ {
....
 include fastcgi_params;
 fastcgi_pass unix:/var/run/php5-fpm.sock;
 fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
 fastcgi_cache_bypass $skip_cache;
 fastcgi_no_cache $skip_cache;
 fastcgi_cache site1;
 fastcgi_cache_valid  60m;
}

server {
....
location ~ \.php$ {
....
 include fastcgi_params;
 fastcgi_pass unix:/var/run/php5-fpm.sock;
 fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
 fastcgi_cache_bypass $skip_cache;
 fastcgi_no_cache $skip_cache;
 fastcgi_cache site2;
 fastcgi_cache_valid  60m;
}

server {
....
location ~ \.php$ {
....
 include fastcgi_params;
 fastcgi_pass unix:/var/run/php5-fpm.sock;
 fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
 fastcgi_cache_bypass $skip_cache;
 fastcgi_no_cache $skip_cache;
 fastcgi_cache site3;
 fastcgi_cache_valid  60m;
}
</code></pre>
<p>Any help would be greatly appreciated!</p>
","<nginx><fastcgi>","2016-08-25 20:50:40"
"872896","Apache recompile with mod_ssl and openssl","<p>I have a Nagios (dev) server built on AWS EC2. The server had preinstalled:</p>

<ul>
<li>OpenSSL 1.0.1k-fips </li>
<li>Apache/2.4.25</li>
</ul>

<p>Qualys scan notified that there are vulnerabilities. The package available on AWS repository points to <em>OpenSSL 1.0.1k-fips</em>, which is the latest so <em>backporting fixes to latest version</em> is out of question, since  it was already at the most updated version provided by the vendor. 
Due to vulnerabilities threat, I had Openssl updated from source to latest version. It now has:</p>

<pre><code>[root@ip-172-31-1-222 ~]# openssl version -a
OpenSSL 1.1.0f  25 May 2017
built on: reproducible build, date unspecified
platform: linux-x86_64
compiler: gcc -DZLIB -DZLIB_SHARED -DDSO_DLFCN -DHAVE_DLFCN_H -DNDEBUG -DOPENSSL_THREADS -DOPENSSL_NO_STATIC_ENGINE -DOPENSSL_PIC -DOPENSSL_IA32_SSE2 -DOPENSSL_BN_ASM_MONT -DOPENSSL_BN_ASM_MONT5 -DOPENSSL_BN_ASM_GF2m -DSHA1_ASM -DSHA256_ASM -DSHA512_ASM -DRC4_ASM -DMD5_ASM -DAES_ASM -DVPAES_ASM -DBSAES_ASM -DGHASH_ASM -DECP_NISTZ256_ASM -DPADLOCK_ASM -DPOLY1305_ASM -DOPENSSLDIR=""\""/usr/local/openssl\"""" -DENGINESDIR=""\""/usr/local/usr/lib64/openssl/engines-1.1\""""
OPENSSLDIR: ""/usr/local/openssl""
ENGINESDIR: ""/usr/local/usr/lib64/openssl/engines-1.1""

[root@ip-172-31-1-222 ~]# ldd /usr/local/bin/openssl
linux-vdso.so.1 =&gt;  (0x00007ffe24cb3000)
libssl.so.1.1 =&gt; /usr/lib64/libssl.so.1.1 (0x00007f94d3a6c000)
libcrypto.so.1.1 =&gt; /usr/lib64/libcrypto.so.1.1 (0x00007f94d35e7000)
libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00007f94d33e2000)
libpthread.so.0 =&gt; /lib64/libpthread.so.0 (0x00007f94d31c6000)
libc.so.6 =&gt; /lib64/libc.so.6 (0x00007f94d2e02000)
/lib64/ld-linux-x86-64.so.2 (0x0000564c46955000)
</code></pre>

<p>It still hosts the old version of OpenSSL:</p>

<pre><code>[root@ip-172-31-1-222 ~]# ldd openssl
linux-vdso.so.1 =&gt;  (0x00007ffe5657d000)
libssl.so.10 =&gt; /usr/lib64/libssl.so.10 (0x00007f41198d1000)
libgssapi_krb5.so.2 =&gt; /usr/lib64/libgssapi_krb5.so.2 (0x00007f4119683000)
libkrb5.so.3 =&gt; /usr/lib64/libkrb5.so.3 (0x00007f411939b000)
libcom_err.so.2 =&gt; /usr/lib64/libcom_err.so.2 (0x00007f4119198000)
libk5crypto.so.3 =&gt; /usr/lib64/libk5crypto.so.3 (0x00007f4118f66000)
libcrypto.so.10 =&gt; /lib64/libcrypto.so.10 (0x00007f4118b7d000)
libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00007f4118979000)
libz.so.1 =&gt; /lib64/libz.so.1 (0x00007f4118763000)
libc.so.6 =&gt; /lib64/libc.so.6 (0x00007f411839e000)
libkrb5support.so.0 =&gt; /usr/lib64/libkrb5support.so.0 (0x00007f411818f000)
libkeyutils.so.1 =&gt; /lib64/libkeyutils.so.1 (0x00007f4117f8c000)
libresolv.so.2 =&gt; /lib64/libresolv.so.2 (0x00007f4117d71000)
libpthread.so.0 =&gt; /lib64/libpthread.so.0 (0x00007f4117b55000)
/lib64/ld-linux-x86-64.so.2 (0x000055b36b640000)
libselinux.so.1 =&gt; /usr/lib64/libselinux.so.1 (0x00007f4117933000)
</code></pre>

<p>There are no means to remove the old version. I tried <code>yum remove</code>, it does not work. When I tried to check the <em>mod_ssl</em> &amp; <em>httpd</em>, below output comes:</p>

<pre><code>[root@ip-172-31-1-222 ~]# ldd $(which httpd)
linux-vdso.so.1 =&gt;  (0x00007ffef1511000)
libpcre.so.0 =&gt; /lib64/libpcre.so.0 (0x00007fc6e522c000)
libselinux.so.1 =&gt; /usr/lib64/libselinux.so.1 (0x00007fc6e500b000)
libaprutil-1.so.0 =&gt; /usr/lib64/libaprutil-1.so.0 (0x00007fc6e4de5000)
libcrypt.so.1 =&gt; /lib64/libcrypt.so.1 (0x00007fc6e4bae000)
libexpat.so.1 =&gt; /lib64/libexpat.so.1 (0x00007fc6e4985000)
libdb-4.7.so =&gt; /lib64/libdb-4.7.so (0x00007fc6e4615000)
libapr-1.so.0 =&gt; /usr/lib64/libapr-1.so.0 (0x00007fc6e43e2000)
libpthread.so.0 =&gt; /lib64/libpthread.so.0 (0x00007fc6e41c6000)
libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00007fc6e3fc1000)
libc.so.6 =&gt; /lib64/libc.so.6 (0x00007fc6e3bfd000)
/lib64/ld-linux-x86-64.so.2 (0x0000564690396000)
libuuid.so.1 =&gt; /lib64/libuuid.so.1 (0x00007fc6e39f9000)
libfreebl3.so =&gt; /lib64/libfreebl3.so (0x00007fc6e37f6000)

[root@ip-172-31-1-222 ~]# ldd /etc/httpd/modules/mod_ssl.so
linux-vdso.so.1 =&gt;  (0x00007fffc56fb000)
libssl.so.10 =&gt; /usr/lib64/libssl.so.10 (0x00007f44e49d6000)
libcrypto.so.10 =&gt; /lib64/libcrypto.so.10 (0x00007f44e45ee000)
libpthread.so.0 =&gt; /lib64/libpthread.so.0 (0x00007f44e43d1000)
libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00007f44e41cd000)
libc.so.6 =&gt; /lib64/libc.so.6 (0x00007f44e3e09000)
libgssapi_krb5.so.2 =&gt; /usr/lib64/libgssapi_krb5.so.2 (0x00007f44e3bba000)
libkrb5.so.3 =&gt; /usr/lib64/libkrb5.so.3 (0x00007f44e38d3000)
libcom_err.so.2 =&gt; /usr/lib64/libcom_err.so.2 (0x00007f44e36d0000)
libk5crypto.so.3 =&gt; /usr/lib64/libk5crypto.so.3 (0x00007f44e349d000)
libz.so.1 =&gt; /lib64/libz.so.1 (0x00007f44e3287000)
/lib64/ld-linux-x86-64.so.2 (0x000055f1c2490000)
libkrb5support.so.0 =&gt; /usr/lib64/libkrb5support.so.0 (0x00007f44e3078000)
libkeyutils.so.1 =&gt; /lib64/libkeyutils.so.1 (0x00007f44e2e74000)
libresolv.so.2 =&gt; /lib64/libresolv.so.2 (0x00007f44e2c5a000)
libselinux.so.1 =&gt; /usr/lib64/libselinux.so.1 (0x00007f44e2a38000)
</code></pre>

<p>The mod_ssl is not pointing to the new openssl libraries (<em>lib_ssl</em> and <em>lib_crypto</em>), it is poiting to the old version of openssl. I have read somewhere, I need to recompile Apache with <em>mod_ssl</em> from source to make it point to the correct libraries.<br>
The <em>httpd</em> package was updated using <code>yum</code>.</p>

<pre><code>[root@ip-172-31-1-222 ~]# httpd -V
Server version: Apache/2.4.27 (Amazon)
Server built:   Aug  2 2017 18:02:45
Server's Module Magic Number: 20120211:68
Server loaded:  APR 1.5.1, APR-UTIL 1.4.1
Compiled using: APR 1.5.1, APR-UTIL 1.4.1
Architecture:   64-bit
Server MPM:     prefork
  threaded:     no
    forked:     yes (variable process count)
Server compiled with....
 -D APR_HAS_SENDFILE
 -D APR_HAS_MMAP
 -D APR_HAVE_IPV6 (IPv4-mapped addresses enabled)
 -D APR_USE_SYSVSEM_SERIALIZE
 -D APR_USE_PTHREAD_SERIALIZE
 -D SINGLE_LISTEN_UNSERIALIZED_ACCEPT
 -D APR_HAS_OTHER_CHILD
 -D AP_HAVE_RELIABLE_PIPED_LOGS
 -D DYNAMIC_MODULE_LIMIT=256
 -D HTTPD_ROOT=""/etc/httpd""
 -D SUEXEC_BIN=""/usr/sbin/suexec""
 -D DEFAULT_PIDLOG=""/var/run/httpd/httpd.pid""
 -D DEFAULT_SCOREBOARD=""logs/apache_runtime_status""
 -D DEFAULT_ERRORLOG=""logs/error_log""
 -D AP_TYPES_CONFIG_FILE=""conf/mime.types""
 -D SERVER_CONFIG_FILE=""conf/httpd.conf""
</code></pre>

<p>Now, the questions are:</p>

<ol>
<li>Do I really need to recompile Apache? </li>
<li><p>Is it not possible to just recompile mod_ssl or perhaps some editing to point to new libraries?</p></li>
<li><p>If I <strong>have</strong> to recompile Apache from source, what parameters I
need to choose for <em>./config</em>, so that it doesn't break my existing
setup?  </p></li>
<li>Perhaps, remove <em>httpd</em> using yum and then install it from
source?</li>
</ol>

<p>Note:</p>

<ol>
<li>I have already backed up all <em>httpd</em> related files/conf from the server</li>
<li>We are using company authentication for nagios</li>
</ol>

<p>If you need further information, please do let me know.</p>
","<amazon-web-services><apache-2.4><openssl><mod-ssl>","2017-09-10 09:24:13"
"1045964","Cannot set parent share with Read/Write only access","<p>On a WD MyCloud nas I have a share called Multimedia. The nas is joined to my domain.</p>
<p><strong>What I want:</strong></p>
<p>Domain Users to be able to create/write and delete their own subfolders in the Multimedia share.</p>
<p><strong>What I don't want:</strong></p>
<p>Domain Users to be able to delete existing files in the Multimedia folder if they're not the owner of these files/sub-folders.</p>
<p><strong>The problem:</strong></p>
<p>I cannot set the Multimedia folder so that Domain Users cannot delete files/folders that they're not the owner of. Everytime I try to force Domain Users with Read, Write access (but not delete) and Apply the changes the permissions changes to Domain Users - Full Control.</p>
<p>I tried to make the Multimedia share in the nas gui Full Access or Read-only but it doesn't make a difference. Domain Users can still delete existing files that they're not the owner of in Multimedia.</p>
<p>NOTE: In the nas I can set the Multimedia share as Full Access, Read-Only or No Access to Domain Users. I have tried both Full and Read-Only access but I keep having the same problem.</p>
<p>I should be able to set this share using ntfs permissions where Domain Users can Read and Write in the Multimedia folder but not delete files/folders that aren't theirs.</p>
<p><strong>What I've tried that should work:</strong></p>
<p>I have set the Multimedia folder with CREATOR OWNER Full Control. Domain Users with Read and Write only to the folder (but not subfolders). But as soon as I try to set Domain Users to Read and Write in this folder and click Apply it changes to Domain Users - Full Control.</p>
<p>I tried creating a 2nd permission for Domain Users where they can Read/Write in Multimedia subfolders and its files which works fine (they can delete their own files), but the problem remains that in Multimedia I have files and other folders that they're not the owner of and they can still delete them since the Multimedia folder always has Domain Users - Full Control to this Folder.</p>
<p>I don't think I'm doing anything wrong but I'm open to any help or suggestions.</p>
<p>UPDATE: I forgot to mention that I've googled this and all the docs say to give CREATOR OWNER Full Control, the share Full Access to the user/group and set the subfolders to the permission you want. This is fine for making sure that subfolders and files cannot be deleted unless the user/group is owner, BUT, none of them have the problem I have where the user/group gets Full Control on the parent folder regardless of what custom rights you give it.</p>
","<network-share>","2020-12-12 21:33:07"
"939588","Nginx cache mkdir failed (2: No such file or directory) while reading upstream","<p>Edit for <strong>answer</strong>: </p>

<p>It's the selinux causes this problem, the temporary solution is to run this command: sudo semanage permissive -a httpd_t</p>

<p>But you shouldn't do that, because of security reasons. I found an article wrote by Danila Vershinin on nginx selinux configuration, if you have the same problem like me, you should read it.</p>

<p>Original question:</p>

<p>I have a wordpress + woocommerce website on a nginx server (centos 7), I want to make nginx fastcgi cache work, but it always get miss or passby, never hit.</p>

<p>Here is the errer log:</p>

<pre><code>2018/11/11 00:00:00 [crit] 1900#0: *1 mkdir() ""/etc/nginx/cache/0/53"" failed (2: No such file or directory) while reading upstream, client: 111.111.111.111, server: www.example.com, request: ""GET /page2/ HTTP/1.1"", upstream: ""fastcgi://unix:/run/php-fpm/www.sock:"", host: ""www.example.com"", referrer: ""https://www.example.com/page1/""
</code></pre>

<p>So how do I solve this problem to make cache work? Thanks!</p>

<p>PS:</p>

<p>Cache will be stored in /etc/nginx/cache, its permission is 700(drwx------), user and group is nginx:root</p>

<p>Here is the related nginx conf:</p>

<pre><code>fastcgi_cache_path /etc/nginx/cache levels=1:2 keys_zone=WORDPRESS:500m inactive=240m;
fastcgi_cache_key ""$scheme$request_method$host$request_uri"";
</code></pre>

<p>Here is the upstream conf:</p>

<pre><code>upstream php-fpm {
    server unix:/run/php-fpm/www.sock;
}
</code></pre>

<p>Here are some lines from the /etc/php-fpm.d/www.conf:</p>

<pre><code>user = nginx
group = nginx
listen = /run/php-fpm/www.sock
listen.owner = nobody
listen.group = nobody
listen.mode = 0660
listen.acl_users = nginx
</code></pre>
","<nginx><selinux>","2018-11-12 03:34:53"
"1045983","Pinging 8.8.8.8 via a windows 10 client going through windows server 2012 gives a request timedout error","<p>This is my setup in virtualbox.</p>
<p>A DHCP Scope of range 112.123.1.100 to 112.123.3.188, Length = 16, Router IP is 112.123.2.2 (this also is the IP of the server the DHCP is configured on).</p>
<p>After this I set my IP of the windows server 2012 manually to IP: 112.123.2.2. Subnet: 255.255.0.0 Gateway = 10.0.2.2 DNS1: 10.0.2.2 DNS2: 8.8.8.8</p>
<p>This virtualbox VM that the server is running on is on an internal NAT network type. To access the internet I configured a second adapter as NAT type as per this guide: <a href=""https://www.nakivo.com/blog/virtualbox-network-setting-guide/"" rel=""nofollow noreferrer"">https://www.nakivo.com/blog/virtualbox-network-setting-guide/</a></p>
<p>The server can Ping google.com so it has internet access.</p>
<p>The client:</p>
<p>I configure the VM as an internal NAT type. Now I set my IP address to the following:
IP: 112.123.3.95
Subnet: 255.255.0.0
Gateway: 112.123.2.2</p>
<p>DNS1: 112.123.2.2
DNS2: 8.8.8.8</p>
<p>After that in the VM in windows 10 Pro I join the servers domain (Workgroup).</p>
<p>When I ping the server from the client I get a response back in CMD but when I ping 8.8.8.8 I get a &quot;Request timed-out&quot; message in CMD.</p>
<p>Does anyone know what I am doing wrong? Again, all of this is happening inside VM's off virtualBox.</p>
<p>EDIT: Forgot to mention that the client can indeed ping the server but the server can't ping the client for some reason, but the client was able to join the servers workgroup/domain. so I don't know what is happening there.</p>
","<windows-server-2012><virtual-machines><virtualbox>","2020-12-13 03:33:12"
"799248","Global FTP user","<p>I am setting up a Centos 6.8 VPS to run a website plus it's staging and dev versions. We have some SSH users (authentication is key-only), and I am trying to set up a user (call them foo) for FTP access to all the sites. foo does not have ssh access or a /home/foo directory (does this mean they are a virtual user?</p>

<p>All the site directories beneath /var/www/ have owner foo, group www. (the same person maintains them all, so I see no need to have multiple users)</p>

<p>I'm using vsftpd, but am getting an error 500 OOPS: cannot change directory:/home/foo</p>

<p>SELinux is disabled</p>

<p>vsftpd settings are: </p>

<pre><code>anonymous_enable=NO
local_enable=YES
write_enable=YES
local_umask=022
dirmessage_enable=YES
xferlog_enable=YES
chroot_local_user=NO # me experimenting!
listen=YES

pam_service_name=vsftpd
userlist_enable=YES
tcp_wrappers=YES
</code></pre>

<p>I've obviously missed a step or two - what are they?</p>

<p>EDIT I have now set up TLS, made it mandatory, and set the home directory for user foo to /var/www. Filezilla is logging in fine, but timing out while trying to retrieve a directory listing.</p>

<pre><code>Status: Connection established, waiting for welcome message...
Status: Initializing TLS...
Status: Verifying certificate...
Status: TLS connection established.
Status: Logged in
Status: Retrieving directory listing...
Command:    PWD
Response:   257 ""/var/www""
Command:    TYPE I
Response:   200 Switching to Binary mode.
Command:    PASV
Response:   227 Entering Passive Mode (103,194,112,34,77,0).
Command:    LIST
Error:  Connection timed out after 20 seconds of inactivity
Error:  Failed to retrieve directory listing
</code></pre>

<p>I set log_ftp_protocol=YES, but xferlog is empty.</p>
","<ftp><centos6><vsftpd>","2016-08-26 06:04:12"
"799253","Why there is sleep in log rotation script","<p>I have some problems with nginx log rotation - <code>nginx -s reopen</code> does not reopen the log file, this is why I am researching the topic.</p>

<p>In several places, I found this script:</p>

<pre><code>$ mv access.log access.log.0
$ kill -USR1 `cat master.nginx.pid`
$ sleep 1
$ gzip access.log.0    # do something with access.log.0
</code></pre>

<p>My question is why there is sleep after kill? It seems unnecessary, because even reopen slows, gzip will not be able to compress everything for 1 second anyway?</p>

<p>Do they do it this way in case file is very small, so no data to be lost?</p>
","<nginx><shell-scripting>","2016-08-26 07:04:21"
"799278","Unable to open port 81","<p>Im very new to apache2 stuff... and I'm learning about virtualhost, I tried putting my configuration as like this, the port doesn't open. Any idea what went wrong? Kindly guide me.</p>

<p><strong>000-default.conf</strong></p>

<pre><code>&lt;VirtualHost *:80&gt;
    # The ServerName directive sets the request scheme, hostname and port that
    # the server uses to identify itself. This is used when creating
    # redirection URLs. In the context of virtual hosts, the ServerName
    # specifies what hostname must appear in the request's Host: header to
    # match this virtual host. For the default virtual host (this file) this
    # value is not decisive as it is used as a last resort host regardless.
    # However, you must set it for any further virtual host explicitly.
    #ServerName www.example.com

    ProxyPass / http://127.0.0.1:5000/
        ProxyPassReverse / http://127.0.0.1:5000/
    #ProxyPass / http://127.0.0.1:5001/
        #ProxyPassReverse / http://127.0.0.1:5001/


    ServerAdmin webmaster@localhost
    DocumentRoot /var/www/html/poke1
    #DocumentRoot /var/www/html/poke


    # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,
    # error, crit, alert, emerg.
    # It is also possible to configure the loglevel for particular
    # modules, e.g.
    #LogLevel info ssl:warn

    ErrorLog ${APACHE_LOG_DIR}/error.log
    CustomLog ${APACHE_LOG_DIR}/access.log combined

    # For most configuration files from conf-available/, which are
    # enabled or disabled at a global level, it is possible to
    # include a line for only one particular virtual host. For example the
    # following line enables the CGI configuration for this host only
    # after it has been globally disabled with ""a2disconf"".
    #Include conf-available/serve-cgi-bin.conf
#RewriteEngine on
#RewriteCond %{SERVER_NAME} =abc.com
#RewriteRule ^ https://%{SERVER_NAME}%{REQUEST_URI} [END,QSA,R=permanent]
&lt;/VirtualHost&gt;

&lt;VirtualHost *:81&gt;
    # The ServerName directive sets the request scheme, hostname and port that
    # the server uses to identify itself. This is used when creating
    # redirection URLs. In the context of virtual hosts, the ServerName
    # specifies what hostname must appear in the request's Host: header to
    # match this virtual host. For the default virtual host (this file) this
    # value is not decisive as it is used as a last resort host regardless.
    # However, you must set it for any further virtual host explicitly.
    #ServerName www.example.com

    #ProxyPass / http://127.0.0.1:5000/
        #ProxyPassReverse / http://127.0.0.1:5000/
    ProxyPass / http://127.0.0.1:5001/
        ProxyPassReverse / http://127.0.0.1:5001/


    ServerAdmin webmaster@localhost
    #DocumentRoot /var/www/html/poke1
    DocumentRoot /var/www/html/poke


    # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,
    # error, crit, alert, emerg.
    # It is also possible to configure the loglevel for particular
    # modules, e.g.
    #LogLevel info ssl:warn

    ErrorLog ${APACHE_LOG_DIR}/error.log
    CustomLog ${APACHE_LOG_DIR}/access.log combined

    # For most configuration files from conf-available/, which are
    # enabled or disabled at a global level, it is possible to
    # include a line for only one particular virtual host. For example the
    # following line enables the CGI configuration for this host only
    # after it has been globally disabled with ""a2disconf"".
    #Include conf-available/serve-cgi-bin.conf
RewriteEngine on
RewriteCond %{SERVER_NAME} =abc.com
RewriteRule ^ https://%{SERVER_NAME}%{REQUEST_URI} [END,QSA,R=permanent]
&lt;/VirtualHost&gt;

# vim: syntax=apache ts=4 sw=4 sts=4 sr noet
</code></pre>

<p><strong>000-default-le-ssl.conf</strong></p>

<pre><code>&lt;IfModule mod_ssl.c&gt;
&lt;VirtualHost *:443&gt;
    # The ServerName directive sets the request scheme, hostname and port that
    # the server uses to identify itself. This is used when creating
    # redirection URLs. In the context of virtual hosts, the ServerName
    # specifies what hostname must appear in the request's Host: header to
    # match this virtual host. For the default virtual host (this file) this
    # value is not decisive as it is used as a last resort host regardless.
    # However, you must set it for any further virtual host explicitly.
    #ServerName www.example.com

    #ProxyPass / http://127.0.0.1:5000/
        #ProxyPassReverse / http://127.0.0.1:5000/
    ProxyPass / http://127.0.0.1:5001/
        ProxyPassReverse / http://127.0.0.1:5001/


    ServerAdmin webmaster@localhost
    #DocumentRoot /var/www/html/poke1
    DocumentRoot /var/www/html/poke

    # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,
    # error, crit, alert, emerg.
    # It is also possible to configure the loglevel for particular
    # modules, e.g.
    #LogLevel info ssl:warn

    ErrorLog ${APACHE_LOG_DIR}/error.log
    CustomLog ${APACHE_LOG_DIR}/access.log combined

    # For most configuration files from conf-available/, which are
    # enabled or disabled at a global level, it is possible to
    # include a line for only one particular virtual host. For example the
    # following line enables the CGI configuration for this host only
    # after it has been globally disabled with ""a2disconf"".
    #Include conf-available/serve-cgi-bin.conf
SSLCertificateFile /etc/letsencrypt/live/abc.com/cert.pem
SSLCertificateKeyFile /etc/letsencrypt/live/abc.com/privkey.pem
Include /etc/letsencrypt/options-ssl-apache.conf
ServerName abc.com
SSLCertificateChainFile /etc/letsencrypt/live/abc.com/chain.pem
&lt;/VirtualHost&gt;
# vim: syntax=apache ts=4 sw=4 sts=4 sr noet
&lt;/IfModule&gt;
</code></pre>

<p>Just incase if you didn't aware, when using port 80 it should go to 127.0.0.1:5000 else if port 81 it should go to 127.0.0.1:5001</p>
","<apache-2.2>","2016-08-26 10:11:51"
"873099","Loading files from different partitions","<p>I've been working on a chat engine with file streaming builded with socket.io and nodejs. Everything is working well, messages, notifications, chat rooms and even the file streaming but let's say that we have a server with only 1TB (250gb HDDx4) and my app is instaled in <code>C:/</code>, my friends can upload files in the App directory with nodejs and the path to load that file in chat box is something like <code>&lt;img src=""../.././private/filename.png""&gt;</code> . I don't know if it's the best method to do this but it's working.. Now let's say that we want to switch the partition for uploading in <code>F:/</code>, how can I load the file in chat box from there ? I can not put the absolute URL, e.g <code>&lt;img src=""F:/private/filename.png""&gt;</code> because it will search in the client/user computer in <code>F:/</code> partition, not in my server in <code>F:/</code> partition.. It's someone that can recommand me some methods/solutions for doing this ?</p>
","<windows-server-2008><mysql><node.js>","2017-09-11 18:24:16"
"939771","Install SSL Certificate for a DNS redirection","<p>We have an application developped on a internal server.
A DNS redirection to the livebox IP has been done for our customers to access a service online via OVH.
The online URL is client.lexcelera.com.</p>

<p>We have baught a SSL certificate for client.lexcelera.com but I don't know how to install it.</p>

<p>We are on a CentOs 6.4 OS with an AOL server.
We are using pound to set the ports.</p>

<p>Any idea how to install my certificate?</p>

<p><strong>Edit</strong></p>

<p>Here are the result of the host command:</p>

<pre><code>[root@OpenMat ~]# host client.lexcelera.com
client.lexcelera.com has address 80.15.156.1
</code></pre>

<p>but when I try on 80.15.156.1 I get:</p>

<pre><code>1.156.15.80.in-addr.arpa domain name pointer lneuilly-657-1-25-1.w80-15.abo.wanadoo.fr.
</code></pre>

<p>Here is my pound.cfg:</p>

<pre><code>User            ""pound""
Group           ""pound""
LogLevel        2

ListenHTTP
        Address 0.0.0.0
        Port    80
End

ListenHTTPS
        Address 0.0.0.0
        Port    443
        Cert    ""/etc/pound/localhost.pem""
End

# By default show the ""projop"" production server
Service
        BackEnd
                Address 0.0.0.0
                Port    8000
                TimeOut 600
        End
End
</code></pre>

<p>Thanks!</p>
","<domain-name-system><ssl-certificate><centos6><pound>","2018-11-13 08:50:55"
"1046190","How to configure subnets/VLANs to restrict access to WAN/other VLANs?","<p>I manage the networking for our (very!) small church as a volunteer. Currently everything is set up on a single /24 IPv4 subnet. I'm wanting to break these out into VLANs for increased security, and also implement IPv6 at the same time.</p>
<p>Our hardware is a MikroTik commercial-grade router (behind an AT&amp;T gateway with 5 static WAN IPs), a secondhand Netvanta 1534P PoE switch (plus a Unifi PoE switch some distance away), and some Unifi Wi-fi access points with Unifi Controller running on a Raspberry Pi. We have a Synology NAS which is exposed to the Internet and which serves as our email server and master DNS server. Users are two Windows PCs in secure locations (offices), two more PCs in non-secured locations (sound booth), and guest users on our guest access Wi-fi. We also have security cameras, a few IoT devices (thermostats), and VoIP telephones. Most everything is on wired Cat 5e cabling to a fairly secure server closet.</p>
<p>I have identified the following classes of devices, along with what access I think they should have. I'm asking for advice as to how to implement this setup, or recommendations to improve it:</p>
<ul>
<li>Devices with direct access from WAN: Email, DNS, &amp; Web server. Also Video station and similar on the NAS. This subnet should not be able to access other LAN subnets.</li>
<li>Control and management devices: Management ports for switches, routers, Unifi controller, and similar devices. Should be able to be accessed from secured PCs, but not from WAN (unless at some later date I implement a VPN...fingers crossed).</li>
<li>File sharing devices: All PCs, networked printers, and the NAS (it has 2 LAN ports which can be segregated). Should be able to share files and access as needed.</li>
<li>Secured PCs: Should be able to access any device on the LAN.</li>
<li>Non-secured PCs: Should be able to access the NAS as well as printers, etc., but should not be able to access control and management devices.</li>
<li>IoT devices: Should have access to WAN only; should not see any other network traffic.</li>
<li>Guest Wi-fi users: Should have access to WAN only; any access to NAS would be through the WAN-accessible port.</li>
<li>VoIP Phones: Should have their own subnet.</li>
<li>Security cameras: Should only be able to see the local port for the NAS, which acts as our camera controller and recorder. I don't want them phoning home to China every night.</li>
</ul>
<p>I'm not a professional by any means; I'm learning by doing. (The church is my training lab!) I'd like to know how to give as much protection as possible, especially in implementing IPv6...there are lots of people who'd like to hack a church (I could show you my mail server logs...). Any helpful information will be appreciated.</p>
","<security><routing><ipv6><vlan><subnet>","2020-12-15 05:51:07"
"799424","Moving to SSD storage from Platters: Where does RAID come in?","<p>When moving to SSD, do you keep RAID10?</p>

<p>We have a server with internal SAS disks in a big array:</p>

<p>16x600GB RAID 10</p>

<p>This gives us ~4.6TB of storage. It is fast (this is SQL Server). </p>

<p>Our SQL database is:</p>

<p>-- 600GB MDF
-- 200+GB/Day of log files
-- Heavily transactional
-- Heavy read and write workload</p>

<p>We are looking to move to SSD (for the usual reasons: Speed, speed, and performance). We expect to use Write Intensive 400GB SSDs. (They seem to be the best cost-capacity ratio)</p>

<p>We believe that on SSD, there is less need for RAID10 (a big reason for RAID10 is to boost throughput, but SSDs solve for that natively).</p>

<p>RAID5, with spinning disks, is a performance disaster for SQL (it only made sense when disks were super expensive).</p>

<p>In the Dell world, we would use the ""Write Intensive MLC"" SSDs.</p>

<p>But for SSD, what is the good way to go?</p>

<ul>
<li>RAID10?  Rather expensive on SSDs...</li>
<li>RAID5?  Is the write penalty manageable in an SSD environment?</li>
</ul>

<p>In all cases, we will have one or two hot spare SSDs in the chassis.</p>

<p>What is the appropriate approach?</p>

<p>-- We understand that RAID10 is the cadillac
-- We would prefer to reduce the device count (given that SSDs solve the performance issues that RAID10 is used to solve)
---- Reducing device count helps on the $ side, and on the expansion side (use fewer drive bays...)</p>

<p>But, will going to RAID5 on SSDs penalize us in some way? </p>

<p>-- We know that RAID5 performance, on platters, goes to hell when one drive dies.
-- And RAID5 has poor write performance: Is that hit coming from (a) the controller (calculating parity), or (b) from the need to wait for both writes (data block and parity block) to complete?  If it is from 'b' then the higher IOPS of SSD should solve, right?</p>
","<storage><ssd>","2016-08-26 23:38:32"
"1013793","I need assistance creating a reverse proxy for multiple ports on a Windows server using Nginx","<p>I have  never used Nginx before and have been tasked with configuring it as a reverse proxy to a website in the Production Lan. I have port 80 working but i also need port 443 and port 18081 to function as well to this same webserver. The Nginx server is a windows 2012 server with Mginx 1.17.9 running. I know this config is messy as i took one that was supposed to work for multiple ports and remarked out what i thought i didnt need in my situation. Any assistance would be appreciated in getting this to work to proxy all 3 ports to the inside server.  The URL that the outside users will be using is <a href=""https://website.domain.com/mydealer/#/login/QS36F"" rel=""nofollow noreferrer"">https://website.domain.com/mydealer/#/login/QS36F</a></p>

<pre><code>#user  nobody;
worker_processes  1;

#error_log  logs/error.log;
#error_log  logs/error.log  notice;
#error_log  logs/error.log  info;

#pid        logs/nginx.pid;

 events {
    worker_connections  1024;
}

http{
 server{
     listen 80;
      server_name website.domain.com;

     location / {
       proxy_pass http://1.2.3.4/;
      #proxy_redirect off;
      #proxy_set_header X-Real-IP $remote_addr;
      #proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      #proxy_set_header Host $http_host;
      #proxy_set_header X-Nginx-Proxy true;

      #error_page  404              /404.html;

      # redirect server error pages to the static page /50x.html
      #
      error_page   500 502 503 504  /50x.html;
      location = /50x.html {
          root   html;
      }
      # proxy the PHP scripts to Apache listening on 127.0.0.1:80
      #
      #location ~ \.php$ {
      #    proxy_pass   http://127.0.0.1;
      #}

      # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000
      #
      #location ~ \.php$ {
      #    root           html;
      #    fastcgi_pass   127.0.0.1:9000;
      #    fastcgi_index  index.php;
      #    fastcgi_param  SCRIPT_FILENAME  /scripts$fastcgi_script_name;
      #    include        fastcgi_params;
      #}

      # deny access to .htaccess files, if Apache's document root
      # concurs with nginx's one
      #
      #location ~ /\.ht {
      #    deny  all;
      #}
  }


  # another virtual host using mix of IP-, name-, and port-based configuration
  #
  #server {
    listen       18081;
    #    listen       somename:8080;
    server_name website.domain.com;

    location / {
    proxy_pass http://1.2.3.4/;
    #        root   html;
    #        index  index.html index.htm;
    #    }
  #}

  # HTTPS server
  #
  #server {
      listen       443;
      server_name website.domain.com;

      location / {
        proxy_pass https://1.2.3.4/;

      #    ssl_certificate      cert.pem;
      #    ssl_certificate_key  cert.key;

      #    ssl_session_cache    shared:SSL:1m;
      #    ssl_session_timeout  5m;

      #    ssl_ciphers  HIGH:!aNULL:!MD5;
      #    ssl_prefer_server_ciphers  on;

      #    location / {
      #        root   html;
      #        index  index.html index.htm;
      #    }
    #}
  }
}
</code></pre>
","<nginx><windows-server-2012-r2><reverse-proxy>","2020-04-23 14:29:46"
"1046319","Unattended Shutdown for Smart-UPS on Windows Server 2012","<p>We will have two smart APC ups. Each server is a VMware server with dual power supplies. How would I configure it so that the VMware server does an unattended safe shutdown only when the ups are both on battery and have less than 20 min battery life remaining?</p>
<p>Thanks</p>
","<windows-server-2012-r2><battery><apc-smart-ups>","2020-12-15 19:34:47"
"1013819","SSL and IIS ""Cannot find the original signer"" - p7b file","<p>I created a request for a wildcard certificate from my provider and was given .p7b file (PKCS#7 Binary). When importing this .p7b into IIS, I get ""<strong><em>Cannot Find the Original Signer</em></strong>"".  I have installed other certificates from same company with no issues but this wildcard certificate is giving me headaches. I feel like I'm missing something? </p>

<p>Through MMC I have added the certificate successfully and edited it to add a friendly name - a fix I read about - but it did not change anything. I still get the same error message. I can't seem to find anything else with enough info to get me going.</p>
","<ssl><iis><wildcard-subdomain>","2020-04-23 16:27:55"
"873341","Route to VPN server","<p>Something wrong with routing tables, cannot sort this out :( Need a hint, do not know where start to dig.</p>

<p>I have two openVPN clients - one is under Win10 wich works good, the second(DD-WRT) can't even ping the server.</p>

<p><strong>OpenVPN Server(TAP)</strong> </p>

<ul>
<li>Local network 10.1.0.1/24 </li>
<li>VPN network 10.7.0.1/24</li>
</ul>

<p><strong>Windows10</strong> (10.7.0.9)</p>

<pre><code>C:\Users&gt;route print
      0.0.0.0          0.0.0.0      10.144.78.1    10.144.78.171     35
     10.1.0.0    255.255.255.0         10.7.0.1         10.7.0.9     35
     10.7.0.0    255.255.255.0         On-link          10.7.0.9    291
     10.7.0.9  255.255.255.255         On-link          10.7.0.9    291
   10.7.0.255  255.255.255.255         On-link          10.7.0.9    291
  10.144.78.0    255.255.255.0         On-link     10.144.78.171    291
10.144.78.171  255.255.255.255         On-link     10.144.78.171    291
10.144.78.255  255.255.255.255         On-link     10.144.78.171    291
    127.0.0.0        255.0.0.0         On-link         127.0.0.1    331
    127.0.0.1  255.255.255.255         On-link         127.0.0.1    331
</code></pre>

<p><strong>DD-WRT</strong> (10.7.0.2)</p>

<pre><code>root@gw2:~# route
Kernel IP routing table
Destination Gateway Genmask Flags Metric Ref Use Iface
default 10.33.93.1 0.0.0.0 UG 0 0 0 vlan2
10.1.0.0 10.7.0.1 255.255.255.0 UG 0 0 0 tap1
10.2.0.0 * 255.255.255.0 U 0 0 0 br0
10.7.0.0 * 255.255.255.0 U 0 0 0 tap1
10.7.0.2 * 255.255.255.255 UH 0 0 0 tap1 &lt; (this one I just added by myself to make routing table is similar to Win. Normally, openVPN does not generate this route — anyway does not help)
10.33.93.0 * 255.255.255.0 U 0 0 0 vlan2
127.0.0.0 * 255.0.0.0 U 0 0 0 lo
169.254.0.0 * 255.255.0.0 U 0 0 0 br0

root@gw2:~# traceroute 10.1.0.1
traceroute to 10.1.0.1 (10.1.0.1), 30 hops max, 38 byte packets
 1  10.7.0.2 (10.7.0.2)  2990.779 ms !H  2989.172 ms !H  2996.663 ms !H
root@gw2:~# traceroute 10.7.0.1
traceroute to 10.7.0.1 (10.7.0.1), 30 hops max, 38 byte packets
 1  10.7.0.2 (10.7.0.2)  2999.152 ms !H  2996.294 ms !H  2996.662 ms !H
root@gw2:~# traceroute 10.7.0.9
traceroute to 10.7.0.9 (10.7.0.9), 30 hops max, 38 byte packets
 1  10.7.0.2 (10.7.0.2)  2993.690 ms !H  2995.924 ms !H  2996.669 ms !H
</code></pre>

<p><strong>Windows10 OpenVPN config</strong></p>

<pre><code>proto tcp-client
remote XXX 1194
dev tap

nobind
persist-key

tls-client
ca ca.crt
cert da2.crt
key da2.key

verb 3
ns-cert-type server
cipher AES-256-CBC
auth SHA1
pull

auth-user-pass auth.cfg

route 10.1.0.0 255.255.255.0
</code></pre>

<p><strong>DD-WRT OpenVPN config</strong></p>

<pre><code>root@gw2:~# ps w | grep openvpn
31434 root      3448 S    openvpn --config /tmp/openvpncl/openvpn.conf --route-up /tmp/openvpncl/route-up.sh --down-pre /tmp/openvpncl/route-down.sh --daemon

root@gw2:~# cat /tmp/openvpncl/openvpn.conf 
ca /tmp/openvpncl/ca.crt
cert /tmp/openvpncl/client.crt
key /tmp/openvpncl/client.key
management 127.0.0.1 16
management-log-cache 100
verb 3
mute 3
syslog
writepid /var/run/openvpncl.pid
client
resolv-retry infinite
nobind
persist-key
persist-tun
script-security 2
dev tap1
proto tcp-client
cipher aes-128-cbc
auth sha1
auth-user-pass /tmp/openvpncl/credentials
remote XXX 1194
comp-lzo no
tun-mtu 1500
mtu-disc yes
route 10.1.0.0 255.255.255.0
#ping 20
#ping-restart 60

root@gw2:~# cat /tmp/openvpncl/route-up.sh
#!/bin/sh
iptables -D POSTROUTING -t nat -o tap1 -j MASQUERADE
iptables -I POSTROUTING -t nat -o tap1 -j MASQUERADE
iptables -D INPUT -i tap1 -j ACCEPT
iptables -I INPUT -i tap1 -j ACCEPT
</code></pre>

<p><strong>UPD1</strong>
observing with tcpdump:</p>

<pre><code>root@gw2:~# tcpdump -i lo icmp
15:42:14.922679 IP 10.7.0.2 &gt; 10.7.0.2: ICMP host 10.1.0.1 unreachable, length 46
15:42:17.920105 IP 10.7.0.2 &gt; 10.7.0.2: ICMP host 10.1.0.1 unreachable, length 46
15:42:20.917525 IP 10.7.0.2 &gt; 10.7.0.2: ICMP host 10.1.0.1 unreachable, length 46
</code></pre>

<p>Looks like infinite loop for some reason. Jumps from 10.7.0.2 to 10.7.0.2. Which hop has to be next? 10.7.0.1?</p>
","<routing><openvpn><route>","2017-09-12 20:03:51"
"873351","Nginx won't restart. Any ideas?","<p>I am trying to restart nginix on Ubuntu 16.04, but I keep getting these error messages. Please note that I am beginner-intermediate at navigating through the terminal, or through Ubuntu. Please help. :(</p>

<p><strong>sudo systemctl restart nginx.service</strong></p>

<p><em>Job for nginx.service failed because the control process exited with error code. See ""systemctl status nginx.service"" and ""journalctl -xe"" for details.</em></p>

<p><strong>sudo systemctl status nginx.service</strong></p>

<pre><code>*● nginx.service - A high performance web server and a reverse proxy server
   Loaded: loaded (/lib/systemd/system/nginx.service; enabled; vendor preset: enabled)
   Active: failed (Result: exit-code) since Tue 2017-09-12 21:38:46 UTC; 19s ago
  Process: 3760 ExecStartPre=/usr/sbin/nginx -t -q -g daemon on; master_process on; (code=exited, status=1/FAILURE)
Sep 12 21:38:46 vultr.guest systemd[1]: Stopped A high performance web server and a reverse proxy server.
Sep 12 21:38:46 vultr.guest systemd[1]: Starting A high performance web server and a reverse proxy server...
Sep 12 21:38:46 vultr.guest nginx[3760]: nginx: [emerg] unknown directive ""6H%WO5!JM&amp;Woj!RpOMIS^fPkbs&amp;$!PEYqhU6zPlC6&amp;uhMdo99q&amp;NKvpwvn9rYzMkaQ27CgU!fkS20zvgto@1adbJ#6*8m^TIL*MX"" in /etc/nginx/sites-enabled/mastodon:17
Sep 12 21:38:46 vultr.guest nginx[3760]: nginx: configuration file /etc/nginx/nginx.conf test failed
Sep 12 21:38:46 vultr.guest systemd[1]: nginx.service: Control process exited, code=exited status=1
Sep 12 21:38:46 vultr.guest systemd[1]: Failed to start A high performance web server and a reverse proxy server.
Sep 12 21:38:46 vultr.guest systemd[1]: nginx.service: Unit entered failed state.
Sep 12 21:38:46 vultr.guest systemd[1]: nginx.service: Failed with result 'exit-code'.*

**sudo journalctl -xe**

*Sep 12 21:20:43 vultr.guest sudo[3504]: pam_unix(sudo:session): session closed for user root
Sep 12 21:21:25 vultr.guest ntpd[1102]: 51.15.2.109 local addr 45.32.233.146 -&gt; &lt;null&gt;
Sep 12 21:22:11 vultr.guest sudo[3558]:     root : TTY=pts/0 ; PWD=/root ; USER=root ; COMMAND=/bin/systemctl status nginx.service
Sep 12 21:22:11 vultr.guest sudo[3558]: pam_unix(sudo:session): session opened for user root by root(uid=0)
Sep 12 21:22:11 vultr.guest sudo[3558]: pam_unix(sudo:session): session closed for user root
Sep 12 21:22:21 vultr.guest sshd[3561]: Did not receive identification string from 62.75.190.193
Sep 12 21:23:54 vultr.guest sudo[3574]:     root : TTY=pts/0 ; PWD=/root ; USER=root ; COMMAND=/bin/journalctl -xe
Sep 12 21:23:54 vultr.guest sudo[3574]: pam_unix(sudo:session): session opened for user root by root(uid=0)
Sep 12 21:37:35 vultr.guest sudo[3720]:     root : TTY=pts/0 ; PWD=/root ; USER=root ; COMMAND=/bin/systemctl restart nginx.service
Sep 12 21:37:35 vultr.guest sudo[3720]: pam_unix(sudo:session): session opened for user root by root(uid=0)
Sep 12 21:37:35 vultr.guest systemd[1]: Stopped A high performance web server and a reverse proxy server.
-- Subject: Unit nginx.service has finished shutting down
-- Defined-By: systemd
-- 
-- Unit nginx.service has finished shutting down.
Sep 12 21:37:35 vultr.guest systemd[1]: Starting A high performance web server and a reverse proxy server...
-- Subject: Unit nginx.service has begun start-up
-- Defined-By: systemd
-- 
-- Unit nginx.service has begun starting up.
Sep 12 21:37:35 vultr.guest nginx[3723]: nginx: [emerg] unknown directive ""6H%WO5!JM&amp;Woj!RpOMIS^fPkbs&amp;$!PEYqhU6zPlC6&amp;uhMdo99q&amp;NKvpwvn9rYzMkaQ27CgU!fkS20zvgto@1adbJ#6*8m^TIL*MX"" in /etc/nginx/sites-enabled/mastodon:17
Sep 12 21:37:35 vultr.guest nginx[3723]: nginx: configuration file /etc/nginx/nginx.conf test failed
Sep 12 21:37:35 vultr.guest systemd[1]: nginx.service: Control process exited, code=exited status=1
Sep 12 21:37:35 vultr.guest systemd[1]: Failed to start A high performance web server and a reverse proxy server.
-- Subject: Unit nginx.service has failed
-- Defined-By: systemd
-- 
-- Unit nginx.service has failed.
-- 
-- The result is failed.
Sep 12 21:37:35 vultr.guest systemd[1]: nginx.service: Unit entered failed state.
Sep 12 21:37:35 vultr.guest systemd[1]: nginx.service: Failed with result 'exit-code'.
Sep 12 21:37:35 vultr.guest sudo[3720]: pam_unix(sudo:session): session closed for user root
Sep 12 21:37:57 vultr.guest su[3733]: Successful su for mastodon by root
Sep 12 21:37:57 vultr.guest su[3733]: + /dev/pts/0 root:mastodon
Sep 12 21:37:57 vultr.guest su[3733]: pam_unix(su:session): session opened for user mastodon by root(uid=0)
Sep 12 21:37:57 vultr.guest su[3733]: pam_systemd(su:session): Cannot create session: Already running in a session
Sep 12 21:38:46 vultr.guest sudo[3756]: mastodon : TTY=pts/0 ; PWD=/home/mastodon ; USER=root ; COMMAND=/bin/systemctl restart nginx.service
Sep 12 21:38:46 vultr.guest sudo[3756]: pam_unix(sudo:session): session opened for user root by root(uid=0)
Sep 12 21:38:46 vultr.guest systemd[1]: Stopped A high performance web server and a reverse proxy server.
-- Subject: Unit nginx.service has finished shutting down
-- Defined-By: systemd
-- 
-- Unit nginx.service has finished shutting down.
Sep 12 21:38:46 vultr.guest systemd[1]: Starting A high performance web server and a reverse proxy server...
-- Subject: Unit nginx.service has begun start-up
-- Defined-By: systemd
-- 
-- Unit nginx.service has begun starting up.
Sep 12 21:38:46 vultr.guest nginx[3760]: nginx: [emerg] unknown directive ""6H%WO5!JM&amp;Woj!RpOMIS^fPkbs&amp;$!PEYqhU6zPlC6&amp;uhMdo99q&amp;NKvpwvn9rYzMkaQ27CgU!fkS20zvgto@1adbJ#6*8m^TIL*MX"" in /etc/nginx/sites-enabled/mastodon:17
Sep 12 21:38:46 vultr.guest nginx[3760]: nginx: configuration file /etc/nginx/nginx.conf test failed
Sep 12 21:38:46 vultr.guest systemd[1]: nginx.service: Control process exited, code=exited status=1
Sep 12 21:38:46 vultr.guest systemd[1]: Failed to start A high performance web server and a reverse proxy server.
-- Subject: Unit nginx.service has failed
-- Defined-By: systemd
-- 
-- Unit nginx.service has failed.
-- 
-- The result is failed.
Sep 12 21:38:46 vultr.guest systemd[1]: nginx.service: Unit entered failed state.
Sep 12 21:38:46 vultr.guest systemd[1]: nginx.service: Failed with result 'exit-code'.
Sep 12 21:38:46 vultr.guest sudo[3756]: pam_unix(sudo:session): session closed for user root
Sep 12 21:39:05 vultr.guest sudo[3770]: mastodon : TTY=pts/0 ; PWD=/home/mastodon ; USER=root ; COMMAND=/bin/systemctl status nginx.service
Sep 12 21:39:05 vultr.guest sudo[3770]: pam_unix(sudo:session): session opened for user root by root(uid=0)
Sep 12 21:39:05 vultr.guest sudo[3770]: pam_unix(sudo:session): session closed for user root
Sep 12 21:41:11 vultr.guest sudo[3775]: mastodon : TTY=pts/0 ; PWD=/home/mastodon ; USER=root ; COMMAND=/bin/journalctl -xe
Sep 12 21:41:11 vultr.guest sudo[3775]: pam_unix(sudo:session): session opened for user root by root(uid=0)*
lines 1932-2004/2004 (END)
</code></pre>
","<nginx>","2017-09-12 21:34:20"
"1046437","mysql --innodb_file_per_table","<p>I have a simple question regarding access to mysql via command.
When I tried to access it as a root it bounce back a message:</p>
<pre><code>mysql: unknown option '--innodb_file_per_table'
</code></pre>
<p>How would it get it back to work?</p>
<hr />
<p>Thanks Rick.
I have managed gain access and the server is working what worries me is bunch of errors during start/stop/restart</p>
<p>error.log</p>
<pre><code>2020-12-17 19:16:40 0 [Note] InnoDB: Using Linux native AIO
2020-12-17 19:16:40 0 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
2020-12-17 19:16:40 0 [Note] InnoDB: Uses event mutexes
2020-12-17 19:16:40 0 [Note] InnoDB: Compressed tables use zlib 1.2.11
2020-12-17 19:16:40 0 [Note] InnoDB: Number of pools: 1
2020-12-17 19:16:40 0 [Note] InnoDB: Using SSE2 crc32 instructions
2020-12-17 19:16:40 0 [Note] InnoDB: Initializing buffer pool, total size = 128M, instances = 1, chunk size = 128M
2020-12-17 19:16:40 0 [Note] InnoDB: Completed initialization of buffer pool
2020-12-17 19:16:40 0 [Note] InnoDB: If the mysqld execution user is authorized, page cleaner thread priority can be changed. See the man page of setpriority       ().
2020-12-17 19:16:40 0 [Note] InnoDB: 128 out of 128 rollback segments are active.
2020-12-17 19:16:40 0 [Note] InnoDB: Creating shared tablespace for temporary tables
2020-12-17 19:16:40 0 [Note] InnoDB: Setting file './ibtmp1' size to 12 MB. Physically writing the file full; Please wait ...
2020-12-17 19:16:40 0 [Note] InnoDB: File './ibtmp1' size is now 12 MB.
2020-12-17 19:16:40 0 [Note] InnoDB: 10.3.27 started; log sequence number 8459955162; transaction id 23688770
2020-12-17 19:16:40 0 [Note] InnoDB: Loading buffer pool(s) from /var/lib/mysql/ib_buffer_pool
2020-12-17 19:16:40 0 [Note] Plugin 'FEEDBACK' is disabled.
2020-12-17 19:16:40 0 [Note] Server socket created on IP: '127.0.0.1'.
2020-12-17 19:16:40 0 [Note] Reading of all Master_info entries succeeded
2020-12-17 19:16:40 0 [Note] Added new Master_info '' to hash table
2020-12-17 19:16:40 0 [Note] /usr/sbin/mysqld: ready for connections.
Version: '10.3.27-MariaDB-0+deb10u1'  socket: '/run/mysqld/mysqld.sock'  port: 3306  Debian 10
2020-12-17 19:16:40 0 [Note] InnoDB: Buffer pool(s) load completed at 201217 19:16:40
</code></pre>
","<mysql><root>","2020-12-16 13:50:39"
"940123","Powershell - Zip only files not directory","<p>I'm using a script from site <a href=""http://www.technologytoolbox.com/blog/jjameson/archive/2012/02/28/zip-a-folder-using-powershell.aspx"" rel=""nofollow noreferrer"">link</a>. But it zips the directory. I rather want it to zip only files inside that directory and the zipfile name should be the directory.zip.</p>

<p>Here is the ZipFolder function. Can someone tell me what needs to be changed in order to zip only files inside a directory?</p>

<pre><code>function ZipFolder(
    [IO.DirectoryInfo] $directory)
{
    If ($directory -eq $null)
    {
        Throw ""Value cannot be null: directory""
    }

    Write-Host (""Creating zip file for folder ("" + $directory.FullName + "")..."")

    [IO.DirectoryInfo] $parentDir = $directory.Parent

    [string] $zipFileName

    If ($parentDir.FullName.EndsWith(""\"") -eq $true)
    {
        # e.g. $parentDir = ""C:\""
        $zipFileName = $parentDir.FullName + $directory.Name + "".zip""
    }
    Else
    {
        $zipFileName = $parentDir.FullName + ""\"" + $directory.Name + "".zip""
    }

    If (Test-Path $zipFileName)
    {
        Throw ""Zip file already exists ($zipFileName).""
    }

    Set-Content $zipFileName (""PK"" + [char]5 + [char]6 + (""$([char]0)"" * 18))

    $shellApp = New-Object -ComObject Shell.Application
    $zipFile = $shellApp.NameSpace($zipFileName)

    If ($zipFile -eq $null)
    {
        Throw ""Failed to get zip file object.""
    }

    [int] $expectedCount = (Get-ChildItem $directory -Force -Recurse).Count
    $expectedCount += 1 # account for the top-level folder

    $zipFile.CopyHere($directory.FullName)

    # wait for CopyHere operation to complete
    WaitForZipOperationToFinish $zipFile $expectedCount

    Write-Host -Fore Green (""Successfully created zip file for folder ("" `
        + $directory.FullName + "")."")
}

#Remove-Item ""C:\NotBackedUp\Fabrikam.zip""

[IO.DirectoryInfo] $directory = Get-Item ""D:\tmp""
ZipFolder $directory
</code></pre>
","<powershell>","2018-11-15 08:26:08"
"940129","setting umask for a directory so that all directories, executable file(.sh , .cmd, .bat) are 750 and regular file 640","<p>need to create and change existing file and directory such that all directories and executable files(*.sh, *.bat, *.cmd ..) are 750 and regular file are 640 . I need to this in shell and python both.</p>

<p>I like to set umask to 027 while default is 022 for existing directory .... can't change default umask. Basically need to set umask directory specific</p>
","<command-line-interface><file-permissions><user-permissions><umask><setfacl>","2018-11-15 09:30:19"
"799737","How to change name server and have least downtime?","<p>I am currently hosting on Bigrock and my domain name is also from Bigrock. 
I am facing many issues regarding the server memory and I need to conduct an important event on my website in about 50 hours for which there are registrations going on. I have found a better plan that can solve my problems on Bluehost. 
The problem is that updating the name server will take a lot of time which I cannot afford to lose. Is it possible that I can add the name server from bluehost in the existing list of name servers so that all can be up simultaneously. 
Or is there some way out that the site's downtime is reduced as much as it can like I might be running a copy while the other one is being configured.</p>
","<hosting><web-hosting><nameserver><hostname>","2016-08-29 13:01:38"
"873526","Refused to connect website hosted in Windows 7 Pro","<p>I know it is not ideal to have a website hosted in Windows 7 Pro but unfortunately I need to do some testing in a VMWare environment so after installing IIS 7 and creating a simple website I noticed I cannot access it outside the network.</p>

<p>I can see the website with /localhost/ and with the local IP address just fine. (I disabled Windows Firewall completely.) I can access the website from another computer inside the network. But when I try to see it from the external IP it refuses the connection immediately.</p>

<p>Am I missing something in the IIS configuration?</p>
","<networking><iis><windows-7>","2017-09-13 20:59:39"
"873588","NGINX + phpmyadmin","<p>I installed NGINX / mysql-server and phpmyadmin without an error - but when I am calling <a href=""http://localhost/phpmyadmin"" rel=""nofollow noreferrer"">http://localhost/phpmyadmin</a> it wants to download a binary file.</p>

<p>Why is that so? I can access the normal localhost NGINX default site. I linked the /usr/share/phpmyadmin folder with ln -s and also restarted my php7.0-fpm service as well as my nginx and mysql service.</p>

<pre><code>server {
    listen 80 default_server;
    listen [::]:80 default_server;


    root /var/www/html;

    index index.html index.htm index.nginx-debian.html;

    server_name localhost;

    location / {
        try_files $uri $uri/ =404;
    }


    location ~ \.php$ {
        include snippets/fastcgi-php.conf;

    #   # With php7.0-cgi alone:
    #   fastcgi_pass 127.0.0.1:9000;
    #   # With php7.0-fpm:
        fastcgi_pass unix:/run/php/php7.0-fpm.sock;
    }

    # deny access to .htaccess files, if Apache's document root
    # concurs with nginx's one
    #
    #location ~ /\.ht {
    #   deny all;
    #}


}


# Virtual Host configuration for example.com
#
# You can move that to a different file under sites-available/ and symlink that
# to sites-enabled/ to enable it.
#
#server {
#   listen 80;
#   listen [::]:80;
#
#   server_name example.com;
#
#   root /var/www/example.com;
#   index index.html;
#
#   location / {
#       try_files $uri $uri/ =404;
#   }
#}
</code></pre>

<p>EDIT: info.php with phpinfo(); in /var/www/html works fine also</p>
","<nginx><phpmyadmin>","2017-09-14 09:04:28"
"940383","DNS - Multiple records","<p>I'm having an issue changing the DNS records for my domain.</p>

<p>Currently my DNS configuration is as follows:</p>

<pre><code>domain.global      A      165.227.196.94
www.robbu.global      CNAME     robbu.global

inveniolive.robbu.global      A      206.189.206.222
www.inveniolive.robbu.global      CNAME      inveniolive.robbu.global
</code></pre>

<p>The DNS settings for the <code>165.227.196.94</code> IP address are working properly.</p>

<p>However for the <code>206.189.206.222</code>IP address are not working. I receive the following error message:</p>

<pre><code>This site can’t be reached
inveniolive.robbu.global’s server IP address could not be found.
ERR_NAME_NOT_RESOLVED
</code></pre>

<p>I can not have multiple A records?</p>

<p>How should I configure my DNS?</p>
","<domain-name-system><subdomain>","2018-11-16 16:47:13"
"800051","How to allow a DMZ server trusted remote access to LAN SQL instance?","<p>we have a setup where our web server is hosting on a machine in our DMZ, which is behind a public facing firewall with the typical ports 80 and 443 directing web traffic to our web server.  The DMZ is not domain controlled.</p>
<p>We also have several DB servers living inside our domain-controlled LAN, which is protected by a firewall from the DMZ, with the typical port 1433 open to allow the web site access to the SQL servers.</p>
<p>Currently, it is working, but only when our site's connection strings contain the user/pass to access the database.  This is not ideal.  What I'd like to do is to give the site's application pool identity access to the database from within SSMS and use a trusted connection in our connection string.  However, I am having trouble understanding what kind of user account would be able to do this.</p>
<p>First, I tried creating a domain user in the LAN.  Putting the DOMAIN\User, and password into the application pool identity in IIS, appeared to work correctly, and I was able to add the domain user as a SQL login as well.  However, when I tried to run the application and connect to the database, the following error occurred:</p>
<blockquote>
<p>SSPI handshake failed with error code 0x8009030c, state 14 while establishing a connection with integrated security; the connection has been closed. Reason: AcceptSecurityContext failed. The Windows error code indicates the cause of failure. The logon attempt failed [CLIENT: ]</p>
<p>Error: 18452, Severity: 14, State: 1.</p>
<p>Login failed. The login is from an untrusted domain and cannot be used with Windows authentication. [CLIENT: ]</p>
</blockquote>
<p>Using the web app to output its current user name revealed to me that the application identity was being changed from the domain account to a (non-existent) account on the web server, instead of the expected &quot;DOMAIN\WebAppUser&quot; I put into the application pool identity in IIS, it was outputting &quot;SERVER\WebAppUser&quot;, which is what I believe was the reason for the error message when connecting to the DB.</p>
<p>I then tried creating a local account on the web server and granting that access in SSMS.  However, SQL it is unable to find the user I created on the web server, I assume due to it's being outside of the LAN area the SQL server is in.</p>
<p>I have read other threads and articles, which indicate that we may need to set up a trust relationship between the DMZ space and the LAN space, which would mean setting up a domain controller for our DMZ.  I would prefer to avoid the added cost/complexity of this solution.</p>
<p>Is there some way to grant a user the type of network access needed to accomplish this? Thanks.</p>
","<sql-server><iis-7.5>","2016-08-30 17:54:17"
"940387","Safari can't connect securely to our server? Where is SSL Cert validated?","<p><strong>Issue:</strong></p>

<p>Unable to access authentication site on our server because ""Safari couldn't establish a secure connection"" so i'm troubleshooting and trying to pin point where the problem lies.</p>

<p><strong>More details:</strong></p>

<p>On mac devices Safari can't establish a connection to our server through an HTTPS connection. However accessing through HTTP is possible. On the other hand Chrome no issue at all.</p>

<p>On iOS devices, Safari, Chrome, and Firefox cannot establish a connection to our server.</p>

<p>I checked with our cert issuer, Go Daddy and used their online tool to check if everything was implemented correctly and it comes back positive.</p>

<p>Looking into the logs i get</p>

<p>com.apple.WebKit.Networking: CFNetwork SSLHandshake failed(-9846)
com.apple.WebKit.Networking: CFNetwork SSLHandshake failed(-9802)
com.apple.WebKit.Networking: CFNetwork SSLHandshake failed(-9824)
com.apple.WebKit.Networking: NSURLConnection/CFURLConnection HTTP load failed (kCFStreamErrorDomainSSL, -9824)</p>

<p><strong>Question:</strong></p>

<p>Where is SSL Validation handled? OS or the browser or both?</p>

<p>My thought is it could be related to our recently issued SSL certificate. </p>
","<ssl><mac><iphone><apple><safari>","2018-11-16 16:59:28"
"1046794","Email ""submission"" and ""relay"" event","<p>I want to get to the bottom of the term email <em>&quot;submission&quot;</em> and <em>&quot;relay&quot;</em>. So what does this really mean?</p>
<p>In my  head I have two possible scenarios:</p>
<h2>SCENARIO 1</h2>
<p>What the terms mean is:</p>
<ul>
<li><em>&quot;relaying&quot;</em> ⟹ <em>&quot;event of sending email using SMTP protocol&quot;</em></li>
<li><em>&quot;submission&quot;</em> ⟹ <em>&quot;event of recieving using SMTP protocol&quot;</em></li>
</ul>
<p>In this scenario I have to always tell who in the email chain I am refering to.</p>
<h2>SCENARIO 2:</h2>
<p>What the terms mean is:</p>
<ul>
<li><em>&quot;relaying&quot;</em> ⟹ <em>&quot;event of email  passing through any device using SMTP protocol (email is recieved &amp; sent ie. forwarded)&quot;</em></li>
<li><em>&quot;submission&quot;</em> ⟹ <em>&quot;event of sending using SMTP protocol&quot;</em></li>
</ul>
<p>So which scenario is correct?</p>
","<smtp>","2020-12-18 18:14:26"
"1014325","How to schedule VMs in few specific host?","<p>I have 4 nodes, 1 controller nodes and 3 compute node.</p>

<p>I divined into 2 AZ, AZ1 (compute1) and AZ2(compute2, compute3)</p>

<p>I can use create instance w/ <code>--avaibilty-zone AZ1 (AZ2)</code> to make the instance build in compute1 or compute2/3</p>

<p>But the openstack always schedule VMs on compute3 why I set <code>--avability-zone AZ2</code> (3 compute nodes have like the same config (8GB ram, 4 core, HDD).</p>

<p>So are there anyway to make openstack schedule VMs on compute2 node too when I use <code>--avaibility-zone AZ2</code></p>

<p>I only can use <code>--avaibilty-zone AZ2:compute2:compute2</code> to make the instance create on it.</p>
","<openstack><scheduling><openstack-nova>","2020-04-27 02:20:53"
"873783","dont find ldap library for apache 2.4","<p>I am trying to configure Apache version 2.4.27 and enable ldap. I also install this library without error:</p>

<pre><code>./config --prefix=/opt/lib/openssl-1.1.0f shared

make depend, make, make install

CPPFLAGS=""-I/opt/lib/openssl-1.1.0f/include"" LDFLAGS=""-Lopt/lib/openssl-1.1.0f/lib"" ./configure --prefix=/opt/lib/openldap-2.4.45 --disable-slapd --with-tls=openssl

make depend, make, make install


./configure --prefix=/opt/lib/curl-7.54.1 --with-ssl=/opt/lib/openssl-1.1.0f

make, make install

./configure --prefix=/opt/lib/pcre-8.41

make, make install       

./configure CFLAGS=-DXML_POOR_ENTROPY --prefix=/opt/lib/expat-2.2.2

make, make install
</code></pre>

<p>Below is the command for configuring httpd -2.4.27:</p>

<pre><code>./configure --prefix=/opt/httpd-2.4.27 --with-included-apr --with-pcre=/opt/lib/pcre-8.41/ --with-ssl=/opt/lib/openssl-1.1.0f --enable-so --with-expat=/opt/lib/expat-2.2.2 --enable-suexec --with-openssl=/opt/lib/openssl-1.1.0f --with-ldap-include=/opt/lib/openldap-2.4.45/include --with-ldap-lib=/opt/lib/openldap-2.4.45/lib --with-crypto --with-ldap --enable-authnz-ldap
</code></pre>

<p>I put the APR(1.6.2) APR-util(1.6.0) files in the srclib directory. During the ./configure I have the following error:</p>

<pre><code>checking for ldap support...
  adding ""-I/opt/lib/openldap-2.4.45/include"" to APRUTIL_INCLUDES
  adding ""-L/opt/lib/openldap-2.4.45/lib"" to APRUTIL_LDFLAGS
checking for ldap_init in -lldap50... no
checking for ldap_init in -lldapssl41... no
checking for ldap_init in -lldapssl40... no
checking for ldap_init in -lldapssl30... no
checking for ldap_init in -lldapssl20... no
checking for ldap_init in -lldapsdk... no
checking for ldap_init in -lldapsdk... no
checking for ldap_init in -lldap... no
checking for ldap_init in -lldap... no
checking for ldap_init in -lldap... no
checking for ldap_init in -lldap... no
configure: error: could not find an LDAP library
configure failed for srclib/apr-util
</code></pre>

<p>without LDAP I don't have any errors.</p>

<p>any idea?</p>

<p>Thanks! </p>
","<linux><apache-2.4><redhat><ldap>","2017-09-15 07:30:46"
"873796","Running Jenkins with apache at /jenkins","<p>I need to have access at the <code>/jenkins</code> path to the <strong>Jenkins</strong> server.</p>

<p>This is what I have now in my apache configuration:</p>

<pre><code>ProxyPass /jenkins http://localhost:8080
ProxyPassReverse /jenkins http://localhost:8080
</code></pre>

<p>When I go to <code>example.com/jenkins</code>, <br/>I get redirected to <code>example.com/login?from=%2F</code>, <br/>instead of the expected <code>exmaple.com/jenkins/login?from=%2F</code>.</p>

<p>So is it possible <em>with <strong>apache</strong></em> to make all the requests coming from my <strong>Jenkins</strong> server, be of the form <code>example.com/jenkins/*</code> instead of <code>example.com/*</code>?</p>

<p>P.S.: I know it would be a far better practice create a new server, named something like <code>jenkins.example.com</code>, but that's not an option for me at the moment.</p>
","<apache-2.4><mod-rewrite><mod-proxy><jenkins><proxypass>","2017-09-15 09:30:34"
"940436","Why are primary network interfaces forced to inherit all the security groups of their EC2 instances?","<p>I'm testing a virtual load balancer which is running on an EC2 instance.  I'd like to be able to alter the setup and test the balancer under various conditions.  In order to simulate a physical load balancer I've assigned three network interfaces to the instance:</p>

<ol>
<li>External (Primary) - An exposed interface to the web.  The AWS rules will be set to allow all traffic to this interface.  The load balancer will set up its own IP/port rules to filter this traffic internally.  Connections to this interface will arrive from both inside and outside the VPC.</li>
<li>Management - A dedicated interface with rules set to allow HTTPS traffic to the Load balancer's web interface or SSH traffic to a management shell running on the load balancer.  Management connections will be established from computers which are outside the VPC.</li>
<li>Internal - The interface through which all instances behind the load balancer will connect.  Connections on this interface will be mapped to connections on the external interface by the load balancer.  Connections to the internal interface will only arrive from instances which are inside the VPC.</li>
</ol>

<p>In order to simulate physical connections I'd like to apply some AWS security group rules to these interfaces so that only connections between certain instances in the VPC are allowed (IE: they're plugged into each other).  In order to do this I've created a separate security group for each interface (internal, management, and external).</p>

<p>My understanding is that rules within a security group are applied to all inbound and outbound traffic on the interfaces which belong to them.  Since each interface in this setup serves a distinct role I'd like each interface to belong to one (<em>and only one</em>) security group.</p>

<p><strong>What's unclear to me is what it means for an instance itself (as opposed to its interfaces) to belong to a security group.</strong>  My thinking is that since the load balancer instance will be communicating on all three interfaces that it should belong to all three security groups.</p>

<p>Unfortunately, when I apply the internal and management security groups to the instance it forces the external interface to also be in the internal and management security groups.</p>

<p>Likewise, when I remove the internal and management security groups from the external interface it removes the instance from those security groups too.</p>

<p>Are security groups applied to an instance just a superfluous configuration shortcut (IE: only the security groups applied to the network interfaces actually matter), is there some other way to achieve the configuration that I want, or am I thinking about the problem wrong and there's a reason for AWS to prevent this configuration?</p>
","<amazon-web-services><amazon-ec2><security-groups>","2018-11-16 23:07:01"
"873806","Two ubuntu 16.04 server run unattended upgrade, one need restart.","<p>I have two <code>Ubuntu server 16.04.</code></p>
<p>I have enabled <code>unattended upgrade</code> for the <code>two servers</code> and the <code>same configuration.</code></p>
<p>He <code>upgrades one package</code> the same version into the two servers but <code>only one</code> he shows me the message that needs a <code>reboot</code>.</p>
<p>Anyone know why only one of the server need reboot?</p>
<blockquote>
<p>Unattended upgrade returned: True</p>
<p>Warning: A reboot is required to complete this upgrade.</p>
<p>Packages that were upgraded: A</p>
</blockquote>
","<linux><ubuntu><ubuntu-16.04><unattended>","2017-09-15 10:31:08"
"1046848","How do I change dovecot's imap and pop 'banner'?","<p>How do I change dovecot's imap and pop 'banner'?</p>
<p>When I do a telnet to my server using:</p>
<pre><code>me@my-server:~# telnet localhost 143
Trying 127.0.0.1...
Connected to localhost.
Escape character is '^]'.
* OK [CAPABILITY IMAP4rev1 SASL-IR LOGIN-REFERRALS ID ENABLE IDLE LITERAL+ STARTTLS AUTH=PLAIN AUTH=LOGIN] Dovecot (Ubuntu) ready.
</code></pre>
<p>How can I change <code>Dovecot (Ubuntu) ready</code> to <code>My Mail System Creative Name ready</code>?</p>
<p>If any of you guys wonder why, I usually do not want to be targeted by any attacks that exist for particular mail softwares, so I feel like hiding their name.</p>
<p>And also, why am I using port 143 for this example? I think telnet is unable to handle SSL/TLS, so I get this if I ping port 993, but everything is working properly though, if you are wondering:</p>
<pre><code>Trying 127.0.0.1...
Connected to localhost.
Escape character is '^]'.
</code></pre>
<p>And for POP, I get this, almost forgot the mention:</p>
<pre><code>Trying 127.0.0.1...
Connected to localhost.
Escape character is '^]'.
+OK Dovecot (Ubuntu) ready.
</code></pre>
<p>And I want to change <code>Dovecot (Ubuntu) ready.</code> to <code>My Creative Mail System Name ready.</code>, or to something more meaningful than my creative name.</p>
","<dovecot>","2020-12-19 06:50:51"
"1015113","dig: parse of /etc/resolv.conf failed","<p>I tried using dig and I got the error in the title.</p>

<p>My /etc/resolv.conf is this</p>

<pre><code>nameserver 67.207.67.2
nameserver 67.207.67.3

nameserver ns1.name.com
nameserver ns2.name.com
nameserver ns3.name.com
nameserver ns4.name.com
</code></pre>

<p>The first two are the default digital ocean dns, the other are for my domain registrar.</p>

<p>So I tried to add two cloudflare dns, and the file is this:</p>

<pre><code>nameserver 1.1.1.1
nameserver 1.0.0.1

nameserver 67.207.67.2
nameserver 67.207.67.3

nameserver ns1.name.com
nameserver ns2.name.com
nameserver ns3.name.com
nameserver ns4.name.com
</code></pre>

<p>After I did, dig still didn't work.</p>

<p>But if I comment the *name.com nameservers, dig works:</p>

<pre><code>nameserver 1.1.1.1
nameserver 1.0.0.1

nameserver 67.207.67.2
nameserver 67.207.67.3

#nameserver ns1.name.com
#nameserver ns2.name.com
#nameserver ns3.name.com
#nameserver ns4.name.com
</code></pre>

<p>The fact is that if I comment these *name.com nameservers, my site went (<strong>and still is, trying do fix</strong>) down after some minutes.</p>

<p>So how can I leave these *name.com nameservers and be able to use dig and nslookup and stuff like that?</p>

<p>EDIT:
users below noticed that I must have misconfigured something in my site. And it's true. I noticed that there is a ""domain"" setting in digitalocean panel, and I never configured that. Now I did it and also changed the nameservers of my registrar to the digitalocean nameserver. Now I should only wait for nameserver propagation I guess?</p>

<p>My <code>tcpdump -ni any port 53 | tee dns_problem.log</code> output as suggested by a user below:</p>

<pre><code>08:27:34.721547 IP MYSERVERIP.55951 &gt; 67.207.67.2.53: 20768+ AAAA? ams3.sonar.digitalocean.com. (45)
08:27:34.721634 IP MYSERVERIP.41382 &gt; 67.207.67.2.53: 28628+ A? ams3.sonar.digitalocean.com. (45)
08:27:34.722304 IP 67.207.67.2.53 &gt; MYSERVERIP.55951: 20768 0/1/0 (103)
08:27:34.722304 IP 67.207.67.2.53 &gt; MYSERVERIP.41382: 28628 1/0/0 A 5.101.110.176 (61)
08:29:34.732206 IP MYSERVERIP.44766 &gt; 67.207.67.2.53: 29521+ AAAA? ams3.sonar.digitalocean.com. (45)
08:29:34.732384 IP MYSERVERIP.45803 &gt; 67.207.67.2.53: 49118+ A? ams3.sonar.digitalocean.com. (45)
08:29:34.734239 IP 67.207.67.2.53 &gt; MYSERVERIP.44766: 29521 0/1/0 (103)
08:29:34.734239 IP 67.207.67.2.53 &gt; MYSERVERIP.45803: 49118 1/0/0 A 5.101.110.176 (61)
08:29:34.844794 IP 54.91.82.218.54035 &gt; MYSERVERIP.53: 23982+ ANY? www.example.com. (33)
08:31:34.740307 IP MYSERVERIP.54008 &gt; 67.207.67.2.53: 47094+ AAAA? ams3.sonar.digitalocean.com. (45)
08:31:34.740643 IP MYSERVERIP.49591 &gt; 67.207.67.2.53: 21439+ A? ams3.sonar.digitalocean.com. (45)
08:31:34.741079 IP 67.207.67.2.53 &gt; MYSERVERIP.54008: 47094 0/1/0 (103)
08:31:34.741079 IP 67.207.67.2.53 &gt; MYSERVERIP.49591: 21439 1/0/0 A 5.101.110.176 (61)
08:32:48.328903 IP 54.91.82.218.33095 &gt; MYSERVERIP.53: 53251+ A? www.example.com. (33)
08:33:34.748240 IP MYSERVERIP.33811 &gt; 67.207.67.2.53: 20882+ AAAA? ams3.sonar.digitalocean.com. (45)
08:33:34.748596 IP MYSERVERIP.40348 &gt; 67.207.67.2.53: 63964+ A? ams3.sonar.digitalocean.com. (45)
08:33:34.749127 IP 67.207.67.2.53 &gt; MYSERVERIP.33811: 20882 0/1/0 (103)
08:33:34.749127 IP 67.207.67.2.53 &gt; MYSERVERIP.40348: 63964 1/0/0 A 5.101.110.176 (61)
08:35:34.762328 IP MYSERVERIP.46593 &gt; 67.207.67.2.53: 52540+ AAAA? ams3.sonar.digitalocean.com. (45)
08:35:34.762875 IP MYSERVERIP.34757 &gt; 67.207.67.2.53: 23545+ A? ams3.sonar.digitalocean.com. (45)
08:35:34.763153 IP 67.207.67.2.53 &gt; MYSERVERIP.46593: 52540 0/1/0 (103)
08:35:34.763208 IP 67.207.67.2.53 &gt; MYSERVERIP.34757: 23545 1/0/0 A 5.101.110.176 (61)
08:37:34.772318 IP MYSERVERIP.60307 &gt; 67.207.67.2.53: 25440+ AAAA? ams3.sonar.digitalocean.com. (45)
08:37:34.772691 IP MYSERVERIP.35584 &gt; 67.207.67.2.53: 14199+ A? ams3.sonar.digitalocean.com. (45)
08:37:34.773173 IP 67.207.67.2.53 &gt; MYSERVERIP.60307: 25440 0/1/0 (103)
08:37:34.773225 IP 67.207.67.2.53 &gt; MYSERVERIP.35584: 14199 1/0/0 A 5.101.110.176 (61)
08:39:34.782271 IP MYSERVERIP.35598 &gt; 67.207.67.2.53: 17037+ AAAA? ams3.sonar.digitalocean.com. (45)
08:39:34.782652 IP MYSERVERIP.41388 &gt; 67.207.67.2.53: 46756+ A? ams3.sonar.digitalocean.com. (45)
08:39:34.783076 IP 67.207.67.2.53 &gt; MYSERVERIP.35598: 17037 0/1/0 (103)
08:39:34.783100 IP 67.207.67.2.53 &gt; MYSERVERIP.41388: 46756 1/0/0 A 5.101.110.176 (61)
08:41:34.790595 IP MYSERVERIP.37914 &gt; 67.207.67.2.53: 25940+ AAAA? ams3.sonar.digitalocean.com. (45)
08:41:34.790929 IP MYSERVERIP.42627 &gt; 67.207.67.2.53: 1440+ A? ams3.sonar.digitalocean.com. (45)
08:41:34.791391 IP 67.207.67.2.53 &gt; MYSERVERIP.37914: 25940 0/1/0 (103)
08:41:34.791493 IP 67.207.67.2.53 &gt; MYSERVERIP.42627: 1440 1/0/0 A 5.101.110.176 (61)
08:41:49.641648 IP MYSERVERIP.53649 &gt; 67.207.67.2.53: 5397+ [1au] A? google.com. (51)
08:41:49.642872 IP 67.207.67.2.53 &gt; MYSERVERIP.53649: 5397 6/0/1 A 108.177.126.138, A 108.177.126.102, A 108.177.126.100, A 108.177.126.139, A 108.177.126.101, A 108.177.126.113 (135)
08:43:34.799037 IP MYSERVERIP.53977 &gt; 67.207.67.2.53: 3711+ AAAA? ams3.sonar.digitalocean.com. (45)
08:43:34.799376 IP MYSERVERIP.37354 &gt; 67.207.67.2.53: 64810+ A? ams3.sonar.digitalocean.com. (45)
08:43:34.799854 IP 67.207.67.2.53 &gt; MYSERVERIP.53977: 3711 0/1/0 (103)
08:43:34.799869 IP 67.207.67.2.53 &gt; MYSERVERIP.37354: 64810 1/0/0 A 5.101.110.176 (61)
08:45:34.810213 IP MYSERVERIP.37036 &gt; 67.207.67.2.53: 49990+ AAAA? ams3.sonar.digitalocean.com. (45)
08:45:34.810328 IP MYSERVERIP.47914 &gt; 67.207.67.2.53: 4935+ A? ams3.sonar.digitalocean.com. (45)
08:45:34.811003 IP 67.207.67.2.53 &gt; MYSERVERIP.47914: 4935 1/0
</code></pre>
","<linux><domain-name-system><web-hosting><nameserver><resolv.conf>","2020-05-01 07:00:48"
"800437","setup ssl with nginx on linux","<p>I have this clients server where I would like to set up ssl on a site like this app..net</p>

<p>The server is running ubunto and written in laravel / php.</p>

<pre><code>server {
    server_name &lt;domain&gt;.net www.&lt;domain&gt;.net;
    return 301 https://app.&lt;domain&gt;.net$request_uri;
}

server {
    listen 443 ssl;
    server_name app.&lt;domain&gt;.net;

    root /home/&lt;domain&gt;/www/site/public;

    location / {
        index index.php;
        try_files $uri $uri/ /index.php?q=$uri&amp;$args;
    }

    error_page 404 /index.php;

    location ~ \.php? {
        include fastcgi_params;
        fastcgi_index index.php;
        fastcgi_pass unix:/var/run/php5-fpm.&lt;domain&gt;.sock;
        fastcgi_split_path_info ^(.+\.php)(/.+)$;
        fastcgi_param PATH_INFO $fastcgi_path_info;
        fastcgi_param PATH_TRANSLATED $document_root$fastcgi_path_info;
        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
    }

    ssl on;
    ssl_certificate /etc/nginx/ssl/&lt;domain&gt;/ssl-bundle.crt;
    ssl_certificate_key /etc/nginx/ssl/&lt;domain&gt;/&lt;domain&gt;.key;
    ssl_protocols TLSv1 TLSv1.1 TLSv1.2;
    ssl_ciphers 'ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:DHE-R$
    ssl_prefer_server_ciphers on;

    access_log /home/&lt;domain&gt;/logs/access.log;
    error_log /home/&lt;domain&gt;/logs/error.log;
}
</code></pre>

<p>This is my config file for my specifik site. Right now when I go to app..net I land on www..com, I have no idea what the problem is. </p>

<p>EDIT:</p>

<p>If i change the config file to the following:</p>

<pre><code>server {
    server_name &lt;domain&gt;.net www.&lt;domain&gt;.net;
    return 301 http://&lt;domain&gt;.net$request_uri;
}

server {
    server_name app.&lt;domain&gt;.net;

    root /home/&lt;domain&gt;/www/site/public;

    location / {
        index index.php;
        try_files $uri $uri/ /index.php?q=$uri&amp;$args;
    }

    error_page 404 /index.php;

    location ~ \.php? {
        include fastcgi_params;
        fastcgi_index index.php;
        fastcgi_pass unix:/var/run/php5-fpm.&lt;domain&gt;.sock;
        fastcgi_split_path_info ^(.+\.php)(/.+)$;
        fastcgi_param PATH_INFO $fastcgi_path_info;
        fastcgi_param PATH_TRANSLATED $document_root$fastcgi_path_info;
        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
    }

    ssl on;
    ssl_certificate /etc/nginx/ssl/&lt;domain&gt;/ssl-bundle.crt;
    ssl_certificate_key /etc/nginx/ssl/&lt;domain&gt;/&lt;domain&gt;.key;
    ssl_protocols TLSv1 TLSv1.1 TLSv1.2;
    ssl_ciphers 'ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:DHE-R$    ssl_prefer_server_ciphers on;

    access_log /home/&lt;domain&gt;/logs/access.log;
    error_log /home/&lt;domain&gt;/logs/error.log;
}
</code></pre>

<p>Then I end up on the right page but no https</p>
","<ubuntu><nginx><php><php-fpm>","2016-09-01 10:19:53"
"940759","Is it possible to recover Microsoft Office files from a flash drive when they were edited and saved on the flash drive directly?","<p>Several .docx and .xlsx documents were saved on a flash drive, edited in word and excel applications, and changes saved directly to that flash drive again.</p>

<p>These files, when opened, now contain only gibberish of the ""}eZ1Ѕqдњ]2^ХЭ*sЖЎfЮcнv1Т2TN"" type. Using different encodings to view the file produces only different sets of gibberish. The files have correct names, extensions, and seemingly correct sizes on the flash drive. </p>

<p>An attempt was made to recover the files on the PC where files were edited using word version control (it came up with no files) and recuva (it failed citing ""the system cannot find the path specified"").</p>

<p>Is it possible to somehow recover the contents of the files? </p>
","<microsoft-office><corruption><usb-flash-drive>","2018-11-19 19:26:12"
"873945","how can i clean the inodes safely - CentOS","<p>I have a problem with the server. My <em>inodes</em> are full and my web can not do anything, I have done several ways, and I am afraid of making mistakes</p>

<pre><code>df -i
</code></pre>

<p><img src=""https://i.sstatic.net/MI64y.png"" alt=""result of command""></p>

<pre><code>df -h
</code></pre>

<p><img src=""https://i.sstatic.net/Z8DaB.png"" alt=""enter image description here""></p>

<p>What is <em>/dev/ploop17501p1</em> and how do I clean it safely?</p>
","<centos>","2017-09-16 02:53:14"
"800458","Shoretel - Maximum length of name for Shoregear SG-50","<p>For a Shoregear SG-50, what is the maximum number of characters you can use for the name of the switch? We'd like to use 20 characters but before we do, are there any practical limitations of a name this long?</p>

<p>How many characters are allowed in this field:</p>

<p><img src=""https://i.sstatic.net/XdTLF.jpg"" alt=""How many characters are allowed in this field?""></p>
","<switch><voip><shoretel>","2016-09-01 12:35:21"
"1015141","MySQL 8 unusual behavior","<p>The project that I'm working on using database frequently and does heavy procedures.</p>
<p>I'm always monitoring the system. with 16GB of RAM and 8 CPU's system usually works fine and not reaching it's limits but MySQL goes away very frequently with different errors:</p>
<p><a href=""https://i.sstatic.net/w8Ro6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/w8Ro6.png"" alt=""enter image description here"" /></a></p>
<p>Here is MySQL 8 configuration</p>
<pre><code>   [mysqld]
    sql_mode=&quot;&quot;
    pid-file        = /var/run/mysqld/mysqld.pid
    socket          = /var/run/mysqld/mysqld.sock
    datadir         = /var/lib/mysql
    log-error       = /var/log/mysql/error.log
    log_bin_trust_function_creators = 1
    join_buffer_size = 256M
    innodb_buffer_pool_size = 16G
    innodb_log_file_size=4G
    innodb_buffer_pool_instances=12
    expire_logs_days=3
    long_query_time=5
    innodb_lock_wait_timeout=100
</code></pre>
<p>How can I prevent this? What am I doing wrong?</p>
<p>Here is error log: <a href=""https://gist.github.com/turalus/d581b46f349aa815ab49f486605c0061"" rel=""nofollow noreferrer"">https://gist.github.com/turalus/d581b46f349aa815ab49f486605c0061</a></p>
","<mysql><mysql8>","2020-05-01 11:22:38"
"874029","Not sending encrypted messages","<p>My mail server appears not to send encrypted mail.</p>

<p><a href=""https://i.sstatic.net/aar8I.png"" rel=""nofollow noreferrer"">Image of red padlock in Gmail indicating unencrypted</a></p>

<p>It seems like most of the information/tutorials focus on setting up TLS so that a client can connect, which I believe I have achieved. I've learned that in the config file, SMTPD is mainly focused on receiving mail. I believe I'm missing some SMTP parameters, have tried a few, but no luck.</p>

<p>This is from my postconf - n:</p>

<pre><code>alias_database = hash:/etc/aliases
alias_maps = hash:/etc/aliases
append_dot_mydomain = no
biff = no
inet_interfaces = all
inet_protocols = ipv4 
mailbox_size_limit = 0
mydestination = localhost
myhostname = xxxxxx.com
mynetworks = 127.0.0.0/8 [::ffff:127.0.0.0]/104 [::1]/128
myorigin = /etc/mailname
readme_directory = no
recipient_delimiter = +
relayhost =
smtpd_banner = $myhostname ESMTP $mail_name (Ubuntu)
smtpd_recipient_restrictions = permit_sasl_authenticated permit_mynetworks reject_unauth_destination
smtpd_relay_restrictions = permit_mynetworks permit_sasl_authenticated defer_unauth_destination
smtpd_sasl_auth_enable = yes
smtpd_sasl_path = private/auth
smtpd_sasl_type = dovecot
smtpd_tls_auth_only = yes
smtpd_tls_cert_file = /etc/letsencrypt/live/xxxxxxx/fullchain.pem
smtpd_tls_key_file = /etc/letsencrypt/live/xxxxxxxx/privkey.pem
smtpd_use_tls = yes
virtual_alias_maps = mysql:/etc/postfix/mysql-virtual-alias-maps.cf
virtual_mailbox_domains = mysql:/etc/postfix/mysql-virtual-mailbox-domains.cf
virtual_mailbox_maps = mysql:/etc/postfix/mysql-virtual-mailbox-maps.cf
virtual_transport = lmtp:unix:private/dovecot-lmtp
</code></pre>
","<ssl><postfix><smtp><email-server><ubuntu-16.04>","2017-09-16 19:59:31"
"940842","strange character ? added at the end of URL","<p>I have a very very strange thing only on some websites.
I have restarted the server where many websites are hosted (for maintenance reasons) and after reboot, some websites only have a <strong>?</strong> at the end of their URL.
At the end of website URL I get <strong>index.php?</strong> instead of index.php only previously.</p>

<p>Normally URL is <a href=""https://www.example.com/index.php"" rel=""nofollow noreferrer"">https://www.example.com/index.php</a> , but now suddenly is <a href=""https://www.example.com/"" rel=""nofollow noreferrer"">https://www.example.com/</a><strong>index.php?</strong></p>

<p>For info, before restart everything was fine.
Nothing has been changed (on nginx or admin tools file maker..nothing)</p>

<p>Do you have an idea why?
Do you think admin tools nginx file maker can be the root cause?</p>

<p>I don't understand why and how, as everything was ok before server reboot.</p>

<p>I'm using debian 9.5, nginx 1.15.5, PHP 7.2.11, MySQL 5.7.24</p>

<p>Below more infos about my Nginx conf files.</p>

<p>Main file <strong>nginx.conf</strong> located here : <strong>/etc/nginx/</strong></p>

<pre><code>user  www-data;
worker_processes  4;
pid        /var/run/nginx.pid;
load_module modules/ngx_http_modsecurity_module.so;

events {
worker_connections  1024;
}

http {
# Basic Settings
disable_symlinks off; 
include       /etc/nginx/mime.types;
default_type  application/octet-stream;
log_format  main  '$remote_addr - $remote_user [$time_local] ""$request"" '
                  '$status $body_bytes_sent ""$http_referer"" '
                  '""$http_user_agent"" ""$http_x_forwarded_for""';
access_log  /var/log/nginx/access.log  main;
error_log /var/log/nginx/error.log warn;
server_tokens off;
sendfile on;
tcp_nopush on;
#tcp_nodelay on;
server_names_hash_max_size 1024;
server_names_hash_bucket_size 64;
types_hash_max_size 2048;

# (default is 8k or 16k) The directive specifies the client request body buffer size.
# If the request body is more than the buffer, then the entire request body or some part is written in a temporary file.
client_body_buffer_size 16K;

# Directive sets the headerbuffer size for the request header from client. For the overwhelming 
# majority of requests a buffer size of 1K is sufficient. Increase this if you have a custom header
# or a large cookie sent from the client (e.g., wap client).
client_header_buffer_size 1k;

# Directive assigns the maximum accepted body size of client request, indicated by the line Content-Length
# in the header of request. If size is greater the given one, then the client gets the error 
# ""Request Entity Too Large"" (413). Increase this when you are getting file uploads via the POST method.
client_max_body_size 32m;

# Directive assigns the maximum number and size of buffers for large headers to read from client request. 
# By default the size of one buffer is equal to the size of page, depending on platform this either 4K or 8K, 
# if at the end of working request connection converts to state keep-alive, then these buffers are freed. 
# 2x1k will accept 2kB data URI. This will also help combat bad bots and DoS attacks.
large_client_header_buffers 4 8k;

# The first parameter assigns the timeout for keep-alive connections with the client. 
# The server will close connections after this time. The optional second parameter assigns 
# the time value in the header Keep-Alive: timeout=time of the response. This header can 
# convince some browsers to close the connection, so that the server does not have to. Without 
# this parameter, nginx does not send a Keep-Alive header (though this is not what makes a connection ""keep-alive"").
keepalive_timeout 300 300;

# Directive sets the read timeout for the request body from client. 
# The timeout is set only if a body is not get in one readstep. If after 
# this time the client send nothing, nginx returns error ""Request time out"" 
# (408). The default is 60.
client_body_timeout 600;

# Directive assigns timeout with reading of the title of the request of client. 
# The timeout is set only if a header is not get in one readstep. If after this
# time the client send nothing, nginx returns error ""Request time out"" (408).
client_header_timeout 600;

# Directive assigns response timeout to client. Timeout is established not on entire
## transfer of answer, but only between two operations of reading, if after this time
#  client will take nothing, then nginx is shutting down the connection.
send_timeout 600;

# Solve upstream sent too big header while reading response header from upstream
# http://wiki.nginx.org/HttpProxyModule
# This directive set the buffer size, into which will be read the first part of the response, obtained from the proxied server.
#  Default: 4k|8k
#proxy_buffer_size   128k;
# This directive sets the number and the size of buffers, into which will be 
read the answer, obtained from the proxied server. 
# By default, the size of one buffer is equal to the size of page. Depending 
on platform this is either 4K or 8K.
#proxy_buffers   4 256k;
#proxy_busy_buffers_size   256k;

proxy_connect_timeout 600;
proxy_send_timeout 600;
proxy_read_timeout 600;

fastcgi_buffers 8 128k;
fastcgi_buffer_size 256k;
fastcgi_read_timeout 600;
fastcgi_ignore_client_abort on;

gzip on;
gzip_vary on;
gzip_proxied any;
gzip_comp_level 6;
gzip_min_length 1000;
gzip_buffers 4 32k;
gzip_http_version 1.1;
gzip_types text/plain text/css application/xhtml application/json 
application/x-javascript text/xml application/xml application/xml+rss 
text/javascript application/x-font-ttf application/javascript font/eot 
font/opentype image/svg+xml image/x-icon;
#gzip_disable     ""MSIE [1-6]\."";
gzip_disable ""MSIE [1-6].(?!.*SV1)"";

include /etc/nginx/conf.d/*.conf;
}
</code></pre>

<p>File <strong>00-default.conf</strong> located here : <strong>/etc/nginx/conf.d/</strong></p>

<pre><code>server {
server_name _;
listen 80 default_server;
listen 443 ssl default_server;
ssl_certificate /etc/nginx/ssl/nginx.crt;
ssl_certificate_key /etc/nginx/ssl/nginx.key;
return 404;
}
</code></pre>

<p>File <strong>example.conf</strong> for each website hosted on the server located here : <strong>/etc/nginx/conf.d/</strong></p>

<pre><code>server {
listen 80;
listen [::]:80;
server_name www.example.fr example.fr;
return 301 https://$host$request_uri;
}

server {
listen 443 ssl http2;
listen [::]:443 ssl http2;
server_name www.example.fr example.fr;
root /home/example/www/;
index index.html index.htm index.php;

access_log /var/log/nginx/example.access_log;
error_log /var/log/nginx/example.error_log info;

location ~ \.php$ {
    fastcgi_pass unix:/var/run/php/php7.2-fpm-example.sock;
    fastcgi_index index.php;
    include fastcgi_params;
    fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
}

include /etc/nginx/conf/security.conf;

location ^~ /administrator {
    auth_basic ""Authentification Requise"";
    auth_basic_user_file /home/example/www/administrator/.htpasswd;
}

include /etc/nginx/conf/joomla.conf;

ssl_certificate /etc/letsencrypt/live/example.fr/fullchain.pem;
ssl_certificate_key /etc/letsencrypt/live/example.fr/privkey.pem;
ssl_trusted_certificate /etc/letsencrypt/live/example.fr/chain.pem;

include /etc/nginx/conf/ssl.conf;
}
</code></pre>

<p>File <strong>security.conf</strong> for global security settings located here : <strong>/etc/nginx/conf/</strong></p>

<pre><code>### Prevent access to this file
location = /nginx.conf {
log_not_found off;
access_log off;
return 404;
break;
}

######################################################################
## Protect against common file injection attacks
######################################################################
set $file_injection 0;
if ($query_string ~ ""[a-zA-Z0-9_]=http://"") {
set $file_injection 1;
}
if ($query_string ~ ""[a-zA-Z0-9_]=(\.\.//?)+"") {
set $file_injection 1;
}
if ($query_string ~ ""[a-zA-Z0-9_]=/([a-z0-9_.]//?)+"") {
set $file_injection 1;
}
if ($file_injection = 1) {
return 403;
break;
}
######################################################################
## Disable PHP Easter Eggs
######################################################################
if ($query_string ~ ""\=PHP[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}"") {
return 403;
break;
}
######################################################################
## Block access to configuration.php-dist and htaccess.txt
######################################################################
location = /configuration.php-dist {
log_not_found off;
access_log off;
return 404;
break;
}

location = /htaccess.txt {
log_not_found off;
access_log off;
return 404;
break;
}

location = /web.config {
log_not_found off;
access_log off;
return 404;
break;
}

location = /configuration.php {
log_not_found off;
access_log off;
return 404;
break;
}

location = /CONTRIBUTING.md {
log_not_found off;
access_log off;
return 404;
break;
}

location = /joomla.xml {
log_not_found off;
access_log off;
return 404;
break;
}

location = /LICENSE.txt {
log_not_found off;
access_log off;
return 404;
break;
}

location = /phpunit.xml {
log_not_found off;
access_log off;
return 404;
break;
}

location = /README.txt {
log_not_found off;
access_log off;
return 404;
break;
}

location = /web.config.txt {
log_not_found off;
access_log off;
return 404;
break;
}
## Protect against clickjacking
add_header X-Frame-Options SAMEORIGIN;

######################################################################
## Directory indices and no automatic directory listings
## Forces index.php to be read before the index.htm(l) files
## Also disables showing files in a directory automatically
######################################################################
index index.php index.html index.htm;
######################################################################
### Redirect non-www to www
#######################################################################
if ($host = 'alex-alu.fr' ) {
rewrite ^/(.*)$ $scheme://www.example.fr/$1 permanent;
}
######################################################################
## Disable following symlinks
######################################################################
disable_symlinks if_not_owner;
######################################################################
## Automatic compression of static resources
## Compress text, html, javascript, css, xml and other static resources
## May kill access to your site for old versions of Internet Explorer
######################################################################
# The following is the actual automatic compression setup
gzip            on;
gzip_vary       on;
gzip_comp_level 6;
gzip_proxied    expired no-cache no-store private auth;
gzip_min_length 1000;
gzip_http_version 1.1;
gzip_types      text/plain text/css application/xhtml+xml 
application/xml+rss application/rss+xml application/x-javascript 
application/javascript text/javascript application/json text/xml 
application/xml image/svg+xml;
gzip_buffers    16 8k;
gzip_disable ""MSIE [1-6]\.(?!.*SV1)"";
## HSTS Header - See 
http://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security
add_header Strict-Transport-Security max-age=31536000;
## Referrer-policy
add_header Referrer-Policy ""unsafe-url"";
## Disable HTTP methods TRACE and TRACK (protect against XST)
if ($request_method ~ ^(TRACE|TRACK)$ ) {
return 405;
}
## Reduce MIME type security risks
add_header X-Content-Type-Options ""nosniff"";
## Reflected XSS prevention
add_header X-XSS-Protection ""1; mode=block"";
## Prevent content transformation
add_header Cache-Control ""no-transform"";
# -- Socket settings, see http://wiki.nginx.org/HttpCoreModule
connection_pool_size        8192;
client_header_buffer_size   4k;
large_client_header_buffers 8 8k;
request_pool_size           8k;
# -- Performance, see http://wiki.nginx.org/HttpCoreModule
sendfile on;
sendfile_max_chunk 1m;
postpone_output 0;
tcp_nopush on;
tcp_nodelay on;
# -- Output buffering, see http://wiki.nginx.org/HttpCoreModule
output_buffers 8 32k;
# -- Filehandle Cache, useful when serving a large number of static files 
(Joomla! sites do that)
open_file_cache max=2000 inactive=20s;
open_file_cache_valid 30s;
open_file_cache_min_uses 2;
open_file_cache_errors on;
# -- Character encoding, see http://wiki.nginx.org/HttpCharsetModule
charset                 utf-8;
source_charset          utf-8;
# -- Security options, see http://wiki.nginx.org/HttpCoreModule
server_name_in_redirect off;
server_tokens off;
ignore_invalid_headers on;
# -- Maximum client body size set to 1 Gigabyte
client_max_body_size 1G;
set $common_exploit 0;
if ($query_string ~ ""proc/self/environ"") {
set $common_exploit 1;
}
if ($query_string ~ ""mosConfig_[a-zA-Z_]{1,21}(=|\%3D)"") {
set $common_exploit 1;
}
if ($query_string ~ ""base64_(en|de)code\(.*\)"") {
set $common_exploit 1;
}
if ($query_string ~ ""(&lt;|%3C).*script.*(&gt;|%3E)"") {
set $common_exploit 1;
}
if ($query_string ~ ""GLOBALS(=|\[|\%[0-9A-Z]{0,2})"") {
set $common_exploit 1;
}
if ($query_string ~ ""_REQUEST(=|\[|\%[0-9A-Z]{0,2})"") {
set $common_exploit 1;
}
if ($common_exploit = 1) {
return 403;
}
## Enable SEF URLs
location / {
try_files $uri $uri/ /index.php?$args;
}
location ~* /index.php$ {
fastcgi_pass unix:/var/run/php/php7.2-fpm-example.sock;
fastcgi_index index.php;
include /etc/nginx/fastcgi_params;
fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
break;
}
######################################################################
## Advanced server protection rules exceptions
######################################################################
location = /administrator/components/com_akeeba/restore.php {
fastcgi_pass unix:/var/run/php/php7.2-fpm-example.sock;
fastcgi_index index.php;
include /etc/nginx/fastcgi_params;
fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
break;
}
location = /administrator/components/com_admintools/restore.php {
fastcgi_pass unix:/var/run/php/php7.2-fpm-example.sock;
fastcgi_index index.php;
include /etc/nginx/fastcgi_params;
fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
break;
}
location = /administrator/components/com_joomlaupdate/restore.php {
fastcgi_pass unix:/var/run/php/php7.2-fpm-example.sock;
fastcgi_index index.php;
include /etc/nginx/fastcgi_params;
fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
break;
}
location = /robots.txt {
break;
}
location ~* ^/cache/.*\.php$
{
break;
}
location ~* ^/cache/.*$
{
break;
}
location ~* ^/cache\/nextend/.*\.php$
{
break;
}
location ~* ^/cache\/nextend/.*$
{
break;
}
location ~* ^/cache\/t3_assets/.*\.php$
{
break;
}
location ~* ^/cache\/t3_assets/.*$
{
break;
}
location ~* ^/cache\/t3_pages/.*\.php$
{
break;
}
location ~* ^/cache\/t3_pages/.*$
{
break;
}
location ~* ^/images/.*\.php$
{
break;
}
location ~* ^/images/.*$
{
break;
}
location ~* ^/t3\-assets/.*\.php$
{
break;
}
location ~* ^/t3\-assets/.*$
{
break;
}
location ~* ^/t3\-assets\/css/.*\.php$
{
break;
}
location ~* ^/t3\-assets\/css/.*$
{
break;
}
location ~* ^/t3\-assets\/js/.*\.php$
{
break;
}
location ~* ^/t3\-assets\/js/.*$
{
break;
}
######################################################################
## Advanced server protection
######################################################################
# Allow media files in select back-end directories
location ~* 
^/administrator/(components|modules|templates|images|plugins)/.*. (jpe|jpg|jpeg|jp2|jpe2|png|gif|bmp|css|js|swf|html|mpg|mp3|mpeg|mp4|avi|wav|ogg|ogv|xls|xlsx|doc|docx|ppt|pptx|zip|rar|pdf|xps|txt|7z|svg|odt|ods|odp|flv|mov|htm|ttf|woff|woff2|eot|JPG|JPEG|PNG|GIF|CSS|JS|TTF|WOFF|WOFF2|EOT)$ {
break;
}

# Allow access to the back-end index.php file
location = /administrator/index.php {
fastcgi_pass unix:/var/run/php/php7.2-fpm-example.sock;
fastcgi_index index.php;
include /etc/nginx/fastcgi_params;
fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
break;
}
location ~* ^/administrator$ {
return 301 /administrator/index.php?$args;
}
location ~* ^/administrator/$ {
return 301 /administrator/index.php?$args;
}

# Disable access to everything else.
location ~* /administrator.*$ {
# If it is a file, directory or symlink and I haven't deliberately
# enabled access to it, forbid any access to it!
if (-e $request_filename) {
    return 403;
}
# In any other case, just treat as a SEF URL
try_files $uri $uri/ /administrator/index.php?$args;
}
# Allow media files in select front-end directories
location ~* ^/(components|modules|templates|images|plugins|media|libraries|media/jui/fonts)/.*. (jpe|jpg|jpeg|jp2|jpe2|png|gif|bmp|css|js|swf|html|mpg|mp3|mpeg|mp4|avi|wav|ogg|ogv|xls|xlsx|doc|docx|ppt|pptx|zip|rar|pdf|xps|txt|7z|svg|odt|ods|odp|flv|mov|ico|htm|ttf|woff|woff2|eot|JPG|JPEG|PNG|GIF|CSS|JS|TTF|WOFF|WOFF2|EOT)$ {
break;
}

## Disallow front-end access for certain Joomla! system directories (unless 
access to their files is allowed above)
location ~* ^/includes/js/ {
return 403;
}
location ~* ^/(cache|includes|language|logs|log|tmp)/ {
return 403;
}
# Allow access to /
location ~* ^/$ {
return 301 /index.php?$args;
}

# Disable access to everything else.
location ~* ^/.*$ {
# If it is a file, directory or symlink and I haven't deliberately
# enabled access to it, forbid any access to it!
if (-e $request_filename) {
    return 403;
}
# In any other case, just treat as a SEF URL
try_files $uri $uri/ /index.php?$args;
}
</code></pre>

<p>Thanks
L.</p>
","<nginx><web-server><rewrite><web-hosting><url>","2018-11-20 07:52:08"
"1046964","Differences between VMware and XCP-ng Hypervisor","<p>I am currently researching XCP-ng hypervisor and I was wondering what are the main differences between the XCP-ng and VMware hypervisor.</p>
<p>As far as I can see the XCP-ng does not have high availability features as advanced as VMware's.</p>
<p>The VMware has a shadow VM for every primary VM on another host that has the a copy of RAM state of the primary VM, and when the host that is hosting the VM fails the shadow VM becomes primary and continues all operations seamlessly and we get no server down time.</p>
<p>But on the XCP-ng on host failure the VM reboots on the other host and you have server down time of 1 to 2 minutes. Did I get that right? What are some other differences?</p>
","<vmware-esxi><high-availability><xenserver><xcp-ng>","2020-12-20 18:41:44"
"874083","Restoring folder and files from Backblaze using Cloudberry","<p>I'm investigating alternative solutions to Crashplan. Currently I have Crashplan running headless on my Netgear NAS (ReadyNasOS 6.8).
I have installed Cloudberry (free version, 200GB datalimit) which is taking backups to BackBlaze B2 Cloud Storage (also a free account, as it is for testing only right now).</p>

<p>I have created a backup plan which backs up a testing folder to B2 Cloud Storage. This plan seems to be running just fine.</p>

<p>The only thing I don't seem to get working is the restore plan. I would like to restore a folder with some files in it but it doesnt work. The restore only needs to run once. I have deliberately deleted the folder on my NAS.</p>

<p>The backup job looks like this: ./cbb addRestorePlan -n ""RestoreTask"" -a ""B2"" -sy no -f ""/data/Documenten/TESTMAP/20170723_Fotos/"" -se yes -ol yes </p>

<p>I run the task manually once it is created: ./cbb plan -r ""RestoreTask""</p>

<p>The files are located at: Buckets / nas-ea-94-93 / CBB_nas-EA-93-94 / CBB_VOLUMES / 33ea9394:data / Documenten / TESTMAP / 20170723_Fotos </p>

<p>The files are located on my NAS at /data/Documenten/TESTMAP/20170723_Fotos</p>

<p>If I would get any output from Cloudberry, that would also be very handy</p>

<hr>

<p>I have deleted the folder from the source (my nas device) which I'm backing up to the B2 Cloud using cloudberry.</p>

<p>I think that this is the most realistic scenario (= accidentally deleting files from my NAS device, as this is my 'work' storage device).</p>
","<linux><backup><restore><cloudberry>","2017-09-17 10:07:32"
"1047010","Power-BI API reporting 403 unauthorized (while trying to fetch the groups) with the service principle","<p>Power-BI API is reporting <code>403 unauthorized</code> (while trying to fetch the groups) with the service principle.</p>
<p><img src=""https://i.sstatic.net/vCMET.png"" alt=""403 forbidden"" /></p>
","<azure><entra-id><azure-networking><azure-web-apps>","2020-12-21 09:27:52"
"1047011","PHP fsockopen giving error with SSL","<p>I am getting the error while connecting to fsockopen.</p>
<pre><code>echo $result = fsockopen('ssl://smtp.gmail.com', 465, $error_no, $error_message, 5);
//$result = fsockopen('tls://smtp.gmail.com', 587, $error_no, $error_message, 5);
if ($result === false) {
  echo &quot;error no: $error_no error message: $error_message&quot;;
  echo print_r($result, true);
} else {
  echo 'success';
}
</code></pre>
<blockquote>
<p>error no: 0 error message:</p>
</blockquote>
<p>I checked the manual <a href=""https://www.php.net/manual/en/function.fsockopen.php"" rel=""nofollow noreferrer"">here</a> which says</p>
<blockquote>
<p>If the value returned in errno is 0 and the function returned
false, it is an indication that the error occurred before the
connect() call. This is most likely due to a problem initializing the
socket.</p>
</blockquote>
<p>But I do not know how to resolve this. I have an AWS Ubuntu 18.04 instance and the website is https.</p>
<p>The problem is I am trying to set up gmail smtp. This settings works fine on local as well as other AWS instances where SSL isn't installed</p>
<p><strong>UPDATE 1</strong>:</p>
<pre><code>print_r(stream_get_transports());
</code></pre>
<p>The above statement gives</p>
<pre><code>Array (
    [0] =&gt; tcp
    [1] =&gt; udp
    [2] =&gt; unix
    [3] =&gt; udg
    [4] =&gt; ssl
    [5] =&gt; tls
    [6] =&gt; tlsv1.0
    [7] =&gt; tlsv1.1
    [8] =&gt; tlsv1.2 
)
</code></pre>
","<php><ssl><smtp><ubuntu-18.04>","2020-12-21 09:32:06"
"874145","In Windows 10 Pro, Unable to save a file in C drive?","<p>I got a new Windows 10 Pro laptop. When I tried to save a file in the C drive, I am getting Permission error. However, I am able to save a file in the Sub folder of c drive. And also , I am seeing an exclamation sign on the C drive.</p>

<p>What would be the reason ?</p>

<p><a href=""https://i.sstatic.net/iLwBH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/iLwBH.png"" alt=""enter image description here""></a></p>
","<windows><hard-drive><windows-10>","2017-09-18 01:00:22"
"941132","Word translator tool tracing","<p>If anyone (on our school wired network/Wi-fi) uses the Microsoft translator tool implemented in Word, would we be able to trace it or are we not able to see such activity going on in the network like we can trace students using google translate or some browser-based translator tool. Haven't been able to catch anyone using it yet so I'm wondering...</p>

<p>Hope you can help!</p>
","<microsoft-office-365><network-traffic>","2018-11-21 20:19:04"
"941191","File .pem Permission denied","<p>I have received from my partner a pfx file that I converted to pem for API</p>

<p>The idea is that I do not know where to put the ubuntu server and how to access the plug-in on the woocomerce</p>

<p>my log it's this</p>

<pre><code>#Next GuzzleHttp\Exception\RequestException: cURL error 58: could not load PEM client certificate, OpenSSL error error:0200100D:system library:fopen:Permission denied, (no key found, wrong pass phrase, or wrong file format?) in /var/www/html/magazin/wp-content/plugins/wc-moldovaagroindbank/vendor/guzzlehttp/guzzle/src/Exception/RequestException.php:51
Stack trace
0 /var/www/html/magazin/wp-content/plugins/wc-moldovaagroindbank/vendor/guzzlehttp/guzzle/src/RequestFsm.php(103): GuzzleHttp\Exception\RequestException::wrapException(Object(GuzzleHttp\Message\Request), Object(GuzzleHttp\Ring\Exception\RingException))
/var/www/html/magazin/wp-content/plugins/wc-moldovaagroindbank/vendor/guzzlehttp/guzzle/src/RequestFsm.php(132): GuzzleHttp\RequestFsm-&gt;__invoke(Object(GuzzleHttp\Transaction))
/var/www/html/magazin/wp-content/plugins/wc-moldovaagroindbank/vendor/react/promise/src/FulfilledPromise.php(25): GuzzleHttp\RequestFsm-&gt;GuzzleHttp\{closure}(Array)
/var/www/html/magazin/wp-content/plugins/wc-moldovaagroindbank/vendor/guzzlehttp/ringphp/src/Future/CompletedFutureValue.php(55): React\Promise\FulfilledPromise-&gt;then(Object(Closure), NULL, NULL)
/var/www/html/magazin/wp-content/plugins/wc-moldovaagroindbank/vendor/guzzlehttp/guzzle/src/Message/FutureResponse.php(43): GuzzleHttp\Ring\Future\CompletedFutureValue-&gt;then(Object(Closure), NULL, NULL)
/var/www/html/magazin/wp-content/plugins/wc-moldovaagroindbank/vendor/guzzlehttp/guzzle/src/RequestFsm.php(134): GuzzleHttp\Message\FutureResponse::proxy(Object(GuzzleHttp\Ring\Future\CompletedFutureArray), Object(Closure))
</code></pre>
","<openssl><curl>","2018-11-22 09:21:17"
"800875","ubuntu // running an ssh command as a specific user","<p>I'm looking to add github to known_hosts for the deploy user when the command is ran by the root user.</p>

<p>The command would be :</p>

<pre><code>ssh -T -o ""StrictHostKeyChecking no"" git@github.com
</code></pre>

<p>Using cloudinit, it runs initially with the root user, so this command in the cloudinit script will only allow the root user.</p>

<p>This is not tied cloudinit specifically, but is there a way/option to specify which user is running a specific ssh command ?</p>
","<ubuntu><ssh><cloud-init><known-hosts>","2016-09-03 12:16:03"
"800882","how to force OS using which alias ip address","<p>i have a server running debian with 2 ip address :</p>

<pre><code>eth3   44.44.44.1
eth3:0 44.44.44.2
</code></pre>

<p>the problem is my outband request goes with second ip address ""44.44.44.2""</p>

<p>how can i force OS to use primary ip address ?</p>
","<ssh><debian><routing><tcpip><ip-aliasing>","2016-09-03 13:44:46"
"1015560","SSH connection not established, but standard TCP/IP connection works","<p>I'm working on a custom yocto Linux for a Raspberry PI 3 and try to get the WIFI connection working with SSH. However when trying to connect from my PC (Ubuntu 19.10, SSH OpenSSH_8.0p1 Ubuntu-6build1, OpenSSL 1.1.1c  28 May 2019) to the PI on which Dropbear v2019.78 runs, the connection attempt times out. But only when I try this via SSH, and via <code>wlan0</code>. Other TCP/IP traffic works, and also using the same participants but with <code>eth0</code>. As this is for a robot, I would prefer to not use a tether though...</p>

<p>To try &amp; debug this, I </p>

<ul>
<li>enabled a serial console so I can work on the PI</li>
<li>disabled eth0</li>
<li>started a tcpdump on the PI (ip.host == 192.168.0.105)</li>
<li>started a tcpdump on the PC (ip.host == 192.168.0.106)</li>
<li>used a dirt-simple TCP/IP socket example written in Python (taken from <a href=""https://realpython.com/python-sockets/#echo-server"" rel=""nofollow noreferrer"">https://realpython.com/python-sockets/#echo-server</a>) to verify I can in fact communicate. The transmission is successful. I am aware that the example is lacking (no proper protocol etc), but that's not the point of it. It just works enough. The PI runs the server listening on port 2222.</li>
<li>attempted a SSH connection, it timed out.</li>
</ul>

<p>I filtered the resulting PCAP down to contain just TCP, as there is other information (e.g. Dropbox discovery) that I don't think matters and might potentially be information leaking. On the host side (enp4s0-tcp-and-pi.pcap) I also filtered with <code>ip.host == 192.168.0.105</code> to only contain any traffic to the PI.</p>

<p>Another note on my setup here: I use a TP-Link router which LAN ports the PC is connected to, and who provides the 2.4GHz WIFI for the PI. So both are part of the same subnet, and no special routing or anything is configured.</p>

<p>Also I stopped the dropbear daemon and adapted my Python code to use port 22. It works.</p>

<p>I'm only broadly aware of the inner workings of TCP, so I can't really make much sense of the things I see here. Any insights are more than welcome.</p>

<p><a href=""https://www.dropbox.com/s/5o4rqr5zdws2wq7/wlan0-tcp-only.pcap?dl=0"" rel=""nofollow noreferrer"">https://www.dropbox.com/s/5o4rqr5zdws2wq7/wlan0-tcp-only.pcap?dl=0</a></p>

<p><a href=""https://www.dropbox.com/s/amypjtk1nvja4qb/enp4s0-tcp-and-pi.pcap?dl=0"" rel=""nofollow noreferrer"">https://www.dropbox.com/s/amypjtk1nvja4qb/enp4s0-tcp-and-pi.pcap?dl=0</a></p>

<p>Output of <code>ssh -vvv root@192.168.0.102</code> (different IP due to DHCP):</p>

<pre><code>14:27 $ ssh -vvv root@192.168.0.102
OpenSSH_8.0p1 Ubuntu-6build1, OpenSSL 1.1.1c  28 May 2019
debug1: Reading configuration data /home/deets/.ssh/config
debug1: Reading configuration data /etc/ssh/ssh_config
debug1: /etc/ssh/ssh_config line 19: Applying options for *
debug2: resolve_canonicalize: hostname 192.168.0.102 is address
debug2: ssh_connect_direct
debug1: Connecting to 192.168.0.102 [192.168.0.102] port 22.
debug1: connect to address 192.168.0.102 port 22: Connection timed out
ssh: connect to host 192.168.0.102 port 22: Connection timed out
</code></pre>

<p>Originally asked on SO: <a href=""https://stackoverflow.com/questions/61576538/ssh-connection-not-established-but-standard-tcp-ip-connection-works?noredirect=1#comment108936218_61576538"">https://stackoverflow.com/questions/61576538/ssh-connection-not-established-but-standard-tcp-ip-connection-works?noredirect=1#comment108936218_61576538</a></p>
","<ssh><wireshark><tcpip><tcpdump><pcap>","2020-05-04 11:17:05"
"800887","How to set some kind of authentication for forword proxy in apache2?","<p>I have a VPS and have configured it to run apache2 as a forward proxy:</p>

<pre><code>&lt;IfModule mod_proxy.c&gt;
ProxyRequests On
&lt;Proxy vps_ip:80&gt;
Order deny,allow
Deny from all
Allow from all
&lt;/Proxy&gt;
&lt;/IfModule&gt;
</code></pre>

<p>This allows me to use my VPS from my home machine to download resources from the web, using curl:</p>

<pre><code>curl -x ""vps_ip:80""  http://www.someresource.com -o /tmp/mydown
</code></pre>

<p>However, I understand that this is considered an open proxy and I want to restrict its use just for my home machine. Apache configuration supports static IP whitelist for proxy clients like my home machine. However, my ISP assigns me different dynamic IPs every time I connect to the internet. </p>

<p>How can I prevent others from using my forward proxy on my VPS? </p>
","<proxy><authentication><curl><apache2>","2016-09-03 14:32:33"
"1047324","No FREE Tier compute instances available in GCP?","<p>&quot;Get free hands-on experience with popular products, including Compute Engine and Cloud Storage, up to monthly limits. <strong>These free services don't expire</strong>.&quot; - Team, first time I'm trying to create a free tier(F1-micro) VM instance and it's not available for selection. The min I can choose in the defined NA locations is e-micro instances. Is there any special buttons I have to use to select free tier instance?</p>
","<google-cloud-platform>","2020-12-23 14:10:08"
"800916","Linux , restrict users/processes to one folder, chroot in docker container?","<p>I'm looking for a way to run multiple python/php apps on one server. Each app in it's own /bob_app folder.</p>

<p>I need for users not to be able to run sth like:</p>

<pre><code>&gt;&gt;&gt; import glob
&gt;&gt;&gt; glob.glob(""/*"")
['/boot', '/cdrom', '/dev', '/lib64', '/run', '/initrd.img', '/sys', '/media', '/var', '/etc', '/srv', '/initrd.img.old', '/root', '/sbin', '/tmp', '/opt', '/vmlinuz', '/usr', '/home', '/lost+found', '/bin', '/proc', '/lib', '/mnt', '/vmlinuz.old']
</code></pre>

<p>Or the php etc equivalent. The apps should only see the contents of the folder they are running in and nothing above that.</p>

<p>Edit:
The apps are in docker containers and using a chroot environment within docker is not something I'm sure is the right thing to do.</p>
","<linux><permissions><chroot>","2016-09-03 19:47:08"
"1047422","Postfix removes ""friendly"" Email","<p>I'm running Postfix, Dovecot and Amavis on Debian. Somehow an email from a colleague wasn't delivered and I started to search the logs.</p>
<p>Here is what I've found. I don't know where the problem is.</p>
<ul>
<li>My domain: mydomain.com</li>
<li>Sender: name.name@foreigndomain.com</li>
<li>Receiver: me@mydomain.com (alias for the mail address: mk@mydomain.com)</li>
</ul>
<p>Log:</p>
<pre><code>Dec 22 12:50:38 mail postfix/smtpd[30263]: connect from localhost[127.0.0.1]
Dec 22 12:50:38 mail postfix/smtpd[30263]: 9FFCD1682106: client=localhost[127.0.0.1]
Dec 22 12:50:38 mail postfix/cleanup[30259]: 9FFCD1682106: message-id=&lt;4a3578f1b5824ec881e8936b8be7385d@mail.foreigndomain.com&gt;
Dec 22 12:50:38 mail postfix/smtpd[30263]: disconnect from localhost[127.0.0.1] ehlo=1 mail=1 rcpt=1 data=1 quit=1 commands=5
Dec 22 12:50:38 mail postfix/qmgr[1138]: 9FFCD1682106: from=&lt;name.name@foreigndomain.com&gt;, size=36743, nrcpt=1 (queue active)
Dec 22 12:50:38 mail amavis[29079]: (29079-02) Passed CLEAN {RelayedInbound}, [127.0.0.1] [90.xx.xx.xx] &lt;name.name@foreigndomain.com&gt; -&gt; &lt;me@mydomain.com&gt;, Message-ID: &lt;4a3578f1b5824ec881e8936b8be7385d@mail.foreigndomain.com&gt;, mail_id: 3WR65-x-uRfu, Hits: 0.002, size: 36262, queued_as: 9FFCD1682106, 1089 ms
Dec 22 12:50:38 mail postfix/lmtp[30260]: 80C6F1682101: to=&lt;me@mydomain.com&gt;, relay=127.0.0.1[127.0.0.1]:10024, delay=6.3, delays=5.1/0.02/0.04/1.1, dsn=2.0.0, status=sent (250 2.0.0 from MTA(smtp:[127.0.0.1]:10025): 250 2.0.0 Ok: queued as 9FFCD1682106)
Dec 22 12:50:38 mail postfix/qmgr[1138]: 80C6F1682101: removed
Dec 22 12:50:38 mail dovecot: lmtp(30265): Connect from local
Dec 22 12:50:38 mail dovecot: lmtp(mk@mydomain.com)&lt;30265&gt;&lt;ZzG0K47d4V85dgAAz+uO0w&gt;: sieve: msgid=&lt;4a3578f1b5824ec881e8936b8be7385d@mail.foreigndomain.com&gt;: stored mail into mailbox 'INBOX'
Dec 22 12:50:38 mail postfix/lmtp[30260]: 9FFCD1682106: to=&lt;mk@mydomain.com&gt;, orig_to=&lt;me@mydomain.com&gt;, relay=mail.mydomain.com[private/dovecot-lmtp], delay=0.22, delays=0.02/0.04/0.02/0.14, dsn=2.0.0, status=sent (250 2.0.0 &lt;mk@mydomain.com&gt; ZzG0K47d4V85dgAAz+uO0w Saved)
Dec 22 12:50:38 mail postfix/qmgr[1138]: 9FFCD1682106: removed
Dec 22 12:50:38 mail dovecot: lmtp(30265): Disconnect from local: Client has quit the connection (state=READY)
</code></pre>
<p>Does anyone have an explanation or solution for this problem?</p>
<p>Thanks!</p>
<p>postconf:</p>
<pre><code>postconf: warning: /etc/postfix/master.cf: undefined parameter: mua_client_restrictions
postconf: warning: /etc/postfix/master.cf: undefined parameter: mua_client_restrictions
address_verify_negative_refresh_time = 60s
address_verify_sender_ttl = 15686s
alias_database = hash:/etc/aliases, hash:/var/lib/mailman/data/aliases
alias_maps = hash:/etc/aliases, hash:/var/lib/mailman/data/aliases
append_dot_mydomain = no
biff = no
body_checks = regexp:/etc/postfix/body_checks
broken_sasl_auth_clients = yes
compatibility_level = 2
content_filter = lmtp:[127.0.0.1]:10024
dovecot_destination_recipient_limit = 1
enable_original_recipient = no
greylisting = check_policy_service inet:127.0.0.1:10023
header_checks = regexp:/etc/postfix/header_checks
html_directory = /usr/share/doc/postfix/html
inet_interfaces = all
inet_protocols = all
mailbox_size_limit = 0
maildrop_destination_concurrency_limit = 1
maildrop_destination_recipient_limit = 1
message_size_limit = 0
mime_header_checks = regexp:/etc/postfix/mime_header_checks
mydestination = mail.mydomain.com, xxxxx.contaboserver.net, localhost, localhost.localdomain
myhostname = mail.mydomain.com
mynetworks = 127.0.0.0/8 [::1]/128
myorigin = /etc/mailname
nested_header_checks = regexp:/etc/postfix/nested_header_checks
owner_request_special = no
proxy_read_maps = $local_recipient_maps $mydestination $virtual_alias_maps $virtual_alias_domains $sender_bcc_maps $virt                                                                                                                     ual_mailbox_maps $virtual_mailbox_domains $relay_recipient_maps $relay_domains $canonical_maps $sender_canonical_maps $r                                                                                                                     ecipient_canonical_maps $relocated_maps $transport_maps $mynetworks $smtpd_sender_login_maps $virtual_uid_maps $virtual_                                                                                                                     gid_maps $smtpd_client_restrictions $smtpd_sender_restrictions $smtpd_recipient_restrictions
readme_directory = /usr/share/doc/postfix
receive_override_options = no_address_mappings
recipient_delimiter = +
relay_domains = proxy:mysql:/etc/postfix/mysql-virtual_relaydomains.cf
relay_recipient_maps = proxy:mysql:/etc/postfix/mysql-virtual_relayrecipientmaps.cf
relayhost =
sender_bcc_maps = proxy:mysql:/etc/postfix/mysql-virtual_outgoing_bcc.cf
smtp_dns_support_level = dnssec
smtp_tls_exclude_ciphers = RC4, aNULL
smtp_tls_protocols = !SSLv2,!SSLv3
smtp_tls_security_level = dane
smtp_tls_session_cache_database = btree:${data_directory}/smtp_scache
smtpd_banner = $myhostname ESMTP $mail_name (Debian/GNU)
smtpd_client_message_rate_limit = 100
smtpd_client_restrictions = check_client_access proxy:mysql:/etc/postfix/mysql-virtual_client.cf, permit_inet_interfaces                                                                                                                     , permit_mynetworks, reject_rbl_client zen.spamhaus.org, permit_sasl_authenticated, reject_unauth_pipelining , permit
smtpd_data_restrictions = permit_mynetworks, reject_unauth_pipelining, reject_multi_recipient_bounce, permit
smtpd_etrn_restrictions = permit_mynetworks, reject
smtpd_forbidden_commands = CONNECT,GET,POST,USER,PASS
smtpd_helo_required = yes
smtpd_helo_restrictions = reject_invalid_helo_hostname, permit_mynetworks, check_helo_access regexp:/etc/postfix/helo_ac                                                                                                                     cess, permit_sasl_authenticated, reject_non_fqdn_helo_hostname, check_helo_access regexp:/etc/postfix/blacklist_helo, ,r                                                                                                                     eject_unknown_helo_hostname, permit
smtpd_recipient_restrictions = permit_mynetworks, reject_unknown_recipient_domain, reject_unlisted_recipient, check_reci                                                                                                                     pient_access proxy:mysql:/etc/postfix/mysql-verify_recipients.cf, permit_sasl_authenticated, reject_non_fqdn_recipient,                                                                                                                      reject_unauth_destination, check_recipient_access proxy:mysql:/etc/postfix/mysql-virtual_recipient.cf, check_recipient_a                                                                                                                     ccess mysql:/etc/postfix/mysql-virtual_policy_greylist.cf, check_policy_service unix:private/quota-status
smtpd_relay_restrictions = permit_mynetworks, permit_sasl_authenticated, reject_unauth_destination
smtpd_restriction_classes = greylisting
smtpd_sasl_auth_enable = yes
smtpd_sasl_authenticated_header = yes
smtpd_sasl_path = private/auth
smtpd_sasl_security_options = noanonymous
smtpd_sasl_type = dovecot
smtpd_sender_login_maps = proxy:mysql:/etc/postfix/mysql-virtual_sender_login_maps.cf
smtpd_sender_restrictions = check_sender_access regexp:/etc/postfix/tag_as_originating.re, permit_mynetworks, permit_sas                                                                                                                     l_authenticated, reject_non_fqdn_sender, check_sender_access regexp:/etc/postfix/tag_as_foreign.re, check_sender_access                                                                                                                      proxy:mysql:/etc/postfix/mysql-virtual_sender.cf
smtpd_tls_cert_file = /etc/postfix/smtpd.cert
smtpd_tls_exclude_ciphers = RC4, aNULL
smtpd_tls_key_file = /etc/postfix/smtpd.key
smtpd_tls_mandatory_ciphers = medium
smtpd_tls_mandatory_protocols = !SSLv2, !SSLv3
smtpd_tls_protocols = !SSLv2,!SSLv3
smtpd_tls_security_level = may
smtpd_tls_session_cache_database = btree:${data_directory}/smtpd_scache
smtpd_use_tls = yes
tls_medium_cipherlist = ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RS                                                                                                                     A-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-G                                                                                                                     CM-SHA384
tls_preempt_cipherlist = no
transport_maps = hash:/var/lib/mailman/data/transport-mailman, proxy:mysql:/etc/postfix/mysql-virtual_transports.cf
virtual_alias_domains = proxy:mysql:/etc/postfix/mysql-virtual_alias_domains.cf
virtual_alias_maps = hash:/var/lib/mailman/data/virtual-mailman, proxy:mysql:/etc/postfix/mysql-virtual_forwardings.cf,                                                                                                                      proxy:mysql:/etc/postfix/mysql-virtual_alias_maps.cf, proxy:mysql:/etc/postfix/mysql-virtual_email2email.cf
virtual_gid_maps = proxy:mysql:/etc/postfix/mysql-virtual_gids.cf
virtual_mailbox_base = /var/vmail
virtual_mailbox_domains = proxy:mysql:/etc/postfix/mysql-virtual_domains.cf
virtual_mailbox_maps = proxy:mysql:/etc/postfix/mysql-virtual_mailboxes.cf
virtual_transport = lmtp:unix:private/dovecot-lmtp
virtual_uid_maps = proxy:mysql:/etc/postfix/mysql-virtual_uids.cf
</code></pre>
<p>dovecot:</p>
<pre><code># 2.3.4.1 (f79e8e7e4): /etc/dovecot/dovecot.conf
# Pigeonhole version 0.5.4 ()
# OS: Linux 4.19.0-12-amd64 x86_64 Debian 10.7
# Hostname: mail.mydomain.com
auth_mechanisms = plain login
disable_plaintext_auth = no
imap_capability = +SEPCIAL-USE XLIST
listen = *,[::]
lmtp_rcpt_check_quota = yes
log_timestamp = &quot;%Y-%m-%d %H:%M:%S &quot;
mail_max_userip_connections = 100
mail_plugins = quota
mail_privileged_group = vmail
namespace inbox {
  inbox = yes
  location =
  mailbox Drafts {
    special_use = \Drafts
  }
  mailbox Junk {
    special_use = \Junk
  }
  mailbox Sent {
    special_use = \Sent
  }
  mailbox &quot;Sent Messages&quot; {
    special_use = \Sent
  }
  mailbox Trash {
    special_use = \Trash
  }
  prefix =
  separator = .
}
passdb {
  args = /etc/dovecot/dovecot-sql.conf
  driver = sql
}
plugin {
  quota = dict:user::file:/var/vmail/%d/%n/.quotausage
  quota_status_nouser = DUNNO
  quota_status_overquota = 552 5.2.2 Mailbox is full
  quota_status_success = DUNNO
  sieve = /var/vmail/%d/%n/.sieve
  sieve_after = /var/vmail/%d/%n/.ispconfig.sieve
  sieve_before = /var/vmail/%d/%n/.ispconfig-before.sieve
  sieve_max_actions = 100
  sieve_max_redirects = 25
  sieve_max_script_size = 2M
}
protocols = imap pop3 lmtp
service auth {
  unix_listener /var/spool/postfix/private/auth {
    group = postfix
    mode = 0660
    user = postfix
  }
  unix_listener auth-userdb {
    group = vmail
    mode = 0600
    user = vmail
  }
  user = root
}
service imap-login {
  client_limit = 1000
  process_limit = 512
}
service lmtp {
  unix_listener /var/spool/postfix/private/dovecot-lmtp {
    group = postfix
    mode = 0600
    user = postfix
  }
}
service quota-status {
  client_limit = 1
  executable = quota-status -p postfix
  unix_listener /var/spool/postfix/private/quota-status {
    group = postfix
    mode = 0660
    user = postfix
  }
}
service stats {
  unix_listener stats-reader {
    group = vmail
    mode = 0660
    user = vmail
  }
  unix_listener stats-writer {
    group = vmail
    mode = 0660
    user = vmail
  }
}
ssl_cert = &lt;/etc/postfix/smtpd.cert
ssl_cipher_list = ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384
ssl_dh = # hidden, use -P to show it
ssl_key = # hidden, use -P to show it
ssl_min_protocol = TLSv1.2
userdb {
  driver = prefetch
}
userdb {
  args = /etc/dovecot/dovecot-sql.conf
  driver = sql
}
protocol imap {
  auth_verbose = yes
  mail_plugins = quota imap_quota
}
protocol pop3 {
  auth_verbose = yes
  mail_plugins = quota
  pop3_uidl_format = %08Xu%08Xv
}
protocol lda {
  mail_plugins = sieve quota
  postmaster_address = postmaster@xxxxxx.contaboserver.net
}
protocol lmtp {
  mail_plugins = quota sieve
  postmaster_address = postmaster@xxxxxx.contaboserver.net
}
</code></pre>
","<linux><postfix><email-server><dovecot><amavis>","2020-12-24 08:55:02"
"941394","Apache Redirect Help 'Pretty Links'","<p>I want to be able to redirect to <a href=""https://example.com/somehtmlfile.html"" rel=""nofollow noreferrer"">https://example.com/somehtmlfile.html</a> to <a href=""https://example.com/somehtmlfile"" rel=""nofollow noreferrer"">https://example.com/somehtmlfile</a>, in apache; not sure if this would work?</p>
<pre><code>RewriteRule (.*) https://example.com/$1 [L]
</code></pre>
","<apache-2.4><https>","2018-11-23 20:14:20"
"800992","Migrating from one Linux distribution to another","<p>How should I approach <strong>migrating</strong> a server <strong>from Mint to Ubuntu</strong>? I don't know how go about it since I have a lot on my Mint server.</p>

<p>EDIT: I also would like to keep my mail server running, if possible, during the migration.</p>
","<ubuntu><migration><operating-system>","2016-09-04 13:20:52"
"941599","crond service running after removing the entry in crontab","<p>I removed the entries in crontab for root and still, I can see the scripts running tail -f /var/log/cron.</p>

<p>How can I remove those entries.</p>

<p>Output of cron log:</p>

<p>CROND[2736]: (root) CMD (/home/centos/aws-scripts-mon/mon-put-instance-data.pl --mem-avail --mem-used --swap-util --swap-used --from-cron)</p>

<p>Nov 26 06:57:01 CROND[2738]: (root) CMD (/opt/aws-scripts-mon/jvm_stat.sh)</p>
","<linux><centos7><cron><incrontab><incrond>","2018-11-26 07:03:08"
"1047724","error code 401 when call ping search image api","<p><a href=""https://api.bing.microsoft.com/v7.0/images/search?Ocp-Apim-Subscription-Key=9e572eb4609e4042b490fc42b2cb004c&amp;q=apples"" rel=""nofollow noreferrer"">https://api.bing.microsoft.com/v7.0/images/search?Ocp-Apim-Subscription-Key=9e572eb4609e4042b490fc42b2cb004c&amp;q=apples</a></p>
<p>{&quot;error&quot;:{&quot;code&quot;:&quot;401&quot;,&quot;message&quot;:&quot;Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.&quot;}}</p>
","<authentication><search><images>","2020-12-27 21:50:22"
"874557","Cannot access or delete AWS S3 bucket from console","<p>Learning AWS and mucking about with buckets. I tried to configure a logging bucket for a project, with the following <code>logging.json</code> used for the <code>put-bucket-logging</code> command in the cli:</p>

<pre><code>{
    ""LoggingEnabled"": {
        ""TargetBucket"": ""logs.bucketname.com"",
        ""TargetPrefix"": ""bucketLogs/"",
        ""TargetGrants"": [
            {
                ""Grantee"": {
                    ""Type"": ""AmazonCustomerByEmail"",
                    ""EmailAddress"": ""username@email.com""
                },
                ""Permission"": ""FULL_CONTROL""
            },
            {
                ""Grantee"": {
                    ""Type"": ""Group"",
                    ""URI"": ""http://acs.amazonaws.com/groups/global/AllUsers""
                },
                ""Permission"": ""READ""
            }
        ]
    }
}
</code></pre>

<p>And after receiving a message in the buckets overview tab in the S3 management console stating ""Error: Access Denied"", I've been trying to set up a policy that will give me access back so I can just delete the bucket and start over. The policy now looks like this:</p>

<pre><code>{
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Effect"": ""Allow"",
            ""Principal"": {
                ""AWS"": ""*""
            },
            ""Action"": [
                ""s3:GetObject"",
                ""s3:DeleteObject"",
                ""s3:PutObject""
            ],
            ""Resource"": ""arn:aws:s3:::logs.bucketname.com/*""
        }
    ]
}
</code></pre>

<p>I'm not sure how any of this has caused the root user to be unable to access or delete the bucket in the console. Please advise.</p>
","<amazon-web-services><amazon-s3>","2017-09-20 02:51:23"
"1015895","Does windows 2016 support Lotus Notes Client 8.5.3?","<p>I want to install Lotus Notes Client 8.5.3 on windows 2016. I am not sure whether 8.5.3 version is compatible with windows 2016. 
Does windows 2016 support Lotus Notes Client 8.5.3? Any help would be appreciated.</p>
","<windows-server-2016><ibm-domino><lotus-notes>","2020-05-06 08:57:31"
"801018","disabled firewall then apache stopped","<p>i was having some unrelated to the website server problems, so i stopped iptables. 
ever since then, apache wont serve pages. i only get the error This site can’t be reached. my.site.org took too long to respond. 
i tried reinstalling apache, disabling php, changing the owner/group of the www file, and a dozen other things. 
there is no server side error, and it appears apache is running, how can i get apache to serve pages again without restarting iptables as that breaks the rest of the server?</p>

<p>EDIT:</p>

<p>wouldnt let me copy the first two commands but it would the third:</p>

<p><a href=""https://i.sstatic.net/oouDQ.png"" rel=""nofollow noreferrer"">iptables -L -n</a></p>

<p><a href=""https://i.sstatic.net/hZRKC.png"" rel=""nofollow noreferrer"">iptables -t nat -L -n</a></p>

<pre><code>root@mineos ~# lsof -i
(LISTEN)
apache2     649          root    4u  IPv6   11604      0t0  TCP *:http (LISTEN)
apache2    9079      www-data    4u  IPv6   11604      0t0  TCP *:http (LISTEN)
apache2    9080      www-data    4u  IPv6   11604      0t0  TCP *:http (LISTEN)
apache2    9081      www-data    4u  IPv6   11604      0t0  TCP *:http (LISTEN)
apache2    9082      www-data    4u  IPv6   11604      0t0  TCP *:http (LISTEN)
apache2    9083      www-data    4u  IPv6   11604      0t0  TCP *:http (LISTEN)
</code></pre>

<p>i tried all these ports and got connection refused on all.</p>
","<iptables><apache-2.4>","2016-09-04 17:21:27"
"1047789","Prevent closing of other people's RDP sessions on Windows Server","<p>I work on a windows server machine where all users have admin privileges and can, using the task manager, terminate each other's sessions  (<code>taskmanager -&gt; users -&gt; select user -&gt; sign off -&gt; confirm</code>).</p>
<p>Sometimes people close sessions and close the wrong one (likely by accident) and this can lead to data loss. What would be some (technological) measures (group policy, regedit?) to protect sessions from being closed remotely (i.e. from another open session on the same server) and minimize chance of this happening?</p>
","<remote-desktop><group-policy><windows><windows-server-2016>","2020-12-28 12:46:26"
"1047808","Suggestions for AWS setup wanted","<p>For my next project, I'm giving AWS and server-less a go, but I'm not sure how to reason about the different AWS services. So here's hoping that someone here has a good feel for it.</p>
<p>The project currently consists of three different frontend applications, each hosted on a separate sub-domain:</p>
<ul>
<li>root domain + [www]: Main webpage which mainly consists of &quot;selling&quot;-content</li>
<li>[administration]: Frontend application for our administration systems</li>
<li>[user]: Frontend application for our user experience</li>
</ul>
<p>Then there, for now, is one backend application which serves all frontend applications. This backend must be reversed proxied by at least the [administration] and [user] sub-domains (i.e. as user.domain.com/service/backend). My backend service currently runs as a docker container and it uses WebSockets for some parts, so that has to be supported.</p>
<p>I've been looking at some options for my frontends, either AWS Amplify or S3+Cloudfront. However, I'm not sure if either of these is what I'm looking for.</p>
<p>Same goes for my backend. I've been looking at AWS Fargate, AWS API Gateway and possible AWS AppSync (since much of my backend is a GraphQL application).</p>
<p>But I'm kind of lost. Could anyone give me a push in the right direction? Hopefully, I've given enough and clear information, otherwise, I'd be happy to expand! :)</p>
<p>Thanks!</p>
","<amazon-web-services><amazon-s3><amazon-cloudfront><aws-fargate>","2020-12-28 16:24:36"
"801140","Copy a file with SCP on Ubuntu 14 with PEM key on both servers","<p>I have 2 servers on Amazon AWS. I need to pass a file from A to B.
Both servers have pem keys (different).
I put the pem file of B on A (Is that correct?)
Then tried to do from A:  </p>

<pre><code>scp -i /tmp/B.pem backup.sql.gz ubuntu@ec2-XX-XX-XXX-XX.eu-central-1.compute.amazonaws.com:bckup.sql.gz
</code></pre>

<p>I get an error: </p>

<blockquote>
  <p>Permission denied (publickey).<br>
  lost connection</p>
</blockquote>

<p>Whats the problem?</p>

<p>BTW - I opened port 22 on both</p>
","<ubuntu-14.04><copy>","2016-09-05 13:25:42"
"874711","Hosting hundreds of web sites on Amazon AWS","<p>We are considering hosting our web sites on amazon aws, so that they can scale on-demand.  Currently we host in the region of 500 web sites on multi-tenanted servers, but we want to move away from this architecture and have at least 1 server per website.  It appears that amazon aws has a limit of 5 ip addresses per account, so we would not be able to have a unique public ip address for each site, which is fair enough considering the shortage of ipv4 addresses.</p>

<p>So the next thought that comes to mind is to host each web site as an application on elastic beanstalk, but it seems there is a limit of 75 applications per account, so I guess this is not an option either.  Amazon say you can request more, but would they really agree to allow us to have 425 more?</p>

<p>The next thought that comes to mind is to use an amazon application load balancer.  Apparently you can have 50 listeners and each of these listeners can have an ssl certificate and can be routed to a specific ec2 instance, so we could have www.site1.co.uk map to one ec2 server and www.site2.co.uk map to another ec2 server etc.  However I have read that you can only have 20 ec2 servers per account, so I guess we would not be able to host our web sites like this either?  Amazon say you can requrest more ec2 instances, but would they really agree to allow us to have 480 more?</p>

<p>So what options do we have on amazon aws?  (Bearing in mind that we do not want to have multi-tenanted servers).  Does anyone here have experience of running hundreds of separate web sites on amazon aws?</p>

<p>Many thanks in advance for any advice that you can offer.</p>
","<amazon-ec2><amazon-web-services><web-hosting><multi-tenancy>","2017-09-20 19:59:36"
"874765","How to set up nginx in ubuntu with virtualbox","<p>I have installed ubuntu on virtual box. Then, I installed nginx, php, php-fpm, and mysql. When I try to set configurations for nginx, it keeps giving me the error below. What could I solve this problem?</p>

<p>Any suggestion or advice would be appreciated.</p>

<p>Thank you in advance.</p>

<p>Error</p>

<pre><code>Sep 21 14:58:35 develop-truckup systemd[1]: Starting A high performance web server and a reverse proxy server...
Sep 21 14:58:35 develop-truckup nginx[27774]: nginx: [emerg] ""fastcgi_index"" directive is duplicate in /etc/nginx/sites-enabled/default:58
Sep 21 14:58:35 develop-truckup nginx[27774]: nginx: configuration file /etc/nginx/nginx.conf test failed
Sep 21 14:58:35 develop-truckup systemd[1]: nginx.service: Control process exited, code=exited status=1
Sep 21 14:58:35 develop-truckup systemd[1]: Failed to start A high performance web server and a reverse proxy server.
Sep 21 14:58:35 develop-truckup systemd[1]: nginx.service: Unit entered failed state.
Sep 21 14:58:35 develop-truckup systemd[1]: nginx.service: Failed with result 'exit-code'.
</code></pre>

<p>This is sites-available/default/www.conf</p>

<pre><code>server {
        listen 80 default_server;
        listen [::]:80 default_server;

        # SSL configuration
        #
        # listen 443 ssl default_server;
        # listen [::]:443 ssl default_server;
        #
        # Note: You should disable gzip for SSL traffic.
        # See: https://bugs.debian.org/773332
        #
        # Read up on ssl_ciphers to ensure a secure configuration.
        # See: https://bugs.debian.org/765782
        #
        # Self signed certs generated by the ssl-cert package
        # Don't use them in a production server!
        #
        # include snippets/snakeoil.conf;

        root {dir where my index.php is (e.g. /home/example }

        # Add index.php to the list if you are using PHP
        index index.php index.html index.htm index.nginx-debian.html;

        server_name localhost;

        location / {
                # First attempt to serve request as file, then
                # as directory, then fall back to displaying a 404.
                try_files $uri $uri/ =404;
        }

        # pass PHP scripts to FastCGI server
        #
        location ~ \.php$ {
                include snippets/fastcgi-php.conf;
                fastcgi_index  index.php;
                fastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;
                include        fastcgi_params;
                # With php-fpm (or other unix sockets):
                fastcgi_pass unix:/var/run/php/php7.0-fpm.sock;
        #       # With php-cgi (or other tcp sockets):
        #       fastcgi_pass 127.0.0.1:9000;
        }

        # deny access to .htaccess files, if Apache's document root
        # concurs with nginx's one
        #
        #location ~ /\.ht {
        #       deny all;
        #}
}
</code></pre>

<p>Here is /etc/php/7.0/fpm/pool.d/</p>

<pre><code>; Start a new pool named 'www'.
; the variable $pool can be used in any directive and will be replaced by the
; pool name ('www' here)
[www]

; Per pool prefix
; It only applies on the following directives:
; - 'access.log'
; - 'slowlog'
; - 'listen' (unixsocket)
; - 'chroot'
; - 'chdir'
; - 'php_values'
; - 'php_admin_values'
; When not set, the global prefix (or /usr) applies instead.
; Note: This directive can also be relative to the global prefix.
; Default Value: none
;prefix = /path/to/pools/$pool

; Unix user/group of processes
; Note: The user is mandatory. If the group is not set, the default user's group
;       will be used.
user = www-data
group = www-data

; The address on which to accept FastCGI requests.
; Valid syntaxes are:
;   'ip.add.re.ss:port'    - to listen on a TCP socket to a specific IPv4 address on
;                            a specific port;
;   '[ip:6:addr:ess]:port' - to listen on a TCP socket to a specific IPv6 address on
;                            a specific port;
;   'port'                 - to listen on a TCP socket to all addresses
;                            (IPv6 and IPv4-mapped) on a specific port;
;   '/path/to/unix/socket' - to listen on a unix socket.
; Note: This value is mandatory.
listen = /run/php/php7.0-fpm.sock

; Set listen(2) backlog.
; Default Value: 511 (-1 on FreeBSD and OpenBSD)
;listen.backlog = 511

; Set permissions for unix socket, if one is used. In Linux, read/write
; permissions must be set in order to allow connections from a web server. Many
; BSD-derived systems allow connections regardless of permissions.
; Default Values: user and group are set as the running user
;                 mode is set to 0660
listen.owner = www-data
listen.group = www-data
;listen.mode = 0660
; When POSIX Access Control Lists are supported you can set them using
; these options, value is a comma separated list of user/group names.
; When set, listen.owner and listen.group are ignored
;listen.acl_users =
;listen.acl_groups =
</code></pre>

<p>Here is /etc/nginx/nginx.conf</p>

<pre><code>user www-data;
worker_processes auto;
pid /run/nginx.pid;
include /etc/nginx/modules-enabled/*.conf;

events {
        worker_connections 768;
        # multi_accept on;
}

http {

        ##
        # Basic Settings
        ##

        sendfile on;
        tcp_nopush on;
        tcp_nodelay on;
        keepalive_timeout 65;
        types_hash_max_size 2048;
        # server_tokens off;

        # server_names_hash_bucket_size 64;
        # server_name_in_redirect off;

        include /etc/nginx/mime.types;
        default_type application/octet-stream;

        ##
        # SSL Settings
        ##

        ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # Dropping SSLv3, ref: POODLE
        ssl_prefer_server_ciphers on;

        ##
        # Logging Settings
        ##

        access_log /var/log/nginx/access.log;
        error_log /var/log/nginx/error.log;

        ##
        # Gzip Settings
        ##

        gzip on;
        gzip_disable ""msie6"";

        # gzip_vary on;
        # gzip_proxied any;
        # gzip_comp_level 6;
        # gzip_buffers 16 8k;
        # gzip_http_version 1.1;
        # gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;

        ##
        # Virtual Host Configs
        ##

        include /etc/nginx/conf.d/*.conf;
        include /etc/nginx/sites-enabled/*;
}
</code></pre>
","<ubuntu><nginx>","2017-09-21 06:06:33"
"1047959","Computers can't join domain - probably DNS issue","<p>I installed and configured an Active Directory Controller (Windows Server 2019), and also did a clean install of Windows 10 Enterprise. Both were installed in Hyper-V and both use the same Private switch.</p>
<p>Domain on the Server is successfully created, it is promoted to a domain controller and DNS server is running. I followed this video: <a href=""https://www.youtube.com/watch?v=aC-KIVYewAM"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=aC-KIVYewAM</a> and this video: <a href=""https://www.youtube.com/watch?v=XzMDb-ZtzRY"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=XzMDb-ZtzRY</a> to set both up.</p>
<p>However, when I try to join domain on the client, I get the following error:</p>
<blockquote>
<p>DNS was successfully queried for the service location (SRV) resource
record used to locate a domain controller for domain
&quot;testdomain.local&quot;:</p>
<p>The query was for the SRV record for
_ldap._tcp.dc._msdcs.testdomain.local</p>
<p>The following domain controllers were identified by the query:
WIN-A9VDVHB8U8M.testdomain.local</p>
<p>However no domain controllers could be contacted.</p>
</blockquote>
<p>I am using the IP address of the server as a primary DNS server for the client and I verified that both can ping each other.</p>
<p>I also tried <code>ping testdomain.local</code> on both client and server, and both fail to resolve the domain. I would get the failure on the client, but how can it fail on the server? Am I missing some record on the DNS server?</p>
<p>This is what my DNS server zone looks like at my server:</p>
<p><a href=""https://i.sstatic.net/JU3Fs.png"" rel=""nofollow noreferrer"">DNS Manager</a></p>
<p>Also ipconfig-all outputs from server:</p>
<pre><code>C:\Users\Administrator&gt;ipconfig -all

Windows IP Configuration

   Host Name . . . . . . . . . . . . : WIN-A9VDVHB8U8M
   Primary Dns Suffix  . . . . . . . : testdomain.local
   Node Type . . . . . . . . . . . . : Hybrid
   IP Routing Enabled. . . . . . . . : No
   WINS Proxy Enabled. . . . . . . . : No
   DNS Suffix Search List. . . . . . : testdomain.local

Ethernet adapter Ethernet:

   Connection-specific DNS Suffix  . :
   Description . . . . . . . . . . . : Microsoft Hyper-V Network Adapter
   Physical Address. . . . . . . . . : 00-15-5D-26-E1-12
   DHCP Enabled. . . . . . . . . . . : Yes
   Autoconfiguration Enabled . . . . : Yes
   Link-local IPv6 Address . . . . . : fe80::b8f7:6aea:3eb4:8957%4(Preferred)
   Autoconfiguration IPv4 Address. . : 169.254.137.87(Preferred)
   Subnet Mask . . . . . . . . . . . : 255.255.0.0
   Default Gateway . . . . . . . . . :
   DHCPv6 IAID . . . . . . . . . . . : 67114333
   DHCPv6 Client DUID. . . . . . . . : 00-01-00-01-27-7D-9A-F9-00-15-5D-26-E1-12
   DNS Servers . . . . . . . . . . . : ::1
                                       127.0.0.1
   NetBIOS over Tcpip. . . . . . . . : Enabled
</code></pre>
<p>and from the client:</p>
<pre><code>C:\Users\User&gt;ipconfig -all

Windows IP Configuration

   Host Name . . . . . . . . . . . . : DESKTOP-BMP0DM7
   Primary Dns Suffix  . . . . . . . :
   Node Type . . . . . . . . . . . . : Hybrid
   IP Routing Enabled. . . . . . . . : No
   WINS Proxy Enabled. . . . . . . . : No

Ethernet adapter Ethernet:

   Connection-specific DNS Suffix  . :
   Description . . . . . . . . . . . : Microsoft Hyper-V Network Adapter
   Physical Address. . . . . . . . . : 00-15-5D-26-E1-13
   DHCP Enabled. . . . . . . . . . . : Yes
   Autoconfiguration Enabled . . . . : Yes
   Link-local IPv6 Address . . . . . : fe80::9c44:a92d:4d65:50bf%6(Preferred)
   Autoconfiguration IPv4 Address. . : 169.254.80.191(Preferred)
   Subnet Mask . . . . . . . . . . . : 255.255.0.0
   Default Gateway . . . . . . . . . :
   DHCPv6 IAID . . . . . . . . . . . : 100668765
   DHCPv6 Client DUID. . . . . . . . : 00-01-00-01-27-7D-A1-8F-00-15-5D-26-E1-13
   DNS Servers . . . . . . . . . . . : 169.254.137.87
   NetBIOS over Tcpip. . . . . . . . : Enabled
</code></pre>
<p>Could you please help me resolve the issue? I followed the video step-by-step, yet it isn't working.</p>
<p>If you need any additional info, let me know.</p>
<p>Thanks</p>
<p>EDIT:</p>
<p>Per request:</p>
<p>Ipconfig - server:</p>
<pre><code>C:\Users\Administrator&gt;ipconfig -all

Windows IP Configuration

   Host Name . . . . . . . . . . . . : WIN-A9VDVHB8U8M
   Primary Dns Suffix  . . . . . . . : testdomain.local
   Node Type . . . . . . . . . . . . : Hybrid
   IP Routing Enabled. . . . . . . . : No
   WINS Proxy Enabled. . . . . . . . : No
   DNS Suffix Search List. . . . . . : testdomain.local

Ethernet adapter Ethernet:

   Connection-specific DNS Suffix  . :
   Description . . . . . . . . . . . : Microsoft Hyper-V Network Adapter
   Physical Address. . . . . . . . . : 00-15-5D-26-E1-12
   DHCP Enabled. . . . . . . . . . . : No
   Autoconfiguration Enabled . . . . : Yes
   Link-local IPv6 Address . . . . . : fe80::b8f7:6aea:3eb4:8957%4(Preferred)
   IPv4 Address. . . . . . . . . . . : 169.254.137.91(Preferred)
   Subnet Mask . . . . . . . . . . . : 255.255.0.0
   Default Gateway . . . . . . . . . :
   DHCPv6 IAID . . . . . . . . . . . : 67114333
   DHCPv6 Client DUID. . . . . . . . : 00-01-00-01-27-7D-9A-F9-00-15-5D-26-E1-12
   DNS Servers . . . . . . . . . . . : ::1
                                       169.254.137.91
   NetBIOS over Tcpip. . . . . . . . : Enabled
</code></pre>
<p>ipconfig client:</p>
<pre><code>C:\Users\User&gt;ipconfig -all

Windows IP Configuration

   Host Name . . . . . . . . . . . . : DESKTOP-BMP0DM7
   Primary Dns Suffix  . . . . . . . :
   Node Type . . . . . . . . . . . . : Hybrid
   IP Routing Enabled. . . . . . . . : No
   WINS Proxy Enabled. . . . . . . . : No

Ethernet adapter Ethernet:

   Connection-specific DNS Suffix  . :
   Description . . . . . . . . . . . : Microsoft Hyper-V Network Adapter
   Physical Address. . . . . . . . . : 00-15-5D-26-E1-13
   DHCP Enabled. . . . . . . . . . . : Yes
   Autoconfiguration Enabled . . . . : Yes
   Link-local IPv6 Address . . . . . : fe80::9c44:a92d:4d65:50bf%11(Preferred)
   Autoconfiguration IPv4 Address. . : 169.254.80.191(Preferred)
   Subnet Mask . . . . . . . . . . . : 255.255.0.0
   Default Gateway . . . . . . . . . :
   DHCPv6 IAID . . . . . . . . . . . : 100668765
   DHCPv6 Client DUID. . . . . . . . : 00-01-00-01-27-7D-A1-8F-00-15-5D-26-E1-13
   DNS Servers . . . . . . . . . . . : 169.254.137.91
   NetBIOS over Tcpip. . . . . . . . : Enabled
</code></pre>
<p>DNS A records:</p>
<p><a href=""https://i.sstatic.net/RmJEG.png"" rel=""nofollow noreferrer"">First record</a></p>
<p><a href=""https://i.sstatic.net/EuNDc.png"" rel=""nofollow noreferrer"">Second record</a></p>
<p>Nslookup on client:</p>
<pre><code>C:\Users\User&gt;nslookup testdomain.local
DNS request timed out.
    timeout was 2 seconds.
Server:  UnKnown
Address:  169.254.137.91

Name:    testdomain.local
Address:  169.254.137.91
</code></pre>
","<windows><active-directory><domain-controller><windows-server-2019>","2020-12-29 18:25:10"
"801232","Downloading a file chokes the entire network, can't browse etc","<p>(I had a look at this post here: <a href=""https://serverfault.com/questions/156281/downloading-a-file-brings-network-to-a-crawl"">Downloading a file brings network to a crawl</a> But it doesn't provide any answers for my situation,)</p>

<p>We have dedicated enterprise fibre lines with bandwidth at 2MB/s. If one user watches a video, or downloads a file, the entire network crawls to a halt. Can't browse internet. If I run speedtest, the network crawls to a halt.</p>

<p>This is new behaviour, and may be wiring related with regards to the firewall, but I'm trying to diagnose where the potential issues could be:</p>

<p>20 pc's on the domain network, 2x Fibre in -> 2x Cisco router -> 2x Juniper Firewall -> 2x Dell Switches -> Patch panel -> PC</p>

<p>Each router connects to a different firewall. The 2 firewalls are connected. Each firewall connects to a different switch. The switches are connected. The switches then run to the patch panel and out to the office. </p>

<p>We have failover fibre lines, so if one line goes down, we can immediately switch over to the failover line.</p>

<p>Is there a config issue on the firewalls? Are certain ports set to dedicated priority transfer? There is no COS set on the firewalls. </p>

<p>The domain is a server 2012 R2 essentials server.</p>

<p>Does the network setup sound poor? What could potentially be causing this bandwidth hogging?</p>

<p>Thanks.</p>
","<firewall><domain><internet><qos><bandwidth-control>","2016-09-06 01:21:00"
"1016143","Yahoo mail always sending 250 code for SMTP rcpt","<p>As the title suggest yahoo.com gives back a 250 codefor good and bad e-mail addresses, i.e for one's that exist and those that don't.</p>

<p>Is this just Yahoo policy or am I doing something wrong?</p>

<p>If it is their policy is there any programmatic way to confirm if a Yahoo address is real or not?</p>
","<email><smtp>","2020-05-07 16:16:18"
"1048009","Extend LVM in Ubuntu","<p>I did a fresh install of Ubuntu 20.04. I have 3 drives on the server. SDA=1.8TB, SDB=1.8TB, SDC=6.4TB. The OS is installed on SDA. When execute a df-h command I appear to be missing some space on SDA. It's not adding up to 1.8TB. It seems the OS install did not allocate the lvm buntu--vg-ubuntu--lv to use the full amount of space. There is about 1TB missing. How do I extend the LVM and reclaim that space? or see where it's being used?</p>
<pre><code>$ df-h
Filesystem                         Size  Used Avail Use% Mounted on
udev                               126G     0  126G   0% /dev
tmpfs                               26G  2.4M   26G   1% /run
/dev/mapper/ubuntu--vg-ubuntu--lv  196G   11G  176G   6% /
tmpfs                              126G     0  126G   0% /dev/shm
tmpfs                              5.0M     0  5.0M   0% /run/lock
tmpfs                              126G     0  126G   0% /sys/fs/cgroup
/dev/sda2                          976M  105M  805M  12% /boot
/dev/sda1                          511M  7.9M  504M   2% /boot/efi
/dev/sdb                           1.8T   77M  1.7T   1% /drives/raid10-1
/dev/sdc                           6.4T   89M  6.0T   1% /drives/raid5-1
/dev/loop0                          30M   30M     0 100% /snap/snapd/8542
/dev/loop1                          55M   55M     0 100% /snap/core18/1880
/dev/loop2                          72M   72M     0 100% /snap/lxd/16099
tmpfs                               26G     0   26G   0% /run/user/1000


$ lsblk
NAME                      MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
loop0                       7:0    0 29.9M  1 loop /snap/snapd/8542
loop1                       7:1    0   55M  1 loop /snap/core18/1880
loop2                       7:2    0 71.3M  1 loop /snap/lxd/16099
sda                         8:0    0  1.8T  0 disk
├─sda1                      8:1    0  512M  0 part /boot/efi
├─sda2                      8:2    0    1G  0 part /boot
└─sda3                      8:3    0  1.8T  0 part
  └─ubuntu--vg-ubuntu--lv 253:0    0  200G  0 lvm  /
sdb                         8:16   0  1.8T  0 disk /drives/raid10-1
sdc                         8:32   0  6.4T  0 disk /drives/raid5-1
</code></pre>
<p>Thanks in advance.</p>
","<linux><ubuntu>","2020-12-30 04:17:37"
"942230","1 bridge - 1 vNIC - Multi VLAN","<p>I have an issue that I can't resolve since a few weeks... I present to you the configuration for understanding:</p>

<p>I have a NAS Qnap that it makes Virtualization (qemy/kvm). The OS of the Qnap is CentOS. The Qnap is connected to a Switch (Cisco) with VLAN. On Cisco, no problem, it runs fine (VLAN or other)... On the Qnap Virtualization, I put an ESXi (Nested). It's ok too.</p>

<p>The configuration on the Qnap by default : </p>

<pre><code>Eth0 - vlan 100 - 192.168.100.253
</code></pre>

<p>I created a vSwitch1 (Bridge) on the interface <code>eth0</code> (192.168.100.253) and put the vNIC of the ESXi <code>(vnet0)</code>.
So :</p>

<blockquote>
  <p>vswitch1 = qvs0.................Eth0 => that creates Eth0.100</p>
</blockquote>

<p>When I run the command <code>brctl</code>, I have this :</p>

<pre><code>bridge name     bridge id               STP enabled     interfaces
qvs0            8000.00089bfaa4e6       yes             eth0.100
                                                        vnet0
</code></pre>

<p>Ok, now the configuration where I want to go is :</p>

<pre><code>QNAP (vlan).....Bridge.........Hypervisor (vnic)..............vlan 

.........................................................| =&gt; vlan 20

Eth0 (100)......qvs0...........ESXI (vlan 10): vnet0.....| =&gt; vlan 30

.........................................................| =&gt; vlan 40

..........192.168.100.253......192.168.10.253............192.168.20.253

.........................................................192.168.30.253                                                                        

.........................................................192.168.40.253
</code></pre>

<p>Then, I creates a script at startup of the Qnap :</p>

<pre><code># ----------------------------------#
#                                   #                       
#!/bin/sh                           #                       
#                                   #                       
# /etc/config/autorun.sh            #
#                                   #                       
# ----------------------------------#

ip link set eth0 txqueuelen 10000
echo 1 &gt; /proc/sys/net/ipv4/tcp_rfc1337
echo 2 &gt; /proc/sys/net/ipv4/tcp_frto
echo 1 &gt; /proc/sys/net/ipv4/tcp_mtu_probing
echo 1 &gt; /proc/sys/net/ipv4/tcp_window_scaling
echo 1 &gt; /proc/sys/net/ipv4/tcp_workaround_signed_windows
echo 1 &gt; /proc/sys/net/ipv4/tcp_tw_reuse
echo 0 &gt; /proc/sys/net/ipv4/tcp_tw_recycle
echo 1 &gt; /proc/sys/net/ipv4/tcp_low_latency
echo 1 &gt; /proc/sys/net/ipv4/tcp_ecn

# vlan 10 ********************

ip link add link eth0 name eth0.10 type vlan id 10
ip link set eth0.10 mtu 1500


# vlan 20 ********************

ip link add link eth0 name eth0.20 type vlan id 20
ip link set eth0.20 mtu 1500


# vlan 30 ********************

ip link add link eth0 name eth0.30 type vlan id 30
ip link set eth0.30 mtu 1500


# vlan 40 ********************

ip link add link eth0 name eth0.40 type vlan id 40
ip link set eth0.40 mtu 1500


ip link set dev eth0.10 up
ip link set dev eth0.20 up
ip link set dev eth0.30 up
ip link set dev eth0.40 up

/sbin/brctl addif qvs0 eth0.100 eth0.10 eth0.20 eth0.30 eth0.40 vnet0
</code></pre>

<p>The problem is it doesn't work. I can't access to vlan 10, 20, 30 or 40... When the NAS reboots and that script runs, I loose all (no ping, no access to the entire NAS)...
I don't know where is the problem at all....</p>

<p>I hope it's clear for you...</p>

<p>Thanks a lot for your help</p>

<p>Inter-Rupteur</p>
","<centos><cisco><vlan><bridge><network-attached-storage>","2018-11-29 22:12:50"
"1016330","Guest OS loses network connectivity after Host OS connects to VPN","<pre><code>System Configuration:
Virtual Box version: VirtualBox-6.1.6-137129-Win
Host OS: Windows 10
Guest OS: Oracle Linux 8.1 (Virtual Box Guest Additions not installed in it yet)

VPN Software running on Host OS (Windows 10):
Cisco AnyConnect Secure Mobility Client
Version: 4.8.02045
Preferences of VPN Client: Screen Shot Attached

Internet Connectivity on Host OS is through WiFi interface Intel(R) Wireless-AC 9560 160MHz connected to my Home router.
</code></pre>

<p>I have configured Guest VM i.e. Oracle Linux with just 1 NAT network interface (with default settings nothing modified), when the host is not connected to VPN my Guest VM i.e. Oracle Linux is able to use hosts network i.e. I am able to browse Internet from Oracle Linux.</p>

<ul>
<li>IP Configuration screen shot attached of Oracle Linux when VPN not connected in Host.</li>
<li>route print command executed on host machine (i.e. windows 10) output also attached before VPN connection.</li>
</ul>

<p>Now I thought that I will make a connection to VPN from Host OS (i.e. Windows 10) and my Guest OS (i.e. Oracle Linux) should be also able to access the same VPN connection with the configured NAT interface but instead when Host makes a VPN connection <strong>Guest OS is not able to use any of the VPN resources neither the Internet.</strong> Nothing seems to be working. I tried taking wireshark dump on Oracle Linux machine after VPN connect but there nothing captured in wireshark at all totally blank.</p>

<ul>
<li>IP Configuration screen shot attached of Oracle Linux when after VPN is connected in Host.</li>
<li><p>route print command executed on host machine (i.e. windows 10) output also attached after VPN connection.</p></li>
<li><p>VBox.log files also attached</p>

<p>Procedures I tried to troubleshoot the issue:</p>

<ul>
<li>Tried Using older version of Virtual Box 5.2 but still found same issue there.</li>
<li>Tried using older version of Virtual Box 6.1.4 with and without VBoxGuestAdditions_6.1.7-137622 but still same issue there.</li>
<li>I wanted to install VBoxGuest Additions as mentioned in ticket virtualbox . org / ticket /19336 but link shared to patched VBoxGuestAdditions_6.1.97-136310.iso in the ticked is broken not accessible.</li>
<li><p>Executed below commands collectively and one-by-one also but still issue persists:</p>

<p>C:\Program Files\Oracle\VirtualBox> VBoxManage modifyvm ""VM name"" --natdnshostresolver1 on</p>

<p>C:\Program Files\Oracle\VirtualBox> VBoxManage modifyvm ""VM name"" --natdnsproxy1 on</p>

<p>C:\Program Files\Oracle\VirtualBox> VBoxManage setextradata global VBoxInternal2/HostDNSSuffixesIgnore 1</p></li>
<li><p>Tried Installing Cisco AnyConnect Directly in Guest OS(Oracle Linux), it shows that it is connected but still can't access Internet or any VPN sites.</p></li>
</ul></li>
</ul>

<p>If I configure Oracle Linux VM network adapter as Bridged then atleast I have internet connectivity on Oracle VM, when VPN is connected on windows host, <strong>but the goal is to use the same VPN connection to which Host System i.e. Windows 10 is connected.</strong></p>

<p>Any help regarding this issue would be great.</p>

<p>All the logs can be downloaded from this <a href=""https://www.dropbox.com/s/7p2iwexuomvgyd7/logs.zip?dl=0"" rel=""nofollow noreferrer"">link.</a></p>
","<vpn><virtual-machines><virtualbox><cisco-vpn><oracle-linux>","2020-05-08 19:06:44"
"801399","Unable to login as jenkins user","<p>I'm trying to setup Jenkins for one of my projects but get this host key verification failed error.</p>

<p>Now, I'm trying to setup an SSH key for my Jenkins user but have issues logging as Jenkins user.</p>

<pre><code>sudo su -s /bin/bash jenkins
</code></pre>

<p>When I try the above command it takes me to </p>

<pre><code>bash-4.1$
</code></pre>

<p>instead of the bash user.</p>
","<git><jenkins>","2016-09-06 18:22:41"
"874956","How can I get a MAC address to resolve on machine connected via VPN?","<p>I run a small business and I have purchase an inventory software. This software requires there be a server, and workstations that connect to the server in order to run the client software. </p>

<p>For authentication purposes, when the client software tries to connect to the server, the asks for the client's IP and MAC addresses. Everything works just fine when 6 of the client computers, which are on the same local network as the server, connect to server and use the software. </p>

<p>However, I have a remote computer which I have connected to the network via VPN through the router, and I've joined the Windows Domain using this method. However, the server software is programmed to resolve both the IP and the MAC address of any connecting client machines. But because this remote computer is connected via VPN, remotely, it fails to ""Resolve"" the MAC address. </p>

<p>Is there any hardware, or any kind of configuration that will allow me to have an actual MAC address on the network when I'm connected via VPN? I'm currently using Windows 10 PRO on the remote machine and using its built in VPN connector feature to connect to the Domain. I guess it needs to be as if I'm sitting there on the actual network with a physical MAC.</p>

<p>Also, before you ask, no I am not doing anything illegal or out of bounds for the software we purchased. The company we got it from said if we can make it work with a remote machine, that would be perfectly fine. Hope to have some help!</p>
","<windows><networking><vpn>","2017-09-22 01:27:14"
"1048061","Virtualmin + Nginx + SSL setup error with cloudflare 521","<p>OS: Ubuntu 20.04
Installed Virtualmin + Nginx + PHP-FPM</p>
<p>Everything set up fine, however, when I installed SSL Certificate 3 month’s trial (Fully Valid Cert) not from Letsencrypt ( Reached limit ) everything works perfectly but, when I switch to Cloudflare my website crashed with 521 server not found. Tried switching to a different option like Flexible, Full, Full (strict) with no luck</p>
<p>command</p>
<pre><code>tail -f /var/log/nginx/error.log
</code></pre>
<p>got a bunch of errors in the log file.</p>
<pre><code>root@server:~# tail -f /var/log/nginx/error.log

2020/12/29 21:59:05 [emerg] 6813#6813: bind() to 2xx.2xx.xx3.xxx:443 failed (98: Address already in use)

2020/12/29 21:59:05 [emerg] 6813#6813: bind() to [2xxx:a1xx:xxxx:8xxx::1]:443 failed (98: Address already in use)

2020/12/29 21:59:05 [emerg] 6813#6813: still could not bind()

2020/12/29 22:31:47 [emerg] 512#512: cannot load certificate “/home/example/ssl.cert”: BIO_new_file() failed (SSL: error:02001002:system library:fopen:No such file or directory:fopen(’/home/example/ssl.cert’,‘r’) error:2006D080:BIO routines:BIO_new_file:no such file)

2020/12/29 22:32:37 [emerg] 1986#1986: cannot load certificate “/home/example/ssl.cert”: BIO_new_file() failed (SSL: error:02001002:system library:fopen:No such file or directory:fopen(’/home/example/ssl.cert’,‘r’) error:2006D080:BIO routines:BIO_new_file:no such file)

2020/12/29 23:58:45 [alert] 12025#12025: *1168 open socket #17 left in connection 93

2020/12/29 23:58:45 [alert] 12025#12025: aborting
</code></pre>
<p>I’m pretty sure those SSL files are there but nginx thinks something else ;(</p>
<p>This is the output I get</p>
<pre><code>Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    
tcp        0      0 127.0.0.1:3306          0.0.0.0:*               LISTEN      678/mysqld          
tcp        0      0 0.0.0.0:587             0.0.0.0:*               LISTEN      1722/master         
tcp        0      0 0.0.0.0:110             0.0.0.0:*               LISTEN      787/dovecot         
tcp        0      0 0.0.0.0:143             0.0.0.0:*               LISTEN      787/dovecot         
tcp        0      0 0.0.0.0:10000           0.0.0.0:*               LISTEN      19353/perl          
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      19330/nginx: master 
tcp        0      0 0.0.0.0:465             0.0.0.0:*               LISTEN      1722/master         
tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      15079/systemd-resol 
tcp        0      0 207.244.253.107:53      0.0.0.0:*               LISTEN      575/named           
tcp        0      0 127.0.0.1:53            0.0.0.0:*               LISTEN      575/named           
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      630/sshd: /usr/sbin 
tcp        0      0 127.0.0.1:11000         0.0.0.0:*               LISTEN      1044/lookup-domain- 
tcp        0      0 0.0.0.0:25              0.0.0.0:*               LISTEN      1722/master         
tcp        0      0 127.0.0.1:953           0.0.0.0:*               LISTEN      575/named           
tcp        0      0 207.244.253.107:443     0.0.0.0:*               LISTEN      19330/nginx: master 
tcp        0      0 0.0.0.0:20000           0.0.0.0:*               LISTEN      19398/perl          
tcp        0      0 0.0.0.0:993             0.0.0.0:*               LISTEN      787/dovecot         
tcp        0      0 0.0.0.0:995             0.0.0.0:*               LISTEN      787/dovecot         
tcp        0      0 127.0.0.1:33060         0.0.0.0:*               LISTEN      678/mysqld          
tcp        0      0 127.0.0.1:10023         0.0.0.0:*               LISTEN      542/postgrey --pidf 
tcp6       0      0 :::587                  :::*                    LISTEN      1722/master         
tcp6       0      0 :::2222                 :::*                    LISTEN      8361/proftpd: (acce 
tcp6       0      0 :::110                  :::*                    LISTEN      787/dovecot         
tcp6       0      0 :::143                  :::*                    LISTEN      787/dovecot         
tcp6       0      0 :::10000                :::*                    LISTEN      19353/perl          
tcp6       0      0 :::80                   :::*                    LISTEN      19330/nginx: master 
tcp6       0      0 :::465                  :::*                    LISTEN      1722/master         
tcp6       0      0 :::21                   :::*                    LISTEN      8361/proftpd: (acce 
tcp6       0      0 ::1:53                  :::*                    LISTEN      575/named           
tcp6       0      0 :::22                   :::*                    LISTEN      630/sshd: /usr/sbin 
tcp6       0      0 :::25                   :::*                    LISTEN      1722/master         
tcp6       0      0 ::1:953                 :::*                    LISTEN      575/named           
tcp6       0      0 :::993                  :::*                    LISTEN      787/dovecot         
tcp6       0      0 :::995                  :::*                    LISTEN      787/dovecot         
tcp6       0      0 ::1:10023               :::*                    LISTEN      542/postgrey --pidf 
root@server:~# 
</code></pre>
<p>And yes the files are there and permission are set chmod 600</p>
<p>Thank you</p>
","<nginx><ssl><cloudflare>","2020-12-30 17:12:00"
"801434","Why does my nserver fail on uptime.com checking?","<p>I've just tried my site via <a href=""http://uptime.com"" rel=""nofollow noreferrer"">uptime.com</a> and get the following: <a href=""https://i.sstatic.net/AGVgR.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AGVgR.png"" alt=""following""></a></p>

<p>My site is hosted on my own server, but through CloudFlare. What should I do to resolve the error? What am I missing, and what does it mean to fail this kind of test?</p>

<p><strong>UPDATE:</strong> The site I'm checking for is <a href=""https://uptime.com/upwork.com"" rel=""nofollow noreferrer"">upwork.com</a></p>
","<dns-hosting><cloudflare>","2016-09-06 21:19:55"
"875021","nginx - redirect all requests for files which do not exist to index page?","<p>I have a simple website with 1 html file <code>index.html</code>. I want that all user that type <code>example.com/abcde</code> will be redirected to <code>example.com</code>
Of course, all links to resource files (images/css/js etc.) should work</p>

<p>Currently, I have this:</p>

<pre><code>location / {
    try_files $uri /index.html;
}
</code></pre>

<p>This setup works, users see index page if they try to open <code>example.com/abcde</code>. But URL in the browser does not change... How to rewrite the URL?</p>
","<nginx><rewrite>","2017-09-22 13:15:06"
"801600","redhat linux + yum updateinfo list + what is the equivalent command for redhat 5","<p>the command </p>

<pre><code>yum updateinfo list
</code></pre>

<p>running well on redhat 6 and 7 as the following</p>

<pre><code>  yum updateinfo list
  Loaded plugins: product-id, rhnplugin, subscription-manager
  This system is receiving updates from RHN Classic or Red Hat Satellite.
  RHEA-2015:2461 enhancement Red_Hat_Enterprise_Linux-Release_Notes-7-en-US-      7-2.el7.noarch
  RHBA-2015:2371 bugfix      alsa-tools-firmware-1.0.28-2.el7.x86_64
  RHBA-2015:2403 bugfix      authconfig-6.2.8-10.el7.x86_64
  RHBA-2015:2144 bugfix      bash-4.2.46-19.el7.x86_64
</code></pre>

<p>but on redhat 5.X</p>

<p>I get the following results because  updateinfo not defined on redhat 5</p>

<pre><code>  #  yum updateinfo list
  Loaded plugins: rhnplugin, security
  This system is receiving updates from RHN Classic or RHN Satellite.
  usage: yum [options] COMMAND

  List of Commands:

  check-update   Check for available package updates
  clean          Remove cached data
  deplist        List a package's dependencies
  downgrade      downgrade a package
  erase          Remove a package or packages from your system
  groupinfo      Display details about a package group
  groupinstall   Install the packages in a group on your system
</code></pre>
","<linux><redhat><yum>","2016-09-07 14:14:29"
"942540","How to recover a TAR file from a partially overwritten XFS disk?","<p>This is the scenario.</p>

<p>The environment is Linux (Arch Linux actually).</p>

<p>An uncompressed 1,3TB <code>tar</code> file has been written on a <strong>freshely XFS-formatted</strong> 2TB disk as a backup.</p>

<p>Later on, a 586MB UEFI boot image has been written (by mistake) to the very same disk device with this very command: <code>dd if=./bootimage of=/dev/sdd bs=4M</code>.</p>

<p>What I understand is, besides the idiocy of the action, that the disk hasn't been reformatted nor wiped. ""<em>Just</em>"" its first 500+MB worth of sectors have been overwritten.</p>

<p>My first attempt has been based on the assumption that the blocks were allocated linearly by XFS and that I knew the exact size of the overwritten portion. The idea was to skip all those blocks and then to try to pipe all subsequent blocks to the <code>cpio</code> tool: it can do its best to handle a damaged <code>tar</code> file (mine has been truncated on the head).</p>

<pre><code>FILESIZE=614465536
SECTSIZE=$(( 2 * 1024 * 1024 )) # 2M
SKIPSIZE=$(( $FILESIZE / $SECTSIZE ))

dd if=/dev/sdd ibs=$SECTSIZE obs=$SECTSIZE skip=$SKIPSIZE | cpio -ivd -H ustar
</code></pre>

<p>(I switched to 2M block transfer becasue the file size is a mutiple of it but not of 4M).
No luck at all for recovery. But now I know that the disk layout used by XFS isn't linear.</p>

<p>Next step has been to try to <em>repair</em> the file system (well a copy of it) with <code>xfs_repair</code> once the partition table had been fixed with <code>fdisk</code>.
It found the ""xfs signature"" and I accessed that single partition with the help of a <code>loop</code> device. Unluckily <code>xfs_repair</code> failed with a ""read only 0 of 512 bytes"".
Moreover it seems tyhere's no way to recover lost files under XFS.</p>

<p>Third attempt has been done with the help tools like <code>foremost</code> and <code>testdisk</code>. But my attempts have shown little success so far. They actually have been able to recover some files, mainly multimedia files (GIFs, JPGs, PNGs, WAVs and MP3s). But those are a fraction of the actual content of the backup. It looks like <code>foremost</code> has a focus on typical Windows files. But they cover about 15% of the 1.3TB of data. There should also be lots of text files, libreoffice files, and also <code>gzip</code> and <code>bzip2</code> files. So far 15% is better than 0%.</p>

<p>I have also searched through all documentation I have at hand and also ""googled"" for similar scenarios (also here on <strong>serverfault</strong>). The more relevant ones were about sending the disk to data recovery firms. No similar task seems to be documented on the Internet.</p>

<p>What'd be the best strategy in order to maximize the file recover?</p>

<p>The perfect one would aim at recovering the surviving part of that single <code>tar</code> file by recovering the remaining part of the i-node chain.</p>
","<linux><data-recovery><tar><xfs><arch-linux>","2018-12-02 14:36:44"
"942551","Specifying lower priority backup IP for a nameserver","<p>Let's assume that I use EasyDNS as a DNS service provider.</p>

<p>There I have an <code>A</code>-record entry for <code>ns-at-premises.example.com</code> pointing to the IP address <code>192.0.2.123</code>, where a DNS server is located, at my premises, listening on port 53.</p>

<p>There's also a <code>CNAME</code>-record entry for <code>ns.example.com</code> with the value of <code>ns-at-premises.example.com</code> (I read somewhere that <code>NS</code>-Records should get <code>CNAMEd</code>).</p>

<p><code>example.com</code> has an <code>A</code>-record of <code>192.0.2.234</code>, a VPS hosted on AWS and <code>www.example.com</code> has a <code>CNAME</code> of <code>example.com</code>, but that is irrelevant.</p>

<p>There is a subdomain <code>dynamic.example.com</code> which has a <code>NS</code>-Record of <code>ns.example.com</code>, which will send any request for <code>*.dynamic.example.com</code> or <code>dynamic.example.com</code> to be resolved at <code>192.0.2.123</code>, where <code>*</code> can be anyting.</p>

<p>--</p>

<p>What I now want to do is add a backup nameserver in case <code>192.0.2.123</code> goes down. This means that a port-53 server at <code>192.0.2.124</code> should get queried in case <code>192.0.2.123</code> is not reachable.</p>

<p>I would add an <code>A</code>-Record for <code>ns-backup.example.com</code> pointing to <code>192.0.2.124</code>, and a <code>CNAME</code> for <code>ns2.example.com</code> with the value of <code>ns-backup.example.com</code></p>

<p>--</p>

<p>The question is, how can I now tell EasyDNS that <code>dynamic.example.com</code> should query or redirect to <code>ns2.example.com</code> when <code>ns.example.com</code> is not reachable?</p>

<p>I read that just adding a second <code>NS</code>-Record with the value of <code>ns2.example.com</code> to <code>dynamic.example.com</code> would result in clients using <code>ns.example.com</code> and <code>ns2.example.com</code> for lookups in a Round-Robin fashion, but what I actually want is that <code>ns2.example.com</code> only gets queried if <code>ns.example.com</code> is down.</p>

<p>So I am actually looking for a way to add weights to the nameservers, like <code>MX</code>-Records are able to provide.</p>

<p>Is this possible? Or must I settle with the idea that none of those two nameservers will be a primary and deal with them as loadbalanced non-prioritized servers?</p>
","<domain-name-system><nameserver><dns-zone>","2018-12-02 17:54:04"
"801757","The requested URL /magento/ was not found on this server","<p>I try to install magaento2 with php7 apache2 from web.</p>

<p>it succeded. Installation succeeded.</p>

<pre><code>Magento Admin Info:

Username:
admin
Email:
admin@test.com
Password:
******
Your Store Address:
http://localhost/magento/
Magento Admin Address:
http://localhost/magento/admin/
</code></pre>

<p>but when i click those linkss</p>

<p>i got error</p>

<pre><code>Not Found

The requested URL /magento/ was not found on this server.

Apache/2.4.18 (Ubuntu) Server at localhost Port 80
</code></pre>

<p>in other topics, people say</p>

<p>go to</p>

<pre><code>/etc/php/7.0/apache2/conf.d
</code></pre>

<p>and some lines. But at that directory, i hve a lot of ini, it is not a texxtfile</p>

<pre><code>“/etc/php/7.0/apache2/conf.d” is a directory.
</code></pre>

<p>when i try to open wit gedit</p>

<p>what can i do now?</p>

<p>but for</p>

<pre><code>gedit /etc/apache2/apache2.conf
</code></pre>

<p>it shows</p>

<pre><code># This is the main Apache server configuration file.  It contains the
# configuration directives that give the server its instructions.
# See http://httpd.apache.org/docs/2.4/ for detailed information about
# the directives and /usr/share/doc/apache2/README.Debian about Debian specific
# hints.
#
#
# Summary of how the Apache 2 configuration works in Debian:
# The Apache 2 web server configuration in Debian is quite different to
# upstream's suggested way to configure the web server. This is because Debian's
# default Apache2 installation attempts to make adding and removing modules,
# virtual hosts, and extra configuration directives as flexible as possible, in
# order to make automating the changes and administering the server as easy as
# possible.

# It is split into several files forming the configuration hierarchy outlined
# below, all located in the /etc/apache2/ directory:
#
#   /etc/apache2/
#   |-- apache2.conf
#   |   `--  ports.conf
#   |-- mods-enabled
#   |   |-- *.load
#   |   `-- *.conf
#   |-- conf-enabled
#   |   `-- *.conf
#   `-- sites-enabled
#       `-- *.conf
#
#
# * apache2.conf is the main configuration file (this file). It puts the pieces
#   together by including all remaining configuration files when starting up the
#   web server.
#
# * ports.conf is always included from the main configuration file. It is
#   supposed to determine listening ports for incoming connections which can be
#   customized anytime.
#
# * Configuration files in the mods-enabled/, conf-enabled/ and sites-enabled/
#   directories contain particular configuration snippets which manage modules,
#   global configuration fragments, or virtual host configurations,
#   respectively.
#
#   They are activated by symlinking available configuration files from their
#   respective *-available/ counterparts. These should be managed by using our
#   helpers a2enmod/a2dismod, a2ensite/a2dissite and a2enconf/a2disconf. See
#   their respective man pages for detailed information.
#
# * The binary is called apache2. Due to the use of environment variables, in
#   the default configuration, apache2 needs to be started/stopped with
#   /etc/init.d/apache2 or apache2ctl. Calling /usr/bin/apache2 directly will not
#   work with the default configuration.


# Global configuration
#

#
# ServerRoot: The top of the directory tree under which the server's
# configuration, error, and log files are kept.
#
# NOTE!  If you intend to place this on an NFS (or otherwise network)
# mounted filesystem then please read the Mutex documentation (available
# at &lt;URL:http://httpd.apache.org/docs/2.4/mod/core.html#mutex&gt;);
# you will save yourself a lot of trouble.
#
# Do NOT add a slash at the end of the directory path.
#
#ServerRoot ""/etc/apache2""

#
# The accept serialization lock file MUST BE STORED ON A LOCAL DISK.
#
Mutex file:${APACHE_LOCK_DIR} default

#
# PidFile: The file in which the server should record its process
# identification number when it starts.
# This needs to be set in /etc/apache2/envvars
#
PidFile ${APACHE_PID_FILE}

#
# Timeout: The number of seconds before receives and sends time out.
#
Timeout 300

#
# KeepAlive: Whether or not to allow persistent connections (more than
# one request per connection). Set to ""Off"" to deactivate.
#
KeepAlive On

#
# MaxKeepAliveRequests: The maximum number of requests to allow
# during a persistent connection. Set to 0 to allow an unlimited amount.
# We recommend you leave this number high, for maximum performance.
#
MaxKeepAliveRequests 100

#
# KeepAliveTimeout: Number of seconds to wait for the next request from the
# same client on the same connection.
#
KeepAliveTimeout 5


# These need to be set in /etc/apache2/envvars
User ${APACHE_RUN_USER}
Group ${APACHE_RUN_GROUP}

#
# HostnameLookups: Log the names of clients or just their IP addresses
# e.g., www.apache.org (on) or 204.62.129.132 (off).
# The default is off because it'd be overall better for the net if people
# had to knowingly turn this feature on, since enabling it means that
# each client request will result in AT LEAST one lookup request to the
# nameserver.
#
HostnameLookups Off

# ErrorLog: The location of the error log file.
# If you do not specify an ErrorLog directive within a &lt;VirtualHost&gt;
# container, error messages relating to that virtual host will be
# logged here.  If you *do* define an error logfile for a &lt;VirtualHost&gt;
# container, that host's errors will be logged there and not here.
#
ErrorLog ${APACHE_LOG_DIR}/error.log

#
# LogLevel: Control the severity of messages logged to the error_log.
# Available values: trace8, ..., trace1, debug, info, notice, warn,
# error, crit, alert, emerg.
# It is also possible to configure the log level for particular modules, e.g.
# ""LogLevel info ssl:warn""
#
LogLevel warn

# Include module configuration:
IncludeOptional mods-enabled/*.load
IncludeOptional mods-enabled/*.conf

# Include list of ports to listen on
Include ports.conf


# Sets the default security model of the Apache2 HTTPD server. It does
# not allow access to the root filesystem outside of /usr/share and /var/www.
# The former is used by web applications packaged in Debian,
# the latter may be used for local directories served by the web server. If
# your system is serving content from a sub-directory in /srv you must allow
# access here, or in any related virtual host.
&lt;Directory /&gt;
    Options FollowSymLinks
    AllowOverride None
    Require all denied
&lt;/Directory&gt;

&lt;Directory /usr/share&gt;
    AllowOverride None
    Require all granted
&lt;/Directory&gt;

&lt;Directory /var/www/&gt;
    Options Indexes FollowSymLinks
    AllowOverride All 
    Require all granted
&lt;/Directory&gt;

#&lt;Directory /srv/&gt;
#   Options Indexes FollowSymLinks
#   AllowOverride None
#   Require all granted
#&lt;/Directory&gt;




# AccessFileName: The name of the file to look for in each directory
# for additional configuration directives.  See also the AllowOverride
# directive.
#
AccessFileName .htaccess

#
# The following lines prevent .htaccess and .htpasswd files from being
# viewed by Web clients.
#
&lt;FilesMatch ""^\.ht""&gt;
    Require all denied
&lt;/FilesMatch&gt;


#
# The following directives define some format nicknames for use with
# a CustomLog directive.
#
# These deviate from the Common Log Format definitions in that they use %O
# (the actual bytes sent including headers) instead of %b (the size of the
# requested file), because the latter makes it impossible to detect partial
# requests.
#
# Note that the use of %{X-Forwarded-For}i instead of %h is not recommended.
# Use mod_remoteip instead.
#
LogFormat ""%v:%p %h %l %u %t \""%r\"" %&gt;s %O \""%{Referer}i\"" \""%{User-Agent}i\"""" vhost_combined
LogFormat ""%h %l %u %t \""%r\"" %&gt;s %O \""%{Referer}i\"" \""%{User-Agent}i\"""" combined
LogFormat ""%h %l %u %t \""%r\"" %&gt;s %O"" common
LogFormat ""%{Referer}i -&gt; %U"" referer
LogFormat ""%{User-agent}i"" agent

# Include of directories ignores editors' and dpkg's backup files,
# see README.Debian for details.

# Include generic snippets of statements
IncludeOptional conf-enabled/*.conf

# Include the virtual host configurations:
IncludeOptional sites-enabled/*.conf

# vim: syntax=apache ts=4 sw=4 sts=4 sr noet
</code></pre>

<p>when i change some parts, it affects the localhost root. for example when i dsable, localhost becomes forbidden.</p>

<p>this is error log </p>

<pre><code>[Thu Sep 08 10:08:07.838247 2016] [mpm_prefork:notice] [pid 27202] AH00163: Apache/2.4.18 (Ubuntu) configured -- resuming normal operations
[Thu Sep 08 10:08:07.838287 2016] [core:notice] [pid 27202] AH00094: Command line: '/usr/sbin/apache2'
[Thu Sep 08 10:08:10.217465 2016] [mpm_prefork:notice] [pid 27202] AH00171: Graceful restart requested, doing restart
AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 127.0.1.1. Set the 'ServerName' directive globally to suppress this message
[Thu Sep 08 10:08:10.264056 2016] [mpm_prefork:notice] [pid 27202] AH00163: Apache/2.4.18 (Ubuntu) configured -- resuming normal operations
[Thu Sep 08 10:08:10.264067 2016] [core:notice] [pid 27202] AH00094: Command line: '/usr/sbin/apache2'
</code></pre>

<p>in</p>

<pre><code>/var/log/apache2/error.log
</code></pre>

<p>i also created apache.conf like this</p>

<pre><code>&lt;VirtualHost *:80&gt;
    DocumentRoot /var/www/html
    &lt;Directory /var/www/html/&gt;
        Options Indexes FollowSymLinks MultiViews
        AllowOverride All
    &lt;/Directory&gt;
&lt;/VirtualHost&gt;
</code></pre>

<p>in</p>

<pre><code>/etc/apache2/sites-available/magento.conf
</code></pre>

<p>and theen did this</p>

<pre><code>sudo a2ensite magento.conf
sudo a2dissite 000-default.conf
</code></pre>

<p>i looked from here but</p>

<p><a href=""https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-magento-on-ubuntu-14-04"" rel=""nofollow noreferrer"">https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-magento-on-ubuntu-14-04</a></p>

<p>but nothing changed</p>

<p>this is error log</p>

<pre><code>[Thu Sep 08 10:08:07.838247 2016] [mpm_prefork:notice] [pid 27202] AH00163: Apache/2.4.18 (Ubuntu) configured -- resuming normal operations
[Thu Sep 08 10:08:07.838287 2016] [core:notice] [pid 27202] AH00094: Command line: '/usr/sbin/apache2'
[Thu Sep 08 10:08:10.217465 2016] [mpm_prefork:notice] [pid 27202] AH00171: Graceful restart requested, doing restart
AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 127.0.1.1. Set the 'ServerName' directive globally to suppress this message
[Thu Sep 08 10:08:10.264056 2016] [mpm_prefork:notice] [pid 27202] AH00163: Apache/2.4.18 (Ubuntu) configured -- resuming normal operations
[Thu Sep 08 10:08:10.264067 2016] [core:notice] [pid 27202] AH00094: Command line: '/usr/sbin/apache2'
[Thu Sep 08 10:16:00.449232 2016] [mpm_prefork:notice] [pid 27202] AH00171: Graceful restart requested, doing restart
AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 127.0.1.1. Set the 'ServerName' directive globally to suppress this message
[Thu Sep 08 10:16:00.507141 2016] [mpm_prefork:notice] [pid 27202] AH00163: Apache/2.4.18 (Ubuntu) configured -- resuming normal operations
[Thu Sep 08 10:16:00.507153 2016] [core:notice] [pid 27202] AH00094: Command line: '/usr/sbin/apache2'
[Thu Sep 08 10:16:43.105955 2016] [mpm_prefork:notice] [pid 27202] AH00171: Graceful restart requested, doing restart
AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 127.0.1.1. Set the 'ServerName' directive globally to suppress this message
[Thu Sep 08 10:16:43.153499 2016] [mpm_prefork:notice] [pid 27202] AH00163: Apache/2.4.18 (Ubuntu) configured -- resuming normal operations
[Thu Sep 08 10:16:43.153511 2016] [core:notice] [pid 27202] AH00094: Command line: '/usr/sbin/apache2'
[Thu Sep 08 10:16:48.271177 2016] [mpm_prefork:notice] [pid 27202] AH00169: caught SIGTERM, shutting down
[Thu Sep 08 10:16:49.447808 2016] [mpm_prefork:notice] [pid 27820] AH00163: Apache/2.4.18 (Ubuntu) configured -- resuming normal operations
[Thu Sep 08 10:16:49.447838 2016] [core:notice] [pid 27820] AH00094: Command line: '/usr/sbin/apache2'
[Thu Sep 08 10:17:51.567331 2016] [mpm_prefork:notice] [pid 27820] AH00171: Graceful restart requested, doing restart
AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 127.0.1.1. Set the 'ServerName' directive globally to suppress this message
[Thu Sep 08 10:17:51.614927 2016] [mpm_prefork:notice] [pid 27820] AH00163: Apache/2.4.18 (Ubuntu) configured -- resuming normal operations
[Thu Sep 08 10:17:51.614939 2016] [core:notice] [pid 27820] AH00094: Command line: '/usr/sbin/apache2'
[Thu Sep 08 10:17:52.825263 2016] [mpm_prefork:notice] [pid 27820] AH00169: caught SIGTERM, shutting down
[Thu Sep 08 10:17:53.964814 2016] [mpm_prefork:notice] [pid 27945] AH00163: Apache/2.4.18 (Ubuntu) configured -- resuming normal operations
[Thu Sep 08 10:17:53.964843 2016] [core:notice] [pid 27945] AH00094: Command line: '/usr/sbin/apache2'
[Thu Sep 08 10:41:21.587129 2016] [mpm_prefork:notice] [pid 27945] AH00169: caught SIGTERM, shutting down
[Thu Sep 08 10:41:22.768479 2016] [mpm_prefork:notice] [pid 29209] AH00163: Apache/2.4.18 (Ubuntu) configured -- resuming normal operations
[Thu Sep 08 10:41:22.768509 2016] [core:notice] [pid 29209] AH00094: Command line: '/usr/sbin/apache2'
[Thu Sep 08 10:42:13.155839 2016] [mpm_prefork:notice] [pid 29209] AH00169: caught SIGTERM, shutting down
[Thu Sep 08 10:42:14.279786 2016] [mpm_prefork:notice] [pid 29294] AH00163: Apache/2.4.18 (Ubuntu) configured -- resuming normal operations
[Thu Sep 08 10:42:14.279817 2016] [core:notice] [pid 29294] AH00094: Command line: '/usr/sbin/apache2'
[Thu Sep 08 11:08:48.917446 2016] [mpm_prefork:notice] [pid 29294] AH00169: caught SIGTERM, shutting down
[Thu Sep 08 11:08:49.998267 2016] [mpm_prefork:notice] [pid 30868] AH00163: Apache/2.4.18 (Ubuntu) configured -- resuming normal operations
[Thu Sep 08 11:08:49.998299 2016] [core:notice] [pid 30868] AH00094: Command line: '/usr/sbin/apache2'
[Thu Sep 08 11:12:36.227958 2016] [mpm_prefork:notice] [pid 30868] AH00171: Graceful restart requested, doing restart
AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 127.0.1.1. Set the 'ServerName' directive globally to suppress this message
[Thu Sep 08 11:12:36.276950 2016] [mpm_prefork:notice] [pid 30868] AH00163: Apache/2.4.18 (Ubuntu) configured -- resuming normal operations
[Thu Sep 08 11:12:36.276962 2016] [core:notice] [pid 30868] AH00094: Command line: '/usr/sbin/apache2'
[Thu Sep 08 11:12:43.920686 2016] [mpm_prefork:notice] [pid 30868] AH00169: caught SIGTERM, shutting down
[Thu Sep 08 11:12:44.975202 2016] [mpm_prefork:notice] [pid 31184] AH00163: Apache/2.4.18 (Ubuntu) configured -- resuming normal operations
[Thu Sep 08 11:12:44.975231 2016] [core:notice] [pid 31184] AH00094: Command line: '/usr/sbin/apache2'
[Thu Sep 08 11:15:09.691761 2016] [mpm_prefork:notice] [pid 31184] AH00169: caught SIGTERM, shutting down
[Thu Sep 08 11:15:10.852670 2016] [mpm_prefork:notice] [pid 31344] AH00163: Apache/2.4.18 (Ubuntu) configured -- resuming normal operations
[Thu Sep 08 11:15:10.852705 2016] [core:notice] [pid 31344] AH00094: Command line: '/usr/sbin/apache2'
[Thu Sep 08 11:15:13.627029 2016] [mpm_prefork:notice] [pid 31344] AH00171: Graceful restart requested, doing restart
AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 127.0.1.1. Set the 'ServerName' directive globally to suppress this message
[Thu Sep 08 11:15:13.675088 2016] [mpm_prefork:notice] [pid 31344] AH00163: Apache/2.4.18 (Ubuntu) configured -- resuming normal operations
[Thu Sep 08 11:15:13.675101 2016] [core:notice] [pid 31344] AH00094: Command line: '/usr/sbin/apache2'
[Thu Sep 08 11:17:09.348813 2016] [mpm_prefork:notice] [pid 31344] AH00171: Graceful restart requested, doing restart
AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 127.0.1.1. Set the 'ServerName' directive globally to suppress this message
[Thu Sep 08 11:17:09.395006 2016] [mpm_prefork:notice] [pid 31344] AH00163: Apache/2.4.18 (Ubuntu) configured -- resuming normal operations
[Thu Sep 08 11:17:09.395019 2016] [core:notice] [pid 31344] AH00094: Command line: '/usr/sbin/apache2'
[Thu Sep 08 11:17:13.322077 2016] [mpm_prefork:notice] [pid 31344] AH00169: caught SIGTERM, shutting down
[Thu Sep 08 11:17:14.497462 2016] [mpm_prefork:notice] [pid 31549] AH00163: Apache/2.4.18 (Ubuntu) configured -- resuming normal operations
[Thu Sep 08 11:17:14.497501 2016] [core:notice] [pid 31549] AH00094: Command line: '/usr/sbin/apache2'
[Thu Sep 08 11:21:17.796019 2016] [mpm_prefork:notice] [pid 31549] AH00169: caught SIGTERM, shutting down
[Thu Sep 08 11:21:18.939220 2016] [mpm_prefork:notice] [pid 31739] AH00163: Apache/2.4.18 (Ubuntu) configured -- resuming normal operations
[Thu Sep 08 11:21:18.939252 2016] [core:notice] [pid 31739] AH00094: Command line: '/usr/sbin/apache2'
[Thu Sep 08 11:21:21.925933 2016] [mpm_prefork:notice] [pid 31739] AH00171: Graceful restart requested, doing restart
AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 127.0.1.1. Set the 'ServerName' directive globally to suppress this message
[Thu Sep 08 11:21:21.971902 2016] [mpm_prefork:notice] [pid 31739] AH00163: Apache/2.4.18 (Ubuntu) configured -- resuming normal operations
[Thu Sep 08 11:21:21.971914 2016] [core:notice] [pid 31739] AH00094: Command line: '/usr/sbin/apache2'
[Thu Sep 08 11:27:29.795435 2016] [mpm_prefork:notice] [pid 31739] AH00169: caught SIGTERM, shutting down
[Thu Sep 08 11:27:30.880443 2016] [mpm_prefork:notice] [pid 32048] AH00163: Apache/2.4.18 (Ubuntu) configured -- resuming normal operations
[Thu Sep 08 11:27:30.880477 2016] [core:notice] [pid 32048] AH00094: Command line: '/usr/sbin/apache2'
[Thu Sep 08 11:41:14.883168 2016] [mpm_prefork:notice] [pid 32048] AH00169: caught SIGTERM, shutting down
[Thu Sep 08 11:41:16.021082 2016] [mpm_prefork:notice] [pid 606] AH00163: Apache/2.4.18 (Ubuntu) configured -- resuming normal operations
[Thu Sep 08 11:41:16.021125 2016] [core:notice] [pid 606] AH00094: Command line: '/usr/sbin/apache2'
[Thu Sep 08 11:47:06.305602 2016] [mpm_prefork:notice] [pid 606] AH00169: caught SIGTERM, shutting down
[Thu Sep 08 11:47:07.457794 2016] [mpm_prefork:notice] [pid 864] AH00163: Apache/2.4.18 (Ubuntu) configured -- resuming normal operations
[Thu Sep 08 11:47:07.457825 2016] [core:notice] [pid 864] AH00094: Command line: '/usr/sbin/apache2'
[Thu Sep 08 11:50:48.006471 2016] [mpm_prefork:notice] [pid 864] AH00169: caught SIGTERM, shutting down
[Thu Sep 08 11:50:49.149710 2016] [mpm_prefork:notice] [pid 1126] AH00163: Apache/2.4.18 (Ubuntu) configured -- resuming normal operations
[Thu Sep 08 11:50:49.149743 2016] [core:notice] [pid 1126] AH00094: Command line: '/usr/sbin/apache2'
[Thu Sep 08 11:51:20.235219 2016] [mpm_prefork:notice] [pid 1126] AH00169: caught SIGTERM, shutting down
[Thu Sep 08 11:51:21.377260 2016] [mpm_prefork:notice] [pid 1210] AH00163: Apache/2.4.18 (Ubuntu) configured -- resuming normal operations
[Thu Sep 08 11:51:21.377295 2016] [core:notice] [pid 1210] AH00094: Command line: '/usr/sbin/apache2'
</code></pre>

<p>for</p>

<pre><code>AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 127.0.1.1. Set the 'ServerName' directive globally to suppress this message
</code></pre>

<p>i did</p>

<p><a href=""https://askubuntu.com/a/448403/590851"">https://askubuntu.com/a/448403/590851</a> </p>

<p>here</p>

<p>those are logs which @gloom700 suggested </p>

<pre><code>Generate custom log for your site. Below DocumentRoot add 2 lines . ""CustomLog /var/log/apache2/mysite-access.log combined"" ""ErrorLog /var/log/apache2/mysite-error.log"" Restart the service and check these log files 
</code></pre>

<p>i put those to my apache conf.</p>

<p>this is error log only after apache restart</p>

<pre><code>[Thu Sep 08 12:38:08.630069 2016] [mpm_prefork:notice] [pid 4059] AH00163: Apache/2.4.18 (Ubuntu) configured -- resuming normal operations
[Thu Sep 08 12:38:08.630108 2016] [core:notice] [pid 4059] AH00094: Command line: '/usr/sbin/apache2'


now i try to navigate to
</code></pre>

<p>localhost/magento</p>

<p>no errorlog but for acccess</p>

<pre><code>127.0.0.1 - - [08/Sep/2016:12:38:49 +0300] ""GET /magento HTTP/1.1"" 404 497 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.92 Safari/537.36""
</code></pre>

<p>now i try to navigate to</p>

<p>localhost/magento2
url changes to </p>

<p><a href=""http://localhost/magento/?SID=cq4mo1k2p1pik3me0ufk0p7ae4"" rel=""nofollow noreferrer"">http://localhost/magento/?SID=cq4mo1k2p1pik3me0ufk0p7ae4</a></p>

<p>access log (inclding previosu)</p>

<pre><code>127.0.0.1 - - [08/Sep/2016:12:38:49 +0300] ""GET /magento HTTP/1.1"" 404 497 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.92 Safari/537.36""
127.0.0.1 - - [08/Sep/2016:12:39:25 +0300] ""GET /magento2/ HTTP/1.1"" 302 490 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.92 Safari/537.36""
127.0.0.1 - - [08/Sep/2016:12:39:25 +0300] ""GET /magento/?SID=cq4mo1k2p1pik3me0ufk0p7ae4 HTTP/1.1"" 404 497 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.92 Safari/537.36""
</code></pre>

<p>error log</p>

<pre><code>[Thu Sep 08 12:38:08.630069 2016] [mpm_prefork:notice] [pid 4059] AH00163: Apache/2.4.18 (Ubuntu) configured -- resuming normal operations
[Thu Sep 08 12:38:08.630108 2016] [core:notice] [pid 4059] AH00094: Command line: '/usr/sbin/apache2'
</code></pre>

<p>now magento/admin
no error but accesss</p>

<pre><code>127.0.0.1 - - [08/Sep/2016:12:38:49 +0300] ""GET /magento HTTP/1.1"" 404 497 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.92 Safari/537.36""
127.0.0.1 - - [08/Sep/2016:12:39:25 +0300] ""GET /magento2/ HTTP/1.1"" 302 490 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.92 Safari/537.36""
127.0.0.1 - - [08/Sep/2016:12:39:25 +0300] ""GET /magento/?SID=cq4mo1k2p1pik3me0ufk0p7ae4 HTTP/1.1"" 404 497 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.92 Safari/537.36""
127.0.0.1 - - [08/Sep/2016:12:40:44 +0300] ""GET /magento/admin HTTP/1.1"" 404 503 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.92 Safari/537.36""
</code></pre>

<p>now magento/setup</p>

<p>access</p>

<pre><code>127.0.0.1 - - [08/Sep/2016:12:38:49 +0300] ""GET /magento HTTP/1.1"" 404 497 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.92 Safari/537.36""
127.0.0.1 - - [08/Sep/2016:12:39:25 +0300] ""GET /magento2/ HTTP/1.1"" 302 490 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.92 Safari/537.36""
127.0.0.1 - - [08/Sep/2016:12:39:25 +0300] ""GET /magento/?SID=cq4mo1k2p1pik3me0ufk0p7ae4 HTTP/1.1"" 404 497 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.92 Safari/537.36""
127.0.0.1 - - [08/Sep/2016:12:40:44 +0300] ""GET /magento/admin HTTP/1.1"" 404 503 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.92 Safari/537.36""
127.0.0.1 - - [08/Sep/2016:12:41:20 +0300] ""GET /magento/setup/ HTTP/1.1"" 404 504 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.92 Safari/537.36""
</code></pre>

<p>error</p>

<pre><code>[Thu Sep 08 12:38:08.630069 2016] [mpm_prefork:notice] [pid 4059] AH00163: Apache/2.4.18 (Ubuntu) configured -- resuming normal operations
[Thu Sep 08 12:38:08.630108 2016] [core:notice] [pid 4059] AH00094: Command line: '/usr/sbin/apache2'
</code></pre>

<p>i think error still was nt changed</p>

<p>now magento 2/admin</p>

<p>access</p>

<pre><code>127.0.0.1 - - [08/Sep/2016:12:38:49 +0300] ""GET /magento HTTP/1.1"" 404 497 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.92 Safari/537.36""
127.0.0.1 - - [08/Sep/2016:12:39:25 +0300] ""GET /magento2/ HTTP/1.1"" 302 490 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.92 Safari/537.36""
127.0.0.1 - - [08/Sep/2016:12:39:25 +0300] ""GET /magento/?SID=cq4mo1k2p1pik3me0ufk0p7ae4 HTTP/1.1"" 404 497 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.92 Safari/537.36""
127.0.0.1 - - [08/Sep/2016:12:40:44 +0300] ""GET /magento/admin HTTP/1.1"" 404 503 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.92 Safari/537.36""
127.0.0.1 - - [08/Sep/2016:12:41:20 +0300] ""GET /magento/setup/ HTTP/1.1"" 404 504 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.92 Safari/537.36""
127.0.0.1 - - [08/Sep/2016:12:42:10 +0300] ""GET /magento2/admin/ HTTP/1.1"" 302 647 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.92 Safari/537.36""
127.0.0.1 - - [08/Sep/2016:12:42:14 +0300] ""GET /magento/admin/admin/index/index/key/5693df064035428c19e5fa184b4afa34e077e014143dd178179a37cff78cb0e1/ HTTP/1.1"" 404 590 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.92 Safari/537.36""127.0.0.1 - - [08/Sep/2016:12:38:49 +0300] ""GET /magento HTTP/1.1"" 404 497 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.92 Safari/537.36""
127.0.0.1 - - [08/Sep/2016:12:39:25 +0300] ""GET /magento2/ HTTP/1.1"" 302 490 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.92 Safari/537.36""
127.0.0.1 - - [08/Sep/2016:12:39:25 +0300] ""GET /magento/?SID=cq4mo1k2p1pik3me0ufk0p7ae4 HTTP/1.1"" 404 497 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.92 Safari/537.36""
127.0.0.1 - - [08/Sep/2016:12:40:44 +0300] ""GET /magento/admin HTTP/1.1"" 404 503 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.92 Safari/537.36""
127.0.0.1 - - [08/Sep/2016:12:41:20 +0300] ""GET /magento/setup/ HTTP/1.1"" 404 504 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.92 Safari/537.36""
127.0.0.1 - - [08/Sep/2016:12:42:10 +0300] ""GET /magento2/admin/ HTTP/1.1"" 302 647 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.92 Safari/537.36""
127.0.0.1 - - [08/Sep/2016:12:42:14 +0300] ""GET /magento/admin/admin/index/index/key/5693df064035428c19e5fa184b4afa34e077e014143dd178179a37cff78cb0e1/ HTTP/1.1"" 404 590 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.92 Safari/537.36""
</code></pre>

<p>error
same stilli think.</p>
","<apache-2.2><php><magento>","2016-09-08 07:11:18"
"875315","CAA SSL Lets encrypt lookup","<p>If a domain is connected by pointing is it possible for the CNAME to provide the CAA response required by lets encrypt, or can this response only be provided by the controlling NameServers?</p>

<p>Thanks for any help a bit stuck on this. </p>

<hr>

<p>I reviewed the article <a href=""https://letsencrypt.org/docs/caa/"" rel=""nofollow noreferrer"">https://letsencrypt.org/docs/caa/</a></p>

<p>However, Its unclear what it querys for these records, are the provided by the NS, if such for a pointed domain the host they are pointed to would not be able to provide this record. </p>
","<ssl><lets-encrypt>","2017-09-25 09:55:45"
"801782","htaccess mod_rewrite for SEO URLs are incorrectly rewriting","<p>What I am attempting to do, is have pretty URLs. I have achieved this for the case where I want the entire string to point to one PHP script. I do not.</p>

<p>So, this works: <code>example.com/login/1/2</code> => <code>/index.php?go=login&amp;do=1&amp;msg=2</code></p>

<p>Using:</p>

<pre><code>RewriteEngine on
RewriteBase /
RewriteRule ^([0-9a-zA-Z_-]+)/?$ /index.php?go=$1 [L,QSA]
RewriteRule ^([0-9a-zA-Z_-]+)/([0-9a-zA-Z_-]+)/?$ /index.php?go=$1&amp;do=$2 [L,QSA]
RewriteRule ^([0-9a-zA-Z_-]+)/([0-9a-zA-Z_-]+)/([0-9a-zA-Z_-]+)/?$ /index.php?go=$1&amp;do=$2&amp;q1=$3 [L,QSA]
</code></pre>

<p>Now let's say I want <code>example.com/webapp/1/2/3</code> => <code>webapp.php?1=1&amp;2=2&amp;3=3</code></p>

<p>By adding this below the above:</p>

<pre><code>RewriteRule ^webapp/?$ /webapp.php [L,QSA]
RewriteRule ^webapp/([0-9a-zA-Z_-]+)/?$ /webapp.php?go=$1 [L,QSA]
RewriteRule ^webapp/([0-9a-zA-Z_-]+)/([0-9a-zA-Z_-]+)/?$ /webapp.php?go=$1&amp;do=$2 [L,QSA]
RewriteRule ^webapp/([0-9a-zA-Z_-]+)/([0-9a-zA-Z_-]+)/([0-9a-zA-Z_-]+)/?$ /webapp.php?go=$1&amp;do=$2&amp;q1=$3 [L,QSA]
</code></pre>

<p>Navigating to <code>example.com/webapp</code> redirects to <code>index.php</code> every time, stripping any query string.</p>

<p>If I comment out the first code block, and navigate to <code>example.com/webapp</code> I get <code>webapp.php</code> as I should.</p>

<p>I get no errors in any log. The access log during the call reports 302 to webapp</p>

<p>What am I doing wrong?</p>
","<apache-2.4><.htaccess><mod-rewrite><rewrite><seo>","2016-09-08 08:35:42"
"801797","incoming email is rejected","<p>I have built a mail server using ubuntu LTS 16.04 , postfix, dovecot, and mysql. There are several errors when there is an email sent to my server.</p>

<p>When I check <code>/var/log/mail.log</code>, these are the errors:</p>

<pre><code>Sep  8 07:12:50 mail postfix/smtpd[5269]: NOQUEUE: reject: RCPT from mail-yw0-f169.google.com[209.85.161.169]: 454 4.7.1 &lt;hari@bandungtalentsource.com&gt;: Relay access denied; from=&lt;kantorqu28@gmail.com&gt; to=&lt;hari@bandungtalentsource.com&gt; proto=ESMTP helo=&lt;mail-yw0-f169.google.com&gt;

Sep  8 07:12:51 mail postfix/smtpd[5269]: disconnect from mail-yw0-f169.google.com[209.85.161.169] ehlo=2 starttls=1 mail=1 rcpt=0/1 data=0/1 quit=1 commands=5/7
</code></pre>

<p>Here is my <code>postconf -n</code>:</p>

<p><a href=""https://i.sstatic.net/zh9hn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zh9hn.png"" alt=""enter image description here""></a></p>

<p>In this case, I have not yet implemented sasl authentication, spamassasin, clamav, or Amavis.</p>

<p>Thanks for the help</p>
","<postfix><smtp>","2016-09-08 09:30:36"
"875334","acl url_regex on squid3 is not working using an online tested regular expression","<p>I was asked to block Facebook access from 8:00am to 3:00pm for almost all users but they are bypassing the current defined rules to access the social network anyway. This is consuming a lot of our low bandwidth and we can't even work. I decided to design a <strong>regular expression (regex) to parse these URLs and block them</strong>. I don't want to block all facebook URLs but only alternatives. An alternative Facebook URLs mostly contains the words <strong>prod</strong> or <strong>iphone</strong>. The next ones are alternative Facebook URLs registered by our proxy server:</p>
<pre><code>m.iphone.touch.prod.facebook.com
m.iphone.haid.prod.facebook.com:443
m.ct.prod.facebook.com
m.vi-vn.prod.facebook.com
</code></pre>
<p>The designed regex: <code>/((?=.*\biphone\b)|(?=.*\bprod\b)).*\.facebook\.com(\:|\d|)/</code></p>
<p>I tested this regex on <a href=""https://regex101.com/"" rel=""nofollow noreferrer"">https://regex101.com/</a> and <a href=""https://www.regextester.com"" rel=""nofollow noreferrer"">https://www.regextester.com</a>. The regex is <strong>matching</strong> for:</p>
<pre><code>m.iphone.touch.prod.facebook.com
m.iphone.haid.prod.facebook.com:443
m.ct.prod.facebook.com
m.vi-vn.prod.facebook.com
</code></pre>
<p>And is <strong>not matching</strong> for:</p>
<pre><code>www.facebook.com
m.facebook.com
mqtt.facebook.com (for purple-facebook)
graph.facebook.com
connect.facebook.com
3-edge-chat.facebook.com
</code></pre>
<p>So far this is what I wanted, alternative URLs blocked and regular Facebook URLs allowed. <strong>My regex looks good to be used in squid</strong>.</p>
<p>Next step is to modify the file /etc/squid3/squid.conf by adding a new acl pointing the file that contains the regex:</p>
<pre><code>acl facebook dstdom_regex &quot;/etc/squid3/acl/facebook&quot; //The file contains the regex
http_access deny pass facebook
</code></pre>
<p>When I run <strong>squid3 -k parse</strong> for check the configuration file I am getting the errors:</p>
<pre><code>2017/09/22 11:12:26| Processing: acl facebook dstdom_regex &quot;/etc/squid3/acl/facebook&quot;
2017/09/22 11:12:26| squid.conf line 78: acl facebook dstdom_regex &quot;/etc/squid3/acl/facebook&quot;
2017/09/22 11:12:26| aclParseRegexList: Invalid regular expression '((?=.*\biphone\b)|(?=.*\bprod\b)).*\.facebook\.com(\:|\d|)': Invalid preceding regular expression
2017/09/22 12:39:33| Warning: empty ACL: acl facebook dstdom_regex &quot;/etc/squid3/acl/facebook&quot;
</code></pre>
<p>Obviously, the squid3 parser is tagging my acl as <strong>wrong</strong>, but I already tested it online and it was good to use. Also it says the acl is empty. What does it mean? The acl was declared with the name <strong>facebook</strong>. I am very confused at this.</p>
","<squid><access-control-list><regex>","2017-09-25 12:27:19"
"943076","Correct DNS Settings for iRedMail (or other mail servers)","<p>Given a fresh install of Ubuntu with a FQDN of example.com</p>

<pre><code>$ hostname -f
example.com
</code></pre>

<p>If user <a href=""https://docs.iredmail.org/install.iredmail.on.debian.ubuntu.html"" rel=""nofollow noreferrer"">installs iRedMail</a> (or another mail server) using demo.example.com as their first mail domain name.</p>

<p><a href=""https://i.sstatic.net/KTNxZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KTNxZ.png"" alt=""enter image description here""></a></p>

<p>His mail account is therefore postmaster@demo.example.com</p>

<p>What should the A and MX DNS settings look like?</p>

<p>Here is my attempt:</p>

<pre><code>A example.com SERVER_IP
A demo.example.com SERVER_IP
MX demo.example.com example.com
</code></pre>

<p>Edit:
My reasoning, is that both example.com and demo.example.com need to resolve to the server IP.</p>

<p>And the hostname demo.example.com should resolve to the demo.example.com for the MX record because the postmaster@demo.example.com needs to be processed via the main server example.com.</p>

<p>Is that correct?</p>
","<domain-name-system><email-server><iredmail>","2018-12-06 00:27:31"
"1017385","Does 6 cores ryzen 5 3600 x or 8 cores ryzen 7 3800x or intel i7 6700k or ryzen 9 3950x used for website/Nas enterprise based server","<p>I was thinking to set up a server for my tracking system or inventory management system. and I would like to know is 6 cores or 4 cores enough for the bare minimum or should i get 8 cores or the 16 cores cpu because I don't want to buy some overkill server that just do relatively simple task?</p>

<p>What do you guys think?</p>

<pre><code>Here is what i'll be doing for my server:
-NAS
-Website Hosting
-Inventory and Tracking System. 

I'm also planning to set RAID Array for it too here is the 2 i'm considering:
- Raid 1+0
- Raid 6+1
- Raid 1 
- Raid 6+1+0

And yes is there any recommendations for raid controller.

I would also need to consider the ram and mother:
For Ram (ECC) :
- 8gb
- 16gb
- 32gb
- 64gb


Motherboard:
- refurbished desktop ( i7 6th gen)
- MSI Z170A (ATX)
- X570M (mini ITX)
- X570 (ATX)
- B540M

</code></pre>
","<network-attached-storage><cpu-usage><website>","2020-05-16 07:44:39"
"801828","Cannot bind socket [0.0.0.0:5004]","<p>I need to start an haproxy server and I ran into a problem. It shows me this line:</p>

<blockquote>
  <p>[ALERT] 251/140351 (749) : Starting frontend localnodes: cannot bind
  socket [0.0.0.0:5004].</p>
</blockquote>

<p>I tried other ports like 9000 and 80, but the same problem remains.
So the first question is why is it happening and what can I do to solve it? </p>

<p>Second question is, as a part of the haproxy start line is ""<code>-p /run/haproxy.pid</code>"". What is this file?</p>

<p>Will appreciate the quick response. Thanks in advance :)</p>

<p>My haproxy.conf:</p>

<pre><code>global
   log /dev/log     local0
   log 127.0.0.1    local1 notice
   maxconn          256
   user             nobody
   group            haproxy
   daemon

defaults
   log              global
   mode             http
   option           httplog
   option           httplognull
   retries          3
   option           redispatch
   maxconn          2000
   timeout connect  5000
   timeout client   50000
   timeout server   50000

 frontend localnodes
   bind             *:5004
   mode             http
   default_backend  nodes
   option           forwardfor

backend nodes
   mode             http
   balance          roundrobin 
   server backend1 101.16.170.180:5004 check
</code></pre>
","<linux><configuration><port><haproxy>","2016-09-08 11:12:12"
"943102","Setting up firewall using iptables","<p>So, its getting more serious. I set up a ""real"" server. Before that I already configured firewalls using VMs. Now I got a question about which IP I need to allow, as I want to reject connections by default.</p>

<p>So lets say my local IP is <code>192.168.178.99</code> and my networks IP is <code>74.154.255.115</code>. 
I would suggest that I need to accept any connection from the networks address(<code>74.154.255.115</code>). But that would mean that anyone, who is using connecting to my router can access the server. 
Is there a some kind of common way to set up a secure server but still making sure I can connect to it from that specific device, not regarding the network the device is connected to? </p>
","<iptables><firewall>","2018-12-06 08:16:14"
"801855","CentOS7: configure static ip and ifconfig: command not found","<p>All I am trying to do is assign a static IP to a server. As per instructions online, I made following changes in the <code>/etc/sysconfig/network-scripts</code></p>

<pre><code>TYPE=Ethernet
BOOTPROTO=static
DEFROUTE=yes
PEERDNS=yes
PEERROUTES=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_PEERDNS=yes
IPV6_PEERROUTES=yes
IPV6_FAILURE_FATAL=no
NAME=xxxx (a valid name)
UUID=0c1c2330-b5aa-4fac-952e-ac61153853a7
DEVICE=(valid name)
ONBOOT=yes
IPADDR=192.168.1.xx (valid IP)
NETMASK=255.255.255.0
</code></pre>

<p>After this I ran :</p>

<p><code>service network restart</code></p>

<p>After which I try running <code>ifconfig</code> and I get:</p>

<p><code>bash: ifconfig: command not found</code></p>

<p>I cant ping google.com or install any packages using yum, for example when I try to run <code>yum provides ifconfig</code> , I get the following :</p>

<p>Could not retrieve mirrorlist <a href=""http://mirrorlist.centos.org/"" rel=""nofollow noreferrer"">http://mirrorlist.centos.org/</a>?          release=7&amp;arch=x86_64&amp;repo=os&amp;infra=stock error was
14: curl#6 - ""Could not resolve host: mirrorlist.centos.org; Unknown error""`</p>

<p>One of the configured repositories failed (Unknown),
 and yum doesn't have enough cached data to continue. At this point the only
 safe thing yum can do is fail.</p>

<p>This made me think that I need to add nameserver in /etc/resolv.conf file, so I added this to that file:</p>

<pre><code>nameserver 8.8.8.8
nameserver 8.8.4.4
</code></pre>

<p>and then I again restarted network service and tried to ping google.com and ended up getting:</p>

<pre><code>ping: unknown host google.com
</code></pre>

<p>I would appreciate any help that I can get in solving this. Thank you.</p>
","<ip><centos7>","2016-09-08 13:17:13"
"943211","HP G8 trays keep on showing disk activity when there should be none","<p>I have a DL380 G8 with a P420i raid controller.  2 RAID1 logical drives have been setup - one containing the OS (CentOS7) and one made of a single almost empty partition (contains a single tiny text file - it is a test machine at the moment).</p>

<p>The HP trays are meant to show activity, failures, etc. The OS logical drive trays do act seemingly normally (sometimes show activity, sometimes don't), but that second logical drive shows constant activity (the ""ring LEDs"" keep on moving around) even though there is no real reason for it to show activity as it contains a text file that no process is using at any time.</p>

<p>A few ""failure tests"" (removing a disk while running) show that otherwise the trays work correctly in showing a degraded array, etc.</p>

<p>The question is simply : what's going on? </p>
","<hard-drive><centos7><hp-proliant>","2018-12-06 21:06:34"
"1017589","How to add IPV4 range to unmanaged server","<p>I am really inexperienced with Linux and networking. </p>

<p>I bought an unmanaged dedicated server running ubuntu 16.04. I am having problems with binding the ipv4/24 range with my dedicated server. How would I bind/add my IP range to my dedicated server?</p>
","<linux><linux-networking><ubuntu-16.04><ip-address><ipv4>","2020-05-18 09:11:25"
"802031","Default Gateway is skipped when adding an additional IP address","<p>I'm currently faced with an issue were when adding an addition IP address, Tracert results to intended IP address but skips the gateway and goes straight to that IP as the 1st/last hop.But when I remove the added IP, then the gateway appears</p>

<p>This, unfortunately messes a couple of things i.e. IIS sites
How can I resolve this issue</p>

<p>Server 1</p>

<p>Network Adapter 1</p>

<pre><code>IP address: 192.168.2.10
Subnet Mask: 255.255.0.0
Gateway: 192.168.2.1
</code></pre>

<p>Additional IP address: <code>193.168.2.10</code><br>
Subnet Mask: <code>255.255.255.0</code>  </p>

<p>Server 2</p>

<p>Network Adapter 1</p>

<pre><code>IP address: 193.168.2.11
Subnet Mask: 255.255.0.0
Gateway: 193.168.2.1
</code></pre>

<p>Thank you in advance</p>
","<ip><gateway><route>","2016-09-09 06:40:49"
"875664","rsync copy contents of newest directory on remote server","<p>I'm trying to copy the contents of the newest directory from a remote server to a local server. Problem is that the directory names are date based, so it can't be static value in the command. I'd like to run the command from a cron as well so I'm hoping this is possible without some script.</p>

<p>Remote directory example (ls):</p>

<pre><code>/directory/backups
.
..
2017-09-23/
2017-09-24/
2017-09-25/
weekly/
</code></pre>

<p>So in this case, I'd like to copy the contents of the 2017-09-25 directory as that is the newest and both name and modified stamp are same date.</p>

<p>To give a visual example:</p>

<pre><code>rsync -chavzP --stats root@0.0.0.0 /directory/backups/(ls -td -- */ | head -n 1) /my_local_dir
</code></pre>

<p>Is there a way to do this with rsync or even scp?</p>

<p><strong>Edit:</strong></p>

<p>I realize this may seem like a trivial task. However, there are several reasons why I am seeking a way to do the copy in this manner. I've searched quite a bit but have not found anything to make it clear to me how to write the command to copy using either rsync or scp or whether it is even possible. My command-line-fu is basic. </p>
","<rsync><scp>","2017-09-27 01:20:09"
"802098","How to filter out identical double requests on server","<p>I've noticed that my Django application from time to time makes double database queries, instead of just one. The log file shows two similar requests at (almost) the exact same time.</p>

<p>Example:</p>

<pre><code>[pid: 749|app: 0|req: 892/2837] x.x.x.x () {44 vars in 896 bytes} [Fri Sep  9 12:00:55 2016] GET /clinic_profile/61 =&gt; generated 0 bytes in 1 msecs (HTTP/1.1 301) 3 headers in 134 bytes (1 switches on core 0)
[pid: 750|app: 0|req: 659/2838] x.x.x.x () {44 vars in 898 bytes} [Fri Sep  9 12:00:55 2016] GET /clinic_profile/61/ =&gt; generated 84989 bytes in 374 msecs (HTTP/1.1 200) 3 headers in 102 bytes (1 switches on core 0)
</code></pre>

<p>This is a critical flaw in my system and I need to fix it asap.</p>

<p><strong>Question:</strong> How can I filter out double requests?</p>

<p>Im using Django, Nginx and Ubuntu 14.04</p>
","<nginx><django><request-filtering>","2016-09-09 12:55:26"
"802099","Apache - change php version","<p>I have installed a new vesion of php on ubuntu.</p>

<p>But apache uses old version.</p>

<p>How to change version from 5.3 to 5.6 ?</p>

<p><a href=""http://joxi.net/1A5zRK5hKexKyr"" rel=""nofollow noreferrer"">http://joxi.net/1A5zRK5hKexKyr</a></p>

<p>I googled the question but nothing has helped</p>

<p>About question <a href=""https://serverfault.com/questions/428800/how-do-i-tell-apache-which-php-to-use"">How do I tell Apache which PHP to use?</a>
That question is like ""where I can find libphp5.so"". But I know where it is and apache loads that module but verion is still 5.3. </p>
","<ubuntu><php>","2016-09-09 12:55:35"
"875868","Postfix config to require authentication for outgoing mail etc","<p>I am looking for a config, but not been able to achieve.</p>

<ol>
<li><p>To make postfix require authentication for all outgoing mail. what I see like this:
<code>Passed CLEAN {RelayedOutbound}, LOCAL [127.0.0.1]</code></p></li>
<li><p>There are mails which are sent as spam by bots or malicious script like xitpl@mydomain.com.</p></li>
</ol>

<p>Now mydomain.com is my domain, but xitpl is no mailbox on my server/domain, yet the mail is queued as valid sender.</p>

<p>my config for postfix is like this:</p>

<pre><code>smtpd_relay_restrictions = 
permit_mynetworks 
permit_sasl_authenticated 
defer_unauth_destination

smtpd_reject_unlisted_sender = yes

smtpd_recipient_restrictions = permit_mynetworks, 
permit_sasl_authenticated, reject_unauth_destination, 
reject_rbl_client zen.spamhaus.org, 
check_recipient_access mysql:/etc/postfix/mysql-virtual_recipient.cf,
check_recipient_access mysql:/etc/postfix/mysql-virtual_policy_greylist.cf

smtpd_sender_restrictions = 
check_sender_access regexp:/etc/postfix/tag_as_originating.re, 

reject_authenticated_sender_login_mismatch, permit_mynetworks, 
permit_sasl_authenticated,
check_sender_access mysql:/etc/postfix/mysql-virtual_sender.cf,

check_sender_access regexp:/etc/postfix/tag_as_foreign.re
</code></pre>

<p>Is there anything which I am still missing or done a wrong config.</p>
","<postfix>","2017-09-28 03:46:37"
"943466","Edit fstab on unmounted device","<p>I tried to add swapfile to the root file system on AWS EC2 instance. Did  I made a mistake during the edit of fstab? </p>

<pre><code>sudo dd if=/dev/zero of=/swapfile bs=1G count=4
chmod 600 /swapfile
mkswap /swapfile
swapon /swapfile
swapon -s
</code></pre>

<p>I have edited /etc/fstab</p>

<pre><code>vi /etc/fstab
/swapfile swap swap defaults 0 0
</code></pre>

<p>Swap worked fine but after reboot, a system doesn't work. I have decided to attach device to another instance and remove a line in fstab, but it can't be mounted on another instance. </p>

<pre><code>mount: wrong fs type, bad option, bad superblock on /dev/xvdf,
       missing codepage or helper program, or other error

       In some cases useful info is found in syslog - try
       dmesg | tail or so.
</code></pre>

<p>How can I edit fstab and remove a line with swap data on unmounted device?</p>

<p><code>lsblk -f</code> output</p>

<pre><code>NAME    FSTYPE LABEL           UUID                             MOUNTPOINT
xvda                                                                
`-xvda1 ext4   cloudimg-rootfs 7b2XXX-16d3-XXXX-b32e-1857XXXXXXXX /
xvdf                                                                
`-xvdf1
</code></pre>
","<mount><ubuntu-16.04><fstab>","2018-12-08 19:49:57"
"802298","Enable ssl on open ldap","<p>I have a requirement to add ssl on ldap because the application use port 636. My question is both 636/ 3268 can enable same time or not? I havr multiple applications use one need regular 636 othet 3268. I am wobdering I need to build secondary ldap for ssl or can you the existing one?</p>

<p>Thanks,
Eli</p>
","<ldap><openldap>","2016-09-10 15:06:28"
"802319","Mesos: Web UI cannot connect to the server","<p>I'm newbie to Mesos and DC/OS. I set up DC/OS using vagrant and all seemed to go well at first. I am on Mac Yosemite, Virtualbox Version 5.0.26,  vagrant version 1.8.5 with the following patch: <a href=""https://github.com/mitchellh/vagrant/pull/7611"" rel=""nofollow noreferrer"">https://github.com/mitchellh/vagrant/pull/7611</a>.</p>

<p>I launched the DC/OS stack with following command: vagrant up m1 a1 a2 a3 a4 p1 boot. And I can see all instances launched and running, I can log in into each of them via ssh.  I can also see the web UI at: m1.dcos. In this UI, I managed to launch kafka and cassandra. The only problem is that after a few minutes, the UI stops working. I get the following error messages in the UI:</p>

<pre><code>Cannot Connect With The Server
</code></pre>

<p>or:
    Unable to complete request due to service [adminRouter] unavailability</p>

<p>The following site (<a href=""https://github.com/dcos/dcos-vagrant/blob/master/docs/troubleshooting.md"" rel=""nofollow noreferrer"">https://github.com/dcos/dcos-vagrant/blob/master/docs/troubleshooting.md</a>), has an entry for a similar issue and suggests running: service dcos-cosmos restart. The UI is still not accessible although all my 7 DC/OS nodes are up and running in Virtualbox. </p>

<p>I can ssh into each of these VMs but the whole web UI is not responding. If I use another browser and try m1.docs, the site is not reachable at all. What am I missing here?</p>
","<apache-mesos>","2016-09-10 18:54:47"
"875918","Custom internal IP instance","<p>I am trying to setup an instance (landingpage2) to use custom internal ip, after instance is up, I could not ssh to public IP, I even can't ping it.</p>

<p>If I don't use custom internal IP for the instance, everything is working fine.</p>

<p>Did I miss anything?</p>

<pre><code>fwissue@gcp2017-181116:~$ gcloud compute routes list | grep dmz1

default-route-4d479ca761d23b53  dmz1     10.8.0.0/24                              1000
default-route-552ffd32014e8b04  dmz1     0.0.0.0/0      default-internet-gateway  1000

fwissue@gcp2017-181116:~$ gcloud compute instances list

NAME          ZONE        MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP     STATUS
landingpage2  us-east1-b  n1-standard-1               10.8.0.2     35.190.156.124  RUNNING

fwissue@gcp2017-181116:~$ ping 35.190.156.124

PING 35.190.156.124 (35.190.156.124): 56 data bytes
--- 35.190.156.124 ping statistics ---
21 packets transmitted, 0 packets received, 100% packet loss
</code></pre>
","<networking><google-compute-engine>","2017-09-28 10:39:43"
"875945","server average load exact formula","<p>I was in assumption cpu average load is calculated depend on number of process in running state (R state) + process waiting for I/O ( D state). But today I noticed that server is showing high load average and number of running process is 1 and there is no process waiting for I/O. </p>

<p>I checked threads count also for running and I/O process that is minimum. Can someone give input from where this higher load average number is generating.</p>

<p>System is having 4 core cpu with ubuntu OS.</p>

<pre><code>top - 21:10:01 up 4 days, 23:29,  0 users,  load average: 32.31, 43.25, 19.64
Threads: 2077 total,   1 running, 2076 sleeping,   0 stopped,   0 zombie
%Cpu(s):  4.9 us,  2.0 sy,  0.1 ni, 92.0 id,  0.6 wa,  0.4 hi,  0.0 si,  0.0 st
KiB Mem:  16434332 total, 15447756 used,   986576 free,  1593972 buffers
</code></pre>

<p>KiB Swap:  6287356 total,  1259140 used,  5028216 free.  2754608 cached Mem</p>

<p>so here I am here looking for exact formula to calculate system average load.
Thanks in advance.</p>
","<linux><ubuntu-14.04><high-load><load-average>","2017-09-28 13:10:53"
"943589","MissingRegistrationforLocation","<p>I have some background in developing .NET applications, but I am new to Azure. When trying to publish my first web app, I am getting this error statement:</p>

<p><a href=""https://i.sstatic.net/tTvrz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tTvrz.png"" alt=""Error Message""></a></p>

<p>When logging into the cloud the app is up and running but apparently does not perform the tasks it is supposed to do.</p>
","<azure><web><web-applications>","2018-12-10 07:27:37"
"802426","Openvpn server route traffic for everybody except for himself?","<p>I want to set a subnet to subnet vpn gateway with Openvpn which should look like this:</p>

<p>[gwA]===[devA1]===[ovpnServer]&lt;---vpn--->[ovpnClient]===[devB1]===[gwB]</p>

<p>[gw?] are default gateways of my subnets<br/>
[devA1] is the first of N computers on my openvpn server's subnet<br/>
[devB1] is the first of M computers on my openvpn client's subnet<br/></p>

<p>This is not my first try with openvpn. I did several configuration like this, without having problem I can not resolve by myself. But this time, I have no more ideas.</p>

<p>I use tun interfaces on my openvpn nodes and apply all the documentation on routing and firewalling correctly.</p>

<p>My problem is:<br/>
- I can ping ovpnClient from ovpnServer<br/>
- I can ping ovpnServer from ovpnClient<br/>
- I can ping ovpnClient from any devA?<br/>
- I can ping ovpnServer from any devB?<br/>
- I can ping any devA? from ovpnClient<br/>
- I <strong>can not</strong> ping any devB? from ovpnServer (1)<br/>
- I can ping any devA? from any devB?<br/>
- I can ping any devB? from any devA?<br/>
(1) But I can ping any devB? from ovpnServer if I use my ovpnServer eth0 interface as my ping interface (""ping -I ${lanIP} ${any-devB-IP}"")</p>

<p>I'm note sure, but I suppose that my Linux ovpnServer system choose the tun0's IP as the IP from which to ping devB? machines. If that is correct, then this dont work with that IP.
But it works with the eth0's IP as the ping source IP!</p>

<p>Any advice to make my routing fully works?</p>

<p>Here are my iptables rules:</p>

<p>ovpnServer >$ iptables --list-rules:<br/></p>

<pre><code>-P INPUT ACCEPT
-P FORWARD ACCEPT
-P OUTPUT ACCEPT
-N DOCKER
-N DOCKER-ISOLATION
-N f2b-sshd
-A INPUT -p udp -m udp --dport 68 -j ACCEPT
-A INPUT -p tcp -m multiport --dports 22 -j f2b-sshd
-A FORWARD -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -p tcp -m set --match-set minuteman dst,dst -m tcp --tcp-flags FIN,SYN,RST,ACK SYN -j REJECT --reject-with icmp-port-unreachable
-A FORWARD -j DOCKER-ISOLATION
-A FORWARD -o docker0 -j DOCKER
-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -i docker0 ! -o docker0 -j ACCEPT
-A FORWARD -i docker0 -o docker0 -j ACCEPT
-A FORWARD -i eth0 -o tun0 -j ACCEPT
-A FORWARD -i tun0 -o eth0 -j ACCEPT
-A FORWARD -m state --state RELATED,ESTABLISHED -j ACCEPT
-A OUTPUT -p tcp -m set --match-set minuteman dst,dst -m tcp --tcp-flags FIN,SYN,RST,ACK SYN -j REJECT --reject-with icmp-port-unreachable
-A DOCKER-ISOLATION -j RETURN
-A f2b-sshd -j RETURN
</code></pre>

<p>ovpnServer >$ iptables -t nat --list-rules:<br/></p>

<pre><code>-P PREROUTING ACCEPT
-P INPUT ACCEPT
-P OUTPUT ACCEPT
-P POSTROUTING ACCEPT
-N DOCKER
-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER
-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER
-A POSTROUTING -s 192.168.254.0/24 ! -o docker0 -j MASQUERADE
-A POSTROUTING -o tun0 -j MASQUERADE
-A DOCKER -i docker0 -j RETURN
</code></pre>

<p>ovpnClient >$ iptables --list-rules:<br/></p>

<pre><code>-P INPUT ACCEPT
-P FORWARD ACCEPT
-P OUTPUT ACCEPT
-N DOCKER
-N DOCKER-ISOLATION
-N f2b-sshd
-A INPUT -p udp -m udp --dport 68 -j ACCEPT
-A INPUT -p tcp -m multiport --dports 22 -j f2b-sshd
-A FORWARD -j DOCKER-ISOLATION
-A FORWARD -o docker0 -j DOCKER
-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -i docker0 ! -o docker0 -j ACCEPT
-A FORWARD -i docker0 -o docker0 -j ACCEPT
-A FORWARD -p tcp -m set --match-set minuteman dst,dst -m tcp --tcp-flags FIN,SYN,RST,ACK SYN -j REJECT --reject-with icmp-port-unreachable
-A FORWARD -i eth0 -o tun0 -j ACCEPT
-A FORWARD -i tun0 -o eth0 -j ACCEPT
-A FORWARD -m state --state RELATED,ESTABLISHED -j ACCEPT
-A OUTPUT -p tcp -m set --match-set minuteman dst,dst -m tcp --tcp-flags FIN,SYN,RST,ACK SYN -j REJECT --reject-with icmp-port-unreachable
-A DOCKER-ISOLATION -j RETURN
-A f2b-sshd -j RETURN
</code></pre>

<p>ovpnClient >$ iptables -t nat --list-rules:<br/></p>

<pre><code>-P PREROUTING ACCEPT
-P INPUT ACCEPT
-P OUTPUT ACCEPT
-P POSTROUTING ACCEPT
-N DOCKER
-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER
-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER
-A POSTROUTING -s 192.168.254.0/24 ! -o docker0 -j MASQUERADE
-A POSTROUTING -o tun0 -j MASQUERADE
-A DOCKER -i docker0 -j RETURN
</code></pre>
","<iptables><routing><openvpn><interface>","2016-09-11 15:49:18"
"876100","Unable to access port 8080 externally (Windows server 2012 r2)","<p>I have developed a web-service on Visual Studio 2015 and using <code>MySQL</code> DB on <code>Windows 10</code>. I want to deploy it on <code>IIS</code>. So I installed it on my <code>Windows Server 2012 R2</code> by using <a href=""http://thesolving.com/server-room/how-to-install-and-configure-iis-on-windows-server-2012-r2/"" rel=""nofollow noreferrer"">How to Install IIS on Windows server 2012 R2</a>. After installation I have placed a test <code>html</code> file and placed it in a folder through which I have given access to created local site. The port set to the site is <code>8080</code>. When I run it on my server it's working and displaying me the page. But when I want to access it on my system it doesn't shows me anything. </p>

<p><a href=""https://i.sstatic.net/aqdZp.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/aqdZp.png"" alt=""enter image description here""></a></p>

<p>At server firewall is totally closed. </p>

<p><a href=""https://i.sstatic.net/RjxlD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RjxlD.png"" alt=""enter image description here""></a></p>

<p>Also I have tried <a href=""https://serverfault.com/a/108565"">this</a> solution. But still no progress. </p>

<p><strong>Note:-</strong></p>

<ol>
<li>On server side, 80 and 443 is used by Xampp server.</li>
<li>At my system, 8080 and 443 is used by Xampp server. But I have also stopped it and tried to run the URL but still no progress.</li>
</ol>

<p>Any help would be highly appreciated.</p>
","<windows-server-2012-r2><port><windows-10><windows-firewall><iis-8.5>","2017-09-29 08:47:24"
"802575","Nagios server service_perfdata not saved at the good place","<p>I have a server nagios  on my Raspberry who monitored some Windows 7 client and i want to save the performance data of my service in particular file.
So i have set the nagios.cfg for it :</p>

<pre><code>    process_performance_data=1





host_perfdata_command=process-host-perfdata-file
service_perfdata_command=process-service-perfdata-file



# HOST AND SERVICE PERFORMANCE DATA FILES
# These files are used to store host and service performance data.




host_perfdata_file=/home/pi/partage/
service_perfdata_file=/home/pi/partage/
</code></pre>

<p>But i dont see my service_perfdata_file  saved in this folder so what I missed plz ?
That folder was samba share folder.</p>
","<nagios><database-performance>","2016-09-12 14:14:26"
"943845","Single iptable rule matching /32 and jump","<p>We need to install a single iptables rule that match every source /32 user ip and jump to 'user chain':</p>

<pre><code>iptables -A INPUT -s ""EVERY_USER-IP/32"" -j ""USER_IP/32_CHAIN""
</code></pre>

<p>because we have hundreds of users we need/prefer to avoid using a lot of user-ip/32 rules like:</p>

<pre><code>iptables -A INPUT -s ""USER-IP1/32"" -j ""USER_IP1/32_CHAIN""

iptables -A INPUT -s ""USER-IP2/32"" -j ""USER_IP2/32_CHAIN""
</code></pre>

<p>...</p>
","<iptables><firewall><linux-networking>","2018-12-11 13:55:38"
"943849","Linux server fails file copy over 2.0 GB","<p>When using RSYNC, CP or DD on Linux to a Windows SMB share the file gets to 2.0 GB of 6.3 GB and errors stating file is too large.  </p>

<p>This has started since disabling SMB1 on the Windows share.  I have re-mapped the share using SMB 2.1 in <code>fstab</code> on Linux.</p>

<pre><code>Linux SERVERNAME 3.8.13-68.2.2.el6uek.x86_64 #2 SMP Tue May 12 15:10:51 PDT 2015 x86_64 x86_64 x86_64 GNU/Linux
</code></pre>

<hr>

<pre><code>rsync  version 3.0.6  protocol version 30
Copyright (C) 1996-2009 by Andrew Tridgell, Wayne Davison, and others.
Web site: http://rsync.samba.org/
Capabilities:
    64-bit files, 64-bit inums, 64-bit timestamps, 64-bit long ints,
    socketpairs, hardlinks, symlinks, IPv6, batchfiles, inplace,
    append, ACLs, xattrs, iconv, symtimes

rsync comes with ABSOLUTELY NO WARRANTY.  This is free software, and you
are welcome to redistribute it under certain conditions.  See the GNU
General Public Licence for details.
</code></pre>

<hr>

<p>I have tried <code>--partial-dir</code> and it does place the file in the partial directory but when you attempt to run it again to resume it, the file does not resume.  It starts over.  </p>

<pre><code>[root@SERVERNAME dpdump]# rsync -av --progress --partial-dir=.rsync-partial /u01/dpdump /BackupFolder
sending incremental file list
dpdump/
dpdump/PD20181119.dmp
  2084536320  31%  113.23MB/s    0:00:39
rsync: writefd_unbuffered failed to write 4 bytes to socket [sender]: Broken pipe (32)
rsync: write failed on ""/BackupFolder/dpdump/PD20181119.dmp"": File too large (27)
rsync error: error in file IO (code 11) at receiver.c(301) [receiver=3.0.6]
rsync: connection unexpectedly closed (303 bytes received so far) [sender]
rsync error: error in rsync protocol data stream (code 12) at io.c(600) [sender=3.0.6]
</code></pre>

<hr>

<p>I have tried using <code>block-size</code> and <code>protocol</code> from another SO post, still doesn't work.  </p>

<pre><code>[root@SERVERNAME dpdump]# rsync -av --progress --partial-dir=.rsync-partial --block-size=108485760 --protocol=29  /u01/dpdump /BackupFolder
building file list ...
220 files to consider
dpdump/
dpdump/PD20181119.dmp
  1998913536  29%  187.58MB/s    0:00:24
rsync: writefd_unbuffered failed to write 4092 bytes to socket [sender]: Broken pipe (32)
rsync: write failed on ""/BackupFolder/dpdump/PD20181119.dmp"": File too large (27)
rsync error: error in file IO (code 11) at receiver.c(301) [receiver=3.0.6]
rsync: connection unexpectedly closed (577 bytes received so far) [sender]
rsync error: error in rsync protocol data stream (code 12) at io.c(600) [sender=3.0.6]
</code></pre>

<hr>

<p>I have tried <code>append-verify</code> as the manpage recommends but it does not resume the file, it overwrites and starts from 0.  </p>

<pre><code>[root@SERVERNAME dpdump]# rsync -av --progress --append-verify  /u01/dpdump /BackupFolder
sending incremental file list
dpdump/
dpdump/PD20181119.dmp
  2147549183  32%   33.88MB/s    0:02:10
rsync: writefd_unbuffered failed to write 4 bytes to socket [sender]: Broken pipe (32)
rsync: write failed on ""/BackupFolder/dpdump/PD20181119.dmp"": File too large (27)
rsync error: error in file IO (code 11) at receiver.c(301) [receiver=3.0.6]
rsync: connection unexpectedly closed (302 bytes received so far) [sender]
rsync error: error in rsync protocol data stream (code 12) at io.c(600) [sender=3.0.6]
</code></pre>

<hr>

<p>Trying to do a 1GB file using <code>dd</code> works.  Trying to use a 3GB file using <code>dd</code> does not.  It stops at 2.1 GB.  </p>

<pre><code># dd if=/dev/zero of=/BackupFoldertest.img count=1 bs=1G
1+0 records in
1+0 records out
1073741824 bytes (1.1 GB) copied, 13.8795 s, 77.4 MB/s

# dd if=/dev/zero of=/BackupFoldertest.img count=1 bs=3G
0+1 records in
0+1 records out
2147479552 bytes (2.1 GB) copied, 28.625 s, 75.0 MB/s
#
</code></pre>

<p><strong>Previous rsync's that worked are shown.  Stopped when I disabled SMB1 on the Windows share.  The 12/10/2018 ones are the ones where it broke.</strong>  </p>

<p><a href=""https://i.sstatic.net/gxwCB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gxwCB.png"" alt=""enter image description here""></a></p>
","<linux><rsync>","2018-12-11 14:15:07"
"876212","dns not resolving remote domain when local zone file exist","<p>after I have done million researchs, I still no sure if this right or wrong. Please allow me to ask..</p>

<p>I have Bind DNS setup on Linux, and Resolver setup to listen localhost first then others.</p>

<p>I have a domain, <strong>example.com</strong> , which has amazon name servers.</p>

<p>On Linux, I also have zone file for <strong>example.com</strong> and its written as</p>

<pre><code>example.com.  86400   IN      NS     &lt; local host name &gt;
</code></pre>

<p>When I command </p>

<pre><code>nslookup example.com
</code></pre>

<p>It will return </p>

<pre><code>Server:     &lt; local host ip &gt;
Address:    &lt; local host ip &gt;#53
</code></pre>

<p>After removed/disabled zone file</p>

<p>It will return correct info</p>

<pre><code>Server:     &lt; amazon ns ip &gt;
Address:    &lt; amazon ns ip &gt;#53
</code></pre>

<p>I assume the correct setup is even when the zone file exists, dns can figure it out where is the real name server.</p>

<p>Can you guide me what is wrong, and where I should look into deeper?</p>
","<domain-name-system><domain><bind><dns-zone><dns-hosting>","2017-09-29 20:27:01"
"802709","Implications of hosting CNAMES and A records on separate name servers","<p>We host several websites but in most cases the client hosts their own DNS and mail separately.</p>

<p>I want to set up a system where we will create a subdomain on our domain for each client site which will act as the clients A record, so we can control the IP.</p>

<p>For example on the clients name server for domain clientwebsite.com set up:</p>

<pre><code>@    CNAME clientwebsite.sharedhostingenv.com
www  CNAME clientwebsite.sharedhostingevn.com
</code></pre>

<p>And on our name servers create the subdomain on sharedhostingenv.com:</p>

<pre><code>clientwebsite IN A 222.33.55.66
</code></pre>

<p>Will the fact that the client no longer hosts their own A record have any effect on any other records they have? Will this adversely affect any MX records that the client hosts?</p>

<p>edit:
Thank you for the feedback!!! I can't comment, but cheers.</p>
","<domain-name-system>","2016-09-13 05:27:45"
"1018573","Office 365 migration PST","<p>I can't migrate from Outlook to Office 365. In fact , I can't get into the cloud. My version of 365 is E1</p>

<p>It tells me that I am not authorized to identify myself on Azure while it is my admin account.</p>

<p>I am not added as an external user at the level of Tenant in AzureAD. What do you think about that ? Have you seen this problem ?</p>
","<migration><outlook><office365>","2020-05-25 08:25:02"
"1018598","a site use Linux Server take long time to load","<p>i get this problem first time since 5 years i use same site same script without any change .</p>

<p>i have a site xxxxxxx.com (php script) use a ip <a href=""http://1.1.1.1"" rel=""nofollow noreferrer"">http://1.1.1.1</a> (example) , i use centos as OS for the server . i use directadmin as panel to manage site config via address <a href=""http://1.1.1.1:2222"" rel=""nofollow noreferrer"">http://1.1.1.1:2222</a>
normaly i have 16cpu site can support as 60 000 visitors online without problem , sometimes when more i get message on directadmin panel that cpu reached 100% and site down so i need to restart server and waiting , i understand it's CPU issue.</p>

<p>But from last week , i get a weird thing , the site start to down and up and down and became unreachabl when visiotrs became more than 5000 online , also in same time the ip <a href=""http://1.1.1.1"" rel=""nofollow noreferrer"">http://1.1.1.1</a> of the site not accessible when the site down also <a href=""http://1.1.1.1/phpmyadmin"" rel=""nofollow noreferrer"">http://1.1.1.1/phpmyadmin</a> not working , BUT <a href=""http://1.1.1.1:2222"" rel=""nofollow noreferrer"">http://1.1.1.1:2222</a> is working normal so i can login and i need to access to service monitor to restar HTTPD and the site backs after 15s and may down also after some second if more visitors , ofcourse it is not problem of CPU.
i ask server support and they said nothing wrong and all good from theire side .</p>

<p>my question is : is it possible this problem can became from port network ? maybe they downgrade the speed of the port network ?</p>

<p>is possible that hosting provider can limit speed of port 80 ?</p>

<p>what i don't understand is ip of the site and apache <a href=""http://1.1.1.1(port"" rel=""nofollow noreferrer"">http://1.1.1.1(port</a> 80 normal) not works when more than 5000 online , but <a href=""http://1.1.1.1:2222"" rel=""nofollow noreferrer"">http://1.1.1.1:2222</a> panel ip stay works normal when the site down . maybe hosting provider limited or downgraded speed of port 80 ?</p>
","<linux><apache-2.2><centos><php><httpd>","2020-05-25 11:47:01"
"944097","Transfer RAID 0 from FreeNAS to Windows‌Ⓡ","<p>I have three disks in a RAID 0 with some data on it (not mission‐critical). This pool was created on a FreeNAS installation and I was wondering if there is a way to re‐mount the same RAID 0 in Windows Server.</p>
<p>I created three <code>vdisks</code> in my RAID card (one for each drive) and FreeNAS dealt with the pool creation across the three drives. Is there a way to open the array on Windows or at least link the three drives back together in Windows?</p>
<p>I know the filesystem is going to be a problem but I first want to know if it would be possible at all to re‐link these drives in Windows.</p>
","<windows><raid><freebsd><software-raid><truenas>","2018-12-12 21:33:45"
"876447","Many slow sectors on HDD","<p>After 7 years of using my Western Digital Caviar Black WD1002FAEX-00Z3A0 1Gb HDD looks like this: <a href=""https://i.sstatic.net/5Bcpq.jpg"" rel=""nofollow noreferrer"">https://i.sstatic.net/5Bcpq.jpg</a></p>

<p>I did this Victoria test after experiencing some glitches while watching movies. So, I have 1051110+21155+28797+24313+213+47 = 1125635 = 549 Mb of sectors, which are taking 20+ ms to respond. Can I somehow get rid of them, so HDD will just skip them and don't use them?</p>

<p>I don't want to throw away this disk. I want to know, what can I do to continue using this HDD.</p>
","<hard-drive><hardware><bad-blocks>","2017-10-02 13:32:20"
"944117","Setting SPF record for mail relay servers to avoid softfail","<p>I have an issue with correctly setting the SPF record for my own domain.
Initially my DNS settings as set by the domainhoster were as follows:</p>

<blockquote>
  <p>@  TXT     ""v=spf1 mx ~all""</p>
  
  <p>@  MX  10 mx101.solcon.nl</p>
  
  <p>@  MX  10 mx102.solcon.nl</p>
</blockquote>

<p>I tested sending an email and the receiving gmail account gave an SPF soft fail as seen below with the mail ending up in the spam folder.</p>

<blockquote>
  <p>Received: from mailrelay01.solcon.nl (mailrelay01.solcon.nl [XXXX:XXX:X:XX::XXX]) spf=softfail (google.com: domain of transitioning info@mydomain.com does not designate XXXX:XXX:X:XX::XXX as permitted </p>
</blockquote>

<p>I added the IP address that was not designated as permitted to the SPF record.</p>

<blockquote>
  <p>@  TXT     ""v=spf1 mx ip6:XXXX:XXX:X:XX::XXX ~all""</p>
</blockquote>

<p>However a week later, another one of my mails ended up in gmails spam folder.</p>

<blockquote>
  <p>Received: from mailrelay04.solcon.nl (mailrelay04.solcon.nl [XXXX:XXX:X:XX::XXX]) spf=softfail (google.com: domain of transitioning info@mydomain.com does not designate XXXX:XXX:X:XX::XXX as permitted </p>
</blockquote>

<p>It seems to me that the mailrelay servers are not set as mailservers in my default DNS settings of solcon.nl? Should I add these myself as MX or do I add an include in the SPF record?</p>

<blockquote>
  <p>include:_spf.solcon.nl</p>
</blockquote>
","<domain-name-system><email><spf>","2018-12-13 00:47:37"
"1018770","Wordpress setup using EC2 Instance","<p>As of now I can visit the wordpress installer on the browser</p>

<p>But the problem is I having an error <code>504 Gateway Time out</code></p>

<p><a href=""https://i.sstatic.net/BSrjQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BSrjQ.png"" alt=""enter image description here""></a></p>

<p>As I open the log file shown in the frontend this gaves multiple error like this</p>

<pre><code>POST-EXTACT-CHECKS
--------------------------------------
PERMISSION UPDATES:
   -DIRS:  '755'
    -FILES: '644'
[PHP ERR][WARN] MSG:chmod(): Operation not permitted [CODE:2|FILE:/var/www/html/etha/dup-
installer/lib/snaplib/class.snaplib.u.io.php|LINE:479]
</code></pre>

<p>I've already made <code>sudo chmod -R 777 /var/www/html/projectwp</code> but seems nothing works</p>

<p><strong>UPDATE</strong></p>

<p><a href=""https://i.sstatic.net/fUiP5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fUiP5.png"" alt=""enter image description here""></a></p>
","<linux><centos><wordpress>","2020-05-26 16:09:53"
"944292","How do I do traceroute/tracepath without any admin privileges?","<p>Basically, I need to ""trace a packet from my computer to the host"" to diagnose a network issue and I do not need any fancy additional functionality.</p>

<hr>

<p>Redirect me to the right place if this is not the place for this question, but I need to do something like traceroute/tracepath/tracert, but I have no admin privileges and I do not have these utilities installed. I think that the admin might have deliberately removed or disabled tracert. All I have is Java 8, Python 3, cygwin, and Windows 7. I can set local environment variables (ex. PATH, JAVA_HOME, etc) and I can put things in my home directory, but I cannot do full installations. Do you have or know of a way to do traceroute/tracepath/tracert given these limitations?</p>

<hr>

<p>I am considering <a href=""https://github.com/search?utf8=%E2%9C%93&amp;q=Traceroute%20stars%3A%3E40&amp;type=Repositories&amp;ref=advsearch&amp;l=&amp;l="" rel=""nofollow noreferrer"">these repos</a> (which I have not tried yet), but in general most of them will not work given the above limitations:</p>

<ul>
<li><a href=""https://github.com/simulatedsimian/tracetcp"" rel=""nofollow noreferrer"">https://github.com/simulatedsimian/tracetcp</a></li>
<li><a href=""https://github.com/trimstray/htrace.sh"" rel=""nofollow noreferrer"">https://github.com/trimstray/htrace.sh</a></li>
<li><a href=""https://github.com/Manisso/Crips"" rel=""nofollow noreferrer"">https://github.com/Manisso/Crips</a></li>
<li><a href=""https://github.com/simulatedsimian/tracetcp"" rel=""nofollow noreferrer"">https://github.com/simulatedsimian/tracetcp</a></li>
</ul>

<hr>

<p>Oh, also, I have Google Chrome installed and I can add these extensions:</p>

<ul>
<li><a href=""https://chrome.google.com/webstore/detail/ipvfoo/ecanpcehffngcegjmadlcijfolapggal"" rel=""nofollow noreferrer"">https://chrome.google.com/webstore/detail/ipvfoo/ecanpcehffngcegjmadlcijfolapggal</a></li>
<li><a href=""https://chrome.google.com/webstore/detail/ip-address/ghlojgpiinfelppegaabbiphgomaidml"" rel=""nofollow noreferrer"">https://chrome.google.com/webstore/detail/ip-address/ghlojgpiinfelppegaabbiphgomaidml</a></li>
</ul>

<hr>

<p>Note: I need to test/verify stuff inside inside my own local network. An online tool or service that runs in the context of a remote server (like for example <a href=""https://chrome.google.com/webstore/detail/network-and-internet-tool/ekpdpmpcgcmpaeokmclflfpadaklgpji?hl=en"" rel=""nofollow noreferrer"">this Google Chrome extension</a>) will not do it. </p>

<p>I also cannot use Raw Socket because ""only members of the Administrators group can create sockets of type SOCK_RAW on Windows 2000 and later.""</p>

<p>Follow up question: <a href=""https://superuser.com/q/1383578/453021"">https://superuser.com/q/1383578/453021</a> </p>
","<networking><security><linux-networking><icmp><traceroute>","2018-12-14 02:39:54"
"803103","Sending mail when relay = 127.0.0.1 gives me 550-5.7.1 (gmail)","<p>When sending email using web UI (for example roundcube) and my relay is =127.0.0.1
I'm getting  this error</p>

<blockquote>
  <p>host gmail-smtp-in.l.google.com[64.233.184.27] said: 550-5.7.1
  [193.198.1.11      12] Our system has detected that this message is
  550-5.7.1 likely unsolicited mail. To reduce the amount of spam sent
  to Gmail, 550-5.7.1 this message has been blocked. Please visit
  550-5.7.1</p>
</blockquote>

<p>Excerpt from the /var/log/mail.log</p>

<pre><code>Sep 14 21:28:06 mail postfix/qmgr[2915]: BB7418551: from=&lt;xxxx@gkri.hr&gt;, size=1082, nrcpt=1 (queue active)
Sep 14 21:28:07 mail amavis[2978]: (02978-01) Passed CLEAN {RelayedOpenRelay}, [193.198.1.11]:43798 [193.198.1.11] &lt;xxxxxx@gkri.hr&gt; -&gt; &lt;yyyyyyy@gmail.com&gt;, Queue-ID: C953F422, Message-ID: &lt;03227ea1376656afb567873be8981a60@gkri.hr&gt;, mail_id: 15_o4VKBS9UP, Hits: -7.293, size: 700, queued_as: BB7418551, 3127 ms
Sep 14 21:28:07 mail postfix/smtp[3045]: C953F422: to=&lt;yyyyyy@gmail.com&gt;, relay=127.0.0.1[127.0.0.1]:10024, delay=3.4, delays=0.17/0.05/0.04/3.1, dsn=2.0.0, status=sent (250 2.0.0 from MTA(smtp:[127.0.0.1]:10025): 250 2.0.0 Ok: queued as BB7418551)
Sep 14 21:28:07 mail postfix/qmgr[2915]: C953F422: removed
Sep 14 21:28:07 mail postfix/smtpd[3021]: disconnect from mail.gkri.hr[193.198.1.11]
Sep 14 21:28:07 mail dovecot: imap(xxxxx@gkri.hr): Disconnected: Logged out in=605 out=520
Sep 14 21:28:07 mail postfix/smtpd[3004]: rewrite stream disconnect
Sep 14 21:28:07 mail postfix/smtp[3052]: BB7418551: to=&lt;yyyyyy@gmail.com&gt;, relay=gmail-smtp-in.l.google.com[64.233.184.27]:25, delay=0.73, delays=0.19/0.06/0.34/0.15, dsn=5.7.1, status=bounced (host gmail-smtp-in.l.google.com[64.233.184.27] said: 550-5.7.1 [193.198.1.11      12] Our system has detected that this message is 550-5.7.1 likely unsolicited mail. To reduce the amount of spam sent to Gmail, 550-5.7.1 this message has been blocked. Please visit 550-5.7.1  https://support.google.com/mail/?p=UnsolicitedMessageError 550 5.7.1  for more information. qe3si5844139wjc.196 - gsmtp (in reply to end of DATA command))
Sep 14 21:28:07 mail postfix/cleanup[3043]: 954E88552: message-id=&lt;xxxxx@mail.gkri.hr&gt;
Sep 14 21:28:07 mail postfix/bounce[3053]: BB7418551: sender non-delivery notification: 954E88552
</code></pre>

<p>When sending from a e-mail client application (Android) I'm able to send 
my status is sent and no error is visible (e-mail goes right through)</p>

<blockquote>
  <p>status=sent 250 2.0.0 OK</p>
</blockquote>

<pre><code>Sep 14 21:37:04 mail postfix/qmgr[2915]: BE59C8551: from=&lt;xxxxxxx@gkri.hr&gt;, size=1893, nrcpt=1 (queue active)
Sep 14 21:37:04 mail postfix/smtpd[3179]: disconnect from localhost[127.0.0.1]
Sep 14 21:37:04 mail amavis[2978]: (02978-04) Passed CLEAN {RelayedOpenRelay}, [89.172.131.36]:56139 [89.172.131.36] &lt;xxxxxx@gkri.hr&gt; -&gt; &lt;yyyyyyy@gmail.com&gt;, Queue-ID: 0C126422, mail_id: 0O_1VMAZ9_sa, Hits: -3.523, size: 1511, queued_as: BE59C8551, 1501 ms
Sep 14 21:37:04 mail postfix/smtp[3177]: 0C126422: to=&lt;yyyyyy@gmail.com&gt;, relay=127.0.0.1[127.0.0.1]:10024, delay=1.9, delays=0.36/0.04/0.01/1.5, dsn=2.0.0, status=sent (250 2.0.0 from MTA(smtp:[127.0.0.1]:10025): 250 2.0.0 Ok: queued as BE59C8551)
Sep 14 21:37:04 mail postfix/qmgr[2915]: 0C126422: removed
Sep 14 21:37:05 mail postfix/smtp[3180]: BE59C8551: to=&lt;yyyyyyy@gmail.com&gt;, relay=gmail-smtp-in.l.google.com[74.125.71.26]:25, delay=0.62, delays=0.08/0.05/0.33/0.16, dsn=2.0.0, status=sent (250 2.0.0 OK 1473881825 tv2si5887390wjb.173 - gsmtp)
</code></pre>

<p>I have reverse DNS (ptr) set up as well as SPF.</p>

<p>Headers of the rejected mail</p>

<pre><code>eporting-MTA: dns; mail.gkri.hr
X-Postfix-Queue-ID: 92014114E0
X-Postfix-Sender: rfc822; rene.brakus@gkri.hr
Arrival-Date: Thu, 15 Sep 2016 14:31:29 +0200 (CEST)

Final-Recipient: rfc822; rene.brakus@gmail.com
Original-Recipient: rfc822;rene.brakus@gmail.com
Action: failed
Status: 5.7.1
Remote-MTA: dns; gmail-smtp-in.l.google.com
Diagnostic-Code: smtp; 550-5.7.1 [193.198.1.11      12] Our system has detected
    that this message is 550-5.7.1 likely unsolicited mail. To reduce the
    amount of spam sent to Gmail, 550-5.7.1 this message has been blocked.
    Please visit 550-5.7.1
    https://support.google.com/mail/?p=UnsolicitedMessageError 550 5.7.1  for
    more information. d5si660227wjm.249 - gsmtp

Return-Path: &lt;rene.brakus@gkri.hr&gt;
Received: from localhost (localhost [127.0.0.1])
    by mail.gkri.hr (Postfix) with ESMTP id 92014114E0
    for &lt;rene.brakus@gmail.com&gt;; Thu, 15 Sep 2016 14:31:29 +0200 (CEST)
Received: from mail.gkri.hr ([127.0.0.1])
    by localhost (mail.gkri.hr [127.0.0.1]) (amavisd-new, port 10024)
    with ESMTP id hFJ_WfGPDrby for &lt;rene.brakus@gmail.com&gt;;
    Thu, 15 Sep 2016 14:31:28 +0200 (CEST)
Received: from mail.gkri.hr (mail.gkri.hr [193.198.1.11])
    by mail.gkri.hr (Postfix) with ESMTPA id DEF0F114AA
    for &lt;rene.brakus@gmail.com&gt;; Thu, 15 Sep 2016 14:31:27 +0200 (CEST)
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8;
 format=flowed
Content-Transfer-Encoding: 8bit
Date: Thu, 15 Sep 2016 14:31:27 +0200
From: Rene Brakus &lt;rene.brakus@gkri.hr&gt;
To: &lt;rene.brakus@gmail.com&gt;
Subject: 87687686
Organization: =?UTF-8?Q?Gradska_knji=C5=BEnica_Rijeka?=
Message-ID: &lt;63143a0aefe8e00c9d080433cfe50492@gkri.hr&gt;
X-Sender: rene.brakus@gkri.hr
User-Agent: Roundcube Webmail/0.9-svn
</code></pre>

<p>Headers of the e-mail that went through </p>

<pre><code>Delivered-To: rene.brakus@gmail.com
Received: by 10.25.155.130 with SMTP id d124csp2856951lfe;
        Thu, 15 Sep 2016 01:09:06 -0700 (PDT)
X-Received: by 10.194.139.236 with SMTP id rb12mr6579607wjb.101.1473926946908;
        Thu, 15 Sep 2016 01:09:06 -0700 (PDT)
Return-Path: &lt;rene.brakus@gkri.hr&gt;
Received: from mail.gkri.hr (mail.gkri.hr. [193.198.1.11])
        by mx.google.com with ESMTP id t124si1544078wmt.5.2016.09.15.01.09.06
        for &lt;rene.brakus@gmail.com&gt;;
        Thu, 15 Sep 2016 01:09:06 -0700 (PDT)
Received-SPF: pass (google.com: best guess record for domain of rene.brakus@gkri.hr designates 193.198.1.11 as permitted sender) client-ip=193.198.1.11;
Authentication-Results: mx.google.com;
       spf=pass (google.com: best guess record for domain of rene.brakus@gkri.hr designates 193.198.1.11 as permitted sender) smtp.mailfrom=rene.brakus@gkri.hr
Message-Id: &lt;57da5722.822d1c0a.c3e70.4b27SMTPIN_ADDED_MISSING@mx.google.com&gt;
Received: from localhost (localhost [127.0.0.1]) by mail.gkri.hr (Postfix) with ESMTP id 63D741148C for &lt;rene.brakus@gmail.com&gt;; Thu, 15 Sep 2016 10:09:07 +0200 (CEST)
Received: from mail.gkri.hr ([127.0.0.1]) by localhost (mail.gkri.hr [127.0.0.1]) (amavisd-new, port 10024) with ESMTP id nTE8xSdhwfaS for &lt;rene.brakus@gmail.com&gt;; Thu, 15 Sep 2016 10:09:07 +0200 (CEST)
Received: from [192.168.88.209] (unknown [193.198.1.29]) by mail.gkri.hr (Postfix) with ESMTPA id 646EA11496 for &lt;rene.brakus@gmail.com&gt;; Thu, 15 Sep 2016 10:06:07 +0200 (CEST)
To: Rene &lt;rene.brakus@gmail.com&gt;
From: ""rene.brakus@gkri.hr"" &lt;rene.brakus@gkri.hr&gt;
Subject: Test
Date: Thu, 15 Sep 2016 10:06:06 +0200
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary=""----=_Part_0_1473926766295""

------=_Part_0_1473926766295
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: base64
Content-Disposition: inline

VGVzcgoK
------=_Part_0_1473926766295
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: base64
Content-Disposition: inline

PGRpdiBzdHlsZT0iZm9udC1mYW1pbHk6ICdDYWxpYnJpJywgJ3NhbnMtc2VyaWYnOyI+PGRpdiBk
aXI9Imx0ciI+VGVzcjwvZGl2PjwvZGl2Pjxicj4=
------=_Part_0_1473926766295--
</code></pre>
","<postfix><spam><gmail><email-bounces>","2016-09-14 20:03:03"
"1018825","Does Windows Server 2016 RRAS support NAT reflection/loopback?","<p>I have a home server running Windows Server 2016 Datacenter acting as the NAT on the network. On the same server I'm using Hyper-V to host various services (game servers, GitLab, Plex, etc). I'm using Routing and Remote Access to forward ports to the correct virtual machines.</p>

<p>From outside the network, everything works perfectly. Inside the network, me and other users that want to connect to specific servers need to know the ip address assigned to the server vs using a domain name I have through Cloudflare.</p>

<p>I've been researching all day on this, does RRAS not support this at all?</p>

<p>Or, as an alternate solution, is there a way for me to apply port forwarding rules on my internal network using a VM? Ie, connect to 192.168.1.5 at tcp port 58, redirect to 192.168.1.4 at tcp port 25565.</p>
","<nat><windows-server-2016><port-forwarding><rras>","2020-05-26 22:46:37"
"803178","Why ""Enter passphrase for key 'devenv-key.pem' "" if I didn't create any passprhase?","<p>I'm launching a EC2 from AWS on CLI. After I launch I can't ssh to it. When I try, I get the following message where I'm stuck:</p>

<p><code>Enter passphrase for key 'devenv-key.pem':</code></p>

<p>following the tutorial: <a href=""http://docs.aws.amazon.com/cli/latest/userguide/tutorial-ec2-ubuntu.html"" rel=""nofollow noreferrer"">aws tutorial</a></p>

<p>Using ubuntu 16.04</p>
","<amazon-web-services><ubuntu-16.04>","2016-09-15 04:51:02"
"876781","How to display filebeat logs as a single file in kibana?","<p>I have used ELK for displaying my middleware logs in kibana. for that I am using multiline patterns which mentioned below, after those logs displaying seperatly like separate expandable. How to display those logs as a single file ? as a single expandable ?
Because last couple of weeks I stucked in the same problem.</p>

<p><strong><em>Pattern I used:</em></strong></p>

<pre><code> multiline.pattern: '^\?'
 multiline.negate: true
 multiline.match: after
 multiline.max_lines: 100000
</code></pre>

<p><strong><em>Logs:</em></strong></p>

<pre><code>04:02:22,878 DEBUG [org.jboss.modules] (main) Module org.jboss.dmr:main defined by local module loader @42f30e0a (finder: local module finder @24273305 (roots: xxxxxx))
04:02:22,885 DEBUG [org.jboss.modules] (main) Module org.jboss.as.core-security:main defined by local module loader @42f30e0a (finder: local module finder @24273305 (roots: xxxxx)
04:02:22,887 DEBUG [org.jboss.modules] (main) Module org.jboss.as.core-security-api:main defined by local module loader @42f30e0a (finder: local module finder @24273305 (roots: xxxxx))
04:02:22,894 DEBUG [org.jboss.modules] (main) Module org.jboss.as.domain-management:main defined by local module loader @42f30e0a (finder: local module finder @24273305 (roots: xxxxx))
</code></pre>
","<logstash><elk><logstash-forwarder>","2017-10-04 08:50:30"
"803454","How can I confirm my server is running on a 1 Gbit network interface?","<p>I am not a network administrator by any means but need to confirm something for an automation project. I need to verify by screenshot whether my server is running on 1 Gbit network interface. How can I confirm that this feature exists on my Windows 2008 Server?</p>
","<windows-server-2008>","2016-09-16 08:07:02"
"803472","Any way for automated user profiles restore","<p>We had a Windows 2012 R2 server on a VPS hosting with Terminal services installed and about 40 active users.
Recently we encountered a system crash without any possibility to restore, so we managed to backup all user data and decided to reinstall the system.</p>

<p>Is there any automated way to grab user data and create user profiles not to do it manually? Some WSH script, migration tool, whatever.
Let's say, read directory content, create a user per profile with default password, and then copy all their desktop environments, settings, documents etc</p>

<p>I'm not very familiar with the Windows administering, thank you for understanding.</p>

<p>If possible I'd appreciate some script code snippet</p>
","<windows><restore><user-profile>","2016-09-16 09:13:21"
"1019179","Ubuntu 16.04 Postfix works, but email goes to SPAM","<p>Hello i follow this guide to install Postfix</p>

<p><a href=""https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-postfix-as-a-send-only-smtp-server-on-ubuntu-18-04"" rel=""nofollow noreferrer"">https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-postfix-as-a-send-only-smtp-server-on-ubuntu-18-04</a></p>

<p>and i added SPF, dmarc and DKIM 
I follow this guide for dkim <a href=""https://help.ubuntu.com/community/Postfix/DKIM"" rel=""nofollow noreferrer"">https://help.ubuntu.com/community/Postfix/DKIM</a></p>

<p>But still email going to SPAM.</p>

<p>Verification's: </p>

<pre><code>dig +short txt muhammadumerfarooq.me

""google-site-verification=L6ZS4-VgKMcG1cGygBfywF84DI-ANF4g6rrzninNbsI""
""-----BEGIN RSA PRIVATE KEY-----\010MIICXQIBAAKBgQDW25bTkpFlqxFhUG1kgTtSVGq0jloWB0rN7iNdXy/6OHZIUvmm\01040cZkqCn+AuiOeKYR68uA/wRabS2z3TkX2Xm25y4XArbc7LzRDg4M"" ""Eo79bm+q/Hr\010RBfgkvKs7namKBP7krdXNDg0Das14+DQ5hNlKgyyfeH5ZHXsfli64zJnwwIDAQAB\010AoGBAKeWE+jm074HVsEe6JSSMGEhMzGuLxorie9iJfd4fYWgcLIs9klz0UtjESiy\0100vAwUkwQ"" ""7dToir5SQwCshDJ1LpcarXlHTTt+wszEzvQsML8uv6HtfMLI8u3q9g0D\010Vx266tK5CYUUV4JBJP1/a/CfUvaNwcG9LNHs5ECpUqE58t5xAkEA9whbib14P9of\0104iKIV3Q32+4ABPDTenbZMG7LXtFz"" ""1hktNDUMTV+KIwb3WTc4e4DO3U2wlks48/15\010arRqW6up/wJBAN6oPBCt/P0l3Iedv/olDkUC0c60VCAt1wxG9rKw1wD6KrpQlRdc\010cRoxBszhn/jaQKOVadD/+Yg/aRAUZr35Gj0CQQCDvEQ8dvra"" ""ajLYf/vfT02+jfQa\010rmbIhvqZlmwDm4S/ZtuxXJy74jgjJ8LeI9GOIwmuAJEsBN9RVhnaqm2Rh0D3AkAq\010tKXgpR5zB4IG2PDrb5QPFH1dYiUIjwJCLpI+r8BtRY5QcghGlMp0tZaSUWw3dNUV\010CyF"" ""intYjldX26ZLTOSYZAkBeaZwaCPT/Y/sMf0iVGhxbwcn8DJmTLMW6uLtzdWyG\010NWHaANKPwNqsl8426ZZ27YPtPUJpaFxTBy5mE2K+UXSc\010-----END RSA PRIVATE KEY-----""
""v=spf1 include:_spf.google.com ~all""
""v=spf1 include:_mailcust.gandi.net ?all""


dig +short txt _dmarc.muhammadumerfarooq.me

""v=DMARC1;p=quarantine;pct=100;v=DMARC1;p=reject;pct=50;v=DMARC1;p=none;rua=mailto:contact@muhammadumerfarooq.me""
</code></pre>

<p>Is there any way to fix this?</p>

<h2>Edit</h2>

<p>Sometime email get back to me with following Output:</p>

<pre><code>This is the mail system at host server.muhammadumerfarooq.me.

I'm sorry to have to inform you that your message could not
be delivered to one or more recipients. It's attached below.

For further assistance, please send mail to postmaster.

If you do so, please include this problem report. You can
delete your own text from the attached returned message.

                   The mail system

&lt;mumerfarooqlablnet01@gmail.com&gt;: host
    gmail-smtp-in.l.google.com[2607:f8b0:400e:c07::1b] said: 550-5.7.1
    [2607:f130:0:d7::1e2] Our system has detected that this message does
    550-5.7.1 not meet IPv6 sending guidelines regarding PTR records and
    550-5.7.1 authentication. Please review 550-5.7.1
    https://support.google.com/mail/?p=IPv6AuthError for more information 550
    5.7.1 . 11si7555041plk.101 - gsmtp (in reply to end of DATA command)
Reporting-MTA: dns; server.muhammadumerfarooq.me
X-Postfix-Queue-ID: 69E01A0F92
X-Postfix-Sender: rfc822; www-data@muhammadumerfarooq.me
Arrival-Date: Fri, 29 May 2020 14:07:35 +0000 (UTC)

Final-Recipient: rfc822; mumerfarooqlablnet01@gmail.com
Original-Recipient: rfc822;mumerfarooqlablnet01@gmail.com
Action: failed
Status: 5.7.1
Remote-MTA: dns; gmail-smtp-in.l.google.com
Diagnostic-Code: smtp; 550-5.7.1 [2607:f130:0:d7::1e2] Our system has detected
    that this message does 550-5.7.1 not meet IPv6 sending guidelines regarding
    PTR records and 550-5.7.1 authentication. Please review 550-5.7.1
    https://support.google.com/mail/?p=IPv6AuthError for more information 550
    5.7.1 . 11si7555041plk.101 - gsmtp
Subject [Being Learning] Password Reset
From    WordPress
To  mumerfarooqlablnet01@gmail.com
Date    Today 19:07
Someone has requested a password reset for the following account:

Site Name: Being Learning

Username: lablnet

If this was a mistake, just ignore this email and nothing will happen.

To reset your password, visit the following address:

https://beinglearning.tech/wp-login.php?action=rp&amp;key=PrkZ2f4JudxRz5PH5ikw&amp;login=lablnet
</code></pre>

<p>Even i added IPv6 to DNS AAAA records.</p>
","<ubuntu><postfix><spam>","2020-05-29 13:56:33"
"803482","Properly Created Windows Share with Different Permissions","<p>In my network, I have 3 workstations and 1 server.</p>

<p>I want the user at each computer to be able to save their files to a share on the server. Each user should have their own directory.</p>

<p>As the names of the users may change I have created user accounts with the names of the computers they are assigned to.</p>

<p>I would like to make sure that each user is only able to access their files not the files of the other users.</p>

<p>My problem is when I give permission to access the share on the server to the user Workstation 1, the Workstation 1 user is not able to access the files. The user Workstation 1 gets the error about sharing permissions are not available contact your network administrator.</p>
","<windows><network-share><password><file-permissions><file-sharing>","2016-09-16 09:58:21"
"803552","Disable accounts on AD automatically","<p>here is a question that I couldn't find online. </p>

<p>Is there a way to disable accounts on AD automatically after a certain amount of time? </p>

<p>I know that you can just put an expiration date and the user wouldn't be able to login to the domain anymore but our audit dpt demand us to disable this account and delete it after 30 days. </p>

<p>I don't think a script that runs every day will be an option as, to be honest, we don't want to have scheduled tasks running all the time. </p>

<p>Let me know if I can provide any more information and thanks in advance! </p>
","<active-directory><users>","2016-09-16 14:04:19"
"945839","Deny Local Logon Windows server 2012 R2","<p>I'm trying to apply a New GPO that deny Local Logon in my client pc's but its not working even if i applied steps :
Computer Configuration > Policies > Security Settings > Local Policies > User Rights Assignment > Deny log on locally</p>

<p><img src=""https://i.sstatic.net/x0CNR.jpg"" alt=""enter image description here""></p>
","<active-directory><windows-server-2012-r2><group-policy>","2018-12-18 15:23:49"
"1019347","Cannot connect to VM from Windows 10","<p>I've created a basic VM in Azure. I've downloaded the RDP file and it keeps saying login attempt failed.</p>

<p>The RDP port is definitely open as I have connected to it via Telnet.</p>

<p>I've tried username as</p>

<p>vmname\vmuser </p>

<p>vmuser</p>

<p>\vmuser</p>

<p>What is a possible solution?</p>
","<azure><virtual-machines>","2020-05-30 16:55:42"
"945942","How to remove connection time out links form RHEL","<p>I want <strong>uninstall LAMP from my REDHAT virtual server</strong> and reinstall back it again but i don't have internet access on my server and when i uninstall mysql from my server i getting below error please help me how i do it. I am new in linux server.</p>

<p><a href=""https://i.sstatic.net/7E3uk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7E3uk.png"" alt=""enter image description here""></a></p>

<p>Also please help me <strong>how to install LAMP server on my redhat with out internet</strong>. I searched on google but not found anything for offline server. Please help me</p>
","<linux><php><httpd><lamp><rhel7>","2018-12-19 09:08:01"
"803745","Block all Ports only open Postfix SMTP port","<p>I am using centOS in my server and the control panel is Webmin Virtualmin. I have 2 IPs in my server: one is Default and 2nd IP is Additional IP. The Additional IP I am using only for mail sending so I want to block all Ports for my Additional IP, except the SMTP port.</p>
","<iptables><webmin>","2016-09-17 17:34:34"
"946105","How to configure managed switch","<p>The work environment is shown below. I want to install two new managed switches (cisco SG250-8) for the ones shown in green. I configuered the switches with one vlan and gave them the ip address 192.168.2.82 and 192.168.2.83 (subnet /24). I set the dns of the switches to the dc (192.168.2.3).</p>

<p>However, if I put them in place, I cannot connect to the internet from the clients. I'm not sure what other information is needed, but I would be happy to provide it. </p>

<p>Edit 22.12.18:</p>

<p>If I put the switch to the second branch (192.168.2.10 aso), the clients are able to connect to the internet. If I put it in the first branch, the clients  are no longer able to conncet to the internet. A ping from the switch to 8.8.8.8 never works.</p>

<p><a href=""https://i.sstatic.net/pCLM8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pCLM8.png"" alt=""enter image description here""></a></p>
","<networking><cisco><switch>","2018-12-20 11:22:51"
"946142","CentOS7 Kickstart issue","<p>I've prepared a <strong>ks.cfg</strong> file and validated it with <em>ksvalidator</em>, however it doesn't seem to do what I wish it was doing. 
For example - it keeps going into graphical mode despite specifying <em>text</em> option. Next it stops at language section, just so I would click ""Continue"" and doesn't seem to pick the language settings I have specified.</p>

<p><a href=""https://i.sstatic.net/zvrOa.png"" rel=""nofollow noreferrer"">enter image description here</a></p>

<p>After that it stops on ""Installation Summary"" section where I need to open ""System"" settings and click ""Done"" for it to proceed. It prompts the message that automated partitioning has been selected, however I have specified in my file what partitions I want to have created and removed the ""autopart"" option.</p>

<p><a href=""https://i.sstatic.net/J36mM.png"" rel=""nofollow noreferrer"">enter image description here</a>
<a href=""https://i.sstatic.net/ONRg8.png"" rel=""nofollow noreferrer"">enter image description here</a></p>

<p>Question is if my Kickstart file is incorrect, or maybe I'm missing some switches or messed up with settings order?
Below the updated config:</p>

<pre><code>#version=DEVEL
#Action
install

#INSTALLATION SOURCE SETTINGS:
# System authorization information
auth --enableshadow --passalgo=sha512
# Use CDROM installation media
cdrom
text
# Accept Eula
eula --agreed
# Run the Setup Agent on first boot
firstboot --enable
ignoredisk --only-use=sda

#LANGUAGE/LOCALE SETTINGS:
# Keyboard layouts
keyboard --vckeymap=pl --xlayouts='pl'
# System language
lang pl_PL.UTF-8

#NETWORK AND TIME/ZONE SETTINGS:
# Network information
network  --bootproto=dhcp --device=eth0 --ipv6=auto --activate
network  --bootproto=dhcp --device=eth1 --onboot=off --ipv6=auto
network  --bootproto=dhcp --device=eth2 --onboot=off --ipv6=auto
network  --hostname=

#USER/AUTHENTICATION SETTINGS:
rootpw --lock
user --groups=wheel --name= --password=

# SYSTEM SERVICES
services --enabled=chronyd, strongswan
# System timezone
timezone Europe/Warsaw --isUtc --ntpservers=0.centos.pool.ntp.org,1.centos.pool.ntp.org,2.centos.pool.ntp.org,3.centos.pool.ntp.org
# System bootloader configuration
bootloader --append="" crashkernel=auto"" --location=mbr --boot-drive=sda
part /boot --fstype=xfs --onpart=sda1 --size=512
part /root --fstype=xfs --onpart=sda2 --size=4096
part swap --fstype=swap --size=1024

#PACKAGES
%packages
@^minimal
@core
chrony
kexec-tools
strongswan

%end

%addon com_redhat_kdump --enable --reserve-mb='auto'

%end

%anaconda
pwpolicy root --minlen=6 --minquality=1 --notstrict --nochanges --notempty
pwpolicy user --minlen=6 --minquality=1 --notstrict --nochanges --emptyok
pwpolicy luks --minlen=6 --minquality=1 --notstrict --nochanges --notempty
%end
</code></pre>

<p>I've deleted the user details for obvious reasons.</p>
","<centos7><kickstart>","2018-12-20 16:13:55"
"877453","ssh force user to user ssh-add","<p>I am trying to understand how this functionality works.  I have digital ocean account.  I have given digital ocean an ssh public key to associate with any server that I spin up.  Once I've created a droplet if I try to ssh as root to the server it fails, but if I do an ssh-add and give it a the specific key that I defined with my digital ocean account then it allows me to log in.  If I were to add another user account and put a different public key in that accounts authorized_hosts file I would be able to log in without using ssh-add.</p>

<p>Can someone explain to me how this functionality works?  How do I force a user to use ssh-add?   </p>

<p>EDIT:  I did a verbose ssh, it seems to only try these keys, but not any of the other keys that I have in my .ssh directory including the one I have setup for digital ocean.  That I suspect may be part of the problem.</p>

<pre><code>debug1: Authentications that can continue: publickey
debug1: Trying private key: /Users/username/.ssh/id_dsa
debug3: no such identity: /Users/username/.ssh/id_dsa: No such file or directory
debug1: Trying private key: /Users/username/.ssh/id_ecdsa
debug3: no such identity: /Users/username/.ssh/id_ecdsa: No such file or directory
debug1: Trying private key: /Users/username/.ssh/id_ed25519
debug3: no such identity: /Users/username/.ssh/id_ed25519: No such file or directory
debug2: we did not send a packet, disable method
debug1: No more authentication methods to try.
</code></pre>
","<ssh><ssh-keys><ssh-agent>","2017-10-08 14:09:38"
"803937","How to find original sender of email?","<p>I found some messages that were to be delievered to rpub@domain.tld and would like to know who sent them. When searching the logs I find the sender is ""&lt;>"" so it might be postfix (mailer daemon), but how can I trace back the original sender ?</p>

<pre><code>Sep 18 14:34:02 messagerie postfix/cleanup[610]: 6766E1E922DB: message-id=&lt;20160918133402.6766E1E922DB@messagerie.domain.tld&gt;
Sep 18 14:34:02 messagerie postfix/qmgr[2749]: 6766E1E922DB: from=&lt;&gt;, size=35673, nrcpt=1 (queue active)
Sep 18 14:34:03 messagerie postfix/pipe[648]: 6766E1E922DB: to=&lt;rpub@domain.tld&gt;, relay=maildrop, delay=0.59, delays=0.03/0.19/0/0.37, dsn=4.3.0, status=deferred (temporary failure. Command output: /usr/bin/maildrop: Unable to create a dot-lock at /var/vmail/domain.tld/rpub/1116.0.messagerie.domain.tld.  )
Sep 18 14:41:30 messagerie postfix/qmgr[2749]: 6766E1E922DB: from=&lt;&gt;, size=35673, nrcpt=1 (queue active)
Sep 18 14:41:30 messagerie postfix/pipe[656]: 6766E1E922DB: to=&lt;rpub@domain.tld&gt;, relay=maildrop, delay=448, delays=448/0.07/0/0.05, dsn=4.3.0, status=deferred (temporary failure. Command output: /usr/bin/maildrop: Unable to create a dot-lock at /var/vmail/domain.tld/rpub/4281.0.messagerie.domain.tld.  )
Sep 18 14:51:30 messagerie postfix/qmgr[2749]: 6766E1E922DB: from=&lt;&gt;, size=35673, nrcpt=1 (queue active)
Sep 18 14:51:31 messagerie postfix/pipe[5595]: 6766E1E922DB: to=&lt;rpub@domain.tld&gt;, relay=maildrop, delay=1049, delays=1049/0.11/0/0.04, dsn=4.3.0, status=deferred (temporary failure. Command output: /usr/bin/maildrop: Unable to create a dot-lock at /var/vmail/domain.tld/rpub/5601.0.messagerie.domain.tld.  )
Sep 18 15:11:30 messagerie postfix/qmgr[2749]: 6766E1E922DB: from=&lt;&gt;, size=35673, nrcpt=1 (queue active)
Sep 18 15:11:30 messagerie postfix/pipe[8843]: 6766E1E922DB: to=&lt;rpub@domain.tld&gt;, relay=maildrop, delay=2248, delays=2248/0.11/0/0.05, dsn=4.3.0, status=deferred (temporary failure. Command output: /usr/bin/maildrop: Unable to create a dot-lock at /var/vmail/domain.tld/rpub/9050.0.messagerie.domain.tld.  )
</code></pre>

<hr>

<h1>Edit</h1>

<p>As pointed out by @RyanBabchishin I have skipped some lines in the logs. By searching carefully again for the qid I found this : </p>

<pre><code>Sep 18 14:34:02 messagerie postfix/cleanup[610]: 6766E1E922DB: message-id=&lt;20160918133402.6766E1E922DB@messagerie.algerian-radio.dz&gt;
[... Many lines later ...]
Sep 18 14:34:02 messagerie postfix/bounce[777]: 283821E922D9: sender non-delivery notification: 6766E1E922DB
Sep 18 14:34:02 messagerie postfix/qmgr[2749]: 6766E1E922DB: from=&lt;&gt;, size=35673, nrcpt=1 (queue active)
</code></pre>

<p>So in reality 6766E1E922DB is a bounce of 283821E922D9. If I search for 283821E922D9 I can get to the original sender (which should be rpub itself) :</p>

<pre><code>Sep 18 14:34:01 messagerie postfix/smtpd[31851]: 283821E922D9: client=localhost[127.0.0.1]
Sep 18 14:34:01 messagerie postfix/cleanup[718]: 283821E922D9: message-id=&lt;6045b91e32d9f289230c7a550015256d@algerian-radio.dz&gt;
Sep 18 14:34:01 messagerie postfix/smtpd[31851]: disconnect from localhost[127.0.0.1]
Sep 18 14:34:01 messagerie postfix/qmgr[2749]: 283821E922D9: from=&lt;rpub@algerian-radio.dz&gt;, size=32590, nrcpt=2 (queue active)
</code></pre>

<p>Quod Erat Demonstrandum</p>
","<postfix>","2016-09-19 10:31:49"
"877572","IPMI : FATAL: Error inserting ipmi_si","<p>i have a problem with ipmi:</p>

<p>i've already install ipmitool, ipmi_devintf, ipmi_msghandler, but when i try to install ipmi_si i receive this message:</p>

<p>$ modprobe ipmi_si module
FATAL: Error inserting ipmi_si (/lib/modules/2.6.32-642.11.1.el6.x86_64/kernel/drivers/char/ipmi/ipmi_si.ko): Unknown symbol in module, or unknown parameter (see dmesg) </p>

<p>this id a VM based on kvm with rhel 6.8.</p>

<p>Any tips ?</p>

<p>Thanks</p>
","<linux><rhel6><ipmi><drive><modprobe>","2017-10-09 10:18:41"
"1019772","Redirect the same connection multiple times [IPTABLES]","<p>Im trying to redirect the TCP connections a port multiple times to another ports, i tried this before but didn't work:</p>
<pre><code>iptables -t nat -A PREROUTING -i eth0 -p tcp --dport 60000 -j REDIRECT --to-port 60001
iptables -t nat -A PREROUTING -i eth0 -p tcp --dport 60001 -j REDIRECT --to-port 60002
</code></pre>
<p>I'm trying to get port 6000 to redirect tcp connections to port 60001, then port 60001 will redirect the connection to port 60002.</p>
<p>X -&gt; Y -&gt; Z</p>
<p>what can I do?</p>
","<ubuntu><networking><iptables><firewall><ubuntu-14.04>","2020-06-02 22:41:15"
"946305","SPF Outlook Web App (OWA) undeliverable email","<p><strong>PROBLEM:</strong> 
Outlook.com web app (OWA), synchronized with my local server via IMAP and SMTP(STARTTLS), returns ""Undeliverable message"" for all messages sent.</p>

<p><strong>PROPOSAL: (rejected)</strong>
This returning message points the issue comes up with the SPF and sugests updating DNS entry with a dynamic IP xx.xxx.xx.xxx A dynamic IP??? Nope.</p>

<p><strong>PROPOSAL: (not working)</strong>
Mocosoft sugests updating SPF entry with this entry ip4:23.103.224.0/19 ip4:206.191.224.0/19 ip4:40.103.0.0/16 include:spf.protection.outlook.com
Working? Nope.</p>

<p>This was the ""undeliverable message"" received.</p>

<blockquote>
  <p>Technical details
  InvalidRecipientsException: Invalid recipients were provided for the message: 'xxxxxxxx@gmail.com' (550 5.7.23 : Recipient address rejected: Message rejected due to: domain owner discourages use of this host. Please see <a href=""http://www.openspf.net/Why?s=mfrom;id=tadeus@mylocaldomain.com;ip=40.101.102.85;r="" rel=""nofollow noreferrer"">http://www.openspf.net/Why?s=mfrom;id=tadeus@mylocaldomain.com;ip=40.101.102.85;r=</a>
  ). --> Respuesta del servidor SMTP inesperada. Esperado: 250, real: 550, respuesta completa: 550 5.7.23 : Recipient address rejected: Message rejected due to: domain owner discourages use of this host. Please see <a href=""http://www.openspf.net/Why?s=mfrom;id=tadeus@mylocaldomain.com;ip=40.101.102.85;r="" rel=""nofollow noreferrer"">http://www.openspf.net/Why?s=mfrom;id=tadeus@mylocaldomain.com;ip=40.101.102.85;r=</a></p>
</blockquote>

<p>Failure code: 8f93</p>
","<smtp><outlook><spf><imap><outlook-web-app>","2018-12-21 17:49:11"
"946319","Port forwarding not working on Google Cloud","<p>I've created a rule in google cloud to allow inbound connections, but the port won't open.  Can't connect through telnet or online port checkers.  Can anyone tell me what could the issue be?  I included all my rules below as well but it's port 27015 I'm trying to open and on a Windows server (with firewall disabled)</p>

<p><a href=""https://i.sstatic.net/0SY80.png"" rel=""nofollow noreferrer"">Rule</a></p>

<p><a href=""https://i.sstatic.net/P7Dci.png"" rel=""nofollow noreferrer"">Rules</a></p>
","<google-cloud-platform><port-forwarding><google-compute-engine><windows-firewall>","2018-12-21 19:50:30"
"877781","A web app isn't visible from the Internet","<p>I have a ruby webapp running on the port 5555 on FreeBsd 11. For some reason it's not visible from the Internet at all, although ipfw isn't running.</p>

<pre><code>$ curl -i 12.34.55.66:5555
curl: (7) Failed to connect to 12.34.55.66:5555 port 5555: Connection refused
</code></pre>

<p>But the app is visible from that server, from localhost:</p>

<pre><code>curl -i 0.0.0.0:5555
[....returns data]

curl -i 127.0.0.1:5555
[....returns data]
</code></pre>

<p>What can be the reason?</p>

<p><strong>update:</strong></p>

<pre><code>$ sockstat
USER     COMMAND    PID   FD PROTO  LOCAL ADDRESS         FOREIGN ADDRESS      
user123  ruby       93839 7  tcp6   ::1:5555              *:*
user123  ruby       93839 8  tcp4   127.0.0.1:5555        *:*
</code></pre>
","<firewall><freebsd><web>","2017-10-10 13:01:10"
"804228","Conflict with Chrome and RFC2606","<p><a href=""https://www.rfc-editor.org/rfc/rfc2606#section-2"" rel=""nofollow noreferrer"">RFC2606</a> states the following for the &quot;localhost&quot; tld:</p>
<blockquote>
<p>The &quot;.localhost&quot; TLD has traditionally been statically defined in
host DNS implementations as having an A record pointing to the
loop back IP address and is reserved for such use.  Any other use
would conflict with widely deployed code which assumes this use.</p>
</blockquote>
<p>So if I'm reading this correctly, the IETF says that &quot;.localhost&quot; is a good tld for local website development. As such, my company requires that all of our local development projects use the tld &quot;.localhost&quot;. (We use Vagrant + Puppet to keep all of our dev environments identical across the team). For example the hosts file will have entries like this:</p>
<pre><code>192.168.10.10 someproject.localhost
10.9.8.7 anotherproject.localhost
</code></pre>
<p>Here's the problem, this tld works fine in all browsers EXCEPT Chrome. Chrome gives a ERR_CONNECTION_REFUSED message whenever the localhost tld is used. If I change the tld like so:</p>
<pre><code>192.168.10.10 someproject.loc
10.9.8.7 anotherproject.loc
</code></pre>
<p>Chrome works fine. In fact, I've tested a number of different arbitrary tlds and they also all work fine in Chrome. &quot;localhost&quot; is the only tld that gives the ERR_CONNECTION_REFUSED message on Chrome.</p>
<p>I'm using Windows 7, Chrome version 53.0.2785.116 m (64-bit). But I get the same error on Windows 10. Everyone on my team also gets the same error whether they're on their home or work computers (Windows and Mac).</p>
<p>Why is Chrome unable to connect when the tld is &quot;.localhost&quot;?</p>
","<virtualization><localhost><google-chrome>","2016-09-20 13:40:34"
"1020153","Convert public certificate to private","<p>I am rewriting the questions, as a result of the comments. I hope that this will increase clarity.</p>

<p>A while back we created with the AWS Certificate manager a public certificate which we are using to terminate the TLS traffic on our network load balancers.</p>

<p><a href=""https://i.sstatic.net/96xST.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/96xST.png"" alt=""enter image description here""></a></p>

<p>Here the option that was selected:</p>

<p><a href=""https://i.sstatic.net/EXbTf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EXbTf.png"" alt=""enter image description here""></a></p>

<p>Here an example for one of the load balancer configuration.</p>

<p><a href=""https://i.sstatic.net/Owh8e.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Owh8e.png"" alt=""enter image description here""></a></p>

<p>Now we would like to terminate the TLS traffic directly on an EC2 instead of the network load balancer. </p>

<p>When I want to export it in the UI the option for export is grayed out</p>

<p><a href=""https://i.sstatic.net/WClp3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WClp3.png"" alt=""enter image description here""></a></p>

<p>Is there a way to export the keys or if not convert our existing domain *.xxx.com to a private certificate.</p>

<p>Thanks, all help is very much appreciated!</p>
","<amazon-web-services><ssl-certificate>","2020-06-05 07:49:45"
"946697","How to install WordPress without using the /wp-admin/install.php script?","<p>I am trying to create a system that will install WordPress without any browser interaction like-</p>

<ul>
<li>I will create a database</li>
<li>I will upload/create <code>WordPress</code> required <code>database</code> tables manually
(<strong>I have installed a <code>WordPress</code> normally then collect the <code>database</code> tables and imported them into database</strong>).</li>
<li>I will create a <code>wp-config.php</code> manually and fill it with necessary
information what it requires(<code>database name,username,password,Authentication Unique Keys</code> etc).</li>
</ul>

<hr>

<p>Now problem is after done this process I ended up on here - 
It redirects me on <code>domain.com/wp-admin/install.php</code>.</p>

<p><strong>How to achieve install <code>WordPress</code> without any interaction from user,completely automatically?</strong>  </p>

<blockquote>
  <p>Note: I am actually trying to install WordPress on behalf of my users
  when they would register on  my platform it will install a WordPress
  on their wildcard subdomain automatically which I don't want to let them touch
  any part of WordPress installation</p>
</blockquote>
","<wordpress><installation><automation>","2018-12-26 19:52:28"
"877963","Powershell to export Windows 2008 R2 SMTP Relay Restrictions IPs","<p>I've been searching all over the interwebs and have had no luck so far. Does anyone know if it is possible to use Powershell to export the list of IP address within the SMTP Relay Restrictions section? We have a simple Windows 2008 R2 SMTP server that we use for internal relay and have a grant list of IP addresses that we need to export.</p>
","<iis><powershell><smtp>","2017-10-11 13:25:06"
"946710","Prove traffic when I own the server","<p>I work for the goverment and one project targets employees that might apply something from their employer, for example a vacation period. They login with username and password.</p>

<p>I want to record their actions and be able to proof that they did so, for example, when one asks for 3 days off, I must record it in a way so I can prove the time and what they asked. So later they can't deny it.</p>

<p>Currently, the best I can do is to log all the data (PHP 7.3 $_SERVER, $_POST, $_GET, $_SESSION), get a trusted timestamp from our official TS provider and save this. Server, Centos 7.</p>

<p>How do I proof however that this data is not manufactured? The connection is encrypted, so I can't get the employee's ISP do return any logs.  We also own the server so there is no 3rd party server provider to ensure the traffic.</p>

<p>Is there a way?</p>

<p>Thanks for your help.</p>
","<php><ssl>","2018-12-26 21:52:36"
"1020194","RTNETLINK answers: No such device /Wireless card /kali linux","<p>I've got an issue with alfa AWUS036NH. I cannot set it up. I get info : ""No such device"" or ""Input/output error"". I used command : ""sudo ip link set wlan0 up"" Kali version :5.5.0-kali1-amd64 release:2020.2 and yes I've made update before I'm stuck
<a href=""https://i.sstatic.net/VbClF.jpg"" rel=""nofollow noreferrer"">enter image description here</a></p>

<p><a href=""https://i.sstatic.net/lPux3.jpg"" rel=""nofollow noreferrer"">enter image description here</a></p>
","<linux><wifi><internet><kali-linux>","2020-06-05 13:10:54"
"878041","Port 457 on SMTP server?","<p>I hope this question is valid on here, if not I apologize.</p>

<p>I have a VPS server setup with PowerMTA and InterSpire, I just purchased the server and then I had an ""expert"" setup the rest. The VPS has all needs for a regular email server setup, like all ports open and rDNS. </p>

<p>Now my issue is the fact that whenever I try to create an email campaign and test it, by sending it to my own hotmail and gmail account. The email goes straight in the spam folder. 
I know my IPs are clean as I have tested them, I know the email content isn't spam as I have also tested that to be 100% clear. 
But then I read somewhere, that often times the issue is the SMTP settings. I am slightly familiar with the different SMTP ports and know the usual one is port 25. However the one setup on my server is port 457, which I have never heard of and can't find any information about when searching for it on Google. </p>

<p>The ""expert"" tells me that the setup is properly made, but I have doubt and want to make sure regarding this port number. </p>

<p>Underneath is the Hotmail email source code, which categorized as spam</p>

<pre><code>Received: from CY1NAM02HT172.eop-nam02.prod.protection.outlook.com
 (2603:10a6:3:1a::14) by HE1P190MB0348.EURP190.PROD.OUTLOOK.COM with HTTPS via
 HE1PR0501CA0004.EURPRD05.PROD.OUTLOOK.COM; Wed, 11 Oct 2017 22:33:29 +0000
Received: from CY1NAM02FT036.eop-nam02.prod.protection.outlook.com
 (10.152.74.52) by CY1NAM02HT172.eop-nam02.prod.protection.outlook.com
 (10.152.74.224) with Microsoft SMTP Server (version=TLS1_2,
 cipher=TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384_P384) id 15.20.77.10; Wed, 11
 Oct 2017 22:33:28 +0000
Authentication-Results: spf=none (sender IP is 000.00.000.00)
 smtp.mailfrom=domain.com; outlook.com; dkim=none (message not signed)
 header.d=none;outlook.com; dmarc=none action=none header.from=domain.com;
Received-SPF: None (protection.outlook.com: weagill.com does not designate
 permitted sender hosts)
Received: from api7.mydomain.com (000.00.000.00) by
 CY1NAM02FT036.mail.protection.outlook.com (10.152.75.124) with Microsoft SMTP
 Server (version=TLS1_2, cipher=TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384_P384) id
 15.20.77.10 via Frontend Transport; Wed, 11 Oct 2017 22:33:28 +0000
X-IncomingTopHeaderMarker: OriginalChecksum:BC992A824A633B40E95CDFBA9D9DCFAB98F7CF27563607D0409892D8DC8FEC87;UpperCasedChecksum:9494047B3F3928A0EF261AE4497490C3F0881861EC441F2C914B60D389C185CD;SizeAsReceived:366;Count:10
To: &lt;testreceiveemail@outlook.com&gt;
Subject: Test Email System
Message-ID: &lt;82cd27fb25d6274f943387cf1dfee31a@mydomain.com&gt;
Return-Path: contact@domain.com
Date: Wed, 11 Oct 2017 22:33:27 +0000
From: &lt;contact@domain.com&gt;
Reply-To: &lt;contact@domain.com&gt;
Content-Type: text/plain; format=flowed; charset=""UTF-8""
Content-Transfer-Encoding: 8bit
X-IncomingHeaderCount: 10
X-MS-Exchange-Organization-Network-Message-Id: e3f87ba3-ae12-4d3f-ddd9-08d510f81826
X-EOPAttributedMessage: 0
X-EOPTenantAttributedMessage: 84df9e7f-e9f6-40af-b435-aaaaaaaaaaaa:0
X-MS-Exchange-Organization-MessageDirectionality: Incoming
X-Microsoft-Exchange-Diagnostics: 1;CY1NAM02FT036;1:LKGiTZfrkJDht58eRPjjnwaf5dHCJUEyrSMNqR69Vdauemu+dWL+vPgeAjORxFa2qa/1TGfnnNtdxWWA5ONrGV/K7NRecNe8b+Dh7UaKid8+dXQB7QNBVSd6VTFB7NDr
X-Forefront-Antispam-Report: EFV:NLI;SFV:NSPM;SFS:(98901004);DIR:INB;SFP:;SCL:1;SRVR:CY1NAM02HT172;H:api7.mydomain.com;FPR:;SPF:None;LANG:;
X-MS-Exchange-Organization-AuthSource: CY1NAM02FT036.eop-nam02.prod.protection.outlook.com
X-MS-Exchange-Organization-AuthAs: Anonymous
X-MS-PublicTrafficType: Email
X-MS-Office365-Filtering-Correlation-Id: e3f87ba3-ae12-4d3f-ddd9-08d510f81826
X-Microsoft-Antispam: BCL:0;PCL:0;RULEID:(23075)(22001)(5000083)(610149)(8291501071);SRVR:CY1NAM02HT172;
X-Microsoft-Exchange-Diagnostics: 1;CY1NAM02HT172;3:CLZG/fxS7bDVhbUHKwp3kHn1e2ixx0PWUFs3DTJW1zj2kr64yV0RqWjhZYbPBdS6bXiXViX/FsPWFo9GAATcuJTYNIj/XoEfdcXb+BDq3/1lF2QG/TjCYhmHGLx4UAEsSAiHLpdK8V5sYyqOysKnvozlucw+Tsi8CuUor83WagGFNIGFSGtMGDGTGAn1DvO+IprUDfxXRIdUNGRsqKsRPBUd6zxFrYekFPSYhzA8UtdXYnEcW2BdBozPTA0zHedLK01SgYUWyd4Xp25M9Gnxh2J8hxNkyxEKlSozNfCmod+fG0kEy8PUGfah1HExOkbi;25:rF/1wkl5whjmn8xlNQF9w76N/NMcqrUkZLDWc3TdA4+5+Hxpy1VLsqKwy5VwNMGd3LIALPsa8k7iX3UT86r+E+M8Q86j/+0RObaWVCW3BuTHIi6sULxsaAd+War+kqYRdlIEwS+pUW0TToRtWokhQaEPpcSiSOJEkI13T7nTS6YIJy0iY+wC5dfpAxItEmbLqydOuZDgayuSKnYhN9WnzOR2fsm/UCrXSdtutaFKVkJm/Ya8ZkSZ+AHs/A/i97YzuVxFLkpbZiuPAd6dKX6WDVmx7o9xuLRyi6LZfFBw70kzDW629mFWhL+qIsx2dJwPshjC8Idr5qS1IdtVelo2Yfa+cyfksnjHgxvIW+4IJMs=;31:5O3/23y09YSW6QpBXl5Kvx84qfPoiPNm26h5CYRlBNPz50FMJuEHv8j6/4RDNstNlrqGoAGuPFYYpeHR2O9m3vKyVhSkc/nRThg7e/aWXDh5G5D0HT2+Vn0sp1zmcuuhh+u/3SUdOHtIv5lBciVaTi2mSQw7E6eJGLYvH0JCFetvYcRWPfz8NuuBTj0+MflUn74qUCYhbnnHwycmb4C36n0OzqzsP2kN8aoDU4e9DEg=
X-MS-TrafficTypeDiagnostic: CY1NAM02HT172:
X-MS-Exchange-EOPDirect: true
X-Sender-IP: 142.44.227.57
X-SID-PRA: CONTACT@DOMAIN.COM
X-SID-Result: NONE
X-MS-Exchange-Organization-PCL: 2
X-Exchange-Antispam-Report-Test: UriScan:;
X-Exchange-Antispam-Report-CFA-Test: BCL:0;PCL:0;RULEID:(100000700101)(100105000095)(100000701101)(100105300095)(100000702101)(100105100095)(444111536)(595095)(82015058);SRVR:CY1NAM02HT172;BCL:0;PCL:0;RULEID:(100000800101)(100110000095)(100000801101)(100110300095)(100000802101)(100110100095)(100000803101)(100110400095)(100000804101)(100110200095)(100000805101)(100110500095);SRVR:CY1NAM02HT172;
X-Microsoft-Exchange-Diagnostics: 1;CY1NAM02HT172;4:dfvZIOwvMFJh2qWv+VQZVz4U98IbpGr39NCQYp4ubHlY9nflcTZarNZ93htqo2bVIyl1KbeDHp+rlmnY/EqTrkdWbvPdRDTefRaA+u13aMPZO8R2sMbHurMvnZ+01or2fEwT6zYOCN6FjZTsG8O/fKvKq4gCjLUnf9SsTnPABgyX7GmfM65zsjuUWfC8HipXHZocWeONenWsK0ORZDADwt/3ddEfHocboohwTY6fZDAd24cIJZovUcc7WJjy2cF6;23:xv/SQRnXXehl3hk0Q5HOaNrxMuAlOirTiRDymf9yv49FKzIkvtWFXnQaOFWYBcXQagB9XFWJOFGJpHDSIx2uWpkSw74HZ/SWWkMHInw9nG1gf83YCsL5P+8cH0iVXNVXixquvwnGhDsozkBATUOLiHz5hWTMvtmFUdQeulN0vss=;6:Z+iDYmnQPjLRiCUAHIjjC6MuvRC/gyVcwImiwajSOxJoeao+lm7KNGWOu/VeMUFlpH0OUbNEhSKOVaMZ3G4DH9FhmKzjZQuSSpUjp6RuYVH9z3PoyYk4t3zUYlARr+VlWnMRqlW//moDSGN6HBHxYVP2kNVonq44jDNXDQE6yaHsd//J7G9bI0WcSgAK+EnkFQE1Kpd6bHE5n+g0g/k6pjs/MiyhqNGLAJveOFyP+CzTckCK+BxjwhK0GFBy084j8LK+zwzavE7L9YAE5DxbaSqfaxIOxASVtF9shIw9QGzyLYNCxUnkGmHAi5P6C9mEP6yHp/dK0YZ8gP2bvL+NDw==;5:ldY4Mpl6iB13bcjtmHK6U5MyicDA947IucWGTlWh9h2icw0lK8g4cp8jWApmRT0FbtjbpjZZZv8vqKsIMnmygxitmgfDI/GjJg8dMzvbMn3vpfYA9C+3KFZHAPhV8YenHEAbzNBX2Z/Lc0nY3J2Bvg==
X-MS-Exchange-Organization-SCL: 1
X-Microsoft-Exchange-Diagnostics: 1;CY1NAM02HT172;24:HmU8+tpGohU4IXfGqVSE9ohPlVij6e/FN5GwE39Vx2FQTPdU1WMQZzt/O+3ZjK4wbkUvwDX4Xio/OdsZKQgpg+wAxHMaixRyTb70Ge9Q30E=;7:P/N5sQLKjePfuJR4psoOu4BUpG5q8XbKOZafgLG97sQIAyd3iIpL/ceiMZOwxa6rxdLyPK/w/x6h2vbLeEMl5eMdG9Uq1nskndfYX+VeRuzCrjhT4vYbIpuz741YkIE41AWmjS9t7YSJo4EidC/VHmc2yRE7mdCp9hrjLwPfUFB8MQqqKca8z/ws874D+3ASMjEos4gJS/4n3o52Y+FnYe2czkxQiVQAjI+c+OXDC1c=
SpamDiagnosticOutput: 1:99
SpamDiagnosticMetadata: NSPM
X-OriginatorOrg: outlook.com
X-MS-Exchange-CrossTenant-OriginalArrivalTime: 11 Oct 2017 22:33:28.3896
 (UTC)
X-MS-Exchange-CrossTenant-Id: 84df9e7f-e9f6-40af-b435-aaaaaaaaaaaa
X-MS-Exchange-CrossTenant-FromEntityHeader: Internet
X-MS-Exchange-Transport-CrossTenantHeadersStamped: CY1NAM02HT172
X-MS-Exchange-Transport-EndToEndLatency: 00:00:01.4980204
X-MS-Exchange-Processed-By-BccFoldering: 15.20.0077.017
X-Microsoft-Exchange-Diagnostics:
    1;HE1P190MB0348;27:yI4sJS7YKaRmYZEwNVojBegJ+0k5rg7mWDgSw23nUMORqoPtsAansNp+QbXSYRxXxUBRECmNukHRNQiILCkLIYGdn7GVNOBiGSvGEBRTsy0fO8Vs2pVWAFhGS0/c69pE
X-Microsoft-Antispam-Mailbox-Delivery:
    abwl:0;wl:0;pcwl:0;kl:0;iwl:0;dwl:0;dkl:0;rwl:0;ex:0;psp:0;auth:0;dest:J;ENG:(400001000128)(400125000095)(5062000261)(5061607266)(5061608174)(4900095)(4920089)(6375004)(4950112)(4990090)(9140004);RF:JunkEmail;OFR:SpamFilterAuthJ;
X-Message-Info:
    qoGN4b5S4yqj4IaGpKlnGKT9fTpXsFuFEQAireWYeme97to4gxKZuA76zNUifWNm6Bdc1VV9v3+tfOHk29cexC4J7sXJ1NuPgP99kz0pZ+dtMR4K8HYzxwToR5Ucmee7alXHAvLCFaEOwani3e4v7FdA3atoe4KNwokGKgXOY6OhYurNCdeiQHH4D6H9zobl5RlL5eag9Dqd0w0Wzu5k+g==
X-Message-Delivery: Vj0xLjE7dXM9MDtsPTA7YT0wO0Q9MjtHRD0yO1NDTD02
X-Microsoft-Antispam-Message-Info:
    9bclqmd+TvztyTVRVuN1NGk6La0+XHh1I3WrbB7AGFwnxn7IqnPn4OSK7G8t53koBiFnhzGJefKc2fEFC+mlclqU0EnQ7Wcv3+BiIEz1x9ayem0LWrCwS3HJlM1aFOBP1zU4gfL6y8a8/glMc+1X3JQTQkGVdozwjukB4sCijF5J5eYyuvZq4eT5F7O4sH7UuCIBHvxmBjXvnp/jq8bt6zpjDt28WYWtLgmcFLxK7WbGejZiwik14qkS39tx4ulGhZfkZJ4i2JGYk7m5WAlH6A==
MIME-Version: 1.0

Hi,
This is a test of the emailing system. If you received this ok, then
everything is working as it should.
</code></pre>
","<smtp><email-server><vps><port>","2017-10-11 23:00:08"
"946815","FreeBSD top disk io read write units","<p>In FreeBSD you can run ""top -m io"" and it will show you disk io.</p>

<p>For example:</p>

<p><a href=""https://i.sstatic.net/bICiA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bICiA.png"" alt=""enter image description here""></a></p>

<p>What units is the WRITE/READ in, bytes or kilobytes or megabytes?</p>
","<freebsd><top>","2018-12-27 19:55:14"
"804353","aws cluster overprovisioning detection framework/tools","<p>Our team is managing many cassandra clusters on aws, one of our problems is when a user ask us to spawn a new cluster, they can't predict how many nodes they need because of lack of production traffic. Most of time it ends up overprovisioned. Since we are managing so many clusters for so many customers, over time, it's hard to examine all of them to decide overprovisioning by hand.</p>

<p>Is there an opensource project or framework tackling this problem in a scientific manner.</p>
","<amazon-web-services><cassandra><provisioning><opscenter>","2016-09-21 02:13:30"
"804409","Permission denied when executing perl via sudo","<p>Command:
<code>
ssh user@testhost ""sudo -u tester env PERL5LIB=/home/tester/perl5/lib/perl5 /home/tester/perl5/bin/testperl""
</code></p>

<p>Result: 
<code>
Can't locate testperl.pm:   Permission denied at /home/mogile/perl5/bin/testperl line 15.
</code></p>

<p>How can I fix the problem?</p>
","<ssh><sudo><perl><environment-variables>","2016-09-21 09:09:59"
"946926","Generate SSL certificate files from text file","<p>I have generated a free SSL from <code>zerossl.com</code> through their wizard, and finally I got a pure text file containing pure text with two <code>BEGIN</code> and <code>END</code>.</p>

<p>I want to know how to generate <code>.crt</code> and <code>key</code> from that file/text.</p>

<p>I have three text files:</p>

<p><code>domain-csr</code>, <code>account-key</code>, <code>domain-crt</code></p>

<p>Here's the guide from the site:</p>

<blockquote>
  <p>Your certificate is ready!
  Congratulations on receiving your Free SSL Certificate. Please note that certificates are valid for 90 days, but they are free to renew. To renew just repeat the process, using the same LE key and CSR as you used last time. Using the same CSR means that you do not need a new domain key (it will stay the same) and will only need to update the certificate file on your server. Keep your keys and CSR safe. Make sure you remember where you saved your generated account key!</p>
  
  <p>Your account ID is 48546566 - please write it down somewhere. You will
  not normally need this, but together with email it might help you to
  restore your account quicker if you lose your account key.</p>
  
  <p>Important:</p>
  
  <p>When installing the certificate, keep in mind that it should be used
  with the domain key, NOT the LE key from the ""Details"" screen. The LE
  key should be only used on the ""Details"" screen when issuing or
  renewing your certificate. On renewal that will allow you to skip the
  verification of already verified domains. Your certificate file
  contains both your domain certificate and the issuer's certificate.
  Most modern web servers will accept them as is. However, old versions
  of Apache, Amazon Web Services (AWS) and some control panels would
  want them separately. In that case just split the certificate file in
  two, preserving BEGIN and END lines around both certificates. The
  first one is your domain certificate, the second one is the issuer's,
  which in control panel interface may be called ""Intermediate
  certificate"", ""Certificate chain"" or ""CA Bundle"".</p>
</blockquote>
","<ssl><ssl-certificate><openssl>","2018-12-28 18:24:15"
"1020381","Squid proxy slow via PS4?","<p>This is my config: <a href=""https://paste.ubuntu.com/p/sRQCkFy4dy/"" rel=""nofollow noreferrer"">https://paste.ubuntu.com/p/sRQCkFy4dy/</a></p>

<p>The system running Squid Proxy is a native Ubuntu 20.04 on Ryzen 3200G/8gb/120GBSSD.</p>

<p>When I use it to connect my PS4 to the internet, downloads are extremely slow. I am not sure how to diagnose this, can you help me out?</p>
","<squid>","2020-06-07 13:34:22"
"946943","SMB File Share Extremely Slow","<p>I have a freshly installed and updated version of Windows Server 2016, I have 4x 4TB disks, connected through SATA. These disks are setup in a parity configuration under a StoragePool, in that I have made a virtual disk of ~10TB.</p>

<p>Write operations to the disk go around 10MB/s after the RAM buffer has filled, an ideas why?</p>

<p>Thanks</p>

<p>Tested:</p>

<ul>
<li>Iperf3 - Network connection is working correctly.</li>
<li>CPU Load - 10% or lower.</li>
<li>Hardware Failure - Disk drives are functional and controller works fine on individual disks.</li>
</ul>

<p><strong>Images</strong></p>

<p>The Storage Pool's Write Pattern
<a href=""https://i.sstatic.net/yzoYq.png"" rel=""nofollow noreferrer"">Link</a></p>

<p>iPerf3 Results
<a href=""https://i.sstatic.net/A4jvD.png"" rel=""nofollow noreferrer"">Link</a></p>

<p>Write Pattern During <strong>LOCAL</strong> C: to O: Transfer (No Networking):
<a href=""https://i.sstatic.net/M5oRd.png"" rel=""nofollow noreferrer"">Link</a></p>

<p><strong>Requested Info:</strong></p>

<ul>
<li>Disk RPM/Size: 7200RPM (LFF)</li>
<li>Disk Models: WD40EFRX (Western Digital: Red ""NAS"" Drives)</li>
<li>Storage Pool Setup: <a href=""https://i.sstatic.net/lUY2m.png"" rel=""nofollow noreferrer"">See Here</a></li>
<li>What kind of server? Custom-Chassis PC Case with a ""Asus P8Z77-V LE Plus"" Motherboard.</li>
<li>How are disks connected to it? 12 Disks, 4 of which are relevant to this file share.</li>
</ul>

<p><strong>Additional Notes</strong>
 - The system was performing in the range of 200MB/s previosuly with FreeNAS 11.4.</p>
","<windows><network-share><windows-server-2016>","2018-12-28 21:26:26"
"804546","Missing File: /etc/init.d/mysql or /etc/init.d/mysqld does not exist","<p>We are running mySQL on a Fedora Linux fc19.</p>

<p>I have a few corrupt event table errors like in this ticket:
<a href=""https://serverfault.com/questions/562282/mysqldump-error-1557-corrupt-event-table/562303#562303"">mysqldump error #1557: corrupt event table?</a></p>

<p>However, when I to to run step two, ""/etc/init.d/mysql restart"", restarting mysql bash returns the following:
""/etc/init.d/mysql: No such file or directory""</p>

<p>phpMyAdmin and the websites appear to be running normally, which makes me believe that the ""/etc/init.d/mysql"" file is somewhere on the box. Is there some sort of search I can do to locate it? Would the pages still operate if it is missing?</p>
","<mysql><web-server><mariadb>","2016-09-21 18:58:41"
"947060","DNS Spoofing in Bind9 DNS Server","<p>I configured Ubuntu Server 18.04 as a master DNS server.
zone ==> google.com</p>

<pre><code>;
; BIND data file for local loopback interface
;
$TTL    12h 
@   IN  SOA ns1.google.com. root.google.com. (
                  2     ; Serial
             604800     ; Refresh
              86400     ; Retry
            2419200     ; Expire
             604800 )   ; Negative Cache TTL
;
@     IN    NS  ns1.google.com.
ns1   IN    A   193.168.10.1
www   IN    A   &lt;fake IP&gt;
mail    IN  A   &lt;fake IP&gt;  
</code></pre>

<p>I configured client to use this server as a DNS server but when I enter <code>mail.google.com</code> in client's browser I get an SSL ERR. How does browser understand this issue? client's browser is up to date.</p>
","<ubuntu><domain-name-system><bind><dns-server><spoofing>","2018-12-30 14:11:42"
"804600","backup website data(image file, database) no downtime","<p>I have a website using nodejs, postgresql, nginx and runnning on linode vps, I know linode have service backup,<br>
but I want to know if I don't use that,<br>
Q1: how to do manual backup and without downtime?   </p>

<p>backup data:   </p>

<pre><code>image files from user create in application  
postgresql database data from use create in application
</code></pre>

<p>Q2: I'm newbie, wondering usually people how to do such thing?<br>
Q3: big website(image host) like instagram, imgur  how to do backup?</p>

<p>I google a long time can' find answer, any suggestion will be appreciate!!</p>

<p>UPDATE: 
I know I can dump database, and things like scp download file but I'm not sure is it ok during the application running?  </p>
","<nginx><backup><vps><postgresql>","2016-09-22 02:30:13"
"804763","How many public IP can a router have?","<p><img src=""https://i.sstatic.net/I91sI.gif"" alt=""enter image description here""></p>

<p>I have one router which connects to two different networks as shown in the diagram . I have few silly doubts. Please help me.</p>

<p>For every NIC the router will have different IP. Lets assume we have two private networks   ( Here the image shows network id's which are actually not private but lets assume they are private ) connected to the same router. So the router will have two ip for each network as the network id will be different. </p>

<p>Then my question is which one is public ip (which is visible over internet) in this case? Or there is another network(our ISP) to which the router  should be connected and that interface will have the public IP for both the private network.I think each router will have at max one public ip even if it is connected to many private networks. I am confused about number of public IP in this case. </p>

<p>But we can see in this diagram the router does have two public IP for two different networks or they are not actually public IP? I am just claiming it to be public IP as these IP address can not be used for private IP allocation.</p>

<p>If a router is connected to another router then the interface connecting to both will have what public IP address? Previously it was the same network ID that router uses but in this case it is connected to another router. </p>

<p>I understand there may be lots of wrong things I would have said in those above lines but badly need help to get into it.</p>
","<networking><ip><router>","2016-09-22 17:18:14"
"804868","Deny access to apache virtual host to the outside world","<p>My website isn't live yet and I want to be the only one who can access it. Up to this point, I just denied from all and allowed my ip. The problem is I'm not at home right now and I don't have an internet connection other than mobile. My guess is that my current ip from the mobile network is shared among many other people, so white listing this ip is something I'd not like to risk. Perhaps I could set a custom user agent that would act like a password? But then how would I configure apache to allow only from specific UA? Are there any other options?</p>

<p>Edit: Apache authentication works, but I have problem with making cross origin ajax requests. My web application requires to make ajax requests to my server. The application is distributed, not hosted so I allow all origins to my endpoint. Apache authentication somehow breaks it for me, ""no access-control-allow-origin"" header is present... origin localhost is therefore not allowed access.</p>

<p>Edit2: It doesn't work even with PayPal! Are there any better solutions?</p>

<p>Edit:3 I found a better solution. It works with my web app and visitors won't see a login form. Simply set a custom header to be sent with each request. I used Requestly Chrome extension for that (also allows to specify which urls should have custom headers sent). Then in vhost file:</p>

<pre><code>SetEnvIf X-MY-TOKEN ""secretmessage"" AllowIp
Require all denied
Require ip some.ip
Require host paypal.com
Require env AllowIp
</code></pre>

<p>However PayPal IPN still won't work. I have also tried wrapping it in  . I guess I have to whitelist my ip for a moment and see if it works.</p>

<p>Edit4: PayPal works now, they have some technical issues at sandbox.paypal.com</p>
","<linux><networking><php><web-server><apache-2.4>","2016-09-23 07:37:02"
"947369","Multiple websites on same server","<p>When running multiple websites on same server:</p>

<pre><code>root/website1
root/website2
root/website3
</code></pre>

<p>What should be done to make these websites run intependently - I mean to completely restrict access of PHP (or any other scripting language) to the root folder and each other's folders?</p>

<p>I am thinking of Apache Virtual Host, but will it restrict file access between each vhost as required?</p>

<pre><code>&lt;VirtualHost *:80&gt;
    DocumentRoot ""/root/website1/""
    ServerName Website1
&lt;/VirtualHost&gt;

&lt;VirtualHost *:80&gt;
    DocumentRoot ""/root/website2/""
    ServerName Website2
&lt;/VirtualHost&gt;

&lt;VirtualHost *:80&gt;
    DocumentRoot ""/root/website3/""
    ServerName Website3
&lt;/VirtualHost&gt;
</code></pre>
","<debian><.htaccess><httpd.conf><php.ini>","2019-01-02 18:15:03"
"805112","Backup strategy on EC2-VPC-AWS","<p>I am new to AWS, we have few AWS EC2 instances. I was wondering what is the best and most cost effective way to take backups. I read the recommended article about EBS snapshots, but it seemed to be very expensive if the server counts go high. </p>

<p>I am wondering what is the standard practice used by others. </p>
","<amazon-web-services><amazon-ebs>","2016-09-24 16:04:05"
"947683","Must I setup SSL certificate if my server is listening on only port 443 with Nginx?","<p>I am using Amazon Load Balancer to receive requests on Port 443. Then I am sending off these request to my EC2 server on port 80. If I try to send the request on port 443 to EC2 nginx <code>error.log</code> file says, </p>

<blockquote>
  <p>no ""...ssl_certificate"" is defined in server listening on SSL port while SSL handshaking...</p>
</blockquote>

<p>Therefore, Must I setup SSL certificate if my server is listening on only port 443 with Nginx?</p>
","<nginx><ssl><ssl-certificate><port-443>","2019-01-05 03:04:37"
"947731","My AWS EC2 VM is not receiving HTTP from an external browser to the elastic IP of my EC2 instance","<p><strong>Problem</strong></p>

<p>Can't browse my web app on my new AWS EC2 VM.
Wireshark on VM, shows a little HTTP activity, but nothing when I browse my URL or its elastic IP from an external browser (at my home office).</p>

<p><strong>My Setup</strong></p>

<p>My ASP.NET Web app is running on my AWS EC2 virtual server with Windows Server 2016 64-bit.
I configured IIS.
I set up an Elastic IP, which shows up in my AWS console instance.
I did nothing special with firewall</p>

<p><strong>Background</strong></p>

<p>My web app has worked ok on AZURE for years, and now I want to move it to AWS.  </p>
","<networking><amazon-web-services><amazon-ec2><http><amazon-elastic-ip>","2019-01-05 18:16:33"
"879034","I'm not sure which files I should generate from the bundle certificate, anyone knows?","<p>I've bought a new wildcard certificate and got a zip file which contains the follow files:</p>

<pre><code>star.domain.com.crt
star.domain.com.ca-bundle
star.domain.com.p7b
</code></pre>

<p>The service provider which requires the certificates wants that I'll send him the new certificate in the following formats:</p>

<pre><code>Private key
Public key (CSR)
Intermediate certificate
</code></pre>

<p>I'm not sure how to generate these files from the ones I got from the SSL provider.</p>

<p>I've ran some openssl commands to try and generate the required files but I'm not sure that the generated files qualify for the service where I need to send the certificates.</p>

<p>Edit:
I've ran the following command to create the .pem file:</p>

<pre><code>openssl pkcs7 -in star.domain.com.p7b -text -out star.domain.com.pem -print_certs
</code></pre>

<p>Is that the right way to create the pem?</p>

<p>Anyone care to assist me with this please?</p>
","<ssl-certificate><pki>","2017-10-18 09:47:32"
"947839","Windows Server multiple ip issue","<p>One windows server 2016 running on some ip address 1.2.3.0. It has Apache server on port 80, DNS Server and running MySql server and so on and connecting through RDP client.</p>

<p>My requirement is to add an additional IP address 1.2.3.1 on the same windows server and separate the ip address for my application servers for apache, mysql due to security reasons and not expose main server ip.</p>

<p>The main ip address 1.2.3.0 should only connect to RDP and server services and the other ip should only connect to Apache and Mysql ports and not connect RDP.</p>

<p>I was able to add the additional ip address to the server and add a firewall inbound rule to block all the ports for 1.2.3.1 ip address. It worked. Now I opened apache port 80 on this 1.2.3.1 ip address, it does not work. What might be the issue ?</p>
","<windows-server-2016><windows-firewall>","2019-01-07 00:49:32"
"879114","How can I ping, tracepath behind the proxy without using vpn?","<p>I am using a network that requires HTTP Proxy.Internet is working fine.</p>

<pre><code>$ wget google.com
--2017-10-18 21:52:48--  http://google.com/
Connecting to xxx.xx.xx.xx:8080... connected.
Proxy request sent, awaiting response... 302 Moved Temporarily
Location: http://www.google.co.in/?gfe_rd=cr&amp;dcr=0&amp;ei=tX_nWcnwLq6A8Qenx4zICw [following]
--2017-10-18 21:52:48--  http://www.google.co.in/?gfe_rd=cr&amp;dcr=0&amp;ei=tX_nWcnwLq6A8Qenx4zICw
Reusing existing connection to xxx.xx.xx.xx:8080.
Proxy request sent, awaiting response... 200 OK
Length: unspecified [text/html]
Saving to: ‘index.html’

index.html                          [ &lt;=&gt;                                                 ]  14.20K  --.-KB/s    in 0.06s   

2017-10-18 21:52:48 (241 KB/s) - ‘index.html’ saved [14537]
</code></pre>

<p>But I am not able to ping.</p>

<pre><code>$ ping google.com
PING google.com (216.58.197.46) 56(84) bytes of data.
^C
--- google.com ping statistics ---
14 packets transmitted, 0 received, 100% packet loss, time 13315ms
</code></pre>

<p>Tracepath </p>

<pre><code>$ tracepath google.com
 1?: [LOCALHOST]                                         pmtu 1500
 1:  192.168.xx.x                                         0.621ms 
 1:  192.168.xx.x                                          0.593ms 
 2:  no reply
 3:  no reply
 4:  no reply
 5:  no reply
 .
 .
</code></pre>

<p>If I use VPN then everything works fine.</p>
","<proxy><ping>","2017-10-18 16:26:37"
"805437","Getting data off of HP Proliant DL380 G5 RAID 5 drives","<p>I have an old DL380 G5 with a dead VGA.  I'd like to pull the data from the 3 drives (I assume configured as RAID 5 given the # of drives – it was before my time).  Is there any way to get at that data or is replacing the VGA board the best option?</p>
","<raid><hp-proliant>","2016-09-26 18:46:41"
"805499","How do I prevent all script execution in a web-accessible image uploads directory?","<p>I am using nginx and PHP, and want to allow images to be uploaded by my users, and placed into a public web accessible directory.</p>

<p>I currently have this rule defined:</p>

<pre><code>location ~ /uploads {
    location ~ \.(jpg|gif|png)$ {}
    deny all;
}
</code></pre>

<p>If it's not a .jpg, .gif, or .png, then deny access to it.</p>

<p>That directory (<code>/var/www/public/uploads</code>) has permissions set to <code>744</code>.</p>

<p>However, I don't know enough about various vulnerabilities with uploaded images to try my own exploits.</p>

<p>Ideally I'd like to make sure that anything ending in jpg, gif, or png is treated like the respective mime-type, so that no script interpreter will even attempt to execute it.</p>

<p>How do I accomplish this?</p>
","<nginx><security><images>","2016-09-27 01:24:41"
"805525","Is it possible to have multiple storage servers in a failover cluster?","<p>I'm familiar with setting up RAID arrays and am running a few in my home environment.</p>

<p>I was wondering if it's possible to have multiple storage servers in a failover configuration.</p>

<p>What I hope to achieve with this is to have a certain redundancy with these servers. When one server breaks down, it can be replaced entirely without loss of data. I don't mean just a disk failure, but something more serious. Maybe a BIOS corruption. I've had those before... It wasn't pretty.</p>

<p>What would be the best way to achieve this and how would I have to set it up?</p>
","<raid><software-raid><failover><hardware-raid><redundancy>","2016-09-27 07:37:50"
"805573","Remote Desktop to 80% of my servers do no longer work (""User account restriction"") from just one of my PCs","<p>I came into work last week, checked my first ticket (easy to fix one), RDP'd into the server needed for this and the login did not work. After clicking 'connect' I got the ""Unable to Log You on Because of an Account Restriction"" message. Checked another server (all machines are 2008R2/2012R2), the same message. No, I do <em>not</em> habe an empty password, not using network auth, my clint is Windows 10 (1607).</p>

<p>Here is what I did:</p>

<ul>
<li>Used another client (Win10.1607), same ou, same setup. Can perfectly login from anywhere to anywhere (so I am asuming it's no my user account or a GPO)</li>
<li>Checked servers: I can RDP into all my DC's and a few other machines (2008R2/2012R2), looks random to me (all server in the same OU, no special software installed)</li>
<li>Deleted the mstsc cache (%appdata%..\local\Microsoft\Terminal Server Client* )</li>
<li>Cleaned up HCU\SOFTWARE\Microsoft\Terminal Server Client</li>
<li>Watched the eventlogs: nothing. Absolutely nothing. So I assume it's my client, not the servers. But I can RDP into all my servers at home and in another (customers) network ...</li>
<li>Checked date/time on client/server (0.0002ms apart)</li>
<li>Checked account restrictions on ma account (neither time nor machine restrictions are present)</li>
<li>Checked if logon at the console works (vm/ilo): works perfectly fine with my credentials</li>
<li>Checked if Share-Access would work (\\server\share): Does <strong>not</strong> work, I am seeing the same error message. Works from clientB, but not from alientA.</li>
<li>When doing the same thing from one of the 'working' machines (sever or client), everything is fine.</li>
</ul>

<p>Any Ideas where to look for this? It is haunting me into my sleep :-(</p>

<p><strong>Updates: Surely I checked the local policies on the server(s). any changes would have surprised me - there are a <em>lot</em> of servers. Also checked the clients GPO, nothing.</strong></p>
","<windows><active-directory><remote-desktop><rdp>","2016-09-27 11:39:37"
"805596","Job for httpd.service failed","<p>I know similar question exists: <a href=""https://serverfault.com/questions/686825/job-for-httpd-service-failed"">Job for httpd.service failed</a></p>

<p>But it is not same as mine. I tried to restart apache using </p>

<pre><code>sudo apachectl restart
</code></pre>

<p>I get the following error:</p>

<blockquote>
  <p>Job for httpd.service failed. See 'systemctl status httpd.service' and 'journalctl -xn' for details.</p>
</blockquote>

<p>Here are the logs from <code>systemctl status httpd.service</code>:</p>

<p><img src=""https://i.sstatic.net/y3ULA.png"" alt=""systemctl status httpd.service output""></p>

<pre><code>[munna@localhost ~]$ systemctl status -l httpd
● httpd.service - The Apache HTTP Server
   Loaded: loaded (/usr/lib/systemd/system/httpd.service; enabled; vendor preset: disabled)
   Active: failed (Result: exit-code) since Tue 2016-09-27 19:08:26 BDT; 12min ago
     Docs: man:httpd(8)
           man:apachectl(8)
  Process: 6262 ExecStop=/bin/kill -WINCH ${MAINPID} (code=exited, status=1/FAILURE)
  Process: 6258 ExecStart=/usr/sbin/httpd $OPTIONS -DFOREGROUND (code=exited, status=1/FAILURE)
 Main PID: 6258 (code=exited, status=1/FAILURE)
[munna@localhost ~]$ su
Password:
[root@localhost munna]# systemctl restart httpd
Job for httpd.service failed because the control process exited with error code. See ""systemctl status httpd.service"" and ""journalctl -xe"" for details.
[root@localhost munna]# systemctl status -l  httpd.service
● httpd.service - The Apache HTTP Server
   Loaded: loaded (/usr/lib/systemd/system/httpd.service; enabled; vendor preset: disabled)
   Active: failed (Result: exit-code) since Tue 2016-09-27 19:21:41 BDT; 10s ago
     Docs: man:httpd(8)
           man:apachectl(8)
  Process: 7056 ExecStop=/bin/kill -WINCH ${MAINPID} (code=exited, status=1/FAILURE)
  Process: 7052 ExecStart=/usr/sbin/httpd $OPTIONS -DFOREGROUND (code=exited, status=1/FAILURE)
 Main PID: 7052 (code=exited, status=1/FAILURE)

Sep 27 19:21:41 localhost.localdomain systemd[1]: Starting The Apache HTTP Server...
Sep 27 19:21:41 localhost.localdomain httpd[7052]: AH00526: Syntax error on line 18 of /etc/httpd/conf.d/phpMyAdmin.conf:
Sep 27 19:21:41 localhost.localdomain httpd[7052]: allow not allowed here
Sep 27 19:21:41 localhost.localdomain systemd[1]: httpd.service: main process exited, code=exited, status=1/FAILURE
Sep 27 19:21:41 localhost.localdomain kill[7056]: kill: cannot find process """"
Sep 27 19:21:41 localhost.localdomain systemd[1]: httpd.service: control process exited, code=exited status=1
Sep 27 19:21:41 localhost.localdomain systemd[1]: Failed to start The Apache HTTP Server.
Sep 27 19:21:41 localhost.localdomain systemd[1]: Unit httpd.service entered failed state.
Sep 27 19:21:41 localhost.localdomain systemd[1]: httpd.service failed.
[root@localhost munna]#
</code></pre>

<p>Here is the output of <code>journalctl -xn</code>:</p>

<p><img src=""https://i.sstatic.net/ndw5I.png"" alt=""enter image description here""></p>

<pre><code> [root@localhost munna]# journalctl -xn
-- Logs begin at Tue 2016-09-27 18:43:34 BDT, end at Tue 2016-09-27 19:00:01 BDT
Sep 27 19:00:01 localhost.localdomain systemd[1]: Created slice user-0.slice.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Sep 27 19:00:01 localhost.localdomain systemd[1]: Starting user-0.slice.
-- Subject: Unit user-0.slice has begun start-up
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has begun starting up.
Sep 27 19:00:01 localhost.localdomain systemd[1]: Started Session 4 of user root
-- Subject: Unit session-4.scope has finished start-up
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-4.scope has finished starting up.
-- 
-- The start-up result is done.
Sep 27 19:00:01 localhost.localdomain systemd[1]: Starting Session 4 of user roo
-- Subject: Unit session-4.scope has begun start-up
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-4.scope has begun starting up.
Sep 27 19:00:01 localhost.localdomain systemd[1]: Started Session 5 of user root
-- Subject: Unit session-5.scope has finished start-up
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-5.scope has finished starting up.
-- 
-- The start-up result is done.
Sep 27 19:00:01 localhost.localdomain systemd[1]: Starting Session 5 of user roo
-- Subject: Unit session-5.scope has begun start-up
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-5.scope has begun starting up.
Sep 27 19:00:01 localhost.localdomain CROND[5612]: (root) CMD (/usr/lib64/sa/sa1
Sep 27 19:00:01 localhost.localdomain CROND[5613]: (root) CMD (/etc/webmin/bandw
Sep 27 19:00:01 localhost.localdomain systemd[1]: Removed slice user-0.slice.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Sep 27 19:00:01 localhost.localdomain systemd[1]: Stopping user-0.slice.
-- Subject: Unit user-0.slice has begun shutting down
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has begun shutting down.
lines 35-57/57 (END)
-- Unit session-5.scope has finished starting up.
-- 
-- The start-up result is done.
Sep 27 19:00:01 localhost.localdomain systemd[1]: Starting Session 5 of user root.
-- Subject: Unit session-5.scope has begun start-up
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-5.scope has begun starting up.
Sep 27 19:00:01 localhost.localdomain CROND[5612]: (root) CMD (/usr/lib64/sa/sa1 1 1)
Sep 27 19:00:01 localhost.localdomain CROND[5613]: (root) CMD (/etc/webmin/bandwidth/rotate.pl)
Sep 27 19:00:01 localhost.localdomain systemd[1]: Removed slice user-0.slice.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Sep 27 19:00:01 localhost.localdomain systemd[1]: Stopping user-0.slice.
-- Subject: Unit user-0.slice has begun shutting down
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has begun shutting down.
~
~
~
~
~
~
~
~
~
~
~
lines 35-57/57 (END)
</code></pre>
","<apache-2.2><centos7>","2016-09-27 12:50:11"
"805697","DNS resource data types differantiation","<p>In RFC1035, it says that in a DNS response, if the resource data is a pointer then the first two bits of those resource data should be 11. If it is a domain name, they should be 00. Two questions:</p>

<ol>
<li>What happens in the case the resource data is an IP address, eg 201.1.2.3 where 0d201 = 0b11001001 ? Are there other fields taken into account as well?</li>
<li>Is this distinction between 11 and 00 for the first two bytes really needed? If the resource data length field is taken into account, then wouldn't a length of 2 uniquely identify a pointer in the resource? A domain name cannot be 2 bytes long in the notation used in DNS messages.</li>
</ol>
","<domain-name-system>","2016-09-27 19:53:31"
"805710","Tell me which ports have been used during a period of time","<p>I've been struggling with a server provider while trying to mount a few services for my company, this because they have an strict Firewall policy which denies all non-allowed IN/OUT internet access, this would be nothing (in fact could be great) if I could control the firewall behavior, but all passes through me and 2 more people, so has been 
fatiguing. I've decided to monitor another host with all ports opened and check which ports are being used during a period of time.</p>

<p>I did try with a continuous netstat, but it didn't work as expected, since short-lived connections weren't listed. </p>

<p>So, how can I know which ports have registered activity during a given period of time? (even one bit of information). I need the following data: port number, traffic direction, origin and destination IP and ideally which program triggered it.</p>

<p>The idea is to catch this information quickly and precisely, not reading long logs with the risk of human error, if possible.</p>

<p>I'm using GNU/linux on both servers.</p>
","<linux><networking><firewall><port><packet-analyzer>","2016-09-27 20:56:47"
"805890","Cron script on reboot not working unless I sleep for a few seconds","<p>First of all I'm not sure whether I should post it here or under the Ubuntu-specific forum but I thought this is probably a more generic question.</p>

<p>I have a Raspberry Pi 3 with Raspbian Jessie, I wrote a script to send a push to my Pushbullet account to tell me everytime it starts.</p>

<p>First I made a bash script, tried to get it to work for several hours. It worked if I executed the script manually, but nothing happened on reboot. So I wrote the same script but in Ruby instead, which gave me the same result; it worked on manual execution but not with cron on reboot.</p>

<p>I got it to work by adding a sleep command for 10 seconds before executing the script. However, I would like to know why it is not working without sleep. Could it be that crontab executes my script before my RaspberryPi 3 has acquired internet access?</p>

<p>This is my script in BASH</p>

<pre><code>#!/bin/bash
current_time=$(date ""+%Y-%m-%d %H:%M:%S"")
STATUS=$(curl --header 'Access-Token: '&lt;ACCESS TOKEN&gt;' --header 'Content-Type: application/json' --data-binary '{""body"":""RaspberryPi 3 started at '""$current_time""'"",""title"":""RaspberryPi 3 started"",""type"":""note""}' --request POST https://api.pushbullet.com/v2/pushes)
echo $STATUS &gt;&gt; log.log
exit 0
</code></pre>

<p>The same script in Ruby with Washbullet gem</p>

<pre><code>require 'washbullet'

client = Washbullet::Client.new('&lt;ACCESS TOKEN&gt;')
timenow = Time.now.strftime(""%Y/%m/%d %H:%M"")
client.push_note(
    receiver:    :device,
    params: {
        title: ""RPi3 started"",
        body: ""RaspberryPi 3 started at #{timenow}""
    }
)
</code></pre>

<p>Edited cron with crontab -e looks like this:</p>

<pre><code>@reboot /home/pi/bin/pushbullet.sh
</code></pre>

<p>or</p>

<pre><code>@reboot ruby /home/pi/bin/pushbullet.rb
</code></pre>

<p>These cron only works if i prepend with ""sleep 10;"" i.e:</p>

<pre><code>@reboot sleep 10; /home/pi/bin/pushbullet.sh
</code></pre>

<p>Thankful for any help,</p>

<p>Mattias</p>
","<ubuntu><cron><raspbian>","2016-09-28 14:49:12"
"805901","How to determine what Microsoft kb is at fault (no https connection to legacy devices) without uninstalling them all?","<p>I just inherited a 2012 r2 server that is having issues accessing legacy https devices. I know it is due to kb articles due to previous experiences. </p>

<p>I don't want to do regression testing, is there a better way? I know it is not possible, but I was hoping it would show all the kb patches that are impacting an application by creating a list of of registry paths accessed by the application and cross referencing it with kb patches. </p>

<p>Ugh. </p>

<p>On the off chance someone would know... Ssl3.dhe 128 is what the issue is. I can access them from Firefox by enabling the old protocols, but ie doesn't work... And i know it is kb related, but can't find the darn article. </p>

<p>Thanks! </p>
","<ssl><windows-server-2012-r2>","2016-09-28 15:18:18"
"805914","How to get telnet working in SCO openserver release 3.2v5.0.6?","<p>I have a sco box (SCO openserver release 3.2v5.0.6) and I went and changed the ip address of the server via netconfig.</p>

<p>Sadly, telnet is no longer working when I attempt it from another computer to the sco box. Locally it telnets fine from the sco box. Running a ping to itself I get 0% packet loss. But, when I attempt to ping another computer on the network. I get:</p>

<blockquote>
  <p>ping: sendto: Host is down</p>
</blockquote>

<p>Would anyone know how I could overcome this?</p>

<p>I know telnet has big security issues but it is needed, and I would appreciate it thoroughly if anyone could me get the sco box operating on the network.</p>

<p>Update: attempt to diagnose:</p>

<p>Ping to self on 127.0.0.1 seems to be working fine</p>

<p>Ping to my laptop on the same network fails</p>

<p>No packets received.</p>

<p>Result of ifconfig -a</p>

<p><a href=""https://i.sstatic.net/fcVDg.jpg"" rel=""nofollow noreferrer"">ip config -a result</a></p>

<p>Result of traceroute to router ip</p>

<pre><code>1 * * *
2 * * *
3 * sendmsg: Host is down
</code></pre>

<p>Result of netstat -rn</p>

<p><a href=""https://i.sstatic.net/jn37L.jpg"" rel=""nofollow noreferrer"">netstat -rn</a></p>

<p>Background:</p>

<p>Whats caused this is that an extra router has been added, in that the bt hub has been made to bridge to an apple hub which is using a pppoe connection to the internet. The sco box instead of being directly plugged into the apple hub, it is plugged into another netgear router which is then plugged into the bt hub. Maybe it is taking the sco box as being on a different network. </p>
","<networking><unix><telnet>","2016-09-28 16:11:52"
"948593","kpcli bash script to automate entries","<p>I am trying to create a bash script which uses kpcli in order to automate entries into a kdbx file. While searching over here I found out that you 
could use expect and send, however this does not seem to be working for me.</p>

<pre><code>set timeout 10
spawn kpcli
match_max 100000000
expect  ""kpcli:/&gt;""
send    ""open global.kdbx\n""
expect  ""Please provide the master password:""
send    ""mypassword""
expect  ""kpcli:/&gt;""
send    ""cd Websites/""

while IFS="" "" read -r domainname username password

do

expect  ""kpcli:/Websites&gt;""
send    ""new ""$domainname""""
expect  ""Username:""
send    """"$username""""
expect  ""Password:""
send    """"$password""""
expect  ""Retype to verify: ""
send    ""$password""
expect  ""URL:""
send    """"$domainname""""
expect  ""Tags:"" 
send    ""\n""
expect  ""Strings: (a)dd/(e)dit/(d)elete/(c)ancel/(F)inish?""
send     ""F""
send     ""\n""
expect ""Database was modified. Do you want to save it now? [y/N]: ""
send   ""y""
send   ""y""

done &lt; sites.txt
</code></pre>

<p>Is this the way to do it or is there a better way?</p>
","<bash><expect>","2019-01-11 08:32:25"
"806297","Outlook Anywhere settings changed","<p>I have accidentally changed the Outlook Anywhere settings in Exchange 2013 and didnt make a note! any ideas as to what the original settings should be ? we access outlook web access within the business via <code>https://owa.example.co.uk/owa</code> </p>

<p>All our email addresses are   via <code>firstname.lastname@example.co.uk</code></p>

<p>Any help would be greatly appreciated as I prepare for our cutover migration to Exchange 365.</p>
","<exchange><outlook><microsoft-office-365><outlook-anywhere>","2016-09-30 10:43:24"
"806503","VPS not accepting http connections, but server is running","<p>I'm a developer, not a network administrator, so this is not exactly my cup of tea. Help is appreciated.</p>

<p>From one moment to the next, my VPS, running CentOS Linux 7.2.1511, stopped accepting http connections. A simple telnet to one of my hosted domains returns a ""Connection refused"". while FTP still works and I can also still SSH to the server. I have a total of 17 domains running on this server.</p>

<p>Apache is running (or claims to be running according to Plesk 12.5.30). </p>

<p>When, on the server's command line, I run </p>

<pre><code>/sbin/iptables -L -n
</code></pre>

<p>the result is </p>

<pre><code>Chain INPUT (policy ACCEPT)
target     prot opt source               destination         

Chain FORWARD (policy ACCEPT)
target     prot opt source               destination         

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination  
</code></pre>

<p>When I to a trace route to one of my hosted domains, the fourteenth hop arrives on my server's IP address, but, after that, only asterisks show up, like so:</p>

<pre><code>15  * * *
16  * * *
17  * * *
18  * * *
19  * * *
20  * * *
</code></pre>

<p>With my hosting provider's support offline from Friday night to, incredibly, Monday morning, I'm very much at a loss as to how to resolve this. Some pointers, suggestions, help, would be very much appreciated.</p>

<p><strong>Update</strong>: If I understand <a href=""https://serverfault.com/a/725263/237845"">this answer</a> correctly, using the following command should tell me whether anything is listening on port 80, which is the default port for http connections:</p>

<pre><code> ss -tnlp | grep :80
</code></pre>

<p>This command returns nothing, which I understand to mean that nothing is listening on port 80.</p>

<p>So, does the question then become, how to get Apache to listen to port 80 again?</p>

<p><strong>Update</strong>: I updated httpd.conf and added 'Listen 80', which wasn't there. This worked, somewhat, in that, now, the server was accepting http requests again. However, it was still not showing any websites, but a placeholder page instead.
httpd.conf did already have a 'Listen 7080', and, lo, visiting 'http://{{oneofmywebsites}}:7080 works as expected.</p>

<p>So, does the question now become, how to set the server to, by default, serve http requests over port 7080?</p>

<p><strong>Update</strong>: <a href=""https://stackoverflow.com/a/34536957/1374538"">This answer</a> mentions a vhost.conf, which I couldn't find. However, it seems that it's possible to define a VirtualHost inside httpd.conf as well. Of the 17 domains I host on this server, there's one that's the 'main' domain, which I take it, in a way, represents the server. (Keep in mind, this is not my cup of tea.)</p>

<p>So, I added the following to httpd.conf:</p>

<pre><code>&lt;VirtualHost *:80&gt;
  ServerName {{mymaindomain}}
  ProxyPass / http://localhost:7080/
&lt;/VirtualHost&gt;
</code></pre>

<p>After restarting Apache, browsing to http://{{mymaindomain}} now actually serves the associated website. Hurrah! But, browsing to any other domain that I host as well, also goes to the same content (http://{{mymaindomain}}).</p>

<p>If I add another VirtualHost section in httpd.conf, like so:</p>

<pre><code>&lt;VirtualHost *:80&gt;
  ServerName {{someotherdomain}}
  ProxyPass / http://localhost:7080/
&lt;/VirtualHost&gt;
</code></pre>

<p>Then, the result is that, when browsing to http://{{someotherdomain}}, I get presented with a 'Service Unavailable'.</p>

<p>So now, how do I make sure all websites redirect properly? Wouldn't this be some configuration in Plesk? How is it possible that this stopped working, from one moment to the next, for all my websites?</p>

<p><strong>Update</strong>: Quite in above my head, the following applies:</p>

<pre><code>&lt;VirtualHost *:80&gt;
    ServerName {{mymaindomain}}
    ProxyPass / http://localhost:7080/
&lt;/VirtualHost&gt;
</code></pre>

<p>The above works as I would like it to: browser requests for http://{{mymaindomain}} show the associated website.</p>

<pre><code>&lt;VirtualHost *:80&gt;
    ServerName {{myseconddomain}}
    ProxyPass / http://{{myseconddomain}}:7080/
&lt;/VirtualHost&gt;
</code></pre>

<p>The above domain is using CloudFlare. Browsing to http://{{myseconddomain}} does not work; CloudFlare is not able to reach the server.</p>

<pre><code>&lt;VirtualHost *:80&gt;
    ServerName {{mythirddomain}}
    ProxyPass / http://{{mythirddomain}}:7080/
&lt;/VirtualHost&gt;
</code></pre>

<p>The above domain is not using CloudFlare. Browsing to http://{{mythirddomain}} shows the associated website.</p>

<p>I still don't understand what is causing all this, help is still appreciated.</p>

<p>Meanwhile, I could move all domains off CloudFlare, but then I need to somehow set up https, which was working fine, before.</p>

<p><strong>Update</strong>: Below, @HardyRust's mentions nginx perhaps listening on port 80.</p>

<p>I executed:</p>

<pre><code>systemctl status nginx.service
</code></pre>

<p>Which got me the below. The 'since...' is pretty much when my server stopped working.</p>

<pre><code>● nginx.service - Startup script for nginx service
   Loaded: loaded (/usr/lib/systemd/system/nginx.service; enabled; vendor preset: disabled)
   Active: failed (Result: exit-code) since Sat 2016-10-01 00:23:25 CEST; 1 day 1h ago

Oct 01 00:23:24 {{mymaindomain}} systemd[1]: Starting Startup script for nginx service...
Oct 01 00:23:25 {{mymaindomain}} nginx[1066]: nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
Oct 01 00:23:25 {{mymaindomain}} nginx[1066]: nginx: [emerg] bind() to [{{my ipv6 address}}]:443 failed (99: Cannot assign requested address)
Oct 01 00:23:25 {{mymaindomain}} nginx[1066]: nginx: configuration file /etc/nginx/nginx.conf test failed
Oct 01 00:23:25 {{mymaindomain}} systemd[1]: nginx.service: control process exited, code=exited status=1
Oct 01 00:23:25 {{mymaindomain}} systemd[1]: Failed to start Startup script for nginx service.
Oct 01 00:23:25 {{mymaindomain}} systemd[1]: Unit nginx.service entered failed state.
Oct 01 00:23:25 {{mymaindomain}} systemd[1]: nginx.service failed.
</code></pre>

<p>So, why would it not be possible to 'assign requested address'?</p>

<p>Any suggestions are very welcome, and needed.
I have no idea what /etc/nginx/nginx.conf is supposed to look like, but its 30 or so lines look like they belong.</p>
","<web-hosting><http><centos><firewall>","2016-10-01 12:14:03"
"806635","Best Method to Migrate /www Folder and mariadb","<p>I have a server running under Centos 7 with Raid 10 SATA disks (4TB), I have MSA2040 and created a volume for my server (Raid 6 - 10TB SAS).</p>

<p>I have formatted and created partition to be able to use this volume on my server. Now I am able to use it normally.</p>

<p>It communicates with 8 Gbps fibre channel. </p>

<p>What I want to ask is; Current I created a directory on <code>/msaStorage/myFolder</code> and created a link to this folder in my apache folder.</p>

<p>Current setup is:
<code>/msaStorage/exampledomain.com</code> is linked to <code>/var/www/exampledomain.com</code> (alias)</p>

<p>I can use it without any problem but is there any efficient method should I use? like creating an alias directly in apache config? Or is it logical to use <strong>Apache Mesos</strong>? Also I am planning to migrate my database to this volume, should I follow <a href=""https://dba.stackexchange.com/questions/11884/move-data-folder-and-drive-mysql-centos"">this</a> guide?</p>

<p>Thanks in advance for suggestions.</p>
","<centos><apache-2.4><mariadb>","2016-10-02 14:27:29"
"806723","How to bind use port 514 so graylog can use as input source","<p>How can I bind 514 so it can be used by graylog.</p>

<p>There are examples like</p>

<pre><code>iptables -t nat -A PREROUTING -i eth0 -p udp -m udp --dport 514 -j REDIRECT --to-ports 5514
</code></pre>

<p>But what I want is to use 514 using root.</p>
","<graylog>","2016-10-03 09:01:11"
"806755","How to open a port in Centos","<p>I would like to use my <code>iptables</code> file like this.</p>

<pre><code># Firewall configuration written by system-config-firewall
 # Manual customization of this file is not recommended.
 *filter
 :INPUT ACCEPT [0:0]
 :FORWARD ACCEPT [0:0]
 :OUTPUT ACCEPT [0:0]
 :FIREWALL - [0:0]

 -A INPUT -i lo -j ACCEPT
 -A INPUT -p icmp -m icmp --icmp-type any -j ACCEPT
 -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT

 -A INPUT -p tcp -m state --state NEW -m tcp --dport 113 -j REJECT --reject-with tcp-reset

 # Global

 -A INPUT -i eth0 -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT
 -A INPUT -i eth0 -p tcp -m state --state NEW -m tcp --dport 9117 -j ACCEPT
 -A INPUT -i eth0 -p tcp -m state --state NEW -m tcp --dport 9118 -j ACCEPT
 -A INPUT -i eth0 -p tcp -m state --state NEW -m tcp --dport 9119 -j ACCEPT
 -A INPUT -i eth0 -p tcp -m state --state NEW -m tcp --dport 9116 -j ACCEPT
 -A INPUT -i eth0 -p tcp -m state --state NEW -m tcp --dport 27017 -j ACCEPT
 -A INPUT -i eth0 -p tcp -m state --state NEW -m tcp --dport 80 -j ACCEPT
 -A INPUT -i eth0 -p tcp -m state --state NEW -m tcp --dport 81 -j ACCEPT
 -A INPUT -i eth0 -p tcp -m state --state NEW -m tcp --dport 25 -j ACCEPT
 -A INPUT -i eth0 -m state --state NEW -j DROP

 -A INPUT -j REJECT --reject-with icmp-host-prohibited
 -A FORWARD -j REJECT --reject-with icmp-host-prohibited
 COMMIT
</code></pre>

<p>But after I type <code>netstat -anlp</code> to check which ports were open, I don't see port 9119,9117,9116 open.
Please help me!</p>
","<centos><iptables><port>","2016-10-03 11:12:47"
"806853","SSL stopped working, LAMP on Debian via Turnkey VM","<p>I’ve had our site up and operational for approx. 2 months. The website will no longer load if SSL is enabled. I can edit the /etc/apache2/sites-enabled/wordpress and rem out the SSL configuration and the site will load listening on *:80 The certificates are SHA2 compliant.</p>

<p>Here is what my configuration file looks like</p>

<pre><code>NameVirtualHost *:80
NameVirtualHost *:443

&lt;VirtualHost *:80&gt;
UseCanonicalName Off
ServerAdmin butlera at mehlvilleschooldistrict.net
DocumentRoot /var/www/wordpress
&lt;/VirtualHost&gt;

&lt;VirtualHost *:443&gt;
SSLEngine on
SSLCertificateFile /etc/ssl/certs/prodev.cer
SSLCertificateKeyFile /etc/ssl/private/prodev.key
SSLCertificateChainFile /etc/ssl/certs/gd_bundle-g2.crt
ServerAdmin butlera at mehlvilleschooldistrict.net
ServerName my.server.address
ServerAlias http://www.my.server.address
DocumentRoot /var/www/wordpress
&lt;/VirtualHost&gt;

&lt;Directory /var/www/wordpress&gt;
Options +FollowSymLinks
AllowOverride All
order allow,deny
allow from all
&lt;/Directory&gt;
</code></pre>

<p>Here is my error.log</p>

<pre><code>[Mon Oct 03 17:53:39 2016] [info] Init: Seeding PRNG with 656 bytes of entropy
[Mon Oct 03 17:53:39 2016] [info] Loading certificate &amp; private key of SSL-aware server
[Mon Oct 03 17:53:39 2016] [error] Init: Private key not found
[Mon Oct 03 17:53:39 2016] [error] SSL Library Error: 218529960 error:0D0680A8:asn1 encoding routines:ASN1_CHECK_TLEN:wrong tag
[Mon Oct 03 17:53:39 2016] [error] SSL Library Error: 218640442 error:0D08303A:asn1 encoding routines:ASN1_TEMPLATE_NOEXP_D2I:nested asn1 error
[Mon Oct 03 17:53:39 2016] [error] SSL Library Error: 218529960 error:0D0680A8:asn1 encoding routines:ASN1_CHECK_TLEN:wrong tag
[Mon Oct 03 17:53:39 2016] [error] SSL Library Error: 218595386 error:0D07803A:asn1 encoding routines:ASN1_ITEM_EX_D2I:nested asn1 error
[Mon Oct 03 17:53:39 2016] [error] SSL Library Error: 67710980 error:04093004:rsa routines:OLD_RSA_PRIV_DECODE:RSA lib
[Mon Oct 03 17:53:39 2016] [error] SSL Library Error: 218529960 error:0D0680A8:asn1 encoding routines:ASN1_CHECK_TLEN:wrong tag
[Mon Oct 03 17:53:39 2016] [error] SSL Library Error: 218595386 error:0D07803A:asn1 encoding routines:ASN1_ITEM_EX_D2I:nested asn1 error
</code></pre>

<p>Any Help Would be greatly appreciated</p>
","<debian><ssl><lamp>","2016-10-03 19:56:37"
"806876","Unable to login to email server that I can telnet to?","<p>I just finished following <a href=""https://www.digitalocean.com/community/tutorials/how-to-configure-a-mail-server-using-postfix-dovecot-mysql-and-spamassassin"" rel=""nofollow noreferrer"">this tutorial</a> on Digital Ocean for configuring a Dovecot, MySQL, and Postfix email server. I did not follow it to the end. (I didn't install Spam Assassin.) I also used a self-generated SSL temporarily. Other than that, I followed the tutorial to the letter. At every point where I was told to do a test command, I did so and the results were what the author said they should be. </p>

<p>The problem is that using an email client (like Thunderbird) I cannot login to the server. However, using Telnet I can connect to SMTP on ports 587 and 25. I can also connect to IMAP on port 993. (25, 587, and 993 are the ONLY ports I have forwarded from my external IP.)</p>

<p>I have checked the data in the MySQL DB. My user exists with my desired password.</p>

<p>How can I troubleshoot the issue here? Is there a way to test login credentials using telnet?</p>

<p><strong>EDIT</strong></p>

<p>Authentication is the issue. I've initiated the connection with <code>openssl s_client -connect mail.example.com:587 -starttls smtp</code> (Thanks 84104). My exchange then looks like this...</p>

<pre><code>ehlo example.com
250-hostname.example.com
250-PIPELINING
250-SIZE 10240000
250-VRFY
250-ETRN
250-AUTH PLAIN LOGIN
250-ENHANCEDSTATUSCODES
250-8BITMIME
250 DSN
AUTH LOGIN
334 VXNlcm5hbWU6
MyBase64EncodedUsername
334 UGFzc3dvcmQ6
MyBase64EncodedPassword
535 5.7.8 Error: authentication failed: Connection lost to authentication server
</code></pre>

<p>Is there a debugging route I can take to find the problem? Is this simply authentication or a connection issue?</p>
","<mysql><postfix><dovecot><password><thunderbird>","2016-10-04 01:24:52"
"806888","How to differentiate file & directory in the output of du -sh","<p>The output of <code>du -sh * | sort -h</code> lists all the files and directories in sorted order based on size. But the problem is; the output doesn't differentiate the directories from files.</p>

<p>For example:</p>

<pre><code>15K file1backup
16K Desktop
</code></pre>

<p>Is there any option to du which makes it easier to differentiate the files and directories?</p>
","<linux><disk-space-utilization><du>","2016-10-04 05:04:16"
"949478","Unable to connect Windows Server 2012 R2 to a Domain Controller. Getting Error","<p>I am unable to connect Windows Server 2012 R2 to a Domain Controller. Getting Error,</p>

<p><code>An Attempt to Resolve the DNS name of a domain controller in the domain being joined has failed. Please verify the client is configured to reach a DNS server that can resolve DNS names in the target domain</code></p>

<p>But the Client Server is able to ping the domain as well as it is able to resolve it. (checked it using nslookup), While adding the domain, it popup the authentication window after that it shows the above-provided error.</p>

<p><a href=""https://i.sstatic.net/WkXqf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WkXqf.png"" alt=""enter image description here""></a></p>
","<active-directory><windows-server-2012-r2><internal-dns><netbios>","2019-01-17 08:00:07"
"807143","Will Ubuntu automatically queue hundreds of processes?","<p>I'm creating a service that will be calling potentially hundreds or even 1000s of <code>ffmpeg</code> commands at once. Will Ubuntu 16.04 automatically queue these jobs? </p>

<p>Is there some sort of app that does a better job than its native methods?</p>
","<queue><ubuntu-16.04>","2016-10-05 05:00:26"
"807158","Apache Processes and CPU load oddness","<p>For the last couple of hours I've been trying to battle with my server to keep it up during some pretty minor load (50 concurrent users).</p>

<p>Spec:</p>

<pre><code>6 CPUs
12GB RAM
</code></pre>

<p>During this time, memory usage maxed out at 4GB, so no problems there.</p>

<p>However, Apache was going insane kicking up about 20+ running processes and eating up all 6 CPUs (600% CPU usage), bringing the website to a halt.</p>

<p>Now; with exactly the same traffic and concurrent users, CPU usage is down to 40% of the available 600% - no changes were made.</p>

<p>I cannot for the life of me see why Apache thought it necessary to kick up 20+ running processes, and at the same time use 1 or 2 for the same traffic volumes. </p>

<p>How can I diagnose what these Apache processes are actually doing? I know to limit this through MaxClients but that still bottlenecks the server when its trying to create 20+.</p>
","<apache-2.2><ubuntu-14.04>","2016-10-05 07:25:59"
"807197","Kill TCP Connection for X Seconds (mail / postfix)","<p>I have several IPs that send emails. It goes like this:</p>

<pre>IP1,IP2,IP3,IP4
IP1 -> sendsmail1,sendmail2,sendmail3
IP2 -> sendmail4,sendmail5,sendmail6,sendmail7,...
IP3 -> sendmail8,sendmail9,sendmail10,...
IP4...</pre>

<p>I want to make the switch to the next IP after a single mail sent:</p>

<pre>IP1 -> sendmail1
IP2 -> sendmail2
IP3 -> sendmail3
...</pre>

<p>I tried to make a script that will continue to grep the last line of log file and search for ""sent"" and if found to kill the process having port 25 and force it to get through the next IP, but it requires to restart postfix.
Is there any method to kill the connection long enough to switch to next IP?</p>
","<networking><email><postfix><load-balancing><linux-networking>","2016-10-05 09:59:14"
"949666","No attacking IP shown in logs","<p>My security log shows brute force attempts against RDP but no offending IP number. Cyberarms Intrusion Detection detects nothing. Symantec Endpoint Security detects nothing (in traffic log).</p>

<p>How is this possible ? </p>

<p>How can I find this mysterious IP ?</p>
","<windows-server-2008-r2>","2019-01-18 07:34:20"
"807343","How to redirect the root URL of parked domains, using a variable","<p>I need 1000 parked domains to redirect to a corresponding page.</p>

<p>For example, if one of the parked domains is ""parkeddomain.com"", then it must be redirected to <a href=""http://example.com/landingpage/parkeddomain-com"" rel=""nofollow noreferrer"">http://example.com/landingpage/parkeddomain-com</a></p>

<p>Conditions:</p>

<ul>
<li>Only the root URL of the parked domains must be redirected, not
example.com </li>
<li>The browser url address must stay the same
""parkeddomain.com""</li>
</ul>

<p>The purpose is to create a simple system for a domain broker to host domains, each with one landing page per domain.</p>

<p><a href=""https://i.sstatic.net/Xy0xx.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Xy0xx.png"" alt=""enter image description here""></a></p>

<p>I will try to make this look less complicated, sorry. The idea is: redirect every domain to /landingpage/[thedomainname] except from the primary domain since that shows the homepage of the company website.</p>
","<.htaccess><redirect>","2016-10-05 19:49:09"
"807392","how to deploy customized linux vm on azure using powershell-script","<p>Kindly help deploying customized (Hyper-V) Linux VM (VHD) on Azure. Will be very helpful if you can help me with power-shell script.</p>

<p>We have the power shell script to upload and deploy hyper-v windows vhd images on Azure which is working fine and same script is working good to upload customized Linux (Redhat) hyper-v vhd images on Azure but unable to use the same script to deploy customized uploaded Linux (Redhat) Hyper-v vhd on Azure.</p>
","<linux><linux-networking><azure><cloud><arch-linux>","2016-10-06 04:32:12"
"949913","SPF record permerror showed in an email header, but no reason for it","<p>Our customers are complaining about the spam issue. There are a number of spam emails which are not tagged as spam emails. These spam emails were sent from forged seneder address, which means our SPF record does not work. We have set up the SPF record in our DNS record and we have used some SPF checker tools to check the validity of our SPF record, but no problem was reported in the check results. I have checked an email of our customer and find that there is an item ""X-Bordeaux-SPF: PermError"" in the email header, but no more detail was presented in the email header. I have read many resources about permerror in SPF, but I still did not find the answer which can explain the reason in this case. Now I have no idea what's the problem with our SPF record. What could be the cause for this kind of permerror?</p>

<p>The domain is ""made-in-china.com"". The SPF record is ""v=spf1 include:spf.made-in-china.com -all"".
Here are email headers:</p>

<pre><code>Received: from iredmail.wattan.tv([82.102.216.102]) by smail59.cn4e.com(7.3.0.15a) with ESMTP id FE2E06430007.224.1547615038.691636;
    Wed, 16 Jan 2019 13:04:04 +0800 (CST)

X-BQId: FE2E06430007.224.1547615038.691636.1

X-Bordeaux-Type: SMTP
X-35BMId: FE2E06430007.224.1547615038.691636.1

X-Bordeaux-Action-libantispam.so:
    Action: Relay[NEXT,10180,100:100:100],Relay[NEXT,100001,100:100:100],Relay[NEXT,208,100:100:100],Relay[NEXT,405,100:100:100],Relay[NEXT,10143,100:100:100],Relay[NEXT,10158,100:100:100],Relay[NEXT,10162,100:100:100],Relay[NEXT,10164,100:100:100],Relay[NEXT,10166,100:100:100],Relay[NEXT,10170,100:100:100],Relay[NEXT,10171,100:100:100],Relay[NEXT,10191,100:100:100],Relay[NEXT,10214,100:100:100],Relay[NEXT,10183,100:100:100],Relay[NEXT,10184,100:100:100],Relay[NEXT,10185,100:100:100],Relay[NEXT,10186,100:100:100],Relay[NEXT,10146,100:100:100],Relay[SPAM,1009,100:100:100],Relay[NEXT,10015,100:100:100],Relay[NEXT,10181,100:100:100],Relay[NEXT,100002,100:100:100]

X-Bordeaux-Action-libspamsa.so:

X-Bordeaux-Action-libclamav.so:

X-Bordeaux-Action-libsmtpext.so:

X-FBA-Flag: YES

X-Bordeaux-SPF: PermError
</code></pre>
","<domain-name-system><email><spf><txt-record>","2019-01-20 01:15:36"
"807850","Prestashop with php-fpm gives uncomplete header error","<p>I am trying to run prestashop with apache. I have enabled mpm_event module.<a href=""https://i.sstatic.net/tTdX6.png"" rel=""nofollow noreferrer"">this is my virtual host configuration</a></p>

<p>When I go to website I got the error </p>

<p><a href=""https://i.sstatic.net/MflC2.png"" rel=""nofollow noreferrer"">apache error log</a></p>

<h2>virtual host conf file</h2>

<pre><code>&lt;VirtualHost *:8080&gt;
ServerName prestashop.varnish.com
ServerAlias www.prestashop.varnish.com
DocumentRoot /var/www/prestashop
CustomLog /var/log/apache2/prestashop_access.log combined
ErrorLog  /var/log/apache2/prestashop_error.log
&lt;IfModule mod_fastcgi.c&gt;
    AddHandler php5-fcgi .php
    #Action php5-fcgi /php7-fcgi
    #Alias /php5-fcgi /usr/lib/cgi-bin/php5-fcgi
    FastCgiExternalServer /var/www/prestashop -socket /var/run/php/php5-fpm.sock -pass-header Authorization
&lt;/IfModule&gt;

&lt;Directory /var/www/prestashop&gt;
    Options Indexes Multiviews FollowSymLinks
    Require all granted
    AllowOverride all
&lt;/Directory&gt;
&lt;/VirtualHost&gt;
</code></pre>

<h2>error log</h2>

<pre><code>[Sat Oct 08 08:25:09.896291 2016] [fastcgi:error] [pid 7993:tid 140080677115648] (2)No such file or directory: [client 192.168.10.205:58254] FastCGI: failed to connect to server ""/var/www/prestashop/index.php"": connect() failed
[Sat Oct 08 08:25:09.896360 2016] [fastcgi:error] [pid 7993:tid 140080677115648] [client 192.168.10.205:58254] FastCGI: incomplete headers (0 bytes) received from server ""/var/www/prestashop/index.php""
[Sat Oct 08 08:25:09.909828 2016] [fastcgi:error] [pid 7993:tid 140080668722944] (2)No such file or directory: [client 192.168.10.205:58255] FastCGI: failed to connect to server ""/var/www/prestashop/index.php"": connect() failed, referer:
[Sat Oct 08 08:25:09.909880 2016] [fastcgi:error] [pid 7993:tid 140080668722944] [client 192.168.10.205:58255] FastCGI: incomplete headers (0 bytes) received from server ""/var/www/prestashop/index.php"", referer: 
</code></pre>
","<apache-2.4><php-fpm>","2016-10-08 09:03:02"
"808435","Remoting from Windows Server 2012 64 bit to Windows Server 2008 R1","<p>We need to upgrade our servers which run Windows Server 2008 R1 (32 bit version) to Windows Server 2012 (64 bit version).</p>

<p>Some old components are not supported in Windows Server 2012.</p>

<p>Is it possible to invoke the old components (located on Windows Server 2008 R1) from the new Windows 2012 server via remoting?</p>

<p>Has anyone done something similar? </p>
","<windows><windows-server-2008><windows-server-2012><installation><remoting>","2016-10-11 17:38:31"
"950847","Not receiving ""Fixed/Ordered"" query response","<p>We are testing a query:</p>

<ul>
<li>FQDN: test.example.net</li>
<li>Query Type: AAAA</li>
</ul>

<p>Client > DNS A > DNS B</p>

<p>Simplified DNS query flow:</p>

<ol>
<li>Client queries test.example.net -t AAAA</li>
<li>Query goes to DNS A which then forwards the query to DNS B</li>
<li>DNS B responds with 2 ORDERED IPV6 addresses to DNS A</li>
<li>DNS A receives the 2 ORDERED IPV6 addresses from DNS B</li>
</ol>

<p>Expected</p>

<ol start=""5"">
<li>DNS A responsds with 2 ORDERED IPV6 addresses to Client</li>
</ol>

<p>What is happening</p>

<ol start=""5"">
<li>DNS A responds with 2 NON-ORDERED IPV6 addresses to Client</li>
</ol>

<p>Things we have tried:</p>

<ul>
<li>Turn off cacheing on DNS A and DNS B</li>
</ul>

<p>Things we noticed:</p>

<ul>
<li>When the query goes from DNS A to DNS B, the transaction ID changes</li>
<li>When DNS A responds with 2 IPV6 address to Client, the transaction ID changes back to the original transaction ID.</li>
</ul>

<p>Questions:</p>

<ul>
<li>What options/settings are we overlooking to make sure the Client receives fixed ordered query responses?</li>
<li>What could be the reason for DNS A receiving the ordered query response from DNS B, but does not send the ordered query response back to the client?</li>
</ul>

<p>Thanks
BD</p>
","<linux><domain-name-system><bind>","2019-01-25 21:40:29"
"950863","What is the use of the lost+found directory?","<p>I would like to use digitalocean's block storage decive as a dedicated file system to manage Docker containers. The plan is to have this file system mounted at /var/lib/docker at boot time, before the Docker service is started.</p>

<p>My attemps to do so thus far have been unsuccessful. While my playbook does not report an error, running <code>ls -la /var/lib/docker</code> after formatting and partitioning DO's block storage device indicates that I might have a problem: </p>

<pre><code>drwxr-xr-x  3 root root  4096 
drwx--x--x 14 root root  4096
drwx------  2 root root 16384 Jan 25 16:47 lost+found
</code></pre>

<p>After reading <a href=""https://www.linuxquestions.org/questions/linux-newbie-8/why-is-a-lost-found-folder-in-my-newly-created-disk-4175577420/"" rel=""nofollow noreferrer"">this</a>, <a href=""https://askubuntu.com/questions/2115/whats-lostfound-and-where-did-it-come-from"">this</a>, and <a href=""http://tldp.org/LDP/Linux-Filesystem-Hierarchy/html/lostfound.html"" rel=""nofollow noreferrer"">this</a>, I can't decipher what <code>lost+found</code> means but I do grasp that it isn't a good sign. </p>

<p>I would like to understand why and fix it of course. My playbook is below (please note that playbook below is using static/explicit values due to needing to debug):</p>

<pre><code>---
- name: mount point of attached volume 
  stat: 
    path: /mnt/name_of_attached_volume

- name: get digital_ocean_volume_path_by_name 
  stat:
    path: /dev/disk/by-id/scsi-0DO_Volume_name_of_attached_volume

- name: unmount images volume
  command: umount /mnt/name_of_attached_volume


- name: Label the volume
  command: parted -s /dev/disk/by-id/scsi-0DO_Volume_name_of_attached_volume mklabel gpt

- name: Create an ext4 partition
  command: parted -s -a opt /dev/disk/by-id/scsi-0DO_Volume__name_of_attached_volume mkpart primary ext4 0% 100%

- name: Build the ext4 metadata
  command: mkfs.ext4 /dev/disk/by-id/scsi-0DO_Volume__name_of_attached_volume-part1

####################################################################
#  since the mount point  -- `/var/lib/docker`  -- already exists  #
#  by virtue of docker being installed on the host, no need to     #
#  create a mount point but I do need stop docker running          #
####################################################################

- name: stop docker service 
  service: 
    name: docker 
    state: stopped

- name: mount volume read-write
  mount:
    path: /var/lib/docker
    src: /dev/disk/by-id/scsi-0DO_Volume__name_of_attached_volume-part1
    fstype: ext4 
    opts: defaults,discard
    dump: 0
    passno: 2
    state: mounted

- name: remove mount point for images volume 
  command: rmdir /mnt/name_of_attached_volume

- name: Start docker service 
  service: 
    name: docker
    state: started
    enabled: ""{{ docker_service_enabled }}""

</code></pre>

<p>I am obviously missing/misunderstanding a step. Greatly appreciate tips please. Thank you!</p>
","<docker><ubuntu-18.04><digital-ocean>","2019-01-26 02:28:07"
"808507","Flooded by wpad.dat on apache2","<p>A company I am servicing has a Domain Controller Name called <code>internal.example.com</code> and whenever their employees go home, Apache is flooded by wpad.dat request. This flood of requests are redirected to 404, this overwhelms apache and it bogs down.</p>

<p>There is a wildcard DNS entry for *.example.com to be redirected to the main website.</p>

<p>These are the things I already did:</p>

<ol>
<li><p>Served a wpad.dat file in the root directory of their website. Apache stops bogging down when I started serving a static file.</p></li>
<li><p>Added DNS entries for <code>wpad.internal.example.com</code>, <code>internal.example.com</code>, <code>wpad.example.com</code> to <code>127.0.0.1</code>.</p></li>
</ol>

<p>After adding the DNS and letting it propagate, I can still see requests on my log files.</p>

<p>I don't know what domain they are trying to access. The log file is referring to the main website instead of what they are trying to access.</p>

<p>Here are the applications that are requesting the wpad.dat file</p>

<ul>
<li>WinHttp-Autoproxy-Service/5.1</li>
<li>Microsoft Office  2014</li>
<li>Kaspersky Proxy-Server detection agent</li>
<li>Mozilla/5.0</li>
</ul>

<p>This thread is related <a href=""https://serverfault.com/questions/510396/being-flooded-by-wpad-dat"">Being flooded by wpad.dat</a> but there is no conclusive answer how to fix or at least block requests.</p>

<p>EDIT: 
I don't know what subdomain they are trying to access and apache reporting it is coming from the main website. The entries I added to the DNS are intelligent guesses, I don't have concrete proof on what they are accessing.</p>
","<apache-2.2><wpad.dat>","2016-10-12 02:48:50"
"950915","Can't access Apache installed on virtual machine (KVM) running Ubuntu 18.04","<p>VPS: KVM (SolusVM),
VM OS: Ubuntu 18.04,
Network card: Virtio / Intel Pro 1000,</p>

<p>I can ping VM IP 21X.XXX.XXX.XXX from my machine(at home). But cannot access Apache port 80 using 21X.XXX.XXX.XXX or domain.</p>

<p>I installed docker in my VM, which is also not accessible outside using  21X.XXX.XXX.XXX:8000.</p>

<p>Am I forgetting something to configure?</p>

<p>Note:
If I curl 21X.XXX.XXX.XXX:8000 <strong>inside VNC</strong>, it works but not on internet.</p>
","<docker><kvm-virtualization>","2019-01-26 16:02:06"
"808559","Wordpress | Keep getting same error over and over","<p>i did move a website to another server now i keep getting the same error over and over again and the page will freeze. But when i put the backup of the website on xampp it does work?</p>

<p>This is the code that repeats itself</p>

<pre><code>Warning: is_dir(): open_basedir restriction in effect. File(/var/home/lapermanen/domains/lapermanence.vdbc.nl/public_html/wp-content) is not within the allowed path(s): (/var/home/cyclosport/:/tmp/:/var/tmp/:/usr/local/php56/lib/:/usr/local/php54/lib/:/usr/local/php55/lib/:/usr/local/lib/php/) in /var/home/cyclosport/domains/cyclosportive.coersonline.nl/public_html/wp-includes/functions.php on line 158
</code></pre>

<p>Thanks.</p>

<p>HTACCESS</p>

<pre><code># BEGIN WordPress
&lt;IfModule mod_rewrite.c&gt;
RewriteEngine On
RewriteBase //
RewriteRule ^index\.php$ - [L]
RewriteCond %{REQUEST_FILENAME} !-f
RewriteCond %{REQUEST_FILENAME} !-d
RewriteRule . //index.php [L]
&lt;/IfModule&gt;

# END WordPress
</code></pre>
","<php><wordpress>","2016-10-12 09:24:40"
"950922","Postfix Package Openarc","<p>I have a server running Ubuntu 16.  I'm trying to get postfix configured to use OpenArc.</p>

<p>I've installed the OpenArc package, but it's not automatically starting on reboot.</p>

<p>If I run</p>

<pre><code>openarc -c /etc/openarc/openarc.conf -n
</code></pre>

<p>it doesn't return an error.  /etc/openarc/openarc.conf is as follows:</p>

<pre><code>AuthservID mi domain
Domain mi domain
KeyFile /etc/opendkim/keys/201901.private
Selector 201901
Mode sv
Socket inet:8895@localhost
SoftwareHeader yes
Syslog Yes
UserID opendkim
AutoRestart yes
PidFile /var/run/opendkim/openarc.pid
</code></pre>

<p>If I run</p>

<pre><code>openarc -c /etc/openarc/openarc.conf
</code></pre>

<p>It tells me the milter is busy.</p>

<p>When I run <code>lsof -i | grep openarc</code> it reports:</p>

<pre><code>openarc  10849 opendkim   7u  IPv4 160111      0t0 TCP localhost: 8895-&gt; localhost: 44132 (ESTABLISHED)
</code></pre>

<p>I've also configured postfix to access the milter by adding to /etc/postfix/main.cf:</p>

<pre><code>smtpd_milters = inet: localhost: 8891, inet: localhost: 8892, inet: localhost: 8895
non_smtpd_milters = inet: localhost: 8891, inet : localhost: 8892, inet: localhost: 8895
</code></pre>

<p>But when postfix tries to use it, it reports</p>

<pre><code>postfix / smtpd [1396]: warning: connect to Milter service inet: localhost: 8895: Connection refused
</code></pre>

<p>Please let me know what else you may need to know to help me with my problem.  Thank you very much.</p>

<hr>

<p>gracias por intentar ayudarme pese a que no se entiende el idioma .. mi idioma nativo es el español .. mira cuando reinicio la maquina ... el openarc no se ejecuta .. pero si ... pongo el codigo .. openarc -c /etc/openarc/openarc.conf -n se ejecuta ... te paso el contenido de mi openarc.conf...</p>

<pre><code>AuthservID mi domain
Domain mi domain
KeyFile /etc/opendkim/keys/201901.private
Selector 201901
Mode sv
Socket inet:8895@localhost
SoftwareHeader yes
Syslog Yes
UserID opendkim
AutoRestart yes
PidFile /var/run/opendkim/openarc.pid
</code></pre>

<p>al hacer <code>lsoft-i</code> me da el resultado:</p>

<pre><code>openarc 10849 opendkim 7u IPv4 160111 0t0 TCP localhost:8895-&gt;localhost:44132 (ESTABLISHED)
</code></pre>

<p>pero una vez que cargo el openarc.conf ... me da el resultado de que esta conectado el milter .. se me entiende?</p>

<p>otra cosa que hice fue poner el milter en el main.cf ... de esta manera ... smtpd_milters = inet:localhost:8891, inet:localhost:8892, inet:localhost:8895 non_smtpd_milters = inet:localhost:8891, inet:localhost:8892, inet:localhost:8895</p>

<p>pero al iniciar si no pongo el openarc -c .. no me abre y me salta error ..postfix/smtpd[2174]: warning: connect to Milter service inet:localhost:8895: Connection refused</p>

<hr>

<p>I raised the openarc once I put the comomando to load the milter .. it would be really load the openarc.conf .. file as follows</p>

<p><strong><code>openarc -c /etc/openarc/openarc.conf -n</code></strong></p>

<p>Y:</p>

<p><strong><code>openarc -c /etc/openarc/openarc.conf</code></strong></p>

<p><em>when I put this the milter loads me and the openarc walks me ... but it throws me error that the milter is busy .. but I have nothing but the openarc in that milter ...</em></p>

<p><strong>postfix / smtpd [1396]: warning: connect to Milter service inet: localhost: 8895: Connection refused</strong></p>

<p>It's the first time I write a problem with postfix I do not know if they need to see other files to help me ... thank you very much ...</p>
","<linux><ubuntu><postfix><milter>","2019-01-26 18:41:13"
"950981","Aws s3 object acl everyone read access denied","<p>I have upload one image but its not publicly accessible. I have granted full AWS s3 permission for the user. My canonical user id 9ebb86750cf9111e69a4c95e1c3c53062209080c56399ddc7919be48897cf25f. Same as attached screenshot. Screenshot link:</p>

<p>While I am pressing make public button or give public permission in ACL by checking ""everyone"". It is showing that ""Access denied"". Recently I have seen that AWS has ACL for bucket and object. There is CORS configuration.</p>

<p>What could be the possible cause? My IAM has administrator access and s3full access.
<a href=""https://i.sstatic.net/bJcFw.png"" rel=""nofollow noreferrer"">enter image description here</a></p>

<p><a href=""https://i.sstatic.net/4HURF.png"" rel=""nofollow noreferrer"">enter image description here</a></p>

<p><a href=""https://i.sstatic.net/mEnEt.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","<amazon-web-services><amazon-s3><cors>","2019-01-27 11:55:07"
"950997","Combine disks/directories in a virtual volume","<p>Say I have 4 disks, they are mounted on <code>/home/disk[1-4]</code>. The file tree looks like this:</p>

<pre><code>disk1:
    file1
    dir1:
        file2

disk2:
    dir1:
        file3

disk3:
    file4
    dir2:
        file5

disk4:
    file6
</code></pre>

<p>Is there a solution to create a virtual device or directory that combine all of this in something like this?</p>

<pre><code>CombinedVolume:
    file1
    file4
    file6
    dir1:
        file2
        file3
    dir2:
        file5
</code></pre>

<p>Edit: As I would not be confident in my disks, it's better if a file is not split in multiple parts written in different disks. Say I lose <code>disk2</code>, the file tree will look like</p>

<pre><code>CombinedVolume:
    file1
    file4
    file6
    dir1:
        file2
    dir2:
        file5
</code></pre>

<p><code>file3</code> is gone but there is no alteration to other files.</p>
","<linux>","2019-01-27 15:05:17"
"808792","How to prevent default usernames/passwords on network appliances?","<p>I have a number of ready to use network appliances like router, print server, L2/L3 network switches, ip-pbx, access point and many others.</p>

<p>These devices are usually managed by web portal by accessing port 80 (http).  And asking for user name and password for accessing.</p>

<p>With so many devices to manage, I usually use default user name and password without changing it in case I might forgot in future.  However, this habit is risky and easy for someone to change the configuration easily.</p>

<p>Is there a way to leave the default username / password as is but prevent other local network users to do the changes on the configuration?  </p>

<p>Can VLAN segmentation help by grouping all those devices in one VLAN?  But doing so seems prevent workstations from other VLAN accessing the services of those devices.</p>

<p>Or may be firewall + VLAN may help?</p>
","<networking><vlan>","2016-10-13 08:58:26"
"951199","HP ProLiant DL380 G6 RAID Controllers","<p>I have gained responsibility over a system utilizing several HP ProLiant DL380 servers. I would like to switch to RAID6 and use SSDs, of which we have dozens (Samsung Evo 860). Unfortunately, these SSDs are consumer level SATA devices, and the current onboard RAID controller in these DL380s, being a P410, will throttle the interface speed down to 3Gbps.</p>

<p>Taking all of this into consideration, could I simply purchase a newer RAID controller (like a P800) with a decent sized integrated battery backed cache to utilize these piles of SATA SSDs in a RAID6 array?</p>

<p>Edit: I mistakenly claimed my current RAID card is a p408i, when it is in fact a P410. </p>
","<hp><hp-proliant>","2019-01-29 01:44:04"
"809091","Windows Server 2012r2: Lsass.exe getting hammered from Internet","<p>guys.  My gateway server has been getting hammered for about a day, now.  Internet access slows to a crawl for about an hour at a time, internally, and I noticed that when it is slow, lsass.exe is busy sending something out to external machines on the Internet.  I would think this is some sort of brute force attack, but I don't understand why lsass is sending a bunch of data but receiving nothing.  This box has a software firewall on it until the appliance arrives.  The hostnames talking to lsass indicated by Resource Monitor aren't real, and I haven't found a way to find their IPs.  I tried setting up an inbound rule to block access to lsass on the Internet NIC, but that does not seem to have helped.  Malwarebytes found nothing amiss.  Nothing particularly helpful in failed security audits, either - just attempts at logging in with unknown users every minute or so.
Any suggestions on what I should do to stop this traffic in the near term?  I'm afraid this is not my wheelhouse, and I am doing what I can to try to get this controlled.  Thanks in advance for suggestions.  Screenshot from Resource Monitor is below.</p>
<p><a href=""https://i.sstatic.net/lLDWa.jpg"" rel=""nofollow noreferrer"">lsass getting hammered</a></p>
","<windows><windows-server-2012-r2><lsass>","2016-10-14 16:26:27"
"951491","Many domains with SSL in Nginx","<p>I have many domains like <code>hello1.com</code>, <code>hello2.com</code> ... <code>hello101.com</code>
All of them having SSL Certificates by Let's encrypt.
All of this domains need to be proxied to this URI <code>http://127.0.0.1:8000</code>  (<code>django</code> is there and it will handle everything)</p>

<p>So, this is <code>Nginx</code> config for one domain:</p>

<pre><code>server {
    listen  80 default_server;
    return 404;
}

server {
    listen 443 ssl;
    server_name         hello.com;

    location ~ ^/(static|media)/ {
        root            /hosting/hello/files;
        expires         30d;
    }
    location / {
         proxy_set_header Host $host;
         proxy_set_header X-Forwarded-For $remote_addr;
         proxy_set_header X-Forwarded-Proto https;
         proxy_pass     http://127.0.0.1:8000;
         proxy_set_header Accept-Encoding  """";
    }

    ssl_certificate /etc/letsencrypt/live/hello.com/fullchain.pem; # managed by Certbot
    ssl_certificate_key /etc/letsencrypt/live/hello.com/privkey.pem; # managed by Certbot

    include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot
    ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot

}
</code></pre>

<p>But if I want to config all my servers, there will be a lot of repeating lines of code and it will be very difficult to maintain.</p>

<p>Only 3 lines of configuration are different for each domain:</p>

<pre><code>server_name        &lt;domain&gt;.com;
ssl_certificate /etc/letsencrypt/live/&lt;domain&gt;.com/fullchain.pem; # managed by Certbot
ssl_certificate_key /etc/letsencrypt/live/&lt;domain&gt;.com/privkey.pem; # managed by Certbot
</code></pre>

<p>How can I move all other lines to one place? I don't want to repeat this configuration 100 times.</p>

<hr>

<p>UPD: I can wildcard <code>server_name</code> or list all domains like this:</p>

<pre><code>server_name   hello1.com hello2.com hello3.com etc.
</code></pre>

<p>Then I'll need to put current domain name (is it in <code>$host</code> variable?) into this 2 strings:</p>

<pre><code>ssl_certificate /etc/letsencrypt/live/&lt;domain&gt;/fullchain.pem;
ssl_certificate_key /etc/letsencrypt/live/&lt;domain&gt;/privkey.pem;
</code></pre>

<p>How can I do it?</p>
","<nginx><ssl>","2019-01-30 12:03:35"
"809191","Should traceroute be used to diagnose network latency issues?","<p>I ran the <code>traceroute</code> command from <code>Server A</code> to <code>Server B</code>, two of the hops display a latency between 250ms to 480ms.</p>

<p>However when running <code>ping</code> from <code>Server A</code> to <code>Server B</code>, the response is always within 1ms.</p>

<p>My network admin team is of the opinion that there are no issues since <code>ping</code> always responds back with 1ms and the latency observed via <code>traceroute</code> should't be considered.</p>

<p>How do I rule out network latency issues? </p>
","<linux-networking><ping><latency><traceroute>","2016-10-15 07:48:02"
"809332","RAID Mirroring Without data loss","<p>How can one save data from being lost while configuring RAID 1 mirroring on X 3550 M5 lenovo/IBM server having M1215 RAID controller?</p>
","<raid><raid1>","2016-10-16 10:58:08"
"809381","How does mail work, setting up own server","<p>I am going to set up my own mail server using open source MTA - Postfix.</p>

<p>In general I understand how everything works, but I still cannot find out some things.
Here is diagram I am using for understanding all this stuff </p>

<p><a href=""https://i.sstatic.net/bLx6b.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bLx6b.jpg"" alt=""enter image description here""></a></p>

<ol>
<li><p>MTA (Postifx in this case) is just utility/daemon that understands SMTP protocol </p></li>
<li><p>As I can guess mail is stored on the MDA. Is <code>postfix-maildrop</code> software that is responsible for storing all data on the server ?(Incoming, outgoing, folders) ? And anytime client can request this info using IMAP/POP3 protocol ?</p></li>
</ol>

<p><strong>My main question is where all mail is stored.</strong>  </p>

<p>Consider following example.</p>

<p>For instance I am using <code>GoDaddy</code> mail servers. It has settings for SMTP server and for IMAP server. </p>

<p>Whenever I enter this settings in MUA <code>Outlook</code> I get all mail that was received and sent from my account. </p>

<p>So where this mail is stored ? Is it stored on <code>GoDaddy</code> server specified in IMAP settings ? Or it is stored somewhere else ? </p>

<p>Please help to understand letter flow through all this infrastructure.
And how can I setup my local server that will behave exactly the same as GoDaddy's mail server ?</p>

<p>Thanks</p>
","<email><postfix><smtp><email-server><imap>","2016-10-16 18:45:40"
"951704","How to create a gretap tunnel between two AWS EC2 instances?","<p>To create a tunnel between two machines connected to the same network, I can run the following commands:</p>

<p><strong>From host A:</strong>  </p>

<pre><code>sudo ip link add gretap1 type gretap local HOST_A_IP remote HOST_B_IP
sudo ip link set gretap1 up
</code></pre>

<p><strong>From host B:</strong>  </p>

<pre><code>sudo ip link add gretap1 type gretap local HOST_B_IP remote HOST_A_IP
sudo ip link set gretap1 up
</code></pre>

<p>Here because A and B are on the same network, I can use private IP addresses <code>192.168.x.x</code>.</p>

<p>Now, I would like to create a tunnel between two AWS EC2 instances. I am able to create it using PPP over SSH, but can I create it with the above method ?  </p>

<p><strong>Here is the big picture:</strong>  </p>

<p><a href=""https://i.sstatic.net/3D6ix.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3D6ix.png"" alt=""enter image description here""></a></p>

<p>I need traffic to be exchanged between the local interfaces of hosts A and B through the tunnel.</p>
","<networking><vpn><router><tunneling>","2019-01-31 14:29:13"
"951803","how to set the FQDN on 2008 R2 server using the ip address?","<p>From what I understood from my searches, to avoid the <em>""Access denied - Invalid HELO name (See RFC2821 4.1.1.1)""</em> when sending emails using the SMTP on a 2008 R2 server, its FQDN must be set and to enter it I do the following:</p>

<ol>
<li>Open IIS 6.0 Manager</li>
<li>Click on the server name then right click on SMTP virtual server and select properties</li>
<li>Click on Delivery tab then Advanced</li>
</ol>

<p>At this point there is a text box that says ""Fully qualified domain name"". The server name is SERVER2 and the fixed i.p. is 206.xxx.xxx.5
What should I enter there ? I don't have a DNS to point to.
I tried SERVER2.206.xxx.xxx.5 but when I clicked the ""Check DNS"" button it says it is invalid.</p>
","<windows-server-2008><smtp>","2019-02-01 00:03:40"
"809532","rsyslog execute action on multiple events","<p>I would like to have an application to be executed when an event reaches multiple times in a row in the log, eg:</p>

<pre><code>Oct 17 13:09:24 mail clamav-milter[30942]:
x
x
x
</code></pre>

<p>Happens 4 times but also this counter should be reset like in every hour. Is this possible to do with <code>rsyslog</code> or <code>syslog-ng</code>?</p>

<p>Thanks</p>
","<logging><rsyslog><syslog-ng>","2016-10-17 14:13:33"
"951851","High Available service - IP/DNS single point of failure","<p>I'm looking for High Available solution for websites and already find
Kubernates , Docker Swarm ... but all these things should balance only webserver , db and other running service in the cluster. But what about the IP addresses DNS.
Let's assume that I decide to use Docker Swarm , and create nodes in 3 independent hosting providers: AWS , Digital Ocean , Rackspace or something else.. then I have my websites replicated on 3 locations and all sounds perfect, but in same moment something need to direct the visitors to my servers, for this   purpose probably I have to use Round robin DNS (A record which to point to the 3 IPs). What if one of my nodes fail? The DNS still will point to there and some of visitors will not be able to connect. Please let me know what are the solutions in this case. Probably Round robin DNS is not the only required for HA services.</p>

<p>Assume that AWS ELB or DO LB going down, because some bad people decide to send them few hundred Gbits flood :) My point is what is best solution to keep HA in case of LB fail by some reason.</p>
","<high-availability><redundancy>","2019-02-01 09:37:05"
"951968","Run iptables-save before shutdown/reboot","<p>I have installed iptables-persistent on my Ubuntu 18.04 server and search for a solution that I don't need to save the iptables configuration (iptables-save > /etc/iptables/rules.v4) every time before a reboot or shutdown.</p>

<p>So I created the following script (/etc/init.d/iptables-save) with the command and made it executable.</p>

<pre><code>#!/bin/sh
iptables-save &gt; /etc/iptables/rules.v4
</code></pre>

<p>After that I created a link in /etc/rc6.d:</p>

<pre><code>ln -s /etc/init.d/iptables-save /etc/rc6.d/K01iptables-save
</code></pre>

<p>Now it should be executed before a shutdown, but it doesn't work.</p>

<p>Why doesn't that work? Is there another way to execute this one command before a shutdown/reboot?</p>
","<ubuntu><iptables><bash>","2019-02-01 20:15:26"
"809866","ssh connection stopped working","<p>I'm connecting to a remote server via ssh:</p>

<pre><code>ssh user@ip
</code></pre>

<p>I normally use the wifi network but when it fails, I use a 3G modem.
I have been working this way for few months.
But now I'm unable to connect to the server.</p>

<p>I'm getting the following message:</p>

<pre><code>ssh: connect to host &lt;ip&gt; port 22: Connection timed out
</code></pre>

<p>I had tried to access the different servers also and got the same error, other people are able to connect to the server from their computers so the problems have to be on my side.
I'm using linux mint 17.</p>
","<ssh><remote>","2016-10-18 22:55:59"
"809938","Install Windows Server 2012 on separate disk or in RAID 5?","<p>I have a RAID controller with 4 1TB disks and first I put them all in a RAID 5 array, partitioned 1TB for OS and 2TB for data.</p>

<p>Then I noticed that the raid array was never idle. It was constantly writing 24/7 and I figured it must be the swap file.</p>

<p>It feels like it would wear out all drives in the array faster and slow down other read/write operations.</p>

<p>Now I'm reinstalling the server and need some advice. Should I exclude 1 disk from the array (put it as RAID 0) for the OS and use the other 3 disks in RAID 5 for data?</p>

<p>In this configuration it feels like the swapping will only wear out one drive instead of all 4.</p>

<p>EDIT:
It is an HPE ProLiant DL60 with 8MB RAM and an onboard raid controller capable of RAID 0, RAID 1+0 and RAID 5.</p>

<p>The server will mainly be used as a file server and an SQL Server. It will also run some smaller services such as Subversion Server.</p>

<p>EDIT2:
Yes, I have googled and I didn't find any clear recommendations if an OS was better of on its own drive compared to being in the raid. RAID 5 is for performance not backup (and everyone knows that, right?!?). There is a backup to a single external USB drive twice a day.</p>
","<windows-server-2012-r2><raid5><swap><operating-system><raid0>","2016-10-19 10:23:58"
"952424","Configure routing for multiple USB modems linux","<p>i am trying to create a custom proxy server. I have a Rasp Pi box with 2 USB LTE modems and LAN cable connected to it. I am trying to get them working together with squid proxy so that i could send traffic to specific the modem's NIC depending on the internal ip, also i need to be able to address modem's admin dashboard to be able to reset it programmiracly if it stops working properly. So, when i connect modem alone to the rasp box i see that admin dashboard has default gateway`s address - 192.168.0.1, so i figured i could get going with this setup:</p>

<pre><code>$: cat /etc/network/interfaces
source-directory /etc/network/interfaces.d

auto eth0
iface eth0 inet dhcp

allow-hotplug eth1
iface eth1 inet static
    address 192.168.1.100
    netmask 255.255.255.0

allow-hotplug eth2
iface eth2 inet static
    address 192.168.2.100
    netmask 255.255.255.0

$: ip route add 192.168.1.0/24 dev eth1 table rteth1
$: ip route add default via 192.168.1.1 dev eth1 table rteth1
$: ip route add 10.0.0.0/24 dev eth0 src 10.0.0.1 table rteth1
$: ip rule add from 192.168.1.0/24 table rteth1
$: ip rule add to 192.168.1.0/24 table rteth1
$: iptables -t nat -A POSTROUTING -o eth1 -j MASQUERADE
$: iptables -t mangle -A POSTROUTING -m ttl --ttl-gt 50 -o eth1 -j TTL --ttl-set 65
</code></pre>

<p>The same setup for 192.168.2.0/24 network. When i configure the network this way and try to request admin dashboard(192.168.1.1) - i get this error: </p>

<pre><code>$: curl 192.168.1.1
curl: (7) Failed to connect to 192.168.1.1 port 80: No route to host
</code></pre>

<p>When i try to setup it with:
$: ip route add default via 192.168.0.1 dev eth1 table rteth1</p>

<p>I get: <code>RTNETLINK answers: Network is unreachable</code></p>

<p>How can i properly set up such network?</p>
","<iptables><routing><linux-networking>","2019-02-05 16:48:04"
"952500","I am trying to setup a fault tolerance university campus network","<p><a href=""https://i.sstatic.net/rwOUo.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rwOUo.png"" alt=""My vlan concept""></a></p>

<p>I am creating a network between tutor vlan 10 and student vlan 20 consisting of 3 campuses. Is it possible to connect the campuses together in a fault tolerance VLAN network?</p>
","<cisco>","2019-02-05 23:01:43"
"952743","get string inside quotes with sed","<p>I have a file with text like these:</p>

<pre><code>""id"":1519539,""description"":""xxxxxxxx"",""severity"":""yyy"",""pasahelpdesk"":null,""subSource"":"""",""dateReception"":{""timezone"":{""name"":""Europe\/France"",""timestamp"":1549493320},""sg_datatables_editable"":[
false,false,false,false,false,false,false,false,false,false,false]},{
""id"":1519540,""description"":""xxxxxxxx"",""severity"":""yyy"",""pasahelpdesk"":null,""subSource"":"""",""dateReception"":{""timezone"":{""name"":""Europe\/France"",""timestamp"":1549493340},""sg_datatables_editable"":[
false,false,false,false,false,false,false,false,false,false,false]},{
</code></pre>

<p>I'm trying to get value of timestamp. I'm trying with:</p>

<pre><code>sed ""s/.*timestamp\"":\(.*\).*$/\1/g""
</code></pre>

<p>but it gives the value of timestamp but also all values until end of the line:</p>

<pre><code>1549493320},""sg_datatables_editable"":[
false,false,false,false,false,false,false,false,false,false,false]},{
1549493340},""sg_datatables_editable"":[
false,false,false,false,false,false,false,false,false,false,false]},{
</code></pre>

<p>How can I get only:</p>

<pre><code>1549493320
1549493340
</code></pre>
","<sed>","2019-02-07 08:12:25"
"953182","Postfix: Illegal address from unknown sender","<p>I got this alert from my maillog postfix</p>

<blockquote>
  <p>mail postfix/smtpd[13563]: warning: hostname riservata.miami19.mia.seabone.net does not resolve to address 195.22.199.54: Name or service not known</p>
</blockquote>

<p>Do you think this is a security concern?</p>

<p>How to prevent this to happen again.</p>

<p>TIA</p>
","<postfix><email-server>","2019-02-10 03:13:35"
"953224","Mogrify commands within crontab while ensuring files aren't processed twice","<p>I have an image folder that serves content, it contains jpeg, jpg and gif files. More files are added automatically all the time.</p>

<p>What I intend on doing is adding the following commands to crontab (with the appropriate file path of course)</p>

<p>mogrify -quality 80% ""*.jpg""</p>

<p>mogrify -quality 80% ""*.jpeg""</p>

<p>mogrify -quality 80% ""*.gif""</p>

<p>I'd like to respectfully ask if anyone knows if these commands would end up processing files multiple times, so when a file is converted to 80%, then next time cron runs it reduces it further etc...</p>

<p>Does ImageMagick ensure it doesn't process the same file twice, and if not how do I deal with this given I can't rename the files.</p>

<p>Any help from the community would be greatly appreciated.</p>

<p>thank you.</p>

<p>Edited: Thanks to user: Sven for the answer below. This is the script I am now using that appears to be working:</p>

<p>Thank you for this. It appears that mogrify can write in place, preliminarily.</p>

<p>I will use this script:</p>

<pre><code>#!/bin/bash

watchedFolder=[insert path here]
quality=70

while [ true ]
do
fileName=$(inotifywait -q -e create --format ""%f"" ""$watchedFolder"")
sleep 1s
mogrify -quality 80% ""$watchedFolder/$fileName""
sleep 2s
echo ""Processed "" $fileName
sleep 2s
done
</code></pre>
","<cron><imagemagick>","2019-02-10 14:52:15"
"953682","CNAME that points domain A to domain B results in a Security Warning","<p>I'm using a service (<code>banana.com</code>) that allows me to use my domain (<code>orange.com</code>) to log in.</p>

<p>They mention to add a CNAME record like so:</p>

<pre><code>an.orange.com. 3600 IN CNAME service.banana.com.
</code></pre>

<p>I did that but when I access the URL, I get ( on Chrome ) an error <em>Your connection is not private""""</em> :</p>

<p><img src=""https://i.sstatic.net/ZPsqZ.png"" alt=""Your connection is not private""></p>

<p>My server runs on Apache.</p>

<p>Why would the server have to prove that it is <code>an.orange.com</code>? Why is it even hitting my server in the first place? Shouldn't the address resolve to <code>service.banana.com</code>?</p>
","<domain-name-system><apache-2.4><ssl-certificate><https><cname-record>","2019-02-13 09:45:51"
"953806","IS DNS speed important for a server?","<p>I want to ask if choosing the DNS server selection is important for server speed.</p>

<p>For example: I have small office server connected to ISP internet. I have a modem connected to server, so public IP and DNS servers from ISP are loaded directly to server.</p>

<p>If a client visits the site his set up DNS will resolve my IP and he will be connected to my server, which will serve the static files.</p>

<p>Will my server need look up to DNS for delivering that static file to him?</p>

<p>My understanding of DNS are that its some type of ""library"" of IP addresses for domain names. Choosing faster DNS server can resolve to faster loading of files for client.</p>

<p>But can setting fastest DNS on my server lead to faster loading of my site for client?</p>
","<linux><domain-name-system>","2019-02-13 20:41:49"
"953934","systemd-resolvd seems to make up a non-existing AAAA entry?","<p>One of my worker servers is experiencing problems connecting to a central server. This happened suddenly, it worked earlier, and it still works on an identical other worker.</p>

<p>After some inspection the problem turns out to be this:</p>

<pre><code>root@svc1:~# nslookup central.example.com
Server:     127.0.0.53
Address:    127.0.0.53#53

Non-authoritative answer:
Name:   central.example.com
Address: 26.156.133.221
Name:   central.example.com
Address: 64:ff9b::2ea6:1337
</code></pre>

<p><code>systemd-resolvd</code> seems to think that the central server has an ipv6 address. But it doesn't, and never has:</p>

<pre><code>root@svc1:~# dig central.example.com @ns1.example.com in AAAA    
...    
# there is no ANSWER SECTION
</code></pre>

<p>I could probably somehow clear the local DNS cache and fix the connection issue. But how could this happen, and how can I prevent it from happening again in the future?</p>
","<domain-name-system><ipv6><systemd><ubuntu-18.04>","2019-02-14 12:32:57"